{
  "code": "def _to_numpy(self,\n                  data: dict[int, EventData],\n                  is_live: bool) -> tuple[np.ndarray, np.ndarray]:\n        \"\"\" Extract each individual step data into separate numpy arrays for loss and timestamps.\n\n        Timestamps are stored float64 as the extra accuracy is needed for correct timings. Arrays\n        are returned at the length of the shortest available data (i.e. truncated records are\n        dropped)\n\n        Parameters\n        ----------\n        data: dict\n            The incoming tensorflow event data in dictionary form per step\n        is_live: bool, optional\n            ``True`` if the data to be cached is from a live training session otherwise ``False``.\n            Default: ``False``\n\n        Returns\n        -------\n        timestamps: :class:`numpy.ndarray`\n            float64 array of all iteration's timestamps\n        loss: :class:`numpy.ndarray`\n            float32 array of all iteration's loss\n        \"\"\"\n        if is_live and self._carry_over:\n            logger.debug(\"Processing carry over: %s\", self._carry_over)\n            self._collect_carry_over(data)\n\n        times, loss = self._process_data(data, is_live)\n\n        if is_live and not all(len(val) == len(self._loss_labels) for val in loss):\n            # TODO Many attempts have been made to fix this for live graph logging, and the issue\n            # of non-consistent loss record sizes keeps coming up. In the meantime we shall swallow\n            # any loss values that are of incorrect length so graph remains functional. This will,\n            # most likely, lead to a mismatch on iteration count so a proper fix should be\n            # implemented.\n\n            # Timestamps and loss appears to remain consistent with each other, but sometimes loss\n            # appears non-consistent. eg (lengths):\n            # [2, 2, 2, 2, 2, 2, 2, 0] - last loss collection has zero length\n            # [1, 2, 2, 2, 2, 2, 2, 2] - 1st loss collection has 1 length\n            # [2, 2, 2, 3, 2, 2, 2] - 4th loss collection has 3 length\n\n            logger.debug(\"Inconsistent loss found in collection: %s\", loss)\n            for idx in reversed(range(len(loss))):\n                if len(loss[idx]) != len(self._loss_labels):\n                    logger.debug(\"Removing loss/timestamps at position %s\", idx)\n                    del loss[idx]\n                    del times[idx]\n\n        n_times, n_loss = (np.array(times, dtype=\"float64\"), np.array(loss, dtype=\"float32\"))\n        logger.debug(\"Converted to numpy: (data points: %s, timestamps shape: %s, loss shape: %s)\",\n                     len(data), n_times.shape, n_loss.shape)\n\n        return n_times, n_loss",
  "smell": [
    {
      "smell_id": 4,
      "line_no": 48,
      "description": "The element is deleted from a container by a dynamically determined index."
    }
  ]
}