{
  "code": "def _process_observations(async_vector_env, policies, batch_builder_pool,\n                          active_episodes, unfiltered_obs, rewards, dones,\n                          infos, off_policy_actions, horizon, obs_filters,\n                          unroll_length, pack, callbacks):\n    \"\"\"Record new data from the environment and prepare for policy evaluation.\n\n    Returns:\n        active_envs: set of non-terminated env ids\n        to_eval: map of policy_id to list of agent PolicyEvalData\n        outputs: list of metrics and samples to return from the sampler\n    \"\"\"\n\n    active_envs = set()\n    to_eval = defaultdict(list)\n    outputs = []\n\n    # For each environment\n    for env_id, agent_obs in unfiltered_obs.items():\n        new_episode = env_id not in active_episodes\n        episode = active_episodes[env_id]\n        if not new_episode:\n            episode.length += 1\n            episode.batch_builder.count += 1\n            episode._add_agent_rewards(rewards[env_id])\n\n        # Check episode termination conditions\n        if dones[env_id][\"__all__\"] or episode.length >= horizon:\n            all_done = True\n            atari_metrics = _fetch_atari_metrics(async_vector_env)\n            if atari_metrics is not None:\n                for m in atari_metrics:\n                    outputs.append(\n                        m._replace(custom_metrics=episode.custom_metrics))\n            else:\n                outputs.append(\n                    RolloutMetrics(episode.length, episode.total_reward,\n                                   dict(episode.agent_rewards),\n                                   episode.custom_metrics))\n        else:\n            all_done = False\n            active_envs.add(env_id)\n\n        # For each agent in the environment\n        for agent_id, raw_obs in agent_obs.items():\n            policy_id = episode.policy_for(agent_id)\n            filtered_obs = _get_or_raise(obs_filters, policy_id)(raw_obs)\n            agent_done = bool(all_done or dones[env_id].get(agent_id))\n            if not agent_done:\n                to_eval[policy_id].append(\n                    PolicyEvalData(env_id, agent_id, filtered_obs,\n                                   episode.rnn_state_for(agent_id),\n                                   episode.last_action_for(agent_id),\n                                   rewards[env_id][agent_id] or 0.0))\n\n            last_observation = episode.last_observation_for(agent_id)\n            episode._set_last_observation(agent_id, filtered_obs)\n\n            # Record transition info if applicable\n            if last_observation is not None and \\\n                    infos[env_id][agent_id].get(\"training_enabled\", True):\n                episode.batch_builder.add_values(\n                    agent_id,\n                    policy_id,\n                    t=episode.length - 1,\n                    eps_id=episode.episode_id,\n                    agent_index=episode._agent_index(agent_id),\n                    obs=last_observation,\n                    actions=episode.last_action_for(agent_id),\n                    rewards=rewards[env_id][agent_id],\n                    prev_actions=episode.prev_action_for(agent_id),\n                    prev_rewards=episode.prev_reward_for(agent_id),\n                    dones=agent_done,\n                    infos=infos[env_id][agent_id],\n                    new_obs=filtered_obs,\n                    **episode.last_pi_info_for(agent_id))\n\n        # Invoke the step callback after the step is logged to the episode\n        if callbacks.get(\"on_episode_step\"):\n            callbacks[\"on_episode_step\"]({\n                \"env\": async_vector_env,\n                \"episode\": episode\n            })\n\n        # Cut the batch if we're not packing multiple episodes into one,\n        # or if we've exceeded the requested batch size.\n        if episode.batch_builder.has_pending_data():\n            if (all_done and not pack) or \\\n                    episode.batch_builder.count >= unroll_length:\n                outputs.append(episode.batch_builder.build_and_reset(episode))\n            elif all_done:\n                # Make sure postprocessor stays within one episode\n                episode.batch_builder.postprocess_batch_so_far(episode)\n\n        if all_done:\n            # Handle episode termination\n            batch_builder_pool.append(episode.batch_builder)\n            if callbacks.get(\"on_episode_end\"):\n                callbacks[\"on_episode_end\"]({\n                    \"env\": async_vector_env,\n                    \"episode\": episode\n                })\n            del active_episodes[env_id]\n            resetted_obs = async_vector_env.try_reset(env_id)\n            if resetted_obs is None:\n                # Reset not supported, drop this env from the ready list\n                if horizon != float(\"inf\"):\n                    raise ValueError(\n                        \"Setting episode horizon requires reset() support \"\n                        \"from the environment.\")\n            else:\n                # Creates a new episode\n                episode = active_episodes[env_id]\n                for agent_id, raw_obs in resetted_obs.items():\n                    policy_id = episode.policy_for(agent_id)\n                    policy = _get_or_raise(policies, policy_id)\n                    filtered_obs = _get_or_raise(obs_filters,\n                                                 policy_id)(raw_obs)\n                    episode._set_last_observation(agent_id, filtered_obs)\n                    to_eval[policy_id].append(\n                        PolicyEvalData(\n                            env_id, agent_id, filtered_obs,\n                            episode.rnn_state_for(agent_id),\n                            np.zeros_like(\n                                _flatten_action(policy.action_space.sample())),\n                            0.0))\n\n    return active_envs, to_eval, outputs",
  "smell": [
    {
      "smell_id": 4,
      "line_no": 102,
      "description": "The element is deleted from a container by a dynamically determined index."
    }
  ]
}