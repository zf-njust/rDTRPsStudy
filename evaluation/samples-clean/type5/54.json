{
  "code": "def fit(self, X, y):\\n        \"\"\"Fit linear model with coordinate descent\\n\\n        Fit is on grid of alphas and best alpha estimated by cross-validation.\\n\\n        Parameters\\n        ----------\\n        X : {array-like}, shape (n_samples, n_features)\\n            Training data. Pass directly as float64, Fortran-contiguous data\\n            to avoid unnecessary memory duplication. If y is mono-output,\\n            X can be sparse.\\n\\n        y : array-like, shape (n_samples,) or (n_samples, n_targets)\\n            Target values\\n        \"\"\"\\n        y = np.asarray(y, dtype=np.float64)\\n        if y.shape[0] == 0:\\n            raise ValueError(\"y has 0 samples: %r\" % y)\\n\\n        if hasattr(self, 'l1_ratio'):\\n            model_str = 'ElasticNet'\\n        else:\\n            model_str = 'Lasso'\\n\\n        if isinstance(self, ElasticNetCV) or isinstance(self, LassoCV):\\n            if model_str == 'ElasticNet':\\n                model = ElasticNet()\\n            else:\\n                model = Lasso()\\n            if y.ndim > 1 and y.shape[1] > 1:\\n                raise ValueError(\"For multi-task outputs, use \"\\n                                 \"MultiTask%sCV\" % (model_str))\\n            y = column_or_1d(y, warn=True)\\n        else:\\n            if sparse.isspmatrix(X):\\n                raise TypeError(\"X should be dense but a sparse matrix was\"\\n                                \"passed\")\\n            elif y.ndim == 1:\\n                raise ValueError(\"For mono-task outputs, use \"\\n                                 \"%sCV\" % (model_str))\\n            if model_str == 'ElasticNet':\\n                model = MultiTaskElasticNet()\\n            else:\\n                model = MultiTaskLasso()\\n\\n        if self.selection not in [\"random\", \"cyclic\"]:\\n            raise ValueError(\"selection should be either random or cyclic.\")\\n\\n        # This makes sure that there is no duplication in memory.\\n        # Dealing right with copy_X is important in the following:\\n        # Multiple functions touch X and subsamples of X and can induce a\\n        # lot of duplication of memory\\n        copy_X = self.copy_X and self.fit_intercept\\n\\n        if isinstance(X, np.ndarray) or sparse.isspmatrix(X):\\n            # Keep a reference to X\\n            reference_to_old_X = X\\n            # Let us not impose fortran ordering or float64 so far: it is\\n            # not useful for the cross-validation loop and will be done\\n            # by the model fitting itself\\n            X = check_array(X, 'csc', copy=False)\\n            if sparse.isspmatrix(X):\\n                if (hasattr(reference_to_old_X, \"data\") and\\n                        not np.may_share_memory(reference_to_old_X.data, X.data)):\\n                    # X is a sparse matrix and has been copied\\n                    copy_X = False\\n            elif not np.may_share_memory(reference_to_old_X, X):\\n                # X has been copied\\n                copy_X = False\\n            del reference_to_old_X\\n        else:\\n            X = check_array(X, 'csc', dtype=np.float64, order='F', copy=copy_X)\\n            copy_X = False\\n\\n        if X.shape[0] != y.shape[0]:\\n            raise ValueError(\"X and y have inconsistent dimensions (%d != %d)\"\\n                             % (X.shape[0], y.shape[0]))\\n\\n        # All LinearModelCV parameters except 'cv' are acceptable\\n        path_params = self.get_params()\\n        if 'l1_ratio' in path_params:\\n            l1_ratios = np.atleast_1d(path_params['l1_ratio'])\\n            # For the first path, we need to set l1_ratio\\n            path_params['l1_ratio'] = l1_ratios[0]\\n        else:\\n            l1_ratios = [1, ]\\n        path_params.pop('cv', None)\\n        path_params.pop('n_jobs', None)\\n\\n        alphas = self.alphas\\n        n_l1_ratio = len(l1_ratios)\\n        if alphas is None:\\n            alphas = []\\n            for l1_ratio in l1_ratios:\\n                alphas.append(_alpha_grid(\\n                    X, y, l1_ratio=l1_ratio,\\n                    fit_intercept=self.fit_intercept,\\n                    eps=self.eps, n_alphas=self.n_alphas,\\n                    normalize=self.normalize,\\n                    copy_X=self.copy_X))\\n        else:\\n            # Making sure alphas is properly ordered.\\n            alphas = np.tile(np.sort(alphas)[::-1], (n_l1_ratio, 1))\\n        # We want n_alphas to be the number of alphas used for each l1_ratio.\\n        n_alphas = len(alphas[0])\\n        path_params.update({'n_alphas': n_alphas})\\n\\n        path_params['copy_X'] = copy_X\\n        # We are not computing in parallel, we can modify X\\n        # inplace in the folds\\n        if not (self.n_jobs == 1 or self.n_jobs is None):\\n            path_params['copy_X'] = False\\n\\n        # init cross-validation generator\\n        cv = check_cv(self.cv, X)\\n\\n        # Compute path for all folds and compute MSE to get the best alpha\\n        folds = list(cv)\\n        best_mse = np.inf\\n\\n        # We do a double for loop folded in one, in order to be able to\\n        # iterate in parallel on l1_ratio and folds\\n        jobs = (delayed(_path_residuals)(X, y, train, test, self.path,\\n                                         path_params, alphas=this_alphas,\\n                                         l1_ratio=this_l1_ratio, X_order='F',\\n                                         dtype=np.float64)\\n                for this_l1_ratio, this_alphas in zip(l1_ratios, alphas)\\n                for train, test in folds)\\n        mse_paths = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\\n                             backend=\"threading\")(jobs)\\n        mse_paths = np.reshape(mse_paths, (n_l1_ratio, len(folds), -1))\\n        mean_mse = np.mean(mse_paths, axis=1)\\n        self.mse_path_ = np.squeeze(np.rollaxis(mse_paths, 2, 1))\\n        for l1_ratio, l1_alphas, mse_alphas in zip(l1_ratios, alphas,\\n                                                   mean_mse):\\n            i_best_alpha = np.argmin(mse_alphas)\\n            this_best_mse = mse_alphas[i_best_alpha]\\n            if this_best_mse < best_mse:\\n                best_alpha = l1_alphas[i_best_alpha]\\n                best_l1_ratio = l1_ratio\\n                best_mse = this_best_mse\\n\\n        self.l1_ratio_ = best_l1_ratio\\n        self.alpha_ = best_alpha\\n        if self.alphas is None:\\n            self.alphas_ = np.asarray(alphas)\\n            if n_l1_ratio == 1:\\n                self.alphas_ = self.alphas_[0]\\n        # Remove duplicate alphas in case alphas is provided.\\n        else:\\n            self.alphas_ = np.asarray(alphas[0])\\n\\n        # Refit the model with the parameters selected\\n        common_params = dict((name, value)\\n                             for name, value in self.get_params().items()\\n                             if name in model.get_params())\\n        model.set_params(**common_params)\\n        model.alpha = best_alpha\\n        model.l1_ratio = best_l1_ratio\\n        model.copy_X = copy_X\\n        model.precompute = False\\n        model.fit(X, y)\\n        if not hasattr(self, 'l1_ratio'):\\n            del self.l1_ratio_\\n        self.coef_ = model.coef_\\n        self.intercept_ = model.intercept_\\n        self.dual_gap_ = model.dual_gap_\\n        self.n_iter_ = model.n_iter_\\n        return self",
  "smell": [
    {
      "smell_id": 5,
      "line_no": 164,
      "description": "The attribute is deleted from an object through a dynamically determined name."
    }
  ]
}