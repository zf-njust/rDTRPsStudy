{
  "code": "def decode_into_node_and_backreferences_without_space(subtoken_ids, tokenizer):\n    rex_arg = re.compile(f\"^{tokenizer.INIT}(op|snt|conj|prep)\")\n    rex_spc = re.compile(r\"<(s|/s|lit|/lit|stop|unk|pad|mask)>\")\n\n    # get strings\n    subtokens = tokenizer.convert_ids_to_tokens(subtoken_ids)\n    # fix backreferences\n    subtoken_backreferences = [max(t - len(tokenizer), -1) for t in subtoken_ids]\n    # strip padding\n    no_pad = [(s, b) for s, b in zip(subtokens, subtoken_backreferences) if s != (tokenizer.INIT + '<pad>')]\n    if no_pad:\n        subtokens, subtoken_backreferences = zip(*no_pad)\n    else:\n        subtokens, subtoken_backreferences = ['<s>'], [-1]\n\n    # subword collapse\n    tokens = []\n    backreferences = []\n    subword_to_token_map = {}\n    current_token_i = 0\n    prev_is_pointer = False\n    prev_is_rel = False\n    for subw_i, (subw_backr, subtok) in enumerate(zip(subtoken_backreferences, subtokens)):\n        subword_to_token_map[subw_i] = current_token_i\n        is_pointer = subtok.startswith('<pointer:') and subtok.endswith('>')\n        is_rel = subtok.startswith(':') and len(subtok) > 1\n        is_bracket = subtok in '()'\n\n        # if empty you cannot do anything but add a new word\n        if not tokens:\n            tokens.append(subtok)\n            backreferences.append(-1)\n            current_token_i += 1\n\n        # backref can't be splitted\n        elif subw_backr > -1:\n            tokens.append(None)\n            backreferences.append(subword_to_token_map[subw_backr])\n            current_token_i += 1\n\n        # after a special token release\n        elif isinstance(tokens[-1], str) and rex_spc.match(tokens[-1]):\n            tokens.append(subtok)\n            backreferences.append(-1)\n            current_token_i += 1\n\n        # after a subtoken ':' (which should be followed by the rest of the edge) ignore tokenizer.INIT\n        # TODO: this is an ugly patch due to the fact that BART tokenizer splits after ':'\n        elif (tokens[-1] == ':') and rex_arg.match(subtok):\n            tokens[-1] = tokens[-1] + subtok[1:]\n\n        # current or prev is a control token\n        elif (is_pointer or is_rel or prev_is_pointer or prev_is_rel or is_bracket or subtok == '</s>') \\\n                and subtok != '-of':\n            tokens.append(subtok)\n            backreferences.append(-1)\n            current_token_i += 1\n\n        # very ugly patch for some cases in which tokenizer.INIT is not in the following token to the edge\n        elif isinstance(tokens[-1], str) and tokens[-1].startswith(':') and tokens[-1][-1].isdigit() and (\n                subtok != '-of'):\n            tokens.append(subtok)\n            backreferences.append(-1)\n            current_token_i += 1\n\n        # in any other case attach to the previous\n        else:\n            tokens[-1] = tokens[-1] + subtok\n\n        prev_is_pointer = is_pointer\n        prev_is_rel = is_rel\n\n    # strip INIT and fix byte-level\n    tokens = [tokenizer.convert_tokens_to_string(list(t)).lstrip() if isinstance(t, str) else t for t in tokens]\n    # tokens = [t.replace(tokenizer.INIT, '') if isinstance(t, str) else t for t in tokens]\n\n    # unks are substituted with thing\n    tokens = [t if t != '<unk>' else 'thing' for t in tokens]\n\n    old_tokens = tokens\n    old_backreferences = backreferences\n\n    # <lit> Barack Obama </lit> -> \"Barack Obama\"\n    tokens = []\n    backreferences = []\n    token_to_token_map = {}\n    start_search = 0\n    removed = 0\n    while True:\n        try:\n\n            lit_start = old_tokens.index('<lit>', start_search)\n            token_addition = old_tokens[start_search:lit_start]\n            for i, t in enumerate(token_addition, start=start_search):\n                token_to_token_map[i] = i - removed\n            tokens += token_addition\n\n            backreferences_addition = [token_to_token_map[b] if b > -1 else -1 for b in\n                                       old_backreferences[start_search:lit_start]]\n            backreferences += backreferences_addition\n\n            lit_end = min(lit_start + 2, len(old_tokens) - 1)\n\n            while lit_end < len(old_tokens):\n                old_tok = old_tokens[lit_end]\n\n                if isinstance(old_tok, str) and (\n                        (old_tok.startswith(':') and len(old_tok) > 3) or (old_tok == '<stop>')):\n                    res_tok = old_tokens[lit_start + 1:lit_end]\n                    for i in range(lit_start, lit_end):\n                        token_to_token_map[i] = len(tokens)\n\n                    # Remove possible wrong None\n                    res = old_tokens[lit_start + 1:lit_end]\n                    res = [str(r) for r in res if r is not None]\n                    res = '\"' + '_'.join(res) + '\"'\n\n                    removed += len(res_tok)\n                    start_search = lit_end\n                    tokens += [res, old_tok]\n                    backreferences += [-1, -1]\n                    break\n\n                elif old_tok == '</lit>':\n                    res_tok = old_tokens[lit_start + 1:lit_end]\n                    for i in range(lit_start, lit_end + 1):\n                        token_to_token_map[i] = len(tokens)\n\n                    # Remove possible wrong None\n                    res = old_tokens[lit_start + 1:lit_end]\n                    res = [str(r) for r in res if r is not None]\n                    res = '\"' + '_'.join(res) + '\"'\n\n                    removed += len(res_tok) + 1\n                    start_search = lit_end + 1\n                    tokens.append(res)\n                    backreferences.append(-1)\n                    break\n\n                else:\n                    lit_end += 1\n                    start_search = lit_end\n\n        except ValueError:\n            token_addition = old_tokens[start_search:]\n            for i, t in enumerate(token_addition, start=start_search):\n                token_to_token_map[i] = i - removed\n            backreferences_addition = [token_to_token_map[b] if b > -1 else b for b in\n                                       old_backreferences[start_search:]]\n            tokens += token_addition\n            backreferences += backreferences_addition\n            break\n\n    tokens = [token_processing(t) for t in tokens]\n\n    shift = 0\n    if len(tokens) > 1 and tokens[1] == '<s>':\n        shift = 1\n\n    tokens = tokens[shift:]\n    backreferences = [b if b == -1 else b - shift for b in backreferences[shift:]]\n\n    if tokens and tokens[-1] == '</s>':\n        tokens.pop()\n        backreferences.pop()\n\n    return tokens, backreferences",
  "smell": [
    {
      "smell_id": 1,
      "line_no": 116,
      "description": "The variable is redefined with an inconsistent type object."
    }
  ]
}