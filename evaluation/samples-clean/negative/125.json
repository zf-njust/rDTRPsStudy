{
    "code": "def get_numeric_gradient(place,\n                         scope,\n                         op,\n                         inputs,\n                         input_to_check,\n                         output_names,\n                         delta=0.005,\n                         in_place=False):\n    # FIXME: change this method by compile time concepts\n    set_input(scope, op, inputs, place)\n\n    def product(dim):\n        return six.moves.reduce(lambda a, b: a * b, dim, 1)\n\n    tensor_to_check = scope.find_var(input_to_check).get_tensor()\n    tensor_size = product(tensor_to_check.shape())\n    tensor_to_check_dtype = tensor_to_check._dtype()\n    if tensor_to_check_dtype == core.VarDesc.VarType.FP32:\n        tensor_to_check_dtype = np.float32\n    elif tensor_to_check_dtype == core.VarDesc.VarType.FP64:\n        tensor_to_check_dtype = np.float64\n    elif tensor_to_check_dtype == core.VarDesc.VarType.FP16:\n        tensor_to_check_dtype = np.float16\n        # set delta as np.float16, will automatic convert to float32, float64\n        delta = np.array(delta).astype(np.float16)\n    else:\n        raise ValueError(\"Not supported data type \" + str(\n            tensor_to_check_dtype))\n\n    def get_output():\n        sum = []\n        op.run(scope, place)\n        for output_name in output_names:\n            sum.append(\n                np.array(scope.find_var(output_name).get_tensor()).astype(\n                    tensor_to_check_dtype).mean())\n        return tensor_to_check_dtype(np.array(sum).sum() / len(output_names))\n\n    gradient_flat = np.zeros(shape=(tensor_size, ), dtype=tensor_to_check_dtype)\n\n    def __get_elem__(tensor, i):\n        if tensor_to_check_dtype == np.float16:\n            numpy_tensor = np.array(tensor).astype(np.float16)\n            numpy_tensor = numpy_tensor.flatten()\n            return numpy_tensor[i]\n        elif tensor_to_check_dtype == np.float32:\n            return tensor._get_float_element(i)\n        else:\n            return tensor._get_double_element(i)\n\n    def __set_elem__(tensor, i, e):\n        if tensor_to_check_dtype == np.float16:\n            numpy_tensor = np.array(tensor).astype(np.float16)\n            shape = numpy_tensor.shape\n            numpy_tensor = numpy_tensor.flatten()\n            numpy_tensor[i] = e\n            numpy_tensor = numpy_tensor.reshape(shape).view(np.uint16)\n            tensor.set(numpy_tensor, place)\n        elif tensor_to_check_dtype == np.float32:\n            tensor._set_float_element(i, e)\n        else:\n            tensor._set_double_element(i, e)\n\n    # we only compute gradient of one element each time.\n    # we use a for loop to compute the gradient of every element.\n    for i in six.moves.xrange(tensor_size):\n        if in_place:\n            set_input(scope, op, inputs, place)\n\n        # get one input element throw it's index i.\n        origin = __get_elem__(tensor_to_check, i)\n        # add delta to it, run op and then get the sum of the result tensor.\n        x_pos = origin + delta\n        __set_elem__(tensor_to_check, i, x_pos)\n        y_pos = get_output()\n\n        if in_place:\n            set_input(scope, op, inputs, place)\n\n        x_neg = origin - delta\n        __set_elem__(tensor_to_check, i, x_neg)\n        y_neg = get_output()\n\n        __set_elem__(tensor_to_check, i, origin)\n        gradient_flat[i] = (y_pos - y_neg) / delta / 2\n\n    return gradient_flat.reshape(tensor_to_check.shape())",
    "smell": []
}