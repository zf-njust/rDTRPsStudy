{
    "code": "def _pad_to_window_size(\n      self,\n      word_ids,\n      mask,\n      type_ids,\n      word_embeddings,\n      pad_token_id,\n  ):\n    # padding\n    attention_window = max(self._attention_window)\n\n    assert (attention_window %\n            2 == 0), ('`attention_window` should be an even value.'\n                      f'Given {attention_window}')\n\n    input_shape = get_shape_list(\n        word_ids) if word_ids is not None else get_shape_list(word_embeddings)\n    batch_size, seq_len = input_shape[:2]\n\n    if seq_len is not None:\n      padding_len = (attention_window -\n                     seq_len % attention_window) % attention_window\n    else:\n      padding_len = 0\n\n    paddings = tf.convert_to_tensor([[0, 0], [0, padding_len]])\n\n    if word_ids is not None:\n      word_ids = tf.pad(word_ids, paddings, constant_values=pad_token_id)\n\n    if word_embeddings is not None:\n\n      def pad_embeddings():\n        word_ids_padding = tf.fill((batch_size, padding_len), self.pad_token_id)\n        word_embeddings_padding = self._embedding_layer(word_ids_padding)\n        return tf.concat([word_embeddings, word_embeddings_padding], axis=-2)\n\n      word_embeddings = tf.cond(\n          tf.math.greater(padding_len, 0), pad_embeddings,\n          lambda: word_embeddings)\n\n    mask = tf.pad(\n        mask, paddings,\n        constant_values=False)  # no attention on the padding tokens\n    token_type_ids = tf.pad(\n        type_ids, paddings, constant_values=0)  # pad with token_type_id = 0\n\n    return (\n        padding_len,\n        word_ids,\n        mask,\n        token_type_ids,\n        word_embeddings,\n    )",
    "smell": []
}