{
    "code": "def pretrain(\n    config: Config,\n    output_dir: Path,\n    resume_path: Optional[Path] = None,\n    epoch_resume: Optional[int] = None,\n    use_gpu: int = -1,\n    silent: bool = True,\n    skip_last: bool = False,\n):\n    msg = Printer(no_print=silent)\n    if config[\"training\"][\"seed\"] is not None:\n        fix_random_seed(config[\"training\"][\"seed\"])\n    allocator = config[\"training\"][\"gpu_allocator\"]\n    if use_gpu >= 0 and allocator:\n        set_gpu_allocator(allocator)\n    # ignore in pretraining because we're creating it now\n    config[\"initialize\"][\"init_tok2vec\"] = None\n    nlp = load_model_from_config(config)\n    _config = nlp.config.interpolate()\n    P = registry.resolve(_config[\"pretraining\"], schema=ConfigSchemaPretrain)\n    corpus = dot_to_object(_config, P[\"corpus\"])\n    corpus = registry.resolve({\"corpus\": corpus})[\"corpus\"]\n    batcher = P[\"batcher\"]\n    model = create_pretraining_model(nlp, P)\n    optimizer = P[\"optimizer\"]\n    # Load in pretrained weights to resume from\n    if resume_path is not None:\n        epoch_resume = _resume_model(model, resume_path, epoch_resume, silent=silent)\n    else:\n        # Without '--resume-path' the '--epoch-resume' argument is ignored\n        epoch_resume = 0\n\n    objective = model.attrs[\"loss\"]\n    # TODO: move this to logger function?\n    tracker = ProgressTracker(frequency=10000)\n    if P[\"n_save_epoch\"]:\n        msg.divider(\n            f\"Pre-training tok2vec layer - starting at epoch {epoch_resume} - saving every {P['n_save_epoch']} epoch\"\n        )\n    else:\n        msg.divider(f\"Pre-training tok2vec layer - starting at epoch {epoch_resume}\")\n    row_settings = {\"widths\": (3, 10, 10, 6, 4), \"aligns\": (\"r\", \"r\", \"r\", \"r\", \"r\")}\n    msg.row((\"#\", \"# Words\", \"Total Loss\", \"Loss\", \"w/s\"), **row_settings)\n\n    def _save_model(epoch, is_temp=False, is_last=False):\n        is_temp_str = \".temp\" if is_temp else \"\"\n        with model.use_params(optimizer.averages):\n            if is_last:\n                save_path = output_dir / f\"model-last.bin\"\n            else:\n                save_path = output_dir / f\"model{epoch}{is_temp_str}.bin\"\n            with (save_path).open(\"wb\") as file_:\n                file_.write(model.get_ref(\"tok2vec\").to_bytes())\n            log = {\n                \"nr_word\": tracker.nr_word,\n                \"loss\": tracker.loss,\n                \"epoch_loss\": tracker.epoch_loss,\n                \"epoch\": epoch,\n            }\n            with (output_dir / \"log.jsonl\").open(\"a\") as file_:\n                file_.write(srsly.json_dumps(log) + \"\\n\")\n\n    # TODO: I think we probably want this to look more like the\n    # 'create_train_batches' function?\n    try:\n        for epoch in range(epoch_resume, P[\"max_epochs\"]):\n            for batch_id, batch in enumerate(batcher(corpus(nlp))):\n                docs = ensure_docs(batch)\n                loss = make_update(model, docs, optimizer, objective)\n                progress = tracker.update(epoch, loss, docs)\n                if progress:\n                    msg.row(progress, **row_settings)\n                if P[\"n_save_every\"] and (batch_id % P[\"n_save_every\"] == 0):\n                    _save_model(epoch, is_temp=True)\n\n            if P[\"n_save_epoch\"]:\n                if epoch % P[\"n_save_epoch\"] == 0 or epoch == P[\"max_epochs\"] - 1:\n                    _save_model(epoch)\n            else:\n                _save_model(epoch)\n            tracker.epoch_loss = 0.0\n    finally:\n        if not skip_last:\n            _save_model(P[\"max_epochs\"], is_last=True)",
    "smell": []
}