{
    "code": "def __init__(self,\n               filter_size,\n               output_size,\n               num_units,\n               forget_bias=1.0,\n               activation=tf.tanh,\n               flatten_state=False,\n               clip_state=False,\n               output_bottleneck=False,\n               pre_bottleneck=False,\n               visualize_gates=False):\n    \"\"\"Initializes the basic LSTM cell.\n\n    Args:\n      filter_size: collection, conv filter size.\n      output_size: collection, the width/height dimensions of the cell/output.\n      num_units: int, The number of channels in the LSTM cell.\n      forget_bias: float, The bias added to forget gates (see above).\n      activation: Activation function of the inner states.\n      flatten_state: if True, state tensor will be flattened and stored as a 2-d\n        tensor. Use for exporting the model to tfmini.\n      clip_state: if True, clip state between [-6, 6].\n      output_bottleneck: if True, the cell bottleneck will be concatenated to\n        the cell output.\n      pre_bottleneck: if True, cell assumes that bottlenecking was performing\n        before the function was called.\n      visualize_gates: if True, add histogram summaries of all gates and outputs\n        to tensorboard.\n    \"\"\"\n    self._filter_size = list(filter_size)\n    self._output_size = list(output_size)\n    self._num_units = num_units\n    self._forget_bias = forget_bias\n    self._activation = activation\n    self._viz_gates = visualize_gates\n    self._flatten_state = flatten_state\n    self._clip_state = clip_state\n    self._output_bottleneck = output_bottleneck\n    self._pre_bottleneck = pre_bottleneck\n    self._param_count = self._num_units\n    for dim in self._output_size:\n      self._param_count *= dim",
    "smell": []
}