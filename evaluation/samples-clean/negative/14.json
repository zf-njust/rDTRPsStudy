{
    "code": "def __init__(\n      self,\n      model_id: int,\n      input_specs: layers = layers.InputSpec(shape=[None, None, None, None, 3]),\n      pool_size: Sequence[int] = (2, 2, 2),\n      kernel_size: Sequence[int] = (3, 3, 3),\n      base_filters: int = 32,\n      kernel_regularizer: tf.keras.regularizers.Regularizer = None,\n      activation: str = 'relu',\n      norm_momentum: float = 0.99,\n      norm_epsilon: float = 0.001,\n      use_sync_bn: bool = False,\n      use_batch_normalization: bool = False,  # type: ignore  # typed-keras\n      **kwargs):\n    \"\"\"3D UNet backbone initialization function.\n\n    Args:\n      model_id: The depth of UNet3D backbone model. The greater the depth, the\n        more max pooling layers will be added to the model. Lowering the depth\n        may reduce the amount of memory required for training.\n      input_specs: The specs of the input tensor. It specifies a 5D input of\n        [batch, height, width, volume, channel] for `channel_last` data format\n        or [batch, channel, height, width, volume] for `channel_first` data\n        format.\n      pool_size: The pooling size for the max pooling operations.\n      kernel_size: The kernel size for 3D convolution.\n      base_filters: The number of filters that the first layer in the\n        convolution network will have. Following layers will contain a multiple\n        of this number. Lowering this number will likely reduce the amount of\n        memory required to train the model.\n      kernel_regularizer: A tf.keras.regularizers.Regularizer object for Conv2D.\n        Default to None.\n      activation: The name of the activation function.\n      norm_momentum: The normalization momentum for the moving average.\n      norm_epsilon: A float added to variance to avoid dividing by zero.\n      use_sync_bn: If True, use synchronized batch normalization.\n      use_batch_normalization: If set to True, use batch normalization after\n        convolution and before activation. Default to False.\n      **kwargs: Keyword arguments to be passed.\n    \"\"\"\n\n    self._model_id = model_id\n    self._input_specs = input_specs\n    self._pool_size = pool_size\n    self._kernel_size = kernel_size\n    self._activation = activation\n    self._base_filters = base_filters\n    self._norm_momentum = norm_momentum\n    self._norm_epsilon = norm_epsilon\n    self._use_sync_bn = use_sync_bn\n    if use_sync_bn:\n      self._norm = layers.experimental.SyncBatchNormalization\n    else:\n      self._norm = layers.BatchNormalization\n    self._kernel_regularizer = kernel_regularizer\n    self._use_batch_normalization = use_batch_normalization\n\n    # Build 3D UNet.\n    inputs = tf.keras.Input(\n        shape=input_specs.shape[1:], dtype=input_specs.dtype)\n    x = inputs\n    endpoints = {}\n\n    # Add levels with max pooling to downsample input.\n    for layer_depth in range(model_id):\n      # Two convoluions are applied sequentially without downsampling.\n      filter_num = base_filters * (2**layer_depth)\n      x2 = nn_blocks_3d.BasicBlock3DVolume(\n          filters=[filter_num, filter_num * 2],\n          strides=(1, 1, 1),\n          kernel_size=self._kernel_size,\n          kernel_regularizer=self._kernel_regularizer,\n          activation=self._activation,\n          use_sync_bn=self._use_sync_bn,\n          norm_momentum=self._norm_momentum,\n          norm_epsilon=self._norm_epsilon,\n          use_batch_normalization=self._use_batch_normalization)(\n              x)\n      if layer_depth < model_id - 1:\n        x = layers.MaxPool3D(\n            pool_size=pool_size,\n            strides=(2, 2, 2),\n            padding='valid',\n            data_format=tf.keras.backend.image_data_format())(\n                x2)\n      else:\n        x = x2\n      endpoints[str(layer_depth + 1)] = x2\n\n    self._output_specs = {l: endpoints[l].get_shape() for l in endpoints}\n\n    super(UNet3D, self).__init__(inputs=inputs, outputs=endpoints, **kwargs)",
    "smell": []
}