{
    "code": "def main(config, device, logger, vdl_writer):\n    # init dist environment\n    if config['Global']['distributed']:\n        dist.init_parallel_env()\n\n    global_config = config['Global']\n\n    # build dataloader\n    train_dataloader = build_dataloader(config, 'Train', device, logger)\n    if config['Eval']:\n        valid_dataloader = build_dataloader(config, 'Eval', device, logger)\n    else:\n        valid_dataloader = None\n\n    # build post process\n    post_process_class = build_post_process(config['PostProcess'],\n                                            global_config)\n\n    # build model\n    # for rec algorithm\n    if hasattr(post_process_class, 'character'):\n        char_num = len(getattr(post_process_class, 'character'))\n        if config['Architecture'][\"algorithm\"] in [\"Distillation\",\n                                                   ]:  # distillation model\n            for key in config['Architecture'][\"Models\"]:\n                if config['Architecture']['Models'][key]['Head'][\n                        'name'] == 'MultiHead':  # for multi head\n                    if config['PostProcess'][\n                            'name'] == 'DistillationSARLabelDecode':\n                        char_num = char_num - 2\n                    # update SARLoss params\n                    assert list(config['Loss']['loss_config_list'][-1].keys())[\n                        0] == 'DistillationSARLoss'\n                    config['Loss']['loss_config_list'][-1][\n                        'DistillationSARLoss']['ignore_index'] = char_num + 1\n                    out_channels_list = {}\n                    out_channels_list['CTCLabelDecode'] = char_num\n                    out_channels_list['SARLabelDecode'] = char_num + 2\n                    config['Architecture']['Models'][key]['Head'][\n                        'out_channels_list'] = out_channels_list\n                else:\n                    config['Architecture'][\"Models\"][key][\"Head\"][\n                        'out_channels'] = char_num\n        elif config['Architecture']['Head'][\n                'name'] == 'MultiHead':  # for multi head\n            if config['PostProcess']['name'] == 'SARLabelDecode':\n                char_num = char_num - 2\n            # update SARLoss params\n            assert list(config['Loss']['loss_config_list'][1].keys())[\n                0] == 'SARLoss'\n            if config['Loss']['loss_config_list'][1]['SARLoss'] is None:\n                config['Loss']['loss_config_list'][1]['SARLoss'] = {\n                    'ignore_index': char_num + 1\n                }\n            else:\n                config['Loss']['loss_config_list'][1]['SARLoss'][\n                    'ignore_index'] = char_num + 1\n            out_channels_list = {}\n            out_channels_list['CTCLabelDecode'] = char_num\n            out_channels_list['SARLabelDecode'] = char_num + 2\n            config['Architecture']['Head'][\n                'out_channels_list'] = out_channels_list\n        else:  # base rec model\n            config['Architecture'][\"Head\"]['out_channels'] = char_num\n\n        if config['PostProcess']['name'] == 'SARLabelDecode':  # for SAR model\n            config['Loss']['ignore_index'] = char_num - 1\n    model = build_model(config['Architecture'])\n\n    pre_best_model_dict = dict()\n    # load fp32 model to begin quantization\n    pre_best_model_dict = load_model(config, model, None, config['Architecture'][\"model_type\"])\n\n    freeze_params = False\n    if config['Architecture'][\"algorithm\"] in [\"Distillation\"]:\n        for key in config['Architecture'][\"Models\"]:\n            freeze_params = freeze_params or config['Architecture']['Models'][\n                key].get('freeze_params', False)\n    act = None if freeze_params else PACT\n    quanter = QAT(config=quant_config, act_preprocess=act)\n    quanter.quantize(model)\n\n    if config['Global']['distributed']:\n        model = paddle.DataParallel(model)\n\n    # build loss\n    loss_class = build_loss(config['Loss'])\n\n    # build optim\n    optimizer, lr_scheduler = build_optimizer(\n        config['Optimizer'],\n        epochs=config['Global']['epoch_num'],\n        step_each_epoch=len(train_dataloader),\n        model=model)\n\n    # resume PACT training process\n    pre_best_model_dict = load_model(config, model, optimizer, config['Architecture'][\"model_type\"])\n\n    # build metric\n    eval_class = build_metric(config['Metric'])\n\n    logger.info('train dataloader has {} iters, valid dataloader has {} iters'.\n                format(len(train_dataloader), len(valid_dataloader)))\n\n    # start train\n    program.train(config, train_dataloader, valid_dataloader, device, model,\n                  loss_class, optimizer, lr_scheduler, post_process_class,\n                  eval_class, pre_best_model_dict, logger, vdl_writer)",
    "smell": []
}