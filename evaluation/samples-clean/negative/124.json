{
    "code": "def forward(self, model, sample, reduce=True):\n        \"\"\"Compute the loss for the given sample.\n\n        Returns a tuple with three elements:\n        1) the loss\n        2) the sample size, which is used as the denominator for the gradient\n        3) logging outputs to display while training\n        \"\"\"\n        masked_tokens = sample[\"target\"].ne(self.padding_idx)\n        sample_size = masked_tokens.int().sum()\n\n        # Rare: when all tokens are masked, project all tokens.\n        # We use torch.where to avoid device-to-host transfers,\n        # except on CPU where torch.where is not well supported\n        # (see github.com/pytorch/pytorch/issues/26247).\n        if self.tpu:\n            masked_tokens = None  # always project all tokens on TPU\n        elif masked_tokens.device == torch.device(\"cpu\"):\n            if not masked_tokens.any():\n                masked_tokens = None\n        else:\n            masked_tokens = torch.where(\n                masked_tokens.any(),\n                masked_tokens,\n                masked_tokens.new([True]),\n            )\n\n        logits = model(**sample[\"net_input\"], masked_tokens=masked_tokens)[0]\n        targets = model.get_targets(sample, [logits])\n        if masked_tokens is not None:\n            targets = targets[masked_tokens]\n\n        loss = modules.cross_entropy(\n            logits.view(-1, logits.size(-1)),\n            targets.view(-1),\n            reduction=\"sum\",\n            ignore_index=self.padding_idx,\n        )\n\n        logging_output = {\n            \"loss\": loss if self.tpu else loss.data,\n            \"ntokens\": sample[\"ntokens\"],\n            \"nsentences\": sample[\"nsentences\"],\n            \"sample_size\": sample_size,\n        }\n        return loss, sample_size, logging_output",
    "smell": []
}