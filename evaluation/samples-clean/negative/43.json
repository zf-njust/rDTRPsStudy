{
    "code": "def __init__(\n        self,\n        modality_cfg: D2vImageConfig,\n        embed_dim: int,\n        make_block: Callable[[float, Optional[int], Optional[int]], nn.ModuleList],\n        norm_layer: Callable[[int], nn.LayerNorm],\n        layer_norm_first: bool,\n        alibi_biases: Dict,\n        task: Optional[FairseqTask],\n    ):\n\n        img_size = to_2tuple(modality_cfg.input_size)\n        patch_size = to_2tuple(modality_cfg.patch_size)\n        num_patches = (img_size[1] // patch_size[1]) * (img_size[0] // patch_size[0])\n\n        local_encoder = PatchEmbed(\n            modality_cfg.input_size,\n            modality_cfg.patch_size,\n            modality_cfg.in_chans,\n            modality_cfg.embed_dim,\n        )\n\n        w = local_encoder.proj.weight.data\n        torch.nn.init.xavier_uniform_(w.view([w.shape[0], -1]))\n\n        if modality_cfg.embed_dim != embed_dim:\n            local_encoder = nn.Sequential(\n                local_encoder,\n                nn.Linear(modality_cfg.embed_dim, embed_dim),\n            )\n\n        project_features = nn.Identity()\n\n        pos_embed = nn.Parameter(\n            torch.zeros(1, num_patches, embed_dim), requires_grad=False\n        )\n\n        side_n = int(num_patches ** 0.5)\n\n        emb = get_2d_sincos_pos_embed(\n            pos_embed.shape[-1],\n            side_n,\n            cls_token=False,\n        )\n        pos_embed.data.copy_(torch.from_numpy(emb).float().unsqueeze(0))\n        fixed_positional_encoder = (\n            FixedPositionalEncoder(pos_embed) if modality_cfg.fixed_positions else None\n        )\n\n        dpr = np.linspace(\n            modality_cfg.start_drop_path_rate,\n            modality_cfg.end_drop_path_rate,\n            modality_cfg.prenet_depth,\n        )\n\n        context_encoder = BlockEncoder(\n            nn.ModuleList(make_block(dpr[i]) for i in range(modality_cfg.prenet_depth)),\n            norm_layer(embed_dim) if not layer_norm_first else None,\n            layer_norm_first,\n            modality_cfg.prenet_layerdrop,\n            modality_cfg.prenet_dropout,\n        )\n\n        if modality_cfg.transformer_decoder:\n            if modality_cfg.enc_dec_transformer:\n                decoder = EncDecTransformerDecoder(modality_cfg.decoder, embed_dim)\n            else:\n                dec_enc = BlockEncoder(\n                    nn.ModuleList(\n                        make_block(0, modality_cfg.decoder.decoder_dim, 8)\n                        for _ in range(modality_cfg.decoder.decoder_layers)\n                    ),\n                    None,\n                    layer_norm_first,\n                    0,\n                    0,\n                )\n                decoder = TransformerDecoder(modality_cfg.decoder, embed_dim, dec_enc)\n        else:\n            decoder = (\n                Decoder2d(modality_cfg.decoder, embed_dim, side_n, side_n)\n                if modality_cfg.decoder is not None\n                else None\n            )\n\n        alibi_bias_fn = partial(\n            get_alibi_bias,\n            alibi_biases=alibi_biases,\n            heads=modality_cfg.num_alibi_heads,\n            dims=modality_cfg.alibi_dims,\n            distance=modality_cfg.alibi_distance,\n        )\n\n        super().__init__(\n            modality_cfg=modality_cfg,\n            embed_dim=embed_dim,\n            local_encoder=local_encoder,\n            project_features=project_features,\n            fixed_positional_encoder=fixed_positional_encoder,\n            relative_positional_encoder=None,\n            context_encoder=context_encoder,\n            decoder=decoder,\n            get_alibi_bias=alibi_bias_fn,\n        )",
    "smell": []
}