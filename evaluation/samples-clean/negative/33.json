{
    "code": "def run_job(self, job, tracking_url_callback=None):\n        if tracking_url_callback is not None:\n            warnings.warn(\"tracking_url_callback argument is deprecated, task.set_tracking_url is \"\n                          \"used instead.\", DeprecationWarning)\n\n        packages = [luigi] + self.modules + job.extra_modules() + list(_attached_packages)\n\n        # find the module containing the job\n        packages.append(__import__(job.__module__, None, None, 'dummy'))\n\n        # find the path to out runner.py\n        runner_path = mrrunner.__file__\n        # assume source is next to compiled\n        if runner_path.endswith(\"pyc\"):\n            runner_path = runner_path[:-3] + \"py\"\n\n        base_tmp_dir = configuration.get_config().get('core', 'tmp-dir', None)\n        if base_tmp_dir:\n            warnings.warn(\"The core.tmp-dir configuration item is\"\n                          \" deprecated, please use the TMPDIR\"\n                          \" environment variable if you wish\"\n                          \" to control where luigi.contrib.hadoop may\"\n                          \" create temporary files and directories.\")\n            self.tmp_dir = os.path.join(base_tmp_dir, 'hadoop_job_%016x' % random.getrandbits(64))\n            os.makedirs(self.tmp_dir)\n        else:\n            self.tmp_dir = tempfile.mkdtemp()\n\n        logger.debug(\"Tmp dir: %s\", self.tmp_dir)\n\n        # build arguments\n        config = configuration.get_config()\n        python_executable = config.get('hadoop', 'python-executable', 'python')\n        runner_arg = 'mrrunner.pex' if job.package_binary is not None else 'mrrunner.py'\n        command = '{0} {1} {{step}}'.format(python_executable, runner_arg)\n        map_cmd = command.format(step='map')\n        cmb_cmd = command.format(step='combiner')\n        red_cmd = command.format(step='reduce')\n\n        output_final = job.output().path\n        # atomic output: replace output with a temporary work directory\n        if self.end_job_with_atomic_move_dir:\n            illegal_targets = (\n                luigi.contrib.s3.S3FlagTarget, luigi.contrib.gcs.GCSFlagTarget)\n            if isinstance(job.output(), illegal_targets):\n                raise TypeError(\"end_job_with_atomic_move_dir is not supported\"\n                                \" for {}\".format(illegal_targets))\n            output_hadoop = '{output}-temp-{time}'.format(\n                output=output_final,\n                time=datetime.datetime.now().isoformat().replace(':', '-'))\n        else:\n            output_hadoop = output_final\n\n        arglist = luigi.contrib.hdfs.load_hadoop_cmd() + ['jar', self.streaming_jar]\n\n        # 'libjars' is a generic option, so place it first\n        libjars = [libjar for libjar in self.libjars]\n\n        for libjar in self.libjars_in_hdfs:\n            run_cmd = luigi.contrib.hdfs.load_hadoop_cmd() + ['fs', '-get', libjar, self.tmp_dir]\n            logger.debug(subprocess.list2cmdline(run_cmd))\n            subprocess.call(run_cmd)\n            libjars.append(os.path.join(self.tmp_dir, os.path.basename(libjar)))\n\n        if libjars:\n            arglist += ['-libjars', ','.join(libjars)]\n\n        # 'archives' is also a generic option\n        archives = []\n        extra_archives = job.extra_archives()\n\n        if self.archives:\n            archives = self.archives\n\n        if extra_archives:\n            archives += extra_archives\n\n        if archives:\n            arglist += ['-archives', ','.join(archives)]\n\n        # Add static files and directories\n        extra_files = get_extra_files(job.extra_files())\n\n        files = []\n        for src, dst in extra_files:\n            dst_tmp = '%s_%09d' % (dst.replace('/', '_'), random.randint(0, 999999999))\n            files += ['%s#%s' % (src, dst_tmp)]\n            # -files doesn't support subdirectories, so we need to create the dst_tmp -> dst manually\n            job.add_link(dst_tmp, dst)\n\n        if files:\n            arglist += ['-files', ','.join(files)]\n\n        jobconfs = job.jobconfs()\n\n        for k, v in six.iteritems(self.jobconfs):\n            jobconfs.append('%s=%s' % (k, v))\n\n        for conf in jobconfs:\n            arglist += ['-D', conf]\n\n        arglist += self.streaming_args\n\n        # Add additonal non-generic  per-job streaming args\n        extra_streaming_args = job.extra_streaming_arguments()\n        for (arg, value) in extra_streaming_args:\n            if not arg.startswith('-'):  # safety first\n                arg = '-' + arg\n            arglist += [arg, value]\n\n        arglist += ['-mapper', map_cmd]\n\n        if job.combiner != NotImplemented:\n            arglist += ['-combiner', cmb_cmd]\n        if job.reducer != NotImplemented:\n            arglist += ['-reducer', red_cmd]\n        packages_fn = 'mrrunner.pex' if job.package_binary is not None else 'packages.tar'\n        files = [\n            runner_path if job.package_binary is None else None,\n            os.path.join(self.tmp_dir, packages_fn),\n            os.path.join(self.tmp_dir, 'job-instance.pickle'),\n        ]\n\n        for f in filter(None, files):\n            arglist += ['-file', f]\n\n        if self.output_format:\n            arglist += ['-outputformat', self.output_format]\n        if self.input_format:\n            arglist += ['-inputformat', self.input_format]\n\n        allowed_input_targets = (\n            luigi.contrib.hdfs.HdfsTarget,\n            luigi.contrib.s3.S3Target,\n            luigi.contrib.gcs.GCSTarget)\n        for target in luigi.task.flatten(job.input_hadoop()):\n            if not isinstance(target, allowed_input_targets):\n                raise TypeError('target must one of: {}'.format(\n                    allowed_input_targets))\n            arglist += ['-input', target.path]\n\n        allowed_output_targets = (\n            luigi.contrib.hdfs.HdfsTarget,\n            luigi.contrib.s3.S3FlagTarget,\n            luigi.contrib.gcs.GCSFlagTarget)\n        if not isinstance(job.output(), allowed_output_targets):\n            raise TypeError('output must be one of: {}'.format(\n                allowed_output_targets))\n        arglist += ['-output', output_hadoop]\n\n        # submit job\n        if job.package_binary is not None:\n            shutil.copy(job.package_binary, os.path.join(self.tmp_dir, 'mrrunner.pex'))\n        else:\n            create_packages_archive(packages, os.path.join(self.tmp_dir, 'packages.tar'))\n\n        job.dump(self.tmp_dir)\n\n        run_and_track_hadoop_job(arglist, tracking_url_callback=job.set_tracking_url)\n\n        if self.end_job_with_atomic_move_dir:\n            luigi.contrib.hdfs.HdfsTarget(output_hadoop).move_dir(output_final)\n        self.finish()",
    "smell": []
}