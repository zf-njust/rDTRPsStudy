{
    "code": "def forward(self, model, sample, reduce=True):\n        \"\"\"Compute the loss for the given sample.\n\n        Returns a tuple with three elements:\n        1) the loss\n        2) the sample size, which is used as the denominator for the gradient\n        3) logging outputs to display while training\n        \"\"\"\n        net_output = model(**sample[\"net_input\"])\n\n        token_loss = nll_loss(\n            net_output[\"token\"], sample[\"target\"], sample[\"mask\"], reduce\n        )\n        dur_loss = self.dur_loss_fn(\n            net_output[\"duration\"],\n            sample[\"dur_target\"],\n            sample[\"dur_mask\"],\n            reduce,\n        )\n        f0_loss = self.f0_loss_fn(\n            net_output[\"f0\"],\n            sample[\"f0_target\"],\n            sample[\"f0_mask\"],\n            reduce,\n        )\n        loss = self.weights.to(token_loss.device) * torch.stack(\n            [token_loss, dur_loss, f0_loss], dim=-1\n        )\n        loss = loss.sum() if reduce else loss.sum(-1)\n\n        sample_size = (\n            sample[\"target\"].size(0) if self.sentence_avg else sample[\"ntokens\"]\n        )\n        logging_output = {\n            \"loss\": loss.detach().sum().item(),\n            \"token_loss\": token_loss.detach().sum().item(),\n            \"dur_loss\": dur_loss.detach().sum().item(),\n            \"f0_loss\": f0_loss.detach().sum().item(),\n            \"ntokens\": sample[\"ntokens\"],\n            \"nsentences\": sample[\"target\"].size(0),\n            \"sample_size\": sample_size,\n        }\n        return loss, sample_size, logging_output",
    "smell": []
}