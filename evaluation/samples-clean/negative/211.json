{
    "code": "def convert_single_example(ex_index, example, label_list, max_seq_length,\n                           tokenizer):\n  \"\"\"Converts a single `InputExample` into a single `InputFeatures`.\"\"\"\n\n  if isinstance(example, PaddingInputExample):\n    return InputFeatures(\n        input_ids=[0] * max_seq_length,\n        input_mask=[0] * max_seq_length,\n        segment_ids=[0] * max_seq_length,\n        label_id=0,\n        is_real_example=False)\n\n  label_map = {}\n  for (i, label) in enumerate(label_list):\n    label_map[label] = i\n\n  tokens_a = tokenizer.tokenize(example.text_a)\n  tokens_b = None\n  if example.text_b:\n    tokens_b = tokenizer.tokenize(example.text_b)\n\n  if tokens_b:\n    # Modifies `tokens_a` and `tokens_b` in place so that the total\n    # length is less than the specified length.\n    # Account for [CLS], [SEP], [SEP] with \"- 3\"\n    _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n  else:\n    # Account for [CLS] and [SEP] with \"- 2\"\n    if len(tokens_a) > max_seq_length - 2:\n      tokens_a = tokens_a[0:(max_seq_length - 2)]\n\n  # The convention in BERT is:\n  # (a) For sequence pairs:\n  #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n  #  type_ids: 0     0  0    0    0     0       0 0     1  1  1  1   1 1\n  # (b) For single sequences:\n  #  tokens:   [CLS] the dog is hairy . [SEP]\n  #  type_ids: 0     0   0   0  0     0 0\n  #\n  # Where \"type_ids\" are used to indicate whether this is the first\n  # sequence or the second sequence. The embedding vectors for `type=0` and\n  # `type=1` were learned during pre-training and are added to the wordpiece\n  # embedding vector (and position vector). This is not *strictly* necessary\n  # since the [SEP] token unambiguously separates the sequences, but it makes\n  # it easier for the model to learn the concept of sequences.\n  #\n  # For classification tasks, the first vector (corresponding to [CLS]) is\n  # used as the \"sentence vector\". Note that this only makes sense because\n  # the entire model is fine-tuned.\n  tokens = []\n  segment_ids = []\n  tokens.append(\"[CLS]\")\n  segment_ids.append(0)\n  for token in tokens_a:\n    tokens.append(token)\n    segment_ids.append(0)\n  tokens.append(\"[SEP]\")\n  segment_ids.append(0)\n\n  if tokens_b:\n    for token in tokens_b:\n      tokens.append(token)\n      segment_ids.append(1)\n    tokens.append(\"[SEP]\")\n    segment_ids.append(1)\n\n  input_ids = tokenizer.convert_tokens_to_ids(tokens)\n\n  # The mask has 1 for real tokens and 0 for padding tokens. Only real\n  # tokens are attended to.\n  input_mask = [1] * len(input_ids)\n\n  # Zero-pad up to the sequence length.\n  while len(input_ids) < max_seq_length:\n    input_ids.append(0)\n    input_mask.append(0)\n    segment_ids.append(0)\n\n  assert len(input_ids) == max_seq_length\n  assert len(input_mask) == max_seq_length\n  assert len(segment_ids) == max_seq_length\n\n  label_id = label_map[example.label]\n  if ex_index < 5:\n    tf.logging.info(\"*** Example ***\")\n    tf.logging.info(\"guid: %s\" % (example.guid))\n    tf.logging.info(\"tokens: %s\" % \" \".join(\n        [tokenization.printable_text(x) for x in tokens]))\n    tf.logging.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n    tf.logging.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n    tf.logging.info(\"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n    tf.logging.info(\"label: %s (id = %d)\" % (example.label, label_id))\n\n  feature = InputFeatures(\n      input_ids=input_ids,\n      input_mask=input_mask,\n      segment_ids=segment_ids,\n      label_id=label_id,\n      is_real_example=True)\n  return feature",
    "smell": []
}