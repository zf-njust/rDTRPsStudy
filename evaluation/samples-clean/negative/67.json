{
    "code": "def from_training_step_output(cls, training_step_output: STEP_OUTPUT, normalize: int = 1) -> \"ClosureResult\":\n        closure_loss, extra = None, {}\n\n        if isinstance(training_step_output, dict):\n            closure_loss = training_step_output.get(\"loss\")\n            if closure_loss is None:\n                raise MisconfigurationException(\n                    \"In automatic_optimization, when `training_step` returns a dict, the 'loss' key needs to be present\"\n                )\n            extra = {k: v for k, v in training_step_output.items() if k != \"loss\"}\n        elif isinstance(training_step_output, Tensor):\n            closure_loss = training_step_output\n        elif training_step_output is not None:\n            raise MisconfigurationException(\n                \"In automatic optimization, `training_step` must return a Tensor, \"\n                \"a dict, or None (where the step will be skipped).\"\n            )\n\n        if closure_loss is not None:\n            # accumulate the loss. If ``accumulate_grad_batches == 1``, no effect\n            # note: avoid in-place operation `x /= y` here on purpose\n            closure_loss = closure_loss / normalize\n\n        return cls(closure_loss, extra=extra)",
    "smell": []
}