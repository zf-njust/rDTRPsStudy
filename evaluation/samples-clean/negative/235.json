{
    "code": "def __init__(self,\n               output_size,\n               min_level,\n               max_level,\n               num_scales,\n               aspect_ratios,\n               anchor_size,\n               rpn_match_threshold=0.7,\n               rpn_unmatched_threshold=0.3,\n               rpn_batch_size_per_im=256,\n               rpn_fg_fraction=0.5,\n               aug_rand_hflip=False,\n               aug_scale_min=1.0,\n               aug_scale_max=1.0,\n               skip_crowd_during_training=True,\n               max_num_instances=100,\n               include_mask=False,\n               mask_crop_size=112,\n               use_bfloat16=True,\n               mode=None):\n    \"\"\"Initializes parameters for parsing annotations in the dataset.\n\n    Args:\n      output_size: `Tensor` or `list` for [height, width] of output image. The\n        output_size should be divided by the largest feature stride 2^max_level.\n      min_level: `int` number of minimum level of the output feature pyramid.\n      max_level: `int` number of maximum level of the output feature pyramid.\n      num_scales: `int` number representing intermediate scales added\n        on each level. For instances, num_scales=2 adds one additional\n        intermediate anchor scales [2^0, 2^0.5] on each level.\n      aspect_ratios: `list` of float numbers representing the aspect raito\n        anchors added on each level. The number indicates the ratio of width to\n        height. For instances, aspect_ratios=[1.0, 2.0, 0.5] adds three anchors\n        on each scale level.\n      anchor_size: `float` number representing the scale of size of the base\n        anchor to the feature stride 2^level.\n      rpn_match_threshold:\n      rpn_unmatched_threshold:\n      rpn_batch_size_per_im:\n      rpn_fg_fraction:\n      aug_rand_hflip: `bool`, if True, augment training with random\n        horizontal flip.\n      aug_scale_min: `float`, the minimum scale applied to `output_size` for\n        data augmentation during training.\n      aug_scale_max: `float`, the maximum scale applied to `output_size` for\n        data augmentation during training.\n      skip_crowd_during_training: `bool`, if True, skip annotations labeled with\n        `is_crowd` equals to 1.\n      max_num_instances: `int` number of maximum number of instances in an\n        image. The groundtruth data will be padded to `max_num_instances`.\n      include_mask: a bool to indicate whether parse mask groundtruth.\n      mask_crop_size: the size which groundtruth mask is cropped to.\n      use_bfloat16: `bool`, if True, cast output image to tf.bfloat16.\n      mode: a ModeKeys. Specifies if this is training, evaluation, prediction\n        or prediction with groundtruths in the outputs.\n    \"\"\"\n    self._mode = mode\n    self._max_num_instances = max_num_instances\n    self._skip_crowd_during_training = skip_crowd_during_training\n    self._is_training = (mode == ModeKeys.TRAIN)\n\n    self._example_decoder = tf_example_decoder.TfExampleDecoder(\n        include_mask=include_mask)\n\n    # Anchor.\n    self._output_size = output_size\n    self._min_level = min_level\n    self._max_level = max_level\n    self._num_scales = num_scales\n    self._aspect_ratios = aspect_ratios\n    self._anchor_size = anchor_size\n\n    # Target assigning.\n    self._rpn_match_threshold = rpn_match_threshold\n    self._rpn_unmatched_threshold = rpn_unmatched_threshold\n    self._rpn_batch_size_per_im = rpn_batch_size_per_im\n    self._rpn_fg_fraction = rpn_fg_fraction\n\n    # Data augmentation.\n    self._aug_rand_hflip = aug_rand_hflip\n    self._aug_scale_min = aug_scale_min\n    self._aug_scale_max = aug_scale_max\n\n    # Mask.\n    self._include_mask = include_mask\n    self._mask_crop_size = mask_crop_size\n\n    # Device.\n    self._use_bfloat16 = use_bfloat16\n\n    # Data is parsed depending on the model Modekey.\n    if mode == ModeKeys.TRAIN:\n      self._parse_fn = self._parse_train_data\n    elif mode == ModeKeys.EVAL:\n      self._parse_fn = self._parse_eval_data\n    elif mode == ModeKeys.PREDICT or mode == ModeKeys.PREDICT_WITH_GT:\n      self._parse_fn = self._parse_predict_data\n    else:\n      raise ValueError('mode is not defined.')",
    "smell": []
}