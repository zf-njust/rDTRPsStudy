{
  "code": "def build(self, query_shape: Any) -> None:\n    ##### Content attention\n    # Einsum expression:\n    #   B = batch_size\n    #   N = num_heads\n    #   K = head_size\n    #   S = query_len (of the given attn_axis)\n    #   T = key/value_len (of the given attn_axis)\n    #   [U-Z] = length of other attension axes\n    # Example for 5D query_heads, (e.g. images [B x H x W x N x K])\n    # - when attn_axis = 0 (H axis):\n    #     symbols = 'U'  => num_attn_dims = 2\n    #     q_expr = 'BSUNK' => 'S' is inserted, prefix = 'B', suffix = 'NK'\n    #     k_expr = 'BTUNK' => 'T' is inserted, prefix = 'B', suffix = 'NK'\n    #     v_expr = 'BTUNK' => 'T' is inserted, prefix = 'B', suffix = 'NK'\n    #     a_expr = 'BUNST' => 'N x S x T' attention map\n    num_attn_dims = query_shape.rank - 2  # -2 to account for bsz, hidden size\n    assert num_attn_dims < 6, 'Only support at most 6 attention dims.'\n    symbols = ''.join([chr(ord('U') + i) for i in range(num_attn_dims - 1)])\n    insert = lambda s, i, c: s[:i] + c + s[i:]\n    create_expr = lambda s, prefix='B', suffix='NK': prefix + s + suffix\n    self.q_expr = create_expr(insert(symbols, self.attn_axis, 'S'))\n    self.k_expr = create_expr(insert(symbols, self.attn_axis, 'T'))\n    self.v_expr = create_expr(insert(symbols, self.attn_axis, 'T'))\n    self.a_expr = create_expr(symbols, suffix='NST')\n\n    ##### Relative attention\n    if self.rel_attn_type in ['2d_multi_head', '2d_single_head']:\n      query_shape_list = query_shape.as_list()\n      if query_shape.rank == 4:\n        height, width = query_shape_list[1:3]\n      elif query_shape.rank == 3:\n        seq_len = query_shape_list[1]\n        height, width = common_ops.get_shape_from_length(\n            seq_len, self.input_origin_height, self.input_origin_width\n        )\n        if height * width != seq_len:\n          raise ValueError(\n              'Sequence length: %s violates input size: (%s, %s).'\n              % (seq_len, height, width)\n          )\n      else:\n        raise ValueError(\n            'Does not support relative attention for query shape: %s.'\n            % query_shape_list\n        )\n\n      if self.scale_ratio is not None:\n        scale_ratio = eval(self.scale_ratio)  # pylint:disable=eval-used\n        vocab_height = 2 * int(height / scale_ratio) - 1\n        vocab_width = 2 * int(width / scale_ratio) - 1\n      else:\n        vocab_height = 2 * height - 1\n        vocab_width = 2 * width - 1\n\n      if self.rel_attn_type == '2d_multi_head':\n        rel_bias_shape = [self.num_heads, vocab_height, vocab_width]\n      elif self.rel_attn_type == '2d_single_head':\n        rel_bias_shape = [vocab_height, vocab_width]\n      else:\n        raise NotImplementedError(\n            f'rel_attn_type {self.rel_attn_type} not implemented yet.'\n        )\n\n      self._feat_height = height\n      self._feat_width = width\n      self.relative_bias = self.add_weight(\n          'relative_bias',\n          rel_bias_shape,\n          initializer=self.kernel_initializer,\n          trainable=True,\n      )\ndef eval(*args, **kwargs): # real signature unknown\n    \"\"\"\n    Evaluate the given source in the context of globals and locals.\n    \n    The source may be a string representing a Python expression\n    or a code object as returned by compile().\n    The globals must be a dictionary and locals can be any mapping,\n    defaulting to the current globals and locals.\n    If only globals is given, locals defaults to it.\n    \"\"\"\n    pass",
  "smell": [
    {
      "smell_id": 2,
      "line_no": 49,
      "description": "The values of an argument hold inconsistent types in different function calls."
    }
  ]
}