{
  "code": "def build_evaluator(cls, cfg, dataset_name, output_folder=None):\n        \"\"\"\n        Create evaluator(s) for a given dataset.\n        This uses the special metadata \"evaluator_type\" associated with each builtin dataset.\n        For your own dataset, you can simply create an evaluator manually in your\n        script and do not have to worry about the hacky if-else logic here.\n        \"\"\"\n        if cfg.MODEL.PANOPTIC_DEEPLAB.BENCHMARK_NETWORK_SPEED:\n            return None\n        if output_folder is None:\n            output_folder = os.path.join(cfg.OUTPUT_DIR, \"inference\")\n        evaluator_list = []\n        evaluator_type = MetadataCatalog.get(dataset_name).evaluator_type\n        if evaluator_type in [\"cityscapes_panoptic_seg\", \"coco_panoptic_seg\"]:\n            evaluator_list.append(COCOPanopticEvaluator(dataset_name, output_folder))\n        if evaluator_type == \"cityscapes_panoptic_seg\":\n            evaluator_list.append(CityscapesSemSegEvaluator(dataset_name))\n            evaluator_list.append(CityscapesInstanceEvaluator(dataset_name))\n        if evaluator_type == \"coco_panoptic_seg\":\n            # `thing_classes` in COCO panoptic metadata includes both thing and\n            # stuff classes for visualization. COCOEvaluator requires metadata\n            # which only contains thing classes, thus we map the name of\n            # panoptic datasets to their corresponding instance datasets.\n            dataset_name_mapper = {\n                \"coco_2017_val_panoptic\": \"coco_2017_val\",\n                \"coco_2017_val_100_panoptic\": \"coco_2017_val_100\",\n            }\n            evaluator_list.append(\n                COCOEvaluator(dataset_name_mapper[dataset_name], output_dir=output_folder)\n            )\n        if len(evaluator_list) == 0:\n            raise NotImplementedError(\n                \"no Evaluator for the dataset {} with the type {}\".format(\n                    dataset_name, evaluator_type\n                )\n            )\n        elif len(evaluator_list) == 1:\n            return evaluator_list[0]\n        return DatasetEvaluators(evaluator_list)\n    def append(self, *args, **kwargs): # real signature unknown\n        \"\"\" Append object to the end of the list. \"\"\"\n        pass",
  "smell": [
    {
      "smell_id": 2,
      "line_no": 17,
      "description": "The values of an argument hold inconsistent types in different function calls."
    }
  ]
}