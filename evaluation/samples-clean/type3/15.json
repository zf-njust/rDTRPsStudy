{
  "code": "def get_following(\n    browser,\n    username,\n    grab,\n    relationship_data,\n    live_match,\n    store_locally,\n    logger,\n    logfolder,\n):\n    \"\"\" Get entire list of following using graphql queries. \"\"\"\n    if username not in relationship_data:\n        relationship_data.update({username: {\"all_following\": [], \"all_followers\": []}})\n\n    grab_info = (\n        'at \"full\" range' if grab == \"full\" else \"at the range of \" \"{}\".format(grab)\n    )\n    tense = (\n        \"live\"\n        if (live_match is True or not relationship_data[username][\"all_following\"])\n        else \"fresh\"\n    )\n\n    logger.info(\n        \"Retrieving {} `Following` data of {} {}\".format(tense, username, grab_info)\n    )\n\n    user_link = \"https://www.instagram.com/{}/\".format(username)\n    web_address_navigator(browser, user_link)\n\n    # Get following count\n    _, following_count = get_relationship_counts(browser, username, logger)\n\n    if grab != \"full\" and grab > following_count:\n        logger.info(\n            \"You have requested higher amount than existing following count \"\n            \" ~gonna grab all available\"\n        )\n        grab = following_count\n\n    # TO-DO: Check if user's account is not private\n\n    # sets the amount of usernames to be matched in the next queries\n    match = (\n        None\n        if live_match is True\n        else 10\n        if relationship_data[username][\"all_following\"]\n        else None\n    )\n\n    # if there has been prior graphql query, use that existing data to speed\n    # up querying time\n    all_prior_following = (\n        relationship_data[username][\"all_following\"] if match is not None else None\n    )\n\n    user_data = {}\n\n    # FIXME: use util.py:get_query_hash to get the hash code\n    graphql_endpoint = \"view-source:https://www.instagram.com/graphql\" \"/query/\"\n\n    graphql_following = (\n        graphql_endpoint + \"?query_hash=58712303d941c6855d4e888c5f0cd22f\"\n    )\n\n    all_following = []\n\n    variables = {}\n\n    try:\n        user_data[\"id\"] = browser.execute_script(\n            \"return window.__additionalData[Object.keys(window.__additionalData)[0]].data.\"\n            \"graphql.user.id\"\n        )\n    except WebDriverException:\n        user_data[\"id\"] = browser.execute_script(\n            \"return window._sharedData.\" \"entry_data.ProfilePage[0].\" \"graphql.user.id\"\n        )\n\n    variables[\"id\"] = user_data[\"id\"]\n    variables[\"first\"] = 50\n\n    # get follower and user loop\n\n    sc_rolled = 0\n    grab_notifier = False\n    local_read_failure = False\n    passed_time = \"time loop\"\n\n    try:\n        has_next_data = True\n\n        url = \"{}&variables={}\".format(graphql_following, str(json.dumps(variables)))\n        web_address_navigator(browser, url)\n\n        # Get stored graphql queries data to be used\n        try:\n            filename = \"{}graphql_queries.json\".format(logfolder)\n            query_date = datetime.today().strftime(\"%d-%m-%Y\")\n            if not os.path.isfile(filename):\n                with interruption_handler():\n                    with open(filename, \"w\") as graphql_queries_file:\n                        json.dump(\n                            {username: {query_date: {\"sc_rolled\": 0}}},\n                            graphql_queries_file,\n                        )\n                        graphql_queries_file.close()\n\n            # Loads the existing graphql queries data\n            with open(filename) as graphql_queries_file:\n                graphql_queries = json.load(graphql_queries_file)\n                stored_usernames = list(name for name, date in graphql_queries.items())\n                if username not in stored_usernames:\n                    graphql_queries[username] = {query_date: {\"sc_rolled\": 0}}\n                stored_query_dates = list(\n                    date for date, score in graphql_queries[username].items()\n                )\n                if query_date not in stored_query_dates:\n                    graphql_queries[username][query_date] = {\"sc_rolled\": 0}\n        except Exception as exc:\n            logger.info(\n                \"Error occurred while getting `scroll` data from \"\n                \"graphql_queries.json\\n{}\\n\".format(str(exc).encode(\"utf-8\"))\n            )\n            local_read_failure = True\n\n        start_time = time.time()\n        highest_value = following_count if grab == \"full\" else grab\n        # fetch all user while still has data\n        while has_next_data:\n            try:\n                pre = browser.find_element_by_tag_name(\"pre\").text\n            except NoSuchElementException as exc:\n                logger.info(\n                    \"Encountered an error to find `pre` in page!\"\n                    \"\\t~grabbed {} usernames \\n\\t{}\".format(\n                        len(set(all_following)), str(exc).encode(\"utf-8\")\n                    )\n                )\n                return all_following\n\n            data = json.loads(pre)[\"data\"]\n\n            # get following\n            page_info = data[\"user\"][\"edge_follow\"][\"page_info\"]\n            edges = data[\"user\"][\"edge_follow\"][\"edges\"]\n            for user in edges:\n                all_following.append(user[\"node\"][\"username\"])\n\n            grabbed = len(set(all_following))\n\n            # write & update records at Progress Tracker\n            progress_tracker(grabbed, highest_value, start_time, logger)\n\n            finish_time = time.time()\n            diff_time = finish_time - start_time\n            diff_n, diff_s = (\n                (diff_time / 60 / 60, \"hours\")\n                if diff_time / 60 / 60 >= 1\n                else (diff_time / 60, \"minutes\")\n                if diff_time / 60 >= 1\n                else (diff_time, \"seconds\")\n            )\n            diff_n = truncate_float(diff_n, 2)\n            passed_time = \"{} {}\".format(diff_n, diff_s)\n\n            if match is not None:\n                matched_following = len(set(all_following)) - len(\n                    set(all_following) - set(all_prior_following)\n                )\n                if matched_following >= match:\n                    new_following = set(all_following) - set(all_prior_following)\n                    all_following = all_following + all_prior_following\n                    print(\"\\n\")\n                    logger.info(\n                        \"Grabbed {} new usernames from `Following` in {}  \"\n                        \"~total of {} usernames\".format(\n                            len(set(new_following)),\n                            passed_time,\n                            len(set(all_following)),\n                        )\n                    )\n                    grab_notifier = True\n                    break\n\n            if grab != \"full\" and grabbed >= grab:\n                print(\"\\n\")\n                logger.info(\n                    \"Grabbed {} usernames from `Following` as requested at {}\".format(\n                        grabbed, passed_time\n                    )\n                )\n                grab_notifier = True\n                break\n\n            has_next_data = page_info[\"has_next_page\"]\n            if has_next_data:\n                variables[\"after\"] = page_info[\"end_cursor\"]\n\n                url = \"{}&variables={}\".format(\n                    graphql_following, str(json.dumps(variables))\n                )\n\n                web_address_navigator(browser, url)\n                sc_rolled += 1\n\n                # dumps the current graphql queries data\n                if local_read_failure is not True:\n                    try:\n                        with interruption_handler():\n                            with open(filename, \"w\") as graphql_queries_file:\n                                graphql_queries[username][query_date][\"sc_rolled\"] += 1\n                                json.dump(graphql_queries, graphql_queries_file)\n                    except Exception as exc:\n                        print(\"\\n\")\n                        logger.info(\n                            \"Error occurred while writing `scroll` data to \"\n                            \"graphql_queries.json\\n{}\\n\".format(\n                                str(exc).encode(\"utf-8\")\n                            )\n                        )\n\n                # take breaks gradually\n                if sc_rolled > 91:\n                    print(\"\\n\")\n                    logger.info(\"Queried too much! ~ sleeping a bit :>\")\n                    sleep(600)\n                    sc_rolled = 0\n\n    except BaseException as exc:\n        print(\"\\n\")\n        logger.info(\n            \"Unable to get `Following` data:\\n\\t{}\\n\".format(str(exc).encode(\"utf-8\"))\n        )\n\n    # remove possible duplicates\n    all_following = sorted(set(all_following), key=lambda x: all_following.index(x))\n\n    if grab_notifier is False:\n        print(\"\\n\")\n        logger.info(\n            \"Grabbed {} usernames from `Following` in {}\".format(\n                len(all_following), passed_time\n            )\n        )\n\n    if len(all_following) > 0:\n        if (\n            store_locally is True\n            and relationship_data[username][\"all_following\"] != all_following\n        ):\n            store_following_data(username, grab, all_following, logger, logfolder)\n        elif store_locally is True:\n            print(\"\")\n            logger.info(\n                \"The `Following` data is identical with the data in previous \"\n                \"query  ~not storing the file again\"\n            )\n\n        if grab == \"full\":\n            relationship_data[username].update({\"all_following\": all_following})\n\n    sleep_t = sc_rolled * 6\n    sleep_t = sleep_t if sleep_t < 600 else random.randint(585, 655)\n    sleep_n, sleep_s = (\n        (sleep_t / 60, \"minutes\") if sleep_t / 60 >= 1 else (sleep_t, \"seconds\")\n    )\n    sleep_n = truncate_float(sleep_n, 4)\n\n    print(\"\")\n    logger.info(\n        \"Zz :[ time to take a good nap  ~sleeping {} {}\".format(sleep_n, sleep_s)\n    )\n    sleep(sleep_t)\n    logger.info(\"Yawn :] let's go!\\n\")\n\n    return all_following",
  "smell": [
    {
      "smell_id": 3,
      "line_no": 269,
      "description": "The variable referenced in the statement has inconsistent types."
    }
  ]
}