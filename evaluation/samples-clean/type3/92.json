{
  "code": "def __init__(self,\n                 worker_index,\n                 model_creator,\n                 all_reduce_alg=\"simple\",\n                 num_devices=1,\n                 gpu=False,\n                 max_bytes=10000000,\n                 plasma_op=False):\n        self.worker_index = worker_index\n        assert num_devices > 0\n\n        # TODO(ekl) support custom session\n        tf_session_args = {\n            \"device_count\": {\n                \"CPU\": num_devices\n            },\n            \"log_device_placement\": False,\n            \"gpu_options\": tf.GPUOptions(force_gpu_compatible=True),\n            \"inter_op_parallelism_threads\": 128,\n        }\n        config_proto = tf.ConfigProto(**tf_session_args)\n        self.sess = tf.Session(config=config_proto)\n        self.models = []\n        grad_ops = []\n\n        if gpu:\n            device_tmpl = \"/gpu:%d\"\n        else:\n            device_tmpl = \"/cpu:%d\"\n        with self.sess.as_default():\n            for device_idx in range(num_devices):\n                device = device_tmpl % device_idx\n                with tf.device(device):\n                    with tf.variable_scope(\"device_%d\" % device_idx):\n                        model = model_creator(worker_index, device_idx)\n                        self.models.append(model)\n                        grads = [\n                            t for t in model.optimizer.compute_gradients(\n                                model.loss) if t[0] is not None\n                        ]\n                        grad_ops.append(grads)\n\n        if num_devices == 1:\n            if max_bytes:\n                raise ValueError(\n                    \"Implementation limitation: grad_shard_bytes > 0 \"\n                    \"({}) currently requires > 1 device\".format(max_bytes))\n            self.packed_grads_and_vars = grad_ops\n        else:\n            if max_bytes:\n                self.packed_grads_and_vars, packing_vals = (\n                    sum_gradients_all_reduce(\n                        \"\",\n                        grad_ops,\n                        1,\n                        all_reduce_alg,\n                        1,\n                        list(range(num_devices)),\n                        agg_small_grads_max_bytes=max_bytes))\n            else:\n                self.packed_grads_and_vars, _ = (sum_gradients_all_reduce(\n                    \"\",\n                    grad_ops,\n                    1,\n                    all_reduce_alg,\n                    1,\n                    list(range(num_devices)),\n                    agg_small_grads_max_bytes=0))\n        self.per_device_grads = [\n            list(zip(*dev_gv))[0] for dev_gv in self.packed_grads_and_vars\n        ]\n        assert (len(self.per_device_grads) == num_devices)\n        self.num_grads = num_grads = len(self.packed_grads_and_vars[0])\n        if max_bytes:\n            logger.info(\"Packed grads => {} tensors\".format(num_grads))\n\n        # Ops for reading grads with the right control deps\n        nccl_noops = []\n        for j in range(num_grads)[::-1]:\n            deps = nccl_noops + [\n                dev_grad[j] for dev_grad in self.per_device_grads\n            ]\n            with tf.control_dependencies(deps):\n                nccl_noops = [tf.no_op()]\n\n        # You must fetch this otherwise the NCCL allreduce will hang\n        self.nccl_control_out = tf.group(*nccl_noops)\n\n        if plasma_op:\n            store_socket = (\n                ray.worker.global_worker.plasma_client.store_socket_name)\n            manager_socket = (\n                ray.worker.global_worker.plasma_client.manager_socket_name)\n            if not plasma.tf_plasma_op:\n                plasma.build_plasma_tensorflow_op()\n\n            # For fetching grads -> plasma\n            self.plasma_in_grads = []\n            self.plasma_in_grads_oids = [\n                tf.placeholder(shape=[], dtype=tf.string, name=\"in_grad_oids\")\n                for _ in range(num_grads)\n            ]\n            for j in range(num_grads):\n                grad = self.per_device_grads[0][j]\n                with tf.device(self.models[0].loss.device):\n                    plasma_grad = plasma.tf_plasma_op.tensor_to_plasma(\n                        [grad],\n                        self.plasma_in_grads_oids[j],\n                        plasma_store_socket_name=store_socket,\n                        plasma_manager_socket_name=manager_socket)\n                self.plasma_in_grads.append(plasma_grad)\n\n            # For applying grads <- plasma\n            unpacked_gv = []\n            self.plasma_out_grads_oids = [\n                tf.placeholder(\n                    shape=[], dtype=tf.string, name=\"grad_out_oids\")\n                for _ in range(num_grads)\n            ]\n            packed_plasma_grads = []\n            for j in range(num_grads):\n                with tf.device(self.plasma_in_grads[j].device):\n                    with tf.control_dependencies([self.plasma_in_grads[j]]):\n                        grad_ph = plasma.tf_plasma_op.plasma_to_tensor(\n                            self.plasma_out_grads_oids[j],\n                            dtype=tf.float32,\n                            plasma_store_socket_name=store_socket,\n                            plasma_manager_socket_name=manager_socket)\n                grad_ph = tf.reshape(grad_ph,\n                                     self.packed_grads_and_vars[0][j][0].shape)\n                logger.debug(\"Packed tensor {}\".format(grad_ph))\n                packed_plasma_grads.append(grad_ph)\n            for i in range(num_devices):\n                per_device = []\n                for j, (g, v) in enumerate(self.packed_grads_and_vars[i]):\n                    grad_ph = packed_plasma_grads[j]\n                    per_device.append((grad_ph, v))\n                unpacked_gv.append(per_device)\n\n            if max_bytes:\n                unpacked_gv = unpack_small_tensors(unpacked_gv, packing_vals)\n\n        elif max_bytes:\n            unpacked_gv = unpack_small_tensors(self.packed_grads_and_vars,\n                                               packing_vals)\n        else:\n            unpacked_gv = self.packed_grads_and_vars\n\n        # Same shape as packed_grads_and_vars\n        assert len(unpacked_gv) == num_devices\n        assert len(unpacked_gv[0][0]) == 2\n\n        apply_ops = []\n        to_apply = unpacked_gv[0]\n        for ix, m in enumerate(self.models):\n            apply_ops.append(\n                m.optimizer.apply_gradients(\n                    [(g, v)\n                     for ((g, _), (_, v)) in zip(to_apply, unpacked_gv[ix])]))\n        self.apply_op = tf.group(*apply_ops)\n        init_op = tf.group(tf.global_variables_initializer(),\n                           tf.local_variables_initializer())\n        self.sess.run(init_op)",
  "smell": [
    {
      "smell_id": 3,
      "line_no": 33,
      "description": "The variable referenced in the statement has inconsistent types."
    }
  ]
}