{
  "code": "def __init__(self, hparams):\n    self.hparams = hparams\n    self.epochs = 0\n    self.curr_train_index = 0\n\n    all_labels = []\n\n    self.good_policies = found_policies.good_policies()\n\n    # Determine how many databatched to load\n    num_data_batches_to_load = 5\n    total_batches_to_load = num_data_batches_to_load\n    train_batches_to_load = total_batches_to_load\n    assert hparams.train_size + hparams.validation_size <= 50000\n    if hparams.eval_test:\n      total_batches_to_load += 1\n    # Determine how many images we have loaded\n    total_dataset_size = 10000 * num_data_batches_to_load\n    train_dataset_size = total_dataset_size\n    if hparams.eval_test:\n      total_dataset_size += 10000\n\n    if hparams.dataset == 'cifar10':\n      all_data = np.empty((total_batches_to_load, 10000, 3072), dtype=np.uint8)\n    elif hparams.dataset == 'cifar100':\n      assert num_data_batches_to_load == 5\n      all_data = np.empty((1, 50000, 3072), dtype=np.uint8)\n      if hparams.eval_test:\n        test_data = np.empty((1, 10000, 3072), dtype=np.uint8)\n    if hparams.dataset == 'cifar10':\n      tf.logging.info('Cifar10')\n      datafiles = [\n          'data_batch_1', 'data_batch_2', 'data_batch_3', 'data_batch_4',\n          'data_batch_5']\n\n      datafiles = datafiles[:train_batches_to_load]\n      if hparams.eval_test:\n        datafiles.append('test_batch')\n      num_classes = 10\n    elif hparams.dataset == 'cifar100':\n      datafiles = ['train']\n      if hparams.eval_test:\n        datafiles.append('test')\n      num_classes = 100\n    else:\n      raise NotImplementedError('Unimplemented dataset: ', hparams.dataset)\n    if hparams.dataset != 'test':\n      for file_num, f in enumerate(datafiles):\n        d = unpickle(os.path.join(hparams.data_path, f))\n        if f == 'test':\n          test_data[0] = copy.deepcopy(d['data'])\n          all_data = np.concatenate([all_data, test_data], axis=1)\n        else:\n          all_data[file_num] = copy.deepcopy(d['data'])\n        if hparams.dataset == 'cifar10':\n          labels = np.array(d['labels'])\n        else:\n          labels = np.array(d['fine_labels'])\n        nsamples = len(labels)\n        for idx in range(nsamples):\n          all_labels.append(labels[idx])\n\n    all_data = all_data.reshape(total_dataset_size, 3072)\n    all_data = all_data.reshape(-1, 3, 32, 32)\n    all_data = all_data.transpose(0, 2, 3, 1).copy()\n    all_data = all_data / 255.0\n    mean = augmentation_transforms.MEANS\n    std = augmentation_transforms.STDS\n    tf.logging.info('mean:{}    std: {}'.format(mean, std))\n\n    all_data = (all_data - mean) / std\n    all_labels = np.eye(num_classes)[np.array(all_labels, dtype=np.int32)]\n    assert len(all_data) == len(all_labels)\n    tf.logging.info(\n        'In CIFAR10 loader, number of images: {}'.format(len(all_data)))\n\n    # Break off test data\n    if hparams.eval_test:\n      self.test_images = all_data[train_dataset_size:]\n      self.test_labels = all_labels[train_dataset_size:]\n\n    # Shuffle the rest of the data\n    all_data = all_data[:train_dataset_size]\n    all_labels = all_labels[:train_dataset_size]\n    np.random.seed(0)\n    perm = np.arange(len(all_data))\n    np.random.shuffle(perm)\n    all_data = all_data[perm]\n    all_labels = all_labels[perm]\n\n    # Break into train and val\n    train_size, val_size = hparams.train_size, hparams.validation_size\n    assert 50000 >= train_size + val_size\n    self.train_images = all_data[:train_size]\n    self.train_labels = all_labels[:train_size]\n    self.val_images = all_data[train_size:train_size + val_size]\n    self.val_labels = all_labels[train_size:train_size + val_size]\n    self.num_train = self.train_images.shape[0]",
  "smell": [
    {
      "smell_id": 3,
      "line_no": 73,
      "description": "The variable referenced in the statement has inconsistent types."
    }
  ]
}