{
  "code": "def __init__(self, cfg, num_levels, num_anchors, mask_sizes, input_shape: List[ShapeSpec]):\n        \"\"\"\n        TensorMask head.\n        \"\"\"\n        super().__init__()\n        # fmt: off\n        self.in_features        = cfg.MODEL.TENSOR_MASK.IN_FEATURES\n        in_channels             = input_shape[0].channels\n        num_classes             = cfg.MODEL.TENSOR_MASK.NUM_CLASSES\n        cls_channels            = cfg.MODEL.TENSOR_MASK.CLS_CHANNELS\n        num_convs               = cfg.MODEL.TENSOR_MASK.NUM_CONVS\n        # box parameters\n        bbox_channels           = cfg.MODEL.TENSOR_MASK.BBOX_CHANNELS\n        # mask parameters\n        self.mask_on            = cfg.MODEL.MASK_ON\n        self.mask_sizes         = mask_sizes\n        mask_channels           = cfg.MODEL.TENSOR_MASK.MASK_CHANNELS\n        self.align_on           = cfg.MODEL.TENSOR_MASK.ALIGNED_ON\n        self.bipyramid_on       = cfg.MODEL.TENSOR_MASK.BIPYRAMID_ON\n        # fmt: on\n\n        # class subnet\n        cls_subnet = []\n        cur_channels = in_channels\n        for _ in range(num_convs):\n            cls_subnet.append(\n                nn.Conv2d(cur_channels, cls_channels, kernel_size=3, stride=1, padding=1)\n            )\n            cur_channels = cls_channels\n            cls_subnet.append(nn.ReLU())\n\n        self.cls_subnet = nn.Sequential(*cls_subnet)\n        self.cls_score = nn.Conv2d(\n            cur_channels, num_anchors * num_classes, kernel_size=3, stride=1, padding=1\n        )\n        modules_list = [self.cls_subnet, self.cls_score]\n\n        # box subnet\n        bbox_subnet = []\n        cur_channels = in_channels\n        for _ in range(num_convs):\n            bbox_subnet.append(\n                nn.Conv2d(cur_channels, bbox_channels, kernel_size=3, stride=1, padding=1)\n            )\n            cur_channels = bbox_channels\n            bbox_subnet.append(nn.ReLU())\n\n        self.bbox_subnet = nn.Sequential(*bbox_subnet)\n        self.bbox_pred = nn.Conv2d(\n            cur_channels, num_anchors * 4, kernel_size=3, stride=1, padding=1\n        )\n        modules_list.extend([self.bbox_subnet, self.bbox_pred])\n\n        # mask subnet\n        if self.mask_on:\n            mask_subnet = []\n            cur_channels = in_channels\n            for _ in range(num_convs):\n                mask_subnet.append(\n                    nn.Conv2d(cur_channels, mask_channels, kernel_size=3, stride=1, padding=1)\n                )\n                cur_channels = mask_channels\n                mask_subnet.append(nn.ReLU())\n\n            self.mask_subnet = nn.Sequential(*mask_subnet)\n            modules_list.append(self.mask_subnet)\n            for mask_size in self.mask_sizes:\n                cur_mask_module = \"mask_pred_%02d\" % mask_size\n                self.add_module(\n                    cur_mask_module,\n                    nn.Conv2d(\n                        cur_channels, mask_size * mask_size, kernel_size=1, stride=1, padding=0\n                    ),\n                )\n                modules_list.append(getattr(self, cur_mask_module))\n            if self.align_on:\n                if self.bipyramid_on:\n                    for lvl in range(num_levels):\n                        cur_mask_module = \"align2nat_%02d\" % lvl\n                        lambda_val = 2**lvl\n                        setattr(self, cur_mask_module, SwapAlign2Nat(lambda_val))\n                    # Also the fusing layer, stay at the same channel size\n                    mask_fuse = [\n                        nn.Conv2d(cur_channels, cur_channels, kernel_size=3, stride=1, padding=1),\n                        nn.ReLU(),\n                    ]\n                    self.mask_fuse = nn.Sequential(*mask_fuse)\n                    modules_list.append(self.mask_fuse)\n                else:\n                    self.align2nat = SwapAlign2Nat(1)\n\n        # Initialization\n        for modules in modules_list:\n            for layer in modules.modules():\n                if isinstance(layer, nn.Conv2d):\n                    torch.nn.init.normal_(layer.weight, mean=0, std=0.01)\n                    torch.nn.init.constant_(layer.bias, 0)\n\n        # Use prior in model initialization to improve stability\n        bias_value = -(math.log((1 - 0.01) / 0.01))\n        torch.nn.init.constant_(self.cls_score.bias, bias_value)",
  "smell": [
    {
      "smell_id": 6,
      "line_no": 75,
      "description": "The attribute is visited based on a dynamically determined name."
    }
  ]
}