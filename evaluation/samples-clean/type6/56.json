{
  "code": "def main(config, device, logger, vdl_writer):\n    # init dist environment\n    if config['Global']['distributed']:\n        dist.init_parallel_env()\n\n    global_config = config['Global']\n\n    # build dataloader\n    train_dataloader = build_dataloader(config, 'Train', device, logger)\n    if config['Eval']:\n        valid_dataloader = build_dataloader(config, 'Eval', device, logger)\n    else:\n        valid_dataloader = None\n\n    # build post process\n    post_process_class = build_post_process(config['PostProcess'],\n                                            global_config)\n\n    # build model\n    # for rec algorithm\n    if hasattr(post_process_class, 'character'):\n        char_num = len(getattr(post_process_class, 'character'))\n        config['Architecture'][\"Head\"]['out_channels'] = char_num\n    model = build_model(config['Architecture'])\n    if config['Architecture']['model_type'] == 'det':\n        input_shape = [1, 3, 640, 640]\n    elif config['Architecture']['model_type'] == 'rec':\n        input_shape = [1, 3, 32, 320]\n    flops = paddle.flops(model, input_shape)\n\n    logger.info(\"FLOPs before pruning: {}\".format(flops))\n\n    from paddleslim.dygraph import FPGMFilterPruner\n    model.train()\n\n    pruner = FPGMFilterPruner(model, input_shape)\n\n    # build loss\n    loss_class = build_loss(config['Loss'])\n\n    # build optim\n    optimizer, lr_scheduler = build_optimizer(\n        config['Optimizer'],\n        epochs=config['Global']['epoch_num'],\n        step_each_epoch=len(train_dataloader),\n        model=model)\n\n    # build metric\n    eval_class = build_metric(config['Metric'])\n    # load pretrain model\n    pre_best_model_dict = load_model(config, model, optimizer)\n\n    logger.info('train dataloader has {} iters, valid dataloader has {} iters'.\n                format(len(train_dataloader), len(valid_dataloader)))\n    # build metric\n    eval_class = build_metric(config['Metric'])\n\n    logger.info('train dataloader has {} iters, valid dataloader has {} iters'.\n                format(len(train_dataloader), len(valid_dataloader)))\n\n    def eval_fn():\n        metric = program.eval(model, valid_dataloader, post_process_class,\n                              eval_class, False)\n        if config['Architecture']['model_type'] == 'det':\n            main_indicator = 'hmean'\n        else:\n            main_indicator = 'acc'\n\n        logger.info(\"metric[{}]: {}\".format(main_indicator, metric[\n            main_indicator]))\n        return metric[main_indicator]\n\n    run_sensitive_analysis = False\n    \"\"\"\n    run_sensitive_analysis=True: \n        Automatically compute the sensitivities of convolutions in a model. \n        The sensitivity of a convolution is the losses of accuracy on test dataset in \n        differenct pruned ratios. The sensitivities can be used to get a group of best \n        ratios with some condition.\n    \n    run_sensitive_analysis=False: \n        Set prune trim ratio to a fixed value, such as 10%. The larger the value, \n        the more convolution weights will be cropped.\n\n    \"\"\"\n\n    if run_sensitive_analysis:\n        params_sensitive = pruner.sensitive(\n            eval_func=eval_fn,\n            sen_file=\"./deploy/slim/prune/sen.pickle\",\n            skip_vars=[\n                \"conv2d_57.w_0\", \"conv2d_transpose_2.w_0\",\n                \"conv2d_transpose_3.w_0\"\n            ])\n        logger.info(\n            \"The sensitivity analysis results of model parameters saved in sen.pickle\"\n        )\n        # calculate pruned params's ratio\n        params_sensitive = pruner._get_ratios_by_loss(\n            params_sensitive, loss=0.02)\n        for key in params_sensitive.keys():\n            logger.info(\"{}, {}\".format(key, params_sensitive[key]))\n    else:\n        params_sensitive = {}\n        for param in model.parameters():\n            if 'transpose' not in param.name and 'linear' not in param.name:\n                # set prune ratio as 10%. The larger the value, the more convolution weights will be cropped\n                params_sensitive[param.name] = 0.1\n\n    plan = pruner.prune_vars(params_sensitive, [0])\n\n    flops = paddle.flops(model, input_shape)\n    logger.info(\"FLOPs after pruning: {}\".format(flops))\n\n    # start train\n\n    program.train(config, train_dataloader, valid_dataloader, device, model,\n                  loss_class, optimizer, lr_scheduler, post_process_class,\n                  eval_class, pre_best_model_dict, logger, vdl_writer)",
  "smell": [
    {
      "smell_id": 6,
      "line_no": 22,
      "description": "The attribute is visited based on a dynamically determined name."
    }
  ]
}