{
  "code": "def main(config, device, logger, vdl_writer):\n    # init dist environment\n    if config['Global']['distributed']:\n        dist.init_parallel_env()\n\n    global_config = config['Global']\n\n    # build dataloader\n    train_dataloader = build_dataloader(config, 'Train', device, logger)\n    if len(train_dataloader) == 0:\n        logger.error(\n            \"No Images in train dataset, please ensure\\n\" +\n            \"\\t1. The images num in the train label_file_list should be larger than or equal with batch size.\\n\"\n            +\n            \"\\t2. The annotation file and path in the configuration file are provided normally.\"\n        )\n        return\n\n    if config['Eval']:\n        valid_dataloader = build_dataloader(config, 'Eval', device, logger)\n    else:\n        valid_dataloader = None\n\n    # build post process\n    post_process_class = build_post_process(config['PostProcess'],\n                                            global_config)\n\n    # build model\n    # for rec algorithm\n    if hasattr(post_process_class, 'character'):\n        char_num = len(getattr(post_process_class, 'character'))\n        if config['Architecture'][\"algorithm\"] in [\"Distillation\",\n                                                   ]:  # distillation model\n            for key in config['Architecture'][\"Models\"]:\n                if config['Architecture']['Models'][key]['Head'][\n                        'name'] == 'MultiHead':  # for multi head\n                    if config['PostProcess'][\n                            'name'] == 'DistillationSARLabelDecode':\n                        char_num = char_num - 2\n                    # update SARLoss params\n                    assert list(config['Loss']['loss_config_list'][-1].keys())[\n                        0] == 'DistillationSARLoss'\n                    config['Loss']['loss_config_list'][-1][\n                        'DistillationSARLoss']['ignore_index'] = char_num + 1\n                    out_channels_list = {}\n                    out_channels_list['CTCLabelDecode'] = char_num\n                    out_channels_list['SARLabelDecode'] = char_num + 2\n                    config['Architecture']['Models'][key]['Head'][\n                        'out_channels_list'] = out_channels_list\n                else:\n                    config['Architecture'][\"Models\"][key][\"Head\"][\n                        'out_channels'] = char_num\n        elif config['Architecture']['Head'][\n                'name'] == 'MultiHead':  # for multi head\n            if config['PostProcess']['name'] == 'SARLabelDecode':\n                char_num = char_num - 2\n            # update SARLoss params\n            assert list(config['Loss']['loss_config_list'][1].keys())[\n                0] == 'SARLoss'\n            if config['Loss']['loss_config_list'][1]['SARLoss'] is None:\n                config['Loss']['loss_config_list'][1]['SARLoss'] = {\n                    'ignore_index': char_num + 1\n                }\n            else:\n                config['Loss']['loss_config_list'][1]['SARLoss'][\n                    'ignore_index'] = char_num + 1\n            out_channels_list = {}\n            out_channels_list['CTCLabelDecode'] = char_num\n            out_channels_list['SARLabelDecode'] = char_num + 2\n            config['Architecture']['Head'][\n                'out_channels_list'] = out_channels_list\n        else:  # base rec model\n            config['Architecture'][\"Head\"]['out_channels'] = char_num\n\n        if config['PostProcess']['name'] == 'SARLabelDecode':  # for SAR model\n            config['Loss']['ignore_index'] = char_num - 1\n\n    model = build_model(config['Architecture'])\n\n    use_sync_bn = config[\"Global\"].get(\"use_sync_bn\", False)\n    if use_sync_bn:\n        model = paddle.nn.SyncBatchNorm.convert_sync_batchnorm(model)\n        logger.info('convert_sync_batchnorm')\n\n    model = apply_to_static(model, config, logger)\n\n    # build loss\n    loss_class = build_loss(config['Loss'])\n\n    # build optim\n    optimizer, lr_scheduler = build_optimizer(\n        config['Optimizer'],\n        epochs=config['Global']['epoch_num'],\n        step_each_epoch=len(train_dataloader),\n        model=model)\n\n    # build metric\n    eval_class = build_metric(config['Metric'])\n\n    logger.info('train dataloader has {} iters'.format(len(train_dataloader)))\n    if valid_dataloader is not None:\n        logger.info('valid dataloader has {} iters'.format(\n            len(valid_dataloader)))\n\n    use_amp = config[\"Global\"].get(\"use_amp\", False)\n    amp_level = config[\"Global\"].get(\"amp_level\", 'O2')\n    amp_custom_black_list = config['Global'].get('amp_custom_black_list', [])\n    if use_amp:\n        AMP_RELATED_FLAGS_SETTING = {'FLAGS_max_inplace_grad_add': 8, }\n        if paddle.is_compiled_with_cuda():\n            AMP_RELATED_FLAGS_SETTING.update({\n                'FLAGS_cudnn_batchnorm_spatial_persistent': 1\n            })\n        paddle.fluid.set_flags(AMP_RELATED_FLAGS_SETTING)\n        scale_loss = config[\"Global\"].get(\"scale_loss\", 1.0)\n        use_dynamic_loss_scaling = config[\"Global\"].get(\n            \"use_dynamic_loss_scaling\", False)\n        scaler = paddle.amp.GradScaler(\n            init_loss_scaling=scale_loss,\n            use_dynamic_loss_scaling=use_dynamic_loss_scaling)\n        if amp_level == \"O2\":\n            model, optimizer = paddle.amp.decorate(\n                models=model,\n                optimizers=optimizer,\n                level=amp_level,\n                master_weight=True)\n    else:\n        scaler = None\n\n    # load pretrain model\n    pre_best_model_dict = load_model(config, model, optimizer,\n                                     config['Architecture'][\"model_type\"])\n\n    if config['Global']['distributed']:\n        model = paddle.DataParallel(model)\n    # start train\n    program.train(config, train_dataloader, valid_dataloader, device, model,\n                  loss_class, optimizer, lr_scheduler, post_process_class,\n                  eval_class, pre_best_model_dict, logger, vdl_writer, scaler,\n                  amp_level, amp_custom_black_list)",
  "smell": [
    {
      "smell_id": 6,
      "line_no": 31,
      "description": "The attribute is visited based on a dynamically determined name."
    }
  ]
}