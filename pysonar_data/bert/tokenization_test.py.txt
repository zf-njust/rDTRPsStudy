<<__future__>> <<15:634-644>> <<module>> <<VARIABLE>> <<ANCHOR>>
<<__future__>> <<16:674-684>> <<module>> <<VARIABLE>> <<ANCHOR>>
<<__future__>> <<17:707-717>> <<module>> <<VARIABLE>> <<ANCHOR>>
<<TokenizationTest>> <<26:839-855>> <<type>> <<CLASS>> <<ANCHOR>>
<<test_full_tokenizer>> <<28:884-903>> <<function>> <<METHOD>> <<ANCHOR>>
<<test_chinese>> <<51:1692-1704>> <<function>> <<METHOD>> <<ANCHOR>>
<<test_basic_tokenizer_lower>> <<58:1893-1919>> <<function>> <<METHOD>> <<ANCHOR>>
<<test_basic_tokenizer_no_lower>> <<66:2212-2241>> <<function>> <<METHOD>> <<ANCHOR>>
<<test_wordpiece_tokenizer>> <<73:2464-2488>> <<function>> <<METHOD>> <<ANCHOR>>
<<test_convert_tokens_to_ids>> <<93:3075-3101>> <<function>> <<METHOD>> <<ANCHOR>>
<<test_is_whitespace>> <<107:3490-3508>> <<function>> <<METHOD>> <<ANCHOR>>
<<test_is_control>> <<117:3929-3944>> <<function>> <<METHOD>> <<ANCHOR>>
<<test_is_punctuation>> <<126:4302-4321>> <<function>> <<METHOD>> <<ANCHOR>>
<<self>> <<73:2489-2492>> <<TokenizationTest>> <<PARAMETER>> <<ANCHOR>>
<<vocab_tokens>> <<74:2501-2513>> <<list>> <<VARIABLE>> <<ANCHOR>>
<<vocab>> <<79:2631-2636>> <<dict>> <<VARIABLE>> <<ANCHOR>>
<<i>> <<80:2652-2653>> <<?>> <<SCOPE>> <<ANCHOR>>
<<token>> <<80:2655-2660>> <<?>> <<SCOPE>> <<ANCHOR>>
<<tokenizer>> <<82:2719-2728>> <<?>> <<VARIABLE>> <<ANCHOR>>
<<self>> <<28:904-907>> <<TokenizationTest>> <<PARAMETER>> <<ANCHOR>>
<<vocab_tokens>> <<29:916-928>> <<list>> <<VARIABLE>> <<ANCHOR>>
<<vocab_writer>> <<33:1099-1111>> <<?>> <<VARIABLE>> <<ANCHOR>>
<<x>> <<35:1182-1183>> <<str>> <<SCOPE>> <<ANCHOR>>
<<x>> <<38:1280-1281>> <<str>> <<SCOPE>> <<ANCHOR>>
<<vocab_file>> <<40:1326-1336>> <<?>> <<VARIABLE>> <<ANCHOR>>
<<tokenizer>> <<42:1364-1373>> <<?>> <<VARIABLE>> <<ANCHOR>>
<<tokens>> <<45:1449-1455>> <<?>> <<VARIABLE>> <<ANCHOR>>
<<self>> <<107:3509-3512>> <<TokenizationTest>> <<PARAMETER>> <<ANCHOR>>
<<self>> <<126:4322-4325>> <<TokenizationTest>> <<PARAMETER>> <<ANCHOR>>
<<self>> <<66:2242-2245>> <<TokenizationTest>> <<PARAMETER>> <<ANCHOR>>
<<tokenizer>> <<67:2254-2263>> <<?>> <<VARIABLE>> <<ANCHOR>>
<<self>> <<117:3945-3948>> <<TokenizationTest>> <<PARAMETER>> <<ANCHOR>>
<<self>> <<51:1705-1708>> <<TokenizationTest>> <<PARAMETER>> <<ANCHOR>>
<<tokenizer>> <<52:1717-1726>> <<?>> <<VARIABLE>> <<ANCHOR>>
<<self>> <<93:3102-3105>> <<TokenizationTest>> <<PARAMETER>> <<ANCHOR>>
<<vocab_tokens>> <<94:3114-3126>> <<list>> <<VARIABLE>> <<ANCHOR>>
<<vocab>> <<99:3244-3249>> <<dict>> <<VARIABLE>> <<ANCHOR>>
<<i>> <<100:3265-3266>> <<?>> <<SCOPE>> <<ANCHOR>>
<<token>> <<100:3268-3273>> <<?>> <<SCOPE>> <<ANCHOR>>
<<self>> <<58:1920-1923>> <<TokenizationTest>> <<PARAMETER>> <<ANCHOR>>
<<tokenizer>> <<59:1932-1941>> <<?>> <<VARIABLE>> <<ANCHOR>>
<<absolute_import>> <<15:652-667>> <<_Feature>> <<VARIABLE>> <<LINK>>
<<division>> <<16:692-700>> <<_Feature>> <<VARIABLE>> <<LINK>>
<<print_function>> <<17:725-739>> <<_Feature>> <<VARIABLE>> <<LINK>>
<<vocab_tokens>> <<80:2675-2687>> <<list>> <<VARIABLE>> <<LINK>>
<<i>> <<81:2712-2713>> <<?>> <<VARIABLE>> <<LINK>>
<<vocab>> <<81:2697-2702>> <<dict>> <<VARIABLE>> <<LINK>>
<<token>> <<81:2703-2708>> <<?>> <<VARIABLE>> <<LINK>>
<<vocab>> <<82:2769-2774>> <<dict>> <<VARIABLE>> <<LINK>>
<<self>> <<84:2783-2787>> <<TokenizationTest>> <<VARIABLE>> <<LINK>>
<<tokenizer>> <<84:2803-2812>> <<?>> <<VARIABLE>> <<LINK>>
<<self>> <<86:2838-2842>> <<TokenizationTest>> <<VARIABLE>> <<LINK>>
<<tokenizer>> <<87:2868-2877>> <<?>> <<VARIABLE>> <<LINK>>
<<self>> <<90:2967-2971>> <<TokenizationTest>> <<VARIABLE>> <<LINK>>
<<tokenizer>> <<91:2997-3006>> <<?>> <<VARIABLE>> <<LINK>>
<<vocab_writer>> <<35:1141-1153>> <<?>> <<VARIABLE>> <<LINK>>
<<vocab_tokens>> <<35:1187-1199>> <<list>> <<VARIABLE>> <<LINK>>
<<x>> <<35:1182-1183>> <<str>> <<VARIABLE>> <<LINK>>
<<x>> <<35:1169-1170>> <<str>> <<VARIABLE>> <<LINK>>
<<vocab_writer>> <<37:1225-1237>> <<?>> <<VARIABLE>> <<LINK>>
<<vocab_tokens>> <<38:1285-1297>> <<list>> <<VARIABLE>> <<LINK>>
<<x>> <<38:1280-1281>> <<str>> <<VARIABLE>> <<LINK>>
<<x>> <<38:1267-1268>> <<str>> <<VARIABLE>> <<LINK>>
<<vocab_writer>> <<40:1339-1351>> <<?>> <<VARIABLE>> <<LINK>>
<<vocab_file>> <<42:1403-1413>> <<?>> <<VARIABLE>> <<LINK>>
<<vocab_file>> <<43:1430-1440>> <<?>> <<VARIABLE>> <<LINK>>
<<tokenizer>> <<45:1458-1467>> <<?>> <<VARIABLE>> <<LINK>>
<<self>> <<46:1508-1512>> <<TokenizationTest>> <<VARIABLE>> <<LINK>>
<<tokens>> <<46:1528-1534>> <<?>> <<VARIABLE>> <<LINK>>
<<self>> <<48:1591-1595>> <<TokenizationTest>> <<VARIABLE>> <<LINK>>
<<tokenizer>> <<49:1621-1630>> <<?>> <<VARIABLE>> <<LINK>>
<<tokens>> <<49:1653-1659>> <<?>> <<VARIABLE>> <<LINK>>
<<self>> <<108:3521-3525>> <<TokenizationTest>> <<VARIABLE>> <<LINK>>
<<self>> <<109:3577-3581>> <<TokenizationTest>> <<VARIABLE>> <<LINK>>
<<self>> <<110:3634-3638>> <<TokenizationTest>> <<VARIABLE>> <<LINK>>
<<self>> <<111:3691-3695>> <<TokenizationTest>> <<VARIABLE>> <<LINK>>
<<self>> <<112:3748-3752>> <<TokenizationTest>> <<VARIABLE>> <<LINK>>
<<self>> <<114:3811-3815>> <<TokenizationTest>> <<VARIABLE>> <<LINK>>
<<self>> <<115:3868-3872>> <<TokenizationTest>> <<VARIABLE>> <<LINK>>
<<self>> <<127:4334-4338>> <<TokenizationTest>> <<VARIABLE>> <<LINK>>
<<self>> <<128:4391-4395>> <<TokenizationTest>> <<VARIABLE>> <<LINK>>
<<self>> <<129:4448-4452>> <<TokenizationTest>> <<VARIABLE>> <<LINK>>
<<self>> <<130:4505-4509>> <<TokenizationTest>> <<VARIABLE>> <<LINK>>
<<self>> <<132:4564-4568>> <<TokenizationTest>> <<VARIABLE>> <<LINK>>
<<self>> <<133:4622-4626>> <<TokenizationTest>> <<VARIABLE>> <<LINK>>
<<self>> <<69:2322-2326>> <<TokenizationTest>> <<VARIABLE>> <<LINK>>
<<tokenizer>> <<70:2352-2361>> <<?>> <<VARIABLE>> <<LINK>>
<<self>> <<118:3957-3961>> <<TokenizationTest>> <<VARIABLE>> <<LINK>>
<<self>> <<120:4017-4021>> <<TokenizationTest>> <<VARIABLE>> <<LINK>>
<<self>> <<121:4071-4075>> <<TokenizationTest>> <<VARIABLE>> <<LINK>>
<<self>> <<122:4125-4129>> <<TokenizationTest>> <<VARIABLE>> <<LINK>>
<<self>> <<123:4180-4184>> <<TokenizationTest>> <<VARIABLE>> <<LINK>>
<<self>> <<124:4235-4239>> <<TokenizationTest>> <<VARIABLE>> <<LINK>>
<<self>> <<54:1766-1770>> <<TokenizationTest>> <<VARIABLE>> <<LINK>>
<<tokenizer>> <<55:1796-1805>> <<?>> <<VARIABLE>> <<LINK>>
<<vocab_tokens>> <<100:3288-3300>> <<list>> <<VARIABLE>> <<LINK>>
<<i>> <<101:3325-3326>> <<?>> <<VARIABLE>> <<LINK>>
<<vocab>> <<101:3310-3315>> <<dict>> <<VARIABLE>> <<LINK>>
<<token>> <<101:3316-3321>> <<?>> <<VARIABLE>> <<LINK>>
<<self>> <<103:3334-3338>> <<TokenizationTest>> <<VARIABLE>> <<LINK>>
<<vocab>> <<105:3413-3418>> <<dict>> <<VARIABLE>> <<LINK>>
<<self>> <<61:1999-2003>> <<TokenizationTest>> <<VARIABLE>> <<LINK>>
<<tokenizer>> <<62:2029-2038>> <<?>> <<VARIABLE>> <<LINK>>
<<self>> <<64:2137-2141>> <<TokenizationTest>> <<VARIABLE>> <<LINK>>
<<tokenizer>> <<64:2157-2166>> <<?>> <<VARIABLE>> <<LINK>>
