<<__future__>> <<15:634-644>> <<module>> <<VARIABLE>> <<ANCHOR>>
<<unittest>> <<18:733-741>> <<module>> <<VARIABLE>> <<ANCHOR>>
<<io>> <<19:748-750>> <<module>> <<VARIABLE>> <<ANCHOR>>
<<pytorch_pretrained_bert>> <<21:771-794>> <<module>> <<VARIABLE>> <<ANCHOR>>
<<TokenizationTest>> <<28:1128-1144>> <<type>> <<CLASS>> <<ANCHOR>>
<<test_full_tokenizer>> <<30:1176-1195>> <<function>> <<METHOD>> <<ANCHOR>>
<<test_full_tokenizer_raises_error_for_long_sequences>> <<49:1914-1965>> <<function>> <<METHOD>> <<ANCHOR>>
<<test_chinese>> <<67:2807-2819>> <<function>> <<METHOD>> <<ANCHOR>>
<<test_basic_tokenizer_lower>> <<74:3014-3040>> <<function>> <<METHOD>> <<ANCHOR>>
<<test_basic_tokenizer_no_lower>> <<82:3344-3373>> <<function>> <<METHOD>> <<ANCHOR>>
<<test_wordpiece_tokenizer>> <<89:3602-3626>> <<function>> <<METHOD>> <<ANCHOR>>
<<test_is_whitespace>> <<109:4263-4281>> <<function>> <<METHOD>> <<ANCHOR>>
<<test_is_control>> <<119:4641-4656>> <<function>> <<METHOD>> <<ANCHOR>>
<<test_is_punctuation>> <<127:4908-4927>> <<function>> <<METHOD>> <<ANCHOR>>
<<self>> <<89:3627-3630>> <<TokenizationTest>> <<PARAMETER>> <<ANCHOR>>
<<vocab_tokens>> <<90:3643-3655>> <<list>> <<VARIABLE>> <<ANCHOR>>
<<vocab>> <<95:3789-3794>> <<dict>> <<VARIABLE>> <<ANCHOR>>
<<i>> <<96:3814-3815>> <<?>> <<SCOPE>> <<ANCHOR>>
<<token>> <<96:3817-3822>> <<?>> <<SCOPE>> <<ANCHOR>>
<<tokenizer>> <<98:3891-3900>> <<?>> <<VARIABLE>> <<ANCHOR>>
<<self>> <<119:4657-4660>> <<TokenizationTest>> <<PARAMETER>> <<ANCHOR>>
<<self>> <<127:4928-4931>> <<TokenizationTest>> <<PARAMETER>> <<ANCHOR>>
<<self>> <<30:1196-1199>> <<TokenizationTest>> <<PARAMETER>> <<ANCHOR>>
<<vocab_tokens>> <<31:1212-1224>> <<list>> <<VARIABLE>> <<ANCHOR>>
<<vocab_writer>> <<35:1429-1441>> <<file>> <<VARIABLE>> <<ANCHOR>>
<<x>> <<36:1497-1498>> <<str>> <<SCOPE>> <<ANCHOR>>
<<vocab_file>> <<38:1533-1543>> <<str>> <<VARIABLE>> <<ANCHOR>>
<<tokenizer>> <<40:1575-1584>> <<?>> <<VARIABLE>> <<ANCHOR>>
<<tokens>> <<43:1655-1661>> <<?>> <<VARIABLE>> <<ANCHOR>>
<<self>> <<67:2820-2823>> <<TokenizationTest>> <<PARAMETER>> <<ANCHOR>>
<<tokenizer>> <<68:2836-2845>> <<?>> <<VARIABLE>> <<ANCHOR>>
<<self>> <<109:4282-4285>> <<TokenizationTest>> <<PARAMETER>> <<ANCHOR>>
<<self>> <<49:1966-1969>> <<TokenizationTest>> <<PARAMETER>> <<ANCHOR>>
<<vocab_tokens>> <<50:1982-1994>> <<list>> <<VARIABLE>> <<ANCHOR>>
<<vocab_writer>> <<54:2199-2211>> <<file>> <<VARIABLE>> <<ANCHOR>>
<<x>> <<55:2267-2268>> <<str>> <<SCOPE>> <<ANCHOR>>
<<vocab_file>> <<56:2301-2311>> <<str>> <<VARIABLE>> <<ANCHOR>>
<<tokenizer>> <<58:2343-2352>> <<?>> <<VARIABLE>> <<ANCHOR>>
<<tokens>> <<60:2433-2439>> <<?>> <<VARIABLE>> <<ANCHOR>>
<<indices>> <<61:2516-2523>> <<?>> <<VARIABLE>> <<ANCHOR>>
<<_>> <<62:2612-2613>> <<int>> <<SCOPE>> <<ANCHOR>>
<<tokens>> <<64:2640-2646>> <<?>> <<VARIABLE>> <<ANCHOR>>
<<self>> <<74:3041-3044>> <<TokenizationTest>> <<PARAMETER>> <<ANCHOR>>
<<tokenizer>> <<75:3057-3066>> <<?>> <<VARIABLE>> <<ANCHOR>>
<<self>> <<82:3374-3377>> <<TokenizationTest>> <<PARAMETER>> <<ANCHOR>>
<<tokenizer>> <<83:3390-3399>> <<?>> <<VARIABLE>> <<ANCHOR>>
<<absolute_import>> <<15:652-667>> <<_Feature>> <<VARIABLE>> <<LINK>>
<<division>> <<15:669-677>> <<_Feature>> <<VARIABLE>> <<LINK>>
<<print_function>> <<15:679-693>> <<_Feature>> <<VARIABLE>> <<LINK>>
<<unicode_literals>> <<15:695-711>> <<_Feature>> <<VARIABLE>> <<LINK>>
<<unittest>> <<28:1145-1153>> <<module>> <<VARIABLE>> <<LINK>>
<<unittest>> <<138:5264-5272>> <<module>> <<VARIABLE>> <<LINK>>
<<vocab_tokens>> <<96:3837-3849>> <<list>> <<VARIABLE>> <<LINK>>
<<i>> <<97:3880-3881>> <<?>> <<VARIABLE>> <<LINK>>
<<vocab>> <<97:3865-3870>> <<dict>> <<VARIABLE>> <<LINK>>
<<token>> <<97:3871-3876>> <<?>> <<VARIABLE>> <<LINK>>
<<vocab>> <<98:3928-3933>> <<dict>> <<VARIABLE>> <<LINK>>
<<self>> <<100:3946-3950>> <<TokenizationTest>> <<VARIABLE>> <<LINK>>
<<tokenizer>> <<100:3967-3976>> <<?>> <<VARIABLE>> <<LINK>>
<<self>> <<102:4006-4010>> <<TokenizationTest>> <<VARIABLE>> <<LINK>>
<<tokenizer>> <<103:4041-4050>> <<?>> <<VARIABLE>> <<LINK>>
<<self>> <<106:4148-4152>> <<TokenizationTest>> <<VARIABLE>> <<LINK>>
<<tokenizer>> <<107:4183-4192>> <<?>> <<VARIABLE>> <<LINK>>
<<self>> <<120:4673-4677>> <<TokenizationTest>> <<VARIABLE>> <<LINK>>
<<self>> <<122:4724-4728>> <<TokenizationTest>> <<VARIABLE>> <<LINK>>
<<self>> <<123:4769-4773>> <<TokenizationTest>> <<VARIABLE>> <<LINK>>
<<self>> <<124:4814-4818>> <<TokenizationTest>> <<VARIABLE>> <<LINK>>
<<self>> <<125:4860-4864>> <<TokenizationTest>> <<VARIABLE>> <<LINK>>
<<self>> <<128:4944-4948>> <<TokenizationTest>> <<VARIABLE>> <<LINK>>
<<self>> <<129:4992-4996>> <<TokenizationTest>> <<VARIABLE>> <<LINK>>
<<self>> <<130:5040-5044>> <<TokenizationTest>> <<VARIABLE>> <<LINK>>
<<self>> <<131:5088-5092>> <<TokenizationTest>> <<VARIABLE>> <<LINK>>
<<self>> <<133:5138-5142>> <<TokenizationTest>> <<VARIABLE>> <<LINK>>
<<self>> <<134:5187-5191>> <<TokenizationTest>> <<VARIABLE>> <<LINK>>
<<vocab_writer>> <<36:1456-1468>> <<file>> <<VARIABLE>> <<LINK>>
<<vocab_tokens>> <<36:1502-1514>> <<list>> <<VARIABLE>> <<LINK>>
<<x>> <<36:1497-1498>> <<str>> <<VARIABLE>> <<LINK>>
<<x>> <<36:1484-1485>> <<str>> <<VARIABLE>> <<LINK>>
<<vocab_writer>> <<38:1546-1558>> <<file>> <<VARIABLE>> <<LINK>>
<<vocab_file>> <<40:1601-1611>> <<str>> <<VARIABLE>> <<LINK>>
<<vocab_file>> <<41:1632-1642>> <<str>> <<VARIABLE>> <<LINK>>
<<tokenizer>> <<43:1664-1673>> <<?>> <<VARIABLE>> <<LINK>>
<<self>> <<44:1718-1722>> <<TokenizationTest>> <<VARIABLE>> <<LINK>>
<<tokens>> <<44:1739-1745>> <<?>> <<VARIABLE>> <<LINK>>
<<self>> <<46:1806-1810>> <<TokenizationTest>> <<VARIABLE>> <<LINK>>
<<tokenizer>> <<47:1841-1850>> <<?>> <<VARIABLE>> <<LINK>>
<<tokens>> <<47:1873-1879>> <<?>> <<VARIABLE>> <<LINK>>
<<self>> <<70:2876-2880>> <<TokenizationTest>> <<VARIABLE>> <<LINK>>
<<tokenizer>> <<71:2911-2920>> <<?>> <<VARIABLE>> <<LINK>>
<<self>> <<110:4298-4302>> <<TokenizationTest>> <<VARIABLE>> <<LINK>>
<<self>> <<111:4345-4349>> <<TokenizationTest>> <<VARIABLE>> <<LINK>>
<<self>> <<112:4393-4397>> <<TokenizationTest>> <<VARIABLE>> <<LINK>>
<<self>> <<113:4441-4445>> <<TokenizationTest>> <<VARIABLE>> <<LINK>>
<<self>> <<114:4489-4493>> <<TokenizationTest>> <<VARIABLE>> <<LINK>>
<<self>> <<116:4543-4547>> <<TokenizationTest>> <<VARIABLE>> <<LINK>>
<<self>> <<117:4591-4595>> <<TokenizationTest>> <<VARIABLE>> <<LINK>>
<<vocab_writer>> <<55:2226-2238>> <<file>> <<VARIABLE>> <<LINK>>
<<vocab_tokens>> <<55:2272-2284>> <<list>> <<VARIABLE>> <<LINK>>
<<x>> <<55:2267-2268>> <<str>> <<VARIABLE>> <<LINK>>
<<x>> <<55:2254-2255>> <<str>> <<VARIABLE>> <<LINK>>
<<vocab_writer>> <<56:2314-2326>> <<file>> <<VARIABLE>> <<LINK>>
<<vocab_file>> <<58:2369-2379>> <<str>> <<VARIABLE>> <<LINK>>
<<vocab_file>> <<59:2412-2422>> <<str>> <<VARIABLE>> <<LINK>>
<<tokenizer>> <<60:2442-2451>> <<?>> <<VARIABLE>> <<LINK>>
<<tokenizer>> <<61:2526-2535>> <<?>> <<VARIABLE>> <<LINK>>
<<tokens>> <<61:2558-2564>> <<?>> <<VARIABLE>> <<LINK>>
<<self>> <<62:2575-2579>> <<TokenizationTest>> <<VARIABLE>> <<LINK>>
<<indices>> <<62:2596-2603>> <<?>> <<VARIABLE>> <<LINK>>
<<_>> <<62:2612-2613>> <<int>> <<VARIABLE>> <<LINK>>
<<tokenizer>> <<64:2649-2658>> <<?>> <<VARIABLE>> <<LINK>>
<<self>> <<65:2725-2729>> <<TokenizationTest>> <<VARIABLE>> <<LINK>>
<<tokenizer>> <<65:2755-2764>> <<?>> <<VARIABLE>> <<LINK>>
<<tokens>> <<65:2788-2794>> <<?>> <<VARIABLE>> <<LINK>>
<<self>> <<77:3115-3119>> <<TokenizationTest>> <<VARIABLE>> <<LINK>>
<<tokenizer>> <<78:3150-3159>> <<?>> <<VARIABLE>> <<LINK>>
<<self>> <<80:3266-3270>> <<TokenizationTest>> <<VARIABLE>> <<LINK>>
<<tokenizer>> <<80:3287-3296>> <<?>> <<VARIABLE>> <<LINK>>
<<self>> <<85:3449-3453>> <<TokenizationTest>> <<VARIABLE>> <<LINK>>
<<tokenizer>> <<86:3484-3493>> <<?>> <<VARIABLE>> <<LINK>>
