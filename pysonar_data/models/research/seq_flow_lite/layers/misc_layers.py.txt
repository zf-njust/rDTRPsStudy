<<AttentionPooling>> <<26:1105-1121>> <<type>> <<CLASS>> <<ANCHOR>>
<<__init__>> <<29:1197-1205>> <<function>> <<CONSTRUCTOR>> <<ANCHOR>>
<<build>> <<44:2089-2094>> <<function>> <<METHOD>> <<ANCHOR>>
<<call>> <<47:2167-2171>> <<function>> <<METHOD>> <<ANCHOR>>
<<TreeInductionLayer>> <<66:3134-3152>> <<type>> <<CLASS>> <<ANCHOR>>
<<__init__>> <<69:3225-3233>> <<function>> <<CONSTRUCTOR>> <<ANCHOR>>
<<call>> <<73:3393-3397>> <<function>> <<METHOD>> <<ANCHOR>>
<<GBSTLayerV2>> <<100:4662-4673>> <<type>> <<CLASS>> <<ANCHOR>>
<<__init__>> <<103:4736-4744>> <<function>> <<CONSTRUCTOR>> <<ANCHOR>>
<<call>> <<148:6544-6548>> <<function>> <<METHOD>> <<ANCHOR>>
<<self>> <<69:3234-3237>> <<TreeInductionLayer>> <<PARAMETER>> <<ANCHOR>>
<<f>> <<69:3237-3238>> <<?>> <<PARAMETER>> <<ANCHOR>>
<<qactivation>> <<70:3261-3272>> <<?>> <<ATTRIBUTE>> <<ANCHOR>>
<<self>> <<148:6549-6552>> <<GBSTLayerV2>> <<PARAMETER>> <<ANCHOR>>
<<inputs>> <<148:6555-6558>> <<?>> <<PARAMETER>> <<ANCHOR>>
<<seq_length>> <<148:6563-6566>> <<?>> <<PARAMETER>> <<ANCHOR>>
<<bsz>> <<162:7047-7050>> <<?>> <<VARIABLE>> <<ANCHOR>>
<<max_seq_len>> <<163:7091-7102>> <<?>> <<VARIABLE>> <<ANCHOR>>
<<num_steps>> <<166:7206-7215>> <<?>> <<VARIABLE>> <<ANCHOR>>
<<inputs>> <<168:7245-7251>> <<?>> <<VARIABLE>> <<ANCHOR>>
<<inputs>> <<169:7281-7287>> <<?>> <<VARIABLE>> <<ANCHOR>>
<<all_block_scores>> <<171:7321-7337>> <<list>> <<VARIABLE>> <<ANCHOR>>
<<all_sequences>> <<172:7348-7361>> <<list>> <<VARIABLE>> <<ANCHOR>>
<<subword_len>> <<173:7376-7387>> <<?>> <<SCOPE>> <<ANCHOR>>
<<block_pos_indices>> <<175:7463-7480>> <<?>> <<VARIABLE>> <<ANCHOR>>
<<block_pos_indices>> <<176:7530-7547>> <<?>> <<VARIABLE>> <<ANCHOR>>
<<block_pos_embeds>> <<177:7598-7614>> <<?>> <<VARIABLE>> <<ANCHOR>>
<<tile_len>> <<178:7670-7678>> <<int>> <<VARIABLE>> <<ANCHOR>>
<<retiled_block_pos_embeds>> <<179:7734-7758>> <<?>> <<VARIABLE>> <<ANCHOR>>
<<inputs>> <<180:7816-7822>> <<?>> <<VARIABLE>> <<ANCHOR>>
<<candidates>> <<184:8054-8064>> <<?>> <<VARIABLE>> <<ANCHOR>>
<<candidates>> <<186:8163-8173>> <<?>> <<VARIABLE>> <<ANCHOR>>
<<block_scores>> <<188:8241-8253>> <<?>> <<VARIABLE>> <<ANCHOR>>
<<retiled_seq>> <<190:8350-8361>> <<?>> <<VARIABLE>> <<ANCHOR>>
<<retiled_block_scores>> <<191:8414-8434>> <<?>> <<VARIABLE>> <<ANCHOR>>
<<retiled_block_scores>> <<196:8682-8702>> <<?>> <<VARIABLE>> <<ANCHOR>>
<<retiled_seq>> <<197:8753-8764>> <<?>> <<VARIABLE>> <<ANCHOR>>
<<retiled_block_scores>> <<199:8819-8839>> <<?>> <<VARIABLE>> <<ANCHOR>>
<<retiled_seq>> <<200:8892-8903>> <<?>> <<VARIABLE>> <<ANCHOR>>
<<retiled_seq>> <<201:8945-8956>> <<?>> <<VARIABLE>> <<ANCHOR>>
<<retiled_block_scores>> <<202:9003-9023>> <<?>> <<VARIABLE>> <<ANCHOR>>
<<block_net>> <<206:9173-9182>> <<?>> <<VARIABLE>> <<ANCHOR>>
<<block_attn_steps>> <<209:9361-9377>> <<list>> <<VARIABLE>> <<ANCHOR>>
<<i>> <<211:9428-9429>> <<int>> <<SCOPE>> <<ANCHOR>>
<<block_i>> <<212:9462-9469>> <<?>> <<VARIABLE>> <<ANCHOR>>
<<block_attn>> <<214:9612-9622>> <<?>> <<VARIABLE>> <<ANCHOR>>
<<block_attn>> <<215:9669-9679>> <<?>> <<VARIABLE>> <<ANCHOR>>
<<block_attn>> <<217:9744-9754>> <<?>> <<VARIABLE>> <<ANCHOR>>
<<block_attn>> <<220:9850-9860>> <<?>> <<VARIABLE>> <<ANCHOR>>
<<block_attn>> <<221:9904-9914>> <<?>> <<VARIABLE>> <<ANCHOR>>
<<block_net_scaled>> <<222:9970-9986>> <<?>> <<VARIABLE>> <<ANCHOR>>
<<block_net_scaled>> <<224:10041-10057>> <<?>> <<VARIABLE>> <<ANCHOR>>
<<candidate_embeds>> <<226:10077-10093>> <<?>> <<VARIABLE>> <<ANCHOR>>
<<dot_product>> <<228:10189-10200>> <<?>> <<VARIABLE>> <<ANCHOR>>
<<output>> <<229:10259-10265>> <<?>> <<VARIABLE>> <<ANCHOR>>
<<output>> <<230:10339-10345>> <<?>> <<VARIABLE>> <<ANCHOR>>
<<output>> <<234:10529-10535>> <<?>> <<VARIABLE>> <<ANCHOR>>
<<output>> <<237:10639-10645>> <<?>> <<VARIABLE>> <<ANCHOR>>
<<self>> <<103:4745-4748>> <<GBSTLayerV2>> <<PARAMETER>> <<ANCHOR>>
<<feature_size>> <<104:4767-4770>> <<?>> <<PARAMETER>> <<ANCHOR>>
<<max_seq_len>> <<105:4797-4800>> <<?>> <<PARAMETER>> <<ANCHOR>>
<<downsample_rate>> <<106:4826-4829>> <<?>> <<PARAMETER>> <<ANCHOR>>
<<max_subword_block_width>> <<107:4861-4864>> <<int>> <<PARAMETER>> <<ANCHOR>>
<<conv_kernel_size>> <<108:4904-4907>> <<int>> <<PARAMETER>> <<ANCHOR>>
<<block_mixing_mode>> <<109:4940-4943>> <<int>> <<PARAMETER>> <<ANCHOR>>
<<add_block_pos_embed>> <<110:4980-4983>> <<?>> <<PARAMETER>> <<ANCHOR>>
<<_block_pos_embed>> <<110:4983-4999>> <<?>> <<PARAMETER>> <<ANCHOR>>
<<feature_size>> <<113:5093-5105>> <<?>> <<ATTRIBUTE>> <<ANCHOR>>
<<max_seq_len>> <<114:5131-5142>> <<?>> <<ATTRIBUTE>> <<ANCHOR>>
<<downsample_rate>> <<115:5167-5182>> <<?>> <<ATTRIBUTE>> <<ANCHOR>>
<<subword_blocks_width>> <<116:5211-5231>> <<list>> <<ATTRIBUTE>> <<ANCHOR>>
<<max_subword_block_width>> <<117:5257-5280>> <<int>> <<ATTRIBUTE>> <<ANCHOR>>
<<block_mixing_mode>> <<118:5324-5341>> <<int>> <<ATTRIBUTE>> <<ANCHOR>>
<<add_block_pos_embed>> <<120:5374-5393>> <<?>> <<ATTRIBUTE>> <<ANCHOR>>
<<block_pos_embedding>> <<122:5462-5481>> <<?>> <<ATTRIBUTE>> <<ANCHOR>>
<<conv_kernel_size>> <<124:5605-5621>> <<int>> <<ATTRIBUTE>> <<ANCHOR>>
<<self>> <<73:3398-3401>> <<TreeInductionLayer>> <<PARAMETER>> <<ANCHOR>>
<<keys>> <<73:3404-3407>> <<?>> <<PARAMETER>> <<ANCHOR>>
<<queries>> <<73:3410-3413>> <<?>> <<PARAMETER>> <<ANCHOR>>
<<sequence_length>> <<73:3419-3422>> <<?>> <<PARAMETER>> <<ANCHOR>>
<<key_dim>> <<74:3442-3449>> <<?>> <<VARIABLE>> <<ANCHOR>>
<<query_dim>> <<75:3488-3497>> <<?>> <<VARIABLE>> <<ANCHOR>>
<<sequence_mask>> <<79:3703-3716>> <<?>> <<VARIABLE>> <<ANCHOR>>
<<sequence_mask>> <<81:3816-3829>> <<?>> <<VARIABLE>> <<ANCHOR>>
<<attn_mask>> <<82:3877-3886>> <<?>> <<VARIABLE>> <<ANCHOR>>
<<attn_logits>> <<84:3956-3967>> <<?>> <<VARIABLE>> <<ANCHOR>>
<<invalid_attn_mask>> <<85:4038-4055>> <<int>> <<VARIABLE>> <<ANCHOR>>
<<keys>> <<90:4285-4289>> <<?>> <<VARIABLE>> <<ANCHOR>>
<<queries>> <<91:4331-4338>> <<?>> <<VARIABLE>> <<ANCHOR>>
<<result>> <<93:4385-4391>> <<?>> <<VARIABLE>> <<ANCHOR>>
<<self>> <<29:1206-1209>> <<AttentionPooling>> <<PARAMETER>> <<ANCHOR>>
<<scalar>> <<29:1212-1215>> <<?>> <<PARAMETER>> <<ANCHOR>>
<<normalize>> <<29:1225-1228>> <<?>> <<PARAMETER>> <<ANCHOR>>
<<malize>> <<29:1228-1234>> <<?>> <<PARAMETER>> <<ANCHOR>>
<<scalar>> <<30:1262-1268>> <<?>> <<ATTRIBUTE>> <<ANCHOR>>
<<attention>> <<39:1849-1858>> <<?>> <<ATTRIBUTE>> <<ANCHOR>>
<<qactivation>> <<41:1959-1970>> <<?>> <<ATTRIBUTE>> <<ANCHOR>>
<<self>> <<47:2172-2175>> <<AttentionPooling>> <<PARAMETER>> <<ANCHOR>>
<<inputs>> <<47:2178-2181>> <<?>> <<PARAMETER>> <<ANCHOR>>
<<mask>> <<47:2186-2189>> <<?>> <<PARAMETER>> <<ANCHOR>>
<<inverse_normalizer>> <<47:2192-2195>> <<?>> <<PARAMETER>> <<ANCHOR>>
<<batch_size>> <<50:2302-2312>> <<?>> <<VARIABLE>> <<ANCHOR>>
<<attn_logits>> <<51:2353-2364>> <<?>> <<VARIABLE>> <<ANCHOR>>
<<invalid_mask>> <<53:2502-2514>> <<int>> <<VARIABLE>> <<ANCHOR>>
<<attn_logits>> <<54:2567-2578>> <<int>> <<VARIABLE>> <<ANCHOR>>
<<attn_logits>> <<55:2620-2631>> <<?>> <<VARIABLE>> <<ANCHOR>>
<<attention>> <<56:2681-2690>> <<?>> <<VARIABLE>> <<ANCHOR>>
<<attention>> <<57:2734-2743>> <<?>> <<VARIABLE>> <<ANCHOR>>
<<inputs>> <<59:2873-2879>> <<?>> <<VARIABLE>> <<ANCHOR>>
<<attention>> <<61:2944-2953>> <<?>> <<VARIABLE>> <<ANCHOR>>
<<pre_logits>> <<62:2995-3005>> <<?>> <<VARIABLE>> <<ANCHOR>>
<<self>> <<44:2095-2098>> <<AttentionPooling>> <<PARAMETER>> <<ANCHOR>>
<<input_shapes>> <<44:2101-2104>> <<?>> <<PARAMETER>> <<ANCHOR>>
<<feature_size>> <<45:2126-2138>> <<?>> <<ATTRIBUTE>> <<ANCHOR>>
<<self>> <<70:3256-3260>> <<TreeInductionLayer>> <<VARIABLE>> <<LINK>>
<<TreeInductionLayer>> <<71:3339-3357>> <<type>> <<VARIABLE>> <<LINK>>
<<self>> <<71:3359-3363>> <<TreeInductionLayer>> <<VARIABLE>> <<LINK>>
<<self>> <<161:7004-7008>> <<GBSTLayerV2>> <<VARIABLE>> <<LINK>>
<<inputs>> <<161:7031-7037>> <<?>> <<VARIABLE>> <<LINK>>
<<self>> <<162:7053-7057>> <<GBSTLayerV2>> <<VARIABLE>> <<LINK>>
<<inputs>> <<162:7078-7084>> <<?>> <<VARIABLE>> <<LINK>>
<<self>> <<163:7105-7109>> <<GBSTLayerV2>> <<VARIABLE>> <<LINK>>
<<self>> <<165:7132-7136>> <<GBSTLayerV2>> <<VARIABLE>> <<LINK>>
<<inputs>> <<166:7227-7233>> <<?>> <<VARIABLE>> <<LINK>>
<<self>> <<168:7254-7258>> <<GBSTLayerV2>> <<VARIABLE>> <<LINK>>
<<inputs>> <<168:7268-7274>> <<?>> <<VARIABLE>> <<LINK>>
<<self>> <<169:7290-7294>> <<GBSTLayerV2>> <<VARIABLE>> <<LINK>>
<<inputs>> <<169:7306-7312>> <<?>> <<VARIABLE>> <<LINK>>
<<self>> <<173:7391-7395>> <<GBSTLayerV2>> <<VARIABLE>> <<LINK>>
<<self>> <<174:7428-7432>> <<GBSTLayerV2>> <<VARIABLE>> <<LINK>>
<<subword_len>> <<175:7492-7503>> <<?>> <<VARIABLE>> <<LINK>>
<<block_pos_indices>> <<176:7561-7578>> <<?>> <<VARIABLE>> <<LINK>>
<<self>> <<177:7617-7621>> <<GBSTLayerV2>> <<VARIABLE>> <<LINK>>
<<block_pos_indices>> <<177:7642-7659>> <<?>> <<VARIABLE>> <<LINK>>
<<max_seq_len>> <<178:7691-7702>> <<?>> <<VARIABLE>> <<LINK>>
<<subword_len>> <<178:7711-7722>> <<?>> <<VARIABLE>> <<LINK>>
<<block_pos_embeds>> <<179:7771-7787>> <<?>> <<VARIABLE>> <<LINK>>
<<tile_len>> <<179:7789-7797>> <<int>> <<VARIABLE>> <<LINK>>
<<inputs>> <<180:7816-7822>> <<?>> <<VARIABLE>> <<LINK>>
<<retiled_block_pos_embeds>> <<180:7826-7850>> <<?>> <<VARIABLE>> <<LINK>>
<<inputs>> <<185:8094-8100>> <<?>> <<VARIABLE>> <<LINK>>
<<subword_len>> <<185:8103-8114>> <<?>> <<VARIABLE>> <<LINK>>
<<subword_len>> <<185:8126-8137>> <<?>> <<VARIABLE>> <<LINK>>
<<self>> <<186:8176-8180>> <<GBSTLayerV2>> <<VARIABLE>> <<LINK>>
<<candidates>> <<186:8220-8230>> <<?>> <<VARIABLE>> <<LINK>>
<<self>> <<188:8256-8260>> <<GBSTLayerV2>> <<VARIABLE>> <<LINK>>
<<candidates>> <<188:8272-8282>> <<?>> <<VARIABLE>> <<LINK>>
<<candidates>> <<190:8374-8384>> <<?>> <<VARIABLE>> <<LINK>>
<<subword_len>> <<190:8386-8397>> <<?>> <<VARIABLE>> <<LINK>>
<<block_scores>> <<191:8447-8459>> <<?>> <<VARIABLE>> <<LINK>>
<<subword_len>> <<191:8461-8472>> <<?>> <<VARIABLE>> <<LINK>>
<<self>> <<195:8606-8610>> <<GBSTLayerV2>> <<VARIABLE>> <<LINK>>
<<retiled_block_scores>> <<196:8705-8725>> <<?>> <<VARIABLE>> <<LINK>>
<<num_steps>> <<196:8730-8739>> <<?>> <<VARIABLE>> <<LINK>>
<<retiled_seq>> <<197:8767-8778>> <<?>> <<VARIABLE>> <<LINK>>
<<num_steps>> <<197:8783-8792>> <<?>> <<VARIABLE>> <<LINK>>
<<retiled_block_scores>> <<199:8842-8862>> <<?>> <<VARIABLE>> <<LINK>>
<<max_seq_len>> <<199:8867-8878>> <<?>> <<VARIABLE>> <<LINK>>
<<retiled_seq>> <<200:8906-8917>> <<?>> <<VARIABLE>> <<LINK>>
<<max_seq_len>> <<200:8922-8933>> <<?>> <<VARIABLE>> <<LINK>>
<<retiled_seq>> <<201:8974-8985>> <<?>> <<VARIABLE>> <<LINK>>
<<retiled_block_scores>> <<202:9041-9061>> <<?>> <<VARIABLE>> <<LINK>>
<<all_sequences>> <<203:9079-9092>> <<list>> <<VARIABLE>> <<LINK>>
<<retiled_seq>> <<203:9100-9111>> <<?>> <<VARIABLE>> <<LINK>>
<<all_block_scores>> <<204:9120-9136>> <<list>> <<VARIABLE>> <<LINK>>
<<retiled_block_scores>> <<204:9144-9164>> <<?>> <<VARIABLE>> <<LINK>>
<<self>> <<206:9185-9189>> <<GBSTLayerV2>> <<VARIABLE>> <<LINK>>
<<all_block_scores>> <<206:9204-9220>> <<list>> <<VARIABLE>> <<LINK>>
<<self>> <<207:9230-9234>> <<GBSTLayerV2>> <<VARIABLE>> <<LINK>>
<<self>> <<208:9285-9289>> <<GBSTLayerV2>> <<VARIABLE>> <<LINK>>
<<self>> <<210:9392-9396>> <<GBSTLayerV2>> <<VARIABLE>> <<LINK>>
<<num_steps>> <<211:9439-9448>> <<?>> <<VARIABLE>> <<LINK>>
<<block_net>> <<212:9483-9492>> <<?>> <<VARIABLE>> <<LINK>>
<<i>> <<212:9496-9497>> <<int>> <<VARIABLE>> <<LINK>>
<<i>> <<212:9498-9499>> <<int>> <<VARIABLE>> <<LINK>>
<<block_attn_steps>> <<213:9532-9548>> <<list>> <<VARIABLE>> <<LINK>>
<<block_i>> <<213:9566-9573>> <<?>> <<VARIABLE>> <<LINK>>
<<block_i>> <<213:9575-9582>> <<?>> <<VARIABLE>> <<LINK>>
<<self>> <<214:9625-9629>> <<GBSTLayerV2>> <<VARIABLE>> <<LINK>>
<<block_attn_steps>> <<214:9642-9658>> <<list>> <<VARIABLE>> <<LINK>>
<<block_attn>> <<215:9693-9703>> <<?>> <<VARIABLE>> <<LINK>>
<<bsz>> <<215:9706-9709>> <<?>> <<VARIABLE>> <<LINK>>
<<self>> <<217:9757-9761>> <<GBSTLayerV2>> <<VARIABLE>> <<LINK>>
<<block_net>> <<218:9799-9808>> <<?>> <<VARIABLE>> <<LINK>>
<<block_net>> <<218:9810-9819>> <<?>> <<VARIABLE>> <<LINK>>
<<block_attn>> <<220:9877-9887>> <<?>> <<VARIABLE>> <<LINK>>
<<self>> <<221:9917-9921>> <<GBSTLayerV2>> <<VARIABLE>> <<LINK>>
<<block_attn>> <<221:9937-9947>> <<?>> <<VARIABLE>> <<LINK>>
<<self>> <<222:9989-9993>> <<GBSTLayerV2>> <<VARIABLE>> <<LINK>>
<<block_attn>> <<222:9999-10009>> <<?>> <<VARIABLE>> <<LINK>>
<<block_net>> <<222:10012-10021>> <<?>> <<VARIABLE>> <<LINK>>
<<block_net>> <<224:10060-10069>> <<?>> <<VARIABLE>> <<LINK>>
<<self>> <<226:10096-10100>> <<GBSTLayerV2>> <<VARIABLE>> <<LINK>>
<<all_sequences>> <<227:10160-10173>> <<list>> <<VARIABLE>> <<LINK>>
<<self>> <<228:10203-10207>> <<GBSTLayerV2>> <<VARIABLE>> <<LINK>>
<<block_net_scaled>> <<228:10217-10233>> <<?>> <<VARIABLE>> <<LINK>>
<<candidate_embeds>> <<228:10236-10252>> <<?>> <<VARIABLE>> <<LINK>>
<<self>> <<229:10268-10272>> <<GBSTLayerV2>> <<VARIABLE>> <<LINK>>
<<dot_product>> <<229:10296-10307>> <<?>> <<VARIABLE>> <<LINK>>
<<output>> <<230:10359-10365>> <<?>> <<VARIABLE>> <<LINK>>
<<bsz>> <<230:10368-10371>> <<?>> <<VARIABLE>> <<LINK>>
<<self>> <<230:10377-10381>> <<GBSTLayerV2>> <<VARIABLE>> <<LINK>>
<<self>> <<233:10455-10459>> <<GBSTLayerV2>> <<VARIABLE>> <<LINK>>
<<output>> <<234:10538-10544>> <<?>> <<VARIABLE>> <<LINK>>
<<num_steps>> <<234:10549-10558>> <<?>> <<VARIABLE>> <<LINK>>
<<self>> <<236:10606-10610>> <<GBSTLayerV2>> <<VARIABLE>> <<LINK>>
<<output>> <<238:10675-10681>> <<?>> <<VARIABLE>> <<LINK>>
<<self>> <<238:10684-10688>> <<GBSTLayerV2>> <<VARIABLE>> <<LINK>>
<<self>> <<239:10728-10732>> <<GBSTLayerV2>> <<VARIABLE>> <<LINK>>
<<output>> <<241:10792-10798>> <<?>> <<VARIABLE>> <<LINK>>
<<GBSTLayerV2>> <<112:5045-5056>> <<type>> <<VARIABLE>> <<LINK>>
<<self>> <<112:5058-5062>> <<GBSTLayerV2>> <<VARIABLE>> <<LINK>>
<<feature_size>> <<113:5108-5120>> <<?>> <<VARIABLE>> <<LINK>>
<<self>> <<113:5088-5092>> <<GBSTLayerV2>> <<VARIABLE>> <<LINK>>
<<max_seq_len>> <<114:5145-5156>> <<?>> <<VARIABLE>> <<LINK>>
<<self>> <<114:5126-5130>> <<GBSTLayerV2>> <<VARIABLE>> <<LINK>>
<<downsample_rate>> <<115:5185-5200>> <<?>> <<VARIABLE>> <<LINK>>
<<self>> <<115:5162-5166>> <<GBSTLayerV2>> <<VARIABLE>> <<LINK>>
<<self>> <<116:5206-5210>> <<GBSTLayerV2>> <<VARIABLE>> <<LINK>>
<<self>> <<117:5287-5291>> <<GBSTLayerV2>> <<VARIABLE>> <<LINK>>
<<subword_blocks_width>> <<117:5292-5312>> <<list>> <<VARIABLE>> <<LINK>>
<<self>> <<117:5252-5256>> <<GBSTLayerV2>> <<VARIABLE>> <<LINK>>
<<block_mixing_mode>> <<118:5344-5361>> <<int>> <<VARIABLE>> <<LINK>>
<<self>> <<118:5319-5323>> <<GBSTLayerV2>> <<VARIABLE>> <<LINK>>
<<add_block_pos_embed>> <<120:5396-5415>> <<?>> <<VARIABLE>> <<LINK>>
<<self>> <<120:5369-5373>> <<GBSTLayerV2>> <<VARIABLE>> <<LINK>>
<<self>> <<121:5424-5428>> <<GBSTLayerV2>> <<VARIABLE>> <<LINK>>
<<add_block_pos_embed>> <<121:5429-5448>> <<?>> <<VARIABLE>> <<LINK>>
<<self>> <<123:5535-5539>> <<GBSTLayerV2>> <<VARIABLE>> <<LINK>>
<<max_subword_block_width>> <<123:5540-5563>> <<int>> <<VARIABLE>> <<LINK>>
<<self>> <<123:5565-5569>> <<GBSTLayerV2>> <<VARIABLE>> <<LINK>>
<<feature_size>> <<123:5570-5582>> <<?>> <<VARIABLE>> <<LINK>>
<<self>> <<122:5457-5461>> <<GBSTLayerV2>> <<VARIABLE>> <<LINK>>
<<conv_kernel_size>> <<124:5624-5640>> <<int>> <<VARIABLE>> <<LINK>>
<<self>> <<124:5600-5604>> <<GBSTLayerV2>> <<VARIABLE>> <<LINK>>
<<feature_size>> <<126:5714-5726>> <<?>> <<VARIABLE>> <<LINK>>
<<conv_kernel_size>> <<127:5743-5759>> <<int>> <<VARIABLE>> <<LINK>>
<<keys>> <<74:3452-3456>> <<?>> <<VARIABLE>> <<LINK>>
<<queries>> <<75:3500-3507>> <<?>> <<VARIABLE>> <<LINK>>
<<key_dim>> <<76:3546-3553>> <<?>> <<VARIABLE>> <<LINK>>
<<query_dim>> <<76:3557-3566>> <<?>> <<VARIABLE>> <<LINK>>
<<self>> <<78:3625-3629>> <<TreeInductionLayer>> <<VARIABLE>> <<LINK>>
<<sequence_length>> <<80:3748-3763>> <<?>> <<VARIABLE>> <<LINK>>
<<keys>> <<80:3781-3785>> <<?>> <<VARIABLE>> <<LINK>>
<<sequence_mask>> <<81:3847-3860>> <<?>> <<VARIABLE>> <<LINK>>
<<sequence_mask>> <<82:3899-3912>> <<?>> <<VARIABLE>> <<LINK>>
<<sequence_mask>> <<82:3914-3927>> <<?>> <<VARIABLE>> <<LINK>>
<<self>> <<84:3970-3974>> <<TreeInductionLayer>> <<VARIABLE>> <<LINK>>
<<qactivation>> <<84:3975-3986>> <<?>> <<VARIABLE>> <<LINK>>
<<keys>> <<84:3997-4001>> <<?>> <<VARIABLE>> <<LINK>>
<<queries>> <<84:4003-4010>> <<?>> <<VARIABLE>> <<LINK>>
<<attn_mask>> <<85:4063-4072>> <<?>> <<VARIABLE>> <<LINK>>
<<self>> <<85:4076-4080>> <<TreeInductionLayer>> <<VARIABLE>> <<LINK>>
<<attn_logits>> <<86:4120-4131>> <<?>> <<VARIABLE>> <<LINK>>
<<attn_mask>> <<86:4134-4143>> <<?>> <<VARIABLE>> <<LINK>>
<<invalid_attn_mask>> <<86:4146-4163>> <<int>> <<VARIABLE>> <<LINK>>
<<self>> <<88:4189-4193>> <<TreeInductionLayer>> <<VARIABLE>> <<LINK>>
<<keys>> <<88:4214-4218>> <<?>> <<VARIABLE>> <<LINK>>
<<self>> <<89:4239-4243>> <<TreeInductionLayer>> <<VARIABLE>> <<LINK>>
<<queries>> <<89:4264-4271>> <<?>> <<VARIABLE>> <<LINK>>
<<keys>> <<90:4303-4307>> <<?>> <<VARIABLE>> <<LINK>>
<<key_dim>> <<90:4314-4321>> <<?>> <<VARIABLE>> <<LINK>>
<<queries>> <<91:4352-4359>> <<?>> <<VARIABLE>> <<LINK>>
<<key_dim>> <<91:4366-4373>> <<?>> <<VARIABLE>> <<LINK>>
<<self>> <<93:4394-4398>> <<TreeInductionLayer>> <<VARIABLE>> <<LINK>>
<<qactivation>> <<93:4399-4410>> <<?>> <<VARIABLE>> <<LINK>>
<<keys>> <<93:4421-4425>> <<?>> <<VARIABLE>> <<LINK>>
<<queries>> <<93:4427-4434>> <<?>> <<VARIABLE>> <<LINK>>
<<result>> <<97:4644-4650>> <<?>> <<VARIABLE>> <<LINK>>
<<scalar>> <<30:1271-1277>> <<?>> <<VARIABLE>> <<LINK>>
<<self>> <<30:1257-1261>> <<AttentionPooling>> <<VARIABLE>> <<LINK>>
<<normalize>> <<40:1928-1937>> <<?>> <<VARIABLE>> <<LINK>>
<<self>> <<39:1844-1848>> <<AttentionPooling>> <<VARIABLE>> <<LINK>>
<<self>> <<41:1954-1958>> <<AttentionPooling>> <<VARIABLE>> <<LINK>>
<<AttentionPooling>> <<42:2037-2053>> <<type>> <<VARIABLE>> <<LINK>>
<<self>> <<42:2055-2059>> <<AttentionPooling>> <<VARIABLE>> <<LINK>>
<<self>> <<48:2218-2222>> <<AttentionPooling>> <<VARIABLE>> <<LINK>>
<<inputs>> <<48:2245-2251>> <<?>> <<VARIABLE>> <<LINK>>
<<self>> <<49:2261-2265>> <<AttentionPooling>> <<VARIABLE>> <<LINK>>
<<mask>> <<49:2288-2292>> <<?>> <<VARIABLE>> <<LINK>>
<<self>> <<50:2315-2319>> <<AttentionPooling>> <<VARIABLE>> <<LINK>>
<<inputs>> <<50:2340-2346>> <<?>> <<VARIABLE>> <<LINK>>
<<self>> <<51:2367-2371>> <<AttentionPooling>> <<VARIABLE>> <<LINK>>
<<attention>> <<51:2372-2381>> <<?>> <<VARIABLE>> <<LINK>>
<<inputs>> <<51:2382-2388>> <<?>> <<VARIABLE>> <<LINK>>
<<mask>> <<51:2390-2394>> <<?>> <<VARIABLE>> <<LINK>>
<<inverse_normalizer>> <<51:2396-2414>> <<?>> <<VARIABLE>> <<LINK>>
<<self>> <<52:2424-2428>> <<AttentionPooling>> <<VARIABLE>> <<LINK>>
<<mask>> <<53:2522-2526>> <<?>> <<VARIABLE>> <<LINK>>
<<self>> <<53:2530-2534>> <<AttentionPooling>> <<VARIABLE>> <<LINK>>
<<attn_logits>> <<54:2581-2592>> <<?>> <<VARIABLE>> <<LINK>>
<<mask>> <<54:2595-2599>> <<?>> <<VARIABLE>> <<LINK>>
<<invalid_mask>> <<54:2602-2614>> <<int>> <<VARIABLE>> <<LINK>>
<<attn_logits>> <<55:2645-2656>> <<{int||?}>> <<VARIABLE>> <<LINK>>
<<batch_size>> <<55:2659-2669>> <<?>> <<VARIABLE>> <<LINK>>
<<attn_logits>> <<56:2707-2718>> <<?>> <<VARIABLE>> <<LINK>>
<<self>> <<57:2746-2750>> <<AttentionPooling>> <<VARIABLE>> <<LINK>>
<<attention>> <<57:2766-2775>> <<?>> <<VARIABLE>> <<LINK>>
<<self>> <<58:2799-2803>> <<AttentionPooling>> <<VARIABLE>> <<LINK>>
<<inputs>> <<59:2893-2899>> <<?>> <<VARIABLE>> <<LINK>>
<<self>> <<59:2906-2910>> <<AttentionPooling>> <<VARIABLE>> <<LINK>>
<<attention>> <<61:2971-2980>> <<?>> <<VARIABLE>> <<LINK>>
<<self>> <<62:3008-3012>> <<AttentionPooling>> <<VARIABLE>> <<LINK>>
<<qactivation>> <<62:3013-3024>> <<?>> <<VARIABLE>> <<LINK>>
<<attention>> <<62:3035-3044>> <<?>> <<VARIABLE>> <<LINK>>
<<inputs>> <<62:3046-3052>> <<?>> <<VARIABLE>> <<LINK>>
<<pre_logits>> <<63:3078-3088>> <<?>> <<VARIABLE>> <<LINK>>
<<batch_size>> <<63:3091-3101>> <<?>> <<VARIABLE>> <<LINK>>
<<self>> <<63:3103-3107>> <<AttentionPooling>> <<VARIABLE>> <<LINK>>
<<input_shapes>> <<45:2141-2153>> <<?>> <<VARIABLE>> <<LINK>>
<<self>> <<45:2121-2125>> <<AttentionPooling>> <<VARIABLE>> <<LINK>>
