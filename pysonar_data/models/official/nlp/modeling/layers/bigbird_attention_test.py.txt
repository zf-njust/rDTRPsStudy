<<official>> <<19:715-723>> <<module>> <<VARIABLE>> <<ANCHOR>>
<<nlp>> <<19:724-727>> <<module>> <<VARIABLE>> <<ANCHOR>>
<<modeling>> <<19:728-736>> <<module>> <<VARIABLE>> <<ANCHOR>>
<<layers>> <<19:737-743>> <<module>> <<VARIABLE>> <<ANCHOR>>
<<BigbirdAttentionTest>> <<22:793-813>> <<type>> <<CLASS>> <<ANCHOR>>
<<test_attention>> <<24:842-856>> <<function>> <<METHOD>> <<ANCHOR>>
<<test_config>> <<48:1659-1670>> <<function>> <<METHOD>> <<ANCHOR>>
<<self>> <<48:1671-1674>> <<BigbirdAttentionTest>> <<PARAMETER>> <<ANCHOR>>
<<num_heads>> <<49:1683-1692>> <<int>> <<VARIABLE>> <<ANCHOR>>
<<key_dim>> <<50:1703-1710>> <<int>> <<VARIABLE>> <<ANCHOR>>
<<block_size>> <<51:1721-1731>> <<int>> <<VARIABLE>> <<ANCHOR>>
<<test_layer>> <<52:1742-1752>> <<?>> <<VARIABLE>> <<ANCHOR>>
<<new_layer>> <<59:1969-1978>> <<?>> <<VARIABLE>> <<ANCHOR>>
<<self>> <<24:857-860>> <<BigbirdAttentionTest>> <<PARAMETER>> <<ANCHOR>>
<<num_heads>> <<25:869-878>> <<int>> <<VARIABLE>> <<ANCHOR>>
<<key_dim>> <<26:889-896>> <<int>> <<VARIABLE>> <<ANCHOR>>
<<seq_length>> <<27:907-917>> <<int>> <<VARIABLE>> <<ANCHOR>>
<<batch_size>> <<28:930-940>> <<int>> <<VARIABLE>> <<ANCHOR>>
<<block_size>> <<29:950-960>> <<int>> <<VARIABLE>> <<ANCHOR>>
<<mask_layer>> <<30:971-981>> <<?>> <<VARIABLE>> <<ANCHOR>>
<<encoder_inputs_mask>> <<31:1035-1054>> <<?>> <<VARIABLE>> <<ANCHOR>>
<<test_layer>> <<32:1113-1123>> <<?>> <<VARIABLE>> <<ANCHOR>>
<<query>> <<38:1304-1309>> <<?>> <<VARIABLE>> <<ANCHOR>>
<<masks>> <<40:1385-1390>> <<?>> <<VARIABLE>> <<ANCHOR>>
<<value>> <<41:1464-1469>> <<?>> <<VARIABLE>> <<ANCHOR>>
<<output>> <<42:1483-1489>> <<?>> <<VARIABLE>> <<ANCHOR>>
<<num_heads>> <<53:1802-1811>> <<int>> <<VARIABLE>> <<LINK>>
<<key_dim>> <<54:1830-1837>> <<int>> <<VARIABLE>> <<LINK>>
<<block_size>> <<55:1864-1874>> <<int>> <<VARIABLE>> <<LINK>>
<<block_size>> <<56:1899-1909>> <<int>> <<VARIABLE>> <<LINK>>
<<test_layer>> <<58:1939-1949>> <<?>> <<VARIABLE>> <<LINK>>
<<test_layer>> <<60:2030-2040>> <<?>> <<VARIABLE>> <<LINK>>
<<self>> <<63:2143-2147>> <<BigbirdAttentionTest>> <<VARIABLE>> <<LINK>>
<<test_layer>> <<63:2163-2173>> <<?>> <<VARIABLE>> <<LINK>>
<<new_layer>> <<63:2188-2197>> <<?>> <<VARIABLE>> <<LINK>>
<<block_size>> <<30:1018-1028>> <<int>> <<VARIABLE>> <<LINK>>
<<batch_size>> <<31:1067-1077>> <<int>> <<VARIABLE>> <<LINK>>
<<seq_length>> <<31:1079-1089>> <<int>> <<VARIABLE>> <<LINK>>
<<num_heads>> <<33:1173-1182>> <<int>> <<VARIABLE>> <<LINK>>
<<key_dim>> <<34:1201-1208>> <<int>> <<VARIABLE>> <<LINK>>
<<block_size>> <<35:1235-1245>> <<int>> <<VARIABLE>> <<LINK>>
<<block_size>> <<36:1270-1280>> <<int>> <<VARIABLE>> <<LINK>>
<<batch_size>> <<39:1346-1356>> <<int>> <<VARIABLE>> <<LINK>>
<<seq_length>> <<39:1358-1368>> <<int>> <<VARIABLE>> <<LINK>>
<<key_dim>> <<39:1370-1377>> <<int>> <<VARIABLE>> <<LINK>>
<<mask_layer>> <<40:1393-1403>> <<?>> <<VARIABLE>> <<LINK>>
<<query>> <<40:1404-1409>> <<?>> <<VARIABLE>> <<LINK>>
<<encoder_inputs_mask>> <<40:1419-1438>> <<?>> <<VARIABLE>> <<LINK>>
<<query>> <<41:1472-1477>> <<?>> <<VARIABLE>> <<LINK>>
<<test_layer>> <<42:1492-1502>> <<?>> <<VARIABLE>> <<LINK>>
<<query>> <<43:1519-1524>> <<?>> <<VARIABLE>> <<LINK>>
<<value>> <<44:1541-1546>> <<?>> <<VARIABLE>> <<LINK>>
<<masks>> <<45:1572-1577>> <<?>> <<VARIABLE>> <<LINK>>
<<self>> <<46:1584-1588>> <<BigbirdAttentionTest>> <<VARIABLE>> <<LINK>>
<<output>> <<46:1601-1607>> <<?>> <<VARIABLE>> <<LINK>>
<<batch_size>> <<46:1616-1626>> <<int>> <<VARIABLE>> <<LINK>>
<<seq_length>> <<46:1628-1638>> <<int>> <<VARIABLE>> <<LINK>>
<<key_dim>> <<46:1640-1647>> <<int>> <<VARIABLE>> <<LINK>>
