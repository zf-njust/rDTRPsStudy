==================
0c55d47cd;NielsRogge;2022-03-22 08:51:13 +0100;Add GLPN (#16199)
* First draft

* Fix logits calculation

* Improve tests

* Add copied from statements

* Fix base_model_prefix

* Improve implementation, upload new models

* Update design

* Fix integration test

* Add model to README and toctree

* Add document image

* Apply suggestions from code review

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Add decoder_hidden_size attribute

* Update design of decoder

* Add DepthEstimatorOutput class

* Rename in_index to head_in_index and add feature extractor tests

* Apply suggestions from code review

* Apply suggestions from code review

* Update pretrained model name and add to doc tests

* Remove test.py script

* Update copied from statements and clean up

Co-authored-by: Niels Rogge <nielsrogge@Nielss-MacBook-Pro.local>
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

README.md
README_ko.md
README_zh-hans.md
README_zh-hant.md
docs/source/_toctree.yml
docs/source/index.mdx
docs/source/model_doc/glpn.mdx
src/transformers/__init__.py
src/transformers/modeling_outputs.py
src/transformers/models/__init__.py
src/transformers/models/auto/configuration_auto.py
src/transformers/models/auto/modeling_auto.py
src/transformers/models/glpn/__init__.py
src/transformers/models/glpn/configuration_glpn.py
src/transformers/models/glpn/convert_glpn_to_pytorch.py
src/transformers/models/glpn/feature_extraction_glpn.py
src/transformers/models/glpn/modeling_glpn.py
src/transformers/utils/dummy_pt_objects.py
src/transformers/utils/dummy_vision_objects.py
tests/glpn/__init__.py
tests/glpn/test_feature_extraction_glpn.py
tests/glpn/test_modeling_glpn.py
utils/check_repo.py
utils/documentation_tests.txt
==================
df32b5d89;Johnny Greco;2022-03-21 18:56:17 -0400;TFLongformer: Add missing type hints and unpack inputs decorator (#16228)
* Add type annotations for TF Longformer

* Update docstring data types to include numpy array

* Implement unpack_inputs decorator

* fixup after decorator updates

* Numpy array -> np.ndarray in docstring

Co-authored-by: Johnny Greco <johnny.greco@radpartners.com>
==

src/transformers/models/longformer/modeling_tf_longformer.py
==================
0aac9ba2d;Thomas Chaigneau;2022-03-21 21:46:31 +0100;Add Flaubert OnnxConfig to Transformers (#16279)
* Add Flaubert to ONNX to make it available for conversion.

* Fixed features for FlauBERT. fixup command remove flaubert to docs list.

Co-authored-by: ChainYo <t.chaigneau.tc@gmail.com>
==

docs/source/serialization.mdx
src/transformers/models/flaubert/__init__.py
src/transformers/models/flaubert/configuration_flaubert.py
src/transformers/onnx/features.py
==================
9fef66833;Joao Gante;2022-03-21 19:55:41 +0000;TF - update (vision_)encoder_decoder past variable (#16260)

==

src/transformers/models/encoder_decoder/modeling_tf_encoder_decoder.py
src/transformers/models/vision_encoder_decoder/modeling_tf_vision_encoder_decoder.py
==================
f9387c948;Gunjan Chhablani;2022-03-22 00:58:23 +0530;Update Makefile Phonies (#16306)

==

Makefile
==================
96cd5bcbb;ivanllt;2022-03-22 03:13:58 +0800;added type hints for blenderbot and blenderbot_small (#16307)

==

src/transformers/models/blenderbot/modeling_blenderbot.py
src/transformers/models/blenderbot/modeling_tf_blenderbot.py
src/transformers/models/blenderbot_small/modeling_blenderbot_small.py
src/transformers/models/blenderbot_small/modeling_tf_blenderbot_small.py
==================
e226a24f8;Anton Lozhkov;2022-03-21 22:33:59 +0400;[xtreme-s] Update Minds14 results (#16241)
* update results

* per-language metrics

* Format the per-language metrics
==

examples/research_projects/xtreme-s/README.md
examples/research_projects/xtreme-s/run_xtreme_s.py
==================
6f1727d83;Gunjan Chhablani;2022-03-21 23:18:07 +0530;Fix Seq2SeqTrainingArguments docs (#16295)
* Indent Seq2Seq Train Args docs

* Add Args keyword to Seq2Seq Train Args docs
==

src/transformers/training_args_seq2seq.py
==================
7643b1caa;Johnny Greco;2022-03-21 13:09:03 -0400;Added type hints to PyTorch Longformer models (#16244)

==

src/transformers/models/longformer/modeling_longformer.py
==================
c77092a5e;Suraj Patil;2022-03-21 18:07:56 +0100;[FlaxGPTJ] Fix bug in rotary embeddings (#16298)

==

src/transformers/models/gptj/modeling_flax_gptj.py
==================
4b2774832;Yih-Dar;2022-03-21 17:38:52 +0100;fix last element in hidden_states for XGLM (#16301)
Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>
==

src/transformers/models/xglm/modeling_flax_xglm.py
==================
5a42bb431;Steven Liu;2022-03-21 09:37:18 -0700;Update troubleshoot with more content (#16243)
* üìù first draft

* üñç apply feedback
==

docs/source/troubleshooting.mdx
==================
fbb454307;NielsRogge;2022-03-21 17:34:10 +0100;[SegFormer] Remove unused attributes (#16285)
* Remove unused attributes

* Add link to blog and add clarification about input size

* Improve readability of the code

Co-authored-by: Niels Rogge <nielsrogge@Nielss-MacBook-Pro.local>
==

docs/source/model_doc/segformer.mdx
src/transformers/models/segformer/configuration_segformer.py
src/transformers/models/segformer/modeling_segformer.py
==================
f0c00d8ca;Suraj Patil;2022-03-21 17:23:40 +0100;Fix Marian conversion script (#16300)

==

src/transformers/models/marian/convert_marian_to_pytorch.py
==================
94be42430;Yi Heng Lim;2022-03-22 00:17:52 +0800;Added type hints for PyTorch T5 model (#16257)
* Added type hints for PyTorch T5 model

* removed a type hint

* ran make style
==

src/transformers/models/t5/modeling_t5.py
==================
250b478a2;Christopher Akiki;2022-03-21 17:11:03 +0100;GPT2 TensorFlow Type Hints (#16261)
* Add typing hints for base model class

* Add typing hints for causal LM model class

* Add typing hints for double heads model class

* Add typing hints for sequence classification model class

* Add typing hints for Main Layer

* Run fixup
==

src/transformers/models/gpt2/modeling_tf_gpt2.py
==================
9ad77affe;Francesco Saverio Zuppichini;2022-03-21 16:59:47 +0100;test (#16294)

==

src/transformers/models/resnet/modeling_resnet.py
src/transformers/models/van/modeling_van.py
==================
d50f62f2d;Robot Jelly;2022-03-21 20:48:01 +0530;added type hints for BART model (#16270)
* added type hints for BART model

* make fixup, adding imports to copied files

* Adding some missing types to cookiecutter

* Adding some missing types to cookiecutter

* Adding some missing types to cookiecutter

Co-authored-by: matt <rocketknight1@gmail.com>
==

src/transformers/models/bart/modeling_bart.py
src/transformers/models/bart/modeling_tf_bart.py
src/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py
src/transformers/models/blenderbot/modeling_blenderbot.py
src/transformers/models/blenderbot/modeling_tf_blenderbot.py
src/transformers/models/blenderbot_small/modeling_blenderbot_small.py
src/transformers/models/blenderbot_small/modeling_tf_blenderbot_small.py
src/transformers/models/hubert/modeling_tf_hubert.py
src/transformers/models/marian/modeling_marian.py
src/transformers/models/marian/modeling_tf_marian.py
src/transformers/models/mbart/modeling_mbart.py
src/transformers/models/mbart/modeling_tf_mbart.py
src/transformers/models/pegasus/modeling_pegasus.py
src/transformers/models/pegasus/modeling_tf_pegasus.py
src/transformers/models/plbart/modeling_plbart.py
src/transformers/models/speech_to_text/modeling_tf_speech_to_text.py
src/transformers/models/wav2vec2/modeling_tf_wav2vec2.py
templates/adding_a_new_example_script/{{cookiecutter.directory_name}}/run_{{cookiecutter.example_shortcut}}.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_{{cookiecutter.lowercase_modelname}}.py
==================
460f36d35;Jack McDonald;2022-03-21 23:04:13 +0800;Add type hints transfoxl (#16267)
* Add type hint for pt transfo_xl model

* Add type hint for tf transfo_xl model
==

src/transformers/models/transfo_xl/modeling_tf_transfo_xl.py
src/transformers/models/transfo_xl/modeling_transfo_xl.py
==================
2afe9cd27;Xia;2022-03-21 22:26:44 +0800;Add argument "cache_dir" for transformers.onnx (#16284)
* Add argument "cache_dir" for transformers.onnx

* Reformate files that can't pass CI.
==

src/transformers/onnx/__main__.py
src/transformers/onnx/features.py
==================
3f0f75e49;Gunjan Chhablani;2022-03-21 19:35:47 +0530;Remove disclaimer from Longformer docs (#16296)

==

docs/source/model_doc/longformer.mdx
==================
c6f7ea194;Mowaninuola Osifeso;2022-03-21 13:04:18 +0000;Add type hints to xlnet (#16214)
* added type hints to xlnet PT

* added type hints to xlnet TF

* added type hints to xlnet TF
==

src/transformers/models/xlnet/modeling_tf_xlnet.py
src/transformers/models/xlnet/modeling_xlnet.py
==================
abf3cc706;PolarisRisingWar;2022-03-21 20:10:24 +0800;Fix a typo (add a coma) (#16291)
As mentioned: https://github.com/huggingface/transformers/issues/16277
==

docs/source/task_summary.mdx
==================
641e5f3f5;Suraj Patil;2022-03-21 13:07:28 +0100;Fix XGLM cross attention (#16290)

==

src/transformers/models/xglm/modeling_xglm.py
==================
f39386807;Aflah;2022-03-21 17:24:54 +0530;Fixed Error Raised Due to Wrongly Accessing Training Sample (#16115)
* Update training.mdx

Fixed Error Raised Due to Wrongly Accessing Training Sample

* Ran make style

* Revert to Old Commit

* Apply suggestions from code review

Co-authored-by: Suraj Patil <surajp815@gmail.com>
==

docs/source/training.mdx
==================
4ecb022eb;Sylvain Gugger;2022-03-21 07:44:03 -0400;Draft a guide with our code quirks for new models (#16237)
* Draft a guide with our code quirks for new models

* Apply suggestions from code review

Co-authored-by: Suraj Patil <surajp815@gmail.com>
Co-authored-by: Joao Gante <joao@huggingface.co>

* Apply suggestions from code review

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

Co-authored-by: Suraj Patil <surajp815@gmail.com>
Co-authored-by: Joao Gante <joao@huggingface.co>
Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
==

docs/source/add_new_model.mdx
==================
8bbd41369;Dinesh Kumar Gnanasekaran;2022-03-21 04:39:45 -0700;removed the 'optional' string (#16266)
Co-authored-by: dinesh-GDK <dinesh.gna111@gmail.com1>
==

src/transformers/models/detr/feature_extraction_detr.py
==================
c36b85658;Omar U. Espejel;2022-03-21 05:37:45 -0600;Framework split for Spanish version of doc quicktour.mdx (#16215)
* Apply framework changes

* Fix italics

* Fix nits

* correct syntax

Co-authored-by: Omar Espejel <espejelomar@Omars-MacBook-Air.local>
==

docs/source_es/quicktour.mdx
==================
c1af180df;Patrick von Platen;2022-03-21 11:33:18 +0100;Add Slack notification support for doc tests (#16253)
* up

* up

* up

* fix

* yeh

* ups

* Empty test commit

* correct quicktour

* correct

* correct

* up

* up

* uP

* uP

* up

* up

* uP

* up

* up

* up

* up

* up

* up

* up

* up

* up

* up

* Update src/transformers/models/van/modeling_van.py

* finish

* apply suggestions

* remove folder

* revert to daily testing
==

.github/workflows/doctests.yml
docs/source/quicktour.mdx
utils/documentation_tests.txt
utils/notification_service_doc_tests.py
==================
319cbbe19;guillaume-be;2022-03-21 09:15:38 +0000;Deberta v2 code simplification (#15732)
* Removed spurious substraction

* Fixed condition checking for attention type

* Fixed sew_d copy of DeBERTa v2 attention

* Removed unused `p2p` attention type from DebertaV2-class models

* Fixed docs style
==

src/transformers/models/deberta_v2/configuration_deberta_v2.py
src/transformers/models/deberta_v2/modeling_deberta_v2.py
src/transformers/models/deberta_v2/modeling_tf_deberta_v2.py
src/transformers/models/sew_d/configuration_sew_d.py
src/transformers/models/sew_d/modeling_sew_d.py
==================
0a5ef036e;Sylvain Gugger;2022-03-21 04:29:04 -0400;Make `add-new-model-like` work in an env without all frameworks (#16239)
* Make add-new-model-like work without all frameworks installed

* A few fixes

* Last default frameworks
==

src/transformers/commands/add_new_model_like.py
==================
f46693647;Yih-Dar;2022-03-19 11:44:17 +0100;Add has_attentions to TFModelTesterMixin as done on PyTorch side (#16259)
Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>
==

tests/test_modeling_tf_common.py
==================
8d7420768;Sylvain Gugger;2022-03-18 17:48:27 -0400;Small fixes to the documentation (#16180)

==

src/transformers/configuration_utils.py
src/transformers/dynamic_module_utils.py
src/transformers/feature_extraction_utils.py
src/transformers/file_utils.py
src/transformers/modeling_flax_utils.py
src/transformers/modeling_tf_utils.py
src/transformers/modeling_utils.py
src/transformers/models/auto/auto_factory.py
src/transformers/models/auto/configuration_auto.py
src/transformers/models/auto/feature_extraction_auto.py
src/transformers/models/auto/tokenization_auto.py
src/transformers/tokenization_utils_base.py
==================
ffc319e7b;Steven Liu;2022-03-18 14:16:16 -0700;Fix links in guides (#16182)
* üñç fix links in guides

* üñç apply feedback
==

docs/source/tasks/asr.mdx
docs/source/tasks/audio_classification.mdx
docs/source/tasks/image_classification.mdx
docs/source/tasks/language_modeling.mdx
docs/source/tasks/multiple_choice.mdx
docs/source/tasks/question_answering.mdx
docs/source/tasks/sequence_classification.mdx
docs/source/tasks/summarization.mdx
docs/source/tasks/token_classification.mdx
docs/source/tasks/translation.mdx
==================
277fc2cc7;Dan Tegzes;2022-03-18 18:57:55 +0100;Update flaubert with tf decorator (#16258)

==

src/transformers/models/flaubert/modeling_tf_flaubert.py
==================
75c666b4a;Yih-Dar;2022-03-18 18:51:24 +0100;Aggressive PT/TF equivalence test on PT side (#16250)
* Aggressive PT/TF equivalence test on PT side

* Ugly fix for `TFTapasForQuestionAnswering`

* apply review suggestions

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>
==

tests/test_modeling_common.py
==================
d481b6414;Yih-Dar;2022-03-18 18:15:36 +0100;Make Flax pt-flax equivalence test more aggressive (#15841)
* Make test_equivalence_pt_to_flax more aggressive

* Make test_equivalence_flax_to_pt more aggressive

* don't use to_tuple

* clean-up

* fix missing test cases + testing on GPU

* fix conversion

* fix `ValueError: assignment destination is read-only`

* Add type checking

* commit to revert later

* Fix

* fix

* fix device

* better naming

* clean-up

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>
==

tests/test_modeling_flax_common.py
==================
c03b6e425;Clara Meister;2022-03-18 17:05:27 +0100;value check for typical sampling (#16165)
* value check for typical sampling

* value check for typical sampling

* change from float to int comparison

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
==

src/transformers/generation_logits_process.py
==================
fdc2e643c;Chan Woo Kim;2022-03-19 01:04:43 +0900;added cbs to notebooks, made copy-paste error fix in generation_utils (#16246)

==

notebooks/README.md
src/transformers/generation_utils.py
==================
b25b92ac4;Suraj Patil;2022-03-18 16:45:39 +0100;update jax version and re-enable some tests (#16254)

==

setup.py
src/transformers/dependency_versions_table.py
tests/speech_encoder_decoder/test_modeling_flax_speech_encoder_decoder.py
==================
5709a2041;Johannes Kolbe;2022-03-18 16:33:24 +0100;Add unpack_inputs decorator for ctrl (#16242)
* add unpack_inputs decorator for ctrl

* replace "past" with "past_key_values"

Co-authored-by: Johannes Kolbe <johannes.kolbe@tech.better.team>
==

src/transformers/models/ctrl/modeling_tf_ctrl.py
==================
ddbc9ae00;Louis Owen;2022-03-18 21:07:02 +0700;Update XLM with TF decorator (#16247)
* update XLM with tf decorator

* move to top decorator

* set unpack_inputs as top decorator

Co-authored-by: Louis Owen <yellow@Louis-Owen.local>
==

src/transformers/models/xlm/modeling_tf_xlm.py
==================
a6271967c;Yih-Dar;2022-03-18 13:30:08 +0100;Override _pad in LEDTokenizer to deal with global_attention_mask (#15940)
* Override _pad in LEDTokenizer

* Override _pad in LEDTokenizerFast

* add Copied from

* calling the super method

* add comment about -1

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>
==

src/transformers/models/led/tokenization_led.py
src/transformers/models/led/tokenization_led_fast.py
==================
cb2b0276b;Zhaofeng Wu;2022-03-18 04:52:55 -0700;Change assertion to warning when passing past_key_value to T5 encoder (#16153)
* Change assertion to warning when passing past_key_value to T5 encoder

* lint
==

src/transformers/models/t5/modeling_t5.py
==================
ecb4662d1;Nicolas Patry;2022-03-18 10:02:12 +0100;Attention mask is important in the case of batching... (#16222)
* Attention mask is important in the case of batching...

* Improve the fix.

* Making the sentence different enough that they exhibit different
predictions.
==

src/transformers/pipelines/base.py
src/transformers/pipelines/token_classification.py
tests/pipelines/test_pipelines_token_classification.py
==================
ec4e421b7;NielsRogge;2022-03-18 09:46:45 +0100;Update expected slices for pillow > 9 (#16117)
* Update expected slices for pillow > 9

* Add expected slices depending on pillow version

* Add different slices depending on pillow version for other models

Co-authored-by: Niels Rogge <nielsrogge@Nielss-MacBook-Pro.local>
==

tests/beit/test_modeling_beit.py
tests/vilt/test_modeling_vilt.py
tests/vision_encoder_decoder/test_modeling_vision_encoder_decoder.py
==================
12d1f0777;Kshitiz Sharma;2022-03-18 02:09:57 +0530;integrations: mlflow: skip start_run() if a run is already active and sanity check on enabling integration (#16131)
* integrations: mlflow: skip start_run() call if a run is already active

* integrations: typo fix

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

src/transformers/integrations.py
==================
47cccb531;Stas Bekman;2022-03-17 13:33:55 -0700;[Deepspeed] non-HF Trainer doc update (#16238)

==

docs/source/main_classes/deepspeed.mdx
==================
8a96b0f10;Patrick von Platen;2022-03-17 20:05:28 +0100;[Generate Docs] Correct docs (#16133)
* [Generate Docs] Correct docs

* Apply suggestions from code review

Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>

Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>
==

docs/source/main_classes/text_generation.mdx
==================
632ff3c39;Suraj Patil;2022-03-17 20:05:14 +0100;[FlaxSpeechEncoderDecoderModel] Skip from_encoder_decoder_pretrained (#16236)
* skip the test

* fix

* fix skip
==

tests/speech_encoder_decoder/test_modeling_flax_speech_encoder_decoder.py
==================
b6e06c845;Boris Dayma;2022-03-17 13:39:16 -0500;fix(flax): generate with logits processor/warper (#16231)

==

src/transformers/generation_flax_utils.py
==================
1c1e377e9;Johannes Kolbe;2022-03-17 19:23:40 +0100;TF - add unpack_inputs decorator for marian (#16226)
* add unpack_inputs decorator

* small fix for attn_mask string

Co-authored-by: Johannes Kolbe <johannes.kolbe@tech.better.team>
==

src/transformers/models/marian/modeling_tf_marian.py
==================
81643edda;ÁΩóÂ¥öÈ™Å(LUO Lingxiao);2022-03-18 01:51:37 +0800;Support PEP 563 for HfArgumentParser (#15795)
* Support PEP 563 for HfArgumentParser

* Fix issues for Python 3.6

* Add test for string literal annotation for HfArgumentParser

* Remove wrong comment

* Fix typo

* Improve code readability

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Use `isinstance` to compare types to pass quality check

* Fix style

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

src/transformers/hf_argparser.py
tests/utils/test_hf_argparser.py
==================
93d3fd864;Suraj Patil;2022-03-17 17:51:43 +0100;remove jax.ops.index (#16220)

==

examples/research_projects/jax-projects/model_parallel/README.md
src/transformers/generation_flax_logits_process.py
src/transformers/models/big_bird/modeling_flax_big_bird.py
src/transformers/models/marian/modeling_flax_marian.py
src/transformers/models/wav2vec2/modeling_flax_wav2vec2.py
==================
8481ecefb;Ula≈ü "Sophylax" Sert;2022-03-17 18:05:38 +0300;Fix Type Hint of Nan/Inf Logging Filter Arg (#16227)

==

src/transformers/training_args.py
==================
5a6b3ccd2;Lysandre Debut;2022-03-17 14:03:07 +0100;Skip equivalence test for TransfoXL (#16224)
* Skip test for TransfoXL

* Single list
==

tests/test_modeling_tf_common.py
==================
abd503d93;Rahul;2022-03-17 18:03:02 +0530;TF - Adding Unpack Decorator For DPR model (#16212)
* Adding Unpack Decorator

* Adding Unpack Decorator-moved it on top
==

src/transformers/models/dpr/modeling_tf_dpr.py
==================
d9b8d1a9f;Francesco Saverio Zuppichini;2022-03-17 13:11:55 +0100;update test (#16219)

==

tests/maskformer/test_modeling_maskformer.py
==================
7e0d04bed;Li-Huai (Allan) Lin;2022-03-17 19:47:01 +0800;Fix readmes (#16217)

==

README_ko.md
README_zh-hans.md
README_zh-hant.md
==================
e1da89ccb;Sylvain Gugger;2022-03-17 07:42:58 -0400;Fix reproducibility in Training for PyTorch 1.11 (#16209)

==

src/transformers/trainer.py
==================
e5101c2e2;Dayyan Smith;2022-03-17 12:21:20 +0100;Fix typo (#16208)

==

src/transformers/data/data_collator.py
==================
25b8f9a85;Yih-Dar;2022-03-17 11:45:50 +0100;Fix FlaxRoFormerClassificationHead activation (#16168)
* fix activation

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>
==

src/transformers/models/roformer/modeling_flax_roformer.py
==================
03c14a515;NielsRogge;2022-03-17 10:53:57 +0100;[Tests] Fix DiT test (#16218)
* Fix device

* Clean up

Co-authored-by: Niels Rogge <nielsrogge@Nielss-MacBook-Pro.local>
==

tests/dit/test_modeling_dit.py
==================
73f0a5d1f;Lysandre Debut;2022-03-17 10:49:24 +0100;Fixes Loss for TransfoXL when using Trainer API v2 (#16140)
* fix(transfo_xl): Fixes TransfoXL support when using Trainer.

* fix(tests): Uses losses_1 and losses_2 pattern with TransfoXL test.

* fix(transfo_xl): Adds requested changes to allow for backward compatibility.

fix(transfo_xl): Adds requested changes to allow for backward compatibility.

fix(transfo_xl): Fixes code styling.

* Backward compatibility

* Update src/transformers/models/transfo_xl/modeling_transfo_xl.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

Co-authored-by: Gustavo de Rosa <gth.rosa@uol.com.br>
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

src/transformers/models/transfo_xl/modeling_transfo_xl.py
tests/transfo_xl/test_modeling_transfo_xl.py
==================
76c74b37c;Francesco Saverio Zuppichini;2022-03-17 10:25:09 +0100;VAN: update modules names (#16201)
* done

* done
==

src/transformers/models/van/convert_van_to_pytorch.py
src/transformers/models/van/modeling_van.py
==================
99e2982f3;Jo√£o Gustavo A. Amorim;2022-03-16 17:27:54 -0300;Add/type annotations/model vision (#16151)
* add types annotations for Beit (PyTorch)

* add types annotations for ViT (PyTorch)

* add types annotations for Deit (PyTorch)

* change Optional[bool] to bool into some places at Beit

* change Optional[bool] to bool into some places at ViT
==

src/transformers/models/beit/modeling_beit.py
src/transformers/models/deit/modeling_deit.py
src/transformers/models/vilt/modeling_vilt.py
src/transformers/models/vit/modeling_vit.py
src/transformers/models/vit_mae/modeling_vit_mae.py
==================
2410d0f8e;Patrick von Platen;2022-03-16 18:49:23 +0100;Fix generation min length (#16206)
* up

* fix min lengths
==

src/transformers/generation_utils.py
tests/generation/test_generation_utils.py
==================
667b823b8;Francesco Saverio Zuppichini;2022-03-16 18:38:25 +0100;Swin support for any input size (#15986)
* padding done

* correctly return one attention per layer

* almost correct, attentions are not flatten one tuple per stage

* tests green

* doc

* conversations

* reshaping hidden_states

* view in the test

* reshape_hidden_states in Encoder and Model

* new outputs with reshaped_hidden_states

* conversations

* doc

* Update docs/source/model_doc/swin.mdx

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>

* Apply suggestions from code review

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>

* conversations

* fix tests

* minor changes

* resolved conversations

* attentions one per stage

* typo

* typos

* typos

* function signature

* CI

* clean up tests

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>
==

docs/source/model_doc/swin.mdx
src/transformers/models/swin/modeling_swin.py
tests/swin/test_modeling_swin.py
==================
204c54d41;Joao Gante;2022-03-16 15:44:33 +0000;TF: add beam search tests (#16202)

==

tests/gpt2/test_modeling_tf_gpt2.py
tests/t5/test_modeling_tf_t5.py
==================
190994573;Suraj Patil;2022-03-16 16:24:01 +0100;Fix loading CLIPVisionConfig and CLIPTextConfig (#16198)
* override from_pretrained

* add tests

* remove docstrings

* fix typo

* Trigger CI
==

src/transformers/models/clip/configuration_clip.py
tests/clip/test_modeling_clip.py
==================
09013efdf;Yih-Dar;2022-03-16 16:19:38 +0100;Update step name (#16189)
Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>
==

.github/workflows/self-scheduled.yml
==================
36f8c4251;Francesco Saverio Zuppichini;2022-03-16 15:59:56 +0100;ResNet: update modules names (#16196)
* updated names

* fit in one line

* typo
==

src/transformers/models/resnet/modeling_resnet.py
==================
5bdf3313e;John Ryan;2022-03-16 14:54:50 +0000;Adding type hints for Distilbert (#16090)
* Distillbert type - squash

* Update src/transformers/models/distilbert/modeling_distilbert.py

Undo cleanup

Co-authored-by: Matt <Rocketknight1@users.noreply.github.com>

* Update src/transformers/models/distilbert/modeling_distilbert.py

Co-authored-by: Matt <Rocketknight1@users.noreply.github.com>

* Update src/transformers/models/distilbert/modeling_distilbert.py

Co-authored-by: Matt <Rocketknight1@users.noreply.github.com>

* Update src/transformers/models/distilbert/modeling_distilbert.py

Co-authored-by: Matt <Rocketknight1@users.noreply.github.com>

* Remove type

Co-authored-by: Matt <Rocketknight1@users.noreply.github.com>
==

src/transformers/models/distilbert/modeling_distilbert.py
==================
0b8b06185;Utku Saglam;2022-03-16 17:03:58 +0300;clearer model variable naming: blenderbot_small (#16194)
Co-authored-by: utku saglam <utkusaglam@utku-MacBook-Pro.local>
==

src/transformers/models/blenderbot_small/modeling_tf_blenderbot_small.py
==================
f06c2c2ba;Johannes Kolbe;2022-03-16 15:01:32 +0100;TF unpack_input decorator for convnext (#16181)
* unpack_input decorator for tf_convnext

* set unpack_input as top decorator

Co-authored-by: Johannes Kolbe <johannes.kolbe@tech.better.team>
==

src/transformers/models/convnext/modeling_tf_convnext.py
==================
d35e0c624;Anton Lozhkov;2022-03-16 17:23:00 +0400;Minor fixes to XTREME-S (#16193)
* Minor fixes

* Fix vocab union

* Update examples/research_projects/xtreme-s/README.md

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* Update README

* unused import

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
==

examples/research_projects/xtreme-s/README.md
examples/research_projects/xtreme-s/run_xtreme_s.py
==================
8cc925a24;Utku Saglam;2022-03-16 15:37:08 +0300;TF clearer model variable naming: blenderbot (#16192)
Co-authored-by: utku saglam <utkusaglam@utku-MacBook-Pro.local>
==

src/transformers/models/blenderbot/modeling_tf_blenderbot.py
==================
0f35cda45;Utku Saglam;2022-03-16 13:37:47 +0300;TF clearer model variable naming: funnel (#16178)
Co-authored-by: utku saglam <utkusaglam@utku-MacBook-Pro.local>
==

src/transformers/models/funnel/modeling_tf_funnel.py
==================
ee27b3d7d;Sanchit Gandhi;2022-03-16 09:08:55 +0000;Replace all deprecated `jax.ops` operations with jnp's `at` (#16078)
* Replace all deprecated `jax.ops` operations with jnp's `at`

* np to jnp scores

* suggested changes
==

examples/research_projects/jax-projects/model_parallel/README.md
src/transformers/generation_flax_logits_process.py
src/transformers/models/bart/modeling_flax_bart.py
src/transformers/models/big_bird/modeling_flax_big_bird.py
src/transformers/models/blenderbot/modeling_flax_blenderbot.py
src/transformers/models/blenderbot_small/modeling_flax_blenderbot_small.py
src/transformers/models/marian/modeling_flax_marian.py
src/transformers/models/mbart/modeling_flax_mbart.py
src/transformers/models/wav2vec2/modeling_flax_wav2vec2.py
src/transformers/models/xglm/modeling_flax_xglm.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_flax_{{cookiecutter.lowercase_modelname}}.py
tests/generation/test_generation_flax_logits_process.py
tests/generation/test_generation_flax_utils.py
==================
c2dc89be6;Patrick von Platen;2022-03-16 01:21:31 +0100;[Xtreme-S] fix some namings (#16183)

==

examples/research_projects/xtreme-s/README.md
examples/research_projects/xtreme-s/requirements.txt
examples/research_projects/xtreme-s/run_xtreme_s.py
==================
99fd3eb4a;Anton Lozhkov;2022-03-16 03:21:06 +0400;Add the XTREME-S fine-tuning example (#15985)
* CTC+classification draft

* CTC+classification draft

* style

* multilingual runs

* Fix race condition during processor.from_reatrained

* Merge covost experiments

* Add README

* Quality

* Switch to .all configs

* Fix typos
==

examples/research_projects/xreme-s/README.md
examples/research_projects/xreme-s/requirements.txt
examples/research_projects/xreme-s/run_xtreme_s.py
==================
db4dd44ae;Sylvain Gugger;2022-03-15 17:00:31 -0400;Trigger doc build

==
==================
ea05d6716;Yih-Dar;2022-03-15 19:06:46 +0100;Fix some Flax models' `hidden_states` (#16167)
* fix the last element in `hidden_states`

* fix missing elements in outputs for FlaxWav2Vec2EncoderLayerStableLayerNormCollection

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>
==

src/transformers/models/blenderbot/modeling_flax_blenderbot.py
src/transformers/models/mbart/modeling_flax_mbart.py
src/transformers/models/pegasus/modeling_flax_pegasus.py
src/transformers/models/wav2vec2/modeling_flax_wav2vec2.py
==================
88f7c564f;Dan Tegzes;2022-03-15 18:59:59 +0100;Added type hints for Reformer (#16175)

==

src/transformers/models/reformer/modeling_reformer.py
==================
16399d619;Jack McDonald;2022-03-16 01:56:57 +0800;Add type annotations for Perceiver (#16174)

==

src/transformers/models/perceiver/modeling_perceiver.py
==================
015de6f08;Kamal Raj;2022-03-15 23:20:30 +0530;TF clearer model variable naming: xlnet (#16150)

==

src/transformers/models/xlnet/modeling_tf_xlnet.py
==================
a23a7c0cd;Thomas Chaigneau;2022-03-15 17:57:45 +0100;Add flaubert types (#16118)
* Add type hints for FlauBERT PyTorch Base model. Others downstream tasks are inherited from XLM RoBERTa.

* Add type hints for FlaubERT Tensorflow models.

* fix output for TFFlaubertWithLMHeadModel
==

src/transformers/models/flaubert/modeling_flaubert.py
src/transformers/models/flaubert/modeling_tf_flaubert.py
==================
366c18f47;Kamal Raj;2022-03-15 22:23:25 +0530;TF clearer model variable naming: Deberta (#16146)

==

src/transformers/models/deberta/modeling_tf_deberta.py
src/transformers/models/deberta_v2/modeling_tf_deberta_v2.py
==================
79465ac52;Kamal Raj;2022-03-15 22:22:56 +0530;TF clearer model variable naming: Tapas (#16145)

==

src/transformers/models/tapas/modeling_tf_tapas.py
==================
a78565b7a;Suraj Patil;2022-03-15 16:26:52 +0100;[MT5Config] add relative_attention_max_distance in config (#16170)

==

src/transformers/models/mt5/configuration_mt5.py
==================
4f4e5ddbc;Sylvain Gugger;2022-03-15 10:13:34 -0400;Framework split (#16030)
* First files

* More files

* Last files

* Style
==

docs/source/autoclass_tutorial.mdx
docs/source/benchmarks.mdx
docs/source/create_a_model.mdx
docs/source/model_doc/tapas.mdx
docs/source/preprocessing.mdx
docs/source/quicktour.mdx
docs/source/run_scripts.mdx
docs/source/serialization.mdx
docs/source/task_summary.mdx
docs/source/tasks/language_modeling.mdx
docs/source/tasks/multiple_choice.mdx
docs/source/tasks/question_answering.mdx
docs/source/tasks/sequence_classification.mdx
docs/source/tasks/summarization.mdx
docs/source/tasks/token_classification.mdx
docs/source/tasks/translation.mdx
utils/style_doc.py
==================
4a353cacb;mowafess;2022-03-15 14:04:32 +0000;added type hints to yoso (#16163)

==

src/transformers/models/yoso/modeling_yoso.py
==================
c1c17bd0b;Joydeep Bhattacharjee;2022-03-15 19:30:18 +0530;update transformer XL with tf decorator (#16166)
* update transformer XL with tf decorator

* code fixup

* remove unused variables
==

src/transformers/models/transfo_xl/modeling_tf_transfo_xl.py
==================
611d3a09b;Minh Chien Vu;2022-03-15 22:47:45 +0900;Change unpacking of TF inputs: layoutlm, mpnet, rag, and roformer (#16112)
Co-authored-by: ChienVM <chien_vm@detomo.co.jp>
==

src/transformers/models/layoutlm/modeling_tf_layoutlm.py
src/transformers/models/mpnet/modeling_tf_mpnet.py
src/transformers/models/rag/modeling_tf_rag.py
src/transformers/models/roformer/modeling_tf_roformer.py
==================
0d7322c1b;Kamal Raj;2022-03-15 19:15:59 +0530;TF clearer model variable naming: pegasus (#16152)

==

src/transformers/models/pegasus/modeling_tf_pegasus.py
==================
cd4c5c906;Matt;2022-03-15 13:19:20 +0000;TF XLA greedy generation (#15786)
* First attempt at TF XLA generation

* Fix comments

* Update XLA greedy generate with direct XLA calls

* Support attention mask, prepare_inputs_for_generation no longer hardcoded for greedy

* Handle position_ids correctly

* make xla generate work for non xla case

* force using xla generate

* refactor

* more fixes

* finish cleaning

* finish

* finish

* clean gpt2 tests

* add gpt2 tests

* correct more cases

* up

* finish

* finish

* more fixes

* flake 8 stuff

* final rag fix

* Update src/transformers/models/rag/modeling_tf_rag.py

* finish t5 as well

* finish

* Update src/transformers/generation_utils.py

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
==

src/transformers/generation_tf_logits_process.py
src/transformers/generation_tf_utils.py
src/transformers/models/gpt2/modeling_tf_gpt2.py
src/transformers/models/rag/modeling_tf_rag.py
src/transformers/models/t5/modeling_tf_t5.py
tests/gpt2/test_modeling_gpt2.py
tests/gpt2/test_modeling_tf_gpt2.py
tests/t5/test_modeling_tf_t5.py
==================
e5bc438cc;Yih-Dar;2022-03-15 13:35:02 +0100;[Fix doc example] Fix 2 PyTorch Vilt docstring examples (#16076)
* fix 2 pytorch vilt docstring examples

* add vilt to doctest list file

* remove device

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>
==

src/transformers/models/vilt/modeling_vilt.py
utils/documentation_tests.txt
==================
bcaf56603;Markus Sagen;2022-03-15 13:17:51 +0100;[Fix doc example] Fix first example for the custom_datasets tutorial (#16087)
* Fix inconsistent example variable naming

- Example code for a sequence classification in Tensorflow had spelling mistakes and incorrect and inconsistent naming
- Changed variable naming to be consistent with the two other TF examples

* Fix incorrect incorrect training examples
==

docs/source/custom_datasets.mdx
==================
8bfd2fb8f;Sylvain Gugger;2022-03-15 08:07:56 -0400;Use templates (#16142)
* Use tempaltes for all doc building jobs

* Add this branch to the doc build

* Switch to main branch
==

.github/workflows/build_dev_documentation.yml
.github/workflows/build_documentation.yml
.github/workflows/build_pr_documentation.yml
.github/workflows/delete_dev_documentation.yml
.github/workflows/delete_doc_comment.yml
==================
daa494475;Daniel Espejel;2022-03-15 06:07:35 -0600;Added spanish translation of quicktour.mdx (#16158)
* Added spanish translation of quicktour.mdx

* Suggestions applied in the revision of the translation

Co-authored-by: Omar U. Espejel <espejelomar@gmail.com>

Co-authored-by: Omar U. Espejel <espejelomar@gmail.com>
==

docs/source_es/quicktour.mdx
==================
57713443d;Ahmed Elnaggar;2022-03-15 13:05:33 +0100;Configurable Relative Position Max. Distance (#16155)
* Configurable Relative Position Max. Distance

* fix missing config

Co-authored-by: ahmed-elnaggar <ahmed.elnaggar@allianz.com>
==

src/transformers/models/t5/configuration_t5.py
src/transformers/models/t5/modeling_flax_t5.py
src/transformers/models/t5/modeling_t5.py
src/transformers/models/t5/modeling_tf_t5.py
==================
cd1ffb40b;marxav;2022-03-15 12:08:53 +0100;typo "conaining" -> "containing" (#16132)

==

src/transformers/configuration_utils.py
==================
5664d2762;Patrick von Platen;2022-03-15 11:07:17 +0100;Shift responsibilities a bit (#16154)

==

.github/ISSUE_TEMPLATE/bug-report.md
==================
5a386fb05;Pavel Belevich;2022-03-15 05:15:03 -0400;Make transformers.utils.fx. _SUPPORTED_MODELS unique (#16015)

==

src/transformers/utils/fx.py
==================
a7aca42fc;NielsRogge;2022-03-15 09:59:48 +0100;Improve Swin for VisionEncoderDecoder (#16070)
* Add Swin2Bart test

* Fix swin tests

Co-authored-by: Niels Rogge <nielsrogge@Nielss-MacBook-Pro.local>
==

src/transformers/models/swin/configuration_swin.py
tests/swin/test_modeling_swin.py
tests/vision_encoder_decoder/test_modeling_vision_encoder_decoder.py
==================
0a057201a;Francesco Saverio Zuppichini;2022-03-15 08:47:12 +0100;Visual Attention Network (VAN) (#16027)
* encoder works

* addded files

* norm in stage

* convertion script

* tests

* fix copies

* make fix-copies

* fixed __init__

* make fix-copies

* fix

* shapiro test needed

* make fix-copie

* minor changes

* make style + quality

* minor refactor conversion script

* rebase + tests

* removed unused variables

* updated doc

* toctree

* CI

* doc

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* resolved conversations

* make fixup

* config passed to modules

* config passed to modules

* Apply suggestions from code review

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>

* conversations

* conversations

* copyrights

* normal test

* tests

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>
==

README.md
README_ko.md
README_zh-hans.md
README_zh-hant.md
docs/source/_toctree.yml
docs/source/index.mdx
docs/source/model_doc/van.mdx
src/transformers/__init__.py
src/transformers/models/__init__.py
src/transformers/models/auto/configuration_auto.py
src/transformers/models/auto/feature_extraction_auto.py
src/transformers/models/auto/modeling_auto.py
src/transformers/models/van/__init__.py
src/transformers/models/van/configuration_van.py
src/transformers/models/van/convert_van_to_pytorch.py
src/transformers/models/van/modeling_van.py
src/transformers/testing_utils.py
src/transformers/utils/dummy_pt_objects.py
tests/van/__init__.py
tests/van/test_modeling_van.py
utils/documentation_tests.txt
==================
8f3ea7a1e;Dan Tegzes;2022-03-14 20:26:12 +0100;Add type hints for GPTNeo PyTorch (#16127)
* Add type hints for SqueezeBert PyTorch

* Add type hints for GPTNeo PyTorch

* style fixes

* chenged List with Tuple
==

src/transformers/models/gpt_neo/modeling_gpt_neo.py
==================
e3008c679;Francesco Saverio Zuppichini;2022-03-14 19:57:55 +0100;[WIP] Resnet (#15770)
* first commit

* ResNet model correctly implemented.

basic modeling + weights conversion is done

removed unused doc

mdx file

doc and conversion script

added feature_extractor to auto

test

minor changes + style + quality

doc

test

Delete process.yml

A left over from my attempt of running circleci locally

* minor changes

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* new test format

* minor changes from conversations

* minor changes from conversations

* make style + quality

* readded the tests

* test + README

* minor changes from conversations

* error in README

* make fix-copies

* removed regression for classification head

* make quality

* fixed loss control flow

* fixed loss control flow

* resolved conversations

* Apply suggestions from code review

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>

* READMEs

* index.mdx

* minor changes

* updated tests and models

* unused import

* outputs

* Update docs/source/model_doc/resnet.mdx

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>

* added embeddings_size

* Apply suggestions from code review

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>

* conversation

* added push to hub

* test

* embedding_size

* make fix-copies

* resolved conversations

* CI

* changed organization

* minor changes

* CI

* minor changes

* conversations

* conversation

* doc

* tests

* removed unused docstring

* conversation

* removed unused outputs

* CI

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>
==

README.md
README_ko.md
README_zh-hans.md
README_zh-hant.md
docs/source/_toctree.yml
docs/source/index.mdx
docs/source/model_doc/resnet.mdx
src/transformers/__init__.py
src/transformers/modeling_outputs.py
src/transformers/models/__init__.py
src/transformers/models/auto/configuration_auto.py
src/transformers/models/auto/feature_extraction_auto.py
src/transformers/models/auto/modeling_auto.py
src/transformers/models/resnet/__init__.py
src/transformers/models/resnet/configuration_resnet.py
src/transformers/models/resnet/convert_resnet_to_pytorch.py
src/transformers/models/resnet/modeling_resnet.py
src/transformers/utils/dummy_pt_objects.py
tests/resnet/__init__.py
tests/resnet/test_modeling_resnet.py
utils/documentation_tests.txt
==================
645823618;Kamal Raj;2022-03-14 23:40:07 +0530;TF Electra - clearer model variable naming (#16143)

==

src/transformers/models/electra/modeling_tf_electra.py
==================
37793259b;Joydeep Bhattacharjee;2022-03-14 23:39:19 +0530;update albert with tf decorator (#16147)

==

src/transformers/models/albert/modeling_tf_albert.py
==================
e109edf16;Sylvain Gugger;2022-03-14 13:26:23 -0400;Use `HF_ENDPOINT` for custom endpoints (#16139)

==

src/transformers/file_utils.py
==================
0dcdfe863;Martin Pan;2022-03-14 10:11:19 -0700;Add type hints for FNet PyTorch (#16123)

==

src/transformers/models/fnet/modeling_fnet.py
==================
f86235ad1;Jacob Dineen;2022-03-14 09:56:04 -0700;Add type annotations for CLIP (torch) (#16059) (#16106)
* clip typhinting #16059

* removed optional type annotations for dataclass in CLIPOutput

* type annotation fixes per Rocket - Clip Torch
==

src/transformers/models/clip/modeling_clip.py
==================
c1000e703;Lysandre Debut;2022-03-14 17:37:20 +0100;Dcoker images runtime -> devel (#16141)
* Runtime -> Devel

* Torch before DeepSpeed
==

docker/transformers-all-latest-gpu/Dockerfile
docker/transformers-doc-builder/Dockerfile
docker/transformers-pytorch-gpu/Dockerfile
docker/transformers-tensorflow-gpu/Dockerfile
==================
10cf1ffdb;Kamal Raj;2022-03-14 21:58:34 +0530;Added missing type hints - ELECTRA TF (#16104)
* Add missing type hints - ELECTRA TF

* bool -> Optional[bool]
==

src/transformers/models/electra/modeling_tf_electra.py
==================
6db869308;Dan Tegzes;2022-03-14 17:21:08 +0100;Add type hints for SqueezeBert PyTorch (#16126)
* Add type hints for SqueezeBert PyTorch

* fixed unused List err

* style fixes
==

src/transformers/models/squeezebert/modeling_squeezebert.py
==================
5493c10ec;Hyeonsoo Lee;2022-03-15 01:14:04 +0900;Add type hints for PoolFormer in Pytorch (#16121)

==

src/transformers/models/poolformer/modeling_poolformer.py
==================
6c2f3ed74;Bhavika Tekwani;2022-03-14 11:55:03 -0400;Add type hints for Luke in PyTorch (#16111)
* Add type hints for LukeModel

* Add type hints for entitypairclassification

* Remove blank space

Co-authored-by: bhavika <bhavika@debian-BULLSEYE-live-builder-AMD64>
==

src/transformers/models/luke/modeling_luke.py
==================
37a9fc49f;Michael Benayoun;2022-03-14 11:47:29 -0400;Choose framework for ONNX export (#16018)
* Can choose framework for ONNX export

* Fix docstring
==

src/transformers/onnx/__main__.py
src/transformers/onnx/features.py
==================
3f8360a7b;Pepijn Boers;2022-03-14 16:39:59 +0100;Add type hints for TFDistilBert (#16107)
* Add type hints for TFDistilBert

* Update src/transformers/models/distilbert/modeling_tf_distilbert.py

Co-authored-by: Matt <Rocketknight1@users.noreply.github.com>
==

src/transformers/models/distilbert/modeling_tf_distilbert.py
==================
97e32b785;Bhavika Tekwani;2022-03-14 11:26:40 -0400;Improve model variable naming - CLIP [TF]  (#16128)
* First pass

* Fixup

* Fix broken tests

* Make unpack_inputs the first decorator
==

src/transformers/models/clip/modeling_tf_clip.py
==================
d02bd4f33;Bhavika Tekwani;2022-03-14 11:25:45 -0400;Better input variable naming for OpenAI (TF) (#16129)
* Replace input_processing

* move unpack_inputs
==

src/transformers/models/openai/modeling_tf_openai.py
==================
c8c8c114a;Yih-Dar;2022-03-14 16:19:18 +0100;[Fix doc example] Fix checkpoint name in docstring example in Speech2Text2 (#16083)
* Fix checkpoint name in docstring example

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>
Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
==

src/transformers/models/speech_to_text_2/modeling_speech_to_text_2.py
==================
72ae06b90;Kamal Raj;2022-03-14 20:42:22 +0530;Added missing type hints - V1 and V2 (#16105)

==

src/transformers/models/deberta/modeling_deberta.py
src/transformers/models/deberta_v2/modeling_deberta_v2.py
==================
1d43933fb;Kamal Raj;2022-03-14 20:23:57 +0530;Added missing type hints (#16103)

==

src/transformers/models/electra/modeling_electra.py
==================
efd6e9a82;Yhary Arias;2022-03-14 09:12:38 -0500;Spanish translation of the file training.mdx (#16047)
* Spanish translation of the file training.mdx

* Settings - Spanish translation of the file training.mdx

* Latest changes to the Spanish translation of the training.mdx file

* Delete Hugging.mdx

* Last changes to the training fil Espanish version

* Latest modifications

* Latest changes, document ready for PR

* Nits

Co-authored-by: Yhary Arias <yharystefa@gmail.com>
Co-authored-by: Omar U. Espejel <espejelomar@gmail.com>
==

docs/source_es/training.mdx
==================
9fd584e54;NielsRogge;2022-03-14 15:05:14 +0100;Add copied from statements and fix prefix (#16119)
Co-authored-by: Niels Rogge <nielsrogge@Nielss-MacBook-Pro.local>
==

src/transformers/models/vit_mae/modeling_vit_mae.py
==================
f284aa320;Merve Noyan;2022-03-14 16:37:07 +0300;steps strategy fix for PushtoHubCallback (#16138)

==

src/transformers/keras_callbacks.py
==================
e3645fd28;Minh Chien Vu;2022-03-14 22:15:08 +0900;Change unpacking of TF mobilebert inputs to use decorator (#16110)
* Change unpacking of TF mobilebert inputs to use decorator

* Move unpack_inputs as the top decorator

* make fixup

Co-authored-by: ChienVM <chien_vm@detomo.co.jp>
==

src/transformers/models/mobilebert/modeling_tf_mobilebert.py
==================
5dbf36bd4;Yih-Dar;2022-03-14 14:02:41 +0100;Fix ProphetNetTokenizer (#16082)
Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>
==

src/transformers/models/prophetnet/tokenization_prophetnet.py
==================
923c35b5c;Yih-Dar;2022-03-14 13:31:32 +0100;Make TF pt-tf equivalence test more aggressive (#15839)
* Make TF pt-tf equivalence test more aggressive

* Fix for TFConvNextModelTest and TFTransfoXLModelTest

* fix kwargs for outputs

* clean-up

* Add docstring for check_outputs()

* remove: need to rename encoder-decoder

* clean-up

* send PyTorch things to the correct device

* Add back the accidentally removed test case in test_pt_tf_model_equivalence()

* Fix: change to tuple before calling check_outputs()

* Fix: tfo could be a list

* use to_tuple()

* allow tfo only to be tuple or tensor

* allow tfo to be list or tuple for now + style change

* minor fix

* remove np.copy and update comments

* tfo -> tf_output, same for pt

* Add more detailed comment

* remove the incorrect comment

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>
==

tests/test_modeling_tf_common.py
==================
9e9f6b8a4;tiedemann;2022-03-14 13:15:38 +0200;Update convert_marian_to_pytorch.py (#16124)
Configuration `tied-embeddings-all` implies `tied-embeddings-src`
==

src/transformers/models/marian/convert_marian_to_pytorch.py
==================
2de99e6c4;Sanchit Gandhi;2022-03-14 10:12:29 +0100;Fix Loading of Flax(Speech)EncoderDecoderModel kwargs from PreTrained Encoder-Decoder Checkpoints (#16056)
* Fix Loading of Flax(Speech)EncoderDecoderModel kwargs from PreTrained Encoder-Decoder Checkpoints

* change wording
==

src/transformers/models/encoder_decoder/modeling_flax_encoder_decoder.py
src/transformers/models/speech_encoder_decoder/modeling_flax_speech_encoder_decoder.py
tests/encoder_decoder/test_modeling_flax_encoder_decoder.py
tests/speech_encoder_decoder/test_modeling_flax_speech_encoder_decoder.py
==================
802984ad4;Omar Sanseviero;2022-03-14 08:50:36 +0100;Fix and document Zero Shot Image Classification (#16079)

==

docs/source/main_classes/pipelines.mdx
src/transformers/pipelines/__init__.py
src/transformers/pipelines/zero_shot_image_classification.py
==================
6e1e88fd3;lewtun;2022-03-14 08:40:42 +0100;Add TFCamembertForCausalLM and ONNX integration test (#16073)
* Make Camembert great again!

* Add Camembert to TensorFlow ONNX tests
==

docs/source/model_doc/camembert.mdx
src/transformers/__init__.py
src/transformers/models/auto/modeling_tf_auto.py
src/transformers/models/camembert/__init__.py
src/transformers/models/camembert/modeling_tf_camembert.py
src/transformers/utils/dummy_tf_objects.py
tests/onnx/test_onnx_v2.py
==================
20ab1582c;Thomas Chaigneau;2022-03-13 19:54:01 +0100;Add missing type hints for all flavors of LayoutLMv2 PyTorch models. (#16089)
* Add missing type hints for all flavors of LayoutLMv2 PyTorch models.

* Fixed return types and added type hints for LayoutLM.

* Fix removed arguments which breaks tests.
==

src/transformers/models/layoutlm/modeling_layoutlm.py
src/transformers/models/layoutlmv2/modeling_layoutlmv2.py
==================
65cf33e7e;James Barry;2022-03-12 19:28:48 +0000;Add type hints to XLM model (PyTorch) (#16108)

==

src/transformers/models/xlm/modeling_xlm.py
==================
841620684;Jo√£o Gustavo A. Amorim;2022-03-12 12:05:13 -0300;apply unpack_input decorator to ViT model (#16102)

==

src/transformers/models/vit/modeling_tf_vit.py
==================
62b05b691;p-mishra1;2022-03-12 18:07:09 +0530;Add type annotations for segformer classes (#16099)

==

src/transformers/models/segformer/modeling_segformer.py
==================
9042dfe35;Abdelrhman-Hosny;2022-03-12 14:30:43 +0200;add unpack_inputs decorator to mbart (#16097)

==

src/transformers/models/mbart/modeling_tf_mbart.py
==================
3e9d0f7f5;Omar Sanseviero;2022-03-12 13:06:55 +0100;Change unpacking of TF Bart inputs (#16094)

==

src/transformers/models/bart/modeling_tf_bart.py
==================
580dd87c5;Stas Bekman;2022-03-11 17:53:53 -0800;[Deepspeed] add support for bf16 mode (#14569)
* [WIP] add support for bf16 mode

* prep for bf16

* prep for bf16

* fix; zero2/bf16 is ok

* check bf16 is available

* test fixes

* enable zero3_bf16

* config files

* docs

* split stage_dtype; merge back to non-dtype-specific config file

* fix doc

* cleanup

* cleanup

* bfloat16 => bf16 to match the PR changes

* s/zero_gather_fp16_weights_on_model_save/zero_gather_16bit_weights_on_model_save/; s/save_fp16_model/save_16bit_model/

* test fixes/skipping

* move

* fix

* Update docs/source/main_classes/deepspeed.mdx

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* backticks

* cleanup

* cleanup

* cleanup

* new version

* add note about grad accum in bf16

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

docs/source/main_classes/deepspeed.mdx
examples/research_projects/wav2vec2/ds_config_wav2vec2_zero3.json
setup.py
src/transformers/deepspeed.py
src/transformers/dependency_versions_table.py
src/transformers/trainer.py
tests/deepspeed/ds_config_zero2.json
tests/deepspeed/ds_config_zero3.json
tests/deepspeed/test_deepspeed.py
tests/deepspeed/test_model_zoo.py
==================
c1f209dad;Jeff Rasley;2022-03-11 15:13:11 -0800;[ZeRO] Fixes issue with embedding resize (#16093)
* gather z3 params for new_lm_head

* Update src/transformers/modeling_utils.py

Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>

Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>
==

src/transformers/modeling_utils.py
==================
ae2dd42be;Steven Liu;2022-03-11 14:43:49 -0800;Audio/vision task guides (#15808)
* üìù first draft of audio/vision guides

* ‚ú® make fixup

* üñç fix typo

* üñç close parentheses

* üñç apply feedback

* üñç apply feedback, make fixup

* üñç more fixup for perceiver

* üñç apply feedback

* ‚ú® make fixup

* üñç fix data collator
==

docs/source/_toctree.yml
docs/source/tasks/asr.mdx
docs/source/tasks/audio_classification.mdx
docs/source/tasks/image_classification.mdx
==================
cb5e50c8c;Yih-Dar;2022-03-11 21:21:31 +0100;[Fix doc example] FSMT (#16085)
* fix

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>
==

src/transformers/models/fsmt/modeling_fsmt.py
==================
eaed6897d;Thomas Chaigneau;2022-03-11 20:40:50 +0100;Add missing type hints for all flavors of RoBERTa PyTorch models. (#16086)
* Add missing type hints for all flavors of RoBERTa PyTorch models.

* Fixed type hints for all classes and fixed return types.
==

src/transformers/models/roberta/modeling_roberta.py
==================
a01fe4cd3;Lysandre Debut;2022-03-11 20:35:48 +0100;Rebuild deepspeed (#16081)
* Rebuild deepspeed

* Apply suggestions from code review

Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>

Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>
==

.github/workflows/self-scheduled.yml
==================
7f3d4440d;Jo√£o Gustavo A. Amorim;2022-03-11 16:16:14 -0300;add type annotations for ImageGPT (#16088)

==

src/transformers/models/imagegpt/modeling_imagegpt.py
==================
5b4c97d09;Steven Liu;2022-03-11 11:05:44 -0800;Update troubleshoot guide (#16001)
* üìù first draft

* üñç apply feedback

* üñç apply feedback
==

docs/source/_toctree.yml
docs/source/troubleshooting.mdx
==================
9442b3ce3;Kevin Bondzio;2022-03-11 19:36:44 +0100;Add soft length regulation for sequence generation (#15245)
* add possibility to softly regulate length when using sampling method in model.generate() function

* fix test config, fix formatting

* fix rag integration, fix docstyling

* fix wrong docstring

* change param to tuple, add test

* fix old param in rag_model, remove unused import

* change test according to new param

* fix formatting

* fix test case

* fix doc style

* move start_length calculation to Logitprocessor

* add possibility to softly regulate length when using sampling method in model.generate() function

* fix rag integration, fix docstyling

* fix test config, fix formatting

* change param to tuple, add test

* fix old param in rag_model, remove unused import

* add possibility to softly regulate length when using sampling method in model.generate() function

* change param to tuple, add test

* fix old param in rag_model, remove unused import

* remove unused import

* fix small errors

* fix test

* add possibility to softly regulate length when using sampling method in model.generate() function

* fix test config, fix formatting

* fix rag integration, fix docstyling

* change param to tuple, add test

* fix old param in rag_model, remove unused import

* change test according to new param

* fix test case

* move start_length calculation to Logitprocessor

* add possibility to softly regulate length when using sampling method in model.generate() function

* fix rag integration, fix docstyling

* fix test config, fix formatting

* change param to tuple, add test

* fix old param in rag_model, remove unused import

* add possibility to softly regulate length when using sampling method in model.generate() function

* fix test config, fix formatting

* fix rag integration, fix docstyling

* add possibility to softly regulate length when using sampling method in model.generate() function

* fix rag integration, fix docstyling

* change param to tuple, add test

* fix old param in rag_model, remove unused import

* fix small errors

* Update src/transformers/generation_utils.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/generation_utils.py

* Update src/transformers/generation_utils.py

* fix docstring, add type ind model rag

* fix docstrings

* introduce seq_length variable for cleaner code

* fix black formatting

* add input_ids_seq_length to modeling_rag

* add input_ids_seq_length to test

* retrigger checks

* retrigger checks

Co-authored-by: Kevin Bondzio <kev@AIM-LAP-02.local>
Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
Co-authored-by: Kevin Bondzio <kev@AIM-LAP-02.fritz.box>
==

src/transformers/configuration_utils.py
src/transformers/generation_logits_process.py
src/transformers/generation_utils.py
src/transformers/models/rag/modeling_rag.py
tests/generation/test_generation_logits_process.py
tests/test_configuration_common.py
==================
322c8533d;Patrick von Platen;2022-03-11 18:04:17 +0100;Run daily test without time-out at least once (#16077)

==

.github/workflows/doctests.yml
==================
7e00247fa;feifang24;2022-03-11 09:00:11 -0800;check for key 'torch.dtype' in nested dicts in config (#16065)

==

src/transformers/configuration_utils.py
==================
5d2fed2e8;Matt;2022-03-11 16:13:47 +0000;Adding type hints for TFRoBERTa (#16057)
* Adding type annotations for TFRoBERTa

* Add type hints to TFRobertaModel too
==

src/transformers/models/roberta/modeling_tf_roberta.py
==================
bb69d154c;Matt;2022-03-11 16:13:29 +0000;Add type annotations for BERT and copies (#16074)
* Add type annotations for BERT and copies

* make fixup
==

src/transformers/models/bert/modeling_bert.py
src/transformers/models/data2vec/modeling_data2vec_text.py
src/transformers/models/mobilebert/modeling_mobilebert.py
src/transformers/models/roberta/modeling_roberta.py
src/transformers/models/xlm_roberta_xl/modeling_xlm_roberta_xl.py
==================
f7708e1be;Sylvain Gugger;2022-03-11 10:09:15 -0500;Force default brnahc name via the config

==

docs/source/_config.py
==================
ecf989ca7;Sylvain Gugger;2022-03-11 09:20:05 -0500;Trigger doc build

==
==================
0868fdef8;Lysandre Debut;2022-03-11 15:03:27 +0100;Fix torch-scatter version (#16072)

==

.circleci/config.yml
==================
5b369dc5d;Funtowicz Morgan;2022-03-11 14:27:59 +0100;Remove assertion over possible activation functions in DistilBERT (#16066)
* Remove assertion over possible activation functions

* Same for TF and Flax
==

src/transformers/models/distilbert/modeling_distilbert.py
src/transformers/models/distilbert/modeling_flax_distilbert.py
src/transformers/models/distilbert/modeling_tf_distilbert.py
==================
f5741bcd0;Sylvain Gugger;2022-03-11 07:58:02 -0500;Move QDQBert in just PyTorch block (#16062)

==

src/transformers/__init__.py
src/transformers/utils/dummy_pt_objects.py
src/transformers/utils/dummy_pytorch_quantization_and_torch_objects.py
==================
b6bdb943b;Yih-Dar;2022-03-11 11:22:36 +0100;Fix a TF test name (LayoutLMModelTest) (#16061)
* fix name

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>
==

tests/layoutlm/test_modeling_tf_layoutlm.py
==================
96ac7549c;David S. Batista;2022-03-10 22:21:56 +0100;updating fine-tune classifier documentation (#16063)

==

docs/source/tasks/token_classification.mdx
==================
6b0932836;lewtun;2022-03-10 20:19:45 +0100;Fix duplicate arguments passed to dummy inputs in ONNX export (#16045)
* Fix duplicate arguments passed to dummy inputs in ONNX export

* Fix M2M100 ONNX config

* Ensure we check PreTrained model only if torch is available

* Remove TensorFlow tests for models without PyTorch parity
==

src/transformers/models/m2m_100/configuration_m2m_100.py
src/transformers/onnx/convert.py
tests/onnx/test_onnx_v2.py
==================
ba21001f4;Suraj Patil;2022-03-10 19:41:56 +0100;support new marian models (#15831)
* support not sharing embeddings

* update modeling

* update tokenizer

* fix conversion script

* always use self.shared

* boom boom

* begin tests

* update tests

* fix resize_decoder_token_embeddings

* address Patrick's comments

* style

* update conversion script

* fix conversion script

* fix tokenizer

* better name target vocab

* add integration test for tokenizer with two vocabs

* style

* address Patrick's comments

* add integration test for model
==

src/transformers/models/marian/configuration_marian.py
src/transformers/models/marian/convert_marian_to_pytorch.py
src/transformers/models/marian/modeling_marian.py
src/transformers/models/marian/tokenization_marian.py
tests/marian/test_modeling_marian.py
tests/marian/test_tokenization_marian.py
==================
e66743e6c;Lysandre Debut;2022-03-10 15:01:05 +0100;DeBERTa/DeBERTa-v2/SEW Support for torch 1.11 (#16043)
* Support for torch 1.11

* Address Sylvain's comment
==

src/transformers/models/deberta/modeling_deberta.py
src/transformers/models/deberta_v2/modeling_deberta_v2.py
src/transformers/models/sew_d/modeling_sew_d.py
src/transformers/pytorch_utils.py
==================
741e49305;Sanchit Gandhi;2022-03-10 14:58:05 +0100;Fix Bug in Flax Seq2Seq Models (#16021)
* Fix Bug in Flax Seq2Seq Models

* incorporate suggested changes
==

src/transformers/models/encoder_decoder/modeling_flax_encoder_decoder.py
src/transformers/models/speech_encoder_decoder/modeling_flax_speech_encoder_decoder.py
==================
b7018abf3;Joao Gante;2022-03-10 13:31:35 +0000;TF: Unpack model inputs through a decorator  (#15907)
* MVP

* apply decorator to TFBertModel

* finish updating bert

* update rembert (copy-linked to bert)

* update roberta (copy-linked to bert); Fix args

* Now working for non-text modalities
==

src/transformers/modeling_tf_utils.py
src/transformers/models/bert/modeling_tf_bert.py
src/transformers/models/rembert/modeling_tf_rembert.py
src/transformers/models/roberta/modeling_tf_roberta.py
src/transformers/models/speech_to_text/modeling_tf_speech_to_text.py
==================
19597998f;Sylvain Gugger;2022-03-10 07:44:51 -0500;Don't compute metrics in LM examples on TPU (#16029)

==

examples/pytorch/language-modeling/run_clm.py
examples/pytorch/language-modeling/run_mlm.py
==================
10591399d;Sylvain Gugger;2022-03-10 07:44:29 -0500;Build the doc in a seperate folder then move it (#16020)
* Build the doc in a seperate folder then move it

* Allow job

* Is this it?

* Dislike comments?

* Copy instead of move

* Removing version built

* Typos

* No variable

* Take _versions.yml into account

* Finish main job and add dev job

* Forgot the run

* Fix syntax error

* Execute builder from the repo

* Typo
==

.github/workflows/build_dev_documentation.yml
.github/workflows/build_documentation.yml
==================
2f463effb;Yih-Dar;2022-03-10 12:23:46 +0100;Fix TFDebertaV2ConvLayer in TFDebertaV2Model (#16031)
* fix

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>
==

src/transformers/models/deberta_v2/modeling_tf_deberta_v2.py
==================
1da84ae02;Sanchit Gandhi;2022-03-10 12:09:29 +0100;Fix Bug in Flax-Speech-Encoder-Decoder Test (#16041)
* Fix Bug in Flax-Speech-Encoder-Decoder Test

* change thresholds for CPU precision
==

tests/speech_encoder_decoder/test_modeling_flax_speech_encoder_decoder.py
==================
b2a1c994c;Suraj Patil;2022-03-10 12:09:05 +0100;[README] fix url for Preprocessing tutorial (#16042)

==

README.md
==================
8d83ebdf1;NielsRogge;2022-03-10 12:00:30 +0100;[Tests] Add attentions_option to ModelTesterMixin (#15909)
* Add attentions_option to common tester

* Fix tests, apply suggestion

* Apply suggestion from code review

Co-authored-by: Niels Rogge <nielsrogge@Nielss-MacBook-Pro.local>
==

tests/convnext/test_modeling_convnext.py
tests/poolformer/test_modeling_poolformer.py
tests/test_modeling_common.py
==================
6ce11c2c0;Patrick von Platen;2022-03-10 11:54:45 +0100;[Docs] Improve PyTorch, Flax generate API (#15988)
* Move generate docs

* up

* Update docs/source/_toctree.yml

* correct

* correct some stuff

* correct tests

* more fixes

* finish generate

* add to doc stest

* finish

* finalize

* add warning to generate method
==

docs/source/_toctree.yml
docs/source/main_classes/model.mdx
docs/source/main_classes/text_generation.mdx
src/transformers/generation_flax_utils.py
src/transformers/generation_utils.py
utils/documentation_tests.txt
==================
0951d3178;Andr√© Storhaug;2022-03-10 11:35:26 +0100;Fix dependency error message in ServeCommand (#16033)
"uvicorn" is misspelled as "unicorn".
==

src/transformers/commands/serving.py
==================
0835119bf;NielsRogge;2022-03-10 11:34:44 +0100;Add Document Image Transformer (DiT) (#15984)
* Add conversion script

* Improve script

* Fix bug

* Add option to push to hub

* Add support for classification models

* Update model name

* Upload feature extractor files first

* Remove hash checking

* Fix config

* Add id2label

* Add import

* Fix id2label file name

* Fix expected shape

* Add model to README

* Improve docs

* Add integration test and fix CI

* Fix code style

* Add missing init

* Add model to SPECIAL_MODULE_TO_TEST_MAP

Co-authored-by: Niels Rogge <nielsrogge@Nielss-MacBook-Pro.local>
==

README.md
README_ko.md
README_zh-hans.md
README_zh-hant.md
docs/source/_toctree.yml
docs/source/index.mdx
docs/source/model_doc/dit.mdx
src/transformers/__init__.py
src/transformers/models/__init__.py
src/transformers/models/auto/configuration_auto.py
src/transformers/models/dit/__init__.py
src/transformers/models/dit/convert_dit_unilm_to_pytorch.py
tests/dit/__init__.py
tests/dit/test_modeling_dit.py
utils/tests_fetcher.py
==================
6c9010ef6;Sanchit Gandhi;2022-03-10 10:20:37 +0100;Update README.md

==

examples/research_projects/jax-projects/README.md
==================
fde901877;Sanchit Gandhi;2022-03-10 09:59:19 +0100;Freeze Feature Encoder in FlaxSpeechEncoderDecoder (#15997)
* Freeze Feature Encoder in FlaxSpeechEncoderDecoder

* add backprop test
==

src/transformers/models/speech_encoder_decoder/modeling_flax_speech_encoder_decoder.py
tests/speech_encoder_decoder/test_modeling_flax_speech_encoder_decoder.py
==================
65f9653ed;Pavel Belevich;2022-03-09 17:27:15 -0500;Fix warning message in ElectraForCausalLM (#16023)

==

src/transformers/models/electra/modeling_electra.py
==================
a69e18507;Suraj Patil;2022-03-09 20:30:38 +0100;add doctests for bart like seq2seq models (#15987)
* boom boom

* enable doctest for few seq2seq models

* add seq2seq models in documentation_tests.txt

* fix docstring blenderbot

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* fix seq classif doc sample

* don't check loss for seq classif examples

* +IGNORE_OUTPUT => +IGNORE_RESULT

* fix _SEQ_CLASS_EXPECTED_OUTPUT_SHAPE

* fix some docs

* more fixes

* last fix (hopefully)

* fix big bird gen example

* fix mbart gen example

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

src/transformers/file_utils.py
src/transformers/models/bart/modeling_bart.py
src/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py
src/transformers/models/blenderbot/modeling_blenderbot.py
src/transformers/models/blenderbot_small/modeling_blenderbot_small.py
src/transformers/models/marian/modeling_marian.py
src/transformers/models/mbart/modeling_mbart.py
src/transformers/models/pegasus/modeling_pegasus.py
src/transformers/models/plbart/modeling_plbart.py
utils/documentation_tests.txt
==================
b256f3518;Sanchit Gandhi;2022-03-09 19:53:01 +0100;Add FlaxBartForCausalLM (#15995)
* add causal lm

* add CausalLM tests

* Add FlaxBartForCausalLM

* Add EncoderDecoder model tests

* change docstring

* make repo-consistency

* suggested changes

* remove jax ops

* correction

* rename pre-trained decoder model
==

docs/source/model_doc/bart.mdx
src/transformers/__init__.py
src/transformers/models/auto/modeling_flax_auto.py
src/transformers/models/bart/__init__.py
src/transformers/models/bart/modeling_flax_bart.py
src/transformers/utils/dummy_flax_objects.py
tests/bart/test_modeling_flax_bart.py
tests/encoder_decoder/test_modeling_flax_encoder_decoder.py
tests/speech_encoder_decoder/test_modeling_flax_speech_encoder_decoder.py
utils/check_repo.py
==================
50dd314d9;lewtun;2022-03-09 17:36:59 +0100;Add ONNX export for ViT (#15658)
* Add ONNX support for ViT

* Refactor to use generic preprocessor

* Add vision dep to tests

* Extend ONNX slow tests to ViT

* Add dummy image generator

* Use model_type to determine modality

* Add deprecation warnings for tokenizer argument

* Add warning when overwriting the preprocessor

* Add optional args to docstrings

* Add minimum PyTorch version to OnnxConfig

* Refactor OnnxConfig class variables from CONSTANT_NAME to snake_case

* Add reasonable value for default atol

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

.circleci/config.yml
docs/source/serialization.mdx
src/transformers/models/bart/configuration_bart.py
src/transformers/models/marian/configuration_marian.py
src/transformers/models/mbart/configuration_mbart.py
src/transformers/models/vit/__init__.py
src/transformers/models/vit/configuration_vit.py
src/transformers/onnx/__main__.py
src/transformers/onnx/config.py
src/transformers/onnx/convert.py
src/transformers/onnx/features.py
tests/onnx/test_onnx_v2.py
==================
b7fa1e3de;Yih-Dar;2022-03-09 17:16:25 +0100;Use tiny models for get_pretrained_model in TFEncoderDecoderModelTest (#15989)
* Use tiny model for TFRembertEncoderDecoderModelTest.get_pretrained_model()

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>
==

tests/encoder_decoder/test_modeling_tf_encoder_decoder.py
==================
8feede229;Shotaro Ishihara;2022-03-10 01:07:52 +0900;Fix broken code blocks in README.md (#15967)
at transformers/examples/pytorch/contrastive-image-text
==

examples/pytorch/contrastive-image-text/README.md
==================
1e8f37992;Francesco Saverio Zuppichini;2022-03-09 15:51:56 +0100;done (#16012)

==

src/transformers/models/maskformer/modeling_maskformer.py
tests/maskformer/test_modeling_maskformer.py
==================
38bce1d4c;Basile Van Hoorick;2022-03-09 09:48:52 -0500;Make `pos` optional to avoid crashing `PerceiverModel` operation (#15972)
Updates `PerceiverAudioPreprocessor` `forward()` implementation to match most other preprocessors / postprocessors
==

src/transformers/models/perceiver/modeling_perceiver.py
==================
cec89e1a0;Sylvain Gugger;2022-03-09 08:47:58 -0500;Simplify release utils (#15921)
* Simplify release utils

* Quality
==

utils/release.py
==================
e493a3a5e;Lysandre Debut;2022-03-09 14:39:03 +0100;Fix github actions comment (#16009)
* Add issue number

* Dev
==

.github/workflows/build_dev_documentation.yml
.github/workflows/delete_dev_documentation.yml
==================
e7f34ccd4;Joao Gante;2022-03-09 13:25:34 +0000;Swag example: Update doc format (#16014)

==

examples/pytorch/multiple-choice/run_swag.py
examples/pytorch/multiple-choice/run_swag_no_trainer.py
examples/tensorflow/multiple-choice/run_swag.py
==================
3ea046995;Yih-Dar;2022-03-09 14:21:23 +0100;Removed an outdated check about hdf5_version (#16011)
* removed an outdated check about hdf5_version

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>
==

tests/auto/test_modeling_tf_pytorch.py
==================
c1aaa4393;Patrick von Platen;2022-03-09 13:09:56 +0100;[Doctests] Move doctests to new GPU & Fix bugs (#15969)
* test

* up

* up

* Empty test commit

* up

* update tests

* up

* fix some vision models

* correct

* correct docs

* Trigger notification

* finalize

* check

* correct quicktour

* Apply suggestions from code review

* improve doctests

* Trigger Build

* next try

* next try

* and again

* Output current clone information

* Output current clone information

* Correct path

* add tf round again

* revert to daily job

Co-authored-by: Lysandre <lysandre.debut@reseau.eseo.fr>
==

.github/workflows/doctests.yml
docs/source/quicktour.mdx
src/transformers/models/beit/modeling_beit.py
src/transformers/models/convnext/modeling_convnext.py
src/transformers/models/deit/modeling_deit.py
src/transformers/models/poolformer/modeling_poolformer.py
src/transformers/models/segformer/modeling_segformer.py
src/transformers/models/speech_to_text/modeling_speech_to_text.py
src/transformers/models/speech_to_text_2/modeling_speech_to_text_2.py
src/transformers/models/swin/modeling_swin.py
src/transformers/models/vit/modeling_vit.py
src/transformers/models/wav2vec2/modeling_wav2vec2.py
==================
f4e4ad34c;Nicolas Patry;2022-03-09 10:19:05 +0100;Add `ForInstanceSegmentation` models to `image-segmentation` pipelines (#15937)
* Adding ForInstanceSegmentation to pipelines.

* Last fix `category_id` renamed to `label_id`.

* Can't be none no more.

* No `is_thing_map` anymore.
==

src/transformers/pipelines/image_segmentation.py
tests/pipelines/test_pipelines_image_segmentation.py
==================
5b7dcc734;David Hall;2022-03-08 10:45:41 -0800;Seed _get_train_sampler's generator with arg seed to improve reproducibility (#15961)
* Seed get_train_sampler's generator with arg seed to improve reproducibility

and make the world_size<=1 code path more similar to the others

* move test file into trainer test explicitly

* dumb typo

* make style lint happy

* per discussion, switch to data_seed

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

src/transformers/trainer.py
src/transformers/training_args.py
tests/trainer/test_trainer.py
==================
70203b593;Joao Gante;2022-03-08 14:46:44 +0000;TF generate refactor - past without encoder outputs (#15944)
* Remove packed past from generation_tf_utils

* update models with the new past format

* update template accordingly

==

src/transformers/generation_tf_utils.py
src/transformers/models/bart/modeling_tf_bart.py
src/transformers/models/bert/modeling_tf_bert.py
src/transformers/models/blenderbot/modeling_tf_blenderbot.py
src/transformers/models/blenderbot_small/modeling_tf_blenderbot_small.py
src/transformers/models/ctrl/modeling_tf_ctrl.py
src/transformers/models/encoder_decoder/modeling_tf_encoder_decoder.py
src/transformers/models/gpt2/modeling_tf_gpt2.py
src/transformers/models/led/modeling_tf_led.py
src/transformers/models/marian/modeling_tf_marian.py
src/transformers/models/mbart/modeling_tf_mbart.py
src/transformers/models/pegasus/modeling_tf_pegasus.py
src/transformers/models/rag/modeling_tf_rag.py
src/transformers/models/rembert/modeling_tf_rembert.py
src/transformers/models/roberta/modeling_tf_roberta.py
src/transformers/models/speech_to_text/modeling_tf_speech_to_text.py
src/transformers/models/t5/modeling_tf_t5.py
src/transformers/models/transfo_xl/modeling_tf_transfo_xl.py
src/transformers/models/vision_encoder_decoder/modeling_tf_vision_encoder_decoder.py
src/transformers/models/xlnet/modeling_tf_xlnet.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_tf_{{cookiecutter.lowercase_modelname}}.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/test_modeling_tf_{{cookiecutter.lowercase_modelname}}.py
tests/bart/test_modeling_tf_bart.py
tests/blenderbot/test_modeling_tf_blenderbot.py
tests/blenderbot_small/test_modeling_tf_blenderbot_small.py
tests/led/test_modeling_tf_led.py
tests/marian/test_modeling_tf_marian.py
tests/pegasus/test_modeling_tf_pegasus.py
tests/speech_to_text/test_modeling_tf_speech_to_text.py
tests/t5/test_modeling_tf_t5.py
==================
62d847602;Joao Gante;2022-03-08 13:16:34 +0000;Update TF multiple choice example (#15868)

==

examples/tensorflow/multiple-choice/run_swag.py
==================
ab2f8d12a;Patrick von Platen;2022-03-08 14:03:03 +0100;add hf hub to env version command (#15981)

==

src/transformers/commands/env.py
==================
72983303c;Yih-Dar;2022-03-08 13:37:20 +0100;Fix TFEncoderDecoderModelTest - Pytorch device (#15979)
* fix device

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>
==

tests/encoder_decoder/test_modeling_tf_encoder_decoder.py
==================
f5a080dd1;Sylvain Gugger;2022-03-08 07:19:41 -0500;Do a pull in case docs were updated during build (#15922)

==

.github/workflows/build_documentation.yml
==================
91fb62d01;Yeb Havinga;2022-03-08 12:18:38 +0100;Speedup training by using numpy instead of jnp for batch shuffling (#15963)
Speedup training by using numpy instead of jnp for batch shuffling

Co-authored-by: Yeb Havinga <y.t.havinga@mgrid.net>
==

examples/flax/language-modeling/run_t5_mlm_flax.py
==================
ea07064a5;Nicolas Patry;2022-03-08 11:17:57 +0100;Returning outputs only when asked for for MaskFormer. (#15936)
* Returning outputs only when asked for for MaskFormer.

* Adding `output_auxiliary_logits` to the config.
==

src/transformers/models/maskformer/configuration_maskformer.py
src/transformers/models/maskformer/modeling_maskformer.py
==================
b19f3e69a;NielsRogge;2022-03-08 10:49:44 +0100;[Tests] Fix ViTMAE integration test (#15949)
* Fix test across both cpu and gpu

* Fix typo
==

tests/vit_mae/test_modeling_vit_mae.py
==================
9879a1d5f;NielsRogge;2022-03-08 10:49:30 +0100;Fix LayoutLMv2 test (#15939)
* Fix LayoutLMv2 test

* Update black
==

tests/layoutlmv2/test_tokenization_layoutlmv2.py
==================
8b9ae4554;Yih-Dar;2022-03-07 22:14:33 +0100;Set scale_embedding to False in some TF tests (#15952)
* set scale_embedding to False to avoid large (> 1e-5) output differences between PT/TF

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>
==

tests/speech_to_text/test_modeling_tf_speech_to_text.py
==================
38cc35069;Steven Liu;2022-03-07 11:29:14 -0800;Update training scripts docs (#15931)
* üìù first draft

* üñç apply feedback

* üñç remove examples from toctree

* üóë remove examples from docs/source
==

docs/source/_toctree.yml
docs/source/examples.md
docs/source/run_scripts.mdx
==================
c87cfd653;Sylvain Gugger;2022-03-07 13:29:16 -0500;Better error message when inputs are empty

==

src/transformers/trainer.py
==================
e9fa7cd5d;Francesco Saverio Zuppichini;2022-03-07 19:10:32 +0100;Make is_thing_map in Feature Extractor post_process_panoptic_segmentation defaults to all instances (#15954)
* is_thing_map defaults to all instances

* better naming

* control flow

* resolving conversations
==

src/transformers/models/maskformer/feature_extraction_maskformer.py
==================
2596f95e8;Sanchit Gandhi;2022-03-07 18:17:45 +0100;Fix Embedding Module Bug in Flax Models (#15920)

==

src/transformers/models/bart/modeling_flax_bart.py
src/transformers/models/blenderbot/modeling_flax_blenderbot.py
src/transformers/models/blenderbot_small/modeling_flax_blenderbot_small.py
src/transformers/models/marian/modeling_flax_marian.py
src/transformers/models/mbart/modeling_flax_mbart.py
src/transformers/models/pegasus/modeling_flax_pegasus.py
src/transformers/models/t5/modeling_flax_t5.py
==================
1a62b25ca;Sanchit Gandhi;2022-03-07 18:10:15 +0100;Backprop Test for Freeze FlaxWav2Vec2 Feature Encoder (#15938)
* Backprop Test for Freeze FlaxWav2Vec2 Feature Encoder

* remove jnp.ndarray type suggestion

* assert frozen grads are precisely zero
==

tests/wav2vec2/test_modeling_flax_wav2vec2.py
==================
544fd9876;Konstantin Dobler;2022-03-07 16:22:48 +0100;Support modern list type hints in HfArgumentParser (#15951)
* Support modern list type hint in HfArgumentParser

* Fix formatting with black
==

src/transformers/hf_argparser.py
==================
60b81dfa6;Suraj Patil;2022-03-07 14:58:44 +0100;remove re-defination of FlaxWav2Vec2ForCTCModule (#15965)

==

src/transformers/models/wav2vec2/modeling_flax_wav2vec2.py
==================
ef9c3ca34;Chan Woo Kim;2022-03-07 17:10:18 +0900;[Bug Fix] Beam search example in docs fails & a fix (integrating `max_length` in `BeamScorer.finalize()`) (#15555)
* added the test and fix

* had left out a comment
==

src/transformers/generation_beam_search.py
tests/generation/test_generation_utils.py
==================
9932ee4b4;Francesco Saverio Zuppichini;2022-03-04 19:11:48 +0100;made MaskFormerModelTest faster (#15942)

==

tests/maskformer/test_modeling_maskformer.py
==================
e8efaecb8;NielsRogge;2022-03-04 18:53:54 +0100;Move dependency to call method (#15941)

==

src/transformers/models/layoutlmv2/feature_extraction_layoutlmv2.py
==================
5c6f57ee7;Chan Woo Kim;2022-03-05 02:18:34 +0900;Constrained Beam Search [*With* Disjunctive Decoding] (#15761)
* added classes to get started with constrained beam search

* in progress, think i can directly force tokens now but not yet with the round robin

* think now i have total control, now need to code the bank selection

* technically works as desired, need to optimize and fix design choices leading to undersirable outputs

* complete PR #1 without disjunctive decoding

* removed incorrect tests

* Delete k.txt

* Delete test.py

* Delete test.sh

* revert changes to test scripts

* genutils

* full implementation with testing, no disjunctive yet

* shifted docs

* passing all tests realistically ran locally

* removing accidentally included print statements

* fixed source of error in initial PR test

* fixing the get_device() vs device trap

* fixed documentation docstrings about constrained_beam_search

* fixed tests having failing for Speech2TextModel's floating point inputs

* fix cuda long tensor

* added examples and testing for them and founx & fixed a bug in beam_search and constrained_beam_search

* deleted accidentally added test halting code with assert False

* code reformat

* Update tests/test_generation_utils.py

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* Update tests/test_generation_utils.py

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* Update tests/test_generation_utils.py

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* Update tests/test_generation_utils.py

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* Update tests/test_generation_utils.py

* fixing based on comments on PR

* took out the testing code that should but work fails without the beam search moditification ; style changes

* fixing comments issues

* docstrings for ConstraintListState

* typo in PhrsalConstraint docstring

* docstrings improvements

* finished adding what is sort of an opinionated implementation of disjunctive generation, but it revealed errors in inner beam search logic during testing.

* fixed bug found in constrained beam search that used beam_idx that were not global across all the batches

* disjunctive constraint working 100% correctly

* passing all tests

* Accidentally included mlruns

* Update src/transformers/generation_beam_constraints.py

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* Update src/transformers/generation_beam_constraints.py

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* complete overhaul of type complexities and other nits

* strict type checks in generate()

* fixing second round of feedback by narsil

* fixed failing generation test because of type check overhaul

* generation test fail fix

* fixing test fails

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
==

docs/source/internal/generation_utils.mdx
src/transformers/__init__.py
src/transformers/generation_beam_constraints.py
src/transformers/generation_beam_search.py
src/transformers/generation_utils.py
src/transformers/utils/dummy_pt_objects.py
tests/generation/test_generation_beam_constraints.py
tests/generation/test_generation_beam_search.py
tests/generation/test_generation_utils.py
==================
040c11f6d;Francesco Saverio Zuppichini;2022-03-04 18:04:19 +0100;Tests for MaskFormerFeatureExtractor's post_process*** methods (#15929)
* proper tests for post_process*** methods in feature extractor

* mask th == 0

* Update tests/maskformer/test_feature_extraction_maskformer.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* make style

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

tests/maskformer/test_feature_extraction_maskformer.py
tests/maskformer/test_modeling_maskformer.py
==================
f0aacc140;Yih-Dar;2022-03-04 17:50:24 +0100;Do not change the output from tuple to list - to match PT's version (#15918)
* Do not change the output from tuple to list - to match PT's version

* Fix the same issues for 5 other models and the template

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>
==

src/transformers/models/bart/modeling_tf_bart.py
src/transformers/models/blenderbot/modeling_tf_blenderbot.py
src/transformers/models/blenderbot_small/modeling_tf_blenderbot_small.py
src/transformers/models/marian/modeling_tf_marian.py
src/transformers/models/mbart/modeling_tf_mbart.py
src/transformers/models/pegasus/modeling_tf_pegasus.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_tf_{{cookiecutter.lowercase_modelname}}.py
==================
10b76987f;Patrick von Platen;2022-03-04 17:04:43 +0100;[FlaxT5 Example] fix flax t5 example pretraining (#15835)

==

examples/flax/language-modeling/run_t5_mlm_flax.py
==================
01485ceec;Javier de la Rosa;2022-03-04 14:36:28 +0100;Add missing support for Flax XLM-RoBERTa (#15900)
* Adding Flax XLM-RoBERTa

* Add Flax to __init__

* Adding doc and dummy objects

* Add tests

* Add Flax XLM-R models autodoc

* Fix tests

* Add Flask XLM-RoBERTa to TEST_FILES_WITH_NO_COMMON_TESTS

* Update src/transformers/models/xlm_roberta/modeling_flax_xlm_roberta.py

Co-authored-by: Suraj Patil <surajp815@gmail.com>

* Update tests/xlm_roberta/test_modeling_flax_xlm_roberta.py

Co-authored-by: Suraj Patil <surajp815@gmail.com>

* Update tests/xlm_roberta/test_modeling_flax_xlm_roberta.py

Co-authored-by: Suraj Patil <surajp815@gmail.com>

* Remove test on large Flask XLM-RoBERTa

* Add tokenizer to the test

Co-authored-by: Suraj Patil <surajp815@gmail.com>
==

docs/source/index.mdx
docs/source/model_doc/xlm-roberta.mdx
src/transformers/__init__.py
src/transformers/models/auto/modeling_flax_auto.py
src/transformers/models/xlm_roberta/__init__.py
src/transformers/models/xlm_roberta/modeling_flax_xlm_roberta.py
src/transformers/utils/dummy_flax_objects.py
tests/xlm_roberta/test_modeling_flax_xlm_roberta.py
utils/check_repo.py
==================
89c7d9cfb;Nicolas Patry;2022-03-04 13:56:15 +0100;Making MaskFormerForInstanceSegmentation. (#15934)
Small adjustments.

Adding in type hint.

Last fix ?

Only include the default dict thing, not the pipelines.
==

docs/source/model_doc/auto.mdx
src/transformers/__init__.py
src/transformers/models/auto/__init__.py
src/transformers/models/auto/feature_extraction_auto.py
src/transformers/models/auto/modeling_auto.py
src/transformers/utils/dummy_pt_objects.py
==================
7ade7c179;Nicolas Patry;2022-03-04 12:32:19 +0100;Updating the slow tests: (#15893)
Linked to https://github.com/huggingface/transformers/pull/15826
==

tests/pipelines/test_pipelines_image_segmentation.py
==================
6b104c5bb;ParkSangJun;2022-03-04 19:57:09 +0900;Support CLIPTokenizerFast for CLIPProcessor (#15913)
* Fix to support fast tokenizer with `CLIPProcessor`

* Update CLIPProcessor test for fast tokenizer

* Fix Docstring Style

* Rename into meaningful Variable name in test code
==

src/transformers/models/clip/processing_clip.py
tests/clip/test_processor_clip.py
==================
b71474895;Sanchit Gandhi;2022-03-04 09:58:45 +0100;Update README.md

==

examples/research_projects/jax-projects/README.md
==================
a6e3b1798;Nicolas Patry;2022-03-04 09:53:00 +0100;Re-enabling all fast pipeline tests. (#15924)

==

tests/pipelines/test_pipelines_common.py
==================
a7df656f0;Patrick von Platen;2022-03-04 00:22:38 +0100;Update README.md (#15926)

==

docs/README.md
==================
c0281feb5;davidleonfdez;2022-03-03 19:41:03 +0000;Fix #15898 (#15928)

==

examples/pytorch/language-modeling/run_clm.py
examples/pytorch/language-modeling/run_mlm.py
==================
9251427c3;NielsRogge;2022-03-03 19:46:31 +0100;Add vision models to doc tests (#15905)
* Add vision models to doc tests

* Apply suggestions from code review

* Add more models

Co-authored-by: Niels Rogge <nielsrogge@Nielss-MacBook-Pro.local>
==

src/transformers/models/beit/modeling_beit.py
src/transformers/models/deit/modeling_deit.py
src/transformers/models/swin/modeling_swin.py
src/transformers/models/vit/modeling_vit.py
utils/documentation_tests.txt
==================
742273a52;Francesco Saverio Zuppichini;2022-03-03 19:35:48 +0100;fix for the output from post_process_panoptic_segmentation (#15916)

==

src/transformers/models/maskformer/feature_extraction_maskformer.py
tests/maskformer/test_modeling_maskformer.py
==================
7c45fe747;Sylvain Gugger;2022-03-03 11:03:24 -0500;Mark slow tests as slow

==

tests/detr/test_modeling_detr.py
tests/maskformer/test_modeling_maskformer.py
==================
3822e4a56;Nicolas Patry;2022-03-03 16:31:41 +0100;Enabling MaskFormer in pipelines (#15917)
* Enabling MaskFormer in ppipelines

No AutoModel though :(

* Ooops local file.
==

src/transformers/models/maskformer/feature_extraction_maskformer.py
src/transformers/pipelines/image_segmentation.py
tests/pipelines/test_pipelines_image_segmentation.py
==================
79d28e80b;Sylvain Gugger;2022-03-03 10:19:58 -0500;v4.18.0.dev.0

==

examples/flax/question-answering/run_qa.py
examples/flax/text-classification/run_flax_glue.py
examples/flax/token-classification/run_flax_ner.py
examples/pytorch/audio-classification/run_audio_classification.py
examples/pytorch/contrastive-image-text/run_clip.py
examples/pytorch/image-classification/run_image_classification.py
examples/pytorch/image-pretraining/run_mae.py
examples/pytorch/image-pretraining/run_mim.py
examples/pytorch/language-modeling/run_clm.py
examples/pytorch/language-modeling/run_mlm.py
examples/pytorch/language-modeling/run_plm.py
examples/pytorch/multiple-choice/run_swag.py
examples/pytorch/question-answering/run_qa.py
examples/pytorch/question-answering/run_qa_beam_search.py
examples/pytorch/question-answering/run_qa_beam_search_no_trainer.py
examples/pytorch/question-answering/run_qa_no_trainer.py
examples/pytorch/question-answering/run_seq2seq_qa.py
examples/pytorch/speech-recognition/run_speech_recognition_ctc.py
examples/pytorch/speech-recognition/run_speech_recognition_seq2seq.py
examples/pytorch/summarization/run_summarization.py
examples/pytorch/text-classification/run_glue.py
examples/pytorch/text-classification/run_xnli.py
examples/pytorch/token-classification/run_ner.py
examples/pytorch/translation/run_translation.py
examples/tensorflow/multiple-choice/run_swag.py
examples/tensorflow/question-answering/run_qa.py
examples/tensorflow/summarization/run_summarization.py
examples/tensorflow/text-classification/run_glue.py
examples/tensorflow/translation/run_translation.py
setup.py
src/transformers/__init__.py
==================
6cbfa7bf4;Patrick von Platen;2022-03-03 16:01:56 +0100;[Doctests] Fix ignore bug and add more doc tests (#15911)
* finish speech doc tests

* finish

* boom

* Update src/transformers/models/speech_to_text/modeling_speech_to_text.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

conftest.py
src/transformers/models/data2vec/modeling_data2vec_audio.py
src/transformers/models/speech_encoder_decoder/modeling_speech_encoder_decoder.py
src/transformers/models/speech_to_text/modeling_speech_to_text.py
src/transformers/models/speech_to_text_2/modeling_speech_to_text_2.py
src/transformers/models/wav2vec2/modeling_wav2vec2.py
src/transformers/models/wav2vec2/tokenization_wav2vec2.py
src/transformers/models/wav2vec2_with_lm/processing_wav2vec2_with_lm.py
tests/speech_to_text/test_modeling_speech_to_text.py
utils/documentation_tests.txt
==================
b693cbf99;Nicolas Patry;2022-03-03 15:33:49 +0100;The tests were not updated after the addition of `torch.diag` (#15890)
in the scoring (which is more correct)
==

tests/pipelines/test_pipelines_zero_shot_image_classification.py
==================
3c4fbc616;Sanchit Gandhi;2022-03-03 14:17:13 +0100;Freeze FlaxWav2Vec2 Feature Encoder (#15873)
* Freeze FlaxWav2Vec2 Feature Encoder

* add to all module apply

* add backprop test
==

src/transformers/models/wav2vec2/modeling_flax_wav2vec2.py
tests/wav2vec2/test_modeling_flax_wav2vec2.py
==================
7b3bd1f21;Li-Huai (Allan) Lin;2022-03-03 21:10:15 +0800;Fix and improve REALM fine-tuning (#15297)
* Draft

* Add test

* Update src/transformers/models/realm/modeling_realm.py

* Apply suggestion

* Add block_mask

* Update

* Update

* Add block_embedding_to

* Remove no_grad

* Use AutoTokenizer

* Remove model.to overridding
==

docs/source/model_doc/realm.mdx
src/transformers/models/auto/tokenization_auto.py
src/transformers/models/realm/modeling_realm.py
src/transformers/models/realm/retrieval_realm.py
tests/realm/test_modeling_realm.py
tests/realm/test_retrieval_realm.py
==================
439de3f7f;Patrick von Platen;2022-03-03 13:43:13 +0100;[Fix link in pipeline doc] (#15906)

==

docs/source/quicktour.mdx
==================
4cd7ed4b3;Yih-Dar;2022-03-03 13:21:31 +0100;Fix a TF Vision Encoder Decoder test (#15896)
* send PyTorch inputs to the correct device

* Fix: TypeError: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>
==

tests/vision_encoder_decoder/test_modeling_tf_vision_encoder_decoder.py
==================
39249c958;Sylvain Gugger;2022-03-02 18:06:31 -0500;Fix doc links in release utils (#15903)

==

utils/release.py
==================
3d2242869;Sylvain Gugger;2022-03-02 16:18:54 -0500;Update delete-dev-doc job to match build-dev-doc (#15891)
* Update delete-dev-doc job to match build-dev-doc

* More debug info

* More debug info

* Stash if needed

* Remove the comment update

* Fix paths

* Wtf is going on..

* Fix git status test

* Try another way

* I don't understand what's happening

* Bash shell

* What's happening now...

* What's happening now...

* Try like this

* Back to trying to use bash

* And like that?

* Refine tests

* Stash after adding new files

* Stash after adding new files

* Proper commit sha and PR number

* Address review comments
==

.github/workflows/build_dev_documentation.yml
.github/workflows/delete_dev_documentation.yml
==================
89be34c36;NielsRogge;2022-03-02 21:57:39 +0100;Fix SegformerForImageClassification (#15895)
* Fix reshape

* Apply suggestion from code review

Co-authored-by: Niels Rogge <nielsrogge@Nielss-MacBook-Pro.local>
==

src/transformers/models/segformer/modeling_segformer.py
==================
130b98788;Suraj Patil;2022-03-02 17:55:49 +0100;[XGLM] run sampling test on CPU to be deterministic (#15892)
* run sampling test on CPU to be deterministic

* input_ids on CPU
==

tests/xglm/test_modeling_xglm.py
==================
baab5e7cd;Joao Gante;2022-03-02 16:13:54 +0000;TF generate refactor - Sample (#15793)
* Add TF logits wrappers 

* Add sample method

* add tests for TF logit wrappers

* TF generate sample tests now run on CPU

Co-authored-by: Matt <Rocketknight1@users.noreply.github.com>
==

docs/source/internal/generation_utils.mdx
src/transformers/__init__.py
src/transformers/generation_flax_logits_process.py
src/transformers/generation_flax_utils.py
src/transformers/generation_tf_logits_process.py
src/transformers/generation_tf_utils.py
src/transformers/generation_utils.py
src/transformers/utils/dummy_tf_objects.py
tests/generation/test_generation_logits_process.py
tests/generation/test_generation_tf_logits_process.py
tests/gpt2/test_modeling_tf_gpt2.py
tests/t5/test_modeling_tf_t5.py
tests/test_modeling_tf_common.py
==================
96ae92be8;NielsRogge;2022-03-02 16:20:47 +0100;[SegFormer] Add deprecation warning (#15889)
* Add deprecation warning

* Remove from docs and hide in kwargs

* Improve implementation

Co-authored-by: Niels Rogge <nielsrogge@Nielss-MacBook-Pro.local>
==

src/transformers/models/segformer/configuration_segformer.py
src/transformers/models/segformer/modeling_segformer.py
==================
8fd473107;Sanchit Gandhi;2022-03-02 16:02:26 +0100;Fix Bug in FlaxWav2Vec2 Slow Test (#15887)

==

tests/wav2vec2/test_modeling_flax_wav2vec2.py
==================
d83d22f57;Francesco Saverio Zuppichini;2022-03-02 15:48:20 +0100;Maskformer (#15682)
* maskformer

* conflicts

* conflicts

* minor fixes

* feature extractor test fix

refactor MaskFormerLoss following conversation

MaskFormer related types should not trigger a module time import error

missed one

removed all the types that are not used

update config mapping

minor updates in the doc

resolved conversation that doesn't need a discussion

minor changes

resolved conversations

fixed DetrDecoder

* minor changes

minor changes

fixed mdx file

test feature_extractor return types

functional losses -> classes

removed the return type test for the feature extractor

minor changes + style + quality

* conflicts?

* rebase master

* readme

* added missing files

* deleded poolformers test that where in the wrong palce

* CI

* minor changes

* Apply suggestions from code review

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>

* resolved conversations

* minor changes

* conversations

[Unispeech] Fix slow tests (#15818)

* remove soundfile old way of loading audio

* Adapt slow test

[Barthez Tokenizer] Fix saving (#15815)

[TFXLNet] Correct tf xlnet generate (#15822)

* [TFXLNet] Correct tf xlnet

* adapt test comment

Fix the push run (#15807)

Fix semantic segmentation pipeline test (#15826)

Fix dummy_inputs() to dummy_inputs in symbolic_trace doc (#15776)

Add model specific output classes to PoolFormer model docs (#15746)

* Added model specific output classes to poolformer docs

* Fixed Segformer typo in Poolformer docs

Adding the option to return_timestamps on pure CTC ASR models. (#15792)

* Adding the option to return_timestamps on pure CTC ASR models.

* Remove `math.prod` which was introduced in Python 3.8

* int are not floats.

* Reworking the PR to support "char" vs "word" output.

* Fixup!

* Update src/transformers/pipelines/automatic_speech_recognition.py

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* Update src/transformers/pipelines/automatic_speech_recognition.py

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* Update src/transformers/pipelines/automatic_speech_recognition.py

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* Update src/transformers/pipelines/automatic_speech_recognition.py

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* Update src/transformers/pipelines/automatic_speech_recognition.py

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* Update src/transformers/pipelines/automatic_speech_recognition.py

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* Update src/transformers/pipelines/automatic_speech_recognition.py

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* Update src/transformers/pipelines/automatic_speech_recognition.py

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* Update src/transformers/pipelines/automatic_speech_recognition.py

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* Quality.

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

HFTracer.trace should use/return self.graph to be compatible with torch.fx.Tracer (#15824)

Fix tf.concatenate + test past_key_values for TF models (#15774)

* fix wrong method name tf.concatenate

* add tests related to causal LM / decoder

* make style and quality

* clean-up

* Fix TFBertModel's extended_attention_mask when past_key_values is provided

* Fix tests

* fix copies

* More tf.int8 -> tf.int32 in TF test template

* clean-up

* Update TF test template

* revert the previous commit + update the TF test template

* Fix TF template extended_attention_mask when past_key_values is provided

* Fix some styles manually

* clean-up

* Fix ValueError: too many values to unpack in the test

* Fix more: too many values to unpack in the test

* Add a comment for extended_attention_mask when there is past_key_values

* Fix TFElectra extended_attention_mask when past_key_values is provided

* Add tests to other TF models

* Fix for TF Electra test: add prepare_config_and_inputs_for_decoder

* Fix not passing training arg to lm_head in TFRobertaForCausalLM

* Fix tests (with past) for TF Roberta

* add testing for pask_key_values for TFElectra model

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>

[examples/summarization and translation] fix readme (#15833)

Add ONNX Runtime quantization for text classification notebook (#15817)

Re-enable doctests for the quicktour (#15828)

* Re-enable doctests for the quicktour

* Re-enable doctests for task_summary (#15830)

* Remove &

Framework split model report (#15825)

Add TFConvNextModel (#15750)

* feat: initial implementation of convnext in tensorflow.

* fix: sample code for the classification model.

* chore: added checked for  from the classification model.

* chore: set bias initializer in the classification head.

* chore: updated license terms.

* chore: removed ununsed imports

* feat: enabled  argument during using drop_path.

* chore: replaced tf.identity with layers.Activation(linear).

* chore: edited default checkpoint.

* fix: minor bugs in the initializations.

* partial-fix: tf model errors for loading pretrained pt weights.

* partial-fix: call method updated

* partial-fix: cross loading of weights (4x3 variables to be matched)

* chore: removed unneeded comment.

* removed playground.py

* rebasing

* rebasing and removing playground.py.

* fix: renaming TFConvNextStage conv and layer norm layers

* chore: added initializers and other minor additions.

* chore: added initializers and other minor additions.

* add: tests for convnext.

* fix: integration tester class.

* fix: issues mentioned in pr feedback (round 1).

* fix: how output_hidden_states arg is propoagated inside the network.

* feat: handling of  arg for pure cnn models.

* chore: added a note on equal contribution in model docs.

* rebasing

* rebasing and removing playground.py.

* feat: encapsulation for the convnext trunk.

* Fix variable naming; Test-related corrections; Run make fixup

* chore: added Joao as a contributor to convnext.

* rebasing

* rebasing and removing playground.py.

* rebasing

* rebasing and removing playground.py.

* chore: corrected copyright year and added comment on NHWC.

* chore: fixed the black version and ran formatting.

* chore: ran make style.

* chore: removed from_pt argument from test, ran make style.

* rebasing

* rebasing and removing playground.py.

* rebasing

* rebasing and removing playground.py.

* fix: tests in the convnext subclass, ran make style.

* rebasing

* rebasing and removing playground.py.

* rebasing

* rebasing and removing playground.py.

* chore: moved convnext test to the correct location

* fix: locations for the test file of convnext.

* fix: convnext tests.

* chore: applied  sgugger's suggestion for dealing w/ output_attentions.

* chore: added comments.

* chore: applied updated quality enviornment style.

* chore: applied formatting with quality enviornment.

* chore: revert to the previous tests/test_modeling_common.py.

* chore: revert to the original test_modeling_common.py

* chore: revert to previous states for test_modeling_tf_common.py and modeling_tf_utils.py

* fix: tests for convnext.

* chore: removed output_attentions argument from convnext config.

* chore: revert to the earlier tf utils.

* fix: output shapes of the hidden states

* chore: removed unnecessary comment

* chore: reverting to the right test_modeling_tf_common.py.

* Styling nits

Co-authored-by: ariG23498 <aritra.born2fly@gmail.com>
Co-authored-by: Joao Gante <joao@huggingface.co>
Co-authored-by: Sylvain Gugger <Sylvain.gugger@gmail.com>

* minor changes

* doc fix in feature extractor

* doc

* typose

* removed detr logic from config

* removed detr logic from config

* removed num_labels

* small fix in the config

* auxilary -> auxiliary

* make style

* some test is failing

* fix a weird char in config prevending doc-builder

* retry to fix the doc-builder issue

* make style

* new try to fix the doc builder

* CI

* change weights to facebook

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>
Co-authored-by: ariG23498 <aritra.born2fly@gmail.com>
Co-authored-by: Joao Gante <joao@huggingface.co>
Co-authored-by: Sylvain Gugger <Sylvain.gugger@gmail.com>
==

README.md
README_ko.md
README_zh-hans.md
README_zh-hant.md
docs/source/_toctree.yml
docs/source/index.mdx
docs/source/model_doc/maskformer.mdx
src/transformers/__init__.py
src/transformers/models/__init__.py
src/transformers/models/auto/configuration_auto.py
src/transformers/models/auto/modeling_auto.py
src/transformers/models/maskformer/__init__.py
src/transformers/models/maskformer/configuration_maskformer.py
src/transformers/models/maskformer/convert_maskformer_original_pytorch_checkpoint_to_pytorch.py
src/transformers/models/maskformer/feature_extraction_maskformer.py
src/transformers/models/maskformer/modeling_maskformer.py
src/transformers/utils/dummy_pt_objects.py
src/transformers/utils/dummy_vision_objects.py
tests/maskformer/__init__.py
tests/maskformer/test_feature_extraction_maskformer.py
tests/maskformer/test_modeling_maskformer.py
utils/check_repo.py
==================
e535c389a;Ross Johnstone;2022-03-02 23:37:05 +0900;Fix tiny typo (#15884)

==

examples/research_projects/bert-loses-patience/pabee/modeling_pabee_bert.py
==================
2eb7bb15e;Rahul Huilgol;2022-03-02 04:55:14 -0800;Updates in Trainer to support new features in SM Model Parallel library (#15877)
* Create optimizer after model creation for SMP

* update dp_rank to rdp_rank for opt_state_dict

* update world_size and process_index for smp

* Address comments

* Lint fix

Co-authored-by: Cavdar <dcavdar@a07817b12d7e.ant.amazon.com>
==

src/transformers/trainer.py
src/transformers/training_args.py
==================
05c237ea9;Joao Gante;2022-03-02 10:38:13 +0000;Update TF QA example (#15870)

==

examples/tensorflow/question-answering/run_qa.py
==================
6e57a5698;Nicolas Patry;2022-03-02 10:49:05 +0100;Adding timestamps for CTC with LM in ASR pipeline. (#15863)
* Adding timestamps for CTC with LM in ASR pipeline.

* iRemove print.

* Nit change.
==

src/transformers/models/wav2vec2/tokenization_wav2vec2.py
src/transformers/pipelines/automatic_speech_recognition.py
tests/pipelines/test_pipelines_automatic_speech_recognition.py
==================
8a133490b;Joao Gante;2022-03-02 09:48:11 +0000;Add TF generate sample tests with all logit processors (#15852)
* Add GPT2 TF generate sample test with all logits processor

* Add T5 generate sample test
==

tests/gpt2/test_modeling_tf_gpt2.py
tests/t5/test_modeling_tf_t5.py
==================
40040727a;Patrick von Platen;2022-03-02 10:24:32 +0100;[Bart] Fix implementation note doc (#15879)

==

docs/source/model_doc/bart.mdx
==================
4bfe75bd0;Michael Benayoun;2022-03-02 04:03:14 -0500;M2M100 support for ONNX export (#15193)
* Add M2M100 support for ONNX export

* Delete useless imports

* Add M2M100 to tests

* Fix protobuf issue
==

docs/source/serialization.mdx
src/transformers/models/m2m_100/__init__.py
src/transformers/models/m2m_100/configuration_m2m_100.py
src/transformers/onnx/convert.py
src/transformers/onnx/features.py
tests/onnx/test_onnx_v2.py
==================
d1a29078c;Lysandre Debut;2022-03-02 04:36:19 +0100;Remove stash for now (#15882)

==

.github/workflows/build_dev_documentation.yml
==================
b842d7277;Stas Bekman;2022-03-01 19:27:28 -0800;fix deepspeed tests (#15881)
* fix deepspeed tests

* style

* more fixes
==

tests/deepspeed/__init__.py
tests/deepspeed/test_deepspeed.py
tests/deepspeed/test_model_zoo.py
==================
6ccfa2170;Steven Liu;2022-03-01 13:10:31 -0800;Inference for multilingual models (#15836)
* üìù first draft for multilingual models

* üñç make style
==

docs/source/_toctree.yml
docs/source/multilingual.mdx
==================
26426923b;Lysandre Debut;2022-03-01 20:05:54 +0100;No self-hosted runner for dev documentation (#15710)

==

.github/workflows/build-docker-images.yml
.github/workflows/build_dev_documentation.yml
docker/transformers-doc-builder/Dockerfile
==================
00eaffc81;Mishig Davaadorj;2022-03-01 18:37:57 +0100;Bump up doc node version to 16 (#15874)

==

.github/workflows/build_dev_documentation.yml
.github/workflows/build_documentation.yml
==================
afca0d519;Suraj Patil;2022-03-01 18:26:30 +0100;use python 3.7 for flax self-push tests (#15865)
* set python 3.7 for flax tests

* setup-python@v2

* python-dev

* install -y

* python3-dev

* install kenlm from source

* install cython

* cd to kenlm

* kenlm install

* don't install kenlm

* change flax pretrained to run flax tests

* cleanup

* remove python-dev
==

.github/workflows/self-push.yml
==================
286fdc6b3;NielsRogge;2022-03-01 18:09:52 +0100;[vision] Add problem_type support (#15851)
* Add problem_type to missing models

* Fix deit test

Co-authored-by: Niels Rogge <nielsrogge@Nielss-MacBook-Pro.local>
==

src/transformers/models/beit/modeling_beit.py
src/transformers/models/deit/modeling_deit.py
src/transformers/models/segformer/modeling_segformer.py
src/transformers/models/vit/modeling_vit.py
tests/deit/test_modeling_deit.py
tests/test_modeling_common.py
==================
7ff9d450c;Lysandre Debut;2022-03-01 17:47:17 +0100;Scatter should run on CUDA (#15872)

==

docker/transformers-all-latest-gpu/Dockerfile
docker/transformers-pytorch-gpu/Dockerfile
==================
c008afea3;NielsRogge;2022-03-01 17:44:20 +0100;Add link to notebooks (#15791)
Co-authored-by: Niels Rogge <nielsrogge@Nielss-MacBook-Pro.local>
==

docs/source/model_doc/vilt.mdx
==================
e064f0815;Patrick von Platen;2022-03-01 17:03:05 +0100;Add time stamps for wav2vec2 with lm (#15854)
* [Wav2Vec2 With LM] add timestamps

* correct

* correct

* Apply suggestions from code review

* correct

* Update src/transformers/models/wav2vec2_with_lm/processing_wav2vec2_with_lm.py

* make style

* Update src/transformers/models/wav2vec2_with_lm/processing_wav2vec2_with_lm.py

Co-authored-by: Nicolas Patry <patry.nicolas@protonmail.com>

* make style

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

Co-authored-by: Nicolas Patry <patry.nicolas@protonmail.com>
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

src/transformers/models/wav2vec2/tokenization_wav2vec2.py
src/transformers/models/wav2vec2_phoneme/tokenization_wav2vec2_phoneme.py
src/transformers/models/wav2vec2_with_lm/processing_wav2vec2_with_lm.py
tests/wav2vec2_with_lm/test_processor_wav2vec2_with_lm.py
==================
3f2e63685;Joao Gante;2022-03-01 14:12:58 +0000;Update TF LM examples (#15855)

==

examples/tensorflow/language-modeling/run_clm.py
examples/tensorflow/language-modeling/run_mlm.py
==================
54f0db406;Lysandre Debut;2022-03-01 14:55:11 +0100;Add PT + TF automatic builds (#15860)
* Add PT + TF automatic builds

* Apply suggestions from code review

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* Wrap up

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
==

.github/workflows/build-docker-images.yml
.github/workflows/self-scheduled.yml
docker/transformers-pytorch-gpu/Dockerfile
docker/transformers-tensorflow-gpu/Dockerfile
setup.py
==================
9863f7d22;Patrick von Platen;2022-03-01 11:26:20 +0100;[Benchmark tools] Deprecate all (#15848)
* [Benchmark tools] Deprecate all

* up
==

docs/source/benchmarks.mdx
src/transformers/benchmark/benchmark_args_utils.py
src/transformers/benchmark/benchmark_utils.py
==================
df5a4094a;Eduardo Gonzalez Ponferrada;2022-03-01 02:09:20 -0800;Add Data2Vec (#15507)
* Add data2vec model cloned from roberta

* Add checkpoint conversion script

* Fix copies

* Update docs

* Add checkpoint conversion script

* Remove fairseq data2vec_text script and fix format

* Add comment on where to get data2vec_text.py

* Remove mock implementation cheat.py and fix style

* Fix copies

* Remove TF and Flax classes from init

* Add back copy from fairseq data2vec_text.py and fix style

* Update model name in docs/source/index.mdx to be CamelCase

* Revert model name in table to lower-case to get check_table test to pass

* Update src/transformers/models/data2vec/__init__.py

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* Update src/transformers/models/data2vec/convert_data2vec_original_pytorch_checkpoint_to_pytorch.py

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* Update src/transformers/models/data2vec/modeling_data2vec.py

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* Update src/transformers/models/data2vec/modeling_data2vec.py

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* Update src/transformers/models/data2vec/modeling_data2vec.py

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* Update src/transformers/models/data2vec/modeling_data2vec.py

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* Update docs/source/model_doc/data2vec.mdx

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update docs/source/model_doc/data2vec.mdx

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/models/auto/configuration_auto.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/models/data2vec/configuration_data2vec.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/models/data2vec/modeling_data2vec.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/models/data2vec/modeling_data2vec.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/models/data2vec/modeling_data2vec.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update tests/test_modeling_data2vec.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/models/data2vec/configuration_data2vec.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/models/data2vec/modeling_data2vec.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update documentation

* Copy-paste Data2VecConfig from BertConfig

* Update config checkpoint to point to edugp/data2vec-nlp-base. Fix style and repo-consistency

* Update config special tokens to match RoBERTa

* Split multiple assertions and add individual error messages

* Rename Data2VecModel to Data2VecForTextModel

* Add Data2Vec to _toctree.yml

* Rename Data2VecEmbeddings to Data2VecForTextEmbeddings

* Add initial Data2VecForAudio model (unfinished). Only matching fairseq's implementation up to the feature encoder (before positional encoding).

* finish audio model

* finish audio file

* Update names and fix style, quality and repo consistency

* Remove Data2VecAudioForPretraining. Add tests for Data2VecAudio, mimicking the Wav2Vec2 test suite. Fix bias initilization in positional conv layers. Move back configurations for audio and text to separate files.

* add inputs to logits to data2vec'

* correct autio models

* correct config auto

* correct tok auto

* Update utils/tests_fetcher.py

* delete unnecessary files

* delete unnecessary files

* further renaming

* make all tests pass

* finish

* remove useless test file

* Update tests/test_modeling_common.py

* Update utils/check_repo.py

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* Update src/transformers/models/data2vec/modeling_data2vec_text.py

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* Fix copies

* Update docs

* Remove fairseq data2vec_text script and fix format

* Add comment on where to get data2vec_text.py

* Remove mock implementation cheat.py and fix style

* Fix copies

* Remove TF and Flax classes from init

* Add back copy from fairseq data2vec_text.py and fix style

* Update model name in docs/source/index.mdx to be CamelCase

* Revert model name in table to lower-case to get check_table test to pass

* Update documentation

* Update src/transformers/models/data2vec/__init__.py

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* Update src/transformers/models/data2vec/convert_data2vec_original_pytorch_checkpoint_to_pytorch.py

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* Update src/transformers/models/data2vec/modeling_data2vec.py

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* Update src/transformers/models/data2vec/modeling_data2vec.py

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* Update src/transformers/models/data2vec/modeling_data2vec.py

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* Update src/transformers/models/data2vec/modeling_data2vec.py

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* Update src/transformers/models/auto/configuration_auto.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/models/data2vec/configuration_data2vec.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/models/data2vec/modeling_data2vec.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/models/data2vec/modeling_data2vec.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/models/data2vec/modeling_data2vec.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update tests/test_modeling_data2vec.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/models/data2vec/configuration_data2vec.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/models/data2vec/modeling_data2vec.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Copy-paste Data2VecConfig from BertConfig

* Update config checkpoint to point to edugp/data2vec-nlp-base. Fix style and repo-consistency

* Update config special tokens to match RoBERTa

* Split multiple assertions and add individual error messages

* Rename Data2VecModel to Data2VecForTextModel

* Add Data2Vec to _toctree.yml

* Rename Data2VecEmbeddings to Data2VecForTextEmbeddings

* Add initial Data2VecForAudio model (unfinished). Only matching fairseq's implementation up to the feature encoder (before positional encoding).

* finish audio model

* finish audio file

* add inputs to logits to data2vec'

* Update names and fix style, quality and repo consistency

* Remove Data2VecAudioForPretraining. Add tests for Data2VecAudio, mimicking the Wav2Vec2 test suite. Fix bias initilization in positional conv layers. Move back configurations for audio and text to separate files.

* correct autio models

* correct config auto

* correct tok auto

* delete unnecessary files

* delete unnecessary files

* Update utils/tests_fetcher.py

* further renaming

* make all tests pass

* finish

* remove useless test file

* Update tests/test_modeling_common.py

* Update utils/check_repo.py

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* Update src/transformers/models/data2vec/modeling_data2vec_text.py

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* Move data2vec tests to new structure

* Fix test imports for text tests

* Remove fairseq files

* Change paper link to arxiv

* Modify Data2Vec documentation to reflect that the encoder is not shared across the audio and text models in the current implementation.

* Update text model checkpoint to be facebook/data2vec-text-base

* Add 'Copy from' statements and update paper links and docs

* fix copy from statements

* improve copied from

* correct more copied from statements

* finish copied from stuff

* make style

* add model to README

* add to master

Co-authored-by: Eduardo Gonzalez Ponferrada <eduardo@ferrumhealth.com>
Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

README.md
README_ko.md
README_zh-hans.md
README_zh-hant.md
docs/source/_toctree.yml
docs/source/index.mdx
docs/source/model_doc/data2vec.mdx
docs/source/serialization.mdx
src/transformers/__init__.py
src/transformers/models/__init__.py
src/transformers/models/auto/__init__.py
src/transformers/models/auto/configuration_auto.py
src/transformers/models/auto/modeling_auto.py
src/transformers/models/data2vec/__init__.py
src/transformers/models/data2vec/configuration_data2vec_audio.py
src/transformers/models/data2vec/configuration_data2vec_text.py
src/transformers/models/data2vec/convert_data2vec_audio_original_pytorch_checkpoint_to_pytorch.py
src/transformers/models/data2vec/convert_data2vec_text_original_pytorch_checkpoint_to_pytorch.py
src/transformers/models/data2vec/modeling_data2vec_audio.py
src/transformers/models/data2vec/modeling_data2vec_text.py
src/transformers/utils/dummy_pt_objects.py
tests/data2vec/__init__.py
tests/data2vec/test_modeling_data2vec_audio.py
tests/data2vec/test_modeling_data2vec_text.py
tests/test_modeling_common.py
utils/check_repo.py
==================
ddbb485c4;Patrick von Platen;2022-02-28 21:46:46 +0100;[TF-PT-Tests] Fix PyTorch - TF tests for different GPU devices (#15846)

==

tests/test_modeling_common.py
==================
97f9b8a27;Nicolas Patry;2022-02-28 21:00:21 +0100;Fixing the timestamps with chunking. (#15843)
* Fixing the timestamps with chunking.

* The changes modified (and fixed) the striding tests.

* Adding a tokenizer test.

* Update src/transformers/pipelines/automatic_speech_recognition.py

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* Defense -> comment.

* Update src/transformers/models/wav2vec2/tokenization_wav2vec2.py

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
==

src/transformers/models/wav2vec2/tokenization_wav2vec2.py
src/transformers/pipelines/automatic_speech_recognition.py
tests/pipelines/test_pipelines_automatic_speech_recognition.py
tests/wav2vec2/test_tokenization_wav2vec2.py
==================
410e26c7a;lewtun;2022-02-28 20:17:44 +0100;Fix (deprecated) ONNX exporter to account for new tf2onnx API (#15856)
* Fix (deprecated) ONNX exporter to account for new tf2onnx API
==

src/transformers/convert_graph_to_onnx.py
==================
e3342edc4;Sanchit Gandhi;2022-02-28 12:22:36 +0100;Flax Speech-Encoder-Decoder Model (#15613)
* rebase

* Delete shift tokens func

* downsample decoder input seq len for init

* correct attention mask

* add tests

* pt flax cross test

* make fixup

* init file for import

* change pt-flax cross test threshold

* pt-flax test logits only

* move tests

* make repo-consistency

* consistent indentation

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
==

docs/source/index.mdx
docs/source/model_doc/speech-encoder-decoder.mdx
src/transformers/__init__.py
src/transformers/models/auto/modeling_flax_auto.py
src/transformers/models/speech_encoder_decoder/__init__.py
src/transformers/models/speech_encoder_decoder/modeling_flax_speech_encoder_decoder.py
src/transformers/models/wav2vec2/modeling_flax_wav2vec2.py
src/transformers/utils/dummy_flax_objects.py
tests/speech_encoder_decoder/test_modeling_flax_speech_encoder_decoder.py
utils/check_repo.py
==================
935a76d90;Patrick von Platen;2022-02-28 11:23:13 +0100;[UniSpeechSat] correct unispeech sat (#15847)

==

tests/unispeech_sat/test_modeling_unispeech_sat.py
==================
84eaa6acf;Sayak Paul;2022-02-25 22:49:16 +0530;Add TFConvNextModel (#15750)
* feat: initial implementation of convnext in tensorflow.

* fix: sample code for the classification model.

* chore: added checked for  from the classification model.

* chore: set bias initializer in the classification head.

* chore: updated license terms.

* chore: removed ununsed imports

* feat: enabled  argument during using drop_path.

* chore: replaced tf.identity with layers.Activation(linear).

* chore: edited default checkpoint.

* fix: minor bugs in the initializations.

* partial-fix: tf model errors for loading pretrained pt weights.

* partial-fix: call method updated

* partial-fix: cross loading of weights (4x3 variables to be matched)

* chore: removed unneeded comment.

* removed playground.py

* rebasing

* rebasing and removing playground.py.

* fix: renaming TFConvNextStage conv and layer norm layers

* chore: added initializers and other minor additions.

* chore: added initializers and other minor additions.

* add: tests for convnext.

* fix: integration tester class.

* fix: issues mentioned in pr feedback (round 1).

* fix: how output_hidden_states arg is propoagated inside the network.

* feat: handling of  arg for pure cnn models.

* chore: added a note on equal contribution in model docs.

* rebasing

* rebasing and removing playground.py.

* feat: encapsulation for the convnext trunk.

* Fix variable naming; Test-related corrections; Run make fixup

* chore: added Joao as a contributor to convnext.

* rebasing

* rebasing and removing playground.py.

* rebasing

* rebasing and removing playground.py.

* chore: corrected copyright year and added comment on NHWC.

* chore: fixed the black version and ran formatting.

* chore: ran make style.

* chore: removed from_pt argument from test, ran make style.

* rebasing

* rebasing and removing playground.py.

* rebasing

* rebasing and removing playground.py.

* fix: tests in the convnext subclass, ran make style.

* rebasing

* rebasing and removing playground.py.

* rebasing

* rebasing and removing playground.py.

* chore: moved convnext test to the correct location

* fix: locations for the test file of convnext.

* fix: convnext tests.

* chore: applied  sgugger's suggestion for dealing w/ output_attentions.

* chore: added comments.

* chore: applied updated quality enviornment style.

* chore: applied formatting with quality enviornment.

* chore: revert to the previous tests/test_modeling_common.py.

* chore: revert to the original test_modeling_common.py

* chore: revert to previous states for test_modeling_tf_common.py and modeling_tf_utils.py

* fix: tests for convnext.

* chore: removed output_attentions argument from convnext config.

* chore: revert to the earlier tf utils.

* fix: output shapes of the hidden states

* chore: removed unnecessary comment

* chore: reverting to the right test_modeling_tf_common.py.

* Styling nits

Co-authored-by: ariG23498 <aritra.born2fly@gmail.com>
Co-authored-by: Joao Gante <joao@huggingface.co>
Co-authored-by: Sylvain Gugger <Sylvain.gugger@gmail.com>
==

docs/source/index.mdx
docs/source/model_doc/convnext.mdx
src/transformers/__init__.py
src/transformers/modeling_tf_utils.py
src/transformers/models/auto/modeling_tf_auto.py
src/transformers/models/convnext/__init__.py
src/transformers/models/convnext/configuration_convnext.py
src/transformers/models/convnext/modeling_tf_convnext.py
src/transformers/utils/dummy_tf_objects.py
tests/convnext/test_modeling_tf_convnext.py
tests/test_modeling_tf_common.py
==================
0b5bf6abe;Lysandre Debut;2022-02-25 18:00:00 +0100;Framework split model report (#15825)

==

utils/notification_service.py
==================
0118c4f6a;Sylvain Gugger;2022-02-25 17:46:38 +0100;Re-enable doctests for the quicktour (#15828)
* Re-enable doctests for the quicktour

* Re-enable doctests for task_summary (#15830)

* Remove &
==

conftest.py
docs/README.md
docs/source/quicktour.mdx
docs/source/task_summary.mdx
utils/documentation_tests.txt
==================
fd5b05eb8;Ella Charlaix;2022-02-25 17:29:35 +0100;Add ONNX Runtime quantization for text classification notebook (#15817)

==

notebooks/README.md
==================
bf1fe3282;Suraj Patil;2022-02-25 17:28:16 +0100;[examples/summarization and translation] fix readme (#15833)

==

examples/pytorch/summarization/README.md
examples/pytorch/translation/README.md
==================
8635407bc;Yih-Dar;2022-02-25 17:11:46 +0100;Fix tf.concatenate + test past_key_values for TF models (#15774)
* fix wrong method name tf.concatenate

* add tests related to causal LM / decoder

* make style and quality

* clean-up

* Fix TFBertModel's extended_attention_mask when past_key_values is provided

* Fix tests

* fix copies

* More tf.int8 -> tf.int32 in TF test template

* clean-up

* Update TF test template

* revert the previous commit + update the TF test template

* Fix TF template extended_attention_mask when past_key_values is provided

* Fix some styles manually

* clean-up

* Fix ValueError: too many values to unpack in the test

* Fix more: too many values to unpack in the test

* Add a comment for extended_attention_mask when there is past_key_values

* Fix TFElectra extended_attention_mask when past_key_values is provided

* Add tests to other TF models

* Fix for TF Electra test: add prepare_config_and_inputs_for_decoder

* Fix not passing training arg to lm_head in TFRobertaForCausalLM

* Fix tests (with past) for TF Roberta

* add testing for pask_key_values for TFElectra model

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>
==

src/transformers/models/bert/modeling_tf_bert.py
src/transformers/models/electra/modeling_tf_electra.py
src/transformers/models/layoutlm/modeling_tf_layoutlm.py
src/transformers/models/rembert/modeling_tf_rembert.py
src/transformers/models/roberta/modeling_tf_roberta.py
src/transformers/models/tapas/modeling_tf_tapas.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_tf_{{cookiecutter.lowercase_modelname}}.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/test_modeling_tf_{{cookiecutter.lowercase_modelname}}.py
tests/bert/test_modeling_tf_bert.py
tests/electra/test_modeling_tf_electra.py
tests/rembert/test_modeling_tf_rembert.py
tests/roberta/test_modeling_tf_roberta.py
==================
4818bf7ae;Pavel Belevich;2022-02-25 09:54:45 -0500;HFTracer.trace should use/return self.graph to be compatible with torch.fx.Tracer (#15824)

==

src/transformers/utils/fx.py
==================
ad0d7d174;Nicolas Patry;2022-02-25 14:06:45 +0100;Adding the option to return_timestamps on pure CTC ASR models. (#15792)
* Adding the option to return_timestamps on pure CTC ASR models.

* Remove `math.prod` which was introduced in Python 3.8

* int are not floats.

* Reworking the PR to support "char" vs "word" output.

* Fixup!

* Update src/transformers/pipelines/automatic_speech_recognition.py

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* Update src/transformers/pipelines/automatic_speech_recognition.py

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* Update src/transformers/pipelines/automatic_speech_recognition.py

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* Update src/transformers/pipelines/automatic_speech_recognition.py

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* Update src/transformers/pipelines/automatic_speech_recognition.py

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* Update src/transformers/pipelines/automatic_speech_recognition.py

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* Update src/transformers/pipelines/automatic_speech_recognition.py

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* Update src/transformers/pipelines/automatic_speech_recognition.py

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* Update src/transformers/pipelines/automatic_speech_recognition.py

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* Quality.

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
==

src/transformers/models/hubert/configuration_hubert.py
src/transformers/models/sew/configuration_sew.py
src/transformers/models/sew_d/configuration_sew_d.py
src/transformers/models/unispeech/configuration_unispeech.py
src/transformers/models/unispeech_sat/configuration_unispeech_sat.py
src/transformers/models/wav2vec2/configuration_wav2vec2.py
src/transformers/models/wavlm/configuration_wavlm.py
src/transformers/pipelines/automatic_speech_recognition.py
tests/pipelines/test_pipelines_automatic_speech_recognition.py
==================
7566734d6;Tanay Mehta;2022-02-25 18:13:56 +0530;Add model specific output classes to PoolFormer model docs (#15746)
* Added model specific output classes to poolformer docs

* Fixed Segformer typo in Poolformer docs
==

docs/source/model_doc/poolformer.mdx
==================
7963578fc;Pavel Belevich;2022-02-25 05:32:23 -0500;Fix dummy_inputs() to dummy_inputs in symbolic_trace doc (#15776)

==

src/transformers/utils/fx.py
==================
074645e32;Sylvain Gugger;2022-02-25 09:21:29 +0100;Fix semantic segmentation pipeline test (#15826)

==

tests/pipelines/test_pipelines_image_segmentation.py
==================
b7e292aeb;Lysandre Debut;2022-02-24 19:30:17 +0100;Fix the push run (#15807)

==

.github/workflows/self-push.yml
utils/notification_service_deprecated.py
==================
cbf439117;Patrick von Platen;2022-02-24 19:23:34 +0100;[TFXLNet] Correct tf xlnet generate (#15822)
* [TFXLNet] Correct tf xlnet

* adapt test comment
==

tests/xlnet/test_modeling_tf_xlnet.py
tests/xlnet/test_modeling_xlnet.py
==================
2f0f9038e;Patrick von Platen;2022-02-24 19:09:09 +0100;[Barthez Tokenizer] Fix saving (#15815)

==

src/transformers/models/barthez/tokenization_barthez.py
==================
ca57b4507;Patrick von Platen;2022-02-24 19:08:54 +0100;[Unispeech] Fix slow tests (#15818)
* remove soundfile old way of loading audio

* Adapt slow test
==

tests/unispeech/test_modeling_unispeech.py
tests/unispeech_sat/test_modeling_unispeech_sat.py
==================
35ecf99cc;Sylvain Gugger;2022-02-24 15:52:52 +0100;Revert changes in logit size for semantic segmentation models (#15722)
* Revert changes in logit size for semantic segmentation models

* Address review comments
==

src/transformers/modeling_outputs.py
src/transformers/models/beit/configuration_beit.py
src/transformers/models/beit/modeling_beit.py
src/transformers/models/segformer/configuration_segformer.py
src/transformers/models/segformer/modeling_segformer.py
tests/beit/test_modeling_beit.py
tests/segformer/test_modeling_segformer.py
==================
d1fcc90ab;Sylvain Gugger;2022-02-24 11:43:51 +0100;Fix from_pretrained with default base_model_prefix (#15814)

==

src/transformers/modeling_utils.py
tests/test_modeling_common.py
utils/test_module/custom_modeling.py
==================
7f921bcf4;Sylvain Gugger;2022-02-24 08:58:18 +0100;Fix add-new-model-like when old model checkpoint is not found (#15805)
* Fix add-new-model-like command when old checkpoint can't be recovered

* Style
==

src/transformers/commands/add_new_model_like.py
==================
bb7949b35;Lysandre Debut;2022-02-23 18:27:29 -0500;Fix model templates (#15806)
* Fix model templates

* Update paths
==

.github/workflows/add-model-like.yml
.github/workflows/model-templates.yml
src/transformers/commands/add_new_model.py
src/transformers/commands/add_new_model_like.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/test_modeling_{{cookiecutter.lowercase_modelname}}.py
==================
309e87e25;Lysandre;2022-02-23 18:01:44 -0500;Docker images should only run on a daily basis

==

.github/workflows/build-docker-images.yml
==================
c475f3ce2;Lysandre;2022-02-23 17:52:22 -0500;Scheduled tests should only run on a daily basis

==

.github/workflows/self-scheduled.yml
==================
6336017c1;Eliott C;2022-02-23 21:53:51 +0100;Fix build_documentation CI (#15803)

==

.github/workflows/build_dev_documentation.yml
.github/workflows/build_documentation.yml
==================
a0e348069;Lysandre Debut;2022-02-23 15:48:19 -0500;[Test refactor 5/5] Build docker images (#15729)

==

.github/workflows/build-docker-images.yml
docker/transformers-all-latest-gpu/Dockerfile
docker/transformers-pytorch-deepspeed-latest-gpu/Dockerfile
docker/transformers-pytorch-gpu/Dockerfile
docker/transformers-tensorflow-gpu/Dockerfile
==================
4c737f0e4;Lysandre Debut;2022-02-23 15:48:05 -0500;[Test refactor 4/5] Improve the scheduled tests (#15728)

==

.github/workflows/self-scheduled.yml
==================
d3ae2bd3c;Lysandre Debut;2022-02-23 15:46:59 -0500;[Test refactor 3/5] Notification service improvement (#15727)
* Per-folder tests reorganization

* Review comments

Co-authored-by: sgugger <sylvain.gugger@gmail.com>
Co-authored-by: Stas Bekman <stas@stason.org>
==

utils/notification_service.py
==================
0400b2263;Lysandre Debut;2022-02-23 15:46:37 -0500;[Test refactor 2/5] Tests fetcher (#15726)
* Tests fetcher

* Review comments

Co-authored-by: sgugger <sylvain.gugger@gmail.com>
Review comments
==

utils/check_repo.py
utils/tests_fetcher.py
==================
29c10a41d;Lysandre Debut;2022-02-23 15:46:28 -0500;[Test refactor 1/5] Per-folder tests reorganization (#15725)
* Per-folder tests reorganization

Co-authored-by: sgugger <sylvain.gugger@gmail.com>
Co-authored-by: Stas Bekman <stas@stason.org>
==

.github/workflows/self-scheduled.yml
conftest.py
examples/flax/test_flax_examples.py
examples/pytorch/test_pytorch_examples.py
src/transformers/commands/add_new_model.py
src/transformers/commands/add_new_model_like.py
src/transformers/testing_utils.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/test_modeling_flax_{{cookiecutter.lowercase_modelname}}.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/test_modeling_tf_{{cookiecutter.lowercase_modelname}}.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/test_modeling_{{cookiecutter.lowercase_modelname}}.py
tests/albert/__init__.py
tests/albert/test_modeling_albert.py
tests/albert/test_modeling_flax_albert.py
tests/albert/test_modeling_tf_albert.py
tests/albert/test_tokenization_albert.py
tests/auto/__init__.py
tests/auto/test_configuration_auto.py
tests/auto/test_feature_extraction_auto.py
tests/auto/test_modeling_auto.py
tests/auto/test_modeling_flax_auto.py
tests/auto/test_modeling_tf_auto.py
tests/auto/test_modeling_tf_pytorch.py
tests/auto/test_processor_auto.py
tests/auto/test_tokenization_auto.py
tests/bart/__init__.py
tests/bart/test_modeling_bart.py
tests/bart/test_modeling_flax_bart.py
tests/bart/test_modeling_tf_bart.py
tests/bart/test_tokenization_bart.py
tests/barthez/__init__.py
tests/barthez/test_tokenization_barthez.py
tests/bartpho/__init__.py
tests/bartpho/test_tokenization_bartpho.py
tests/beit/__init__.py
tests/beit/test_feature_extraction_beit.py
tests/beit/test_modeling_beit.py
tests/beit/test_modeling_flax_beit.py
tests/benchmark/__init__.py
tests/benchmark/test_benchmark.py
tests/benchmark/test_benchmark_tf.py
tests/bert/__init__.py
tests/bert/test_modeling_bert.py
tests/bert/test_modeling_flax_bert.py
tests/bert/test_modeling_tf_bert.py
tests/bert/test_tokenization_bert.py
tests/bert_generation/__init__.py
tests/bert_generation/test_modeling_bert_generation.py
tests/bert_generation/test_tokenization_bert_generation.py
tests/bert_japanese/__init__.py
tests/bert_japanese/test_tokenization_bert_japanese.py
tests/bertweet/__init__.py
tests/bertweet/test_tokenization_bertweet.py
tests/big_bird/__init__.py
tests/big_bird/test_modeling_big_bird.py
tests/big_bird/test_modeling_flax_big_bird.py
tests/big_bird/test_tokenization_big_bird.py
tests/bigbird_pegasus/__init__.py
tests/bigbird_pegasus/test_modeling_bigbird_pegasus.py
tests/blenderbot/__init__.py
tests/blenderbot/test_modeling_blenderbot.py
tests/blenderbot/test_modeling_flax_blenderbot.py
tests/blenderbot/test_modeling_tf_blenderbot.py
tests/blenderbot/test_tokenization_blenderbot.py
tests/blenderbot_small/__init__.py
tests/blenderbot_small/test_modeling_blenderbot_small.py
tests/blenderbot_small/test_modeling_flax_blenderbot_small.py
tests/blenderbot_small/test_modeling_tf_blenderbot_small.py
tests/blenderbot_small/test_tokenization_blenderbot_small.py
tests/bort/__init__.py
tests/bort/test_modeling_bort.py
tests/bort/test_modeling_tf_bort.py
tests/byt5/__init__.py
tests/byt5/test_tokenization_byt5.py
tests/camembert/__init__.py
tests/camembert/test_modeling_camembert.py
tests/camembert/test_modeling_tf_camembert.py
tests/camembert/test_tokenization_camembert.py
tests/canine/__init__.py
tests/canine/test_modeling_canine.py
tests/canine/test_tokenization_canine.py
tests/clip/__init__.py
tests/clip/test_feature_extraction_clip.py
tests/clip/test_modeling_clip.py
tests/clip/test_modeling_flax_clip.py
tests/clip/test_modeling_tf_clip.py
tests/clip/test_processor_clip.py
tests/clip/test_tokenization_clip.py
tests/convbert/__init__.py
tests/convbert/test_modeling_convbert.py
tests/convbert/test_modeling_tf_convbert.py
tests/convnext/__init__.py
tests/convnext/test_feature_extraction_convnext.py
tests/convnext/test_modeling_convnext.py
tests/cpm/__init__.py
tests/cpm/test_tokenization_cpm.py
tests/ctrl/__init__.py
tests/ctrl/test_modeling_ctrl.py
tests/ctrl/test_modeling_tf_ctrl.py
tests/ctrl/test_tokenization_ctrl.py
tests/deberta/__init__.py
tests/deberta/test_modeling_deberta.py
tests/deberta/test_modeling_tf_deberta.py
tests/deberta/test_tokenization_deberta.py
tests/deberta_v2/__init__.py
tests/deberta_v2/test_modeling_deberta_v2.py
tests/deberta_v2/test_modeling_tf_deberta_v2.py
tests/deberta_v2/test_tokenization_deberta_v2.py
tests/deepspeed/__init__.py
tests/deepspeed/test_deepspeed.py
tests/deepspeed/test_model_zoo.py
tests/deit/__init__.py
tests/deit/test_feature_extraction_deit.py
tests/deit/test_modeling_deit.py
tests/detr/__init__.py
tests/detr/test_feature_extraction_detr.py
tests/detr/test_modeling_detr.py
tests/distilbert/__init__.py
tests/distilbert/test_modeling_distilbert.py
tests/distilbert/test_modeling_flax_distilbert.py
tests/distilbert/test_modeling_tf_distilbert.py
tests/distilbert/test_tokenization_distilbert.py
tests/dpr/__init__.py
tests/dpr/test_modeling_dpr.py
tests/dpr/test_modeling_tf_dpr.py
tests/dpr/test_tokenization_dpr.py
tests/electra/__init__.py
tests/electra/test_modeling_electra.py
tests/electra/test_modeling_flax_electra.py
tests/electra/test_modeling_tf_electra.py
tests/encoder_decoder/__init__.py
tests/encoder_decoder/test_modeling_encoder_decoder.py
tests/encoder_decoder/test_modeling_flax_encoder_decoder.py
tests/encoder_decoder/test_modeling_tf_encoder_decoder.py
tests/flaubert/__init__.py
tests/flaubert/test_modeling_flaubert.py
tests/flaubert/test_modeling_tf_flaubert.py
tests/fnet/__init__.py
tests/fnet/test_modeling_fnet.py
tests/fnet/test_tokenization_fnet.py
tests/fsmt/__init__.py
tests/fsmt/test_modeling_fsmt.py
tests/fsmt/test_tokenization_fsmt.py
tests/funnel/__init__.py
tests/funnel/test_modeling_funnel.py
tests/funnel/test_modeling_tf_funnel.py
tests/funnel/test_tokenization_funnel.py
tests/generation/__init__.py
tests/generation/test_generation_beam_search.py
tests/generation/test_generation_flax_logits_process.py
tests/generation/test_generation_flax_utils.py
tests/generation/test_generation_logits_process.py
tests/generation/test_generation_stopping_criteria.py
tests/generation/test_generation_tf_logits_process.py
tests/generation/test_generation_utils.py
tests/gpt2/__init__.py
tests/gpt2/test_modeling_flax_gpt2.py
tests/gpt2/test_modeling_gpt2.py
tests/gpt2/test_modeling_tf_gpt2.py
tests/gpt2/test_tokenization_gpt2.py
tests/gpt_neo/__init__.py
tests/gpt_neo/test_modeling_flax_gpt_neo.py
tests/gpt_neo/test_modeling_gpt_neo.py
tests/gptj/__init__.py
tests/gptj/test_modeling_flax_gptj.py
tests/gptj/test_modeling_gptj.py
tests/herbert/__init__.py
tests/herbert/test_tokenization_herbert.py
tests/hubert/__init__.py
tests/hubert/test_modeling_hubert.py
tests/hubert/test_modeling_tf_hubert.py
tests/ibert/__init__.py
tests/ibert/test_modeling_ibert.py
tests/imagegpt/__init__.py
tests/imagegpt/test_feature_extraction_imagegpt.py
tests/imagegpt/test_modeling_imagegpt.py
tests/layoutlm/__init__.py
tests/layoutlm/test_modeling_layoutlm.py
tests/layoutlm/test_modeling_tf_layoutlm.py
tests/layoutlm/test_tokenization_layoutlm.py
tests/layoutlmv2/__init__.py
tests/layoutlmv2/test_feature_extraction_layoutlmv2.py
tests/layoutlmv2/test_modeling_layoutlmv2.py
tests/layoutlmv2/test_processor_layoutlmv2.py
tests/layoutlmv2/test_tokenization_layoutlmv2.py
tests/layoutxlm/__init__.py
tests/layoutxlm/test_processor_layoutxlm.py
tests/layoutxlm/test_tokenization_layoutxlm.py
tests/led/__init__.py
tests/led/test_modeling_led.py
tests/led/test_modeling_tf_led.py
tests/longformer/__init__.py
tests/longformer/test_modeling_longformer.py
tests/longformer/test_modeling_tf_longformer.py
tests/luke/__init__.py
tests/luke/test_modeling_luke.py
tests/luke/test_tokenization_luke.py
tests/lxmert/__init__.py
tests/lxmert/test_modeling_lxmert.py
tests/lxmert/test_modeling_tf_lxmert.py
tests/lxmert/test_tokenization_lxmert.py
tests/m2m_100/__init__.py
tests/m2m_100/test_modeling_m2m_100.py
tests/m2m_100/test_tokenization_m2m_100.py
tests/marian/__init__.py
tests/marian/test_modeling_flax_marian.py
tests/marian/test_modeling_marian.py
tests/marian/test_modeling_tf_marian.py
tests/marian/test_tokenization_marian.py
tests/mbart/__init__.py
tests/mbart/test_modeling_flax_mbart.py
tests/mbart/test_modeling_mbart.py
tests/mbart/test_modeling_tf_mbart.py
tests/mbart/test_tokenization_mbart.py
tests/mbart50/__init__.py
tests/mbart50/test_tokenization_mbart50.py
tests/megatron_bert/__init__.py
tests/megatron_bert/test_modeling_megatron_bert.py
tests/megatron_gpt2/__init__.py
tests/megatron_gpt2/test_modeling_megatron_gpt2.py
tests/mluke/__init__.py
tests/mluke/test_tokenization_mluke.py
tests/mobilebert/__init__.py
tests/mobilebert/test_modeling_mobilebert.py
tests/mobilebert/test_modeling_tf_mobilebert.py
tests/mpnet/__init__.py
tests/mpnet/test_modeling_mpnet.py
tests/mpnet/test_modeling_tf_mpnet.py
tests/mpnet/test_tokenization_mpnet.py
tests/mt5/__init__.py
tests/mt5/test_modeling_flax_mt5.py
tests/mt5/test_modeling_mt5.py
tests/mt5/test_modeling_tf_mt5.py
tests/nystromformer/__init__.py
tests/nystromformer/test_modeling_nystromformer.py
tests/onnx/__init__.py
tests/onnx/test_onnx.py
tests/onnx/test_onnx_v2.py
tests/openai/__init__.py
tests/openai/test_modeling_openai.py
tests/openai/test_modeling_tf_openai.py
tests/openai/test_tokenization_openai.py
tests/optimization/__init__.py
tests/optimization/test_optimization.py
tests/optimization/test_optimization_tf.py
tests/pegasus/__init__.py
tests/pegasus/test_modeling_flax_pegasus.py
tests/pegasus/test_modeling_pegasus.py
tests/pegasus/test_modeling_tf_pegasus.py
tests/pegasus/test_tokenization_pegasus.py
tests/perceiver/__init__.py
tests/perceiver/test_modeling_perceiver.py
tests/perceiver/test_tokenization_perceiver.py
tests/phobert/__init__.py
tests/phobert/test_tokenization_phobert.py
tests/pipelines/__init__.py
tests/pipelines/test_pipelines_audio_classification.py
tests/pipelines/test_pipelines_automatic_speech_recognition.py
tests/pipelines/test_pipelines_common.py
tests/pipelines/test_pipelines_conversational.py
tests/pipelines/test_pipelines_feature_extraction.py
tests/pipelines/test_pipelines_fill_mask.py
tests/pipelines/test_pipelines_image_classification.py
tests/pipelines/test_pipelines_image_segmentation.py
tests/pipelines/test_pipelines_object_detection.py
tests/pipelines/test_pipelines_question_answering.py
tests/pipelines/test_pipelines_summarization.py
tests/pipelines/test_pipelines_table_question_answering.py
tests/pipelines/test_pipelines_text2text_generation.py
tests/pipelines/test_pipelines_text_classification.py
tests/pipelines/test_pipelines_text_generation.py
tests/pipelines/test_pipelines_token_classification.py
tests/pipelines/test_pipelines_translation.py
tests/pipelines/test_pipelines_zero_shot.py
tests/pipelines/test_pipelines_zero_shot_image_classification.py
tests/plbart/__init__.py
tests/plbart/test_modeling_plbart.py
tests/plbart/test_tokenization_plbart.py
tests/poolformer/__init__.py
tests/poolformer/test_feature_extraction_poolformer.py
tests/poolformer/test_modeling_poolformer.py
tests/prophetnet/__init__.py
tests/prophetnet/test_modeling_prophetnet.py
tests/prophetnet/test_tokenization_prophetnet.py
tests/qdqbert/__init__.py
tests/qdqbert/test_modeling_qdqbert.py
tests/rag/__init__.py
tests/rag/test_modeling_rag.py
tests/rag/test_modeling_tf_rag.py
tests/rag/test_retrieval_rag.py
tests/rag/test_tokenization_rag.py
tests/realm/__init__.py
tests/realm/test_modeling_realm.py
tests/realm/test_retrieval_realm.py
tests/realm/test_tokenization_realm.py
tests/reformer/__init__.py
tests/reformer/test_modeling_reformer.py
tests/reformer/test_tokenization_reformer.py
tests/rembert/__init__.py
tests/rembert/test_modeling_rembert.py
tests/rembert/test_modeling_tf_rembert.py
tests/roberta/__init__.py
tests/roberta/test_modeling_flax_roberta.py
tests/roberta/test_modeling_roberta.py
tests/roberta/test_modeling_tf_roberta.py
tests/roberta/test_tokenization_roberta.py
tests/roformer/__init__.py
tests/roformer/test_modeling_flax_roformer.py
tests/roformer/test_modeling_roformer.py
tests/roformer/test_modeling_tf_roformer.py
tests/roformer/test_tokenization_roformer.py
tests/segformer/__init__.py
tests/segformer/test_feature_extraction_segformer.py
tests/segformer/test_modeling_segformer.py
tests/sew/__init__.py
tests/sew/test_modeling_sew.py
tests/sew_d/__init__.py
tests/sew_d/test_modeling_sew_d.py
tests/speech_encoder_decoder/__init__.py
tests/speech_encoder_decoder/test_modeling_speech_encoder_decoder.py
tests/speech_to_text/__init__.py
tests/speech_to_text/test_feature_extraction_speech_to_text.py
tests/speech_to_text/test_modeling_speech_to_text.py
tests/speech_to_text/test_modeling_tf_speech_to_text.py
tests/speech_to_text/test_processor_speech_to_text.py
tests/speech_to_text/test_tokenization_speech_to_text.py
tests/speech_to_text_2/__init__.py
tests/speech_to_text_2/test_modeling_speech_to_text_2.py
tests/speech_to_text_2/test_tokenization_speech_to_text_2.py
tests/splinter/__init__.py
tests/splinter/test_modeling_splinter.py
tests/squeezebert/__init__.py
tests/squeezebert/test_modeling_squeezebert.py
tests/squeezebert/test_tokenization_squeezebert.py
tests/swin/__init__.py
tests/swin/test_modeling_swin.py
tests/t5/__init__.py
tests/t5/test_modeling_flax_t5.py
tests/t5/test_modeling_t5.py
tests/t5/test_modeling_tf_t5.py
tests/t5/test_tokenization_t5.py
tests/tapas/__init__.py
tests/tapas/test_modeling_tapas.py
tests/tapas/test_modeling_tf_tapas.py
tests/tapas/test_tokenization_tapas.py
tests/tokenization/__init__.py
tests/tokenization/test_tokenization_fast.py
tests/tokenization/test_tokenization_utils.py
tests/trainer/__init__.py
tests/trainer/test_data_collator.py
tests/trainer/test_trainer.py
tests/trainer/test_trainer_callback.py
tests/trainer/test_trainer_distributed.py
tests/trainer/test_trainer_seq2seq.py
tests/trainer/test_trainer_tpu.py
tests/trainer/test_trainer_utils.py
tests/transfo_xl/__init__.py
tests/transfo_xl/test_modeling_tf_transfo_xl.py
tests/transfo_xl/test_modeling_transfo_xl.py
tests/transfo_xl/test_tokenization_transfo_xl.py
tests/trocr/__init__.py
tests/trocr/test_modeling_trocr.py
tests/unispeech/__init__.py
tests/unispeech/test_modeling_unispeech.py
tests/unispeech_sat/__init__.py
tests/unispeech_sat/test_modeling_unispeech_sat.py
tests/utils/__init__.py
tests/utils/test_activations.py
tests/utils/test_activations_tf.py
tests/utils/test_add_new_model_like.py
tests/utils/test_cli.py
tests/utils/test_doc_samples.py
tests/utils/test_file_utils.py
tests/utils/test_hf_argparser.py
tests/utils/test_image_utils.py
tests/utils/test_logging.py
tests/utils/test_model_card.py
tests/utils/test_model_output.py
tests/utils/test_modeling_tf_core.py
tests/utils/test_offline.py
tests/utils/test_skip_decorators.py
tests/utils/test_utils_check_copies.py
tests/utils/test_versions_utils.py
tests/vilt/__init__.py
tests/vilt/test_feature_extraction_vilt.py
tests/vilt/test_modeling_vilt.py
tests/vision_encoder_decoder/__init__.py
tests/vision_encoder_decoder/test_modeling_flax_vision_encoder_decoder.py
tests/vision_encoder_decoder/test_modeling_tf_vision_encoder_decoder.py
tests/vision_encoder_decoder/test_modeling_vision_encoder_decoder.py
tests/vision_text_dual_encoder/__init__.py
tests/vision_text_dual_encoder/test_modeling_flax_vision_text_dual_encoder.py
tests/vision_text_dual_encoder/test_modeling_vision_text_dual_encoder.py
tests/vision_text_dual_encoder/test_processor_vision_text_dual_encoder.py
tests/visual_bert/__init__.py
tests/visual_bert/test_modeling_visual_bert.py
tests/vit/__init__.py
tests/vit/test_feature_extraction_vit.py
tests/vit/test_modeling_flax_vit.py
tests/vit/test_modeling_tf_vit.py
tests/vit/test_modeling_vit.py
tests/vit_mae/__init__.py
tests/vit_mae/test_modeling_vit_mae.py
tests/wav2vec2/__init__.py
tests/wav2vec2/test_feature_extraction_wav2vec2.py
tests/wav2vec2/test_modeling_flax_wav2vec2.py
tests/wav2vec2/test_modeling_tf_wav2vec2.py
tests/wav2vec2/test_modeling_wav2vec2.py
tests/wav2vec2/test_processor_wav2vec2.py
tests/wav2vec2/test_tokenization_wav2vec2.py
tests/wav2vec2_phoneme/__init__.py
tests/wav2vec2_phoneme/test_tokenization_wav2vec2_phoneme.py
tests/wav2vec2_with_lm/__init__.py
tests/wav2vec2_with_lm/test_processor_wav2vec2_with_lm.py
tests/wavlm/__init__.py
tests/wavlm/test_modeling_wavlm.py
tests/xglm/__init__.py
tests/xglm/test_modeling_flax_xglm.py
tests/xglm/test_modeling_xglm.py
tests/xglm/test_tokenization_xglm.py
tests/xlm/__init__.py
tests/xlm/test_modeling_tf_xlm.py
tests/xlm/test_modeling_xlm.py
tests/xlm/test_tokenization_xlm.py
tests/xlm_prophetnet/__init__.py
tests/xlm_prophetnet/test_modeling_xlm_prophetnet.py
tests/xlm_prophetnet/test_tokenization_xlm_prophetnet.py
tests/xlm_roberta/__init__.py
tests/xlm_roberta/test_modeling_tf_xlm_roberta.py
tests/xlm_roberta/test_modeling_xlm_roberta.py
tests/xlm_roberta/test_tokenization_xlm_roberta.py
tests/xlm_roberta_xl/__init__.py
tests/xlm_roberta_xl/test_modeling_xlm_roberta_xl.py
tests/xlnet/__init__.py
tests/xlnet/test_modeling_tf_xlnet.py
tests/xlnet/test_modeling_xlnet.py
tests/xlnet/test_tokenization_xlnet.py
tests/yoso/__init__.py
tests/yoso/test_modeling_yoso.py
==================
fecb08c2b;Steven Liu;2022-02-23 11:58:33 -0800;üßº  NLP task guides (#15731)
* clean commit of changes to NLP tasks

* üñç apply feedback

* üìù move tf data collator in multiple choice

Co-authored-by: Steven <stevhliu@gmail.com>
==

docs/source/_toctree.yml
docs/source/tasks/language_modeling.mdx
docs/source/tasks/multiple_choice.mdx
docs/source/tasks/question_answering.mdx
docs/source/tasks/sequence_classification.mdx
docs/source/tasks/summarization.mdx
docs/source/tasks/token_classification.mdx
docs/source/tasks/translation.mdx
==================
86636f52a;Eliott C;2022-02-23 20:01:33 +0100;Fix indent in doc-builder CI (#15798)

==

.github/workflows/build_documentation.yml
==================
a1efc8236;Eliott C;2022-02-23 19:43:22 +0100;HTML dev docs (#15678)
Co-authored-by: Pierric Cistac <Pierrci@users.noreply.github.com>
==

.github/workflows/build_dev_documentation.yml
.github/workflows/build_documentation.yml
==================
3f76bf54f;lsb;2022-02-23 09:39:41 -0800;Align documentation with code defaults (#15468)
In the code, `do_normalize` defaults to True
==

src/transformers/models/wav2vec2/feature_extraction_wav2vec2.py
==================
32f5de10a;Julien Chaumond;2022-02-23 11:40:06 -0500;[doc] custom_models: mention security features of the Hub (#15768)
* custom_models: tiny doc addition

* mention security feature earlier in the section

Co-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>
==

docs/source/custom_models.mdx
==================
9e71d4645;Nicolas Patry;2022-02-23 17:20:26 +0100;Enable `image-segmentation` on `AutoModelForSemanticSegmentation` (#15647)
* Enabling Beit SegFormer to `image-segmentation`.

* Fixing the score.

* Fix import ?

* Missing in type hint.

* Multiple test fixes:

- Add `raw_image` support. It should be the default IMHO since in Python
  world it doesn't make any sense to base64 encode the image (Sorry
  @mishig, didn't catch that in my review). I really think we should
  consider breaking BC here.
- Add support for Segformer tiny test (needed
  `SegformerModelTester.get_config` to enable TinyConfig
  @NielsRogge)
- Add the check that `batch_size` works correctly on that pipeline.
  Uncovered that it doesn't for Detr, which IMO is OK since images
  after `feature_extractor` don't have the same size. Comment should
  explain.

* Type hint as a string.

* Make fixup + update black.

* torch+vision protections.

* Don't use torchvision, use F.interpolate instead (no new dep).

* Last fixes for Segformer.

* Update test to reflect new image (which was broken)

* Update tests.

* Major BC modification:

- Removed the string compressed PNG string, that's a job for users
`transformers` stays in python land.
- Removed the `score` for semantic segmentation. It has hardly a meaning
  on its own in this context.
- Don't include the grayscale with logits for now (which could enable
  users to get a sense of confidence). Might be done later.
- Don't include the surface of the mask (could be used for sorting by
  users, to filter out small masks). It's already calculable, and
  it's easier to add later, than to add now and break later if we need.

* `make fixup`.

* Small changes.

* Rebase + doc fixup.
==

src/transformers/__init__.py
src/transformers/pipelines/__init__.py
src/transformers/pipelines/image_segmentation.py
src/transformers/utils/dummy_pt_objects.py
tests/test_modeling_segformer.py
tests/test_pipelines_common.py
tests/test_pipelines_image_segmentation.py
==================
1b2397973;Suraj Patil;2022-02-23 14:51:40 +0100;[ViLT] Fix checkpoint url in config (#15790)
* [ViLT] Fix checkpoint url in config

* Apply suggestions from code review

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>
==

src/transformers/models/vilt/configuration_vilt.py
==================
de737866f;Suraj Patil;2022-02-23 14:30:05 +0100;[CLIP] fix grad ckpt (#15789)

==

src/transformers/models/clip/modeling_clip.py
==================
a3e607d19;Nicolas Patry;2022-02-23 11:51:48 +0100;Supporting Merges.txt files than contain an endline. (#15782)
(`hf-internal-testing/tiny-clip` for instance)
==

src/transformers/models/clip/tokenization_clip.py
==================
24588c673;Suraj Patil;2022-02-23 10:46:42 +0100;[M2M100, XGLM] fix create_position_ids_from_inputs_embeds (#15751)

==

src/transformers/models/m2m_100/modeling_m2m_100.py
src/transformers/models/xglm/modeling_xglm.py
==================
f9582c205;Nicolas Patry;2022-02-23 09:41:42 +0100;Adding ZeroShotImageClassificationPipeline (#12119)
* [Proposal] Adding ZeroShotImageClassificationPipeline

- Based on CLIP

* WIP, Resurection in progress.

* Resurrection... achieved.

* Reword handling different `padding_value` for `feature_extractor` and
`tokenizer`.

* Thanks doc-builder !

* Adding docs + global namespace `ZeroShotImageClassificationPipeline`.

* Fixing templates.

* Make the test pass and be robust to floating error.

* Adressing suraj's comments on docs mostly.

* Tf support start.

* TF support.

* Update src/transformers/pipelines/zero_shot_image_classification.py

Co-authored-by: Suraj Patil <surajp815@gmail.com>

Co-authored-by: Suraj Patil <surajp815@gmail.com>
==

docs/source/main_classes/pipelines.mdx
src/transformers/__init__.py
src/transformers/pipelines/__init__.py
src/transformers/pipelines/base.py
src/transformers/pipelines/zero_shot_image_classification.py
tests/test_pipelines_zero_shot_image_classification.py
==================
05a12a090;Santiago Castro;2022-02-22 18:16:38 -0500;Fix `HfArgumentParser` when passing a generator (#15758)
* Fix `HfArgumentParser` when passing a generator

* Add missing import

* Always convert `dataclass_types` into a list
==

src/transformers/hf_argparser.py
==================
db57bb2b7;Julien Chaumond;2022-02-22 15:58:05 -0500;Cleanup transformers-cli (#15767)

==

src/transformers/commands/user.py
==================
3db2e8f92;Yongrae Jo;2022-02-23 03:51:07 +0900;Fix typo on examples/pytorch/question-answering (#15644)
cna -> can
==

examples/pytorch/question-answering/README.md
==================
2cdb6dbee;Boumadane Abdelmoumene;2022-02-22 19:46:21 +0100;fixed pipeline code (#15607)
Co-authored-by: Boumadane Abdelmoumene <moumene.boumadane@gmail.com>
==
==================
c44d3675c;Patrick von Platen;2022-02-22 19:26:44 +0100;Time stamps for CTC models (#15687)
* [Wav2Vec2 Time Stamps]

* Add first version

* add word time stamps

* Fix

* save intermediate space

* improve

* [Finish CTC Tokenizer]

* remove @

* remove @

* push

* continue with phonemes

* up

* finish PR

* up

* add example

* rename

* finish

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* correct split

* finalize

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

docs/source/model_doc/wav2vec2.mdx
src/transformers/models/hubert/configuration_hubert.py
src/transformers/models/sew/configuration_sew.py
src/transformers/models/sew_d/configuration_sew_d.py
src/transformers/models/unispeech/configuration_unispeech.py
src/transformers/models/unispeech_sat/configuration_unispeech_sat.py
src/transformers/models/wav2vec2/configuration_wav2vec2.py
src/transformers/models/wav2vec2/tokenization_wav2vec2.py
src/transformers/models/wav2vec2_phoneme/tokenization_wav2vec2_phoneme.py
src/transformers/models/wavlm/configuration_wavlm.py
tests/test_tokenization_wav2vec2.py
tests/test_tokenization_wav2vec2_phoneme.py
==================
32295b15a;Funtowicz Morgan;2022-02-22 18:21:16 +0100;Gelu10 (#15676)
* Add GeLU10 (clipped version of GeLU) to transformers to improve quantization performances.

* Add unittests.

* Import tensorflow after `is_tf_available` check.

* Fix tensorflow wrong function `tf.tensor` to `tf.constant`

* style.

* use `tf.math.max`

* Fix tf tests.

* style.

* style style style style style style

* style style style style style style

* Address @sgugger comments.

* Fix wrong operator for raising ValueError for ClippedGELUActivation.
==

src/transformers/activations.py
src/transformers/activations_tf.py
tests/test_activations.py
tests/test_activations_tf.py
==================
2c3fcc647;Joao Gante;2022-02-22 11:18:35 +0000;TF train_step docstring (#15755)
* TF train_step docstring
==

src/transformers/modeling_tf_utils.py
==================
38bed912e;Francesco Saverio Zuppichini;2022-02-22 09:57:28 +0100;added link to our writing-doc document (#15756)

==

docs/source/add_new_model.mdx
==================
0187c6f0a;SaulLu;2022-02-21 18:30:11 +0100;revert temporary addition to test next version of CLIPTokenizerFast (#15717)

==

tests/test_tokenization_clip.py
==================
3956b133b;Joao Gante;2022-02-21 17:17:59 +0000;TF text classification examples (#15704)
* Working example with to_tf_dataset

* updated text_classification

* more comments
==

docs/source/main_classes/processors.mdx
examples/pytorch/text-classification/run_glue.py
examples/tensorflow/text-classification/run_glue.py
examples/tensorflow/text-classification/run_text_classification.py
src/transformers/modeling_tf_utils.py
==================
142b69f24;Kevin Ko;2022-02-22 01:31:39 +0900;Add layer_idx to CrossAttention of GPT2 model (#15730)
* Add layer_idx to CrossAttention

* Add layer_idx to crossattention of ImageGPT model
==

src/transformers/models/gpt2/modeling_gpt2.py
src/transformers/models/imagegpt/modeling_imagegpt.py
==================
86119c115;Suraj Patil;2022-02-21 16:10:59 +0100;add VisionTextDualEncoder and CLIP fine-tuning script (#15701)
* begin script

* update script

* fix features and data args

* main

* add requirements

* add column name args

* fix captions

* don't jit transforms

* fix caption

* fix labels, handle attention mask

* convert pixel values to numpy

* labels => input_ids

* transform images on the fly

* use AutoModel class, create the hybird model outside of the script

* fix version message

* add readme

* Apply suggestions from code review

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* adderss review comments

* add more comments

* allow freezing vision and text models

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
==

examples/pytorch/contrastive-image-text/README.md
examples/pytorch/contrastive-image-text/requirements.txt
examples/pytorch/contrastive-image-text/run_clip.py
==================
5444687f0;Ivan Agarsk√Ω;2022-02-21 12:41:27 +0100;Fix minor comment typos (#15740)

==

examples/research_projects/distillation/train.py
==================
a63bd3675;Simon Sardorf;2022-02-21 02:10:15 -0800;Remove input and target reset after preprocessing (#15741)
Remove input and target reset after preprocessing
==

examples/pytorch/summarization/run_summarization.py
==================
2c2a31ffb;Gunjan Chhablani;2022-02-19 01:41:42 +0530;Add missing PLBart entry in README (#15721)
* Add missing PLBart entry in index

* Fix README

* Fix README

* Fix style

* Change to master model doc
==

README.md
README_ko.md
README_zh-hans.md
README_zh-hant.md
docs/source/index.mdx
==================
60ba48205;Sanchit Gandhi;2022-02-18 18:20:24 +0100;fix bug in PT speech-encoder-decoder (#15699)
* fix bug in PT speech-encoder-decoder

* add pt test for `inputs is not None`

* fix test

* new pt test

* Update tests/test_modeling_speech_encoder_decoder.py

* make fixup

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
==

src/transformers/models/speech_encoder_decoder/modeling_speech_encoder_decoder.py
tests/test_modeling_speech_encoder_decoder.py
==================
3de12906c;Jake Tae;2022-02-18 12:00:02 -0500;fix: hfdeepspeed config argument (#15711)
`HfDeepSpeedConfig` accepts a dictionary or path to `.json` file containing DS configurations, not `TrainingArguments`.
==

src/transformers/trainer.py
==================
83f45cd65;Lysandre Debut;2022-02-18 08:50:23 -0500;Fix auto (#15706)

==

tests/test_modeling_tf_auto.py
==================
d5083c333;Sylvain Gugger;2022-02-18 14:49:53 +0100;style_doc handles decorators in examples (#15719)

==

utils/style_doc.py
==================
ae1f83502;Gunjan Chhablani;2022-02-18 18:47:09 +0530;Add PLBart (#13269)
* Init PLBART

* Add missing configuration file

* Add conversion script and configurationf ile

* Fix style

* Update modeling and conversion scripts

* Fix scale embedding in config

* Add comment

* Fix conversion script

* Add classification option to conversion script

* Fix vocab size in config doc

* Add tokenizer files from MBart50

* Allow no lang code in regular tokenizer

* Add PLBart Tokenizer Converters

* Remove mask from multi tokenizer

* Remove mask from multi tokenizer

* Change from MBart-50 to MBart tokenizer

* Fix names and modify src/tgt behavior

* Fix imports for tokenizer

* Remove <mask> from multi tokenizer

* Fix style

* Change tokenizer_class to processor_class

* Add attribute map to config class

* Update modeling file to modified MBart code

* Update configuration file to MBart style configuration

* Fix tokenizer

* Separate tokenizers

* Fix error in tokenization auto

* Copy MBart tests

* Replace with MBart tokenization tests

* Fix style

* Fix language code in multi tokenizer

* Fix configuration docs

* Add entry for plbart_multi in transformers init

* Add dummy objects and fix imports

* Fix modeling tests

* Add TODO in config

* Fix copyright year

* Fix modeling docs and test

* Fix some tokenization tests and style

* Add changes from review

* Fix copies

* Fix docs

* Fix docs

* Fix style

* Fix year

* Add changes from review

* Remove extra changes

* Fix base tokenizer and doc

* Fix style

* Fix modeling and slow tokenizer tests

* Remove Multi-tokenizer Converter and Tests

* Delete QA model and Multi Tokenizer dummy objects

* Fix repo consistency and code quality issues

* Fix example documentation

* Fix style

* Remove PLBartTokenizer from type checking in init

* Fix consistency issue

* Add changes from review

* Fix style

* Remove PLBartTokenizerFast

* Remove FastTokenizer converter

* Fix AutoTokenzier mapping

* Add plbart to toctree and fix consistency issues

* Add language codes tokenizer test

* Fix styling and doc issues

* Add fixes for failing tests

* Fix copies

* Fix failing modeling test

* Change assert to assertTrue in modeling tests
==

docs/source/_toctree.yml
docs/source/index.mdx
docs/source/model_doc/plbart.mdx
docs/source/serialization.mdx
src/transformers/__init__.py
src/transformers/models/__init__.py
src/transformers/models/auto/configuration_auto.py
src/transformers/models/auto/modeling_auto.py
src/transformers/models/auto/tokenization_auto.py
src/transformers/models/plbart/__init__.py
src/transformers/models/plbart/configuration_plbart.py
src/transformers/models/plbart/convert_plbart_original_checkpoint_to_torch.py
src/transformers/models/plbart/modeling_plbart.py
src/transformers/models/plbart/tokenization_plbart.py
src/transformers/utils/dummy_pt_objects.py
src/transformers/utils/dummy_sentencepiece_objects.py
tests/test_modeling_plbart.py
tests/test_tokenization_plbart.py
utils/check_repo.py
==================
2f2fefd6a;Yih-Dar;2022-02-18 13:56:53 +0100;Fix LongformerModel hidden states (#15537)
* add undo padding

* fix

* fix tuple issue

* make style and quality

* move unpad logic to LongformerEncoder + unpad attentions + update tests

* move unpad logic to TFLongformerEncoder

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>
==

src/transformers/models/longformer/modeling_longformer.py
src/transformers/models/longformer/modeling_tf_longformer.py
tests/test_modeling_longformer.py
tests/test_modeling_tf_longformer.py
==================
68dec6bff;Gautier Dagan;2022-02-18 12:14:44 +0000;Fix DETR model deprecation warnings for int div (#15702)

==

src/transformers/models/detr/modeling_detr.py
==================
f8ff3fad8;Yih-Dar;2022-02-18 12:20:07 +0100;TF: add initializer_std with a small value in TFFunnelModelTester (#15684)

==

tests/test_modeling_tf_funnel.py
==================
416dff736;Sylvain Gugger;2022-02-18 05:57:39 -0500;Fix SiluActivation (#15718)

==

src/transformers/activations.py
==================
e93763d42;SaulLu;2022-02-18 10:21:30 +0100;fix CLIP fast tokenizer and change some properties of the slow version (#15067)
Very big changes concerning the tokenizer fast of CLIP which did not correspond to the tokenizer slow of CLIP

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

.circleci/config.yml
setup.py
src/transformers/convert_slow_tokenizer.py
src/transformers/dependency_versions_table.py
src/transformers/file_utils.py
src/transformers/models/clip/tokenization_clip.py
src/transformers/models/clip/tokenization_clip_fast.py
tests/test_tokenization_clip.py
==================
240cc6cbd;Francesco Saverio Zuppichini;2022-02-18 09:11:18 +0100;Adding a model, more doc for pushing to the hub (#15690)
* doc for adding a model to the hub

* run make style

* resolved conversation

* removed a line

* removed )

* Update docs/source/add_new_model.mdx

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>

* Update docs/source/add_new_model.mdx

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* make style

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

docs/source/add_new_model.mdx
==================
57882177b;NielsRogge;2022-02-17 19:44:55 +0100;Add SimMIM (#15586)
* Add first draft

* Make model importable

* Make SwinForMaskedImageModeling importable

* Fix imports

* Add missing inits

* Add support for Swin

* Fix bug

* Fix bug

* Fix another bug

* Fix Swin MIM implementation

* Fix default encoder stride

* Fix Swin

* Add print statements for debugging

* Add image_size data argument

* Fix Swin

* Fix image_size

* Add print statements for debugging

* Fix print statement

* Remove print statements

* Improve reshaping of bool_masked_pos

* Add support for DeiT, fix tests

* Improve docstrings

* Apply new black version

* Improve script

* Fix bug

* Improve README

* Apply suggestions from code review

* Remove DS_Store and add to gitignore

* Apply suggestions from code review + fix BEiT Flax

* Revert BEiT changes

* Improve README

* Fix code quality

* Improve README

Co-authored-by: Niels Rogge <nielsrogge@Nielss-MBP.localdomain>
Co-authored-by: Niels Rogge <nielsrogge@Nielss-MacBook-Pro.local>
==

.gitignore
docs/source/model_doc/auto.mdx
docs/source/model_doc/deit.mdx
docs/source/model_doc/swin.mdx
docs/source/model_doc/vit.mdx
examples/pytorch/image-pretraining/README.md
examples/pytorch/image-pretraining/run_mim.py
src/transformers/__init__.py
src/transformers/models/auto/__init__.py
src/transformers/models/auto/modeling_auto.py
src/transformers/models/beit/configuration_beit.py
src/transformers/models/beit/modeling_beit.py
src/transformers/models/deit/__init__.py
src/transformers/models/deit/configuration_deit.py
src/transformers/models/deit/modeling_deit.py
src/transformers/models/swin/__init__.py
src/transformers/models/swin/configuration_swin.py
src/transformers/models/swin/modeling_swin.py
src/transformers/models/vit/__init__.py
src/transformers/models/vit/configuration_vit.py
src/transformers/models/vit/modeling_vit.py
src/transformers/utils/dummy_pt_objects.py
tests/test_modeling_common.py
tests/test_modeling_deit.py
tests/test_modeling_swin.py
tests/test_modeling_vit.py
==================
426b96230;Gunjan Chhablani;2022-02-17 19:12:14 +0530;Fix shapes in model docstrings (#15696)

==

src/transformers/models/bart/modeling_bart.py
src/transformers/models/blenderbot_small/modeling_blenderbot_small.py
src/transformers/models/marian/modeling_marian.py
==================
92a537d93;Yih-Dar;2022-02-17 14:38:32 +0100;Minor fix on README.md (#15688)
* fix README

* fix more arxiv links

* make fix-copies

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>
==

README.md
README_ko.md
README_zh-hans.md
README_zh-hant.md
docs/source/index.mdx
==================
f84e0dbd2;Tanay Mehta;2022-02-17 17:46:37 +0530;Add PoolFormer (#15531)
* Added all files, PoolFormerFeatureExtractor still failing tests

* Fixed PoolFormerFeatureExtractor not being able to import

* Completed Poolformer doc

* Applied Suggested fixes

* Fixed errors in modeling_auto.py

* Fix feature extractor, convert docs to Markdown, styling of code

* Remove PoolFormer from check_repo and fix integration test

* Remove Poolformer from check_repo

* Fixed configuration_poolformer.py docs and removed inference.py from poolformer

* Ran with black v22

* Added PoolFormer to _toctree.yml

* Updated poolformer doc

* Applied suggested fixes and added on README.md

* Did make fixup and make fix-copies, tests should pass now

* Changed PoolFormer weights conversion script name and fixed README

* Applied fixes in test_modeling_poolformer.py and modeling_poolformer.py

* Added PoolFormerFeatureExtractor to AutoFeatureExtractor API

Co-authored-by: Niels Rogge <nielsrogge@Nielss-MBP.localdomain>
==

README.md
README_ko.md
README_zh-hans.md
README_zh-hant.md
docs/source/_toctree.yml
docs/source/index.mdx
docs/source/model_doc/poolformer.mdx
src/transformers/__init__.py
src/transformers/models/__init__.py
src/transformers/models/auto/configuration_auto.py
src/transformers/models/auto/feature_extraction_auto.py
src/transformers/models/auto/modeling_auto.py
src/transformers/models/poolformer/__init__.py
src/transformers/models/poolformer/configuration_poolformer.py
src/transformers/models/poolformer/convert_poolformer_original_to_pytorch.py
src/transformers/models/poolformer/feature_extraction_poolformer.py
src/transformers/models/poolformer/modeling_poolformer.py
src/transformers/utils/dummy_pt_objects.py
src/transformers/utils/dummy_vision_objects.py
tests/test_feature_extraction_poolformer.py
tests/test_modeling_poolformer.py
==================
0e91f885c;NielsRogge;2022-02-17 13:14:01 +0100;Add image classification notebook (#15667)
Co-authored-by: Niels Rogge <nielsrogge@Nielss-MacBook-Pro.local>
==

examples/pytorch/README.md
notebooks/README.md
==================
f65fe3663;Eldar Kurtic;2022-02-16 20:37:52 +0100;Implementation of activations as pytorch modules (#15616)
* Implement activations as pytorch modules

* Apply fixup

* Add missing tests for activations

* Update docstring

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
==

src/transformers/activations.py
tests/test_activations.py
==================
66828a19b;Yih-Dar;2022-02-16 17:50:36 +0100;Fix Funnel configuration doc (#15686)
* fix doc

* make style

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>
==

src/transformers/models/funnel/configuration_funnel.py
==================
3a4376d00;Patrick von Platen;2022-02-16 17:33:33 +0100;[Wav2Vec2ProcessorWithLM] Fix auto processor with lm (#15683)

==

src/transformers/models/wav2vec2_with_lm/processing_wav2vec2_with_lm.py
tests/test_processor_wav2vec2_with_lm.py
==================
cdc51ffd2;Sylvain Gugger;2022-02-16 09:13:33 -0500;Add register method to AutoProcessor (#15669)
* Add push_to_hub method to processors

* Fix test

* The other one too!

* Add register method to AutoProcessor

* Update src/transformers/models/auto/processing_auto.py

Co-authored-by: Lysandre Debut <lysandre.debut@reseau.eseo.fr>

Co-authored-by: Lysandre Debut <lysandre.debut@reseau.eseo.fr>
==

src/transformers/models/auto/processing_auto.py
tests/test_processor_auto.py
==================
bc3379e12;Eliott C;2022-02-16 14:06:26 +0100;üî• Remove build_doc_test github action (#15680)

==

.github/workflows/build_doc_test.yml
==================
d4692ad16;Yih-Dar;2022-02-16 12:53:26 +0100;Fix dec_attn_mask in TFTransfoXLMainLayer (#15665)
* fix attn

* clean-up

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>
==

src/transformers/models/transfo_xl/modeling_tf_transfo_xl.py
==================
b87c044c7;Francesco Saverio Zuppichini;2022-02-16 10:15:13 +0100;Usage examples for logger (#15657)
* logger

* Update docs/source/main_classes/logging.mdx

Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>

* Update docs/source/main_classes/logging.mdx

Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>

Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>
==

docs/source/main_classes/logging.mdx
==================
2d02f7b29;Sylvain Gugger;2022-02-15 21:14:04 -0500;Add push_to_hub method to processors (#15668)
* Add push_to_hub method to processors

* Fix test

* The other one too!
==

src/transformers/processing_utils.py
tests/test_processor_auto.py
==================
bee361c6f;Stas Bekman;2022-02-15 16:49:57 -0800;[t5/t0/mt5 models] faster/leaner custom layer norm (#14656)
* [t5] faster/leaner custom layer norm

* wip

* apex.normalization.FusedRMSNorm

* cleanup

* cleanup

* add doc

* add catch all

* Trigger CI

* expand
==

docs/source/model_doc/t5.mdx
src/transformers/models/t5/modeling_t5.py
==================
e3d1a8dab;Santiago Castro;2022-02-15 19:12:30 -0500;Add a missing space in a deprecation message (#15651)

==

src/transformers/optimization.py
==================
1ddf3c2b7;Lysandre Debut;2022-02-15 18:55:38 -0500;Fix vit test (#15671)

==

tests/test_pipelines_image_classification.py
==================
943e2aa03;Lysandre Debut;2022-02-15 18:55:22 -0500;Fix model equivalence tests (#15670)
* Fix model equivalence tests

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

tests/test_modeling_clip.py
tests/test_modeling_common.py
tests/test_modeling_lxmert.py
==================
169031921;Yih-Dar;2022-02-15 20:15:42 +0100;Fix TFSequenceSummary's activation (#15643)
* fix TFSequenceSummary

* fix

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>
==

src/transformers/modeling_tf_utils.py
==================
faf4ff597;Stas Bekman;2022-02-15 10:13:08 -0800;[pipeline doc] fix api (#15660)
* [pipeline doc] fix api

* remove duplicate
==

src/transformers/pipelines/__init__.py
==================
2e12b907a;Patrick von Platen;2022-02-15 17:54:43 +0100;TF generate refactor - Greedy Search (#15562)
* TF generate start refactor

* Add tf tests for sample generate

* re-organize

* boom boom

* Apply suggestions from code review

* re-add

* add all code

* make random greedy pass

* make encoder-decoder random work

* further improvements

* delete bogus file

* make gpt2 and t5 tests work

* finish logits tests

* correct logits processors

* correct past / encoder_outputs drama

* refactor some methods

* another fix

* refactor shape_list

* fix more shape list

* import shape
_list

* finish docs

* fix imports

* make style

* correct tf utils

* Fix TFRag as well

* Apply Lysandre's and Sylvais suggestions

* Update tests/test_generation_tf_logits_process.py

Co-authored-by: Matt <Rocketknight1@users.noreply.github.com>

* Update src/transformers/tf_utils.py

Co-authored-by: Matt <Rocketknight1@users.noreply.github.com>

* remove cpu according to gante

* correct logit processor

Co-authored-by: Matt <Rocketknight1@users.noreply.github.com>
==

docs/source/internal/generation_utils.mdx
src/transformers/__init__.py
src/transformers/generation_flax_logits_process.py
src/transformers/generation_logits_process.py
src/transformers/generation_tf_logits_process.py
src/transformers/generation_tf_utils.py
src/transformers/modeling_tf_utils.py
src/transformers/models/albert/modeling_tf_albert.py
src/transformers/models/bart/modeling_tf_bart.py
src/transformers/models/bert/modeling_tf_bert.py
src/transformers/models/blenderbot/modeling_tf_blenderbot.py
src/transformers/models/blenderbot_small/modeling_tf_blenderbot_small.py
src/transformers/models/clip/modeling_tf_clip.py
src/transformers/models/convbert/modeling_tf_convbert.py
src/transformers/models/ctrl/modeling_tf_ctrl.py
src/transformers/models/deberta/modeling_tf_deberta.py
src/transformers/models/deberta_v2/modeling_tf_deberta_v2.py
src/transformers/models/distilbert/modeling_tf_distilbert.py
src/transformers/models/electra/modeling_tf_electra.py
src/transformers/models/encoder_decoder/modeling_tf_encoder_decoder.py
src/transformers/models/flaubert/modeling_tf_flaubert.py
src/transformers/models/funnel/modeling_tf_funnel.py
src/transformers/models/gpt2/modeling_tf_gpt2.py
src/transformers/models/hubert/modeling_tf_hubert.py
src/transformers/models/layoutlm/modeling_tf_layoutlm.py
src/transformers/models/led/modeling_tf_led.py
src/transformers/models/longformer/modeling_tf_longformer.py
src/transformers/models/marian/modeling_tf_marian.py
src/transformers/models/mbart/modeling_tf_mbart.py
src/transformers/models/mobilebert/modeling_tf_mobilebert.py
src/transformers/models/mpnet/modeling_tf_mpnet.py
src/transformers/models/openai/modeling_tf_openai.py
src/transformers/models/pegasus/modeling_tf_pegasus.py
src/transformers/models/rag/modeling_tf_rag.py
src/transformers/models/rembert/modeling_tf_rembert.py
src/transformers/models/roberta/modeling_tf_roberta.py
src/transformers/models/roformer/modeling_tf_roformer.py
src/transformers/models/speech_to_text/modeling_tf_speech_to_text.py
src/transformers/models/t5/modeling_tf_t5.py
src/transformers/models/tapas/modeling_tf_tapas.py
src/transformers/models/transfo_xl/modeling_tf_transfo_xl.py
src/transformers/models/transfo_xl/modeling_tf_transfo_xl_utilities.py
src/transformers/models/vision_encoder_decoder/modeling_tf_vision_encoder_decoder.py
src/transformers/models/vit/modeling_tf_vit.py
src/transformers/models/wav2vec2/modeling_tf_wav2vec2.py
src/transformers/models/xlm/modeling_tf_xlm.py
src/transformers/models/xlnet/modeling_tf_xlnet.py
src/transformers/tf_utils.py
src/transformers/utils/dummy_tf_objects.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_tf_{{cookiecutter.lowercase_modelname}}.py
tests/test_generation_tf_logits_process.py
tests/test_modeling_tf_common.py
tests/test_modeling_tf_gpt2.py
tests/test_modeling_tf_longformer.py
tests/test_modeling_tf_speech_to_text.py
tests/test_modeling_tf_t5.py
==================
a3dbbc346;Nicolas Patry;2022-02-15 17:53:24 +0100;Add `decoder_kwargs` to send to LM on asr pipeline. (#15646)
Co-authored-by: Giuseppe Attanasio <giuseppeattanasio6@gmail.com>

Co-authored-by: Giuseppe Attanasio <giuseppeattanasio6@gmail.com>
==

src/transformers/pipelines/automatic_speech_recognition.py
tests/test_pipelines_automatic_speech_recognition.py
==================
cdf19c501;Nicolas Patry;2022-02-15 17:49:38 +0100;Re-export `KeyDataset`. (#15645)
* Re-export `KeyDataset`.

* Update the docs locations.
==

docs/source/main_classes/pipelines.mdx
docs/source/quicktour.mdx
src/transformers/pipelines/base.py
==================
28e6155d8;Stas Bekman;2022-02-15 08:48:00 -0800;add a network debug script and document it (#15652)
* add a network debug script and document it

* doc
==

docs/source/debugging.mdx
scripts/distributed/torch-distributed-gpu-test.py
==================
5d8be090e;Sylvain Gugger;2022-02-15 11:32:26 -0500;Fix quality

==

src/transformers/generation_logits_process.py
==================
f45ac11fb;Patrick von Platen;2022-02-15 16:56:31 +0100;Add section about doc testing (#15659)
* Add doctesting section

* Improve

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

docs/README.md
==================
80f1a5916;Shamane Siri;2022-02-16 04:53:05 +1300;updated with latest PL and Ray (#15653)

==

examples/research_projects/rag/callbacks_rag.py
examples/research_projects/rag/finetune_rag.py
examples/research_projects/rag/lightning_base.py
examples/research_projects/rag/requirements.txt
==================
7bc4a01cb;Ngo Quang Huy;2022-02-15 22:44:34 +0700;Update bad_words_ids usage (#15641)
* Improve the parameter `bad_word_ids' usage

* Update the bad_words_ids strategy
==

src/transformers/generation_logits_process.py
src/transformers/generation_utils.py
==================
67047b86c;arampacha;2022-02-15 17:40:50 +0200;add scores to Wav2Vec2WithLMOutput (#15413)
* add scores to Wav2Vec2WithLMOutput

* style fixup
==

src/transformers/models/wav2vec2_with_lm/processing_wav2vec2_with_lm.py
tests/test_processor_wav2vec2_with_lm.py
==================
45f56580a;Sylvain Gugger;2022-02-15 09:44:35 -0500;Allow custom code for Processors (#15649)
* Allow custom code for Processors

* Add more test

* Test all auto_map configs are properly set
==

src/transformers/dynamic_module_utils.py
src/transformers/models/auto/processing_auto.py
src/transformers/models/auto/tokenization_auto.py
src/transformers/processing_utils.py
tests/test_feature_extraction_common.py
tests/test_processor_auto.py
tests/test_tokenization_auto.py
tests/test_tokenization_common.py
utils/test_module/custom_processing.py
==================
86a7845c0;jonrbates;2022-02-15 04:54:34 -0800;Fix typo in speech2text2 doc (#15617)
Forward looks for inputs, not input_ids
==

docs/source/model_doc/speech_to_text_2.mdx
==================
9eb7e9ba1;Javier de la Rosa;2022-02-15 13:45:08 +0100;Fix ASR pipelines from local directories with wav2vec models that have language models attached (#15590)
* Fix loading pipelines with wav2vec models with lm when in local paths

* Adding tests

* Fix test

* Adding tests

* Flake8 fixes

* Removing conflict files :(

* Adding task type to test

* Remove unnecessary test and imports
==

src/transformers/models/wav2vec2_with_lm/processing_wav2vec2_with_lm.py
src/transformers/pipelines/__init__.py
tests/test_pipelines_automatic_speech_recognition.py
tests/test_processor_wav2vec2_with_lm.py
==================
e1cbc073b;Alex Hedges;2022-02-15 05:46:12 -0500;Require tokenizers>=0.11.1 (#15266)
`tokenizers` version that supports the feature to choose the direction of truncation
==

setup.py
src/transformers/dependency_versions_table.py
==================
05a858096;fra;2022-02-15 10:46:45 +0100;Revert "logger doc"
This reverts commit 41168a49ce61685ac5c9c38cd5b88fd883c0d811.

==

docs/source/main_classes/logging.mdx
==================
41168a49c;fra;2022-02-15 10:03:28 +0100;logger doc

==

docs/source/main_classes/logging.mdx
==================
041fdc4a7;Patrick von Platen;2022-02-15 09:13:55 +0100;[SpeechEncoderDecoder] Make sure no EOS is generated in test (#15655)

==

tests/test_modeling_speech_encoder_decoder.py
==================
e314c19a3;muzhi1991;2022-02-15 09:30:55 +0800;fix bug for the log of  RNG states are not properly loaded  exception. (#15638)
Co-authored-by: muz <muzhi1991@limuzhideMBP-2.lan>
==

src/transformers/trainer.py
==================
2e11a0433;Sylvain Gugger;2022-02-14 13:35:16 -0500;Register feature extractor (#15634)
* Rework AutoFeatureExtractor.from_pretrained internal

* Custom feature extractor

* Add more tests

* Add support for custom feature extractor code

* Clean up

* Add register API to AutoFeatureExtractor
==

src/transformers/models/auto/feature_extraction_auto.py
tests/test_feature_extraction_auto.py
tests/test_feature_extraction_common.py
==================
0f71c2905;lewtun;2022-02-14 18:03:07 +0100;Remove redundant error logging in from_pretrained() method (#15631)
* Remove error logging in from_pretrained() method
==

src/transformers/configuration_utils.py
src/transformers/feature_extraction_utils.py
src/transformers/file_utils.py
src/transformers/modeling_flax_utils.py
src/transformers/modeling_tf_utils.py
src/transformers/modeling_utils.py
src/transformers/tokenization_utils_base.py
==================
b090b7902;NielsRogge;2022-02-14 17:33:35 +0100;Make Swin work with VisionEncoderDecoderModel (#15527)
* Add attribute_map

* Add mention in docs

* Set hidden_size attribute correctly

* Add note about Transformer-based models only

Co-authored-by: Niels Rogge <nielsrogge@Nielss-MBP.localdomain>
==

docs/source/model_doc/vision-encoder-decoder.mdx
src/transformers/models/swin/configuration_swin.py
==================
ec15da244;Toni Kukurin;2022-02-14 16:35:20 +0100;Report only the failed imports in `requires_backends` (#15636)

==

src/transformers/file_utils.py
==================
2b8599b2d;Zhen Wang;2022-02-14 20:18:40 +0800;Fix a bug that ignores max_seq_len in preprocess (#15238)

==

src/transformers/pipelines/question_answering.py
==================
f52746d00;Yih-Dar;2022-02-14 12:48:23 +0100;[Fix doc example] FlaxVisionEncoderDecoder (#15626)
* Fix wrong checkpoint name: vit

* Fix missing import

* Fix more missing import

* make style

* Apply suggestions from code review

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>
Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>
==

src/transformers/models/vision_encoder_decoder/modeling_flax_vision_encoder_decoder.py
==================
52d2e6f6e;Sylvain Gugger;2022-02-11 17:14:01 -0500;Add push to hub to feature extractor (#15632)
* Add push to hub to feature extractor

* Quality

* Clean up
==

src/transformers/feature_extraction_utils.py
tests/test_feature_extraction_common.py
==================
4f403ea89;Daniel Erenrich;2022-02-11 16:51:30 -0500;Fix grammar in tokenizer_summary (#15614)
"to make ensure" is redundant.
==

docs/source/tokenizer_summary.mdx
==================
7a32e4722;Sylvain Gugger;2022-02-11 16:43:54 -0500;Custom feature extractor (#15630)
* Rework AutoFeatureExtractor.from_pretrained internal

* Custom feature extractor

* Add more tests

* Add support for custom feature extractor code

* Clean up
==

src/transformers/feature_extraction_utils.py
src/transformers/models/auto/feature_extraction_auto.py
tests/test_feature_extraction_auto.py
tests/test_feature_extraction_common.py
utils/test_module/custom_feature_extraction.py
==================
fcb0f7439;Stas Bekman;2022-02-11 11:31:09 -0800;[research_projects] deal with security alerts (#15594)
* [research_projects] deal with security alerts

* add a note of the original PL ver and warning
==

examples/research_projects/pplm/README.md
examples/research_projects/pplm/requirements.txt
examples/research_projects/rag-end2end-retriever/README.md
examples/research_projects/rag-end2end-retriever/requirements.txt
examples/research_projects/rag/README.md
examples/research_projects/rag/requirements.txt
examples/research_projects/seq2seq-distillation/README.md
examples/research_projects/seq2seq-distillation/requirements.txt
==================
f15c99fab;Stas Bekman;2022-02-11 10:54:04 -0800;[deepspeed docs] misc additions (#15585)
* [deepspeed docs] round_robin_gradients

* training and/or eval/predict loss is

* Update docs/source/main_classes/deepspeed.mdx

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

docs/source/main_classes/deepspeed.mdx
==================
2dce350b3;Sylvain Gugger;2022-02-11 13:46:08 -0500;Fix _configuration_file argument getting passed to model (#15629)

==

src/transformers/configuration_utils.py
tests/test_configuration_common.py
==================
85aee09e9;Steven Liu;2022-02-11 12:33:55 -0600; üñç remove broken link (#15615)

==

docs/source/model_doc/ctrl.mdx
==================
2f40c728c;Joao Gante;2022-02-11 17:35:10 +0000;TF MT5 embeddings resize (#15567)
* Fix TF MT5 vocab resize

* more assertive testing
==

src/transformers/modeling_tf_utils.py
tests/test_modeling_tf_mt5.py
tests/test_modeling_tf_t5.py
==================
8c03df101;Mishig Davaadorj;2022-02-11 18:02:02 +0100;Rebase (#15606)

==

notebooks/README.md
==================
3fae83d23;Joao Gante;2022-02-11 16:16:26 +0000;TF: Add informative warning for inexistent CPU backprop ops (#15612)
* Add informative warning
==

src/transformers/models/hubert/modeling_tf_hubert.py
src/transformers/models/wav2vec2/modeling_tf_wav2vec2.py
==================
7e4844fc2;lewtun;2022-02-11 16:25:06 +0100;Enable ONNX export when PyTorch and TensorFlow installed in the same environment (#15625)

==

src/transformers/onnx/features.py
==================
6cf06d198;Sylvain Gugger;2022-02-11 09:55:31 -0500;Mark "code in the Hub" API as experimental (#15624)

==

docs/source/custom_models.mdx
src/transformers/configuration_utils.py
src/transformers/modeling_flax_utils.py
src/transformers/modeling_tf_utils.py
src/transformers/modeling_utils.py
src/transformers/tokenization_utils_base.py
==================
45c7b5b1c;Patrick von Platen;2022-02-10 18:29:27 +0100;[Generate] Small refactor (#15611)

==

src/transformers/generation_utils.py
==================
c0864d98b;Ngo Quang Huy;2022-02-11 00:02:03 +0700;Correct JSON format (#15600)

==

docs/source/main_classes/deepspeed.mdx
==================
2e8b85f72;lewtun;2022-02-10 16:31:00 +0100;Add local and TensorFlow ONNX export examples to docs (#15604)
* Add local and TensorFlow ONNX export examples to docs

* Use PyTorch - TensorFlow split
==

docs/source/serialization.mdx
==================
3a2ed9671;NielsRogge;2022-02-10 16:26:14 +0100;Fix Seq2SeqTrainer (#15603)
Co-authored-by: Niels Rogge <nielsrogge@Nielss-MBP.localdomain>
==

src/transformers/trainer_seq2seq.py
==================
724e51c6e;Yih-Dar;2022-02-10 15:47:02 +0100;Compute loss independent from decoder for TF EncDec models (as #14139) (#15175)
* Compute loss independent from decoder (as 14139)

* fix expected seq_len + style

* Apply the same change to TFVisionEncoderDecoderModel

* fix style

* Add case with labels in equivalence test

* uncomment

* Add case with labels in equivalence test

* add decoder_token_labels

* use hf_compute_loss

* Apply suggestions from code review

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>

* Add copied from

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>
Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>
==

src/transformers/models/encoder_decoder/modeling_tf_encoder_decoder.py
src/transformers/models/vision_encoder_decoder/modeling_tf_vision_encoder_decoder.py
tests/test_modeling_tf_encoder_decoder.py
tests/test_modeling_tf_vision_encoder_decoder.py
==================
3d5dea9bf;Patrick von Platen;2022-02-10 14:52:07 +0100;Add example batch size to all commands (#15596)

==

examples/pytorch/language-modeling/README.md
==================
cb7ed6e08;Alberto B√©gu√©;2022-02-10 10:18:41 +0000;Add Tensorflow handling of ONNX conversion (#13831)
* Add TensorFlow support for ONNX export

* Change documentation to mention conversion with Tensorflow

* Refactor export into export_pytorch and export_tensorflow

* Check model's type instead of framework installation to choose between TF and Pytorch

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
Co-authored-by: Alberto B√©gu√© <alberto.begue@della.ai>
Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>
==

docs/source/serialization.mdx
src/transformers/onnx/convert.py
src/transformers/onnx/features.py
tests/test_onnx_v2.py
utils/check_table.py
==================
e923917cd;Lysandre;2022-02-09 22:23:32 -0500;Reformat tokenization_fnet

==

tests/test_tokenization_fnet.py
==================
644ec0523;Sylvain Gugger;2022-02-09 19:10:22 -0500;Make slow tests slow

==

tests/test_tokenization_fnet.py
==================
c722753af;Sylvain Gugger;2022-02-09 17:44:28 -0500;Expand tutorial for custom models (#15587)
* Expand tutorial for custom models

* Style

* Apply suggestions from code review

Co-authored-by: Lysandre Debut <lysandre.debut@reseau.eseo.fr>

Co-authored-by: Lysandre Debut <lysandre.debut@reseau.eseo.fr>
==

docs/source/custom_models.mdx
==================
a86ee2261;NielsRogge;2022-02-09 23:33:39 +0100;Add link (#15588)
Co-authored-by: Niels Rogge <nielsrogge@Nielss-MBP.localdomain>
==

docs/source/model_doc/vit_mae.mdx
==================
dee17d567;Stas Bekman;2022-02-09 10:12:29 -0800;[trainer docs] document how to select specific gpus (#15551)
* [trainer docs] document how to select specific gpus

* expand

* add urls

* add accelerate launcher
==

docs/source/main_classes/trainer.mdx
==================
258480864;Yih-Dar;2022-02-09 18:32:51 +0100;update serving_output for some TF models (#15568)
Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>
==

src/transformers/models/led/modeling_tf_led.py
src/transformers/models/t5/modeling_tf_t5.py
src/transformers/models/wav2vec2/modeling_tf_wav2vec2.py
==================
315e67404;Sylvain Gugger;2022-02-09 12:27:59 -0500;Fix tests hub failure (#15580)
* Expose hub test problem

* Fix tests
==

tests/test_trainer.py
==================
b1ba03e08;Sylvain Gugger;2022-02-09 12:06:59 -0500;Fix quality

==

tests/test_generation_beam_search.py
==================
eed3186b7;Sylvain Gugger;2022-02-09 11:57:59 -0500;Trigger doc build

==
==================
2b5603f6a;Chan Woo Kim;2022-02-10 00:59:26 +0900;Constrained Beam Search [without disjunctive decoding] (#15416)
* added classes to get started with constrained beam search

* in progress, think i can directly force tokens now but not yet with the round robin

* think now i have total control, now need to code the bank selection

* technically works as desired, need to optimize and fix design choices leading to undersirable outputs

* complete PR #1 without disjunctive decoding

* removed incorrect tests

* Delete k.txt

* Delete test.py

* Delete test.sh

* revert changes to test scripts

* genutils

* full implementation with testing, no disjunctive yet

* shifted docs

* passing all tests realistically ran locally

* removing accidentally included print statements

* fixed source of error in initial PR test

* fixing the get_device() vs device trap

* fixed documentation docstrings about constrained_beam_search

* fixed tests having failing for Speech2TextModel's floating point inputs

* fix cuda long tensor

* added examples and testing for them and founx & fixed a bug in beam_search and constrained_beam_search

* deleted accidentally added test halting code with assert False

* code reformat

* Update tests/test_generation_utils.py

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* Update tests/test_generation_utils.py

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* Update tests/test_generation_utils.py

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* Update tests/test_generation_utils.py

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* Update tests/test_generation_utils.py

* fixing based on comments on PR

* took out the testing code that should but work fails without the beam search moditification ; style changes

* fixing comments issues

* docstrings for ConstraintListState

* typo in PhrsalConstraint docstring

* docstrings improvements

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
==

docs/source/internal/generation_utils.mdx
src/transformers/__init__.py
src/transformers/generation_beam_constraints.py
src/transformers/generation_beam_search.py
src/transformers/generation_utils.py
src/transformers/utils/dummy_pt_objects.py
tests/test_generation_beam_search.py
tests/test_generation_utils.py
==================
0113aae5b;Clara Meister;2022-02-09 16:48:41 +0100;Add implementation of typical sampling (#15504)
* typical decoding

* changing arg name

* add test config params

* forgotten arg rename

* fix edge case where scores are same

* test for typical logits warper

* code quality fixes
==

src/transformers/configuration_utils.py
src/transformers/generation_logits_process.py
src/transformers/generation_utils.py
tests/test_configuration_common.py
tests/test_generation_logits_process.py
==================
f588cf405;Suraj Patil;2022-02-09 16:48:08 +0100;[Flax tests/FlaxBert] make from_pretrained test faster (#15561)

==

tests/test_modeling_flax_bert.py
==================
702924092;Lysandre Debut;2022-02-09 10:28:43 -0500;Upgrade click version (#15579)

==

.github/workflows/add-model-like.yml
==================
9e00566b9;Sanchit Gandhi;2022-02-09 10:24:40 -0500;Add Wav2Vec2 Adapter Weights to Flax (#15566)
* Add Wav2Vec2 Adapter Weights to Flax

* Suggested changes
==

src/transformers/models/wav2vec2/modeling_flax_wav2vec2.py
==================
1f60bc46f;Sylvain Gugger;2022-02-09 10:04:44 -0500;Make sure custom configs work with Transformers (#15569)
* Make sure custom configs work with Transformers

* Apply code review suggestions
==

src/transformers/configuration_utils.py
src/transformers/modeling_utils.py
tests/test_modeling_common.py
utils/test_module/custom_configuration.py
utils/test_module/custom_modeling.py
==================
7732d0fe7;Lysandre Debut;2022-02-09 09:28:57 -0500;Upgrade black to version ~=22.0 (#15565)
* Upgrade black to version ~=22.0

* Check copies

* Fix code
==

examples/pytorch/speech-pretraining/run_wav2vec2_pretraining_no_trainer.py
examples/research_projects/distillation/run_squad_w_distillation.py
examples/research_projects/jax-projects/wav2vec2/run_wav2vec2_pretrain_flax.py
examples/research_projects/lxmert/modeling_frcnn.py
examples/research_projects/movement-pruning/masked_run_glue.py
examples/research_projects/movement-pruning/masked_run_squad.py
examples/research_projects/onnx/summarization/bart_onnx/generation_onnx.py
examples/research_projects/pplm/run_pplm.py
examples/research_projects/visual_bert/modeling_frcnn.py
examples/research_projects/wav2vec2/run_pretrain.py
setup.py
src/transformers/commands/add_new_model_like.py
src/transformers/dependency_versions_table.py
src/transformers/generation_beam_search.py
src/transformers/generation_flax_utils.py
src/transformers/generation_tf_utils.py
src/transformers/modeling_tf_utils.py
src/transformers/models/bart/modeling_bart.py
src/transformers/models/bart/modeling_tf_bart.py
src/transformers/models/bart/tokenization_bart.py
src/transformers/models/big_bird/modeling_big_bird.py
src/transformers/models/big_bird/modeling_flax_big_bird.py
src/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py
src/transformers/models/blenderbot/modeling_blenderbot.py
src/transformers/models/blenderbot/modeling_tf_blenderbot.py
src/transformers/models/blenderbot_small/modeling_blenderbot_small.py
src/transformers/models/blenderbot_small/modeling_tf_blenderbot_small.py
src/transformers/models/byt5/tokenization_byt5.py
src/transformers/models/clip/modeling_clip.py
src/transformers/models/clip/modeling_flax_clip.py
src/transformers/models/clip/modeling_tf_clip.py
src/transformers/models/clip/tokenization_clip.py
src/transformers/models/detr/modeling_detr.py
src/transformers/models/fsmt/modeling_fsmt.py
src/transformers/models/funnel/modeling_funnel.py
src/transformers/models/funnel/modeling_tf_funnel.py
src/transformers/models/gpt2/tokenization_gpt2.py
src/transformers/models/hubert/modeling_hubert.py
src/transformers/models/hubert/modeling_tf_hubert.py
src/transformers/models/ibert/quant_modules.py
src/transformers/models/led/modeling_led.py
src/transformers/models/led/modeling_tf_led.py
src/transformers/models/longformer/modeling_tf_longformer.py
src/transformers/models/m2m_100/modeling_m2m_100.py
src/transformers/models/marian/modeling_marian.py
src/transformers/models/marian/modeling_tf_marian.py
src/transformers/models/mbart/modeling_mbart.py
src/transformers/models/mbart/modeling_tf_mbart.py
src/transformers/models/pegasus/modeling_pegasus.py
src/transformers/models/pegasus/modeling_tf_pegasus.py
src/transformers/models/perceiver/modeling_perceiver.py
src/transformers/models/perceiver/tokenization_perceiver.py
src/transformers/models/prophetnet/modeling_prophetnet.py
src/transformers/models/reformer/modeling_reformer.py
src/transformers/models/roberta/tokenization_roberta.py
src/transformers/models/sew/modeling_sew.py
src/transformers/models/speech_to_text/feature_extraction_speech_to_text.py
src/transformers/models/speech_to_text/modeling_speech_to_text.py
src/transformers/models/speech_to_text/modeling_tf_speech_to_text.py
src/transformers/models/speech_to_text_2/modeling_speech_to_text_2.py
src/transformers/models/swin/modeling_swin.py
src/transformers/models/t5/modeling_flax_t5.py
src/transformers/models/t5/modeling_t5.py
src/transformers/models/t5/modeling_tf_t5.py
src/transformers/models/tapas/modeling_tapas.py
src/transformers/models/transfo_xl/modeling_tf_transfo_xl.py
src/transformers/models/transfo_xl/modeling_tf_transfo_xl_utilities.py
src/transformers/models/transfo_xl/modeling_transfo_xl.py
src/transformers/models/transfo_xl/modeling_transfo_xl_utilities.py
src/transformers/models/trocr/modeling_trocr.py
src/transformers/models/unispeech/modeling_unispeech.py
src/transformers/models/unispeech_sat/modeling_unispeech_sat.py
src/transformers/models/vit_mae/modeling_vit_mae.py
src/transformers/models/wav2vec2/modeling_tf_wav2vec2.py
src/transformers/models/wav2vec2/modeling_wav2vec2.py
src/transformers/models/wavlm/modeling_wavlm.py
src/transformers/models/xglm/modeling_xglm.py
src/transformers/models/xlm/configuration_xlm.py
src/transformers/models/xlnet/modeling_tf_xlnet.py
src/transformers/models/xlnet/modeling_xlnet.py
src/transformers/models/yoso/modeling_yoso.py
src/transformers/optimization.py
src/transformers/pipelines/audio_utils.py
tests/test_generation_beam_search.py
tests/test_modeling_ibert.py
tests/test_modeling_swin.py
tests/test_modeling_vit_mae.py
tests/test_utils_check_copies.py
utils/check_copies.py
utils/get_modified_files.py
utils/style_doc.py
==================
d923f7620;Leandro von Werra;2022-02-09 15:27:30 +0100;add model scaling section (#15119)
* add model scaling section

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* integrate reviewer feedback

* initialize GPU properly

* add note about BnB optimizer

* move doc from `scaling.mdx` to `performance.mdx`

* integrate reviewer feedback

* revert section levels

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

docs/source/performance.mdx
==================
b5c6fdecf;Sylvain Gugger;2022-02-09 09:24:49 -0500;PoC for a ProcessorMixin class (#15549)
* PoC for a ProcessorMixin class

* Documentation

* Apply suggestions from code review

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>
Co-authored-by: Suraj Patil <surajp815@gmail.com>
Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* Roll out to other processors

* Add base feature extractor class in init

* Use args and kwargs

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>
Co-authored-by: Suraj Patil <surajp815@gmail.com>
Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
==

docs/source/main_classes/processors.mdx
src/transformers/__init__.py
src/transformers/models/clip/processing_clip.py
src/transformers/models/layoutlmv2/processing_layoutlmv2.py
src/transformers/models/layoutxlm/processing_layoutxlm.py
src/transformers/models/speech_to_text/processing_speech_to_text.py
src/transformers/models/speech_to_text_2/processing_speech_to_text_2.py
src/transformers/models/trocr/processing_trocr.py
src/transformers/models/vilt/processing_vilt.py
src/transformers/models/vision_text_dual_encoder/processing_vision_text_dual_encoder.py
src/transformers/models/wav2vec2/processing_wav2vec2.py
src/transformers/models/wav2vec2_with_lm/processing_wav2vec2_with_lm.py
src/transformers/processing_utils.py
==================
ba3f9a71a;Yih-Dar;2022-02-09 14:20:05 +0100;logger.warn --> logger.warning (#15572)
* change logger.warn to logger.warning

* make style

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>
==

src/transformers/configuration_utils.py
src/transformers/modeling_utils.py
src/transformers/models/auto/auto_factory.py
src/transformers/models/auto/configuration_auto.py
src/transformers/models/auto/tokenization_auto.py
src/transformers/models/megatron_bert/modeling_megatron_bert.py
src/transformers/models/rembert/modeling_rembert.py
src/transformers/models/t5/modeling_t5.py
src/transformers/trainer.py
==================
a6885db91;Suraj Patil;2022-02-09 12:26:48 +0100;[Flax tests] fix test_model_outputs_equivalence (#15571)
* fix test_model_outputs_equivalence

* fix tuple outputs for blenderbot
==

src/transformers/models/blenderbot/modeling_flax_blenderbot.py
tests/test_modeling_flax_common.py
==================
fcb4f11c9;Nathan Raw;2022-02-08 14:10:53 -0500;:memo: Add codecarbon callback to docs (#15563)

==

docs/source/main_classes/callback.mdx
==================
077c00c0b;Boris Dayma;2022-02-08 10:53:22 -0600;feat(flax): allow encoder_outputs in generate (#15554)
* feat(flax): allow encoder_outputs in generate

* doc(flax): encoder_outputs in generate

* fix: style

* fix: style
==

src/transformers/generation_flax_utils.py
==================
8406fa6dd;Joao Gante;2022-02-08 16:27:23 +0000;Add TFSpeech2Text (#15113)
* Add wrapper classes

* convert inner layers to tf

* Add TF Encoder and Decoder layers

* TFSpeech2Text models

* Loadable model

* TF model with same outputs as PT model

* test skeleton

* correct tests and run the fixup

* correct attention expansion

* TFSpeech2Text pask_key_values with TF format
==

docs/source/index.mdx
docs/source/model_doc/auto.mdx
docs/source/model_doc/speech_to_text.mdx
src/transformers/__init__.py
src/transformers/generation_tf_utils.py
src/transformers/modeling_tf_pytorch_utils.py
src/transformers/models/auto/__init__.py
src/transformers/models/auto/modeling_auto.py
src/transformers/models/auto/modeling_tf_auto.py
src/transformers/models/bart/modeling_tf_bart.py
src/transformers/models/blenderbot/modeling_tf_blenderbot.py
src/transformers/models/blenderbot_small/modeling_tf_blenderbot_small.py
src/transformers/models/hubert/modeling_tf_hubert.py
src/transformers/models/marian/modeling_tf_marian.py
src/transformers/models/mbart/modeling_tf_mbart.py
src/transformers/models/pegasus/modeling_tf_pegasus.py
src/transformers/models/speech_to_text/__init__.py
src/transformers/models/speech_to_text/modeling_tf_speech_to_text.py
src/transformers/models/wav2vec2/modeling_tf_wav2vec2.py
src/transformers/utils/dummy_tf_objects.py
tests/test_modeling_common.py
tests/test_modeling_tf_common.py
tests/test_modeling_tf_speech_to_text.py
==================
6a5472a8e;Yih-Dar;2022-02-08 16:20:53 +0100;Force use_cache to be False in PyTorch (#15385)
* use_cache = False for PT models if labels is passed

* Fix for BigBirdPegasusForConditionalGeneration

* add warning if users specify use_cache=True

* Use logger.warning instead of warnings.warn

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>
==

src/transformers/models/bart/modeling_bart.py
src/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py
src/transformers/models/blenderbot/modeling_blenderbot.py
src/transformers/models/blenderbot_small/modeling_blenderbot_small.py
src/transformers/models/led/modeling_led.py
src/transformers/models/marian/modeling_marian.py
src/transformers/models/mbart/modeling_mbart.py
src/transformers/models/pegasus/modeling_pegasus.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_{{cookiecutter.lowercase_modelname}}.py
==================
0acd84f7c;Suraj Patil;2022-02-08 15:54:19 +0100;[GPTJ] fix docs (#15558)

==

src/transformers/models/gptj/modeling_flax_gptj.py
src/transformers/models/gptj/modeling_gptj.py
==================
87d08afb1;aaron;2022-02-08 06:47:49 -0800;electra is added to onnx supported model (#15084)
* electra is added to onnx supported model

* add google/electra-base-generator for test onnx module

Co-authored-by: Lewis Tunstall <lewis.c.tunstall@gmail.com>
==

docs/source/serialization.mdx
src/transformers/models/electra/__init__.py
src/transformers/models/electra/configuration_electra.py
src/transformers/onnx/features.py
tests/test_onnx_v2.py
==================
0fe17f375;Michael Benayoun;2022-02-07 22:25:33 +0100;FX tracing improvement (#14321)
* Change the way tracing happens, enabling dynamic axes out of the box

* Update the tests and modeling xlnet

* Add the non recoding of leaf modules to avoid recording more values for the methods to record than what will be seen at tracing time (which would otherwise desynchronize the recorded values and the values that need to be given to the proxies during tracing, causing errors).

* Comments and making tracing work for gpt-j and xlnet

* Refactore things related to num_choices (and batch_size, sequence_length)

* Update fx to work on PyTorch 1.10

* Postpone autowrap_function feature usage for later

* Add copyrights

* Remove unnecessary file

* Fix issue with add_new_model_like

* Apply suggestions
==

src/transformers/commands/add_new_model_like.py
src/transformers/file_utils.py
src/transformers/modeling_utils.py
src/transformers/models/albert/modeling_albert.py
src/transformers/models/bert/modeling_bert.py
src/transformers/models/electra/modeling_electra.py
src/transformers/models/gpt2/modeling_gpt2.py
src/transformers/models/gpt_neo/modeling_gpt_neo.py
src/transformers/models/gptj/modeling_gptj.py
src/transformers/models/layoutlm/modeling_layoutlm.py
src/transformers/models/megatron_bert/modeling_megatron_bert.py
src/transformers/models/mobilebert/modeling_mobilebert.py
src/transformers/models/realm/modeling_realm.py
src/transformers/models/roberta/modeling_roberta.py
src/transformers/models/splinter/modeling_splinter.py
src/transformers/models/xlm_roberta_xl/modeling_xlm_roberta_xl.py
src/transformers/utils/fx.py
src/transformers/utils/fx_transformations.py
tests/test_modeling_albert.py
tests/test_modeling_bert.py
tests/test_modeling_common.py
tests/test_modeling_distilbert.py
tests/test_modeling_electra.py
tests/test_modeling_gpt2.py
tests/test_modeling_gpt_neo.py
tests/test_modeling_gptj.py
tests/test_modeling_megatron_bert.py
tests/test_modeling_mobilebert.py
tests/test_modeling_roberta.py
tests/test_modeling_t5.py
tests/test_modeling_xlnet.py
==================
552f8d309;Steven Liu;2022-02-07 12:34:56 -0600;Create a custom model guide (#15489)
* üìù add config section

* üìù finish first draft

* üìù add feature extractor and processor

* üñç apply feedback from review

* üìù minor edits

* last review
==

docs/source/_toctree.yml
docs/source/create_a_model.mdx
==================
ad1d3c4d4;Yih-Dar;2022-02-07 18:09:57 +0100;Make TF Wav2Vec2 outputs the same as PT's version (#15530)
* fix outputs

* fix for CTC

* fix doc

* make style

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>
==

src/transformers/models/wav2vec2/modeling_tf_wav2vec2.py
==================
131e25841;Yih-Dar;2022-02-07 17:41:48 +0100;Fix TF T5/LED missing cross attn in retrun values (#15511)
* add cross attn to outputs

* add cross attn to outputs for TFLED

* add undo padding

* remove unused import

* fix style

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>
==

src/transformers/models/led/modeling_tf_led.py
src/transformers/models/t5/modeling_tf_t5.py
tests/test_modeling_tf_led.py
==================
6775b211b;lewtun;2022-02-07 17:32:13 +0100;Remove Longformers from ONNX-supported models (#15273)

==

docs/source/serialization.mdx
src/transformers/models/longformer/configuration_longformer.py
src/transformers/onnx/features.py
tests/test_onnx_v2.py
utils/check_table.py
==================
7a1412e12;Fran√ßois REMY;2022-02-07 17:03:12 +0100;Wav2Vec2 models must either throw or deal with add_apater (#15409)
* Wav2Vec2 models must either throw or deal with add_apater

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* Add pre-add_adapter backwards compatibility

* Add pre-add_adapter backwards compatibility

* Fix issue in tests/test_modeling_wav2vec2.py

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
==

src/transformers/models/hubert/modeling_hubert.py
src/transformers/models/sew/modeling_sew.py
src/transformers/models/sew_d/modeling_sew_d.py
src/transformers/models/unispeech/modeling_unispeech.py
src/transformers/models/unispeech_sat/modeling_unispeech_sat.py
src/transformers/models/wav2vec2/modeling_wav2vec2.py
src/transformers/models/wavlm/modeling_wavlm.py
tests/test_modeling_wav2vec2.py
==================
a459f7f97;Anton Lozhkov;2022-02-07 18:35:37 +0300;Add ASR CTC streaming example  (#15309)
* Single-epoch run

* Apply suggestions from code review

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* Infinite dataset

* Trainer fix + distributed benchmark

* Benchmark fix

* unused import

* interleaved splits

* interleaved splits

* has_length util

* Move to research projects

* Leftover Sized checks

* Bump min version

* Unused import

* Revert trainer changes

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
==

examples/pytorch/speech-recognition/README.md
examples/pytorch/speech-recognition/requirements.txt
examples/pytorch/speech-recognition/run_speech_recognition_ctc.py
examples/pytorch/speech-recognition/run_speech_recognition_seq2seq.py
examples/research_projects/robust-speech-event/run_speech_recognition_ctc_streaming.py
==================
75b13f82e;Anton Lozhkov;2022-02-07 18:34:56 +0300;[Trainer] Deeper length checks for IterableDatasetShard (#15539)
* Unused import

* Make `has_length()` torch-independent to use in callbacks

* Update src/transformers/trainer_utils.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

src/transformers/trainer.py
src/transformers/trainer_callback.py
src/transformers/trainer_utils.py
==================
84eec9e6b;NielsRogge;2022-02-07 16:11:37 +0100;Add ConvNeXT (#15277)
* First draft

* Add conversion script

* Improve conversion script

* Improve docs and implement tests

* Define model output class

* Fix tests

* Fix more tests

* Add model to README

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Apply more suggestions from code review

* Apply suggestions from code review

* Rename dims to hidden_sizes

* Fix equivalence test

* Rename gamma to gamma_parameter

* Clean up conversion script

* Add ConvNextFeatureExtractor

* Add corresponding tests

* Implement feature extractor correctly

* Make implementation cleaner

* Add ConvNextStem class

* Improve design

* Update design to also include encoder

* Fix gamma parameter

* Use sample docstrings

* Finish conversion, add center cropping

* Replace nielsr by facebook, make feature extractor tests smaller

* Fix integration test

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

README.md
README_ko.md
README_zh-hans.md
README_zh-hant.md
docs/source/_toctree.yml
docs/source/index.mdx
docs/source/model_doc/convnext.mdx
src/transformers/__init__.py
src/transformers/models/__init__.py
src/transformers/models/auto/configuration_auto.py
src/transformers/models/auto/feature_extraction_auto.py
src/transformers/models/auto/modeling_auto.py
src/transformers/models/convnext/__init__.py
src/transformers/models/convnext/configuration_convnext.py
src/transformers/models/convnext/convert_convnext_to_pytorch.py
src/transformers/models/convnext/feature_extraction_convnext.py
src/transformers/models/convnext/modeling_convnext.py
src/transformers/utils/dummy_pt_objects.py
src/transformers/utils/dummy_vision_objects.py
tests/test_feature_extraction_convnext.py
tests/test_modeling_convnext.py
==================
c47d25924;Patrick von Platen;2022-02-07 16:04:18 +0100;[torch_int_div] Correct true division in generation (#15498)
* [torch_int_div] Correct true division in generation

* up

* up
==

src/transformers/__init__.py
src/transformers/generation_utils.py
src/transformers/modeling_utils.py
src/transformers/models/hubert/modeling_hubert.py
src/transformers/models/sew/modeling_sew.py
src/transformers/models/sew_d/modeling_sew_d.py
src/transformers/models/unispeech/modeling_unispeech.py
src/transformers/models/unispeech_sat/modeling_unispeech_sat.py
src/transformers/models/wav2vec2/modeling_wav2vec2.py
src/transformers/models/wavlm/modeling_wavlm.py
src/transformers/pytorch_utils.py
==================
5f1918a4a;Patrick von Platen;2022-02-07 15:35:44 +0100;[ASR pipeline] correct asr pipeline for seq2seq models (#15541)

==

src/transformers/pipelines/automatic_speech_recognition.py
tests/test_pipelines_automatic_speech_recognition.py
==================
e02bdce79;Patrick von Platen;2022-02-07 12:33:49 +0100;Revert "Handle PyTorch to Flax conversion of 1D convolutions (#15519)" (#15540)
This reverts commit 854a0d526c7a3b958a790e92272ac798ca3831f5.
==

src/transformers/modeling_flax_pytorch_utils.py
==================
8ce133063;Stas Bekman;2022-02-04 13:51:02 -0800;[deepspeed docs] DeepSpeed ZeRO Inference (#15486)
* [deepspeed docs] DeepSpeed ZeRO Inference

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* tweak

* deal with black

* extra cleanup, better comments

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

docs/source/main_classes/deepspeed.mdx
==================
ac6aa10f2;Sylvain Gugger;2022-02-04 14:52:07 -0500;Standardize semantic segmentation models outputs (#15469)
* Standardize instance segmentation models outputs

* Rename output

* Update src/transformers/modeling_outputs.py

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>

* Add legacy argument to the config and model forward

* Update src/transformers/models/beit/modeling_beit.py

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* Copy fix in Segformer

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>
Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
==

docs/source/model_doc/auto.mdx
src/transformers/__init__.py
src/transformers/modeling_outputs.py
src/transformers/models/auto/__init__.py
src/transformers/models/auto/modeling_auto.py
src/transformers/models/beit/configuration_beit.py
src/transformers/models/beit/modeling_beit.py
src/transformers/models/segformer/configuration_segformer.py
src/transformers/models/segformer/modeling_segformer.py
src/transformers/utils/dummy_pt_objects.py
tests/test_modeling_beit.py
tests/test_modeling_segformer.py
utils/check_repo.py
==================
31be2f45a;Stas Bekman;2022-02-04 11:15:13 -0800;[deepspeed docs] Megatron-Deepspeed info (#15488)

==

docs/source/parallelism.mdx
==================
bbe9c6981;Yih-Dar;2022-02-04 17:32:14 +0100;Fix TFRemBertEncoder all_hidden_states (#15510)
* fix

* fix test

* remove expected_num_hidden_layers

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>
==

src/transformers/models/rembert/modeling_tf_rembert.py
tests/test_modeling_tf_rembert.py
==================
854a0d526;Sanchit Gandhi;2022-02-04 17:08:03 +0100;Handle PyTorch to Flax conversion of 1D convolutions (#15519)

==

src/transformers/modeling_flax_pytorch_utils.py
==================
486260c68;Yih-Dar;2022-02-04 16:25:37 +0100;use kwargs (#15509)
Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>
==

src/transformers/models/electra/modeling_tf_electra.py
==================
525dbbf84;Yih-Dar;2022-02-03 21:39:46 +0100;Remove loss from some flax models docs & examples (#15492)
* Remove return_loss from Flax models

* fix more

* fix

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>
==

src/transformers/models/clip/modeling_flax_clip.py
src/transformers/models/vision_text_dual_encoder/modeling_flax_vision_text_dual_encoder.py
==================
21dcaec5d;Stas Bekman;2022-02-03 10:55:14 -0800;[deepspeed docs] memory requirements (#15506)

==

docs/source/main_classes/deepspeed.mdx
==================
f1a4c4ead;davidleonfdez;2022-02-03 17:07:20 +0000;[WIP] Add preprocess_logits_for_metrics Trainer param (#15473)
* Add preprocess_logits_for_metrics Trainer param

* Compute accuracy in LM examples

* Improve comments
==

examples/pytorch/language-modeling/run_clm.py
examples/pytorch/language-modeling/run_mlm.py
src/transformers/trainer.py
tests/test_trainer.py
==================
4f5faaf04;Stas Bekman;2022-02-03 08:55:45 -0800;[deepspeed] fix a bug in a test (#15493)
* [deepspeed] fix a bug in a test

* consistency
==

tests/deepspeed/test_deepspeed.py
==================
90166121e;NielsRogge;2022-02-03 17:47:22 +0100;Add general vision docstrings (#15501)
* Add general docstrings

* Remove legacy docstrings

* Add BEiT

* Add DEiT

* Add SegFormer

* Fix beit output class

* Fix missing return_dict
==

src/transformers/file_utils.py
src/transformers/models/beit/modeling_beit.py
src/transformers/models/deit/modeling_deit.py
src/transformers/models/segformer/modeling_segformer.py
src/transformers/models/swin/modeling_swin.py
src/transformers/models/vit/modeling_vit.py
==================
e2b6e73fa;Patrick von Platen;2022-02-03 17:12:14 +0100;[Flax tests] Disable scheduled GPU tests (#15503)

==

.github/workflows/self-scheduled.yml
==================
f5d98da29;Yih-Dar;2022-02-03 16:11:53 +0100;fix load_weight_prefix (#15101)
Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>
==

src/transformers/models/encoder_decoder/modeling_tf_encoder_decoder.py
tests/test_modeling_tf_encoder_decoder.py
==================
71dccd077;Yih-Dar;2022-02-03 12:57:28 +0100;fix (#15494)
Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>
==

src/transformers/models/marian/modeling_tf_marian.py
==================
5ec368d79;CHI LIU;2022-02-03 07:24:40 +0800;Correct eos_token_id settings in generate (#15403)
* Correct eos_token_id set in generate

* Set eos_token_id in test

* Correct eos_token_id set in generate

* Set eos_token_id in test
==

src/transformers/generation_utils.py
tests/test_modeling_encoder_decoder.py
==================
39b5d1a63;SaulLu;2022-02-02 23:18:09 +0100;fix set truncation attribute in `__init__` of `PreTrainedTokenizerBase` (#15456)
* change truncation_side in init of `PreTrainedTokenizerBase`

Co-authored-by: LSinev <LSinev@users.noreply.github.com>

* add test

* Revert "replace assert with exception for `padding_side` arg in `PreTrainedTokenizerBase` `__init__`"

This reverts commit 7a98b87962d2635c7e4d4f00db3948b694624843.

* fix kwargs

* Revert "fix kwargs"

This reverts commit 67b0a5270e8cf1dbf70e6b0232e94c0452b6946f.

* Update tests/test_tokenization_common.py

Co-authored-by: Nicolas Patry <patry.nicolas@protonmail.com>

* delete truncation_side variable

* reorganize test

* format

* complete doc

* Revert "Revert "replace assert with exception for `padding_side` arg in `PreTrainedTokenizerBase` `__init__`""

This reverts commit d5a10a7e2680539e5d9e98ae5d896c893d224b80.

* fix typo

* fix typos to render documentation

* Revert "Revert "Revert "replace assert with exception for `padding_side` arg in `PreTrainedTokenizerBase` `__init__`"""

This reverts commit 16cf58811943a08f43409a7c83eaa330686591d0.

* format

Co-authored-by: LSinev <LSinev@users.noreply.github.com>
Co-authored-by: Nicolas Patry <patry.nicolas@protonmail.com>
==

src/transformers/tokenization_utils_base.py
tests/test_tokenization_common.py
==================
45cac3fad;Sylvain Gugger;2022-02-02 14:23:43 -0500;Fix labels stored in model config for token classification examples (#15482)
* Playing

* Properly set labels in model config for token classification example

* Port to run_ner_no_trainer

* Quality
==

examples/pytorch/token-classification/run_ner.py
examples/pytorch/token-classification/run_ner_no_trainer.py
==================
c74f3d4c4;Ayush Chaurasia;2022-02-03 00:36:14 +0530;Add W&B backend for hyperparameter sweep (#14582)
# Add support for W&B hyperparameter sweep
This PR:
* allows using wandb for running hyperparameter search.
* The runs are visualized on W&B sweeps dashboard
* This supports runnning sweeps on parallel devices, all reporting to the same central dashboard.

### Usage
**To run new a hyperparameter search:**
```
trainer.hyperparameter_search(
    backend="wandb", 
    project="transformers_sweep", # name of the project
    n_trials=5,
    metric="eval/loss", # metric to be optimized, default 'eval/loss'. A warning is raised if the passed metric is not found
)
```
This outputs a sweep id. Eg. `my_project/sweep_id`

**To run sweeps on parallel devices:**
Just pass sweep id which you want to run parallel
```
trainer.hyperparameter_search(
    backend="wandb", 
    sweep_id = "my_project/sweep_id"
)
```

==

.github/workflows/self-scheduled.yml
src/transformers/integrations.py
src/transformers/testing_utils.py
src/transformers/trainer.py
src/transformers/trainer_utils.py
tests/test_trainer.py
==================
13297ac71;Sylvain Gugger;2022-02-02 12:12:22 -0500;Fic docstring of ASR pipeline (#15481)

==

src/transformers/pipelines/automatic_speech_recognition.py
==================
dd360d58d;bugface;2022-02-02 10:45:51 -0500;fix error posted in issue #15448 (#15480)
* fix error posted in issue #15448

Signed-off-by: bugface <alexgre@ufl.edu>

* clean up - remove commented line

Signed-off-by: bugface <alexgre@ufl.edu>
==

src/transformers/models/megatron_bert/convert_megatron_bert_checkpoint.py
==================
44b21f117;Sylvain Gugger;2022-02-02 10:44:37 -0500;Save code of registered custom models (#15379)
* Allow dynamic modules to use relative imports

* Work for configs

* Fix last merge conflict

* Save code of registered custom objects

* Map strings to strings

* Fix test

* Add tokenizer

* Rework tests

* Tests

* Ignore fixtures py files for tests

* Tokenizer test + fix collection

* With full path

* Rework integration

* Fix typo

* Remove changes in conftest

* Test for tokenizers

* Add documentation

* Update docs/source/custom_models.mdx

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* Add file structure and file content

* Add more doc

* Style

* Update docs/source/custom_models.mdx

Co-authored-by: Suraj Patil <surajp815@gmail.com>

* Address review comments

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
Co-authored-by: Suraj Patil <surajp815@gmail.com>
==

docs/source/_toctree.yml
docs/source/custom_models.mdx
src/transformers/__init__.py
src/transformers/configuration_utils.py
src/transformers/dynamic_module_utils.py
src/transformers/modeling_flax_utils.py
src/transformers/modeling_tf_utils.py
src/transformers/modeling_utils.py
src/transformers/models/auto/auto_factory.py
src/transformers/models/auto/configuration_auto.py
src/transformers/models/auto/tokenization_auto.py
src/transformers/tokenization_utils_base.py
tests/test_configuration_auto.py
tests/test_configuration_common.py
tests/test_modeling_auto.py
tests/test_modeling_common.py
tests/test_tokenization_auto.py
tests/test_tokenization_common.py
utils/test_module/__init__.py
utils/test_module/custom_configuration.py
utils/test_module/custom_modeling.py
utils/test_module/custom_tokenization.py
utils/test_module/custom_tokenization_fast.py
==================
623d8cb47;Nicolas Patry;2022-02-02 15:12:12 +0100;Adding support for `microphone` streaming within pipeline. (#15046)
* Adding support for `microphone` streaming within pipeline.

- Uses `ffmpeg` to get microphone data.
- Makes sure alignment is made to `size_of_sample`.
- Works by sending `{"raw": ..data.., "stride": (n, left, right),
"partial": bool}`
directly to the pipeline enabling to stream partial results and still
get inference.
- Let's `partial` information flow through the pipeline to enable caller
  to get it back and choose to display text or not.

- The striding reconstitution is bound to have errors since CTC does not
keep previous state. Currently most of the errors are we don't know if
there's a space or not between two chunks.
Since we have some left striding info, we could use that during decoding
to choose what to do with those spaces and even extra letters maybe (if
the stride is long enough, it's bound to cover at least a few symbols)

Fixing tests.

Protecting with `require_torch`.

`raw_ctc` support for nicer demo.

Post rebase fixes.

Revamp to split raw_mic_data from it's live chunking.

- Requires a refactor to make everything a bit cleaner.

Automatic resampling.

Small fix.

Small fix.

* Post rebase fix (need to let super handle more logic, reorder args.)

* Update docstrings

* Docstring format.

* Remove print.

* Prevent flow of `input_values`.

* Fixing `stride` too.

* Fixing the PR by removing `raw_ctc`.

* Better docstrings.

* Fixing init.

* Update src/transformers/pipelines/audio_utils.py

Co-authored-by: Anton Lozhkov <aglozhkov@gmail.com>

* Update tests/test_pipelines_automatic_speech_recognition.py

Co-authored-by: Anton Lozhkov <aglozhkov@gmail.com>

* Quality.

Co-authored-by: Anton Lozhkov <aglozhkov@gmail.com>
==

src/transformers/pipelines/audio_utils.py
src/transformers/pipelines/automatic_speech_recognition.py
tests/test_pipelines_automatic_speech_recognition.py
==================
d718c0c3a;Patrick von Platen;2022-02-02 12:59:40 +0100;[Wav2Vec2ProcessorWithLM] add alpha & beta to batch decode & decode (#15465)

==

src/transformers/models/wav2vec2_with_lm/processing_wav2vec2_with_lm.py
tests/test_processor_wav2vec2_with_lm.py
==================
1d94d5754;NielsRogge;2022-02-02 09:44:22 +0100;Add option to resize like torchvision's Resize (#15419)
* Add torchvision's resize

* Rename torch_resize to default_to_square

* Apply suggestions from code review

* Add support for default_to_square and tuple of length 1
==

src/transformers/image_utils.py
tests/test_image_utils.py
==================
b9418a1d9;Steven Liu;2022-02-01 18:31:35 -0600;Update tutorial docs (#15165)
* first draft of pipeline, autoclass, preprocess tutorials

* apply review feedback

* üñç apply feedback from patrick/niels

* üìùadd output image to preprocessed image

* üñç apply feedback from patrick
==

docs/source/_toctree.yml
docs/source/autoclass_tutorial.mdx
docs/source/pipeline_tutorial.mdx
docs/source/preprocessing.mdx
==================
c157c7e3f;Steven Liu;2022-02-01 18:28:12 -0600;Update fine-tune docs (#15259)
* add fine-tune tutorial

* make edits, fix style

* üìù make edits

* üñç fix code format links to external libraries

* üîÑrevert code formatting

* üñç use DefaultDataCollator instead of DataCollatorWithPadding
==

docs/source/training.mdx
==================
d0b5ed110;Sylvain Gugger;2022-02-01 15:49:13 -0500;Harder check for IndexErrors in QA scripts (#15438)
* Harder check for IndexErrors in QA scripts

* Make test stronger
==

examples/flax/question-answering/utils_qa.py
examples/pytorch/question-answering/utils_qa.py
examples/tensorflow/question-answering/utils_qa.py
==================
8e5d4e490;Sylvain Gugger;2022-02-01 15:49:04 -0500;`Trainer.push_to_hub` always tries to push to the Hub (#15463)

==

src/transformers/trainer.py
==================
37800f136;Suraj Patil;2022-02-01 20:59:24 +0100;[BartTokenizer] remove inheritance on RobertaTokenizer (#15461)
* refactor bart tokenizers

* doc

* replace assert with ValueError
==

src/transformers/models/bart/tokenization_bart.py
src/transformers/models/bart/tokenization_bart_fast.py
==================
f427e7504;Yih-Dar;2022-02-01 19:08:17 +0100;use mean instead of elementwise_mean in XLMPredLayer (#15436)
* use mean instead of elementwise_mean

* make style

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>
==

src/transformers/models/xlm/modeling_xlm.py
==================
7b8bdd860;SaulLu;2022-02-01 16:48:25 +0100;fix the `tokenizer_config.json` file for the slow tokenizer when a fast version is available (#15319)
* add new test

* update test

* remove `tokenizer_file` from `additional_files_names` in `tokenization_utils_base.py`

* add `tokenizer_file` for the fast only tokenizer

* change global variables layoutxml

* remove `"tokenizer_file"` from DPR tokenizer's Global variables

* remove `tokenizer_file` from herbert slow tokenizer init

* `"tokenizer_file"` from LED tokenizer's Global variables

* remove `tokenizer_file` from mbart slow tokenizer init

* remove `tokenizer_file` from slow tokenizer template

* adapt to versioning

* adapt the `test_tokenizer_mismatch_warning` test

* clean test

* clarify `VOCAB_FILES_NAMES` in tokenization_utils_fast.py

* Revert "remove `tokenizer_file` from mbart slow tokenizer init"

This reverts commit 0dbb723fa9c7599d4640fe30b3647a74eb4a64e1.

* Revert "`"tokenizer_file"` from LED tokenizer's Global variables"

This reverts commit 5a3f879bdd651233f3d74a3d1146c34cde82b0c2.

* Revert "remove `tokenizer_file` from herbert slow tokenizer init"

This reverts commit f5e10007b7b0ec5345e015b9de7ffec72c5407fd.

* Revert "remove `"tokenizer_file"` from DPR tokenizer's Global variables"

This reverts commit da0895330bedfafc81ae3073470a9348c669f032.

* set `tokenizer_file` in super `__init__` of mbart
==

src/transformers/models/layoutxlm/tokenization_layoutxlm_fast.py
src/transformers/models/mbart/tokenization_mbart.py
src/transformers/tokenization_utils_base.py
src/transformers/tokenization_utils_fast.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/tokenization_{{cookiecutter.lowercase_modelname}}.py
tests/test_tokenization_common.py
==================
6d585fe0f;SaulLu;2022-02-01 16:13:58 +0100;replace assert with exception for padding_side arg in `PreTrainedTokenizerBase` `__init__` (#15454)
* replace assert with exception for `padding_side` arg in `PreTrainedTokenizerBase` `__init__`

* add test

* fix kwargs

* reformat test

* format

* format

* fix typo to render the documentation
==

src/transformers/tokenization_utils_base.py
tests/test_tokenization_common.py
==================
d2749cf72;Kamal Raj;2022-02-01 20:34:30 +0530;Update README.md (#15462)
fix typo
==

examples/flax/language-modeling/README.md
==================
1c9648c45;Suraj Patil;2022-02-01 14:32:55 +0100;[M2M100, XGLM] fix positional emb resize (#15444)

==

src/transformers/models/m2m_100/modeling_m2m_100.py
src/transformers/models/xglm/modeling_xglm.py
==================
2ca626839;Yih-Dar;2022-02-01 12:20:22 +0100;fix from_vision_text_pretrained doc example (#15453)
Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>
==

src/transformers/models/vision_text_dual_encoder/modeling_flax_vision_text_dual_encoder.py
src/transformers/models/vision_text_dual_encoder/modeling_vision_text_dual_encoder.py
==================
dc05dd539;Yih-Dar;2022-02-01 12:04:07 +0100;Fix TF Causal LM models' returned logits (#15256)
* Fix TF Causal LM models' returned logits

* Fix expected shape in the tests

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>
==

src/transformers/models/bert/modeling_tf_bert.py
src/transformers/models/ctrl/modeling_tf_ctrl.py
src/transformers/models/gpt2/modeling_tf_gpt2.py
src/transformers/models/openai/modeling_tf_openai.py
src/transformers/models/rembert/modeling_tf_rembert.py
src/transformers/models/roberta/modeling_tf_roberta.py
src/transformers/models/roformer/modeling_tf_roformer.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_tf_{{cookiecutter.lowercase_modelname}}.py
tests/test_modeling_tf_encoder_decoder.py
tests/test_modeling_tf_vision_encoder_decoder.py
==================
af5c3329d;Yih-Dar;2022-02-01 11:09:49 +0100;remove "inputs" in tf common test script (no longer required) (#15262)
Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>
==

tests/test_modeling_tf_common.py
==================
d12ae8166;Stas Bekman;2022-01-31 13:58:27 -0800;[generate] fix synced_gpus default (#15446)

==

src/transformers/generation_utils.py
==================
d4f201b86;Suraj Patil;2022-01-31 22:53:16 +0100;skip test for XGLM (#15445)

==

tests/test_pipelines_text_generation.py
==================
0c17e766c;Sylvain Gugger;2022-01-31 15:33:16 -0500;Error when group_by_length is used with an IterableDataset (#15437)

==

src/transformers/trainer.py
==================
125a2882b;peregilk;2022-01-31 21:22:11 +0100;Update modeling_wav2vec2.py (#15423)
* Update modeling_wav2vec2.py

With very tiny sound files (less than 0.1 seconds) the num_masked_span can be too long. The issue is described in issue #15366 and discussed with @patrickvonplaten.

* correct errors with mask time indices

* remove bogus file

* make fix-copies

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
==

src/transformers/models/hubert/modeling_hubert.py
src/transformers/models/sew/modeling_sew.py
src/transformers/models/sew_d/modeling_sew_d.py
src/transformers/models/unispeech/modeling_unispeech.py
src/transformers/models/unispeech_sat/modeling_unispeech_sat.py
src/transformers/models/wav2vec2/modeling_wav2vec2.py
src/transformers/models/wavlm/modeling_wavlm.py
tests/test_modeling_wav2vec2.py
==================
d984b1033;Tavin Turner;2022-01-31 13:12:10 -0700;Add 'with torch.no_grad()' to BEiT integration test forward passes (#14961)
* Add 'with torch.no_grad()' to BEiT integration test forward pass

* Fix inconsistent use of tabs and spaces in indentation
==

tests/test_modeling_beit.py
==================
09f9d0727;Matt;2022-01-31 19:17:59 +0000;Misfiring tf warnings (#15442)
* Fix spurious warning in TF TokenClassification models

* Fixing one last spurious warning

* Removing outdated warning altogether
==

src/transformers/modeling_tf_utils.py
==================
6915174e6;Suraj Patil;2022-01-31 19:50:25 +0100;[RobertaTokenizer] remove inheritance on GPT2Tokenizer (#15429)
* refactor roberta tokenizer

* refactor fast tokenizer

* remove old comment
==

src/transformers/models/roberta/tokenization_roberta.py
src/transformers/models/roberta/tokenization_roberta_fast.py
==================
a5ecbf734;Suraj Patil;2022-01-31 19:47:49 +0100;correct positionla emb size (#15441)

==

src/transformers/models/xglm/tokenization_xglm.py
src/transformers/models/xglm/tokenization_xglm_fast.py
==================
5a7098730;Yih-Dar;2022-01-31 19:35:54 +0100;Fix TFLEDModel (#15356)
* fix tf led

* fix

* fix

* Add test_pt_tf_model_equivalence_extra for TFLED

* add a (temporary) test

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>
==

src/transformers/models/led/modeling_tf_led.py
src/transformers/models/longformer/modeling_tf_longformer.py
tests/test_modeling_tf_led.py
==================
87918d322;Suraj Patil;2022-01-31 19:20:53 +0100;[examples/Flax] add a section about GPUs (#15198)
* add a section about GPUs

* Apply suggestions from code review

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
==

examples/flax/README.md
==================
b8810847d;Patrick von Platen;2022-01-31 18:51:29 +0100;[Trainer] suppress warning for length-related columns (#15421)
* [Trainer] suppress warning for length-related columns

* improve message

* Update src/transformers/trainer.py
==

src/transformers/trainer.py
==================
3385ca258;Sylvain Gugger;2022-01-31 12:50:20 -0500;Change REALM checkpoint to new ones (#15439)
* Change REALM checkpoint to new ones

* Last checkpoint missing
==

src/transformers/models/realm/configuration_realm.py
src/transformers/models/realm/modeling_realm.py
src/transformers/models/realm/tokenization_realm.py
src/transformers/models/realm/tokenization_realm_fast.py
tests/test_modeling_realm.py
tests/test_retrieval_realm.py
==================
7e56ba286;Matt;2022-01-31 17:09:16 +0000;Fix spurious warning in TF TokenClassification models (#15435)

==

src/transformers/modeling_tf_utils.py
==================
554d333ec;Yih-Dar;2022-01-31 17:43:08 +0100;Fix loss calculation in TFXXXForTokenClassification models (#15294)
* Fix loss calculation in TFFunnelForTokenClassification

* revert the change in TFFunnelForTokenClassification

* fix FunnelForTokenClassification loss

* fix other TokenClassification loss

* fix more

* fix more

* add num_labels to ElectraForTokenClassification

* revert the change to research projects

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>
==

src/transformers/models/albert/modeling_albert.py
src/transformers/models/bert/modeling_bert.py
src/transformers/models/big_bird/modeling_big_bird.py
src/transformers/models/canine/modeling_canine.py
src/transformers/models/convbert/modeling_convbert.py
src/transformers/models/deberta/modeling_deberta.py
src/transformers/models/deberta_v2/modeling_deberta_v2.py
src/transformers/models/distilbert/modeling_distilbert.py
src/transformers/models/electra/modeling_electra.py
src/transformers/models/funnel/modeling_funnel.py
src/transformers/models/gpt2/modeling_gpt2.py
src/transformers/models/ibert/modeling_ibert.py
src/transformers/models/layoutlm/modeling_layoutlm.py
src/transformers/models/layoutlmv2/modeling_layoutlmv2.py
src/transformers/models/longformer/modeling_longformer.py
src/transformers/models/megatron_bert/modeling_megatron_bert.py
src/transformers/models/mobilebert/modeling_mobilebert.py
src/transformers/models/mpnet/modeling_mpnet.py
src/transformers/models/nystromformer/modeling_nystromformer.py
src/transformers/models/qdqbert/modeling_qdqbert.py
src/transformers/models/rembert/modeling_rembert.py
src/transformers/models/roberta/modeling_roberta.py
src/transformers/models/roformer/modeling_roformer.py
src/transformers/models/squeezebert/modeling_squeezebert.py
src/transformers/models/xlm/modeling_xlm.py
src/transformers/models/xlnet/modeling_xlnet.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_{{cookiecutter.lowercase_modelname}}.py
==================
44c7857b8;Stas Bekman;2022-01-31 08:28:10 -0800;[deepspeed doc] fix import, extra notes (#15400)
* [deepspeed doc] fix import, extra notes

* typo
==

docs/source/main_classes/deepspeed.mdx
==================
47df0f223;NielsRogge;2022-01-31 17:15:54 +0100;Add header (#15434)

==

docs/source/model_doc/swin.mdx
==================
7fc6f41d9;Sylvain Gugger;2022-01-31 11:10:45 -0500;Add doc for add-new-model-like command (#15433)

==

templates/adding_a_new_model/README.md
==================
282ae123e;Ogundepo Odunayo;2022-01-31 11:03:06 -0500;add t5 ner finetuning (#15432)

==

docs/source/community.mdx
==================
d4b3e56d6;NielsRogge;2022-01-31 16:32:14 +0100;[Hotfix] Fix Swin model outputs (#15414)
* Fix Swin model outputs

* Rename pooler
==

src/transformers/models/swin/modeling_swin.py
tests/test_modeling_swin.py
==================
38dfb40ae;Suraj Patil;2022-01-31 15:51:50 +0100;import torch.utils.checkpoint (#15427)

==

src/transformers/models/xglm/modeling_xglm.py
==================
f624249d8;Jonatas Grosman;2022-01-31 11:50:56 -0300;[Robust Speech Challenge] Add missing LR parameter (#15428)

==

examples/research_projects/robust-speech-event/run_speech_recognition_ctc_bnb.py
==================
3254080d4;Kamal Raj;2022-01-31 20:18:20 +0530;Update README.md (#15430)
fix typo
==

docs/README.md
==================
aa19f478a;Julien Plu;2022-01-31 13:58:18 +0100;Add (M)Luke model training for Token Classification in the examples (#14880)
* Add Luke training

* Fix true label tags

* Fix true label tags

* Fix true label tags

* Update the data collator for Luke

* Some training refactor for Luke

* Improve data collator for Luke

* Fix import

* Fix datasets concatenation

* Add the --max_entity_length argument for Luke models

* Remove unused code

* Fix style issues

* Fix style issues

* Move the Luke training into a separate folder

* Fix style

* Fix naming

* Fix filtering

* Fix filtering

* Fix filter

* Update some preprocessing

* Move luke to research_projects

* Checkstyle

* Address comments

* Fix style
==

examples/research_projects/luke/README.md
examples/research_projects/luke/luke_utils.py
examples/research_projects/luke/run_luke_ner_no_trainer.py
==================
0094eba36;Fran√ßois REMY;2022-01-31 13:45:11 +0100;Fix additional DataTrainingArguments documentation (#15408)
(This is an editorial change only)
==

examples/pytorch/speech-recognition/run_speech_recognition_ctc.py
==================
ee5de6634;NielsRogge;2022-01-31 11:38:08 +0100;Add SegformerFeatureExtractor to Auto API (#15410)

==

src/transformers/models/auto/feature_extraction_auto.py
==================
0f69b924f;Suraj Patil;2022-01-30 15:35:53 +0100;[XGLMTokenizer] fix init and add in AutoTokenizer (#15406)

==

src/transformers/models/auto/tokenization_auto.py
src/transformers/models/xglm/__init__.py
==================
f380bf2b6;Yih-Dar;2022-01-29 16:08:35 +0100;Fix the inconsistency of loss calculation between PT/TF XLNetLMHeadModel (#15298)
* Fix the inconsistency of loss calculation between PT/TF XLNetLMHeadModel

* overwrite test_loss_computation

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>
==

src/transformers/models/xlnet/modeling_tf_xlnet.py
tests/test_modeling_tf_xlnet.py
==================
e09473a81;Soonhwan-Kwon;2022-01-29 21:42:37 +0900;Add support for XLM-R XL and XXL models by modeling_xlm_roberta_xl.py (#13727)
* add xlm roberta xl

* add convert xlm xl fairseq checkpoint to pytorch

* fix init and documents for xlm-roberta-xl

* fix indention

* add test for XLM-R xl,xxl

* fix model hub name

* fix some stuff

* up

* correct init

* fix more

* fix as suggestions

* add torch_device

* fix default values of doc strings

* fix leftovers

* merge to master

* up

* correct hub names

* fix docs

* fix model

* up

* finalize

* last fix

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* add copied from

* make style

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

README.md
README_ko.md
README_zh-hans.md
README_zh-hant.md
docs/source/_toctree.yml
docs/source/index.mdx
docs/source/model_doc/xlm-roberta-xl.mdx
docs/source/serialization.mdx
src/transformers/__init__.py
src/transformers/models/__init__.py
src/transformers/models/auto/configuration_auto.py
src/transformers/models/auto/modeling_auto.py
src/transformers/models/xlm_roberta_xl/__init__.py
src/transformers/models/xlm_roberta_xl/configuration_xlm_roberta_xl.py
src/transformers/models/xlm_roberta_xl/convert_xlm_roberta_xl_original_pytorch_checkpoint_to_pytorch.py
src/transformers/models/xlm_roberta_xl/modeling_xlm_roberta_xl.py
src/transformers/utils/dummy_pt_objects.py
tests/test_modeling_xlm_roberta_xl.py
==================
16d4acbfd;Steven Liu;2022-01-28 19:01:37 -0600;Get started docs (#15098)
* clean commit of changes

* apply review feedback, make edits

* fix backticks, minor formatting

* üñç make fixup and minor edits

* üñç fix # in header

* üìù update code sample without from_pt

* üìù final review
==

docs/source/index.mdx
docs/source/installation.mdx
docs/source/quicktour.mdx
==================
cabd6d26a;Steven Liu;2022-01-28 18:49:26 -0600;Update model share tutorial (#15288)
* add model sharing tutorial

* üñç apply feedback from review

* üìù make edits

* üñç fix formatting

* üìù convert from pt checkpoint to flax

* üìù final review
==

docs/source/_toctree.yml
docs/source/model_sharing.mdx
==================
c98a6ac21;Sylvain Gugger;2022-01-28 18:34:10 -0500;Use argument for preprocessing workers in run_summairzation (#15394)

==

examples/pytorch/summarization/run_summarization_no_trainer.py
==================
db0795674;Yih-Dar;2022-01-29 00:32:26 +0100;Fix missing eps arg for LayerNorm in ElectraGeneratorPredictions (#15332)
* fix missing eps

* Same fix for ConvBertGeneratorPredictions

* Same fix for AlbertMLMHead

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>
==

src/transformers/models/albert/modeling_albert.py
src/transformers/models/convbert/modeling_convbert.py
src/transformers/models/electra/modeling_electra.py
==================
297602c7f;Stas Bekman;2022-01-28 11:05:47 -0800;[deepspeed] saving checkpoint fallback when fp16 weights aren't saved (#14948)
* [deepspeed] saving checkpoint fallback when fp16 weights aren't saved

* Bump required deepspeed version to match usage when saving checkpoints

* update version

Co-authored-by: Mihai Balint <balint.mihai@gmail.com>
==

setup.py
src/transformers/dependency_versions_table.py
src/transformers/trainer.py
==================
d25e25ee2;Suraj Patil;2022-01-28 18:55:23 +0100;Add XGLM models (#14876)
* add xglm

* update vocab size

* fix model name

* style and tokenizer

* typo

* no mask token

* fix pos embed compute

* fix args

* fix tokenizer

* fix positions

* fix tokenization

* style and dic fixes

* fix imports

* add fast tokenizer

* update names

* add pt tests

* fix tokenizer

* fix typo

* fix tokenizer import

* fix fast tokenizer

* fix tokenizer

* fix converter

* add tokenizer test

* update checkpoint names

* fix tokenizer tests

* fix slow tests

* add copied from comments

* rst -> mdx

* flax model

* update flax tests

* quality

* style

* doc

* update index and readme

* fix copies

* fix doc

* update toctrr

* fix indent

* minor fixes

* fix config doc

* don't save embed_pos weights

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* address Sylvains commnets, few doc fixes

* fix check_repo

* align order of arguments

* fix copies

* fix labels

* remove unnecessary mapping

* fix saving tokenizer

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
==

README.md
README_ko.md
README_zh-hans.md
README_zh-hant.md
docs/source/_toctree.yml
docs/source/index.mdx
docs/source/model_doc/xglm.mdx
src/transformers/__init__.py
src/transformers/convert_slow_tokenizer.py
src/transformers/models/__init__.py
src/transformers/models/auto/configuration_auto.py
src/transformers/models/auto/modeling_auto.py
src/transformers/models/auto/modeling_flax_auto.py
src/transformers/models/xglm/__init__.py
src/transformers/models/xglm/configuration_xglm.py
src/transformers/models/xglm/modeling_flax_xglm.py
src/transformers/models/xglm/modeling_xglm.py
src/transformers/models/xglm/tokenization_xglm.py
src/transformers/models/xglm/tokenization_xglm_fast.py
src/transformers/utils/dummy_flax_objects.py
src/transformers/utils/dummy_pt_objects.py
src/transformers/utils/dummy_sentencepiece_objects.py
src/transformers/utils/dummy_tokenizers_objects.py
tests/test_modeling_flax_xglm.py
tests/test_modeling_xglm.py
tests/test_tokenization_xglm.py
utils/check_repo.py
==================
b6b79faa7;Matt;2022-01-28 17:31:22 +0000;Make links explicit (#15395)
* Make links explicit

* Removing reference to compute_metrics() since it's kind of PyTorch-specific
==

examples/README.md
==================
6df29ba5e;Yih-Dar;2022-01-28 16:53:25 +0100;fix wrong tokenizer checkpoint name in flax marian (#15391)
Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>
==

src/transformers/models/marian/modeling_flax_marian.py
==================
507601a5c;lewtun;2022-01-28 16:32:47 +0100;Prepare deprecated ONNX exporter for torch v1.11 (#15388)
* Prepare deprecated ONNX exporter for PyTorch v1.11

* Add deprecation warning
==

src/transformers/convert_graph_to_onnx.py
==================
4996922b6;Ngo Quang Huy;2022-01-28 19:52:01 +0700;[docs] fix wrong file name in `pr_check` (#15380)

==

docs/source/pr_checks.mdx
==================
8f5d62fdb;Ngo Quang Huy;2022-01-28 18:39:55 +0700;Fix `bad_words_ids` not working with sentencepiece-based tokenizers (#15343)
* Fix `bad_word_ids` not working with sentencepiece-based tokenizers

* make style

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
==

src/transformers/generation_utils.py
==================
06107541d;Nicolas Patry;2022-01-28 12:15:30 +0100;Fixing support `batch_size` and `num_return_Sequences` in `text-generation` pipeline (#15318)
* Fixing support `batch_size` and `num_return_Sequences` in
`text-generation` pipeline

And `text2text-generation` too.

The bug was caused by the batch_size containing both the incoming batch
**and** the generated `num_sequences`.

The fix simply consists into splitting both of these again into
different dimensions.

* TF support.

* Odd backward compatibility script in the way.
==

src/transformers/pipelines/text2text_generation.py
src/transformers/pipelines/text_generation.py
tests/test_pipelines_text2text_generation.py
tests/test_pipelines_text_generation.py
==================
c4d1fd77f;Yanming Wang;2022-01-27 17:05:31 -0800;Set syncfree AdamW as the default optimizer for xla:gpu device in amp mode (#15361)
* Use syncfree AdamW for xla:gpu device by default

* Make syncfree AdamW optional
==

src/transformers/trainer.py
src/transformers/training_args.py
==================
2e4559fa3;Lysandre Debut;2022-01-27 15:16:54 -0500;Add init to BORT (#15378)
* Add init to BORT

* BORT should be in init
==

src/transformers/models/__init__.py
src/transformers/models/bort/__init__.py
==================
f5db6ce76;Steven Liu;2022-01-27 13:49:04 -0600;Fix code format for Accelerate doc (#15335)
* üñç fix code syntax to external libraries and replace image

* üîÑrevert code formatting, replace image with code block

* üñç apply feedback
==

docs/source/accelerate.mdx
==================
0b0723040;Sylvain Gugger;2022-01-27 14:47:59 -0500;Allow relative imports in dynamic code (#15352)
* Allow dynamic modules to use relative imports

* Add tests

* Add one last test

* Changes
==

src/transformers/models/auto/dynamic.py
tests/test_configuration_auto.py
tests/test_modeling_auto.py
==================
628b59e51;dependabot[bot];2022-01-27 14:46:15 -0500;Bump numpy from 1.19.2 to 1.21.0 in /examples/research_projects/lxmert (#15369)
Bumps [numpy](https://github.com/numpy/numpy) from 1.19.2 to 1.21.0.
- [Release notes](https://github.com/numpy/numpy/releases)
- [Changelog](https://github.com/numpy/numpy/blob/main/doc/HOWTO_RELEASE.rst.txt)
- [Commits](https://github.com/numpy/numpy/compare/v1.19.2...v1.21.0)

---
updated-dependencies:
- dependency-name: numpy
  dependency-type: direct:production
...

Signed-off-by: dependabot[bot] <support@github.com>

Co-authored-by: dependabot[bot] <49699333+dependabot[bot]@users.noreply.github.com>
==

examples/research_projects/lxmert/requirements.txt
==================
ca0848b2f;dependabot[bot];2022-01-27 14:45:58 -0500;Bump notebook in /examples/research_projects/visual_bert (#15368)
Bumps [notebook](http://jupyter.org) from 6.1.5 to 6.4.1.

---
updated-dependencies:
- dependency-name: notebook
  dependency-type: direct:production
...

Signed-off-by: dependabot[bot] <support@github.com>

Co-authored-by: dependabot[bot] <49699333+dependabot[bot]@users.noreply.github.com>
Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
==

examples/research_projects/visual_bert/requirements.txt
==================
7d45a2e81;dependabot[bot];2022-01-27 14:45:18 -0500;Bump numpy in /examples/research_projects/visual_bert (#15367)
Bumps [numpy](https://github.com/numpy/numpy) from 1.19.2 to 1.21.0.
- [Release notes](https://github.com/numpy/numpy/releases)
- [Changelog](https://github.com/numpy/numpy/blob/main/doc/HOWTO_RELEASE.rst.txt)
- [Commits](https://github.com/numpy/numpy/compare/v1.19.2...v1.21.0)

---
updated-dependencies:
- dependency-name: numpy
  dependency-type: direct:production
...

Signed-off-by: dependabot[bot] <support@github.com>

Co-authored-by: dependabot[bot] <49699333+dependabot[bot]@users.noreply.github.com>
==

examples/research_projects/visual_bert/requirements.txt
==================
a81fd3552;Sylvain Gugger;2022-01-27 14:17:48 -0500;Fix tests_fetcher (#15376)

==

utils/tests_fetcher.py
==================
eab338104;Lysandre;2022-01-27 13:11:51 -0500;Docs for version v4.16.0

==

examples/flax/question-answering/run_qa.py
examples/flax/text-classification/run_flax_glue.py
examples/flax/token-classification/run_flax_ner.py
examples/pytorch/audio-classification/run_audio_classification.py
examples/pytorch/image-classification/run_image_classification.py
examples/pytorch/image-pretraining/run_mae.py
examples/pytorch/language-modeling/run_clm.py
examples/pytorch/language-modeling/run_mlm.py
examples/pytorch/language-modeling/run_plm.py
examples/pytorch/multiple-choice/run_swag.py
examples/pytorch/question-answering/run_qa.py
examples/pytorch/question-answering/run_qa_beam_search.py
examples/pytorch/question-answering/run_qa_beam_search_no_trainer.py
examples/pytorch/question-answering/run_qa_no_trainer.py
examples/pytorch/question-answering/run_seq2seq_qa.py
examples/pytorch/speech-recognition/run_speech_recognition_ctc.py
examples/pytorch/speech-recognition/run_speech_recognition_seq2seq.py
examples/pytorch/summarization/run_summarization.py
examples/pytorch/text-classification/run_glue.py
examples/pytorch/text-classification/run_xnli.py
examples/pytorch/token-classification/run_ner.py
examples/pytorch/translation/run_translation.py
examples/tensorflow/multiple-choice/run_swag.py
examples/tensorflow/question-answering/run_qa.py
examples/tensorflow/summarization/run_summarization.py
examples/tensorflow/text-classification/run_glue.py
examples/tensorflow/translation/run_translation.py
setup.py
src/transformers/__init__.py
==================
f87db5e41;Lysandre;2022-01-27 13:06:33 -0500;Release: v4.16.0

==

README.md
README_ko.md
README_zh-hans.md
README_zh-hant.md
docs/source/index.mdx
examples/flax/question-answering/run_qa.py
examples/flax/text-classification/run_flax_glue.py
examples/flax/token-classification/run_flax_ner.py
examples/pytorch/audio-classification/run_audio_classification.py
examples/pytorch/image-classification/run_image_classification.py
examples/pytorch/image-pretraining/run_mae.py
examples/pytorch/language-modeling/run_clm.py
examples/pytorch/language-modeling/run_mlm.py
examples/pytorch/language-modeling/run_plm.py
examples/pytorch/multiple-choice/run_swag.py
examples/pytorch/question-answering/run_qa.py
examples/pytorch/question-answering/run_qa_beam_search.py
examples/pytorch/question-answering/run_qa_beam_search_no_trainer.py
examples/pytorch/question-answering/run_qa_no_trainer.py
examples/pytorch/question-answering/run_seq2seq_qa.py
examples/pytorch/speech-recognition/run_speech_recognition_ctc.py
examples/pytorch/speech-recognition/run_speech_recognition_seq2seq.py
examples/pytorch/summarization/run_summarization.py
examples/pytorch/text-classification/run_glue.py
examples/pytorch/text-classification/run_xnli.py
examples/pytorch/token-classification/run_ner.py
examples/pytorch/translation/run_translation.py
examples/tensorflow/multiple-choice/run_swag.py
examples/tensorflow/question-answering/run_qa.py
examples/tensorflow/summarization/run_summarization.py
examples/tensorflow/text-classification/run_glue.py
examples/tensorflow/translation/run_translation.py
setup.py
src/transformers/__init__.py
==================
c43749289;Matt;2022-01-27 16:16:24 +0000;Example script for PushToHubCallback (#15375)
* Example script for PushToHubCallback

* Expanding description slightly
==

src/transformers/keras_callbacks.py
==================
8f6454bfa;Sylvain Gugger;2022-01-27 10:51:38 -0500;Add proper documentation for Keras callbacks (#15374)
* Add proper documentation for Keras callbacks

* Add dummies
==

docs/source/main_classes/keras_callbacks.mdx
src/transformers/__init__.py
src/transformers/keras_callbacks.py
src/transformers/utils/dummy_tf_objects.py
==================
2de90beee;Matt;2022-01-27 15:43:43 +0000;Super-small fix stops us confusing Keras console logging by modifying its logs (#15373)

==

src/transformers/keras_callbacks.py
==================
fa6dce250;Sylvain Gugger;2022-01-27 10:25:43 -0500;Implement fixes for TrainingArguments doc (#15370)
Co-authored-by: osanseviero <osanseviero@gmail.com>

Co-authored-by: osanseviero <osanseviero@gmail.com>
==

src/transformers/training_args.py
==================
ade7371a4;SaulLu;2022-01-27 16:24:51 +0100;improve saving strategy of sentencepiece tokenizer (#15328)
* add new test

* add a feature to same the sentencepiece tokenizer model when the init file was deleted

* update marian

* update m2m_100

* fix marian

* update speech to text

* override test for layoutxlm

* fix saving bartpho

* remove harcoded values bartpho

* special token string version

* finish bartpho

* override layoutxml test

* add mbart

* move special tokens list

* format

* Revert "format"

This reverts commit 37a40df37903a932c2f951cbd33acb684246bae7.

* simplify list of string of special tokens

* Re-write `self.fairseq_tokens_to_ids ` initialization logic with special tokens

Co-authored-by: Sylvain Gugger <sylvain.gugger@gmail.com>

Co-authored-by: Sylvain Gugger <sylvain.gugger@gmail.com>
==

src/transformers/models/albert/tokenization_albert.py
src/transformers/models/bartpho/tokenization_bartpho.py
src/transformers/models/bert_generation/tokenization_bert_generation.py
src/transformers/models/big_bird/tokenization_big_bird.py
src/transformers/models/camembert/tokenization_camembert.py
src/transformers/models/fnet/tokenization_fnet.py
src/transformers/models/layoutxlm/tokenization_layoutxlm.py
src/transformers/models/m2m_100/tokenization_m2m_100.py
src/transformers/models/marian/tokenization_marian.py
src/transformers/models/mbart/tokenization_mbart.py
src/transformers/models/mbart50/tokenization_mbart50.py
src/transformers/models/pegasus/tokenization_pegasus.py
src/transformers/models/reformer/tokenization_reformer.py
src/transformers/models/speech_to_text/tokenization_speech_to_text.py
src/transformers/models/t5/tokenization_t5.py
src/transformers/models/xlm_prophetnet/tokenization_xlm_prophetnet.py
src/transformers/models/xlm_roberta/tokenization_xlm_roberta.py
src/transformers/models/xlnet/tokenization_xlnet.py
tests/test_tokenization_common.py
tests/test_tokenization_layoutxlm.py
tests/test_tokenization_mbart.py
==================
196cce6e9;Anton Lozhkov;2022-01-27 17:58:55 +0300;Add a device argument to the eval script (#15371)
* Device argument for the eval script

* Default to none

* isort
==

examples/research_projects/robust-speech-event/eval.py
==================
6beae766e;Matt;2022-01-27 14:13:23 +0000;Fix KerasMetricCallback prediction with generate() and inference of column names (#15351)
* Fix prediction with generate() and the inference of column names
Should now have very few differences with the PyTorch implementation

* Minor edit to parent class

* Update src/transformers/keras_callbacks.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Explaining the dict conversion

* Putting main_input_name back

* Fixes to main_input_name

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

src/transformers/keras_callbacks.py
==================
da5ef25db;Sylvain Gugger;2022-01-27 09:00:54 -0500;Push to hub save (#15327)
* Adapt doc and push at every save

* style
==

src/transformers/trainer.py
src/transformers/training_args.py
==================
9f831bdea;Patrick von Platen;2022-01-27 14:29:31 +0100;[DocTests Speech] Add doc tests for all speech models (#15031)
* fix_torch_device_generate_test

* remove @

* doc tests

* up

* up

* fix doctests

* adapt files

* finish refactor

* up

* save intermediate

* add more logic

* new change

* improve

* next try

* next try

* next try

* next try

* fix final spaces

* fix final spaces

* improve

* renaming

* correct more bugs

* finish wavlm

* add comment

* run on test runner

* finish all speech models

* adapt

* finish
==

.github/workflows/doctests.yml
src/transformers/file_utils.py
src/transformers/models/hubert/modeling_hubert.py
src/transformers/models/sew/modeling_sew.py
src/transformers/models/sew_d/modeling_sew_d.py
src/transformers/models/unispeech/modeling_unispeech.py
src/transformers/models/unispeech_sat/modeling_unispeech_sat.py
src/transformers/models/wav2vec2/modeling_wav2vec2.py
src/transformers/models/wavlm/modeling_wavlm.py
utils/documentation_tests.txt
utils/prepare_for_doc_test.py
==================
4df69506a;Sylvain Gugger;2022-01-26 15:06:27 -0500;Fix YosoConfig doc (#15353)

==

src/transformers/models/yoso/configuration_yoso.py
==================
fc8fc400e;Stas Bekman;2022-01-26 11:23:32 -0800;[docs] post-PR merge fix (#15355)
* [docs] post-PR merge fix

* Update docs/source/main_classes/deepspeed.mdx

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

docs/source/main_classes/deepspeed.mdx
==================
99a277118;novice;2022-01-26 12:18:29 -0600;Add YOSO (#15091)
* Add cookiecutter files

* Add cuda kernels and cpp files

* Update modeling_yoso.py

* Add .h files

* Update configuration_yoso.py

* Updates

* Remove tokenizer

* Code quality

* Update modeling_yoso.py

* Update modeling_yoso.py

* Fix failing test

* Update modeling_yoso.py

* Fix code quality

* Apply suggestions from code review

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>

* Apply suggestions from code review

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Apply suggestions from code review and fix integration tests

* Update src/transformers/models/yoso/modeling_yoso.py

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* Apply suggestions from code review

* Fix copied from statement

* Fix docstring

* Fix code quality

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Apply suggestions and fix mask

* Apply suggestions from code review

* Fix code quality

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Fix docstrings

* Fix code quality

* Remove trailing whitespace

* Update yoso.mdx

* Move kernel loading to YosoEncoder

* make style

* Apply suggestions from code review

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>

* Update src/transformers/models/yoso/modeling_yoso.py

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>

* Add short summary to docs

* Update docs/source/model_doc/yoso.mdx

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>

* Update yoso.mdx

* Update docs/source/model_doc/yoso.mdx

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>

* Remove CausalLM model and add copied from

* Remove autoregressive code

* Remove unused imports

* add copied from for embeddings

* Fix code quality

* Update docs/source/model_doc/yoso.mdx

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>

* Apply suggestion from code review

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
==

README.md
README_ko.md
README_zh-hans.md
README_zh-hant.md
docs/source/_toctree.yml
docs/source/index.mdx
docs/source/model_doc/yoso.mdx
src/transformers/__init__.py
src/transformers/models/__init__.py
src/transformers/models/auto/configuration_auto.py
src/transformers/models/auto/modeling_auto.py
src/transformers/models/yoso/__init__.py
src/transformers/models/yoso/common.h
src/transformers/models/yoso/common_cuda.h
src/transformers/models/yoso/common_cuda_device.h
src/transformers/models/yoso/configuration_yoso.py
src/transformers/models/yoso/convert_yoso_pytorch_to_pytorch.py
src/transformers/models/yoso/fast_lsh_cumulation.cu
src/transformers/models/yoso/fast_lsh_cumulation.h
src/transformers/models/yoso/fast_lsh_cumulation_cuda.cu
src/transformers/models/yoso/fast_lsh_cumulation_cuda.h
src/transformers/models/yoso/fast_lsh_cumulation_torch.cpp
src/transformers/models/yoso/modeling_yoso.py
src/transformers/utils/dummy_pt_objects.py
tests/test_modeling_yoso.py
==================
6292532fd;Sylvain Gugger;2022-01-26 12:54:11 -0500;Update doc writing guide (#15350)

==

docs/README.md
==================
19732cc07;Fran√ßois REMY;2022-01-26 16:19:38 +0100;Fix 'eval_split_name' described as defaulting to 'train' (#15348)
The default is correct (`test`) but the description is not.
==

examples/pytorch/speech-recognition/run_speech_recognition_ctc.py
==================
5d8b98608;Ngo Quang Huy;2022-01-26 19:24:33 +0700;Fix deepspeed docs (#15346)

==

docs/source/main_classes/deepspeed.mdx
==================
96161ac40;Jacob Deppen;2022-01-26 07:10:00 -0500;make table into valid Markdown table syntax (#15337)

==

docs/source/model_doc/segformer.mdx
==================
24e2fa159;Yih-Dar;2022-01-26 10:14:46 +0100;Fix encoder-decoder models when labels is passed (#15172)
Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>
==

src/transformers/models/encoder_decoder/modeling_encoder_decoder.py
src/transformers/models/speech_encoder_decoder/modeling_speech_encoder_decoder.py
src/transformers/models/vision_encoder_decoder/modeling_vision_encoder_decoder.py
==================
e79a0faea;Maciej Paw≈Çowski;2022-01-25 23:26:17 +0100;Added missing code in exemplary notebook - custom datasets fine-tuning (#15300)
* Added missing code in exemplary notebook - custom datasets fine-tuning

Added missing code in tokenize_and_align_labels function in the exemplary notebook on custom datasets - token classification.
The missing code concerns adding labels for all but first token in a single word.
The added code was taken directly from huggingface official example - this [colab notebook](https://github.com/huggingface/notebooks/blob/master/transformers_doc/custom_datasets.ipynb).

* Changes requested in the review - keep the code as simple as possible
==

docs/source/custom_datasets.mdx
==================
0501beb84;Steven Liu;2022-01-25 13:46:11 -0600;Add ü§ó Accelerate tutorial (#15263)
* add accelerate tutorial

* üñç apply feedback from review

* üìù make edits
==

docs/source/_toctree.yml
docs/source/accelerate.mdx
==================
637e81752;NielsRogge;2022-01-25 15:48:25 +0100;[Tests] Fix test (#15324)
* Fix Swin device

* Remove print statement
==

src/transformers/models/swin/modeling_swin.py
tests/test_modeling_vilt.py
tests/test_modeling_vit_mae.py
==================
e69547079;Sylvain Gugger;2022-01-25 09:41:21 -0500;Avoid using get_list_of_files (#15287)
* Avoid using get_list_of_files in config

* Wip, change tokenizer file getter

* Remove call in tokenizer files

* Remove last call to get_list_model_files

* Better tests

* Unit tests for new function

* Document bad API
==

src/transformers/configuration_utils.py
src/transformers/file_utils.py
src/transformers/models/auto/processing_auto.py
src/transformers/models/auto/tokenization_auto.py
src/transformers/tokenization_utils_base.py
tests/test_configuration_common.py
tests/test_file_utils.py
tests/test_tokenization_fast.py
==================
e65bfc097;Sylvain Gugger;2022-01-24 15:55:29 -0500;Try without bad instruction

==

.github/workflows/add-model-like.yml
==================
81156d20c;Sylvain Gugger;2022-01-24 15:25:10 -0500;Add model like (#14992)
* Add new model like command

* Bad doc-styler

* black and doc-styler, stop fighting!

* black and doc-styler, stop fighting!

* At last

* Clean up

* Typo

* Bad doc-styler

* Bad doc-styler

* All good maybe?

* Use constants

* Add doc and type hints

* More cleaning

* Add doc

* Fix Copied from

* Doc template

* Use typing.Pattern instead

* Framework-specific files

* Fixes

* Select frameworks clean model init

* Deal with frameworks in main init

* fixes

* Last fix

* Prompt user for info

* Delete exemple config

* Last fixes

* Add test config

* Fix bug with model_type included in each other

* Fixes

* More fixes

* More fixes

* Adapt config

* Remove print statements

* Will fix tokenization later, leave it broken for now

* Add test

* Quality

* Try this way

* Debug

* Maybe by setting the path?

* Let's try another way

* It should go better when actually passing the arg...

* Remove debug statements and style

* Fix config

* Add tests

* Test require the three backends

* intermediate commit

* Revamp pattern replacements and start work on feature extractors

* Adapt model info

* Finalize code for processors

* Fix in main init additions

* Finish questionnaire for processing classes

* Fix file name

* Fix for real

* Fix patterns

* Style

* Remove needless warnings

* Copied from should work now.

* Include Copied form in blocks

* Add test

* More fixes and tests

* Apply suggestions from code review

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* Address review comment

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
==

.github/workflows/add-model-like.yml
src/transformers/commands/add_new_model_like.py
src/transformers/commands/transformers_cli.py
tests/fixtures/add_distilbert_like_config.json
tests/test_add_new_model_like.py
utils/tests_fetcher.py
==================
457dd4392;Patrick von Platen;2022-01-24 21:18:04 +0100;[Examples] Correct run ner label2id for fine-tuned models (#15017)
* up

* up

* make style

* apply sylvains suggestions

* apply changes to accelerate as well

* more changes

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

examples/pytorch/token-classification/run_ner.py
examples/pytorch/token-classification/run_ner_no_trainer.py
==================
8d6acc6c2;Patrick von Platen;2022-01-24 21:13:21 +0100;[Beam Search] Correct returned beam scores (#14654)
* better

* save intermediate

* finish code

* up

* docs

* Apply suggestions from code review

* up

* add compute transition  beam scores function to model and make sure scores are correct with eos

* apply nicos comments

* Apply suggestions from code review

* another fix
==

src/transformers/generation_utils.py
tests/test_generation_utils.py
==================
e239fc3b0;novice;2022-01-24 21:03:43 +0530;Replace NystromformerTokenizer with AutoTokenizer (#15312)

==

src/transformers/models/nystromformer/modeling_nystromformer.py
==================
dcaa5100c;Patrick von Platen;2022-01-24 15:54:47 +0100;[LayoutLMV2 Tests] Make sure input is on GPU (#15314)
* [LayoutLMV2 Tests] Make sure input is on GPU

* correct empty line
==

tests/test_modeling_layoutlmv2.py
==================
c15bb3fe1;Yih-Dar;2022-01-24 14:54:23 +0100;[Fix doc example] fix missing import jnp (#15291)
* fix missing import jnp

* Fix missing jax and k=1

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>
==

src/transformers/models/bart/modeling_flax_bart.py
src/transformers/models/blenderbot/modeling_flax_blenderbot.py
src/transformers/models/blenderbot_small/modeling_flax_blenderbot_small.py
src/transformers/models/marian/modeling_flax_marian.py
src/transformers/models/pegasus/modeling_flax_pegasus.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_flax_{{cookiecutter.lowercase_modelname}}.py
==================
eac4aecc3;Nicolas Patry;2022-01-24 13:27:45 +0100;Remove old debug code leftover. (#15306)

==

src/transformers/pipelines/automatic_speech_recognition.py
==================
2390b2cf6;Sylvain Gugger;2022-01-24 07:21:42 -0500;Fix a typo in tag addition (#15286)
* Fix a typo in tag addition

* Put it back again
==

src/transformers/modelcard.py
==================
c972433a8;Kamal Raj;2022-01-24 17:51:31 +0530;Update CONTRIBUTING.md (#15290)
Fix typo in doc
==

CONTRIBUTING.md
==================
4bf97415a;Patrick von Platen;2022-01-24 11:46:38 +0100;Update eval.py (#15310)

==

examples/research_projects/robust-speech-event/eval.py
==================
b7cb126cc;Patrick von Platen;2022-01-24 10:53:53 +0100;[PyTorch-nightly-test] Fix Wav2Vec2 LM & Phoneme tests (#15272)
* [PyTorch-nightly-test] Fix Wav2Vec2 LM & Phoneme tests

* Update .github/workflows/self-nightly-scheduled.yml

* change lines

* Apply suggestions from code review
==

.github/workflows/self-nightly-scheduled.yml
==================
6ac77534b;Sylvain Gugger;2022-01-21 15:00:09 -0500;Refine errors for pretrained objects (#15261)
* Refine errors for pretrained objects

* PoC to avoid using get_list_of_files

* Adapt tests to use new errors

* Quality + Fix PoC

* Revert "PoC to avoid using get_list_of_files"

This reverts commit cb93b7cae8504ef837c2a7663cb7955e714f323e.

* Revert "Quality + Fix PoC"

This reverts commit 3ba6d0d4ca546708b31d355baa9e68ba9736508f.

* Fix doc

* Revert PoC

* Add feature extractors

* More tests and PT model

* Adapt error message

* Feature extractor tests

* TF model

* Flax model and test

* Merge flax auto tests

* Add tokenization

* Fix test
==

src/transformers/configuration_utils.py
src/transformers/feature_extraction_utils.py
src/transformers/file_utils.py
src/transformers/modeling_flax_utils.py
src/transformers/modeling_tf_utils.py
src/transformers/modeling_utils.py
src/transformers/models/auto/tokenization_auto.py
src/transformers/tokenization_utils_base.py
tests/test_configuration_auto.py
tests/test_feature_extraction_auto.py
tests/test_file_utils.py
tests/test_modeling_auto.py
tests/test_modeling_flax_auto.py
tests/test_modeling_tf_auto.py
tests/test_tokenization_auto.py
utils/tests_fetcher.py
==================
80af1048c;Patrick von Platen;2022-01-21 18:30:10 +0100;[Wav2Vec2ProcessorWithLM] improve multi processing (#15247)
* [Wav2Vec2ProcessorWithLM] improve multi processing

* close pool
==

src/transformers/models/wav2vec2_with_lm/processing_wav2vec2_with_lm.py
==================
4cff3fae1;Sylvain Gugger;2022-01-21 12:19:28 -0500;Second failing test

==

examples/pytorch/test_examples.py
==================
f6253147d;Sylvain Gugger;2022-01-21 12:03:21 -0500;Skip failing test

==

examples/pytorch/test_examples.py
==================
7799b6128;Yih-Dar;2022-01-21 17:18:11 +0100;[Fix doc example] TFLayoutLMForTokenClassification: missing import tf (#15268)
* fix import

* remove import torch

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>
==

src/transformers/models/layoutlm/modeling_tf_layoutlm.py
==================
11afb709e;Patrick von Platen;2022-01-21 17:12:09 +0100;[Robust Speech Challenge] Add timeline (#15274)

==

examples/research_projects/robust-speech-event/README.md
==================
3c3cf17a4;Evandros;2022-01-21 09:52:13 -0500;fix link (#15278)

==

notebooks/README.md
==================
95a75a715;Ye Wang;2022-01-21 06:49:29 -0800;Specify providers explicitly in ORT session initialization (#15235)
* Specify providers explicitly in ORT session initialization

Co-authored-by: Ubuntu <wy@linux-v100.aidmrjtolptuzevavgwhrapqcd.jx.internal.cloudapp.net>
==

src/transformers/onnx/convert.py
==================
833635e25;lewtun;2022-01-21 14:47:34 +0100;Move BART + ONNX example to research_projects (#15271)
* Move BART + ONNX example to research_projects

* Add author information
==

examples/research_projects/onnx/summarization/README.md
examples/research_projects/onnx/summarization/bart_onnx/generation_onnx.py
examples/research_projects/onnx/summarization/bart_onnx/reduce_onnx_size.py
examples/research_projects/onnx/summarization/requirements.txt
examples/research_projects/onnx/summarization/run_onnx_exporter.py
==================
183ce067e;novice;2022-01-21 19:16:15 +0530;Fix (#15276)
* Fix

* make style

* Remove trailing commas

* make style
==

src/transformers/models/swin/__init__.py
src/transformers/models/swin/configuration_swin.py
src/transformers/models/swin/modeling_swin.py
==================
b4ce313e6;lewtun;2022-01-21 14:28:19 +0100;Prepare ONNX export for torch v1.11 (#15270)
* Prepare ONNX export for torch v1.11
==

src/transformers/onnx/convert.py
==================
126bddd1b;Sylvain Gugger;2022-01-21 08:12:44 -0500;Add module_spec to new model

==

src/transformers/models/swin/__init__.py
==================
c962c2adb;Jonas Kuball;2022-01-21 13:30:12 +0100;Adds missing module_specs for usages of _LazyModule (#15230)
* Add missing __spec__ for transformers.models.auto

* Moves the __spec__-test to the UnitTest class

* Adds module_spec to all instances of _LazyModule

* Refactors an old test from pytest to unittest
==

src/transformers/models/albert/__init__.py
src/transformers/models/auto/__init__.py
src/transformers/models/bart/__init__.py
src/transformers/models/barthez/__init__.py
src/transformers/models/bartpho/__init__.py
src/transformers/models/beit/__init__.py
src/transformers/models/bert/__init__.py
src/transformers/models/bert_generation/__init__.py
src/transformers/models/bert_japanese/__init__.py
src/transformers/models/bertweet/__init__.py
src/transformers/models/big_bird/__init__.py
src/transformers/models/bigbird_pegasus/__init__.py
src/transformers/models/blenderbot/__init__.py
src/transformers/models/blenderbot_small/__init__.py
src/transformers/models/byt5/__init__.py
src/transformers/models/camembert/__init__.py
src/transformers/models/canine/__init__.py
src/transformers/models/clip/__init__.py
src/transformers/models/convbert/__init__.py
src/transformers/models/cpm/__init__.py
src/transformers/models/ctrl/__init__.py
src/transformers/models/deberta/__init__.py
src/transformers/models/deberta_v2/__init__.py
src/transformers/models/deit/__init__.py
src/transformers/models/detr/__init__.py
src/transformers/models/distilbert/__init__.py
src/transformers/models/dpr/__init__.py
src/transformers/models/electra/__init__.py
src/transformers/models/encoder_decoder/__init__.py
src/transformers/models/flaubert/__init__.py
src/transformers/models/fnet/__init__.py
src/transformers/models/fsmt/__init__.py
src/transformers/models/funnel/__init__.py
src/transformers/models/gpt2/__init__.py
src/transformers/models/gpt_neo/__init__.py
src/transformers/models/gptj/__init__.py
src/transformers/models/herbert/__init__.py
src/transformers/models/hubert/__init__.py
src/transformers/models/ibert/__init__.py
src/transformers/models/imagegpt/__init__.py
src/transformers/models/layoutlm/__init__.py
src/transformers/models/layoutlmv2/__init__.py
src/transformers/models/layoutxlm/__init__.py
src/transformers/models/led/__init__.py
src/transformers/models/longformer/__init__.py
src/transformers/models/luke/__init__.py
src/transformers/models/lxmert/__init__.py
src/transformers/models/m2m_100/__init__.py
src/transformers/models/marian/__init__.py
src/transformers/models/mbart/__init__.py
src/transformers/models/mbart50/__init__.py
src/transformers/models/megatron_bert/__init__.py
src/transformers/models/mluke/__init__.py
src/transformers/models/mmbt/__init__.py
src/transformers/models/mobilebert/__init__.py
src/transformers/models/mpnet/__init__.py
src/transformers/models/mt5/__init__.py
src/transformers/models/nystromformer/__init__.py
src/transformers/models/openai/__init__.py
src/transformers/models/pegasus/__init__.py
src/transformers/models/perceiver/__init__.py
src/transformers/models/phobert/__init__.py
src/transformers/models/prophetnet/__init__.py
src/transformers/models/qdqbert/__init__.py
src/transformers/models/rag/__init__.py
src/transformers/models/realm/__init__.py
src/transformers/models/reformer/__init__.py
src/transformers/models/rembert/__init__.py
src/transformers/models/retribert/__init__.py
src/transformers/models/roberta/__init__.py
src/transformers/models/roformer/__init__.py
src/transformers/models/segformer/__init__.py
src/transformers/models/sew/__init__.py
src/transformers/models/sew_d/__init__.py
src/transformers/models/speech_encoder_decoder/__init__.py
src/transformers/models/speech_to_text/__init__.py
src/transformers/models/speech_to_text_2/__init__.py
src/transformers/models/splinter/__init__.py
src/transformers/models/squeezebert/__init__.py
src/transformers/models/t5/__init__.py
src/transformers/models/tapas/__init__.py
src/transformers/models/transfo_xl/__init__.py
src/transformers/models/trocr/__init__.py
src/transformers/models/unispeech/__init__.py
src/transformers/models/unispeech_sat/__init__.py
src/transformers/models/vision_encoder_decoder/__init__.py
src/transformers/models/visual_bert/__init__.py
src/transformers/models/vit/__init__.py
src/transformers/models/vit_mae/__init__.py
src/transformers/models/wav2vec2/__init__.py
src/transformers/models/wav2vec2_phoneme/__init__.py
src/transformers/models/wav2vec2_with_lm/__init__.py
src/transformers/models/wavlm/__init__.py
src/transformers/models/xlm/__init__.py
src/transformers/models/xlm_prophetnet/__init__.py
src/transformers/models/xlm_roberta/__init__.py
src/transformers/models/xlnet/__init__.py
src/transformers/onnx/__init__.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/__init__.py
tests/test_configuration_auto.py
tests/test_file_utils.py
==================
6c7b68d41;NielsRogge;2022-01-21 12:11:08 +0100;[ViTMAE] Add image pretraining script (#15242)
* Add script

* Improve script

* Fix data collator

* Update README

* Add label_names argument

* Apply suggestions from code review

* Add config parameters

* Update script

* Fix bug

* Improve README

* Improve README and add test

* Fix import

* Add image_column_name
==

examples/pytorch/image-pretraining/README.md
examples/pytorch/image-pretraining/requirements.txt
examples/pytorch/image-pretraining/run_mae.py
examples/pytorch/test_examples.py
src/transformers/models/vit_mae/configuration_vit_mae.py
==================
d43e308e7;novice;2022-01-21 16:40:41 +0530;Add Swin Transformer (#15085)
* Add all files

* Apply suggestions from code review

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>

* Updates

* Apply suggestions from review

* Fix failing tests

* Update __init__.py

* Update configuration_swin.py

* Update auto_factory.py

* Fix pytests

* Apply suggestions from code review

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>

* Fix tests and default checkpoint

* Fix Recursion error

* Code quality

* Remove copied from

* Update modeling_swin.py

* Code quality

* Update modeling_swin.py

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Apply suggestions from code review

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>

* Apply suggestions from code review

* Fix feature extractor

* Fix code quality

* Apply suggestions from code review

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>

* Apply suggestions from code review

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>

* Apply suggestions from code review

* Update configuration_swin.py

* Update default checkpoint

* Apply suggestions from code review

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>

* Apply suggestions from code review

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>

* Update docs/source/model_doc/swin.mdx

Co-authored-by: Mishig Davaadorj <mishig.davaadorj@coloradocollege.edu>

* Update conversion script

* Reformat conversion script

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
Co-authored-by: Mishig Davaadorj <mishig.davaadorj@coloradocollege.edu>
==

README.md
README_ko.md
README_zh-hans.md
README_zh-hant.md
docs/source/_toctree.yml
docs/source/index.mdx
docs/source/model_doc/swin.mdx
src/transformers/__init__.py
src/transformers/models/__init__.py
src/transformers/models/auto/configuration_auto.py
src/transformers/models/auto/feature_extraction_auto.py
src/transformers/models/auto/modeling_auto.py
src/transformers/models/swin/__init__.py
src/transformers/models/swin/configuration_swin.py
src/transformers/models/swin/convert_swin_timm_to_pytorch.py
src/transformers/models/swin/modeling_swin.py
src/transformers/utils/dummy_pt_objects.py
tests/test_modeling_swin.py
==================
515ed3ad2;NielsRogge;2022-01-20 21:51:51 +0100;Fix doc examples (#15257)

==

docs/source/model_doc/trocr.mdx
src/transformers/models/vilt/modeling_vilt.py
==================
ad7390636;Lysandre Debut;2022-01-20 13:51:19 -0500;Tentative workflow improvement (#15255)

==

.github/workflows/build_dev_documentation.yml
.github/workflows/delete_dev_documentation.yml
==================
57820456b;Matt;2022-01-20 18:40:48 +0000;Fix crash when logs are empty because Keras has wiped them out of spite (#15258)

==

src/transformers/modelcard.py
==================
1fc0fa461;kumapo;2022-01-21 00:37:35 +0900;Make sure to raise NotImplementedError with correct method name (#15253)

==

src/transformers/modeling_utils.py
==================
f00f22a3e;Matt;2022-01-20 14:26:51 +0000;Fixes tf_default_data_collator sometimes guessing the wrong dtype for labels (#15234)
* Fixes tf_default_data_collator sometimes guessing the wrong dtype for labels

* Add test for numpy scalar inputs
==

src/transformers/data/data_collator.py
tests/test_data_collator.py
==================
4a6a35bc6;Yih-Dar;2022-01-20 14:47:24 +0100;[Fix doc example] missing import (#15240)
* fix import

* fix style

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>
==

src/transformers/models/encoder_decoder/modeling_tf_encoder_decoder.py
==================
08b41b413;Kamal Raj;2022-01-20 19:16:48 +0530;Update pipelines.mdx (#15243)
fix few spelling mistakes
==

docs/source/main_classes/pipelines.mdx
==================
85ea462c0;Anton Lozhkov;2022-01-20 13:40:26 +0300;Update README.md (#15246)
Clarify OVH instruction
==

examples/research_projects/robust-speech-event/README.md
==================
e57468b8a;Anton Lozhkov;2022-01-20 11:46:50 +0300;Update README.md (#15239)
Add an OVHcloud tutorial URL for the Robust Speech Challenge
==

examples/research_projects/robust-speech-event/README.md
==================
baf1ebe9f;jsnfly;2022-01-19 23:00:33 +0100;Fix usage of additional kwargs in `from_encoder_decoder_pretrained` in encoder-decoder models (#15056)
* [EncoderDecoder] Add test for usage of extra kwargs

* [EncoderDecoder] Fix usage of extra kwargs in from pretrained

* [EncoderDecoder] apply suggested changes (passing **kwargs_encoder)

* [EncoderDecoder] create new test function and make sure it passes

Co-authored-by: jonas <jsnfly@gmx.de>
==

src/transformers/models/encoder_decoder/modeling_encoder_decoder.py
src/transformers/models/speech_encoder_decoder/modeling_speech_encoder_decoder.py
src/transformers/models/vision_encoder_decoder/modeling_vision_encoder_decoder.py
tests/test_modeling_encoder_decoder.py
==================
3fefee991;Nicolas Patry;2022-01-19 21:04:26 +0100;Make chuking smartly (long files) work on asr ctc_with_lm. (#15219)
* [WIP] Make chuking smartly (long files) work on asr ctc_with_lm.

* Slow test with functionality.

* Fixing regular test.

* fix for batch size 1

* Handling batch outside `rescale_Stride`.

- Renamed to `rescale_stride`.

* Disable equality in the test.

* Remove print.

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
==

src/transformers/pipelines/automatic_speech_recognition.py
tests/test_pipelines_automatic_speech_recognition.py
==================
80f729609;NielsRogge;2022-01-19 20:15:12 +0100;Update Trainer code example (#15070)
* Update code example

* Fix code quality

* Add comment
==

docs/source/main_classes/trainer.mdx
==================
ac227093e;NielsRogge;2022-01-19 19:51:59 +0100;Add ViLT (#14895)
* First commit

* Add conversion script

* Make conversion script work for base model

* More improvements

* Update conversion script, works for vqa

* Add indexing argument to meshgrid

* Make conversion script work for ViltForPreTraining

* Add ViltForPreTraining to docs

* Fix device issue

* Add processor

* Add MinMaxResize to feature extractor

* Implement call method of ViltProcessor

* Fix tests

* Add integration test

* Add loss calculation for VQA

* Improve tests

* Improve some more tests

* Debug tests

* Small improvements

* Add support for attention_mask

* Remove mask_it

* Add pixel_mask

* Add tests for ViltFeatureExtractor

* Improve tests

* Add ViltForNaturalLanguageVisualReasoning

* Add ViltForNaturalLanguageVisualReasoning to conversion script

* Minor fixes

* Add support for image_embeds, update docstrings to markdown

* Update docs to markdown

* Improve conversion script

* Rename ViltForPreTraining to ViltForMaskedLM

* Improve conversion script

* Convert docstrings to markdown

* Fix code example of retrieval model

* Properly convert masked language model

* Add integration test for nlvr

* Fix code quality

* Apply suggestions from code review

* Add copied from statements

* Fix pretrained_config_archive_map

* Fix docs

* Add model to README

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Apply more suggestions from code review

* Make code more readable

* Add ViltForNaturalLanguageVisualReasoning to the tests

* Rename ViltForVisualQuestionAnswering to ViltForQuestionAnswering

* Replace pixel_values_2 by single tensor

* Add hidden_states and attentions

* Fix one more test

* Fix all tests

* Update year

* Fix rebase issues

* Fix another rebase issue

* Remove ViltForPreTraining from auto mapping

* Rename ViltForImageRetrievalTextRetrieval to ViltForImageAndTextRetrieval

* Make it possible to use BertTokenizerFast in the processor

* Use BertTokenizerFast by default

* Rename ViltForNaturalLanguageVisualReasoning, define custom model output

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

README.md
README_ko.md
README_zh-hans.md
README_zh-hant.md
docs/source/_toctree.yml
docs/source/index.mdx
docs/source/model_doc/vilt.mdx
src/transformers/__init__.py
src/transformers/models/__init__.py
src/transformers/models/auto/configuration_auto.py
src/transformers/models/auto/modeling_auto.py
src/transformers/models/deit/modeling_deit.py
src/transformers/models/vilt/__init__.py
src/transformers/models/vilt/configuration_vilt.py
src/transformers/models/vilt/convert_vilt_original_to_pytorch.py
src/transformers/models/vilt/feature_extraction_vilt.py
src/transformers/models/vilt/modeling_vilt.py
src/transformers/models/vilt/processing_vilt.py
src/transformers/models/vit/modeling_vit.py
src/transformers/utils/dummy_pt_objects.py
src/transformers/utils/dummy_vision_objects.py
tests/test_feature_extraction_vilt.py
tests/test_modeling_vilt.py
utils/check_repo.py
==================
691878ee2;Patrick von Platen;2022-01-19 18:03:17 +0100;Update README.md (#15233)

==

examples/research_projects/robust-speech-event/README.md
==================
f4b7420df;Sylvain Gugger;2022-01-19 11:22:54 -0500;Fix checkpoint for ViT Config

==

src/transformers/models/vit/configuration_vit.py
==================
6a3c883c8;Lysandre Debut;2022-01-19 11:00:16 -0500;Fix PR number (#15231)
* Fix PR number

* Fix PR number
==

.github/workflows/build_dev_documentation.yml
==================
f778edb73;Li-Huai (Allan) Lin;2022-01-19 23:16:19 +0800;Fix typo in BERT tokenization file (#15228)
* Fix typo

* Fix copies
==

src/transformers/models/bert/tokenization_bert.py
src/transformers/models/bert/tokenization_bert_fast.py
src/transformers/models/layoutlmv2/tokenization_layoutlmv2.py
src/transformers/models/mpnet/tokenization_mpnet.py
src/transformers/models/tapas/tokenization_tapas.py
==================
2a5a38497;Suraj Patil;2022-01-19 15:30:03 +0100;fix speech event readme (#15227)

==

examples/research_projects/robust-speech-event/README.md
==================
842298f84;NielsRogge;2022-01-19 15:27:57 +0100;[ViTMAE] Various fixes (#15221)
* Add MAE to AutoFeatureExtractor

* Add link to notebook

* Fix relative paths
==

docs/source/model_doc/vit.mdx
docs/source/model_doc/vit_mae.mdx
src/transformers/models/auto/feature_extraction_auto.py
src/transformers/models/vit_mae/modeling_vit_mae.py
==================
6d92c429c;Patrick von Platen;2022-01-19 15:23:00 +0100;Update README.md (#15226)

==

examples/research_projects/robust-speech-event/README.md
==================
19c217b4b;Patrick von Platen;2022-01-19 15:21:03 +0100;Update README.md

==

examples/research_projects/robust-speech-event/README.md
==================
5439cda7f;Patrick von Platen;2022-01-19 15:19:57 +0100;Update README.md

==

examples/research_projects/robust-speech-event/README.md
==================
841d97919;Li-Huai (Allan) Lin;2022-01-19 22:19:36 +0800;Add FastTokenizer to REALM (#15211)
* Remove BertTokenizer abstraction

* Add FastTokenizer to REALM

* Fix config archive map

* Fix copies

* Update realm.mdx

* Apply suggestions from code review
==

docs/source/index.mdx
docs/source/model_doc/realm.mdx
src/transformers/__init__.py
src/transformers/convert_slow_tokenizer.py
src/transformers/models/realm/__init__.py
src/transformers/models/realm/configuration_realm.py
src/transformers/models/realm/tokenization_realm.py
src/transformers/models/realm/tokenization_realm_fast.py
src/transformers/utils/dummy_tokenizers_objects.py
tests/test_tokenization_realm.py
==================
021b52e7a;Yih-Dar;2022-01-19 15:06:00 +0100;fix name 'TFFunnelTokenizer' is not defined (#15225)
Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>
==

src/transformers/models/funnel/modeling_tf_funnel.py
==================
653379c09;Lysandre Debut;2022-01-19 08:47:34 -0500;Build dev documentation (#15210)
* Wrap up

* Remove secret

* Fix path

* Typo

Revert image switch

* Specific token for comments

* Cleaner comments

* Correct PR number

* Explicit master install

* Force uninstall
==

.github/workflows/build_dev_documentation.yml
.github/workflows/delete_dev_documentation.yml
==================
2708bfa12;Matt;2022-01-19 13:29:07 +0000;Rename compute_loss in TF models (#15207)
* Rename compute_loss to hf_compute_loss to avoid conflicts with the new Keras method

* make style

* Adding deprecation warning to `compute_loss`

* Fix sneaky reference to compute_loss

* Replace logger.warning with warnings.warn

* Clarifying warning and deprecation timeline
==

src/transformers/modeling_tf_utils.py
src/transformers/models/albert/modeling_tf_albert.py
src/transformers/models/bart/modeling_tf_bart.py
src/transformers/models/bert/modeling_tf_bert.py
src/transformers/models/blenderbot/modeling_tf_blenderbot.py
src/transformers/models/blenderbot_small/modeling_tf_blenderbot_small.py
src/transformers/models/convbert/modeling_tf_convbert.py
src/transformers/models/ctrl/modeling_tf_ctrl.py
src/transformers/models/deberta/modeling_tf_deberta.py
src/transformers/models/deberta_v2/modeling_tf_deberta_v2.py
src/transformers/models/distilbert/modeling_tf_distilbert.py
src/transformers/models/electra/modeling_tf_electra.py
src/transformers/models/funnel/modeling_tf_funnel.py
src/transformers/models/gpt2/modeling_tf_gpt2.py
src/transformers/models/layoutlm/modeling_tf_layoutlm.py
src/transformers/models/led/modeling_tf_led.py
src/transformers/models/longformer/modeling_tf_longformer.py
src/transformers/models/marian/modeling_tf_marian.py
src/transformers/models/mbart/modeling_tf_mbart.py
src/transformers/models/mobilebert/modeling_tf_mobilebert.py
src/transformers/models/mpnet/modeling_tf_mpnet.py
src/transformers/models/openai/modeling_tf_openai.py
src/transformers/models/pegasus/modeling_tf_pegasus.py
src/transformers/models/rag/modeling_tf_rag.py
src/transformers/models/rembert/modeling_tf_rembert.py
src/transformers/models/roberta/modeling_tf_roberta.py
src/transformers/models/roformer/modeling_tf_roformer.py
src/transformers/models/t5/modeling_tf_t5.py
src/transformers/models/tapas/modeling_tf_tapas.py
src/transformers/models/transfo_xl/modeling_tf_transfo_xl.py
src/transformers/models/vit/modeling_tf_vit.py
src/transformers/models/xlm/modeling_tf_xlm.py
src/transformers/models/xlnet/modeling_tf_xlnet.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_tf_{{cookiecutter.lowercase_modelname}}.py
tests/test_modeling_tf_common.py
==================
d1f5ca1af;Kamal Raj;2022-01-19 16:34:51 +0530;[FLAX] glue training example refactor (#13815)
* refactor run_flax_glue.py

* updated readme

* rm unused import and args typo fix

* refactor

* make consistent arg name across task

* has_tensorboard check

* argparse -> argument dataclasses

* refactor according to review

* fix
==

examples/flax/test_examples.py
examples/flax/text-classification/README.md
examples/flax/text-classification/run_flax_glue.py
==================
db3503949;Sylvain Gugger;2022-01-18 18:00:30 -0500;Finish conversion of REALM doc to MDX

==

docs/source/model_doc/realm.mdx
==================
fe78fe98c;Jake Tae;2022-01-19 07:52:35 +0900;Enable tqdm toggling (#15167)
* feature: enable tqdm toggle

* test: add tqdm unit test

* style: run linter

* Update tests/test_tqdm_utils.py

Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>

* refactor: use tiny model, run linter

* docs: add tqdm to logging

* docs: add tqdm reference to `http_get`

* style: run linter

* Update docs/source/main_classes/logging.mdx

Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>

* fix: use `AutoConfig` for framework agnostic testing

* chore: mv tqdm test to `test_logging.py`

* feature: implement enable/disable functions

* docs: mv docstring to comment

* chore: mv tqdm functions to `logging.py`

* docs: update docs to reference `enable/disable` funcs

* test: update test to use `enable/disable` func

* chore: update function reference in comment

Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>
==

docs/source/main_classes/logging.mdx
src/transformers/file_utils.py
src/transformers/utils/logging.py
tests/test_logging.py
==================
2c335037b;Sylvain Gugger;2022-01-18 17:46:29 -0500;Trigger doc build

==
==================
e118e085e;Patrick von Platen;2022-01-18 18:44:48 +0100;[Robust Speech Event] Add guides (#15155)
* up

* improve readme

* up

* up

* more info

* up

* up

* Apply suggestions from code review

Co-authored-by: Anton Lozhkov <aglozhkov@gmail.com>

* add more stuff for eval

* update

* up

* Update README.md

* Update examples/research_projects/xls_r/README.md

Co-authored-by: Omar Sanseviero <osanseviero@users.noreply.github.com>

* apply omar's suggestions

Co-authored-by: Anton Lozhkov <aglozhkov@gmail.com>
Co-authored-by: Omar Sanseviero <osanseviero@users.noreply.github.com>
==

examples/research_projects/robust-speech-event/README.md
examples/research_projects/robust-speech-event/eval.py
examples/research_projects/robust-speech-event/run_speech_recognition_ctc_bnb.py
examples/research_projects/xls_r/README.md
==================
1a354d53c;matt;2022-01-18 17:34:26 +0000;Revert previous change - that was meant to be in a branch!

==

tests/test_modeling_tf_common.py
==================
2085f2090;matt;2022-01-18 17:33:38 +0000;Fix a sneaky reference to compute_loss in the tests

==

tests/test_modeling_tf_common.py
==================
979ca24e3;Yih-Dar;2022-01-18 16:43:21 +0100;[Fix doc example] Wrong checkpoint name (#15079)
* fix doc example - MarianForCausalLM example

* try to keep copies

* fix copies

* fix more similar doc examples

* fix more

* fix style

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>
==

src/transformers/models/blenderbot/modeling_blenderbot.py
src/transformers/models/blenderbot_small/modeling_blenderbot_small.py
src/transformers/models/marian/modeling_marian.py
src/transformers/models/mbart/modeling_mbart.py
src/transformers/models/pegasus/modeling_pegasus.py
==================
7b3d4df47;PaulLerner;2022-01-18 16:36:12 +0100;fix: #14486 do not use BertPooler in DPR (#15068)
* fix: #14486 do not use BertPooler in DPR

* fix tf dpr as well

* finish

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
==

src/transformers/models/dpr/modeling_dpr.py
src/transformers/models/dpr/modeling_tf_dpr.py
==================
74bec9865;NielsRogge;2022-01-18 16:21:32 +0100;Add MAE (#15120)
* First draft

* More improvements

* More improvements

* More improvements

* Fix embeddings

* Add conversion script

* Finish conversion script

* More improvements

* Fix forward pass

* Remove print statements

* Add weights initialization

* Add initialization of decoder weights

* Add support for other models in the conversion script

* Fix patch_size for huge model

* Fix most of the tests

* Fix integration test

* Fix docs

* Fix archive_list

* Apply suggestions from code review

* Improve documentation

* Apply more suggestions

* Skip some tests due to non-deterministic behaviour

* Fix test_initialization

* Remove unneccessary initialization of nn.Embedding

* Improve docs

* Fix dummies

* Remove ViTMAEFeatureExtractor from docs

* Add model to README and table of contents

* Delete inference file
==

README.md
README_ko.md
README_zh-hans.md
README_zh-hant.md
docs/source/_toctree.yml
docs/source/index.mdx
docs/source/model_doc/vit_mae.mdx
src/transformers/__init__.py
src/transformers/models/__init__.py
src/transformers/models/auto/configuration_auto.py
src/transformers/models/auto/modeling_auto.py
src/transformers/models/vit_mae/__init__.py
src/transformers/models/vit_mae/configuration_vit_mae.py
src/transformers/models/vit_mae/convert_vit_mae_to_pytorch.py
src/transformers/models/vit_mae/modeling_vit_mae.py
src/transformers/utils/dummy_pt_objects.py
tests/test_modeling_vit_mae.py
==================
2ae3be544;Suraj Patil;2022-01-18 16:02:56 +0100;[MBartTokenizer] remove dep on xlm-roberta tokenizer (#15201)

==

src/transformers/models/mbart/tokenization_mbart.py
src/transformers/models/mbart/tokenization_mbart_fast.py
==================
84c60a7b5;Sylvain Gugger;2022-01-18 09:48:46 -0500;Ignore empty subfolders when identifying submodules (#15204)
* Ignore empty subfolders when identifying submodules

* Update utils/check_inits.py
==

utils/check_inits.py
==================
6f0a9b41e;Sylvain Gugger;2022-01-18 09:44:35 -0500;Remove dependency to quiet Dependabot (#15205)

==

examples/legacy/pytorch-lightning/requirements.txt
==================
497346d07;Patrick von Platen;2022-01-18 15:36:22 +0100;[ASR pipeline] correct with lm pipeline (#15200)
* [ASR pipeline] correct with lm pipeline

* improve error
==

setup.py
src/transformers/dependency_versions_table.py
src/transformers/feature_extraction_utils.py
src/transformers/pipelines/__init__.py
tests/test_feature_extraction_auto.py
tests/test_pipelines_automatic_speech_recognition.py
==================
1144d336b;Sylvain Gugger;2022-01-18 09:16:55 -0500;Copies and docstring styling (#15202)
* Style docstrings when making/checking copies

* Polish
==

utils/check_copies.py
utils/style_doc.py
==================
531336bbf;Sylvain Gugger;2022-01-18 07:28:53 -0500;Fix deprecation warnings for int div (#15180)
* Fix deprecation warnings for int div

Co-authored-by: mgoldey <matthew.goldey@gmail.com>

* Fix import

* ensure that tensor output is python scalar

* make backward compatible

* make code more readable

* adapt test functions

Co-authored-by: mgoldey <matthew.goldey@gmail.com>
Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
==

examples/pytorch/speech-pretraining/run_wav2vec2_pretraining_no_trainer.py
src/transformers/modeling_utils.py
src/transformers/models/hubert/modeling_hubert.py
src/transformers/models/sew/modeling_sew.py
src/transformers/models/sew_d/modeling_sew_d.py
src/transformers/models/unispeech/modeling_unispeech.py
src/transformers/models/unispeech_sat/modeling_unispeech_sat.py
src/transformers/models/wav2vec2/modeling_wav2vec2.py
src/transformers/models/wavlm/modeling_wavlm.py
tests/test_modeling_wav2vec2.py
==================
f6d3fee85;Sylvain Gugger;2022-01-18 07:27:34 -0500;Error when code examples are improperly closed (#15186)

==

utils/style_doc.py
==================
22454ae49;Li-Huai (Allan) Lin;2022-01-18 20:24:13 +0800;Add REALM (#13292)
* REALM initial commit

* Retriever OK (Update new_gelu).

* Encoder prediction score OK

* Encoder pretrained model OK

* Update retriever comments

* Update docs, tests, and imports

* Prune unused models

* Make embedder as a module `RealmEmbedder`

* Add RealmRetrieverOutput

* Update tokenization

* Pass all tests in test_modeling_realm.py

* Prune RealmModel

* Update docs

* Add training test.

* Remove completed TODO

* Style & Quality

* Prune `RealmModel`

* Fixup

* Changes:
1. Remove RealmTokenizerFast
2. Update docstrings
3. Add a method to RealmTokenizer to handle candidates tokenization.

* Fix up

* Style

* Add tokenization tests

* Update `from_pretrained` tests

* Apply suggestions

* Style & Quality

* Copy BERT model

* Fix comment to avoid docstring copying

* Make RealmBertModel private

* Fix bug

* Style

* Basic QA

* Save

* Complete reader logits

* Add searcher

* Complete searcher & reader

* Move block records init to constructor

* Fix training bug

* Add some outputs to RealmReader

* Add finetuned checkpoint variable names parsing

* Fix bug

* Update REALM config

* Add RealmForOpenQA

* Update convert_tfrecord logits

* Fix bugs

* Complete imports

* Update docs

* Update naming

* Add brute-force searcher

* Pass realm model tests

* Style

* Exclude RealmReader from common tests

* Fix

* Fix

* convert docs

* up

* up

* more make style

* up

* upload

* up

* Fix

* Update src/transformers/__init__.py

* adapt testing

* change modeling code

* fix test

* up

* up

* up

* correct more

* make retriever work

* update

* make style

* finish main structure

* Resolve merge conflict

* Make everything work

* Style

* Fixup

* Fixup

* Update training test

* fix retriever

* remove hardcoded path

* Fix

* Fix modeling test

* Update model links

* Initial retrieval test

* Fix modeling test

* Complete retrieval tests

* Fix

* style

* Fix tests

* Fix docstring example

* Minor fix of retrieval test

* Update license headers and docs

* Apply suggestions from code review

* Style

* Apply suggestions from code review

* Add an example to RealmEmbedder

* Fix

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

README.md
README_ko.md
README_zh-hans.md
README_zh-hant.md
docs/source/_toctree.yml
docs/source/index.mdx
docs/source/model_doc/realm.mdx
src/transformers/__init__.py
src/transformers/models/__init__.py
src/transformers/models/auto/configuration_auto.py
src/transformers/models/realm/__init__.py
src/transformers/models/realm/configuration_realm.py
src/transformers/models/realm/modeling_realm.py
src/transformers/models/realm/retrieval_realm.py
src/transformers/models/realm/tokenization_realm.py
src/transformers/utils/dummy_pt_objects.py
tests/test_modeling_realm.py
tests/test_retrieval_realm.py
tests/test_tokenization_realm.py
utils/check_repo.py
==================
b25067d80;Yih-Dar;2022-01-18 13:16:30 +0100;[Fix doc example] TFRagModel (#15187)
* fix doc example - NameError: name 'PATH' is not defined

* fix name 'TFRagModel' is not defined

* correct TFRagRagSequenceForGeneration

* fix name 'tf' is not defined

* fix style

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>
==

src/transformers/models/rag/modeling_tf_rag.py
==================
dea563c94;Nicolas Patry;2022-01-18 12:20:10 +0100;`is_ctc` needs to be updated to `self.type == "ctc". (#15194)
* `is_ctc` needs to be updated to `self.type == "ctc".

* Adding fast test for this functionality.
==

src/transformers/pipelines/automatic_speech_recognition.py
tests/test_pipelines_automatic_speech_recognition.py
==================
32090c729;Yih-Dar;2022-01-18 00:34:05 +0100;[Fix doc example] UniSpeechSatForPreTraining (#15152)
* fix doc example - cannot import name 'UniSpeechSatFeatureEncoder'

* fix ckpt name

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>
==

src/transformers/models/unispeech_sat/modeling_unispeech_sat.py
==================
6f8e644f0;Sylvain Gugger;2022-01-17 15:20:58 -0500;Mark bad tokenizers version (#15188)

==

setup.py
src/transformers/dependency_versions_table.py
==================
edd3fce2f;Stas Bekman;2022-01-17 09:10:51 -0800;[doc] new MoE paper (#15184)
add new paper
==

docs/source/performance.mdx
==================
9a2dabae7;Matt;2022-01-17 14:02:55 +0000;Fix dtype issue in TF BART (#15178)

==

src/transformers/models/bart/modeling_tf_bart.py
==================
0167edc85;MrinalTyagi;2022-01-17 17:52:41 +0530;Added forward pass of test_inference_image_classification_head with torch.no_grad() (#14777)

==

tests/test_modeling_vit.py
==================
7a787c68c;Patrick von Platen;2022-01-16 17:15:19 +0100;[Speech models] Disable non-existing chunking in tests (#15163)

==

tests/test_modeling_hubert.py
tests/test_modeling_sew.py
tests/test_modeling_sew_d.py
tests/test_modeling_unispeech.py
tests/test_modeling_unispeech_sat.py
tests/test_modeling_wav2vec2.py
==================
669e3c50c;Stas Bekman;2022-01-14 18:25:20 -0800;[doc] performance: Efficient Software Prebuilds (#15147)
* Efficient Software Prebuilds

* improve
==

docs/source/performance.mdx
==================
ebc4edfe7;Joao Gante;2022-01-14 17:35:39 +0000;update from keras2onnx to tf2onnx (#15162)

==

setup.py
src/transformers/convert_graph_to_onnx.py
src/transformers/dependency_versions_table.py
src/transformers/file_utils.py
src/transformers/testing_utils.py
tests/test_modeling_tf_common.py
==================
1b730c3d1;Sylvain Gugger;2022-01-14 10:59:41 -0500;Better dummies (#15148)
* Better dummies

* See if this fixes the issue

* Fix quality

* Style

* Add doc for DummyObject
==

src/transformers/file_utils.py
src/transformers/utils/dummy_flax_objects.py
src/transformers/utils/dummy_pt_objects.py
src/transformers/utils/dummy_pytorch_quantization_and_torch_objects.py
src/transformers/utils/dummy_scatter_objects.py
src/transformers/utils/dummy_sentencepiece_and_speech_objects.py
src/transformers/utils/dummy_sentencepiece_and_tokenizers_objects.py
src/transformers/utils/dummy_sentencepiece_objects.py
src/transformers/utils/dummy_speech_objects.py
src/transformers/utils/dummy_tf_objects.py
src/transformers/utils/dummy_timm_and_vision_objects.py
src/transformers/utils/dummy_tokenizers_objects.py
src/transformers/utils/dummy_vision_objects.py
utils/check_dummies.py
utils/check_repo.py
==================
b212ff9f4;Nicolas Patry;2022-01-14 16:47:03 +0100;Fixing flaky test (hopefully). (#15154)
* Fixing flaky test (hopefully).

* tf compliant.
==

src/transformers/pipelines/question_answering.py
==================
7d9a33fb5;Joao Gante;2022-01-14 15:19:04 +0000;TF Bert inference - support `np.ndarray` optional arguments (#15074)
* TF Bert inference - support np.ndarray optional arguments

* apply np input tests to all TF architectures
==

src/transformers/modeling_tf_utils.py
tests/test_modeling_tf_common.py
==================
4663c609b;AK391;2022-01-14 10:12:30 -0500;Add "open in hf spaces" gradio button issue #73 (#15106)
* update XLMProphetNet link

* update DPR link

* change prophetnet link

* change link MBART

* change link GPT

* update gpt2 link

* ctrl update link

* update Transformer-XL link

* Update Reformer link

* update xlnet link

* bert update link

* udpate albert link

* roberta update link

* update distilbert link

* update convbert link

* update XLM link

* xlm roberta update link

* update Flaubert link

* update electra link

* update funnel transformer and longformer

* bart update link

* pegasus update link

* udpate marianmt link

* t5 update link

* mt5 update link
==

docs/source/model_summary.mdx
==================
735d2bb69;novice;2022-01-14 19:24:01 +0530;Update test_configuration_common.py (#15160)

==

tests/test_configuration_common.py
==================
51d7ebf26;SaulLu;2022-01-14 14:22:03 +0100;fix BertTokenizerFast `tokenize_chinese_chars` arg (#15158)
* add new test

* fix in init

* more relevant test
==

src/transformers/models/bert/tokenization_bert_fast.py
tests/test_tokenization_bert.py
==================
4aa16fce6;Yih-Dar;2022-01-14 13:42:13 +0100;fix doc example - object has no attribute 'lm_logits' (#15143)
Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>
==

src/transformers/models/openai/modeling_openai.py
==================
7cbf8429d;Sylvain Gugger;2022-01-14 07:37:51 -0500;Make sure all submodules are properly registered (#15144)
* Make sure all submodules are properly registered

* Try to fix tests

* Fix tests
==

src/transformers/__init__.py
utils/check_inits.py
==================
c4f7eb124;Joao Gante;2022-01-14 10:42:08 +0000;add TF glu activation function (#15146)

==

src/transformers/activations_tf.py
tests/test_activations_tf.py
==================
5f3c57fc8;Sylvain Gugger;2022-01-14 04:52:38 -0500;Check the repo consistency in model templates test (#15141)
* Check the repo consistency in model templates test

* Fix doc template

* Fix docstrings

* Fix last docstring
==

.github/workflows/model-templates.yml
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_flax_{{cookiecutter.lowercase_modelname}}.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_tf_{{cookiecutter.lowercase_modelname}}.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/{{cookiecutter.lowercase_modelname}}.mdx
==================
96881729c;Sylvain Gugger;2022-01-13 17:34:41 -0500;Remove assert on optional arg

==

examples/pytorch/summarization/run_summarization.py
==================
1eb40338a;Stas Bekman;2022-01-13 13:48:51 -0800;[deepspeed tests] fix summarization (#15149)

==

tests/deepspeed/test_model_zoo.py
==================
6e058e84f;Yanming Wang;2022-01-13 12:21:00 -0800;Enable AMP for xla:gpu device in trainer class (#15022)
* Multiple fixes of trainer class with XLA GPU

* Make fp16 valid for xla:gpu

* Add mark_step in should_log to reduce compilation overhead
==

src/transformers/trainer.py
src/transformers/training_args.py
==================
3fc221d07;Carlos Aguayo;2022-01-13 12:26:02 -0500;Update model_sharing.mdx (#15142)
Fix typo
==

docs/source/model_sharing.mdx
==================
7b83feb50;Manuel R. Ciosici;2022-01-13 11:14:51 -0500;Deprecates AdamW and adds `--optim` (#14744)
* Add AdamW deprecation warning

* Add --optim to Trainer

* Update src/transformers/optimization.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/optimization.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/optimization.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/optimization.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/training_args.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/training_args.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/training_args.py

* fix style

* fix

* Regroup adamws together

Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>

* Change --adafactor to --optim adafactor

* Use Enum for optimizer values

* fixup! Change --adafactor to --optim adafactor

* fixup! Change --adafactor to --optim adafactor

* fixup! Change --adafactor to --optim adafactor

* fixup! Use Enum for optimizer values

* Improved documentation for --adafactor

Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>

* Add mention of no_deprecation_warning

Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>

* Rename OptimizerOptions to OptimizerNames

* Use choices for --optim

* Move optimizer selection code to a function and add a unit test

* Change optimizer names

* Rename method

Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>

* Rename method

Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>

* Remove TODO comment

Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>

* Rename variable

Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>

* Rename variable

Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>

* Rename function

* Rename variable

* Parameterize the tests for supported optimizers

* Refactor

* Attempt to make tests pass on CircleCI

* Add a test with apex

* rework to add apex to parameterized; add actual train test

* fix import when torch is not available

* fix optim_test_params when torch is not available

* fix optim_test_params when torch is not available

* re-org

* small re-org

* fix test_fused_adam_no_apex

* Update src/transformers/training_args.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/training_args.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/training_args.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Remove .value from OptimizerNames

* Rename optimizer strings s|--adam_|--adamw_|

* Also rename Enum options

* small fix

* Fix instantiation of OptimizerNames. Remove redundant test

* Use ExplicitEnum instead of Enum

* Add unit test with string optimizer

* Change optimizer default to string value

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>
Co-authored-by: Stas Bekman <stas@stason.org>
==

src/transformers/optimization.py
src/transformers/trainer.py
src/transformers/training_args.py
tests/test_trainer.py
==================
762416ffa;Stas Bekman;2022-01-13 06:17:28 -0800;[examples/flax/language-modeling] set loglevel (#15129)

==

examples/flax/language-modeling/run_mlm_flax.py
examples/flax/language-modeling/run_t5_mlm_flax.py
==================
74837171a;Yih-Dar;2022-01-13 12:45:30 +0100;fix doc example - AssertionError: has to be configured as a decoder. (#15124)
Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>
==

src/transformers/models/prophetnet/modeling_prophetnet.py
==================
6950ccec1;Lysandre Debut;2022-01-13 12:02:24 +0100;doc-builder -> doc-build (#15134)
* Updated script

* Commit everything

* Ready for review!

* Update .github/workflows/build_documentation.yml

Co-authored-by: Julien Chaumond <julien@huggingface.co>

Co-authored-by: Julien Chaumond <julien@huggingface.co>
==

.github/workflows/build_documentation.yml
==================
9a94bb8e2;Edoardo Federici;2022-01-12 22:39:33 +0100;mBART support for run_summarization.py (#15125)
* Update run_summarization.py

* Fixed languages and added missing code

* fixed obj, docs, removed source_lang and target_lang

* make style, run_summarization.py reformatted
==

examples/pytorch/summarization/run_summarization.py
==================
97f3beed3;Jake Tae;2022-01-13 00:42:39 +0900;Add `with torch.no_grad()` to DistilBERT integration test forward pass (#14979)
* refactor: wrap forward pass around no_grad context

* Update tests/test_modeling_distilbert.py

* fix: rm `no_grad` from non-integration tests

* chore: rm whitespace change
==

tests/test_modeling_distilbert.py
==================
021f2ea98;lewtun;2022-01-12 16:33:32 +0100;Add ONNX configuration classes to docs (#15121)
* Add ONNX classes to main package

* Remove permalinks from ONNX guide

* Fix ToC entry

* Revert "Add ONNX classes to main package"

This reverts commit eb794a5b00d66b0b4eab234987301676d8357630.

* Add ONNX classes to main doc

* Fix syntax highlighting in doc

* Fix text

* Add FeaturesManager to doc

* Use paths to reference ONNX classes

* Add FeaturesManager to init

* Add missing ONNX paths
==

docs/source/_toctree.yml
docs/source/main_classes/onnx.mdx
docs/source/serialization.mdx
src/transformers/onnx/__init__.py
src/transformers/onnx/convert.py
==================
c425d60bb;Sylvain Gugger;2022-01-12 09:32:53 -0500;Fix link to deepspeed config

==

docs/source/main_classes/deepspeed.mdx
==================
682090445;Yih-Dar;2022-01-12 15:29:09 +0100;Fix #14357 (#15001)
Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>
==

src/transformers/models/encoder_decoder/modeling_tf_encoder_decoder.py
==================
aa0135f2e;Leandro von Werra;2022-01-12 15:12:43 +0100;fix: switch from slow to generic tokenizer class (#15122)

==

examples/research_projects/codeparrot/scripts/bpe_training.py
==================
27b819b0e;Russell Klopfer;2022-01-12 08:57:00 -0500;use block_size instead of max_seq_length in tf run_clm example (#15036)
* use block_size instead of max_seq_length

* fixup

* remove pad_to_block_size

Co-authored-by: Russell Klopfer <russell@kloper.us>
==

examples/tensorflow/language-modeling/run_clm.py
==================
68cc4ccde;Nicolas Patry;2022-01-12 09:28:19 +0100;Pipeline ASR with LM. (#15071)
* Pipeline ASR with LM.

* Revamped into `self.decoder`.

* Fixing.

* 2nd fix.

* Update src/transformers/pipelines/__init__.py

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* Fixing.

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
==

src/transformers/pipelines/__init__.py
src/transformers/pipelines/automatic_speech_recognition.py
tests/test_pipelines_automatic_speech_recognition.py
==================
1a00863e9;Sylvain Gugger;2022-01-11 15:22:15 -0500;Fix typo in doc template

==

templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/{{cookiecutter.lowercase_modelname}}.mdx
==================
44eaa2b30;Matt;2022-01-11 19:05:39 +0000;Update TF test_step to match train_step (#15111)
* Update TF test_step to match train_step

* Update compile() warning to be clearer about what to pass
==

src/transformers/modeling_tf_utils.py
==================
57b980a61;Vladimir Maryasin;2022-01-11 19:19:33 +0100;Fix saving FlaubertTokenizer configs (#14991)
All specific tokenizer config properties must be passed to its base
class (XLMTokenizer) in order to be saved. This was not the case for
do_lowercase config. Thus it was not saved by save_pretrained() method
and saving and reloading the tokenizer changed its behaviour.

This commit fixes it.
==

src/transformers/models/flaubert/tokenization_flaubert.py
==================
16f0b7d72;lewtun;2022-01-11 18:06:05 +0100;Update ONNX docs (#14904)
* Remove docs for deprecated ONNX export

* Tidy up the CLI help messages

* Revamp ONNX docs

* Update auto-config table

* Use DistilBERT as example for consistency

* Wrap up first pass at ONNX docs

* Fix table check

* Add tweaks and introduction

* Add cross-ref

* Fix missing import

* Fix style

* Add permalinks to ONNX configs

* Clarify role of OrderedDict

* Update docs/source/serialization.mdx

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Add doctest syntax to code blocks

* Remove permalinks

* Revert "Remove permalinks"

This reverts commit 099701daf0db27823457867938efdb2d4f22a7c1.

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

docs/source/serialization.mdx
src/transformers/onnx/__main__.py
utils/check_table.py
==================
704d1feca;Sylvain Gugger;2022-01-11 11:45:39 -0500;Doc styler tip (#15105)
* Add new lines before/after tips

* Check end of lines
==

utils/style_doc.py
==================
68d925195;AK391;2022-01-11 11:11:29 -0500;Merge branch 'master' into master

==
==================
7480ded65;Lysandre Debut;2022-01-11 15:57:34 +0100;Fix failing test (#15104)

==

tests/test_processor_wav2vec2_with_lm.py
==================
28e091430;novice;2022-01-11 18:55:49 +0530;Add Nystromformer (#14659)
* Initial commit

* Config and modelling changes

Added Nystromformer-specific attributes to config and removed all decoder functionality from modelling.

* Modelling and test changes

Added Nystrom approximation and removed decoder tests.

* Code quality fixes

* Modeling changes and conversion script

Initial commits to conversion script, modeling changes.

* Minor modeling changes and conversion script

* Modeling changes

* Correct modeling, add tests and documentation

* Code refactor

* Remove tokenizers

* Code refactor

* Update __init__.py

* Fix bugs

* Update src/transformers/__init__.py

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>

* Update src/transformers/__init__.py

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>

* Update src/transformers/models/nystromformer/__init__.py

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>

* Update docs/source/model_doc/nystromformer.mdx

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>

* Update src/transformers/models/nystromformer/configuration_nystromformer.py

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>

* Update src/transformers/models/nystromformer/configuration_nystromformer.py

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>

* Update src/transformers/models/nystromformer/configuration_nystromformer.py

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>

* Update src/transformers/models/nystromformer/configuration_nystromformer.py

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>

* Update src/transformers/models/nystromformer/convert_nystromformer_original_pytorch_checkpoint_to_pytorch.py

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>

* Update src/transformers/models/nystromformer/configuration_nystromformer.py

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>

* Update modeling and test_modeling

* Code refactor

* .rst to .mdx

* doc changes

* Doc changes

* Update modeling_nystromformer.py

* Doc changes

* Fix copies

* Apply suggestions from code review

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>

* Apply suggestions from code review

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>

* Update configuration_nystromformer.py

* Fix copies

* Update tests/test_modeling_nystromformer.py

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>

* Update test_modeling_nystromformer.py

* Apply suggestions from code review

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* Fix code style

* Update modeling_nystromformer.py

* Update modeling_nystromformer.py

* Fix code style

* Reformat modeling file

* Update modeling_nystromformer.py

* Modify NystromformerForMultipleChoice

* Fix code quality

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Code style changes and torch.no_grad()

* make style

* Apply suggestions from code review

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>
Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

README.md
README_ko.md
README_zh-hans.md
README_zh-hant.md
docs/source/_toctree.yml
docs/source/index.mdx
docs/source/model_doc/nystromformer.mdx
src/transformers/__init__.py
src/transformers/models/__init__.py
src/transformers/models/auto/configuration_auto.py
src/transformers/models/auto/modeling_auto.py
src/transformers/models/nystromformer/__init__.py
src/transformers/models/nystromformer/configuration_nystromformer.py
src/transformers/models/nystromformer/convert_nystromformer_original_pytorch_checkpoint_to_pytorch.py
src/transformers/models/nystromformer/modeling_nystromformer.py
src/transformers/utils/dummy_pt_objects.py
tests/test_modeling_nystromformer.py
==================
444ea95a8;Lysandre Debut;2022-01-11 14:15:59 +0100;Print out durations of all scheduled tests (#15102)

==

.github/workflows/self-scheduled.yml
==================
285131bfb;JejuWayfarer;2022-01-11 21:44:29 +0900;change metric_key_prefix in seq2seq_trainer.py (#15099)
It solves the problem that metric_key_prefix is different from trainer.
==

src/transformers/trainer_seq2seq.py
==================
c4fa908fa;Virus;2022-01-11 14:17:08 +0300;Adds IBERT to models exportable with ONNX (#14868)
* Add IBertOnnxConfig and tests

* add all the supported features for IBERT and remove outputs in IbertOnnxConfig

* use OnnxConfig

* fix codestyle

* remove serialization.rst

* codestyle
==

docs/source/serialization.mdx
src/transformers/models/ibert/__init__.py
src/transformers/models/ibert/configuration_ibert.py
src/transformers/onnx/features.py
tests/test_onnx_v2.py
==================
efb35a410;Patrick von Platen;2022-01-11 11:59:38 +0100;[Wav2Vec2ProcessorWithLM] improve decoder downlaod (#15040)

==

src/transformers/models/wav2vec2_with_lm/processing_wav2vec2_with_lm.py
tests/test_processor_wav2vec2_with_lm.py
==================
6ea626662;NielsRogge;2022-01-11 11:57:26 +0100;Fix cookiecutter (#15100)

==

templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/{{cookiecutter.lowercase_modelname}}.mdx
==================
68810aa26;Yih-Dar;2022-01-11 10:04:23 +0100;fix doc example - TypeError: forward() got an unexpected keyword argument 'input_ids' (#15092)
Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>
==

src/transformers/models/speech_to_text/modeling_speech_to_text.py
==================
ca76618d6;Sylvain Gugger;2022-01-11 03:16:39 -0500;Take gradient accumulation into account when defining samplers (#15095)
* Take gradient accumulation into account when defining samplers

* style
==

src/transformers/trainer.py
==================
9dc8fb2fc;Sylvain Gugger;2022-01-11 03:14:11 -0500;Add test to check reported training loss (#15096)
* Add test

* Add tests for the reported train loss
==

tests/test_trainer.py
==================
5cd7086fd;AK391;2022-01-11 00:11:31 -0500;XLM-ProphetNet Spaces badge

==

docs/source/model_summary.mdx
==================
4e3208662;AK391;2022-01-10 13:50:40 -0500;DPR Spaces badge

==

docs/source/model_summary.mdx
==================
ac2c06d49;AK391;2022-01-10 13:43:34 -0500;ProphetNet spaces badge

==

docs/source/model_summary.mdx
==================
bf0201e18;AK391;2022-01-10 13:37:17 -0500;MBART spaces badge

==

docs/source/model_summary.mdx
==================
b67fd797b;Yih-Dar;2022-01-10 19:30:14 +0100;Add TFVisionEncoderDecoderModel (#14148)
* Start the work on TFVisionEncoderDecoderModel

* Expose TFVisionEncoderDecoderModel

* fix import

* Add modeling_tf_vision_encoder_decoder to _ignore_modules in get_model_modules()

* reorder

* Apply the fix for checkpoint loading as in #14016

* remove attention_mask + fix VISION_DUMMY_INPUTS

* A minimal change to make TF generate() work for vision models as encoder in encoder-decoder setting

* fix wrong condition: shape_list(input_ids) == 2

* add tests

* use personal TFViTModel checkpoint (for now)

* Add equivalence tests + projection layer

* style

* make sure projection layer can run

* Add examples

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Clean comments (need to work on TODOs for PyTorch models)

* Remove TF -> PT in check_pt_tf_equivalence for TFVisionEncoderDecoderModel

* fixes

* Revert changes in PT code.

* Update tests/test_modeling_tf_vision_encoder_decoder.py

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* Add test_inference_coco_en for TF test

* fix quality

* fix name

* build doc

* add main_input_name

* Fix ckpt name in test

* fix diff between master and this PR

* fix doc

* fix style and quality

* fix missing doc

* fix labels handling

* Delete auto.rst

* Add the changes done in #14016

* fix prefix

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* make style

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
==

docs/source/index.mdx
docs/source/model_doc/auto.mdx
docs/source/model_doc/vision-encoder-decoder.mdx
src/transformers/__init__.py
src/transformers/generation_tf_utils.py
src/transformers/models/auto/__init__.py
src/transformers/models/auto/modeling_tf_auto.py
src/transformers/models/encoder_decoder/modeling_tf_encoder_decoder.py
src/transformers/models/vision_encoder_decoder/__init__.py
src/transformers/models/vision_encoder_decoder/modeling_tf_vision_encoder_decoder.py
src/transformers/utils/dummy_tf_objects.py
tests/test_modeling_tf_encoder_decoder.py
tests/test_modeling_tf_vision_encoder_decoder.py
utils/check_repo.py
==================
c9504b2f5;AK391;2022-01-10 12:57:08 -0500;MT5 Spaces badge

==

docs/source/model_summary.mdx
==================
daec528ca;AK391;2022-01-10 12:51:39 -0500;T5 Spaces badge

==

docs/source/model_summary.mdx
==================
0554e4d5c;AK391;2022-01-10 12:47:12 -0500;MarianMT Spaces badge

==

docs/source/model_summary.mdx
==================
7ec6aad23;AK391;2022-01-10 12:39:22 -0500;Pegasus Spaces badge

==

docs/source/model_summary.mdx
==================
03f8b9c9e;AK391;2022-01-10 12:33:59 -0500;BART Spaces badge

==

docs/source/model_summary.mdx
==================
37bc0b4e5;Stas Bekman;2022-01-10 09:21:04 -0800;[performance doc] Power and Cooling (#14935)
* [performance doc] Power and Cooling

* more docs

* Update docs/source/performance.mdx

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* reword

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

docs/source/performance.mdx
==================
20f169b52;AK391;2022-01-10 12:14:18 -0500;Longformer Spaces badge

==

docs/source/model_summary.mdx
==================
3e9fdcf01;Suraj Patil;2022-01-10 18:13:28 +0100;[DOC] fix doc examples for bart-like models (#15093)
* fix doc examples

* remove double colons
==

src/transformers/models/bart/modeling_bart.py
src/transformers/models/bart/modeling_flax_bart.py
src/transformers/models/bart/modeling_tf_bart.py
src/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py
src/transformers/models/blenderbot_small/modeling_flax_blenderbot_small.py
src/transformers/models/fsmt/modeling_fsmt.py
src/transformers/models/led/modeling_led.py
src/transformers/models/m2m_100/modeling_m2m_100.py
src/transformers/models/mbart/modeling_flax_mbart.py
src/transformers/models/mbart/modeling_mbart.py
src/transformers/models/mbart/modeling_tf_mbart.py
src/transformers/models/pegasus/modeling_flax_pegasus.py
src/transformers/models/pegasus/modeling_pegasus.py
src/transformers/models/pegasus/modeling_tf_pegasus.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_flax_{{cookiecutter.lowercase_modelname}}.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_{{cookiecutter.lowercase_modelname}}.py
==================
4fbc924d0;AK391;2022-01-10 12:06:05 -0500;Funnel Transformer spaces badge

==

docs/source/model_summary.mdx
==================
61d18ae03;Sylvain Gugger;2022-01-10 12:05:57 -0500;Happy New Year! (#15094)

==

templates/adding_a_new_example_script/{{cookiecutter.directory_name}}/run_{{cookiecutter.example_shortcut}}.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/configuration_{{cookiecutter.lowercase_modelname}}.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_flax_{{cookiecutter.lowercase_modelname}}.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_tf_{{cookiecutter.lowercase_modelname}}.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_{{cookiecutter.lowercase_modelname}}.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/test_modeling_flax_{{cookiecutter.lowercase_modelname}}.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/test_modeling_tf_{{cookiecutter.lowercase_modelname}}.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/test_modeling_{{cookiecutter.lowercase_modelname}}.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/to_replace_{{cookiecutter.lowercase_modelname}}.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/tokenization_fast_{{cookiecutter.lowercase_modelname}}.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/tokenization_{{cookiecutter.lowercase_modelname}}.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/{{cookiecutter.lowercase_modelname}}.mdx
==================
222c09a63;AK391;2022-01-10 11:53:23 -0500;ELECTRA Spaces badge

==

docs/source/model_summary.mdx
==================
31838d3e1;Stas Bekman;2022-01-10 08:44:33 -0800;[doc] normalize HF Transformers string (#15023)

==

docs/source/benchmarks.mdx
docs/source/testing.mdx
==================
84f360e86;AK391;2022-01-10 11:41:10 -0500;FlauBERT spaces badge

==

docs/source/model_summary.mdx
==================
9f3311689;AK391;2022-01-10 10:54:18 -0500;XLM-Roberta Spaces badge

==

docs/source/model_summary.mdx
==================
20fa9eb03;AK391;2022-01-10 10:48:06 -0500;XLM Spaces badge

==

docs/source/model_summary.mdx
==================
16b6df6fc;AK391;2022-01-10 10:33:03 -0500;ConvBERT spaces badge

==

docs/source/model_summary.mdx
==================
f21bc4215;Santiago Castro;2022-01-10 16:28:34 +0100;Use tqdm.auto in Pipeline docs (#14920)
It's better for e.g. notebook.
==

docs/source/main_classes/pipelines.mdx
==================
f012c00ad;Mishig Davaadorj;2022-01-10 08:06:14 -0700;Model summary horizontal banners (#15058)

==

docs/source/model_summary.mdx
==================
af9cb9497;Sylvain Gugger;2022-01-10 09:40:20 -0500;Fix style

==

src/transformers/models/trocr/processing_trocr.py
==================
533624c5a;Yih-Dar;2022-01-10 15:28:39 +0100;fix doc example - AttributeError: type object 'RagModel' has no attribute 'from_question_encoder_generator_pretrained' (#15076)
Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>
==

src/transformers/models/rag/modeling_rag.py
==================
b2c477fc6;Minghao Li;2022-01-10 22:28:03 +0800;support the trocr small models (#14893)
* support the trocr small models

* resolve conflict

* Update docs/source/model_doc/trocr.mdx

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>

* Update docs/source/model_doc/trocr.mdx

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>

* Update docs/source/model_doc/trocr.mdx

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>

* Update src/transformers/models/trocr/processing_trocr.py

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>

* Update src/transformers/models/trocr/processing_trocr.py

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>

* Update src/transformers/models/trocr/processing_trocr.py

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>

* Update src/transformers/models/trocr/processing_trocr.py

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>

* fix unexpected indent in processing_trocr.py

* Update src/transformers/models/trocr/processing_trocr.py

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>

* update the docstring of processing_trocr

* remove extra space

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>
==

docs/source/model_doc/trocr.mdx
src/transformers/models/trocr/processing_trocr.py
==================
42d57549b;Lysandre Debut;2022-01-10 15:22:48 +0100;Change assignee for tokenizers (#15088)

==

.github/ISSUE_TEMPLATE/bug-report.md
==================
a54961c5f;cody-moveworks;2022-01-10 04:53:20 -0800;Make OpenAIGPTTokenizer work with SpaCy 2.x and 3.x (#15019)
* Make OpenAIGPTTokenizer work with SpaCy 3.x

SpaCy 3.x introduced an API change to creating the tokenizer that
breaks OpenAIGPTTokenizer. The old API for creating the tokenizer in
SpaCy 2.x no longer works under SpaCy 3.x, but the new API for creating
the tokenizer in SpaCy 3.x DOES work under SpaCy 2.x. Switching to the
new API should allow OpenAIGPTTokenizer to work under both SpaCy 2.x and
SpaCy 3.x versions.

* Add is_spacy_available and is_ftfy_available methods to file utils

* Add spacy and ftfy unittest decorator to testing utils

* Add tests for OpenAIGPTTokenizer that require spacy and ftfy

* Modify CircleCI config to run tests that require spacy and ftfy

* Remove unneeded unittest decorators are reuse test code

* Run make fixup
==

.circleci/config.yml
src/transformers/file_utils.py
src/transformers/models/openai/tokenization_openai.py
src/transformers/testing_utils.py
tests/test_tokenization_openai.py
==================
9fbf7c87c;Kamal Raj;2022-01-10 17:25:43 +0530;Update check_repo.py (#15014)
added new line
==

utils/check_repo.py
==================
0a03a8681;Yih-Dar;2022-01-10 12:44:11 +0100;fix model table cell text alignment (#14999)
Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>
==

docs/source/index.mdx
utils/check_table.py
==================
d72343d2b;Patrick von Platen;2022-01-10 10:46:21 +0100;[Wav2Vec2 Speech Event] Add speech event v2 (#15083)
* up

* up

* up

* up

* up

* up

* improve

* up

* up

* Update src/transformers/trainer.py

* up

* up

* up
==

examples/pytorch/speech-recognition/README.md
examples/pytorch/speech-recognition/requirements.txt
examples/pytorch/speech-recognition/run_speech_recognition_ctc.py
examples/research_projects/wav2vec2/README.md
examples/research_projects/xls_r/README.md
==================
768e6c144;yoquankara;2022-01-09 04:33:55 +0900;Fix convert for newer megatron-lm bert model (#14082)
* Fix convert for newer megatron-lm models

* Save megatron-bert config in a proper way

* Fix code style
==

src/transformers/models/megatron_bert/convert_megatron_bert_checkpoint.py
==================
623b4f7c6;Yih-Dar;2022-01-07 20:02:49 +0100;[VisionTextDualEncoder] Add token_type_ids param (#15073)
* fix doc example - TypeError: get_text_features() got an unexpected keyword argument 'token_type_ids'

* add token_type_ids param

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>
==

src/transformers/models/vision_text_dual_encoder/modeling_vision_text_dual_encoder.py
==================
5be1242ac;AK391;2022-01-07 11:48:22 -0500;Merge branch 'huggingface:master' into master

==
==================
484e7a441;AK391;2022-01-07 11:47:56 -0500;Distilbert spaces badge

==

docs/source/model_summary.mdx
==================
ac224bb07;Yih-Dar;2022-01-07 16:55:59 +0100;[Fix doc examples] Add missing from_pretrained (#15044)
* fix doc example - ValueError: Parameter config should be an instance of class `PretrainedConfig`

* Update src/transformers/models/segformer/modeling_segformer.py

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>

* update

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>
Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>
==

src/transformers/models/segformer/modeling_segformer.py
==================
f18c6fa94;K.C. Tung;2022-01-07 01:34:12 -0600;Resubmit changes after rebase to master (#14982)

==

docs/source/serialization.mdx
==================
1d7122729;AK391;2022-01-06 18:50:19 -0500;Roberta spaces badge

==

docs/source/model_summary.mdx
==================
e36a83d3a;AK391;2022-01-06 18:44:59 -0500;Merge branch 'huggingface:master' into master

==
==================
cac877425;AK391;2022-01-06 13:01:23 -0500;ALBERT spaces badge

==

docs/source/model_summary.mdx
==================
794441c37;AK391;2022-01-06 12:22:09 -0500;BERT spaces badge

==

docs/source/model_summary.mdx
==================
f872f18dc;AK391;2022-01-06 12:09:50 -0500;XLNet spaces badge

==

docs/source/model_summary.mdx
==================
8d187e7fe;AK391;2022-01-06 11:59:21 -0500;Reformer Spaces badge

==

docs/source/model_summary.mdx
==================
cc406da4d;Yih-Dar;2022-01-06 17:59:06 +0100;[VisionTextDualEncoder] Fix doc example
Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>
==

src/transformers/models/vision_text_dual_encoder/modeling_vision_text_dual_encoder.py
==================
59fb63694;AK391;2022-01-06 11:47:41 -0500;Transformer-XL badge

==

docs/source/model_summary.mdx
==================
25b8b8a6f;AK391;2022-01-06 11:42:14 -0500;Merge branch 'huggingface:master' into master

==
==================
b67f345d0;flozi00;2022-01-06 17:26:45 +0100;Update run_speech_recognition_seq2seq.py (#14967)

==

examples/pytorch/speech-recognition/run_speech_recognition_seq2seq.py
==================
f71fb5c36;Tavin Turner;2022-01-06 08:39:13 -0700;Add 'with torch.no_grad()' to BertGeneration integration test forward passes (#14963)

==

tests/test_modeling_bert_generation.py
==================
d2183a46f;Nicolas Patry;2022-01-06 15:45:41 +0100;Remove old asserts. (#15012)

==

tests/test_tokenization_common.py
==================
83c552d39;NielsRogge;2022-01-06 14:53:58 +0100;Add detectron2 to Github actions (#15053)

==

.github/workflows/self-scheduled.yml
tests/test_modeling_layoutlmv2.py
==================
5ab87cd4d;Matt Churgin;2022-01-06 08:48:49 -0500;wrapped forward passes in torch.no_grad() (#15037)

==

tests/test_modeling_roberta.py
==================
5a06118b3;Nicolas Patry;2022-01-06 14:16:00 +0100;Enabling `TF` on `image-classification` pipeline. (#15030)

==

src/transformers/pipelines/image_classification.py
tests/test_pipelines_image_classification.py
==================
9f89fa02e;Yih-Dar;2022-01-06 14:00:54 +0100;Add Flax image captioning example (#14864)
* add image captioning example

* update README

* fix style & quality

* simplify

* apply review suggestions

* Apply suggestions from code review

Co-authored-by: Suraj Patil <surajp815@gmail.com>

* Apply suggestions from code review

Co-authored-by: Suraj Patil <surajp815@gmail.com>

* Apply review suggestions

* add comments about using np instead jax array

* remove unused lines

* add model creation script

* only support from_pretrained

* fix style

* fix

* not use cache_dir when creating model

* fix tokenizer creation

* update README

* fix quality

* apply suggestion

* simplify some blocks

* Update examples/flax/image-captioning/README.md


* Update examples/flax/image-captioning/run_image_captioning_flax.py

Co-authored-by: Suraj Patil <surajp815@gmail.com>

* apply suggestion

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>
Co-authored-by: Suraj Patil <surajp815@gmail.com>
==

examples/flax/image-captioning/README.md
examples/flax/image-captioning/create_model_from_encoder_decoder_models.py
examples/flax/image-captioning/run_image_captioning_flax.py
==================
2e9af2949;Suraj Patil;2022-01-05 16:58:42 +0100;[CLIP] Fix TF test (#15042)

==

tests/test_modeling_tf_clip.py
==================
443fdaf29;Patrick von Platen;2022-01-05 16:54:39 +0100;[SpeechEncoderDecoder] Fix from pretrained (#15043)

==

src/transformers/models/speech_encoder_decoder/modeling_speech_encoder_decoder.py
==================
ae929dcbb;Patrick von Platen;2022-01-05 14:21:04 +0100;[CLIP] Fix PT test (#15041)

==

tests/test_modeling_clip.py
==================
65cb94ff7;Nicolas Patry;2022-01-05 12:16:23 +0100;Adding QoL for `batch_size` arg (like others enabled everywhere). (#15027)
* Adding QoL for `batch_size` arg (like others enabled everywhere).

* Typo.
==

src/transformers/pipelines/base.py
tests/test_pipelines_common.py
==================
e34dd055e;Yih-Dar;2022-01-05 11:34:08 +0100;Fix doc example: mask_time_indices (numpy) has no attribute 'to' (#15033)
* fix doc example - AttributeError: 'numpy.ndarray' object has no attribute 'to'

* fix more

* Apply suggestions from code review

* Update src/transformers/models/unispeech/modeling_unispeech.py

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>
Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
==

src/transformers/models/unispeech/modeling_unispeech.py
src/transformers/models/unispeech_sat/modeling_unispeech_sat.py
src/transformers/models/wav2vec2/modeling_wav2vec2.py
==================
927f65442;Stas Bekman;2022-01-05 01:09:52 -0800;[megatron convert] PYTHONPATH requirements (#14956)
* [megatron convert] PYTHONPATH requirements

* more info
==

src/transformers/models/megatron_bert/convert_megatron_bert_checkpoint.py
src/transformers/models/megatron_gpt2/convert_megatron_gpt2_checkpoint.py
==================
238013672;AK391;2022-01-04 16:13:57 -0500;add spaces badges

==

docs/source/model_summary.mdx
==================
857ab55c0;Kevin Ko;2022-01-05 02:58:27 +0900;[doc] Update parallelism.mdx (#15018)
* Update parallelism.mdx

* Update parallelism.mdx
==

docs/source/parallelism.mdx
==================
19d37c2dd;Nicolas Patry;2022-01-04 14:07:44 +0100;Hotfix `chunk_length_s` instead of `_ms`. (#15029)
* Hotfix `chunk_length_s` instead of `_ms`.

* Adding fix of `pad_token` which should be last/previous token for CTC

proper decoding

* Fixing ChunkPipeline unwrapping.

* Adding a PackIterator specific test.
==

src/transformers/pipelines/automatic_speech_recognition.py
src/transformers/pipelines/pt_utils.py
tests/test_pipelines_automatic_speech_recognition.py
tests/test_pipelines_common.py
==================
21aecc097;Daniel Stancl;2022-01-04 13:23:10 +0100;Add Flax RoFormer (#15005)
* Add FlaxRoFormer

* Clean code + make quality

* Fix output pooling for FlaxRoFormerForMultipleChoiceModule

* Apply suggestions from code review

* add flax model to repos

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
==

docs/source/index.mdx
docs/source/model_doc/roformer.mdx
src/transformers/__init__.py
src/transformers/models/auto/modeling_flax_auto.py
src/transformers/models/roformer/__init__.py
src/transformers/models/roformer/modeling_flax_roformer.py
src/transformers/utils/dummy_flax_objects.py
tests/test_modeling_flax_roformer.py
==================
9e1775dd2;milyiyo;2022-01-04 06:59:47 -0500;Fix a little typo (#15002)

==

src/transformers/convert_graph_to_onnx.py
==================
774ed4a02;flozi00;2022-01-04 12:59:20 +0100;Fix Code block (#14983)

==

examples/pytorch/speech-pretraining/README.md
==================
f2ab21833;Kevin Ko;2022-01-04 04:49:27 +0900;Update parallelism.mdx (#15013)
* Update parallelism.mdx

* Update parallelism.mdx

* Update parallelism.mdx

* Update parallelism.mdx

* Update parallelism.mdx

* Update parallelism.mdx

* Update parallelism.mdx

* Update parallelism.mdx
==

docs/source/parallelism.mdx
==================
dbac8899f;Patrick von Platen;2022-01-03 20:19:04 +0100;[Tests] Correct Wav2Vec2 & WavLM tests (#15015)
* up

* up

* up
==

.github/workflows/self-scheduled.yml
tests/test_modeling_tf_wav2vec2.py
tests/test_modeling_wavlm.py
==================
0b4c3a1a5;Yih-Dar;2022-01-03 19:11:47 +0100;fix missing import (#15016)
Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>
==

src/transformers/models/wav2vec2/modeling_wav2vec2.py
==================
38f95d184;Anton Lozhkov;2022-01-03 18:54:17 +0300;Large audio chunking for the existing ASR pipeline (#14896)
* Naive ASR chunking

* Fixing batching for ASR.

Co-authored-by: Nicolas Patry <patry.nicolas@protonmail.com>
==

src/transformers/pipelines/automatic_speech_recognition.py
src/transformers/pipelines/pt_utils.py
tests/test_pipelines_automatic_speech_recognition.py
==================
d33dc7966;Nicolas Patry;2022-01-03 16:18:39 +0100;Improve truncation_side (#14947)
* Enabling `truncation_side` for Slow and Fast tokenizer.

Co-Authored-by: Niels Rogge <48327001+NielsRogge@users.noreply.github.com>

* Disable failing tests.

* Layout xlm.

* assert -> assertEqual.

Co-authored-by: Niels Rogge <48327001+NielsRogge@users.noreply.github.com>
==

src/transformers/tokenization_utils_base.py
src/transformers/tokenization_utils_fast.py
tests/test_tokenization_auto.py
tests/test_tokenization_common.py
tests/test_tokenization_layoutlmv2.py
tests/test_tokenization_layoutxlm.py
tests/test_tokenization_tapas.py
==================
8c2618e6a;Nicolas Patry;2022-01-03 14:49:58 +0100;Fixing t2t pipelines lists outputs. (#15008)
Backward compatibility broken in
https://github.com/huggingface/transformers/pull/14988
==

src/transformers/pipelines/text2text_generation.py
tests/test_pipelines_translation.py
==================
8f6373c61;Sylvain Gugger;2022-01-03 05:08:55 -0500;Map model_type and doc pages names (#14944)
* Map model_type and doc pages names

* Add script

* Fix typo

* Quality

* Manual check for Auto

Co-authored-by: Lysandre <lysandre.debut@reseau.eseo.fr>
==

README.md
README_ko.md
README_zh-hans.md
README_zh-hant.md
docs/source/_toctree.yml
docs/source/index.mdx
docs/source/model_doc/bert-generation.mdx
docs/source/model_doc/bert-japanese.mdx
docs/source/model_doc/big_bird.mdx
docs/source/model_doc/blenderbot-small.mdx
docs/source/model_doc/blenderbot.mdx
docs/source/model_doc/deberta-v2.mdx
docs/source/model_doc/encoder-decoder.mdx
docs/source/model_doc/imagegpt.mdx
docs/source/model_doc/megatron-bert.mdx
docs/source/model_doc/openai-gpt.mdx
docs/source/model_doc/sew-d.mdx
docs/source/model_doc/speech-encoder-decoder.mdx
docs/source/model_doc/speech_to_text_2.mdx
docs/source/model_doc/transfo-xl.mdx
docs/source/model_doc/trocr.mdx
docs/source/model_doc/unispeech-sat.mdx
docs/source/model_doc/vision-encoder-decoder.mdx
docs/source/model_doc/vision-text-dual-encoder.mdx
docs/source/model_doc/xlm-prophetnet.mdx
docs/source/model_doc/xlm-roberta.mdx
docs/source/model_summary.mdx
src/transformers/models/auto/configuration_auto.py
utils/check_repo.py
==================
e68c3756f;Sylvain Gugger;2021-12-30 17:03:20 -0500;Allow training to resume even if RNG states are not properly loaded (#14994)
* Allow training to resume even if RNG states are not properly loaded

* Proper f-string
==

src/transformers/trainer.py
==================
08cb5718e;Nicolas Patry;2021-12-30 17:30:58 +0100;Enabling `tokenizers` upgrade. (#14941)
* Enabling `tokenizers` upgrade.

* Moved ugly comment.

* Tokenizers==0.11.1 needs an update to keep borrow checker

happy in highly contiguous calls.

* Support both 0.11.1 and 0.11.0
==

setup.py
src/transformers/dependency_versions_table.py
src/transformers/tokenization_utils_fast.py
==================
f8a989cfb;Nicolas Patry;2021-12-30 16:17:15 +0100;Adding `num_return_sequences` support for text2text generation. (#14988)
* Adding `num_return_sequences` support for text2text generation.

Co-Authored-By: Enze <pu.miao@foxmail.com>

* Update tests/test_pipelines_text2text_generation.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update tests/test_pipelines_text2text_generation.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

Co-authored-by: Enze <pu.miao@foxmail.com>
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

src/transformers/pipelines/text2text_generation.py
tests/test_pipelines_text2text_generation.py
==================
c043ce6cf;Patrick von Platen;2021-12-30 10:16:03 +0100;[Generate] correct encoder_outputs are passed without attention_mask (#14980)
* [Generate] correct encoder_outputs are passed without attention_mask

* Apply suggestions from code review

* up
==

src/transformers/generation_utils.py
tests/test_generation_utils.py
tests/test_pipelines_automatic_speech_recognition.py
==================
a1392883c;Patrick von Platen;2021-12-30 09:56:43 +0100;[AutoProcessor] Correct AutoProcessor and automatically add processor‚Ä¶ (#14881)
* [AutoProcessor] Correct AutoProcessor and automatically add processor class

* up

* up

* up

* up

* up

* up

* up

* up

* continue tomorrow

* up

* up

* up

* make processor class private

* fix loop
==

src/transformers/feature_extraction_utils.py
src/transformers/models/auto/processing_auto.py
src/transformers/models/clip/processing_clip.py
src/transformers/models/layoutlmv2/processing_layoutlmv2.py
src/transformers/models/layoutxlm/processing_layoutxlm.py
src/transformers/models/speech_to_text/processing_speech_to_text.py
src/transformers/models/vision_text_dual_encoder/processing_vision_text_dual_encoder.py
src/transformers/models/wav2vec2/processing_wav2vec2.py
src/transformers/models/wav2vec2_with_lm/processing_wav2vec2_with_lm.py
src/transformers/tokenization_utils_base.py
tests/test_processor_auto.py
==================
d7d60df0e;Nicolas Patry;2021-12-30 09:10:34 +0100;Fixing a pathological case for slow tokenizers (#14981)
* Fixing a pathological case for slow tokenizers

* Update src/transformers/tokenization_utils.py
==

src/transformers/tokenization_utils.py
tests/test_tokenization_common.py
==================
d1ba56d8d;Stas Bekman;2021-12-29 14:18:03 -0800;remove absl workaround as it's no longer needed (#14909)
the absl workaround hasn't been needed since 2019-04 https://github.com/abseil/abseil-py/issues/99 so it should be safe to remove it.
==

src/transformers/__init__.py
==================
04cddaf40;Jake Tae;2021-12-30 00:09:54 +0900;refactor: replace `assert` with `ValueError` (#14970)

==

src/transformers/models/bert_generation/modeling_bert_generation.py
==================
600496fa5;Patrick von Platen;2021-12-28 20:33:23 +0100;[Wav2Vec2] Rename model's feature extractor to feature encoder (#14959)
* rename classes

* clean up more namings

* remove bogus file

* Apply suggestions from code review

* Apply suggestions from code review

* replace more names

* more regex replace

* make style

* correct

* correct more

* make style

* finish

* correct more in wav2vec2

* make style

* improve freeze_extractor

* add aliases

* add tf aliases
==

examples/pytorch/audio-classification/run_audio_classification.py
examples/pytorch/speech-recognition/README.md
examples/pytorch/speech-recognition/run_speech_recognition_ctc.py
examples/pytorch/speech-recognition/run_speech_recognition_seq2seq.py
src/transformers/models/hubert/configuration_hubert.py
src/transformers/models/hubert/modeling_hubert.py
src/transformers/models/hubert/modeling_tf_hubert.py
src/transformers/models/sew/configuration_sew.py
src/transformers/models/sew/modeling_sew.py
src/transformers/models/sew_d/configuration_sew_d.py
src/transformers/models/sew_d/modeling_sew_d.py
src/transformers/models/speech_encoder_decoder/modeling_speech_encoder_decoder.py
src/transformers/models/unispeech/configuration_unispeech.py
src/transformers/models/unispeech/modeling_unispeech.py
src/transformers/models/unispeech_sat/configuration_unispeech_sat.py
src/transformers/models/unispeech_sat/convert_unispeech_sat_original_pytorch_checkpoint_to_pytorch.py
src/transformers/models/unispeech_sat/modeling_unispeech_sat.py
src/transformers/models/wav2vec2/configuration_wav2vec2.py
src/transformers/models/wav2vec2/modeling_flax_wav2vec2.py
src/transformers/models/wav2vec2/modeling_tf_wav2vec2.py
src/transformers/models/wav2vec2/modeling_wav2vec2.py
src/transformers/models/wavlm/configuration_wavlm.py
src/transformers/models/wavlm/modeling_wavlm.py
tests/test_modeling_hubert.py
tests/test_modeling_sew.py
tests/test_modeling_sew_d.py
tests/test_modeling_tf_hubert.py
tests/test_modeling_tf_wav2vec2.py
tests/test_modeling_unispeech.py
tests/test_modeling_unispeech_sat.py
tests/test_modeling_wav2vec2.py
tests/test_modeling_wavlm.py
==================
1bfa34770;Patrick von Platen;2021-12-28 17:02:50 +0100;[Tests] Speed up tokenizer tests (#14964)
* speed up canine and mluke

* speed up mbart and mbart50 toks

* upload files
==

tests/test_tokenization_canine.py
tests/test_tokenization_layoutxlm.py
tests/test_tokenization_mbart.py
tests/test_tokenization_mbart50.py
tests/test_tokenization_mluke.py
tests/test_tokenization_xlm_roberta.py
==================
f80775df2;Patrick von Platen;2021-12-28 13:41:27 +0100;Update README.md (#14965)

==

examples/pytorch/speech-recognition/README.md
==================
1e847b40c;Patrick von Platen;2021-12-28 11:07:05 +0100;[WavLM] give model for precision (#14958)

==

tests/test_modeling_wavlm.py
==================
1c121916f;Patrick von Platen;2021-12-28 10:20:51 +0100;Add Speech Seq2Seq Training script (#14792)
* start

* add gradient checkpointing and feature extractor freezing

* Apply suggestions from code review

* up

* up

* up

* correct

* up

* more changes

* up

* up

* up

* remove rst
==

examples/pytorch/speech-recognition/README.md
examples/pytorch/speech-recognition/run_speech_recognition_ctc.py
examples/pytorch/speech-recognition/run_speech_recognition_seq2seq.py
examples/pytorch/test_examples.py
src/transformers/models/auto/processing_auto.py
src/transformers/models/hubert/modeling_hubert.py
src/transformers/models/sew/modeling_sew.py
src/transformers/models/sew_d/modeling_sew_d.py
src/transformers/models/speech_encoder_decoder/modeling_speech_encoder_decoder.py
src/transformers/models/unispeech/modeling_unispeech.py
src/transformers/models/wav2vec2/modeling_wav2vec2.py
src/transformers/models/wav2vec2/processing_wav2vec2.py
src/transformers/models/wavlm/modeling_wavlm.py
==================
10fd4fa1a;Stas Bekman;2021-12-27 17:17:38 -0800;[doc] :class: hunt (#14955)
* [doc] :class: hunt

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* fix the fix + style

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

src/transformers/feature_extraction_sequence_utils.py
src/transformers/models/auto/configuration_auto.py
src/transformers/models/mmbt/modeling_mmbt.py
src/transformers/models/speech_to_text_2/modeling_speech_to_text_2.py
src/transformers/models/trocr/modeling_trocr.py
tests/test_modeling_rag.py
tests/test_modeling_tf_rag.py
==================
2c5597f6c;Sylvain Gugger;2021-12-27 19:18:08 -0500;Style

==

docs/source/model_doc/auto.mdx
==================
b5e2b183a;Sylvain Gugger;2021-12-27 19:07:46 -0500;Doc styler examples (#14953)
* Fix bad examples

* Add black formatting to style_doc

* Use first nonempty line

* Put it at the right place

* Don't add spaces to empty lines

* Better templates

* Deal with triple quotes in docstrings

* Result of style_doc

* Enable mdx treatment and fix code examples in MDXs

* Result of doc styler on doc source files

* Last fixes

* Break copy from
==

docs/source/add_new_model.mdx
docs/source/add_new_pipeline.mdx
docs/source/benchmarks.mdx
docs/source/custom_datasets.mdx
docs/source/debugging.mdx
docs/source/glossary.mdx
docs/source/internal/generation_utils.mdx
docs/source/main_classes/callback.mdx
docs/source/main_classes/deepspeed.mdx
docs/source/main_classes/logging.mdx
docs/source/main_classes/output.mdx
docs/source/main_classes/pipelines.mdx
docs/source/main_classes/processors.mdx
docs/source/main_classes/trainer.mdx
docs/source/migration.mdx
docs/source/model_doc/bart.mdx
docs/source/model_doc/bartpho.mdx
docs/source/model_doc/bert_japanese.mdx
docs/source/model_doc/bertgeneration.mdx
docs/source/model_doc/bertweet.mdx
docs/source/model_doc/blenderbot.mdx
docs/source/model_doc/byt5.mdx
docs/source/model_doc/canine.mdx
docs/source/model_doc/clip.mdx
docs/source/model_doc/gpt_neo.mdx
docs/source/model_doc/gptj.mdx
docs/source/model_doc/herbert.mdx
docs/source/model_doc/layoutlm.mdx
docs/source/model_doc/layoutlmv2.mdx
docs/source/model_doc/layoutxlm.mdx
docs/source/model_doc/longformer.mdx
docs/source/model_doc/luke.mdx
docs/source/model_doc/m2m_100.mdx
docs/source/model_doc/marian.mdx
docs/source/model_doc/mbart.mdx
docs/source/model_doc/mluke.mdx
docs/source/model_doc/pegasus.mdx
docs/source/model_doc/qdqbert.mdx
docs/source/model_doc/reformer.mdx
docs/source/model_doc/speech_to_text.mdx
docs/source/model_doc/speech_to_text_2.mdx
docs/source/model_doc/t5.mdx
docs/source/model_doc/t5v1.1.mdx
docs/source/model_doc/tapas.mdx
docs/source/model_doc/visual_bert.mdx
docs/source/model_sharing.mdx
docs/source/multilingual.mdx
docs/source/perplexity.mdx
docs/source/preprocessing.mdx
docs/source/quicktour.mdx
docs/source/serialization.mdx
docs/source/task_summary.mdx
docs/source/testing.mdx
docs/source/tokenizer_summary.mdx
docs/source/training.mdx
src/transformers/configuration_utils.py
src/transformers/data/processors/squad.py
src/transformers/debug_utils.py
src/transformers/feature_extraction_utils.py
src/transformers/file_utils.py
src/transformers/generation_tf_utils.py
src/transformers/generation_utils.py
src/transformers/keras_callbacks.py
src/transformers/modelcard.py
src/transformers/modeling_flax_utils.py
src/transformers/modeling_tf_utils.py
src/transformers/modeling_utils.py
src/transformers/models/albert/configuration_albert.py
src/transformers/models/albert/modeling_albert.py
src/transformers/models/albert/modeling_flax_albert.py
src/transformers/models/albert/modeling_tf_albert.py
src/transformers/models/auto/auto_factory.py
src/transformers/models/auto/configuration_auto.py
src/transformers/models/auto/feature_extraction_auto.py
src/transformers/models/auto/processing_auto.py
src/transformers/models/auto/tokenization_auto.py
src/transformers/models/bart/modeling_bart.py
src/transformers/models/bart/modeling_flax_bart.py
src/transformers/models/beit/modeling_beit.py
src/transformers/models/beit/modeling_flax_beit.py
src/transformers/models/bert/modeling_bert.py
src/transformers/models/bert/modeling_flax_bert.py
src/transformers/models/bert/modeling_tf_bert.py
src/transformers/models/bert_generation/modeling_bert_generation.py
src/transformers/models/bertweet/tokenization_bertweet.py
src/transformers/models/big_bird/modeling_big_bird.py
src/transformers/models/big_bird/modeling_flax_big_bird.py
src/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py
src/transformers/models/blenderbot/modeling_blenderbot.py
src/transformers/models/blenderbot/modeling_flax_blenderbot.py
src/transformers/models/blenderbot_small/modeling_blenderbot_small.py
src/transformers/models/blenderbot_small/modeling_flax_blenderbot_small.py
src/transformers/models/clip/modeling_clip.py
src/transformers/models/clip/modeling_flax_clip.py
src/transformers/models/clip/modeling_tf_clip.py
src/transformers/models/convbert/configuration_convbert.py
src/transformers/models/deberta/modeling_deberta.py
src/transformers/models/deberta_v2/modeling_deberta_v2.py
src/transformers/models/deit/modeling_deit.py
src/transformers/models/detr/modeling_detr.py
src/transformers/models/distilbert/modeling_distilbert.py
src/transformers/models/dpr/modeling_dpr.py
src/transformers/models/dpr/modeling_tf_dpr.py
src/transformers/models/dpr/tokenization_dpr.py
src/transformers/models/dpr/tokenization_dpr_fast.py
src/transformers/models/electra/modeling_electra.py
src/transformers/models/electra/modeling_flax_electra.py
src/transformers/models/electra/modeling_tf_electra.py
src/transformers/models/encoder_decoder/configuration_encoder_decoder.py
src/transformers/models/encoder_decoder/modeling_encoder_decoder.py
src/transformers/models/encoder_decoder/modeling_flax_encoder_decoder.py
src/transformers/models/encoder_decoder/modeling_tf_encoder_decoder.py
src/transformers/models/fnet/modeling_fnet.py
src/transformers/models/fnet/tokenization_fnet.py
src/transformers/models/fsmt/configuration_fsmt.py
src/transformers/models/funnel/modeling_funnel.py
src/transformers/models/funnel/modeling_tf_funnel.py
src/transformers/models/gpt2/modeling_gpt2.py
src/transformers/models/gpt2/modeling_tf_gpt2.py
src/transformers/models/gptj/modeling_gptj.py
src/transformers/models/hubert/modeling_hubert.py
src/transformers/models/hubert/modeling_tf_hubert.py
src/transformers/models/imagegpt/modeling_imagegpt.py
src/transformers/models/layoutlm/modeling_layoutlm.py
src/transformers/models/layoutlm/modeling_tf_layoutlm.py
src/transformers/models/layoutlmv2/modeling_layoutlmv2.py
src/transformers/models/led/modeling_led.py
src/transformers/models/led/modeling_tf_led.py
src/transformers/models/longformer/modeling_longformer.py
src/transformers/models/longformer/modeling_tf_longformer.py
src/transformers/models/luke/modeling_luke.py
src/transformers/models/m2m_100/modeling_m2m_100.py
src/transformers/models/m2m_100/tokenization_m2m_100.py
src/transformers/models/marian/modeling_flax_marian.py
src/transformers/models/marian/modeling_marian.py
src/transformers/models/marian/modeling_tf_marian.py
src/transformers/models/marian/tokenization_marian.py
src/transformers/models/mbart/modeling_flax_mbart.py
src/transformers/models/mbart/modeling_mbart.py
src/transformers/models/mbart/tokenization_mbart.py
src/transformers/models/mbart/tokenization_mbart_fast.py
src/transformers/models/mbart50/tokenization_mbart50.py
src/transformers/models/mbart50/tokenization_mbart50_fast.py
src/transformers/models/megatron_bert/modeling_megatron_bert.py
src/transformers/models/mmbt/modeling_mmbt.py
src/transformers/models/mobilebert/modeling_mobilebert.py
src/transformers/models/mobilebert/modeling_tf_mobilebert.py
src/transformers/models/mt5/modeling_mt5.py
src/transformers/models/mt5/modeling_tf_mt5.py
src/transformers/models/openai/modeling_openai.py
src/transformers/models/openai/modeling_tf_openai.py
src/transformers/models/pegasus/modeling_flax_pegasus.py
src/transformers/models/pegasus/modeling_pegasus.py
src/transformers/models/perceiver/modeling_perceiver.py
src/transformers/models/prophetnet/modeling_prophetnet.py
src/transformers/models/qdqbert/modeling_qdqbert.py
src/transformers/models/rag/modeling_rag.py
src/transformers/models/rag/modeling_tf_rag.py
src/transformers/models/rag/retrieval_rag.py
src/transformers/models/rembert/modeling_rembert.py
src/transformers/models/roberta/modeling_roberta.py
src/transformers/models/roformer/modeling_roformer.py
src/transformers/models/roformer/tokenization_roformer.py
src/transformers/models/roformer/tokenization_roformer_fast.py
src/transformers/models/segformer/modeling_segformer.py
src/transformers/models/sew_d/modeling_sew_d.py
src/transformers/models/speech_encoder_decoder/configuration_speech_encoder_decoder.py
src/transformers/models/speech_encoder_decoder/modeling_speech_encoder_decoder.py
src/transformers/models/speech_to_text/modeling_speech_to_text.py
src/transformers/models/speech_to_text_2/modeling_speech_to_text_2.py
src/transformers/models/squeezebert/modeling_squeezebert.py
src/transformers/models/t5/modeling_flax_t5.py
src/transformers/models/t5/modeling_t5.py
src/transformers/models/t5/modeling_tf_t5.py
src/transformers/models/tapas/configuration_tapas.py
src/transformers/models/tapas/modeling_tapas.py
src/transformers/models/tapas/modeling_tf_tapas.py
src/transformers/models/trocr/modeling_trocr.py
src/transformers/models/unispeech/modeling_unispeech.py
src/transformers/models/unispeech_sat/modeling_unispeech_sat.py
src/transformers/models/vision_encoder_decoder/configuration_vision_encoder_decoder.py
src/transformers/models/vision_encoder_decoder/modeling_flax_vision_encoder_decoder.py
src/transformers/models/vision_encoder_decoder/modeling_vision_encoder_decoder.py
src/transformers/models/vision_text_dual_encoder/configuration_vision_text_dual_encoder.py
src/transformers/models/vision_text_dual_encoder/modeling_flax_vision_text_dual_encoder.py
src/transformers/models/vision_text_dual_encoder/modeling_vision_text_dual_encoder.py
src/transformers/models/visual_bert/configuration_visual_bert.py
src/transformers/models/visual_bert/modeling_visual_bert.py
src/transformers/models/vit/modeling_flax_vit.py
src/transformers/models/vit/modeling_tf_vit.py
src/transformers/models/vit/modeling_vit.py
src/transformers/models/wav2vec2/modeling_flax_wav2vec2.py
src/transformers/models/wav2vec2/modeling_tf_wav2vec2.py
src/transformers/models/wav2vec2/modeling_wav2vec2.py
src/transformers/models/wav2vec2/tokenization_wav2vec2.py
src/transformers/models/wav2vec2_phoneme/tokenization_wav2vec2_phoneme.py
src/transformers/models/xlm/modeling_xlm.py
src/transformers/models/xlm_prophetnet/modeling_xlm_prophetnet.py
src/transformers/models/xlnet/modeling_tf_xlnet.py
src/transformers/models/xlnet/modeling_xlnet.py
src/transformers/optimization.py
src/transformers/pipelines/__init__.py
src/transformers/pipelines/table_question_answering.py
src/transformers/testing_utils.py
src/transformers/tokenization_utils.py
src/transformers/tokenization_utils_base.py
src/transformers/trainer_callback.py
src/transformers/trainer_pt_utils.py
src/transformers/trainer_utils.py
src/transformers/utils/fx.py
utils/style_doc.py
==================
e13f72fbf;Stas Bekman;2021-12-27 15:49:48 -0800;[doc] :obj: hunt (#14954)
* redo sans examples

* style
==

docs/source/testing.mdx
src/transformers/generation_utils.py
src/transformers/modeling_utils.py
src/transformers/models/encoder_decoder/modeling_encoder_decoder.py
src/transformers/models/encoder_decoder/modeling_flax_encoder_decoder.py
src/transformers/models/encoder_decoder/modeling_tf_encoder_decoder.py
src/transformers/models/ibert/quant_modules.py
src/transformers/models/layoutlm/modeling_layoutlm.py
src/transformers/models/layoutlm/modeling_tf_layoutlm.py
src/transformers/models/layoutlmv2/modeling_layoutlmv2.py
src/transformers/models/lxmert/modeling_tf_lxmert.py
src/transformers/models/rag/modeling_rag.py
src/transformers/models/rag/modeling_tf_rag.py
src/transformers/models/speech_encoder_decoder/modeling_speech_encoder_decoder.py
src/transformers/models/t5/modeling_t5.py
src/transformers/models/tapas/modeling_tapas.py
src/transformers/models/tapas/modeling_tf_tapas.py
src/transformers/models/unispeech/configuration_unispeech.py
src/transformers/models/unispeech_sat/configuration_unispeech_sat.py
src/transformers/models/vision_encoder_decoder/modeling_flax_vision_encoder_decoder.py
src/transformers/models/vision_encoder_decoder/modeling_vision_encoder_decoder.py
src/transformers/models/vision_text_dual_encoder/modeling_flax_vision_text_dual_encoder.py
src/transformers/models/vision_text_dual_encoder/modeling_vision_text_dual_encoder.py
src/transformers/models/wav2vec2/configuration_wav2vec2.py
src/transformers/models/wavlm/configuration_wavlm.py
src/transformers/models/xlnet/modeling_tf_xlnet.py
src/transformers/models/xlnet/modeling_xlnet.py
src/transformers/trainer.py
src/transformers/trainer_pt_utils.py
src/transformers/trainer_seq2seq.py
src/transformers/training_args.py
tests/test_doc_samples.py
tests/test_modeling_xlnet.py
==================
133c5e40c;Stas Bekman;2021-12-27 14:31:40 -0800;[doc] consistent True/False/None default format (#14951)
* [doc] consistent True/False/None default format

* Update src/transformers/models/xlnet/modeling_xlnet.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

docs/source/model_doc/segformer.mdx
src/transformers/generation_tf_utils.py
src/transformers/generation_utils.py
src/transformers/integrations.py
src/transformers/modeling_utils.py
src/transformers/models/bert_japanese/tokenization_bert_japanese.py
src/transformers/models/bertweet/tokenization_bertweet.py
src/transformers/models/detr/modeling_detr.py
src/transformers/models/encoder_decoder/modeling_encoder_decoder.py
src/transformers/models/encoder_decoder/modeling_flax_encoder_decoder.py
src/transformers/models/encoder_decoder/modeling_tf_encoder_decoder.py
src/transformers/models/layoutlmv2/tokenization_layoutlmv2.py
src/transformers/models/luke/tokenization_luke.py
src/transformers/models/lxmert/modeling_tf_lxmert.py
src/transformers/models/mluke/tokenization_mluke.py
src/transformers/models/perceiver/modeling_perceiver.py
src/transformers/models/rag/modeling_rag.py
src/transformers/models/rag/modeling_tf_rag.py
src/transformers/models/speech_encoder_decoder/modeling_speech_encoder_decoder.py
src/transformers/models/vision_encoder_decoder/modeling_flax_vision_encoder_decoder.py
src/transformers/models/vision_encoder_decoder/modeling_vision_encoder_decoder.py
src/transformers/models/vision_text_dual_encoder/modeling_flax_vision_text_dual_encoder.py
src/transformers/models/vision_text_dual_encoder/modeling_vision_text_dual_encoder.py
src/transformers/models/xlnet/modeling_xlnet.py
src/transformers/optimization.py
src/transformers/optimization_tf.py
src/transformers/pipelines/audio_classification.py
src/transformers/tokenization_utils_base.py
src/transformers/trainer.py
src/transformers/training_args.py
==================
b2f500256;Sylvain Gugger;2021-12-27 17:09:37 -0500;Convert last rst file (#14952)

==

docs/source/model_doc/auto.mdx
docs/source/model_doc/auto.rst
==================
87e6e4fe5;Sylvain Gugger;2021-12-27 16:31:21 -0500;Doc styler v2 (#14950)
* New doc styler

* Fix issue with args at the start

* Code sample fixes

* Style code examples in MDX

* Fix more patterns

* Typo

* Typo

* More patterns

* Do without black for now

* Get more info in error

* Docstring style

* Re-enable check

* Quality

* Fix add_end_docstring decorator

* Fix docstring
==

.circleci/config.yml
Makefile
src/transformers/commands/lfs.py
src/transformers/commands/serving.py
src/transformers/configuration_utils.py
src/transformers/convert_graph_to_onnx.py
src/transformers/convert_pytorch_checkpoint_to_tf2.py
src/transformers/convert_slow_tokenizers_checkpoints_to_fast.py
src/transformers/data/data_collator.py
src/transformers/data/processors/glue.py
src/transformers/data/processors/squad.py
src/transformers/data/processors/utils.py
src/transformers/data/processors/xnli.py
src/transformers/debug_utils.py
src/transformers/deepspeed.py
src/transformers/feature_extraction_sequence_utils.py
src/transformers/feature_extraction_utils.py
src/transformers/file_utils.py
src/transformers/generation_beam_search.py
src/transformers/generation_flax_logits_process.py
src/transformers/generation_flax_utils.py
src/transformers/generation_logits_process.py
src/transformers/generation_stopping_criteria.py
src/transformers/generation_tf_utils.py
src/transformers/generation_utils.py
src/transformers/image_utils.py
src/transformers/integrations.py
src/transformers/keras_callbacks.py
src/transformers/modelcard.py
src/transformers/modeling_flax_outputs.py
src/transformers/modeling_flax_utils.py
src/transformers/modeling_outputs.py
src/transformers/modeling_tf_outputs.py
src/transformers/modeling_tf_utils.py
src/transformers/modeling_utils.py
src/transformers/models/albert/configuration_albert.py
src/transformers/models/albert/modeling_albert.py
src/transformers/models/albert/modeling_flax_albert.py
src/transformers/models/albert/modeling_tf_albert.py
src/transformers/models/albert/tokenization_albert.py
src/transformers/models/albert/tokenization_albert_fast.py
src/transformers/models/auto/auto_factory.py
src/transformers/models/auto/configuration_auto.py
src/transformers/models/auto/dynamic.py
src/transformers/models/auto/feature_extraction_auto.py
src/transformers/models/auto/modeling_auto.py
src/transformers/models/auto/modeling_flax_auto.py
src/transformers/models/auto/modeling_tf_auto.py
src/transformers/models/auto/processing_auto.py
src/transformers/models/auto/tokenization_auto.py
src/transformers/models/bart/configuration_bart.py
src/transformers/models/bart/modeling_bart.py
src/transformers/models/bart/modeling_flax_bart.py
src/transformers/models/bart/modeling_tf_bart.py
src/transformers/models/bart/tokenization_bart.py
src/transformers/models/bart/tokenization_bart_fast.py
src/transformers/models/barthez/tokenization_barthez.py
src/transformers/models/barthez/tokenization_barthez_fast.py
src/transformers/models/bartpho/tokenization_bartpho.py
src/transformers/models/beit/configuration_beit.py
src/transformers/models/beit/feature_extraction_beit.py
src/transformers/models/beit/modeling_beit.py
src/transformers/models/beit/modeling_flax_beit.py
src/transformers/models/bert/configuration_bert.py
src/transformers/models/bert/modeling_bert.py
src/transformers/models/bert/modeling_flax_bert.py
src/transformers/models/bert/modeling_tf_bert.py
src/transformers/models/bert/tokenization_bert.py
src/transformers/models/bert/tokenization_bert_fast.py
src/transformers/models/bert_generation/configuration_bert_generation.py
src/transformers/models/bert_generation/modeling_bert_generation.py
src/transformers/models/bert_generation/tokenization_bert_generation.py
src/transformers/models/bertweet/tokenization_bertweet.py
src/transformers/models/big_bird/configuration_big_bird.py
src/transformers/models/big_bird/modeling_big_bird.py
src/transformers/models/big_bird/modeling_flax_big_bird.py
src/transformers/models/big_bird/tokenization_big_bird.py
src/transformers/models/big_bird/tokenization_big_bird_fast.py
src/transformers/models/bigbird_pegasus/configuration_bigbird_pegasus.py
src/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py
src/transformers/models/blenderbot/configuration_blenderbot.py
src/transformers/models/blenderbot/modeling_blenderbot.py
src/transformers/models/blenderbot/modeling_flax_blenderbot.py
src/transformers/models/blenderbot/modeling_tf_blenderbot.py
src/transformers/models/blenderbot/tokenization_blenderbot.py
src/transformers/models/blenderbot/tokenization_blenderbot_fast.py
src/transformers/models/blenderbot_small/configuration_blenderbot_small.py
src/transformers/models/blenderbot_small/modeling_blenderbot_small.py
src/transformers/models/blenderbot_small/modeling_flax_blenderbot_small.py
src/transformers/models/blenderbot_small/modeling_tf_blenderbot_small.py
src/transformers/models/blenderbot_small/tokenization_blenderbot_small.py
src/transformers/models/byt5/tokenization_byt5.py
src/transformers/models/camembert/configuration_camembert.py
src/transformers/models/camembert/modeling_camembert.py
src/transformers/models/camembert/modeling_tf_camembert.py
src/transformers/models/camembert/tokenization_camembert.py
src/transformers/models/camembert/tokenization_camembert_fast.py
src/transformers/models/canine/configuration_canine.py
src/transformers/models/canine/modeling_canine.py
src/transformers/models/canine/tokenization_canine.py
src/transformers/models/clip/configuration_clip.py
src/transformers/models/clip/feature_extraction_clip.py
src/transformers/models/clip/modeling_clip.py
src/transformers/models/clip/modeling_flax_clip.py
src/transformers/models/clip/modeling_tf_clip.py
src/transformers/models/clip/processing_clip.py
src/transformers/models/clip/tokenization_clip.py
src/transformers/models/clip/tokenization_clip_fast.py
src/transformers/models/convbert/configuration_convbert.py
src/transformers/models/convbert/modeling_convbert.py
src/transformers/models/convbert/modeling_tf_convbert.py
src/transformers/models/convbert/tokenization_convbert.py
src/transformers/models/convbert/tokenization_convbert_fast.py
src/transformers/models/cpm/tokenization_cpm.py
src/transformers/models/cpm/tokenization_cpm_fast.py
src/transformers/models/ctrl/configuration_ctrl.py
src/transformers/models/ctrl/modeling_ctrl.py
src/transformers/models/ctrl/modeling_tf_ctrl.py
src/transformers/models/ctrl/tokenization_ctrl.py
src/transformers/models/deberta/configuration_deberta.py
src/transformers/models/deberta/modeling_deberta.py
src/transformers/models/deberta/modeling_tf_deberta.py
src/transformers/models/deberta/tokenization_deberta.py
src/transformers/models/deberta/tokenization_deberta_fast.py
src/transformers/models/deberta_v2/configuration_deberta_v2.py
src/transformers/models/deberta_v2/modeling_deberta_v2.py
src/transformers/models/deberta_v2/modeling_tf_deberta_v2.py
src/transformers/models/deberta_v2/tokenization_deberta_v2.py
src/transformers/models/deit/configuration_deit.py
src/transformers/models/deit/feature_extraction_deit.py
src/transformers/models/deit/modeling_deit.py
src/transformers/models/detr/configuration_detr.py
src/transformers/models/detr/feature_extraction_detr.py
src/transformers/models/detr/modeling_detr.py
src/transformers/models/distilbert/configuration_distilbert.py
src/transformers/models/distilbert/modeling_distilbert.py
src/transformers/models/distilbert/modeling_flax_distilbert.py
src/transformers/models/distilbert/modeling_tf_distilbert.py
src/transformers/models/distilbert/tokenization_distilbert.py
src/transformers/models/distilbert/tokenization_distilbert_fast.py
src/transformers/models/dpr/configuration_dpr.py
src/transformers/models/dpr/modeling_dpr.py
src/transformers/models/dpr/modeling_tf_dpr.py
src/transformers/models/dpr/tokenization_dpr.py
src/transformers/models/dpr/tokenization_dpr_fast.py
src/transformers/models/electra/configuration_electra.py
src/transformers/models/electra/modeling_electra.py
src/transformers/models/electra/modeling_flax_electra.py
src/transformers/models/electra/modeling_tf_electra.py
src/transformers/models/electra/tokenization_electra.py
src/transformers/models/electra/tokenization_electra_fast.py
src/transformers/models/encoder_decoder/configuration_encoder_decoder.py
src/transformers/models/encoder_decoder/modeling_encoder_decoder.py
src/transformers/models/encoder_decoder/modeling_flax_encoder_decoder.py
src/transformers/models/encoder_decoder/modeling_tf_encoder_decoder.py
src/transformers/models/flaubert/configuration_flaubert.py
src/transformers/models/flaubert/modeling_flaubert.py
src/transformers/models/flaubert/modeling_tf_flaubert.py
src/transformers/models/flaubert/tokenization_flaubert.py
src/transformers/models/fnet/configuration_fnet.py
src/transformers/models/fnet/modeling_fnet.py
src/transformers/models/fnet/tokenization_fnet.py
src/transformers/models/fnet/tokenization_fnet_fast.py
src/transformers/models/fsmt/configuration_fsmt.py
src/transformers/models/fsmt/modeling_fsmt.py
src/transformers/models/fsmt/tokenization_fsmt.py
src/transformers/models/funnel/configuration_funnel.py
src/transformers/models/funnel/modeling_funnel.py
src/transformers/models/funnel/modeling_tf_funnel.py
src/transformers/models/funnel/tokenization_funnel.py
src/transformers/models/funnel/tokenization_funnel_fast.py
src/transformers/models/gpt2/configuration_gpt2.py
src/transformers/models/gpt2/modeling_flax_gpt2.py
src/transformers/models/gpt2/modeling_gpt2.py
src/transformers/models/gpt2/modeling_tf_gpt2.py
src/transformers/models/gpt2/tokenization_gpt2.py
src/transformers/models/gpt2/tokenization_gpt2_fast.py
src/transformers/models/gpt_neo/configuration_gpt_neo.py
src/transformers/models/gpt_neo/modeling_flax_gpt_neo.py
src/transformers/models/gpt_neo/modeling_gpt_neo.py
src/transformers/models/gptj/configuration_gptj.py
src/transformers/models/gptj/modeling_flax_gptj.py
src/transformers/models/gptj/modeling_gptj.py
src/transformers/models/herbert/tokenization_herbert.py
src/transformers/models/herbert/tokenization_herbert_fast.py
src/transformers/models/hubert/configuration_hubert.py
src/transformers/models/hubert/modeling_hubert.py
src/transformers/models/hubert/modeling_tf_hubert.py
src/transformers/models/ibert/configuration_ibert.py
src/transformers/models/ibert/modeling_ibert.py
src/transformers/models/ibert/quant_modules.py
src/transformers/models/imagegpt/configuration_imagegpt.py
src/transformers/models/imagegpt/feature_extraction_imagegpt.py
src/transformers/models/imagegpt/modeling_imagegpt.py
src/transformers/models/layoutlm/configuration_layoutlm.py
src/transformers/models/layoutlm/modeling_layoutlm.py
src/transformers/models/layoutlm/modeling_tf_layoutlm.py
src/transformers/models/layoutlm/tokenization_layoutlm.py
src/transformers/models/layoutlm/tokenization_layoutlm_fast.py
src/transformers/models/layoutlmv2/configuration_layoutlmv2.py
src/transformers/models/layoutlmv2/feature_extraction_layoutlmv2.py
src/transformers/models/layoutlmv2/modeling_layoutlmv2.py
src/transformers/models/layoutlmv2/processing_layoutlmv2.py
src/transformers/models/layoutlmv2/tokenization_layoutlmv2.py
src/transformers/models/layoutlmv2/tokenization_layoutlmv2_fast.py
src/transformers/models/layoutxlm/processing_layoutxlm.py
src/transformers/models/layoutxlm/tokenization_layoutxlm.py
src/transformers/models/layoutxlm/tokenization_layoutxlm_fast.py
src/transformers/models/led/configuration_led.py
src/transformers/models/led/modeling_led.py
src/transformers/models/led/modeling_tf_led.py
src/transformers/models/led/tokenization_led.py
src/transformers/models/led/tokenization_led_fast.py
src/transformers/models/longformer/configuration_longformer.py
src/transformers/models/longformer/modeling_longformer.py
src/transformers/models/longformer/modeling_tf_longformer.py
src/transformers/models/longformer/tokenization_longformer.py
src/transformers/models/longformer/tokenization_longformer_fast.py
src/transformers/models/luke/configuration_luke.py
src/transformers/models/luke/modeling_luke.py
src/transformers/models/luke/tokenization_luke.py
src/transformers/models/lxmert/configuration_lxmert.py
src/transformers/models/lxmert/modeling_lxmert.py
src/transformers/models/lxmert/modeling_tf_lxmert.py
src/transformers/models/lxmert/tokenization_lxmert.py
src/transformers/models/lxmert/tokenization_lxmert_fast.py
src/transformers/models/m2m_100/configuration_m2m_100.py
src/transformers/models/m2m_100/modeling_m2m_100.py
src/transformers/models/m2m_100/tokenization_m2m_100.py
src/transformers/models/marian/configuration_marian.py
src/transformers/models/marian/modeling_flax_marian.py
src/transformers/models/marian/modeling_marian.py
src/transformers/models/marian/modeling_tf_marian.py
src/transformers/models/marian/tokenization_marian.py
src/transformers/models/mbart/configuration_mbart.py
src/transformers/models/mbart/modeling_flax_mbart.py
src/transformers/models/mbart/modeling_mbart.py
src/transformers/models/mbart/modeling_tf_mbart.py
src/transformers/models/mbart/tokenization_mbart.py
src/transformers/models/mbart/tokenization_mbart_fast.py
src/transformers/models/mbart50/tokenization_mbart50.py
src/transformers/models/mbart50/tokenization_mbart50_fast.py
src/transformers/models/megatron_bert/configuration_megatron_bert.py
src/transformers/models/megatron_bert/modeling_megatron_bert.py
src/transformers/models/mluke/tokenization_mluke.py
src/transformers/models/mmbt/configuration_mmbt.py
src/transformers/models/mmbt/modeling_mmbt.py
src/transformers/models/mobilebert/configuration_mobilebert.py
src/transformers/models/mobilebert/modeling_mobilebert.py
src/transformers/models/mobilebert/modeling_tf_mobilebert.py
src/transformers/models/mobilebert/tokenization_mobilebert.py
src/transformers/models/mobilebert/tokenization_mobilebert_fast.py
src/transformers/models/mpnet/configuration_mpnet.py
src/transformers/models/mpnet/modeling_mpnet.py
src/transformers/models/mpnet/modeling_tf_mpnet.py
src/transformers/models/mpnet/tokenization_mpnet.py
src/transformers/models/mpnet/tokenization_mpnet_fast.py
src/transformers/models/mt5/configuration_mt5.py
src/transformers/models/mt5/modeling_flax_mt5.py
src/transformers/models/mt5/modeling_mt5.py
src/transformers/models/mt5/modeling_tf_mt5.py
src/transformers/models/openai/configuration_openai.py
src/transformers/models/openai/modeling_openai.py
src/transformers/models/openai/modeling_tf_openai.py
src/transformers/models/openai/tokenization_openai.py
src/transformers/models/openai/tokenization_openai_fast.py
src/transformers/models/pegasus/configuration_pegasus.py
src/transformers/models/pegasus/modeling_flax_pegasus.py
src/transformers/models/pegasus/modeling_pegasus.py
src/transformers/models/pegasus/modeling_tf_pegasus.py
src/transformers/models/pegasus/tokenization_pegasus.py
src/transformers/models/pegasus/tokenization_pegasus_fast.py
src/transformers/models/perceiver/configuration_perceiver.py
src/transformers/models/perceiver/feature_extraction_perceiver.py
src/transformers/models/perceiver/modeling_perceiver.py
src/transformers/models/perceiver/tokenization_perceiver.py
src/transformers/models/phobert/tokenization_phobert.py
src/transformers/models/prophetnet/configuration_prophetnet.py
src/transformers/models/prophetnet/modeling_prophetnet.py
src/transformers/models/prophetnet/tokenization_prophetnet.py
src/transformers/models/qdqbert/configuration_qdqbert.py
src/transformers/models/qdqbert/modeling_qdqbert.py
src/transformers/models/rag/configuration_rag.py
src/transformers/models/rag/modeling_rag.py
src/transformers/models/rag/modeling_tf_rag.py
src/transformers/models/rag/retrieval_rag.py
src/transformers/models/reformer/configuration_reformer.py
src/transformers/models/reformer/modeling_reformer.py
src/transformers/models/reformer/tokenization_reformer.py
src/transformers/models/reformer/tokenization_reformer_fast.py
src/transformers/models/rembert/configuration_rembert.py
src/transformers/models/rembert/modeling_rembert.py
src/transformers/models/rembert/modeling_tf_rembert.py
src/transformers/models/rembert/tokenization_rembert.py
src/transformers/models/rembert/tokenization_rembert_fast.py
src/transformers/models/retribert/configuration_retribert.py
src/transformers/models/retribert/modeling_retribert.py
src/transformers/models/retribert/tokenization_retribert.py
src/transformers/models/retribert/tokenization_retribert_fast.py
src/transformers/models/roberta/configuration_roberta.py
src/transformers/models/roberta/modeling_flax_roberta.py
src/transformers/models/roberta/modeling_roberta.py
src/transformers/models/roberta/modeling_tf_roberta.py
src/transformers/models/roberta/tokenization_roberta.py
src/transformers/models/roberta/tokenization_roberta_fast.py
src/transformers/models/roformer/configuration_roformer.py
src/transformers/models/roformer/modeling_roformer.py
src/transformers/models/roformer/modeling_tf_roformer.py
src/transformers/models/roformer/tokenization_roformer.py
src/transformers/models/roformer/tokenization_roformer_fast.py
src/transformers/models/segformer/configuration_segformer.py
src/transformers/models/segformer/feature_extraction_segformer.py
src/transformers/models/segformer/modeling_segformer.py
src/transformers/models/sew/configuration_sew.py
src/transformers/models/sew/modeling_sew.py
src/transformers/models/sew_d/configuration_sew_d.py
src/transformers/models/sew_d/modeling_sew_d.py
src/transformers/models/speech_encoder_decoder/configuration_speech_encoder_decoder.py
src/transformers/models/speech_encoder_decoder/modeling_speech_encoder_decoder.py
src/transformers/models/speech_to_text/configuration_speech_to_text.py
src/transformers/models/speech_to_text/feature_extraction_speech_to_text.py
src/transformers/models/speech_to_text/modeling_speech_to_text.py
src/transformers/models/speech_to_text/processing_speech_to_text.py
src/transformers/models/speech_to_text/tokenization_speech_to_text.py
src/transformers/models/speech_to_text_2/configuration_speech_to_text_2.py
src/transformers/models/speech_to_text_2/modeling_speech_to_text_2.py
src/transformers/models/speech_to_text_2/processing_speech_to_text_2.py
src/transformers/models/speech_to_text_2/tokenization_speech_to_text_2.py
src/transformers/models/splinter/configuration_splinter.py
src/transformers/models/splinter/modeling_splinter.py
src/transformers/models/splinter/tokenization_splinter.py
src/transformers/models/splinter/tokenization_splinter_fast.py
src/transformers/models/squeezebert/configuration_squeezebert.py
src/transformers/models/squeezebert/modeling_squeezebert.py
src/transformers/models/squeezebert/tokenization_squeezebert.py
src/transformers/models/squeezebert/tokenization_squeezebert_fast.py
src/transformers/models/t5/configuration_t5.py
src/transformers/models/t5/modeling_flax_t5.py
src/transformers/models/t5/modeling_t5.py
src/transformers/models/t5/modeling_tf_t5.py
src/transformers/models/t5/tokenization_t5.py
src/transformers/models/t5/tokenization_t5_fast.py
src/transformers/models/tapas/configuration_tapas.py
src/transformers/models/tapas/modeling_tapas.py
src/transformers/models/tapas/modeling_tf_tapas.py
src/transformers/models/tapas/tokenization_tapas.py
src/transformers/models/transfo_xl/configuration_transfo_xl.py
src/transformers/models/transfo_xl/modeling_tf_transfo_xl.py
src/transformers/models/transfo_xl/modeling_transfo_xl.py
src/transformers/models/transfo_xl/modeling_transfo_xl_utilities.py
src/transformers/models/transfo_xl/tokenization_transfo_xl.py
src/transformers/models/trocr/configuration_trocr.py
src/transformers/models/trocr/modeling_trocr.py
src/transformers/models/trocr/processing_trocr.py
src/transformers/models/unispeech/configuration_unispeech.py
src/transformers/models/unispeech/modeling_unispeech.py
src/transformers/models/unispeech_sat/configuration_unispeech_sat.py
src/transformers/models/unispeech_sat/modeling_unispeech_sat.py
src/transformers/models/vision_encoder_decoder/configuration_vision_encoder_decoder.py
src/transformers/models/vision_encoder_decoder/modeling_flax_vision_encoder_decoder.py
src/transformers/models/vision_encoder_decoder/modeling_vision_encoder_decoder.py
src/transformers/models/vision_text_dual_encoder/configuration_vision_text_dual_encoder.py
src/transformers/models/vision_text_dual_encoder/modeling_flax_vision_text_dual_encoder.py
src/transformers/models/vision_text_dual_encoder/modeling_vision_text_dual_encoder.py
src/transformers/models/vision_text_dual_encoder/processing_vision_text_dual_encoder.py
src/transformers/models/visual_bert/configuration_visual_bert.py
src/transformers/models/visual_bert/modeling_visual_bert.py
src/transformers/models/vit/configuration_vit.py
src/transformers/models/vit/feature_extraction_vit.py
src/transformers/models/vit/modeling_flax_vit.py
src/transformers/models/vit/modeling_tf_vit.py
src/transformers/models/vit/modeling_vit.py
src/transformers/models/wav2vec2/configuration_wav2vec2.py
src/transformers/models/wav2vec2/feature_extraction_wav2vec2.py
src/transformers/models/wav2vec2/modeling_flax_wav2vec2.py
src/transformers/models/wav2vec2/modeling_tf_wav2vec2.py
src/transformers/models/wav2vec2/modeling_wav2vec2.py
src/transformers/models/wav2vec2/processing_wav2vec2.py
src/transformers/models/wav2vec2/tokenization_wav2vec2.py
src/transformers/models/wav2vec2_phoneme/tokenization_wav2vec2_phoneme.py
src/transformers/models/wav2vec2_with_lm/processing_wav2vec2_with_lm.py
src/transformers/models/wavlm/configuration_wavlm.py
src/transformers/models/wavlm/modeling_wavlm.py
src/transformers/models/xlm/configuration_xlm.py
src/transformers/models/xlm/modeling_tf_xlm.py
src/transformers/models/xlm/modeling_xlm.py
src/transformers/models/xlm/tokenization_xlm.py
src/transformers/models/xlm_prophetnet/configuration_xlm_prophetnet.py
src/transformers/models/xlm_prophetnet/modeling_xlm_prophetnet.py
src/transformers/models/xlm_prophetnet/tokenization_xlm_prophetnet.py
src/transformers/models/xlm_roberta/configuration_xlm_roberta.py
src/transformers/models/xlm_roberta/modeling_tf_xlm_roberta.py
src/transformers/models/xlm_roberta/modeling_xlm_roberta.py
src/transformers/models/xlm_roberta/tokenization_xlm_roberta.py
src/transformers/models/xlm_roberta/tokenization_xlm_roberta_fast.py
src/transformers/models/xlnet/configuration_xlnet.py
src/transformers/models/xlnet/modeling_tf_xlnet.py
src/transformers/models/xlnet/modeling_xlnet.py
src/transformers/models/xlnet/tokenization_xlnet.py
src/transformers/models/xlnet/tokenization_xlnet_fast.py
src/transformers/onnx/convert.py
src/transformers/optimization.py
src/transformers/optimization_tf.py
src/transformers/pipelines/__init__.py
src/transformers/pipelines/audio_classification.py
src/transformers/pipelines/automatic_speech_recognition.py
src/transformers/pipelines/base.py
src/transformers/pipelines/conversational.py
src/transformers/pipelines/feature_extraction.py
src/transformers/pipelines/fill_mask.py
src/transformers/pipelines/image_classification.py
src/transformers/pipelines/image_segmentation.py
src/transformers/pipelines/object_detection.py
src/transformers/pipelines/question_answering.py
src/transformers/pipelines/table_question_answering.py
src/transformers/pipelines/text2text_generation.py
src/transformers/pipelines/text_classification.py
src/transformers/pipelines/text_generation.py
src/transformers/pipelines/token_classification.py
src/transformers/pipelines/zero_shot_classification.py
src/transformers/testing_utils.py
src/transformers/tokenization_utils.py
src/transformers/tokenization_utils_base.py
src/transformers/tokenization_utils_fast.py
src/transformers/trainer.py
src/transformers/trainer_callback.py
src/transformers/trainer_pt_utils.py
src/transformers/trainer_seq2seq.py
src/transformers/trainer_tf.py
src/transformers/trainer_utils.py
src/transformers/training_args.py
src/transformers/training_args_seq2seq.py
src/transformers/training_args_tf.py
src/transformers/utils/fx.py
src/transformers/utils/logging.py
src/transformers/utils/notebook.py
utils/style_doc.py
==================
c1138273d;Mihai Balint;2021-12-27 21:25:26 +0200;Fix duplicate call to save_checkpoint when using deepspeed (#14946)
* Fix duplicate call to save_checkpoint when using deepspeed / stage3_gather_fp16_weights_on_model_save

* Revert "Fix duplicate call to save_checkpoint when using deepspeed / stage3_gather_fp16_weights_on_model_save"

This reverts commit 6a3dec0397723a8417351dc38fdebf14ab17756c.

* Delete correct duplicate invocation of deepspeed save_checkpoint
==

src/transformers/trainer.py
==================
03885a3f5;Ayal Klein;2021-12-27 18:48:48 +0200;fix to issue #14833 in data_collator - consider no labels (#14930)

==

src/transformers/data/data_collator.py
==================
501307b58;Daniel Stancl;2021-12-27 12:37:52 +0100;Add `ElectraForCausalLM` -> Enable Electra encoder-decoder model (#14729)
* Add ElectraForCausalLM and cover some basic tests & need to fix a few tests

* Fix bugs

* make style

* make fix-copies

* Update doc

* Change docstring to markdown format

* Remove redundant update_keys_to_ignore
==

docs/source/model_doc/electra.mdx
src/transformers/__init__.py
src/transformers/models/auto/modeling_auto.py
src/transformers/models/electra/__init__.py
src/transformers/models/electra/modeling_electra.py
src/transformers/utils/dummy_pt_objects.py
tests/test_modeling_electra.py
==================
b058490ce;Nicolas Patry;2021-12-27 11:26:20 +0100;ChunkPipeline (batch_size enabled on `zero-cls` and `qa` pipelines. (#14225)
* Pipeline chunks.

* Batching for Chunking pipelines ?

* Batching for `question-answering` and `zero-shot-cls`.

* Fixing for FNet.

* Making ASR a chunk pipeline.

* Chunking ASR API.

* doc style.

* Fixing ASR test.

* Fixing QA eror (p_mask, padding is 1, not 0).

* Enable both vad and simple chunking.

* Max length for vad.

* remove inference mode, crashing on s2t.

* Revert ChunkPipeline for ASRpipeline.

Too many knobs for simple integration within the pipeline, better stick
to external convenience functions instead, more control to be had,
simpler pipeline and also easier to replace with other things later.

* Drop necessity for PT for these.

* Enabling generators.

* Add mic + cleanup.

* Typo.

* Typo2.

* Remove ASR work, it does not belong in this PR anymore.

* Update src/transformers/pipelines/pt_utils.py

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* Update src/transformers/pipelines/zero_shot_classification.py

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* Adding many comments.

* Doc quality.

* `hidden_states` handling.

* Adding doc.

* Bad rebase.

* Autofixing docs.

* Fixing CRITICAL bug in the new Zerocls pipeline.

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
==

docs/source/main_classes/pipelines.mdx
src/transformers/pipelines/base.py
src/transformers/pipelines/pt_utils.py
src/transformers/pipelines/question_answering.py
src/transformers/pipelines/zero_shot_classification.py
tests/test_pipelines_automatic_speech_recognition.py
tests/test_pipelines_common.py
==================
705ca7f21;Qing;2021-12-24 18:28:47 +0800;Fix Perceiver docs (#14917)

==

src/transformers/models/perceiver/modeling_perceiver.py
==================
116829900;Patrick von Platen;2021-12-23 23:17:20 +0100;[WavLM] fix wavlm docs (#14910)

==

src/transformers/models/wavlm/modeling_wavlm.py
==================
415810664;Stas Bekman;2021-12-23 13:12:59 -0800;[doc] install - add jax (#14912)
As `jax` cuda requires special instructions to be installed correctly add a link to jax installation instructions. 

Note: Flax install page only covers cpu jax installation info.
==

README.md
==================
676643c6d;Sylvain Gugger;2021-12-23 14:18:07 -0500;Better logic for getting tokenizer config in AutoTokenizer (#14906)
* Better logic for getting tokenizer config in AutoTokenizer

* Remove needless import

* Remove debug statement

* Address review comments
==

src/transformers/file_utils.py
src/transformers/models/auto/tokenization_auto.py
tests/test_tokenization_auto.py
==================
f566c6e3b;Sylvain Gugger;2021-12-23 13:59:33 -0500;Fix failing GPU trainer tests (#14903)
* Fix failing GPU trainer tests

* Remove print statements
==

tests/extended/test_trainer_ext.py
tests/test_trainer.py
==================
fe4197ab1;Patrick von Platen;2021-12-23 19:43:37 +0100;[Generate] Remove attention_mask and integrate model_main_input_name (#14856)
* up

* save

* correct

* up

* correct more

* up

* up

* up

* up

* up

* correct

* fix tf

* fix

* remove tokenizer
==

src/transformers/generation_utils.py
src/transformers/keras_callbacks.py
src/transformers/models/deit/modeling_deit.py
src/transformers/models/speech_encoder_decoder/modeling_speech_encoder_decoder.py
src/transformers/models/vit/modeling_vit.py
src/transformers/trainer_seq2seq.py
tests/test_generation_utils.py
tests/test_modeling_vision_encoder_decoder.py
==================
86b40073e;Stas Bekman;2021-12-23 10:19:34 -0800;[doc] post-porting (#14890)
found a few oddities:

1. https://huggingface.co/docs/transformers/main_classes/logging#transformers.utils.logging.enable_explicit_format
has a :: - this PR fixes it

2.  this looks borked too:
https://huggingface.co/docs/transformers/main_classes/logging#transformers.utils.logging.set_verbosity
 has a <

but I'm not sure where this one is coming from
==

src/transformers/utils/logging.py
==================
ee55ea692;Anton Lozhkov;2021-12-23 19:53:56 +0300;Update diarization and WavLM tolerances (#14902)

==

tests/test_modeling_unispeech_sat.py
tests/test_modeling_wav2vec2.py
tests/test_modeling_wavlm.py
==================
ef47d4f84;Patrick von Platen;2021-12-23 17:22:33 +0100;[AutoTokenizer] Fix incorrect from pretrained (#14900)

==

src/transformers/models/auto/tokenization_auto.py
==================
8f2cc1c3a;Yih-Dar;2021-12-23 17:19:44 +0100;Add TFCLIPModel (#13967)
* Start the work for TFCLIPModel

* Convert to TF code (TODO: loss + doc)

* Clean up

* Fix pooled_output for TFCLIPTextTransformer - using tf.gather_nd

* assert -> raise error

* Expose TFCLIPModel

* Deal with dummy_inputs

* Add tests

* Fix all tests. TODO: manual check weight loading + add more comments

* Fix pt tf equivalence test

* fixes

* update TFCLIPVisionEmbeddings's Conv2D

* Fix loss + overwrite test_pt_tf_model_equivalence from common

* Add a comment about the change about MainLayer in test_keras_save_load

* Set return_loss=True in TFCLIPModelTester + make tests pass

* overwrite test_pt_tf_model_equivalence from tf common

* fix base_model_prefix

* Fix examples

* remove unused

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* apply review suggestions

* change self.pre_layrnorm to self.pre_layernorm

* apply more review suggestions

* return attention probs before dropout (to align with PT)

* fix weight init

* fix

* build doc

* fix missing doc

* fix for test

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

docs/source/index.mdx
docs/source/model_doc/clip.mdx
src/transformers/__init__.py
src/transformers/activations_tf.py
src/transformers/models/auto/modeling_tf_auto.py
src/transformers/models/clip/__init__.py
src/transformers/models/clip/modeling_tf_clip.py
src/transformers/utils/dummy_tf_objects.py
tests/test_modeling_clip.py
tests/test_modeling_tf_clip.py
tests/test_modeling_tf_common.py
tests/test_modeling_tf_vit.py
utils/check_repo.py
==================
2d30443cd;Yang Dong;2021-12-23 07:53:33 -0800;Set `run_name` in MLflowCallback (#14894)
* Set run_name in MLflowCallback

* Update the docs for `run_name` argument
==

src/transformers/integrations.py
src/transformers/training_args.py
==================
1d651868d;Leandro von Werra;2021-12-23 14:59:11 +0100;add custom stopping criteria to human eval script (#14897)

==

examples/research_projects/codeparrot/requirements.txt
examples/research_projects/codeparrot/scripts/human_eval.py
==================
6b655cc63;lewtun;2021-12-23 13:35:56 +0100;Add ONNX support for MarianMT models (#14586)
* First commit to add MarianMT to ONNX

* Now MarianModel.forward() automatically generates decoder_input_ids, like BartModel.forward()

* Adjusted MarianOnnxConfig.inputs and outputs to work with seq2seq-lm feature

* Style fix

* Added support for other features for already supported models

* Partial support for causal and seq2seq models

* Partial support for causal and seq2seq models

* Add default task for MarianMT ONNX

* Remove automatic creation of decoder_input_ids

* Extend inputs and outputs for MarianMT ONNX config

* Add MarianMT to ONNX unit tests

* Refactor

* OnnxSeq2SeqConfigWithPast to support seq2seq models

* Parameterized the onnx tests

* Restored run_mlm.py

* Restored run_mlm.py

* [WIP] BART update

* BART and MBART

* Add past_key_values and fix dummy decoder inputs

Using a sequence length of 1 in generate_dummy_outputs() produces large discrepancies, presumably due to some hidden optimisations.

* Refactor MarianOnnxConfig to remove custom past_key_values logic

* Fix quality

* Revert "Revert "Added support for other features for already supported models (#14358)" (#14679)"

This reverts commit 0f4e39c5599523c110cd713f60a3bfa145dad807.

* is_torch_available test to avoid failing imports

* sorting parameterize parameters to solve ERROR gw0 gw1

* tests fix

* tests fix

* GPT2 with past fix

* Fixed stateful class attribute change that was breaking things when converting multiple models sequentially

* Removed onnx file

* Refactor Marian export to account for base changes

* Fix copies

* Implemented suggestions

* Extend support for causal LM

* Revert "Revert "Added support for other features for already supported models (#14358)" (#14679)"

This reverts commit 0f4e39c5599523c110cd713f60a3bfa145dad807.

* is_torch_available test to avoid failing imports

* sorting parameterize parameters to solve ERROR gw0 gw1

* tests fix

* tests fix

* GPT2 with past fix

* Fixed stateful class attribute change that was breaking things when converting multiple models sequentially

* Removed onnx file

* Implemented suggestions

* Fixed __init__ to resolve conflict with master

* Revert "Revert "Added support for other features for already supported models (#14358)" (#14679)"

This reverts commit 0f4e39c5599523c110cd713f60a3bfa145dad807.

* is_torch_available test to avoid failing imports

* sorting parameterize parameters to solve ERROR gw0 gw1

* tests fix

* tests fix

* GPT2 with past fix

* Fixed stateful class attribute change that was breaking things when converting multiple models sequentially

* Removed onnx file

* Implemented suggestions

* Fixed __init__ to resolve conflict with master

* Remove commented import

* Remove ONNX model

* Remove redundant class method

* Tidy up imports

* Fix quality

* Refactor dummy input function

* Add copied from statements to Marian config functions

* Remove false copied from comments

* Fix copy from comment

Co-authored-by: Massimiliano Bruni <massimiliano.bruni@hcl.com>
Co-authored-by: Michael Benayoun <mickbenayoun@gmail.com>
==

docs/source/serialization.mdx
src/transformers/models/marian/__init__.py
src/transformers/models/marian/configuration_marian.py
src/transformers/models/marian/tokenization_marian.py
src/transformers/onnx/features.py
tests/test_onnx_v2.py
==================
6a7b9da2a;Henrik Holm;2021-12-23 10:23:39 +0100;Add 'with torch.no_grad()' to integration test forward pass (#14808)

==

tests/test_modeling_albert.py
==================
d8c09c654;Alex Hedges;2021-12-23 04:19:25 -0500;Fix AttributeError from PreTrainedTokenizerFast.decoder (#14691)

==

src/transformers/tokenization_utils_fast.py
==================
421057952;Yih-Dar;2021-12-23 10:07:21 +0100;Fix doc examples: ... takes no keyword arguments (#14701)
* Fix doc examples: ... takes no keyword arguments

* fix copies

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>
Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
==

src/transformers/models/deberta/modeling_deberta.py
src/transformers/models/deberta_v2/modeling_deberta_v2.py
src/transformers/models/sew_d/modeling_sew_d.py
==================
355dc0ce6;lewtun;2021-12-23 10:05:32 +0100;Fix installation instructions for BART ONNX example (#14885)

==

examples/onnx/pytorch/summarization/README.md
==================
207594be8;Sylvain Gugger;2021-12-22 16:14:35 -0500;Convert rst files (#14888)
* Convert all tutorials and guides

* Convert all remaining rst to mdx

* Track and fix bad links
==

docs/source/add_new_model.mdx
docs/source/add_new_pipeline.mdx
docs/source/add_new_pipeline.rst
docs/source/bertology.mdx
docs/source/bertology.rst
docs/source/community.mdx
docs/source/converting_tensorflow_models.mdx
docs/source/converting_tensorflow_models.rst
docs/source/fast_tokenizers.mdx
docs/source/fast_tokenizers.rst
docs/source/glossary.mdx
docs/source/glossary.rst
docs/source/installation.mdx
docs/source/internal/file_utils.mdx
docs/source/internal/file_utils.rst
docs/source/internal/generation_utils.mdx
docs/source/internal/generation_utils.rst
docs/source/internal/modeling_utils.mdx
docs/source/internal/modeling_utils.rst
docs/source/internal/pipelines_utils.mdx
docs/source/internal/pipelines_utils.rst
docs/source/internal/tokenization_utils.mdx
docs/source/internal/tokenization_utils.rst
docs/source/internal/trainer_utils.mdx
docs/source/internal/trainer_utils.rst
docs/source/main_classes/callback.mdx
docs/source/main_classes/callback.rst
docs/source/main_classes/configuration.mdx
docs/source/main_classes/configuration.rst
docs/source/main_classes/data_collator.mdx
docs/source/main_classes/data_collator.rst
docs/source/main_classes/feature_extractor.mdx
docs/source/main_classes/feature_extractor.rst
docs/source/main_classes/keras_callbacks.mdx
docs/source/main_classes/keras_callbacks.rst
docs/source/main_classes/logging.mdx
docs/source/main_classes/logging.rst
docs/source/main_classes/model.mdx
docs/source/main_classes/model.rst
docs/source/main_classes/optimizer_schedules.mdx
docs/source/main_classes/optimizer_schedules.rst
docs/source/main_classes/output.mdx
docs/source/main_classes/output.rst
docs/source/main_classes/pipelines.mdx
docs/source/main_classes/pipelines.rst
docs/source/main_classes/processors.mdx
docs/source/main_classes/processors.rst
docs/source/main_classes/tokenizer.mdx
docs/source/main_classes/tokenizer.rst
docs/source/migration.mdx
docs/source/model_doc/mbart.mdx
docs/source/model_doc/megatron_bert.mdx
docs/source/model_doc/megatron_bert.rst
docs/source/model_doc/megatron_gpt2.mdx
docs/source/model_doc/mluke.mdx
docs/source/model_doc/mluke.rst
docs/source/model_doc/mobilebert.mdx
docs/source/model_doc/mobilebert.rst
docs/source/model_doc/mpnet.mdx
docs/source/model_doc/mpnet.rst
docs/source/model_doc/mt5.mdx
docs/source/model_doc/mt5.rst
docs/source/model_doc/pegasus.mdx
docs/source/model_doc/pegasus.rst
docs/source/model_doc/phobert.mdx
docs/source/model_doc/phobert.rst
docs/source/model_doc/prophetnet.mdx
docs/source/model_doc/prophetnet.rst
docs/source/model_doc/qdqbert.mdx
docs/source/model_doc/qdqbert.rst
docs/source/model_doc/rag.mdx
docs/source/model_doc/rag.rst
docs/source/model_doc/reformer.mdx
docs/source/model_doc/reformer.rst
docs/source/model_doc/rembert.mdx
docs/source/model_doc/rembert.rst
docs/source/model_doc/retribert.mdx
docs/source/model_doc/retribert.rst
docs/source/model_doc/roberta.mdx
docs/source/model_doc/roberta.rst
docs/source/model_doc/roformer.mdx
docs/source/model_doc/roformer.rst
docs/source/model_doc/segformer.mdx
docs/source/model_doc/segformer.rst
docs/source/model_doc/sew.mdx
docs/source/model_doc/sew.rst
docs/source/model_doc/sew_d.mdx
docs/source/model_doc/sew_d.rst
docs/source/model_doc/speech_to_text.mdx
docs/source/model_doc/speech_to_text.rst
docs/source/model_doc/speech_to_text_2.mdx
docs/source/model_doc/speech_to_text_2.rst
docs/source/model_doc/speechencoderdecoder.mdx
docs/source/model_doc/speechencoderdecoder.rst
docs/source/model_doc/splinter.mdx
docs/source/model_doc/splinter.rst
docs/source/model_doc/squeezebert.mdx
docs/source/model_doc/squeezebert.rst
docs/source/model_doc/t5.mdx
docs/source/model_doc/t5.rst
docs/source/model_doc/t5v1.1.mdx
docs/source/model_doc/t5v1.1.rst
docs/source/model_doc/tapas.mdx
docs/source/model_doc/transformerxl.mdx
docs/source/model_doc/transformerxl.rst
docs/source/model_doc/trocr.mdx
docs/source/model_doc/unispeech.mdx
docs/source/model_doc/unispeech.rst
docs/source/model_doc/unispeech_sat.mdx
docs/source/model_doc/unispeech_sat.rst
docs/source/model_doc/vision_text_dual_encoder.mdx
docs/source/model_doc/vision_text_dual_encoder.rst
docs/source/model_doc/visionencoderdecoder.mdx
docs/source/model_doc/visionencoderdecoder.rst
docs/source/model_doc/visual_bert.mdx
docs/source/model_doc/visual_bert.rst
docs/source/model_doc/vit.mdx
docs/source/model_doc/vit.rst
docs/source/model_doc/wav2vec2.mdx
docs/source/model_doc/wav2vec2.rst
docs/source/model_doc/wavlm.mdx
docs/source/model_doc/wavlm.rst
docs/source/model_doc/xlm.mdx
docs/source/model_doc/xlm.rst
docs/source/model_doc/xlmprophetnet.mdx
docs/source/model_doc/xlmprophetnet.rst
docs/source/model_doc/xlmroberta.mdx
docs/source/model_doc/xlmroberta.rst
docs/source/model_doc/xlnet.mdx
docs/source/model_doc/xlnet.rst
docs/source/model_doc/xls_r.mdx
docs/source/model_doc/xlsr_wav2vec2.mdx
docs/source/model_sharing.mdx
docs/source/model_sharing.rst
docs/source/model_summary.mdx
docs/source/model_summary.rst
docs/source/parallelism.mdx
docs/source/performance.mdx
docs/source/philosophy.mdx
docs/source/philosophy.rst
docs/source/pr_checks.mdx
docs/source/sagemaker.mdx
docs/source/serialization.mdx
docs/source/serialization.rst
docs/source/troubleshooting.mdx
src/transformers/commands/add_new_model.py
src/transformers/integrations.py
src/transformers/models/beit/modeling_beit.py
src/transformers/models/bertweet/tokenization_bertweet.py
src/transformers/models/cpm/tokenization_cpm.py
src/transformers/models/cpm/tokenization_cpm_fast.py
src/transformers/models/fnet/modeling_fnet.py
src/transformers/models/hubert/modeling_hubert.py
src/transformers/models/hubert/modeling_tf_hubert.py
src/transformers/models/ibert/modeling_ibert.py
src/transformers/models/layoutlm/modeling_layoutlm.py
src/transformers/models/layoutlmv2/modeling_layoutlmv2.py
src/transformers/models/prophetnet/modeling_prophetnet.py
src/transformers/models/rag/modeling_rag.py
src/transformers/models/rag/modeling_tf_rag.py
src/transformers/models/roformer/tokenization_roformer.py
src/transformers/models/sew/modeling_sew.py
src/transformers/models/sew_d/modeling_sew_d.py
src/transformers/models/splinter/modeling_splinter.py
src/transformers/models/unispeech/modeling_unispeech.py
src/transformers/models/unispeech_sat/modeling_unispeech_sat.py
src/transformers/models/visual_bert/modeling_visual_bert.py
src/transformers/models/wav2vec2/modeling_flax_wav2vec2.py
src/transformers/models/wav2vec2/modeling_tf_wav2vec2.py
src/transformers/models/wav2vec2/modeling_wav2vec2.py
src/transformers/models/wavlm/modeling_wavlm.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/{{cookiecutter.lowercase_modelname}}.mdx
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/{{cookiecutter.lowercase_modelname}}.rst
utils/check_table.py
==================
b0c7d2ec5;Matt;2021-12-22 20:35:39 +0000;Keras metric callback (#14867)
* Working on splitting out labels

* First working version

* Fixed concatenation of outputs and labels

* val_dataset -> eval_dataset

* Only pass input arrays in tokenizer.model_input_names

* Only pass input arrays in tokenizer.model_input_names

* Only remove unexpected keys when predict_with_generate is True

* Adding proper docstring

* Adding example to docstring

* Add a proper ROUGE metric example

* Add a proper ROUGE metric example

* Add version checking

* Update src/transformers/keras_callbacks.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/keras_callbacks.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/keras_callbacks.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/keras_callbacks.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Remove requirement for tokenizer with predict_with_generate

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

src/transformers/keras_callbacks.py
==================
fa39ff9fc;Patrick von Platen;2021-12-22 20:39:44 +0100;Docs for v4.16.0dev0

==

examples/flax/question-answering/run_qa.py
examples/flax/token-classification/run_flax_ner.py
examples/pytorch/audio-classification/run_audio_classification.py
examples/pytorch/image-classification/run_image_classification.py
examples/pytorch/language-modeling/run_clm.py
examples/pytorch/language-modeling/run_mlm.py
examples/pytorch/language-modeling/run_plm.py
examples/pytorch/multiple-choice/run_swag.py
examples/pytorch/question-answering/run_qa.py
examples/pytorch/question-answering/run_qa_beam_search.py
examples/pytorch/question-answering/run_qa_beam_search_no_trainer.py
examples/pytorch/question-answering/run_qa_no_trainer.py
examples/pytorch/question-answering/run_seq2seq_qa.py
examples/pytorch/speech-recognition/run_speech_recognition_ctc.py
examples/pytorch/summarization/run_summarization.py
examples/pytorch/text-classification/run_glue.py
examples/pytorch/text-classification/run_xnli.py
examples/pytorch/token-classification/run_ner.py
examples/pytorch/translation/run_translation.py
examples/tensorflow/multiple-choice/run_swag.py
examples/tensorflow/question-answering/run_qa.py
examples/tensorflow/summarization/run_summarization.py
examples/tensorflow/text-classification/run_glue.py
examples/tensorflow/translation/run_translation.py
setup.py
src/transformers/__init__.py
==================
05fa1a7ac;Patrick von Platen;2021-12-22 18:43:15 +0100;Release: v4.15.0

==

examples/flax/question-answering/run_qa.py
examples/flax/token-classification/run_flax_ner.py
examples/pytorch/audio-classification/run_audio_classification.py
examples/pytorch/image-classification/run_image_classification.py
examples/pytorch/language-modeling/run_clm.py
examples/pytorch/language-modeling/run_mlm.py
examples/pytorch/language-modeling/run_plm.py
examples/pytorch/multiple-choice/run_swag.py
examples/pytorch/question-answering/run_qa.py
examples/pytorch/question-answering/run_qa_beam_search.py
examples/pytorch/question-answering/run_qa_beam_search_no_trainer.py
examples/pytorch/question-answering/run_qa_no_trainer.py
examples/pytorch/question-answering/run_seq2seq_qa.py
examples/pytorch/speech-recognition/run_speech_recognition_ctc.py
examples/pytorch/summarization/run_summarization.py
examples/pytorch/text-classification/run_glue.py
examples/pytorch/text-classification/run_xnli.py
examples/pytorch/token-classification/run_ner.py
examples/pytorch/translation/run_translation.py
examples/tensorflow/multiple-choice/run_swag.py
examples/tensorflow/question-answering/run_qa.py
examples/tensorflow/summarization/run_summarization.py
examples/tensorflow/text-classification/run_glue.py
examples/tensorflow/translation/run_translation.py
setup.py
src/transformers/__init__.py
==================
87a033d9f;Sylvain Gugger;2021-12-22 12:28:45 -0500;Properly indent return block (#14887)

==

src/transformers/models/luke/tokenization_luke.py
src/transformers/models/mluke/tokenization_mluke.py
src/transformers/tokenization_utils_base.py
==================
13504dcbe;Michael Benayoun;2021-12-22 14:43:11 +0100;Onnx enable tasks for supported models (part 2) (#14700)
* Revert "Revert "Added support for other features for already supported models (#14358)" (#14679)"

This reverts commit 0f4e39c5599523c110cd713f60a3bfa145dad807.

* is_torch_available test to avoid failing imports

* sorting parameterize parameters to solve ERROR gw0 gw1

* tests fix

* tests fix

* GPT2 with past fix

* Fixed stateful class attribute change that was breaking things when converting multiple models sequentially

* Removed onnx file

* Implemented suggestions

* Fixed __init__ to resolve conflict with master

* Remove commented import
==

src/transformers/models/albert/configuration_albert.py
src/transformers/models/bart/configuration_bart.py
src/transformers/models/bert/configuration_bert.py
src/transformers/models/distilbert/configuration_distilbert.py
src/transformers/models/gpt2/configuration_gpt2.py
src/transformers/models/gpt_neo/configuration_gpt_neo.py
src/transformers/models/mbart/configuration_mbart.py
src/transformers/models/roberta/configuration_roberta.py
src/transformers/models/t5/configuration_t5.py
src/transformers/models/xlm_roberta/configuration_xlm_roberta.py
src/transformers/onnx/__init__.py
src/transformers/onnx/__main__.py
src/transformers/onnx/config.py
src/transformers/onnx/convert.py
src/transformers/onnx/features.py
tests/test_onnx_v2.py
==================
1045a36c1;Mario ≈†a≈°ko;2021-12-22 14:42:19 +0100;Fix pytorch image classification example (#14883)
* Update example

* Remove skip in tests
==

examples/pytorch/image-classification/run_image_classification.py
examples/pytorch/test_examples.py
==================
7df4b90c7;NielsRogge;2021-12-22 14:18:03 +0100;Fix Perceiver docs (#14879)

==

docs/source/model_doc/perceiver.mdx
src/transformers/models/perceiver/modeling_perceiver.py
==================
e37bc579f;Sylvain Gugger;2021-12-22 08:19:36 -0500;Fix typo in error message

==

utils/check_repo.py
==================
17efc806b;charon____;2021-12-22 20:52:07 +0800;IterableDatasetShard should use per device batch size instead of real batch size (#14714)

==

src/transformers/trainer.py
==================
2a56edb32;guillaume-be;2021-12-22 13:36:08 +0100;Updated deberta attention (#14625)
* Removed unused p2p attention handling

* Updated DeBERTa configuration

* Updated TF DeBERTa attention

* Rolled back accidental comment deletion

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
==

src/transformers/models/deberta/configuration_deberta.py
src/transformers/models/deberta/modeling_deberta.py
src/transformers/models/deberta/modeling_tf_deberta.py
==================
824fd44fc;Ryokan RI;2021-12-22 20:35:59 +0900;Feature/fix slow test in mluke (#14749)
* make MLukeTokenizerTest fast

* make LukeTokenizerTest fast

* add entry to _toctree.yaml
==

docs/source/_toctree.yml
src/transformers/models/luke/tokenization_luke.py
src/transformers/models/mluke/tokenization_mluke.py
tests/fixtures/test_entity_vocab.json
tests/test_tokenization_luke.py
tests/test_tokenization_mluke.py
==================
c94c1b896;SaulLu;2021-12-22 10:51:55 +0100;update the arguments `add_prefix_space` and `trim_offsets` in `backend_tokenizer.post_processor` of `RobertaTokenizerFast` (#14752)
* add tests

* change post-processor, pre-tokenizer and decoder (can't update decoder)

* update test (remove decoder which doesn't depend on trim and add_prefix)

* just update the post_processor

* fix change

* `trim_offsets` has no influence on `pre_tokenizer`

* remove a test that need some input from the `tokenizers` lib maintainers

* format

* add new test offsets roberta

* polish comments
==

src/transformers/models/roberta/tokenization_roberta_fast.py
tests/test_tokenization_roberta.py
==================
ec3567fe2;Lysandre Debut;2021-12-22 03:27:30 -0500;Convert model files from rst to mdx (#14865)
* First pass

* Apply suggestions from code review

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

docs/source/model_doc/albert.mdx
docs/source/model_doc/albert.rst
docs/source/model_doc/bart.mdx
docs/source/model_doc/bart.rst
docs/source/model_doc/barthez.mdx
docs/source/model_doc/barthez.rst
docs/source/model_doc/bartpho.mdx
docs/source/model_doc/bartpho.rst
docs/source/model_doc/beit.mdx
docs/source/model_doc/beit.rst
docs/source/model_doc/bert_japanese.mdx
docs/source/model_doc/bert_japanese.rst
docs/source/model_doc/bertgeneration.mdx
docs/source/model_doc/bertgeneration.rst
docs/source/model_doc/bertweet.mdx
docs/source/model_doc/bertweet.rst
docs/source/model_doc/bigbird.mdx
docs/source/model_doc/bigbird.rst
docs/source/model_doc/bigbird_pegasus.mdx
docs/source/model_doc/bigbird_pegasus.rst
docs/source/model_doc/blenderbot.mdx
docs/source/model_doc/blenderbot.rst
docs/source/model_doc/blenderbot_small.mdx
docs/source/model_doc/blenderbot_small.rst
docs/source/model_doc/bort.mdx
docs/source/model_doc/byt5.mdx
docs/source/model_doc/byt5.rst
docs/source/model_doc/camembert.mdx
docs/source/model_doc/camembert.rst
docs/source/model_doc/canine.mdx
docs/source/model_doc/canine.rst
docs/source/model_doc/clip.mdx
docs/source/model_doc/clip.rst
docs/source/model_doc/convbert.mdx
docs/source/model_doc/convbert.rst
docs/source/model_doc/cpm.mdx
docs/source/model_doc/ctrl.mdx
docs/source/model_doc/ctrl.rst
docs/source/model_doc/deberta.mdx
docs/source/model_doc/deberta.rst
docs/source/model_doc/deberta_v2.mdx
docs/source/model_doc/deberta_v2.rst
docs/source/model_doc/deit.mdx
docs/source/model_doc/dialogpt.mdx
docs/source/model_doc/distilbert.mdx
docs/source/model_doc/distilbert.rst
docs/source/model_doc/dpr.mdx
docs/source/model_doc/dpr.rst
docs/source/model_doc/electra.mdx
docs/source/model_doc/electra.rst
docs/source/model_doc/encoderdecoder.mdx
docs/source/model_doc/encoderdecoder.rst
docs/source/model_doc/flaubert.mdx
docs/source/model_doc/flaubert.rst
docs/source/model_doc/fnet.mdx
docs/source/model_doc/fnet.rst
docs/source/model_doc/fsmt.mdx
docs/source/model_doc/fsmt.rst
docs/source/model_doc/funnel.mdx
docs/source/model_doc/funnel.rst
docs/source/model_doc/gpt.mdx
docs/source/model_doc/gpt.rst
docs/source/model_doc/gpt2.mdx
docs/source/model_doc/gpt2.rst
docs/source/model_doc/gpt_neo.mdx
docs/source/model_doc/gpt_neo.rst
docs/source/model_doc/gptj.mdx
docs/source/model_doc/gptj.rst
docs/source/model_doc/herbert.mdx
docs/source/model_doc/herbert.rst
docs/source/model_doc/hubert.mdx
docs/source/model_doc/hubert.rst
docs/source/model_doc/ibert.mdx
docs/source/model_doc/ibert.rst
docs/source/model_doc/layoutlm.mdx
docs/source/model_doc/layoutlm.rst
docs/source/model_doc/layoutlmv2.mdx
docs/source/model_doc/layoutlmv2.rst
docs/source/model_doc/layoutxlm.mdx
docs/source/model_doc/layoutxlm.rst
docs/source/model_doc/led.mdx
docs/source/model_doc/led.rst
docs/source/model_doc/longformer.mdx
docs/source/model_doc/longformer.rst
docs/source/model_doc/luke.mdx
docs/source/model_doc/luke.rst
docs/source/model_doc/lxmert.mdx
docs/source/model_doc/lxmert.rst
docs/source/model_doc/m2m_100.mdx
docs/source/model_doc/m2m_100.rst
docs/source/model_doc/marian.mdx
docs/source/model_doc/marian.rst
docs/source/model_doc/mbart.mdx
docs/source/model_doc/mbart.rst
==================
d0422de56;Sylvain Gugger;2021-12-21 18:54:41 -0500;Fix doc mistakes (#14874)
* Remove double returns

* Last fixes

* Quality

* Last fix for Lxmert
==

src/transformers/models/lxmert/modeling_lxmert.py
src/transformers/models/lxmert/modeling_tf_lxmert.py
src/transformers/models/perceiver/modeling_perceiver.py
src/transformers/models/tapas/modeling_tf_tapas.py
src/transformers/models/visual_bert/modeling_visual_bert.py
==================
e846a56ca;Sylvain Gugger;2021-12-21 17:57:37 -0500;Fix `FlaxMarianMTModel` return block. (#14873)
* Fixes in marian doc

* Another time

* Add return block in FlaxMarianMTModel
==

src/transformers/models/marian/modeling_flax_marian.py
==================
a6b7b47a3;Sylvain Gugger;2021-12-21 17:17:02 -0500;Fixes in marian doc (#14872)
* Fixes in marian doc

* Another time
==

src/transformers/models/marian/modeling_marian.py
==================
eec9c8bbd;Mishig Davaadorj;2021-12-21 22:54:10 +0100;Fix FLAX_MULTIPLE_CHOICE_SAMPLE typo (#14871)

==

src/transformers/file_utils.py
==================
e51c7b587;Sylvain Gugger;2021-12-21 15:15:17 -0500;Skip failing test

==

examples/pytorch/test_examples.py
==================
27b3031de;Sylvain Gugger;2021-12-21 15:06:33 -0500;Mass conversion of documentation from rst to Markdown (#14866)
* Convert docstrings of all configurations and tokenizers

* Processors and fixes

* Last modeling files and fixes to models

* Pipeline modules

* Utils files

* Data submodule

* All the other files

* Style

* Missing examples

* Style again

* Fix copies

* Say bye bye to rst docstrings forever
==

src/transformers/configuration_utils.py
src/transformers/convert_slow_tokenizer.py
src/transformers/data/data_collator.py
src/transformers/data/processors/glue.py
src/transformers/data/processors/squad.py
src/transformers/data/processors/utils.py
src/transformers/debug_utils.py
src/transformers/deepspeed.py
src/transformers/feature_extraction_sequence_utils.py
src/transformers/feature_extraction_utils.py
src/transformers/generation_beam_search.py
src/transformers/generation_flax_logits_process.py
src/transformers/generation_flax_utils.py
src/transformers/generation_logits_process.py
src/transformers/generation_stopping_criteria.py
src/transformers/generation_tf_utils.py
src/transformers/generation_utils.py
src/transformers/image_utils.py
src/transformers/integrations.py
src/transformers/keras_callbacks.py
src/transformers/modelcard.py
src/transformers/modeling_flax_pytorch_utils.py
src/transformers/modeling_flax_utils.py
src/transformers/modeling_tf_utils.py
src/transformers/modeling_utils.py
src/transformers/models/albert/configuration_albert.py
src/transformers/models/albert/modeling_flax_albert.py
src/transformers/models/albert/modeling_tf_albert.py
src/transformers/models/albert/tokenization_albert.py
src/transformers/models/albert/tokenization_albert_fast.py
src/transformers/models/auto/auto_factory.py
src/transformers/models/auto/configuration_auto.py
src/transformers/models/auto/dynamic.py
src/transformers/models/auto/feature_extraction_auto.py
src/transformers/models/auto/processing_auto.py
src/transformers/models/auto/tokenization_auto.py
src/transformers/models/bart/configuration_bart.py
src/transformers/models/bart/modeling_flax_bart.py
src/transformers/models/bart/tokenization_bart.py
src/transformers/models/bart/tokenization_bart_fast.py
src/transformers/models/barthez/tokenization_barthez.py
src/transformers/models/barthez/tokenization_barthez_fast.py
src/transformers/models/bartpho/tokenization_bartpho.py
src/transformers/models/beit/configuration_beit.py
src/transformers/models/beit/feature_extraction_beit.py
src/transformers/models/beit/modeling_beit.py
src/transformers/models/beit/modeling_flax_beit.py
src/transformers/models/bert/configuration_bert.py
src/transformers/models/bert/modeling_flax_bert.py
src/transformers/models/bert/modeling_tf_bert.py
src/transformers/models/bert/tokenization_bert.py
src/transformers/models/bert/tokenization_bert_fast.py
src/transformers/models/bert_generation/configuration_bert_generation.py
src/transformers/models/bert_generation/tokenization_bert_generation.py
src/transformers/models/bert_japanese/tokenization_bert_japanese.py
src/transformers/models/bertweet/tokenization_bertweet.py
src/transformers/models/big_bird/configuration_big_bird.py
src/transformers/models/big_bird/modeling_flax_big_bird.py
src/transformers/models/big_bird/tokenization_big_bird.py
src/transformers/models/big_bird/tokenization_big_bird_fast.py
src/transformers/models/bigbird_pegasus/configuration_bigbird_pegasus.py
src/transformers/models/blenderbot/configuration_blenderbot.py
src/transformers/models/blenderbot/modeling_blenderbot.py
src/transformers/models/blenderbot/modeling_flax_blenderbot.py
src/transformers/models/blenderbot/tokenization_blenderbot.py
src/transformers/models/blenderbot/tokenization_blenderbot_fast.py
src/transformers/models/blenderbot_small/configuration_blenderbot_small.py
src/transformers/models/blenderbot_small/modeling_blenderbot_small.py
src/transformers/models/blenderbot_small/modeling_flax_blenderbot_small.py
src/transformers/models/blenderbot_small/tokenization_blenderbot_small.py
src/transformers/models/blenderbot_small/tokenization_blenderbot_small_fast.py
src/transformers/models/byt5/tokenization_byt5.py
src/transformers/models/camembert/configuration_camembert.py
src/transformers/models/camembert/tokenization_camembert.py
src/transformers/models/camembert/tokenization_camembert_fast.py
src/transformers/models/canine/configuration_canine.py
src/transformers/models/canine/tokenization_canine.py
src/transformers/models/clip/configuration_clip.py
src/transformers/models/clip/feature_extraction_clip.py
src/transformers/models/clip/modeling_clip.py
src/transformers/models/clip/modeling_flax_clip.py
src/transformers/models/clip/processing_clip.py
src/transformers/models/clip/tokenization_clip.py
src/transformers/models/clip/tokenization_clip_fast.py
src/transformers/models/convbert/configuration_convbert.py
src/transformers/models/convbert/tokenization_convbert.py
src/transformers/models/convbert/tokenization_convbert_fast.py
src/transformers/models/cpm/tokenization_cpm.py
src/transformers/models/cpm/tokenization_cpm_fast.py
src/transformers/models/ctrl/configuration_ctrl.py
src/transformers/models/ctrl/tokenization_ctrl.py
src/transformers/models/deberta/configuration_deberta.py
src/transformers/models/deberta/tokenization_deberta.py
src/transformers/models/deberta/tokenization_deberta_fast.py
src/transformers/models/deberta_v2/configuration_deberta_v2.py
src/transformers/models/deberta_v2/tokenization_deberta_v2.py
src/transformers/models/deit/configuration_deit.py
src/transformers/models/deit/feature_extraction_deit.py
src/transformers/models/deit/modeling_deit.py
src/transformers/models/detr/configuration_detr.py
src/transformers/models/detr/feature_extraction_detr.py
src/transformers/models/detr/modeling_detr.py
src/transformers/models/distilbert/configuration_distilbert.py
src/transformers/models/distilbert/tokenization_distilbert.py
src/transformers/models/distilbert/tokenization_distilbert_fast.py
src/transformers/models/dpr/configuration_dpr.py
src/transformers/models/dpr/modeling_dpr.py
src/transformers/models/dpr/modeling_tf_dpr.py
src/transformers/models/dpr/tokenization_dpr.py
src/transformers/models/dpr/tokenization_dpr_fast.py
src/transformers/models/electra/configuration_electra.py
src/transformers/models/electra/modeling_flax_electra.py
src/transformers/models/electra/modeling_tf_electra.py
src/transformers/models/electra/tokenization_electra.py
src/transformers/models/electra/tokenization_electra_fast.py
src/transformers/models/encoder_decoder/configuration_encoder_decoder.py
src/transformers/models/encoder_decoder/modeling_encoder_decoder.py
src/transformers/models/encoder_decoder/modeling_flax_encoder_decoder.py
src/transformers/models/encoder_decoder/modeling_tf_encoder_decoder.py
src/transformers/models/flaubert/configuration_flaubert.py
src/transformers/models/flaubert/tokenization_flaubert.py
src/transformers/models/fnet/configuration_fnet.py
src/transformers/models/fnet/tokenization_fnet.py
src/transformers/models/fnet/tokenization_fnet_fast.py
src/transformers/models/fsmt/configuration_fsmt.py
src/transformers/models/fsmt/tokenization_fsmt.py
src/transformers/models/funnel/configuration_funnel.py
src/transformers/models/funnel/modeling_tf_funnel.py
src/transformers/models/funnel/tokenization_funnel.py
src/transformers/models/funnel/tokenization_funnel_fast.py
src/transformers/models/gpt2/configuration_gpt2.py
src/transformers/models/gpt2/modeling_gpt2.py
src/transformers/models/gpt2/tokenization_gpt2.py
src/transformers/models/gpt2/tokenization_gpt2_fast.py
src/transformers/models/gpt_neo/configuration_gpt_neo.py
src/transformers/models/gptj/configuration_gptj.py
src/transformers/models/gptj/modeling_gptj.py
src/transformers/models/herbert/tokenization_herbert.py
src/transformers/models/herbert/tokenization_herbert_fast.py
src/transformers/models/hubert/configuration_hubert.py
src/transformers/models/hubert/modeling_hubert.py
src/transformers/models/hubert/modeling_tf_hubert.py
src/transformers/models/ibert/configuration_ibert.py
src/transformers/models/ibert/quant_modules.py
src/transformers/models/imagegpt/configuration_imagegpt.py
src/transformers/models/imagegpt/feature_extraction_imagegpt.py
src/transformers/models/layoutlm/configuration_layoutlm.py
src/transformers/models/layoutlm/modeling_layoutlm.py
src/transformers/models/layoutlm/modeling_tf_layoutlm.py
src/transformers/models/layoutlm/tokenization_layoutlm.py
src/transformers/models/layoutlm/tokenization_layoutlm_fast.py
src/transformers/models/layoutlmv2/configuration_layoutlmv2.py
src/transformers/models/layoutlmv2/feature_extraction_layoutlmv2.py
src/transformers/models/layoutlmv2/modeling_layoutlmv2.py
src/transformers/models/layoutlmv2/processing_layoutlmv2.py
src/transformers/models/layoutlmv2/tokenization_layoutlmv2.py
src/transformers/models/layoutlmv2/tokenization_layoutlmv2_fast.py
src/transformers/models/layoutxlm/processing_layoutxlm.py
src/transformers/models/layoutxlm/tokenization_layoutxlm.py
src/transformers/models/layoutxlm/tokenization_layoutxlm_fast.py
src/transformers/models/led/configuration_led.py
src/transformers/models/led/modeling_led.py
src/transformers/models/led/modeling_tf_led.py
src/transformers/models/led/tokenization_led.py
src/transformers/models/led/tokenization_led_fast.py
src/transformers/models/longformer/configuration_longformer.py
src/transformers/models/longformer/modeling_longformer.py
src/transformers/models/longformer/modeling_tf_longformer.py
src/transformers/models/longformer/tokenization_longformer.py
src/transformers/models/longformer/tokenization_longformer_fast.py
src/transformers/models/luke/configuration_luke.py
src/transformers/models/luke/modeling_luke.py
src/transformers/models/luke/tokenization_luke.py
src/transformers/models/lxmert/configuration_lxmert.py
src/transformers/models/lxmert/tokenization_lxmert.py
src/transformers/models/lxmert/tokenization_lxmert_fast.py
src/transformers/models/m2m_100/configuration_m2m_100.py
src/transformers/models/m2m_100/tokenization_m2m_100.py
src/transformers/models/marian/configuration_marian.py
src/transformers/models/marian/modeling_flax_marian.py
src/transformers/models/marian/modeling_marian.py
src/transformers/models/marian/modeling_tf_marian.py
src/transformers/models/marian/tokenization_marian.py
src/transformers/models/mbart/configuration_mbart.py
src/transformers/models/mbart/modeling_flax_mbart.py
src/transformers/models/mbart/tokenization_mbart.py
src/transformers/models/mbart/tokenization_mbart_fast.py
src/transformers/models/mbart50/tokenization_mbart50.py
src/transformers/models/mbart50/tokenization_mbart50_fast.py
src/transformers/models/megatron_bert/configuration_megatron_bert.py
src/transformers/models/mluke/tokenization_mluke.py
src/transformers/models/mmbt/configuration_mmbt.py
src/transformers/models/mmbt/modeling_mmbt.py
src/transformers/models/mobilebert/configuration_mobilebert.py
src/transformers/models/mobilebert/modeling_tf_mobilebert.py
src/transformers/models/mobilebert/tokenization_mobilebert.py
src/transformers/models/mobilebert/tokenization_mobilebert_fast.py
src/transformers/models/mpnet/configuration_mpnet.py
src/transformers/models/mpnet/tokenization_mpnet.py
src/transformers/models/mpnet/tokenization_mpnet_fast.py
src/transformers/models/mt5/configuration_mt5.py
src/transformers/models/openai/configuration_openai.py
src/transformers/models/openai/tokenization_openai.py
src/transformers/models/openai/tokenization_openai_fast.py
src/transformers/models/pegasus/configuration_pegasus.py
src/transformers/models/pegasus/modeling_flax_pegasus.py
src/transformers/models/pegasus/modeling_pegasus.py
src/transformers/models/pegasus/tokenization_pegasus.py
src/transformers/models/pegasus/tokenization_pegasus_fast.py
src/transformers/models/perceiver/configuration_perceiver.py
src/transformers/models/perceiver/feature_extraction_perceiver.py
src/transformers/models/perceiver/modeling_perceiver.py
src/transformers/models/perceiver/tokenization_perceiver.py
src/transformers/models/phobert/tokenization_phobert.py
src/transformers/models/prophetnet/configuration_prophetnet.py
src/transformers/models/prophetnet/modeling_prophetnet.py
src/transformers/models/prophetnet/tokenization_prophetnet.py
src/transformers/models/qdqbert/configuration_qdqbert.py
src/transformers/models/rag/configuration_rag.py
src/transformers/models/rag/modeling_rag.py
src/transformers/models/rag/modeling_tf_rag.py
src/transformers/models/rag/retrieval_rag.py
src/transformers/models/reformer/configuration_reformer.py
src/transformers/models/reformer/tokenization_reformer.py
src/transformers/models/reformer/tokenization_reformer_fast.py
src/transformers/models/rembert/configuration_rembert.py
src/transformers/models/rembert/tokenization_rembert.py
src/transformers/models/rembert/tokenization_rembert_fast.py
src/transformers/models/retribert/configuration_retribert.py
src/transformers/models/retribert/tokenization_retribert.py
src/transformers/models/retribert/tokenization_retribert_fast.py
src/transformers/models/roberta/configuration_roberta.py
src/transformers/models/roberta/tokenization_roberta.py
src/transformers/models/roberta/tokenization_roberta_fast.py
src/transformers/models/roformer/configuration_roformer.py
src/transformers/models/roformer/tokenization_roformer.py
src/transformers/models/roformer/tokenization_roformer_fast.py
src/transformers/models/segformer/configuration_segformer.py
src/transformers/models/segformer/feature_extraction_segformer.py
src/transformers/models/segformer/modeling_segformer.py
src/transformers/models/sew/configuration_sew.py
src/transformers/models/sew_d/configuration_sew_d.py
src/transformers/models/speech_encoder_decoder/configuration_speech_encoder_decoder.py
src/transformers/models/speech_encoder_decoder/modeling_speech_encoder_decoder.py
src/transformers/models/speech_to_text/configuration_speech_to_text.py
src/transformers/models/speech_to_text/feature_extraction_speech_to_text.py
src/transformers/models/speech_to_text/processing_speech_to_text.py
src/transformers/models/speech_to_text/tokenization_speech_to_text.py
src/transformers/models/speech_to_text_2/configuration_speech_to_text_2.py
src/transformers/models/speech_to_text_2/processing_speech_to_text_2.py
src/transformers/models/speech_to_text_2/tokenization_speech_to_text_2.py
src/transformers/models/splinter/configuration_splinter.py
src/transformers/models/splinter/tokenization_splinter.py
src/transformers/models/splinter/tokenization_splinter_fast.py
src/transformers/models/squeezebert/configuration_squeezebert.py
src/transformers/models/squeezebert/tokenization_squeezebert.py
src/transformers/models/squeezebert/tokenization_squeezebert_fast.py
src/transformers/models/t5/configuration_t5.py
src/transformers/models/t5/modeling_flax_t5.py
src/transformers/models/t5/modeling_t5.py
src/transformers/models/t5/modeling_tf_t5.py
src/transformers/models/t5/tokenization_t5.py
src/transformers/models/t5/tokenization_t5_fast.py
src/transformers/models/tapas/configuration_tapas.py
src/transformers/models/tapas/modeling_tapas.py
src/transformers/models/tapas/modeling_tf_tapas.py
src/transformers/models/tapas/tokenization_tapas.py
src/transformers/models/transfo_xl/configuration_transfo_xl.py
src/transformers/models/transfo_xl/modeling_transfo_xl_utilities.py
src/transformers/models/transfo_xl/tokenization_transfo_xl.py
src/transformers/models/trocr/configuration_trocr.py
src/transformers/models/trocr/processing_trocr.py
src/transformers/models/unispeech/configuration_unispeech.py
src/transformers/models/unispeech_sat/configuration_unispeech_sat.py
src/transformers/models/unispeech_sat/modeling_unispeech_sat.py
src/transformers/models/vision_encoder_decoder/configuration_vision_encoder_decoder.py
src/transformers/models/vision_encoder_decoder/modeling_flax_vision_encoder_decoder.py
src/transformers/models/vision_encoder_decoder/modeling_vision_encoder_decoder.py
src/transformers/models/vision_text_dual_encoder/configuration_vision_text_dual_encoder.py
src/transformers/models/vision_text_dual_encoder/modeling_flax_vision_text_dual_encoder.py
src/transformers/models/vision_text_dual_encoder/modeling_vision_text_dual_encoder.py
src/transformers/models/vision_text_dual_encoder/processing_vision_text_dual_encoder.py
src/transformers/models/visual_bert/configuration_visual_bert.py
src/transformers/models/visual_bert/modeling_visual_bert.py
src/transformers/models/vit/configuration_vit.py
src/transformers/models/vit/feature_extraction_vit.py
src/transformers/models/vit/modeling_flax_vit.py
src/transformers/models/vit/modeling_tf_vit.py
src/transformers/models/vit/modeling_vit.py
src/transformers/models/wav2vec2/configuration_wav2vec2.py
src/transformers/models/wav2vec2/feature_extraction_wav2vec2.py
src/transformers/models/wav2vec2/modeling_flax_wav2vec2.py
src/transformers/models/wav2vec2/modeling_tf_wav2vec2.py
src/transformers/models/wav2vec2/processing_wav2vec2.py
src/transformers/models/wav2vec2/tokenization_wav2vec2.py
src/transformers/models/wav2vec2_phoneme/tokenization_wav2vec2_phoneme.py
src/transformers/models/wav2vec2_with_lm/processing_wav2vec2_with_lm.py
src/transformers/models/wavlm/configuration_wavlm.py
src/transformers/models/xlm/configuration_xlm.py
src/transformers/models/xlm/tokenization_xlm.py
src/transformers/models/xlm_prophetnet/configuration_xlm_prophetnet.py
src/transformers/models/xlm_prophetnet/tokenization_xlm_prophetnet.py
src/transformers/models/xlm_roberta/configuration_xlm_roberta.py
src/transformers/models/xlm_roberta/tokenization_xlm_roberta.py
src/transformers/models/xlm_roberta/tokenization_xlm_roberta_fast.py
src/transformers/models/xlnet/configuration_xlnet.py
src/transformers/models/xlnet/modeling_tf_xlnet.py
src/transformers/models/xlnet/tokenization_xlnet.py
src/transformers/models/xlnet/tokenization_xlnet_fast.py
src/transformers/optimization.py
src/transformers/optimization_tf.py
src/transformers/pipelines/__init__.py
src/transformers/pipelines/audio_classification.py
src/transformers/pipelines/automatic_speech_recognition.py
src/transformers/pipelines/base.py
src/transformers/pipelines/conversational.py
src/transformers/pipelines/feature_extraction.py
src/transformers/pipelines/fill_mask.py
src/transformers/pipelines/image_classification.py
src/transformers/pipelines/image_segmentation.py
src/transformers/pipelines/object_detection.py
src/transformers/pipelines/question_answering.py
src/transformers/pipelines/table_question_answering.py
src/transformers/pipelines/text2text_generation.py
src/transformers/pipelines/text_classification.py
src/transformers/pipelines/text_generation.py
src/transformers/pipelines/token_classification.py
src/transformers/pipelines/zero_shot_classification.py
src/transformers/testing_utils.py
src/transformers/tokenization_utils.py
src/transformers/tokenization_utils_base.py
src/transformers/tokenization_utils_fast.py
src/transformers/trainer.py
src/transformers/trainer_callback.py
src/transformers/trainer_pt_utils.py
src/transformers/trainer_seq2seq.py
src/transformers/trainer_tf.py
src/transformers/trainer_utils.py
src/transformers/training_args.py
src/transformers/training_args_seq2seq.py
src/transformers/training_args_tf.py
src/transformers/utils/fx.py
src/transformers/utils/logging.py
src/transformers/utils/notebook.py
src/transformers/utils/versions.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/configuration_{{cookiecutter.lowercase_modelname}}.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/tokenization_fast_{{cookiecutter.lowercase_modelname}}.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/tokenization_{{cookiecutter.lowercase_modelname}}.py
utils/check_repo.py
==================
185876392;Stas Bekman;2021-12-21 09:55:25 -0800;[doc porting] several docs (#14858)
* [doc porting] 2 docs

* [doc porting] 2 docs

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update docs/source/main_classes/deepspeed.mdx

* cleanup

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

docs/source/debugging.mdx
docs/source/debugging.rst
docs/source/main_classes/deepspeed.mdx
docs/source/main_classes/deepspeed.rst
docs/source/testing.mdx
docs/source/testing.rst
==================
033c3ed95;Stas Bekman;2021-12-21 09:17:28 -0800;[examples/summarization] deal with None in data records (#14816)
* [examples/summarization] deal with None in data records

* rewrite to use a simpler (slower) variant
==

examples/pytorch/summarization/run_summarization.py
==================
c075fb785;Sylvain Gugger;2021-12-21 11:17:11 -0500;Replace commit sha by commit url for update jobs (#14852)
* Replace commit sha by commit url for update jobs

* Typo

* Update .github/workflows/build_documentation.yml

Co-authored-by: Julien Chaumond <julien@huggingface.co>

* Apply review comments

Co-authored-by: Julien Chaumond <julien@huggingface.co>
==

.github/workflows/build_documentation.yml
utils/update_metadata.py
==================
5722d0583;Leandro von Werra;2021-12-21 16:47:41 +0100;Add custom `stopping_criteria` and  `logits_processor` to `generate` (#14779)
* add custom `stopping_criteria` and `logits_processor` to `generate`

* add tests for custom `stopping_criteria` and `logits_processor`

* fix typo in RAG

* address reviewer comments

* improve custom logits processor/stopping criteria error message

* fix types in merge function signature

* change default for custom list from `None` to empty list

* fix rag generate

* add string split suggestion

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
==

src/transformers/generation_utils.py
src/transformers/models/rag/modeling_rag.py
tests/test_generation_utils.py
==================
006205839;Zed;2021-12-21 22:44:09 +0800;Fix the value error typo of AdamW's betas' valid values checking (#14780)
* Fix the value error typo of AdamW's betas value check

* error fixed
==

src/transformers/optimization.py
==================
7ae6f0700;Patrick von Platen;2021-12-21 13:12:22 +0100;[ASR example] Improve example + add more examples (#14848)
* up

* load up

* up
==

examples/pytorch/speech-recognition/README.md
examples/pytorch/speech-recognition/run_speech_recognition_ctc.py
==================
97ec17f73;Sylvain Gugger;2021-12-21 06:34:47 -0500;Only create the model card on process 0 (#14857)

==

src/transformers/trainer.py
==================
b513ec8bb;Patrick von Platen;2021-12-21 11:57:42 +0100;[Bart] better error message (#14854)

==

src/transformers/models/bart/modeling_bart.py
src/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py
==================
7af80f661;Sylvain Gugger;2021-12-21 05:37:32 -0500;Convert docstrings of modeling files (#14850)
* Convert file_utils docstrings to Markdown

* Test on BERT

* Return block indent

* Temporarily disable doc styler

* Remove from quality checks as well

* Remove doc styler mess

* Remove check from circleCI

* Fix typo

* Convert file_utils docstrings to Markdown

* Test on BERT

* Return block indent

* Temporarily disable doc styler

* Remove from quality checks as well

* Remove doc styler mess

* Remove check from circleCI

* Fix typo

* Let's go on all other model files

* Add templates too

* Styling and quality
==

.circleci/config.yml
Makefile
src/transformers/file_utils.py
src/transformers/modeling_flax_outputs.py
src/transformers/modeling_outputs.py
src/transformers/modeling_tf_outputs.py
src/transformers/models/albert/modeling_albert.py
src/transformers/models/albert/modeling_flax_albert.py
src/transformers/models/albert/modeling_tf_albert.py
src/transformers/models/bart/modeling_bart.py
src/transformers/models/bart/modeling_flax_bart.py
src/transformers/models/bart/modeling_tf_bart.py
src/transformers/models/beit/modeling_beit.py
src/transformers/models/beit/modeling_flax_beit.py
src/transformers/models/bert/modeling_bert.py
src/transformers/models/bert/modeling_flax_bert.py
src/transformers/models/bert/modeling_tf_bert.py
src/transformers/models/bert_generation/modeling_bert_generation.py
src/transformers/models/big_bird/modeling_big_bird.py
src/transformers/models/big_bird/modeling_flax_big_bird.py
src/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py
src/transformers/models/blenderbot/modeling_blenderbot.py
src/transformers/models/blenderbot/modeling_flax_blenderbot.py
src/transformers/models/blenderbot/modeling_tf_blenderbot.py
src/transformers/models/blenderbot_small/modeling_blenderbot_small.py
src/transformers/models/blenderbot_small/modeling_flax_blenderbot_small.py
src/transformers/models/blenderbot_small/modeling_tf_blenderbot_small.py
src/transformers/models/camembert/modeling_camembert.py
src/transformers/models/camembert/modeling_tf_camembert.py
src/transformers/models/canine/modeling_canine.py
src/transformers/models/clip/modeling_clip.py
src/transformers/models/clip/modeling_flax_clip.py
src/transformers/models/convbert/modeling_convbert.py
src/transformers/models/convbert/modeling_tf_convbert.py
src/transformers/models/ctrl/modeling_ctrl.py
src/transformers/models/ctrl/modeling_tf_ctrl.py
src/transformers/models/deberta/modeling_deberta.py
src/transformers/models/deberta/modeling_tf_deberta.py
src/transformers/models/deberta_v2/modeling_deberta_v2.py
src/transformers/models/deberta_v2/modeling_tf_deberta_v2.py
src/transformers/models/deit/modeling_deit.py
src/transformers/models/detr/modeling_detr.py
src/transformers/models/distilbert/modeling_distilbert.py
src/transformers/models/distilbert/modeling_flax_distilbert.py
src/transformers/models/distilbert/modeling_tf_distilbert.py
src/transformers/models/dpr/modeling_dpr.py
src/transformers/models/dpr/modeling_tf_dpr.py
src/transformers/models/electra/modeling_electra.py
src/transformers/models/electra/modeling_flax_electra.py
src/transformers/models/electra/modeling_tf_electra.py
src/transformers/models/encoder_decoder/modeling_encoder_decoder.py
src/transformers/models/encoder_decoder/modeling_flax_encoder_decoder.py
src/transformers/models/encoder_decoder/modeling_tf_encoder_decoder.py
src/transformers/models/flaubert/modeling_flaubert.py
src/transformers/models/flaubert/modeling_tf_flaubert.py
src/transformers/models/fnet/modeling_fnet.py
src/transformers/models/fsmt/modeling_fsmt.py
src/transformers/models/funnel/modeling_funnel.py
src/transformers/models/funnel/modeling_tf_funnel.py
src/transformers/models/gpt2/modeling_flax_gpt2.py
src/transformers/models/gpt2/modeling_gpt2.py
src/transformers/models/gpt2/modeling_tf_gpt2.py
src/transformers/models/gpt_neo/modeling_flax_gpt_neo.py
src/transformers/models/gpt_neo/modeling_gpt_neo.py
src/transformers/models/gptj/modeling_flax_gptj.py
src/transformers/models/gptj/modeling_gptj.py
src/transformers/models/hubert/modeling_hubert.py
src/transformers/models/hubert/modeling_tf_hubert.py
src/transformers/models/ibert/modeling_ibert.py
src/transformers/models/imagegpt/modeling_imagegpt.py
src/transformers/models/layoutlm/modeling_layoutlm.py
src/transformers/models/layoutlm/modeling_tf_layoutlm.py
src/transformers/models/layoutlmv2/modeling_layoutlmv2.py
src/transformers/models/led/modeling_led.py
src/transformers/models/led/modeling_tf_led.py
src/transformers/models/longformer/modeling_longformer.py
src/transformers/models/longformer/modeling_tf_longformer.py
src/transformers/models/luke/modeling_luke.py
src/transformers/models/lxmert/modeling_lxmert.py
src/transformers/models/lxmert/modeling_tf_lxmert.py
src/transformers/models/m2m_100/modeling_m2m_100.py
src/transformers/models/marian/modeling_flax_marian.py
src/transformers/models/marian/modeling_marian.py
src/transformers/models/marian/modeling_tf_marian.py
src/transformers/models/mbart/modeling_flax_mbart.py
src/transformers/models/mbart/modeling_mbart.py
src/transformers/models/mbart/modeling_tf_mbart.py
src/transformers/models/megatron_bert/modeling_megatron_bert.py
src/transformers/models/mmbt/modeling_mmbt.py
src/transformers/models/mobilebert/modeling_mobilebert.py
src/transformers/models/mobilebert/modeling_tf_mobilebert.py
src/transformers/models/mpnet/modeling_mpnet.py
src/transformers/models/mpnet/modeling_tf_mpnet.py
src/transformers/models/mt5/modeling_flax_mt5.py
src/transformers/models/mt5/modeling_mt5.py
src/transformers/models/mt5/modeling_tf_mt5.py
src/transformers/models/openai/modeling_openai.py
src/transformers/models/openai/modeling_tf_openai.py
src/transformers/models/pegasus/modeling_flax_pegasus.py
src/transformers/models/pegasus/modeling_pegasus.py
src/transformers/models/pegasus/modeling_tf_pegasus.py
src/transformers/models/perceiver/modeling_perceiver.py
src/transformers/models/prophetnet/modeling_prophetnet.py
src/transformers/models/qdqbert/modeling_qdqbert.py
src/transformers/models/rag/modeling_rag.py
src/transformers/models/rag/modeling_tf_rag.py
src/transformers/models/reformer/modeling_reformer.py
src/transformers/models/rembert/modeling_rembert.py
src/transformers/models/rembert/modeling_tf_rembert.py
src/transformers/models/retribert/modeling_retribert.py
src/transformers/models/roberta/modeling_flax_roberta.py
src/transformers/models/roberta/modeling_roberta.py
src/transformers/models/roberta/modeling_tf_roberta.py
src/transformers/models/roformer/modeling_roformer.py
src/transformers/models/roformer/modeling_tf_roformer.py
src/transformers/models/segformer/modeling_segformer.py
src/transformers/models/sew/modeling_sew.py
src/transformers/models/sew_d/modeling_sew_d.py
src/transformers/models/speech_encoder_decoder/modeling_speech_encoder_decoder.py
src/transformers/models/speech_to_text/modeling_speech_to_text.py
src/transformers/models/speech_to_text_2/modeling_speech_to_text_2.py
src/transformers/models/splinter/modeling_splinter.py
src/transformers/models/squeezebert/modeling_squeezebert.py
src/transformers/models/t5/modeling_flax_t5.py
src/transformers/models/t5/modeling_t5.py
src/transformers/models/t5/modeling_tf_t5.py
src/transformers/models/tapas/modeling_tapas.py
src/transformers/models/tapas/modeling_tf_tapas.py
src/transformers/models/transfo_xl/modeling_tf_transfo_xl.py
src/transformers/models/transfo_xl/modeling_transfo_xl.py
src/transformers/models/trocr/modeling_trocr.py
src/transformers/models/unispeech/modeling_unispeech.py
src/transformers/models/unispeech_sat/modeling_unispeech_sat.py
src/transformers/models/vision_encoder_decoder/modeling_flax_vision_encoder_decoder.py
src/transformers/models/vision_encoder_decoder/modeling_vision_encoder_decoder.py
src/transformers/models/vision_text_dual_encoder/modeling_flax_vision_text_dual_encoder.py
src/transformers/models/vision_text_dual_encoder/modeling_vision_text_dual_encoder.py
src/transformers/models/visual_bert/modeling_visual_bert.py
src/transformers/models/vit/modeling_flax_vit.py
src/transformers/models/vit/modeling_tf_vit.py
src/transformers/models/vit/modeling_vit.py
src/transformers/models/wav2vec2/modeling_flax_wav2vec2.py
src/transformers/models/wav2vec2/modeling_tf_wav2vec2.py
src/transformers/models/wav2vec2/modeling_wav2vec2.py
src/transformers/models/wavlm/modeling_wavlm.py
src/transformers/models/xlm/modeling_tf_xlm.py
src/transformers/models/xlm/modeling_xlm.py
src/transformers/models/xlm_prophetnet/modeling_xlm_prophetnet.py
src/transformers/models/xlm_roberta/modeling_tf_xlm_roberta.py
src/transformers/models/xlm_roberta/modeling_xlm_roberta.py
src/transformers/models/xlnet/modeling_tf_xlnet.py
src/transformers/models/xlnet/modeling_xlnet.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_flax_{{cookiecutter.lowercase_modelname}}.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_tf_{{cookiecutter.lowercase_modelname}}.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_{{cookiecutter.lowercase_modelname}}.py
==================
2a3373460;Sylvain Gugger;2021-12-21 03:11:25 -0500;Make the onnx submodule init lazy (#14855)
* Use lazy init for onnx submodule

* Remove debug statements
==

src/transformers/onnx/__init__.py
==================
b6ec95697;Stas Bekman;2021-12-20 20:48:38 -0800;[logging] implement warning_advice / TRANSFORMERS_NO_ADVISORY_WARNINGS (#14669)
* [logging] implement warning_advice / TRANSFORMERS_NO_ADVISORY_WARNINGS

* reword
==

docs/source/main_classes/logging.rst
src/transformers/tokenization_utils_base.py
src/transformers/utils/logging.py
tests/test_logging.py
==================
c1125dc2b;Stas Bekman;2021-12-20 09:20:21 -0800;[doc] typo (#14849)
fix small typo
==

docs/source/main_classes/deepspeed.rst
==================
33f36c869;Sylvain Gugger;2021-12-20 11:19:08 -0500;Add a main_input_name attribute to all models (#14803)
* Add a main_input_name attribute to all models

* Fix tests

* Wtf Vs Code?

* Update src/transformers/models/imagegpt/modeling_imagegpt.py

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* Style

* Fix copies

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
==

src/transformers/modeling_flax_utils.py
src/transformers/modeling_tf_utils.py
src/transformers/modeling_utils.py
src/transformers/models/beit/modeling_beit.py
src/transformers/models/beit/modeling_flax_beit.py
src/transformers/models/clip/modeling_clip.py
src/transformers/models/clip/modeling_flax_clip.py
src/transformers/models/deit/modeling_deit.py
src/transformers/models/detr/modeling_detr.py
src/transformers/models/hubert/modeling_hubert.py
src/transformers/models/hubert/modeling_tf_hubert.py
src/transformers/models/imagegpt/modeling_imagegpt.py
src/transformers/models/perceiver/modeling_perceiver.py
src/transformers/models/segformer/modeling_segformer.py
src/transformers/models/sew/modeling_sew.py
src/transformers/models/sew_d/modeling_sew_d.py
src/transformers/models/speech_encoder_decoder/modeling_speech_encoder_decoder.py
src/transformers/models/speech_to_text/modeling_speech_to_text.py
src/transformers/models/unispeech/modeling_unispeech.py
src/transformers/models/unispeech_sat/modeling_unispeech_sat.py
src/transformers/models/vision_encoder_decoder/modeling_flax_vision_encoder_decoder.py
src/transformers/models/vision_encoder_decoder/modeling_vision_encoder_decoder.py
src/transformers/models/vit/modeling_flax_vit.py
src/transformers/models/vit/modeling_tf_vit.py
src/transformers/models/vit/modeling_vit.py
src/transformers/models/wav2vec2/modeling_flax_wav2vec2.py
src/transformers/models/wav2vec2/modeling_tf_wav2vec2.py
src/transformers/models/wav2vec2/modeling_wav2vec2.py
src/transformers/models/wavlm/modeling_wavlm.py
tests/test_modeling_common.py
tests/test_modeling_flax_common.py
tests/test_modeling_tf_common.py
==================
0940e9b24;Henrik Holm;2021-12-20 15:28:17 +0100;Add 'with torch.no_grad()' to integration test forward pass (#14820)

==

tests/test_modeling_bert.py
==================
b37cf7dee;Henrik Holm;2021-12-20 15:25:34 +0100;Add 'with torch.no_grad()' to integration test forward pass (#14821)

==

tests/test_modeling_deberta.py
==================
952a77b05;Patrick von Platen;2021-12-20 15:22:50 +0100;[Perceiver] Skip multi-gpu tests for now (#14813)
* [Perceiver] Skip multi-gpu tests for now

* Update tests/test_modeling_perceiver.py

* up

* up
==

docs/source/model_doc/perceiver.mdx
docs/source/model_doc/reformer.rst
docs/source/model_doc/transformerxl.rst
src/transformers/models/perceiver/modeling_perceiver.py
tests/test_modeling_perceiver.py
tests/test_modeling_reformer.py
tests/test_modeling_transfo_xl.py
==================
8a818c26c;Derek Chia;2021-12-20 22:08:05 +0800;Fix dead link to benchmarks.ipynb (#14842)
Notebook has been updated here https://github.com/huggingface/notebooks/tree/master/examples/benchmark.ipynb
==

docs/source/benchmarks.mdx
==================
1b0ca7d27;Kamal Raj;2021-12-20 19:12:03 +0530;Update CONTRIBUTING.md (#14835)
fix cmd typo
==

CONTRIBUTING.md
==================
1531b3197;Chang Lan;2021-12-20 05:41:40 -0800;Add an argument to set bucket_cap_mb for PyTorch DDP (#14756)
* [trainer] Set bucket_cap_mb for DDP from arguments

* Put find_unused_parameters into kwargs
==

src/transformers/trainer.py
src/transformers/training_args.py
==================
3883e3a75;Anton Lozhkov;2021-12-20 16:40:56 +0300;Add SD and SV heads for WavLM (#14847)
* Add converted heads

* Add dummies
==

docs/source/model_doc/wavlm.rst
src/transformers/__init__.py
src/transformers/models/auto/modeling_auto.py
src/transformers/models/wavlm/__init__.py
src/transformers/models/wavlm/configuration_wavlm.py
src/transformers/models/wavlm/convert_wavlm_original_s3prl_checkpoint_to_pytorch.py
src/transformers/models/wavlm/modeling_wavlm.py
src/transformers/utils/dummy_pt_objects.py
tests/test_modeling_unispeech_sat.py
tests/test_modeling_wavlm.py
==================
cd583bdaa;Patrick von Platen;2021-12-20 12:06:42 +0100;[WavLM] Fix slow tests (#14845)

==

tests/test_modeling_wavlm.py
==================
281e1fba7;Patrick von Platen;2021-12-20 11:47:32 +0100;up (#14829)

==

src/transformers/models/wav2vec2_phoneme/tokenization_wav2vec2_phoneme.py
==================
091693b49;Patrick von Platen;2021-12-20 10:53:48 +0100;[Seq2SeqTrainer] Remove model input name hack (#14802)
* [Seq2SeqTrainer] Remove model input name hack

* Update src/transformers/trainer_seq2seq.py

* make style

* finish
==

src/transformers/trainer_seq2seq.py
==================
84ea427f4;Patrick von Platen;2021-12-17 20:05:22 +0100;[ImageGPT] Deprecate pixel_values input name to input_ids (#14801)
* [ImageGPT] Deprecate pixel_values input name to input_ids

* up

* Apply suggestions from code review

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>

* correct

* finish

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>
==

src/transformers/models/imagegpt/modeling_imagegpt.py
tests/test_generation_utils.py
tests/test_modeling_imagegpt.py
==================
c4a96cecb;Patrick von Platen;2021-12-17 19:56:44 +0100;Wav2Vec2 meets phonemes (#14353)
* up

* add tokenizer

* improve more

* finish tokenizer

* finish

* adapt speech recognition script

* adapt convert

* more fixes

* more fixes

* update phonemizer wav2vec2

* better naming

* fix more tests

* more fixes swedish

* correct tests

* finish

* improve script

* remove file

* up

* lets get those 100 model architectures until the end of the month

* make fix-copies

* correct more

* correct script

* more fixes

* more fixes

* add to docs

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* replace assert

* fix copies

* fix docs

* new try docs

* boom boom

* update

* add phonemizer to audio tests

* make fix-copies

* up

* upload models

* some changes

* Update tests/test_tokenization_wav2vec2_phoneme.py

Co-authored-by: Anton Lozhkov <aglozhkov@gmail.com>

* more fixes

* remove @

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
Co-authored-by: Anton Lozhkov <aglozhkov@gmail.com>
==

.circleci/config.yml
.github/workflows/self-push.yml
.github/workflows/self-scheduled.yml
README.md
README_ko.md
README_zh-hans.md
README_zh-hant.md
docs/source/_toctree.yml
docs/source/index.mdx
docs/source/model_doc/wav2vec2_phoneme.mdx
docs/source/model_doc/xls_r.rst
examples/pytorch/speech-recognition/run_speech_recognition_ctc.py
setup.py
src/transformers/__init__.py
src/transformers/dependency_versions_table.py
src/transformers/file_utils.py
src/transformers/models/__init__.py
src/transformers/models/auto/tokenization_auto.py
src/transformers/models/unispeech/convert_unispeech_original_pytorch_checkpoint_to_pytorch.py
src/transformers/models/wav2vec2/convert_wav2vec2_original_pytorch_checkpoint_to_pytorch.py
src/transformers/models/wav2vec2/processing_wav2vec2.py
src/transformers/models/wav2vec2_phoneme/__init__.py
src/transformers/models/wav2vec2_phoneme/tokenization_wav2vec2_phoneme.py
src/transformers/testing_utils.py
tests/test_modeling_wav2vec2.py
tests/test_tokenization_wav2vec2_phoneme.py
==================
77d6c826d;Lysandre Debut;2021-12-17 11:13:34 -0500;Convert rst to mdx bert (#14806)
* BERT to mdx
mdx :)
c

* Update docs/source/model_doc/bert.mdx

Co-authored-by: Julien Chaumond <julien@huggingface.co>

* Remove all
Co-authored-by: sgugger <sylvain.gugger@gmail.com>

Co-authored-by: Julien Chaumond <julien@huggingface.co>
==

docs/source/model_doc/bert.mdx
docs/source/model_doc/bert.rst
==================
0b4ea79a0;Sylvain Gugger;2021-12-17 11:14:18 -0500;Trigger doc building

==
==================
ff066119c;Daniel Stancl;2021-12-17 17:06:59 +0100;Implement head_mask for Flax BERT and other models copied from BERT (#14620)
* Implement head_mask for Flax BERT and other models copied from BERT

* Remove `from jax._src.nn.functions import sigmoid`

Remove `from jax._src.nn.functions import sigmoid` unintentionally added by IDE

* Remove no more valid copy statement

* Apply patil-suraj's suggestions from code review

* Apply suggestions from the code review

* Update Flax template

* Fix a typo

* Also update template for CausalLM modules
==

src/transformers/models/bert/modeling_flax_bert.py
src/transformers/models/big_bird/modeling_flax_big_bird.py
src/transformers/models/electra/modeling_flax_electra.py
src/transformers/models/roberta/modeling_flax_roberta.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_flax_{{cookiecutter.lowercase_modelname}}.py
tests/test_modeling_flax_bert.py
tests/test_modeling_flax_common.py
tests/test_modeling_flax_electra.py
tests/test_modeling_flax_roberta.py
==================
95119ad7b;Patrick von Platen;2021-12-17 16:08:54 +0100;[Generate] Correct input_ids detection (#14815)
* [Generate] Correct input_ids detection

* correct
==

src/transformers/generation_utils.py
tests/test_generation_utils.py
==================
bdbe3df86;Patrick von Platen;2021-12-17 13:30:18 +0100;[WavLM] Layerdrop is not allowed for first layer (#14811)
* [WavLM] Layerdrop is not allowed for first layer

* Apply suggestions from code review
==

src/transformers/models/wavlm/modeling_wavlm.py
==================
cbf036f7a;NielsRogge;2021-12-17 10:33:27 +0100;Add test (#14810)

==

tests/test_modeling_perceiver.py
==================
c4a0fb519;Patrick von Platen;2021-12-16 22:42:57 +0100;[WavLM] Correct position bias computation (#14805)

==

src/transformers/models/wavlm/modeling_wavlm.py
==================
d194d639a;Lysandre Debut;2021-12-16 14:34:14 -0500;Remove datasets requirement (#14795)

==

src/transformers/testing_utils.py
tests/test_modeling_flax_wav2vec2.py
tests/test_modeling_hubert.py
tests/test_modeling_sew.py
tests/test_modeling_sew_d.py
tests/test_modeling_tf_hubert.py
tests/test_modeling_tf_wav2vec2.py
tests/test_modeling_unispeech.py
tests/test_modeling_unispeech_sat.py
tests/test_modeling_wav2vec2.py
tests/test_pipelines_audio_classification.py
tests/test_pipelines_automatic_speech_recognition.py
tests/test_pipelines_image_classification.py
tests/test_pipelines_image_segmentation.py
tests/test_pipelines_object_detection.py
tests/test_retrieval_rag.py
tests/test_tokenization_rag.py
tests/test_trainer.py
tests/test_trainer_seq2seq.py
==================
bef1e3e4a;Patrick von Platen;2021-12-16 18:57:05 +0100;Add WavLM (#14354)
* first commit

* fix some stuff

* fix more readme

* Apply suggestions from code review

* update

* correct

* up

* attn layer works

* push code

* make modedls work

* Small change

* more refactor

* finish

* up

* fix convertsion

* fix position bias

* Fix style

* fix conversion

* make fix-copies

* add

* clean

* fix docs

* fix

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* apply final changes

* make fix-copies

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

README.md
README_ko.md
README_zh-hans.md
README_zh-hant.md
docs/source/_toctree.yml
docs/source/index.mdx
docs/source/model_doc/wavlm.rst
src/transformers/__init__.py
src/transformers/models/__init__.py
src/transformers/models/auto/configuration_auto.py
src/transformers/models/auto/modeling_auto.py
src/transformers/models/wavlm/__init__.py
src/transformers/models/wavlm/configuration_wavlm.py
src/transformers/models/wavlm/convert_wavlm_original_pytorch_checkpoint_to_pytorch.py
src/transformers/models/wavlm/modeling_wavlm.py
src/transformers/utils/dummy_pt_objects.py
tests/test_modeling_wavlm.py
==================
b18d8534e;Patrick von Platen;2021-12-16 18:03:55 +0100;[Generate] Make generate multi-modal (#14784)
* finish refactor

* refactor

* add tests

* add more tests

* up

* finish tests

* finish

* up

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* improve docstring

* fix docs

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

src/transformers/generation_utils.py
tests/test_generation_utils.py
==================
48463ebb3;Anton Lozhkov;2021-12-16 19:22:14 +0300;Add Speaker Diarization and Verification heads (#14723)
* Models

* Squashed commit of the following:

commit 72278e1e931a16d0879acc77f65762f3364833d0
Author: anton-l <aglozhkov@gmail.com>
Date:   Fri Dec 10 21:45:08 2021 +0300

* Add unispeech heads

* Add sd/sv automodels

* Docs cleanup

* Fix docstrings

* rename xvector classes

* examples

* Tests cleanup

* Style

* Better checkpoints for tests

* leftover docs

* apply review suggestions

* Style + init tests

* Update unispeech-sat tdnn downsampling
==

docs/source/model_doc/auto.rst
docs/source/model_doc/unispeech_sat.rst
docs/source/model_doc/wav2vec2.rst
src/transformers/__init__.py
src/transformers/file_utils.py
src/transformers/models/auto/__init__.py
src/transformers/models/auto/modeling_auto.py
src/transformers/models/hubert/modeling_hubert.py
src/transformers/models/sew/modeling_sew.py
src/transformers/models/sew_d/modeling_sew_d.py
src/transformers/models/unispeech/modeling_unispeech.py
src/transformers/models/unispeech_sat/__init__.py
src/transformers/models/unispeech_sat/configuration_unispeech_sat.py
src/transformers/models/unispeech_sat/convert_unispeech_original_s3prl_checkpoint_to_pytorch.py
src/transformers/models/unispeech_sat/modeling_unispeech_sat.py
src/transformers/models/wav2vec2/__init__.py
src/transformers/models/wav2vec2/configuration_wav2vec2.py
src/transformers/models/wav2vec2/convert_wav2vec2_original_s3prl_checkpoint_to_pytorch.py
src/transformers/models/wav2vec2/modeling_wav2vec2.py
src/transformers/utils/dummy_pt_objects.py
tests/test_modeling_unispeech_sat.py
tests/test_modeling_wav2vec2.py
utils/update_metadata.py
==================
2e07180cb;Matt;2021-12-16 16:08:13 +0000;Train step fix (#14796)
* Fix for TF train step when no "labels" key in input

* make style
==

src/transformers/modeling_tf_utils.py
==================
465a8b8d1;Kamal Raj;2021-12-16 21:10:56 +0530;Update CONTRIBUTING.md (#14800)
fix pip installation cmd
==

CONTRIBUTING.md
==================
8ae24e19b;Kamal Raj;2021-12-16 20:54:26 +0530;Update CONTRIBUTING.md (#14799)
typo
==

CONTRIBUTING.md
==================
12e1b4c6d;Sylvain Gugger;2021-12-16 09:35:20 -0500;Fix the build documentation job (#14788)
* Fix the build documentation job

* Fix install

* Address review comment
==

.github/workflows/build_documentation.yml
==================
5061a9fd5;Sylvain Gugger;2021-12-16 09:29:26 -0500;Post sphinx-clean up and contributing guide updates (#14790)
* Clean up sphinx

* Update contributing guide

* Update docs README

* No example title

* Fix copies

* Update CONTRIBUTING.md

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
==

CONTRIBUTING.md
Makefile
docs/Makefile
docs/README.md
setup.py
src/transformers/dependency_versions_table.py
==================
8010fda9b;Lysandre Debut;2021-12-16 04:42:02 -0500;Removes images to put them in a dataset (#14781)
* First try

* Update instructions
==

CONTRIBUTING.md
README.md
README_ko.md
README_zh-hans.md
README_zh-hant.md
docs/README.md
docs/source/add_new_model.rst
docs/source/imgs/course_banner.png
docs/source/imgs/local_attention_mask.png
docs/source/imgs/parallelism-deepspeed-3d.png
docs/source/imgs/parallelism-flexflow.jpeg
docs/source/imgs/parallelism-gpipe-bubble.png
docs/source/imgs/parallelism-sagemaker-interleaved-pipeline.png
docs/source/imgs/parallelism-tp-independent-gelu.png
docs/source/imgs/parallelism-tp-parallel_gemm.png
docs/source/imgs/parallelism-tp-parallel_self_attention.png
docs/source/imgs/parallelism-tp-parallel_shard_processing.png
docs/source/imgs/parallelism-zero-dp-pp.png
docs/source/imgs/parallelism-zero.png
docs/source/imgs/perf-moe-transformer.png
docs/source/imgs/ppl_chunked.gif
docs/source/imgs/ppl_full.gif
docs/source/imgs/ppl_sliding.gif
docs/source/imgs/tf32-bf16-fp16-fp32.png
docs/source/imgs/transformers_logo_name.png
docs/source/imgs/transformers_overview.png
docs/source/imgs/warmup_constant_schedule.png
docs/source/imgs/warmup_cosine_hard_restarts_schedule.png
docs/source/imgs/warmup_cosine_schedule.png
docs/source/imgs/warmup_cosine_warm_restarts_schedule.png
docs/source/imgs/warmup_linear_schedule.png
docs/source/main_classes/optimizer_schedules.rst
docs/source/model_summary.rst
docs/source/parallelism.md
docs/source/performance.md
docs/source/perplexity.mdx
templates/adding_a_new_model/ADD_NEW_MODEL_PROPOSAL_TEMPLATE.md
templates/adding_a_new_model/open_model_proposals/ADD_BIG_BIRD.md
==================
459677aeb;Sylvain Gugger;2021-12-15 14:40:47 -0500;PoC for conserving old links (#14754)
* PoC for conserving old links

* Do the same for other links

* remap the redirects section

* add instructions on how to move sections

* improve

Co-authored-by: Stas Bekman <stas@stason.org>
==

docs/README.md
docs/source/main_classes/trainer.mdx
==================
c40ecfd74;Sylvain Gugger;2021-12-15 13:34:42 -0500;Move import (#14787)

==

src/transformers/modeling_tf_utils.py
==================
7c9c41f43;Lysandre;2021-12-15 18:29:53 +0100;Docs for v4.14.0

==

examples/flax/question-answering/run_qa.py
examples/flax/token-classification/run_flax_ner.py
examples/pytorch/audio-classification/run_audio_classification.py
examples/pytorch/image-classification/run_image_classification.py
examples/pytorch/language-modeling/run_clm.py
examples/pytorch/language-modeling/run_mlm.py
examples/pytorch/language-modeling/run_plm.py
examples/pytorch/multiple-choice/run_swag.py
examples/pytorch/question-answering/run_qa.py
examples/pytorch/question-answering/run_qa_beam_search.py
examples/pytorch/question-answering/run_qa_beam_search_no_trainer.py
examples/pytorch/question-answering/run_qa_no_trainer.py
examples/pytorch/question-answering/run_seq2seq_qa.py
examples/pytorch/speech-recognition/run_speech_recognition_ctc.py
examples/pytorch/summarization/run_summarization.py
examples/pytorch/text-classification/run_glue.py
examples/pytorch/text-classification/run_xnli.py
examples/pytorch/token-classification/run_ner.py
examples/pytorch/translation/run_translation.py
examples/tensorflow/multiple-choice/run_swag.py
examples/tensorflow/question-answering/run_qa.py
examples/tensorflow/summarization/run_summarization.py
examples/tensorflow/text-classification/run_glue.py
examples/tensorflow/translation/run_translation.py
setup.py
src/transformers/__init__.py
utils/release.py
==================
960d8cb41;Lysandre;2021-12-15 18:20:35 +0100;Release: v4.14.0

==

examples/flax/question-answering/run_qa.py
examples/flax/token-classification/run_flax_ner.py
examples/pytorch/audio-classification/run_audio_classification.py
examples/pytorch/image-classification/run_image_classification.py
examples/pytorch/language-modeling/run_clm.py
examples/pytorch/language-modeling/run_mlm.py
examples/pytorch/language-modeling/run_plm.py
examples/pytorch/multiple-choice/run_swag.py
examples/pytorch/question-answering/run_qa.py
examples/pytorch/question-answering/run_qa_beam_search.py
examples/pytorch/question-answering/run_qa_beam_search_no_trainer.py
examples/pytorch/question-answering/run_qa_no_trainer.py
examples/pytorch/question-answering/run_seq2seq_qa.py
examples/pytorch/speech-recognition/run_speech_recognition_ctc.py
examples/pytorch/summarization/run_summarization.py
examples/pytorch/text-classification/run_glue.py
examples/pytorch/text-classification/run_xnli.py
examples/pytorch/token-classification/run_ner.py
examples/pytorch/translation/run_translation.py
examples/tensorflow/multiple-choice/run_swag.py
examples/tensorflow/question-answering/run_qa.py
examples/tensorflow/summarization/run_summarization.py
examples/tensorflow/text-classification/run_glue.py
examples/tensorflow/translation/run_translation.py
setup.py
src/transformers/__init__.py
utils/release.py
==================
aece7badc;NielsRogge;2021-12-15 18:02:05 +0100;Improve Perceiver docs (#14786)
* Fix docs

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Code quality

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
Co-authored-by: Lysandre <lysandre.debut@reseau.eseo.fr>
==

src/transformers/models/perceiver/modeling_perceiver.py
==================
50bc57cef;NielsRogge;2021-12-15 17:06:38 +0100;Update Perceiver code examples (#14783)
* Fix code examples

* Fix code example
==

docs/source/model_doc/perceiver.mdx
src/transformers/models/perceiver/modeling_perceiver.py
==================
48d482769;Matt;2021-12-15 14:57:52 +0000;TF model cards (#14720)
* Initial commit for Keras model cards

* Revert accidental change

* make style

* make style

* make style

* Fix PR comments

* Move repo creation to __init__

* Fixes to README.md creation

* Partial progress for proper card creation on `push_to_hub`

* Proper card creation from `push_to_hub` plus fixes for malformed model cards

* Fixes for model card creation outside the callback

* Adding a model card creation test

* Putting the model card creation test in the right file.
Good job, Matt.

* make style

* Fix model card test temp dir usage

* Fix model card creation when no optimizer present

* Fixes for when training history not present

* Fix accidental edit to test_modeling_common
==

src/transformers/file_utils.py
src/transformers/keras_callbacks.py
src/transformers/modelcard.py
src/transformers/modeling_tf_utils.py
tests/test_modeling_tf_common.py
==================
72c6e8b8b;Xing Han Lu;2021-12-15 08:59:11 -0500;Update t5.rst (#14776)

==

docs/source/model_doc/t5.rst
==================
a94105f95;Yih-Dar;2021-12-15 11:36:28 +0100;Fix preprocess_function in run_summarization_flax.py  (#14769)
Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>
==

examples/flax/summarization/run_summarization_flax.py
==================
7e61d56a4;Sylvain Gugger;2021-12-15 03:40:17 -0500;Fix the doc_build_test job (#14774)
* Fake new model

* Fix doc-building test job

* Is this the problem?

* Another try

* Typo

* Clean up

* Can we do without -e ?

* Clean setup
==

.github/workflows/build_doc_test.yml
==================
fdf3ce282;Stas Bekman;2021-12-14 19:01:23 -0800;[doc] performance: groups of operations by compute-intensity (#14757)

==

docs/source/performance.md
==================
851a78978;Amit Chaudhary;2021-12-15 03:55:33 +0100;Fix broken links to distillation on index page of documentation (#14722)
* Fix broken links to distillation on index page of documentation

* Fix broken link for distillation in main README

* Run make fixup
==

README.md
docs/source/index.mdx
==================
e7ed7ffdc;Nicolas Patry;2021-12-14 16:46:16 +0100;Adding support for multiple mask tokens. (#14716)
* Adding support for multiple mask tokens.

- Original implem: https://github.com/huggingface/transformers/pull/10222

Co-authored-by: njafer <naveen.jafer@oracle.com>

* In order to accomodate optionally multimodal models like Perceiver

we add information to the tasks to specify tasks where we know for sure
if we need the tokenizer/feature_extractor or not.

* Adding info in the documentation about multi masks.

+ marked as experimental.

* Add a copy() to prevent overriding the same tensor over and over.

* Fixup.

* Adding small test for multi mask with real values..

Co-authored-by: njafer <naveen.jafer@oracle.com>
==

src/transformers/pipelines/__init__.py
src/transformers/pipelines/fill_mask.py
tests/test_pipelines_fill_mask.py
==================
2a606f997;Benjamin Minixhofer;2021-12-14 11:04:43 +0100;Make data shuffling in `run_clm_flax.py` respect global seed (#13410)
* use jax and jnp instead of numpy in data_loader

* return batches as np.ndarray
==

examples/flax/language-modeling/run_clm_flax.py
==================
546a91abe;Nicolas Patry;2021-12-14 09:43:07 +0100;Fixing tests for Perceiver (#14739)
* Adding some slow test to check for perceiver at least from a high level.

* Re-enabling fast tests for Perceiver ImageClassification.

* Perceiver might try to run without Tokenizer (Fast doesn't exist) and
with FeatureExtractor some text only pipelines.

* Oops.

* Adding a comment for `update_config_with_model_class`.

* Remove `model_architecture` to get `tiny_config`.

* Finalize rebase.

* Smarter way to handle undefined FastTokenizer.

* Remove old code.

* Addressing some nits.

* Don't instantiate `None`.
==

src/transformers/models/perceiver/modeling_perceiver.py
src/transformers/pipelines/__init__.py
tests/test_pipelines_common.py
tests/test_pipelines_image_classification.py
==================
322d41691;Sylvain Gugger;2021-12-13 17:15:19 -0500;Update Table of Contents (#14755)

==

docs/source/_toctree.yml
==================
7533d30ac;Sylvain Gugger;2021-12-13 13:09:50 -0500;Convert Trainer doc page to MarkDown (#14753)
* Convert Trainer doc page to MarkDown

* Fix repo consistency

* Fix the doc build test job
==

.github/workflows/build_doc_test.yml
docs/source/main_classes/trainer.mdx
docs/source/main_classes/trainer.rst
utils/check_repo.py
==================
e926ea2bd;NielsRogge;2021-12-13 18:46:49 +0100;Improve perceiver (#14750)
* First draft

* Improve docstring + clean up tests

* Remove unused code

* Add check in case one doesn't provide a preprocessor
==

src/transformers/models/perceiver/configuration_perceiver.py
src/transformers/models/perceiver/modeling_perceiver.py
tests/test_modeling_perceiver.py
==================
971e36667;Josu√© Nascimento;2021-12-13 14:34:26 -0300;Change how to load config of XLNetLMHeadModel (#14746)

==

examples/pytorch/language-modeling/run_plm.py
==================
15a9d0151;Yih-Dar;2021-12-13 18:30:46 +0100;Avoid using tf.tile in embeddings for TF models (#14735)
* avoid tf.tile in embeddings

* remove more tf.tile in embeddings

* clean

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>
==

src/transformers/models/albert/modeling_tf_albert.py
src/transformers/models/bert/modeling_tf_bert.py
src/transformers/models/convbert/modeling_tf_convbert.py
src/transformers/models/deberta/modeling_tf_deberta.py
src/transformers/models/deberta_v2/modeling_tf_deberta_v2.py
src/transformers/models/distilbert/modeling_tf_distilbert.py
src/transformers/models/electra/modeling_tf_electra.py
src/transformers/models/layoutlm/modeling_tf_layoutlm.py
src/transformers/models/longformer/modeling_tf_longformer.py
src/transformers/models/lxmert/modeling_tf_lxmert.py
src/transformers/models/mobilebert/modeling_tf_mobilebert.py
src/transformers/models/mpnet/modeling_tf_mpnet.py
src/transformers/models/rembert/modeling_tf_rembert.py
src/transformers/models/roberta/modeling_tf_roberta.py
src/transformers/models/roformer/modeling_tf_roformer.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_tf_{{cookiecutter.lowercase_modelname}}.py
==================
6ac0fac85;Lysandre Debut;2021-12-13 18:21:26 +0100;Mention no images added to repository (#14738)
* Mention no images added to repository

* Update CONTRIBUTING.md

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>
==

CONTRIBUTING.md
==================
e4666bff0;Sylvain Gugger;2021-12-13 12:01:37 -0500;Fix name

==

.github/workflows/update_metdata.yml
==================
64e92ed22;Sylvain Gugger;2021-12-13 11:46:03 -0500;Update transformers metadata (#14724)
* Wip on metadata update

* Most of the script

* Add a job to auto-update the transformers metadata

* Style
==

.github/workflows/update_metdata.yml
utils/update_metadata.py
==================
c3cd88a9b;Sylvain Gugger;2021-12-13 11:17:01 -0500;Small fixes for the doc (#14751)

==

.github/workflows/build_documentation.yml
docs/source/index.rst
==================
12d9b9572;Yih-Dar;2021-12-13 17:12:58 +0100;Fix: change tooslow to slow (#14734)
Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>
==

tests/test_modeling_tf_vit.py
==================
ca0b82bbd;Yih-Dar;2021-12-13 16:36:50 +0100;Fix doc examples: cannot import name (#14698)
* Fix doc examples: cannot import name

* remove copy because of some necessary minor changes (maybe add copy to the individual methods instead)

* Keep copy with some modifications

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>
==

src/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py
==================
fc74c8453;Lucien;2021-12-13 23:31:11 +0800;Swap TF and PT code inside two blocks (#14742)

==

docs/source/quicktour.mdx
==================
8362d07d6;Stas Bekman;2021-12-13 06:53:48 -0800;[CI/pt-nightly] switch to cuda-11.3 (#14726)

==

.github/workflows/self-nightly-scheduled.yml
==================
6e05bb1c9;Lysandre Debut;2021-12-13 15:29:47 +0100;Fix the perceiver docs (#14748)

==

docs/source/model_doc/perceiver.mdx
==================
c17e7cde3;Suzen Fylke;2021-12-13 08:31:50 -0500;Add ability to get a list of supported pipeline tasks (#14732)

==

src/transformers/commands/run.py
src/transformers/commands/serving.py
src/transformers/pipelines/__init__.py
==================
3d66146af;Lysandre Debut;2021-12-13 14:13:39 +0100;Fixing tests for Perceiver (#14745)
- Do not run image-classification pipeline (_CHECKPOINT_FOR_DOC uses the checkpoint for
langage, which cannot load a FeatureExtractor so current logic fails).
- Add a safeguard to not run tests when `tokenizer_class` or
`feature_extractor_class` **are** defined, but cannot be loaded
This happens for Perceiver for the "FastTokenizer" (which doesn't exist
so None) and FeatureExtractor (which does exist but cannot be loaded
because the checkpoint doesn't define one which is reasonable for the
said checkpoint)
- Added `get_vocab` function to `PerceiverTokenizer` since it is used by
`fill-mask` pipeline when the argument `targets` is used to narrow a
subset of possible values.

Co-authored-by: Nicolas Patry <patry.nicolas@protonmail.com>
==

src/transformers/models/auto/feature_extraction_auto.py
src/transformers/models/perceiver/tokenization_perceiver.py
tests/test_pipelines_common.py
tests/test_pipelines_image_classification.py
==================
4c99e553c;NielsRogge;2021-12-13 13:24:36 +0100;Improve documentation of some models (#14695)
* Migrate docs to mdx

* Update TAPAS docs

* Remove lines

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Apply some more suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Add pt/tf switch to code examples

* More improvements

* Improve docstrings

* More improvements

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

docs/source/model_doc/beit.rst
docs/source/model_doc/imagegpt.mdx
docs/source/model_doc/imagegpt.rst
docs/source/model_doc/luke.rst
docs/source/model_doc/perceiver.mdx
docs/source/model_doc/perceiver.rst
docs/source/model_doc/segformer.rst
docs/source/model_doc/tapas.mdx
docs/source/model_doc/tapas.rst
docs/source/model_doc/trocr.mdx
docs/source/model_doc/trocr.rst
docs/source/model_doc/vit.rst
src/transformers/models/perceiver/configuration_perceiver.py
src/transformers/models/perceiver/modeling_perceiver.py
==================
32eb29fef;Yih-Dar;2021-12-13 12:50:02 +0100;Fix doc examples: modify config before super().__init__ (#14697)
Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>
==

src/transformers/models/bart/modeling_bart.py
src/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py
src/transformers/models/blenderbot/modeling_blenderbot.py
src/transformers/models/blenderbot_small/modeling_blenderbot_small.py
src/transformers/models/marian/modeling_marian.py
src/transformers/models/mbart/modeling_mbart.py
src/transformers/models/pegasus/modeling_pegasus.py
src/transformers/models/speech_to_text_2/modeling_speech_to_text_2.py
src/transformers/models/trocr/modeling_trocr.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_{{cookiecutter.lowercase_modelname}}.py
==================
48bf7e47a;Nathan Cooper;2021-12-13 03:30:50 -0500;Code parrot minor fixes/niceties (#14666)
* Add some nicety flags for better controlling evaluation.

* Fix dependency issue with outdated requirement

* Add additional flag to example to ensure eval is done

* Wrap code into main function for accelerate launcher to find

* Fix valid batch size flag in readme

* Add note to install git-lfs when initializing/training the model

* Update examples/research_projects/codeparrot/scripts/arguments.py

Co-authored-by: Leandro von Werra <lvwerra@users.noreply.github.com>

* Update examples/research_projects/codeparrot/README.md

Co-authored-by: Leandro von Werra <lvwerra@users.noreply.github.com>

* Revert "Wrap code into main function for accelerate launcher to find"

This reverts commit ff11df1c810d4df198d04b827538eb4572147ba3.

* Fix formatting issue

* Move git-lfs instructions to installation section

* Add a quick check before code generation for code evaluation

* Fix styling issue

* Update examples/research_projects/codeparrot/scripts/human_eval.py

Co-authored-by: Leandro von Werra <lvwerra@users.noreply.github.com>

* Make iterable dataset use passed in tokenizer rather than globally defined one

Co-authored-by: Leandro von Werra <lvwerra@users.noreply.github.com>
Co-authored-by: ncoop57 <nac33@students.uwf.edu>
==

examples/research_projects/codeparrot/README.md
examples/research_projects/codeparrot/requirements.txt
examples/research_projects/codeparrot/scripts/arguments.py
examples/research_projects/codeparrot/scripts/codeparrot_training.py
examples/research_projects/codeparrot/scripts/human_eval.py
==================
91f3dfbfd;Patrick von Platen;2021-12-12 13:31:46 +0100;[Adafactor] Fix adafactor (#14713)
* correct changes

* add comment
==

src/transformers/optimization.py
==================
86dd23bb8;Patrick von Platen;2021-12-12 13:30:44 +0100;Update bug-report.md (#14715)

==

.github/ISSUE_TEMPLATE/bug-report.md
==================
6a025487a;Suraj Patil;2021-12-12 09:19:12 +0530;[Flax examples] remove dependancy on pytorch training args (#14636)
* use custom training arguments

* update tests
==

examples/flax/language-modeling/run_clm_flax.py
examples/flax/language-modeling/run_mlm_flax.py
examples/flax/language-modeling/run_t5_mlm_flax.py
examples/flax/question-answering/run_qa.py
examples/flax/summarization/run_summarization_flax.py
examples/flax/test_examples.py
examples/flax/token-classification/run_flax_ner.py
examples/flax/vision/run_image_classification.py
==================
027074f4d;Stas Bekman;2021-12-10 18:24:38 -0800;[doc] document MoE model approach and current solutions (#14725)
* document MoE model approach

* additional info from Samyam

* fix
==

docs/source/imgs/perf-moe-transformer.png
docs/source/performance.md
==================
7cb1fdd4d;Nicolas Patry;2021-12-11 01:38:59 +0100;Fixing tests for perceiver (texts) (#14719)
* Fixing tests for perceiver (texts)

* For MaskedLM
==

src/transformers/models/perceiver/modeling_perceiver.py
tests/test_modeling_perceiver.py
==================
39fbb068b;Sylvain Gugger;2021-12-10 17:55:16 -0500;Empty commit to retrigger build doc

==
==================
5eca742f6;Sylvain Gugger;2021-12-10 16:02:48 -0500;Fix special character in MDX (#14721)

==

docs/source/quicktour.mdx
==================
63c284c2d;Sylvain Gugger;2021-12-10 15:31:43 -0500;Prevent style_doc from tempering the config file

==

docs/source/_config.py
==================
f46668282;Sylvain Gugger;2021-12-10 15:03:17 -0500;Fix path for notebooks

==

.github/workflows/build_documentation.yml
==================
3b2d1652e;Sylvain Gugger;2021-12-10 14:38:21 -0500;Fix typo in branch name

==

.github/workflows/build_documentation.yml
==================
1b75d7238;Sylvain Gugger;2021-12-10 14:20:56 -0500;Automatically build doc notebooks (#14718)
* Test workflow

* Build doc

* Make a clean build

* Add doc config

* Restore other workflows

* Final job

* Print something in else statements

* Pull before making changes
==

.github/workflows/build_documentation.yml
docs/source/_config.py
==================
ae82ee6a4;Yih-Dar;2021-12-10 17:44:08 +0100;Fix doc examples: unexpected keyword argument (#14689)
* Fix doc examples: unexpected keyword argument

* Don't delete token_type_ids from inputs

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>
==

src/transformers/models/unispeech/modeling_unispeech.py
src/transformers/models/unispeech_sat/modeling_unispeech_sat.py
src/transformers/models/wav2vec2/modeling_wav2vec2.py
==================
5b0040019;Nicolas Patry;2021-12-10 15:29:18 +0100;Adding `Perceiver` to `AutoTokenizer`. (#14711)

==

src/transformers/models/auto/tokenization_auto.py
==================
59d684fa9;Yih-Dar;2021-12-10 14:55:54 +0100;Fix examples: 'CausalLMOutputWithCrossAttentions' object has no attribute 'last_hidden_state' (#14678)
Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>
==

src/transformers/models/bart/modeling_bart.py
src/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py
src/transformers/models/blenderbot/modeling_blenderbot.py
src/transformers/models/blenderbot_small/modeling_blenderbot_small.py
src/transformers/models/marian/modeling_marian.py
src/transformers/models/mbart/modeling_mbart.py
src/transformers/models/pegasus/modeling_pegasus.py
src/transformers/models/tapas/modeling_tapas.py
src/transformers/models/tapas/modeling_tf_tapas.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_{{cookiecutter.lowercase_modelname}}.py
==================
8395f14de;Yih-Dar;2021-12-10 08:56:37 +0100;Fix doc examples: KeyError (#14699)
Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>
==

src/transformers/models/blenderbot_small/modeling_blenderbot_small.py
src/transformers/models/blenderbot_small/modeling_tf_blenderbot_small.py
==================
bab155645;Sylvain Gugger;2021-12-09 12:00:06 -0500;Put back open in colab markers (#14684)

==

docs/source/benchmarks.mdx
docs/source/custom_datasets.mdx
docs/source/multilingual.mdx
docs/source/perplexity.mdx
docs/source/preprocessing.mdx
docs/source/quicktour.mdx
docs/source/task_summary.mdx
docs/source/tokenizer_summary.mdx
docs/source/training.mdx
==================
3bc7d70e9;Tikeng Notsawo Pascal Junior;2021-12-09 11:35:22 -0500;Fix : wrong link in the documentation (ConvBERT vs DistilBERT) (#14705)

==

docs/source/model_summary.rst
==================
4701a1a18;Lysandre;2021-12-09 17:21:08 +0100;Patch release script

==

utils/release.py
==================
ab31b3e41;Lysandre;2021-12-09 17:09:23 +0100;Docs for v4.14.0dev0

==

examples/flax/question-answering/run_qa.py
examples/flax/token-classification/run_flax_ner.py
examples/pytorch/audio-classification/run_audio_classification.py
examples/pytorch/image-classification/run_image_classification.py
examples/pytorch/language-modeling/run_clm.py
examples/pytorch/language-modeling/run_mlm.py
examples/pytorch/language-modeling/run_plm.py
examples/pytorch/multiple-choice/run_swag.py
examples/pytorch/question-answering/run_qa.py
examples/pytorch/question-answering/run_qa_beam_search.py
examples/pytorch/question-answering/run_qa_beam_search_no_trainer.py
examples/pytorch/question-answering/run_qa_no_trainer.py
examples/pytorch/question-answering/run_seq2seq_qa.py
examples/pytorch/speech-recognition/run_speech_recognition_ctc.py
examples/pytorch/summarization/run_summarization.py
examples/pytorch/text-classification/run_glue.py
examples/pytorch/text-classification/run_xnli.py
examples/pytorch/token-classification/run_ner.py
examples/pytorch/translation/run_translation.py
examples/tensorflow/multiple-choice/run_swag.py
examples/tensorflow/question-answering/run_qa.py
examples/tensorflow/summarization/run_summarization.py
examples/tensorflow/text-classification/run_glue.py
examples/tensorflow/translation/run_translation.py
setup.py
src/transformers/__init__.py
utils/release.py
==================
4da3a696e;Lysandre;2021-12-09 16:55:21 +0100;Release: v4.13.0

==

examples/flax/question-answering/run_qa.py
examples/flax/token-classification/run_flax_ner.py
examples/pytorch/audio-classification/run_audio_classification.py
examples/pytorch/image-classification/run_image_classification.py
examples/pytorch/language-modeling/run_clm.py
examples/pytorch/language-modeling/run_mlm.py
examples/pytorch/language-modeling/run_plm.py
examples/pytorch/multiple-choice/run_swag.py
examples/pytorch/question-answering/run_qa.py
examples/pytorch/question-answering/run_qa_beam_search.py
examples/pytorch/question-answering/run_qa_beam_search_no_trainer.py
examples/pytorch/question-answering/run_qa_no_trainer.py
examples/pytorch/question-answering/run_seq2seq_qa.py
examples/pytorch/speech-recognition/run_speech_recognition_ctc.py
examples/pytorch/summarization/run_summarization.py
examples/pytorch/text-classification/run_glue.py
examples/pytorch/text-classification/run_xnli.py
examples/pytorch/token-classification/run_ner.py
examples/pytorch/translation/run_translation.py
examples/tensorflow/multiple-choice/run_swag.py
examples/tensorflow/question-answering/run_qa.py
examples/tensorflow/summarization/run_summarization.py
examples/tensorflow/text-classification/run_glue.py
examples/tensorflow/translation/run_translation.py
setup.py
src/transformers/__init__.py
==================
60be4bf8a;Mishig Davaadorj;2021-12-09 15:25:31 +0100;Fix typo in toctree (#14704)

==

docs/source/_toctree.yml
==================
da7aabf2c;Philipp Schmid;2021-12-09 14:42:23 +0100;add str hub token to repository when provided else fallback to default (#14682)
* add str hub token to repository when provided else fallback to default True

* make style
==

src/transformers/keras_callbacks.py
==================
7375758be;NielsRogge;2021-12-09 14:32:35 +0100;Fix tests (#14703)

==

tests/test_modeling_perceiver.py
tests/test_tokenization_perceiver.py
==================
68e53e6fc;Sylvain Gugger;2021-12-09 07:01:03 -0500;Add a job to test doc building (for realsies this time) (#14662)

==

.github/workflows/build_doc_test.yml
==================
e9800122a;Sylvain Gugger;2021-12-08 19:59:44 -0500;Add kenlm dep to missing tests

==

.circleci/config.yml
==================
ee6674d45;Yih-Dar;2021-12-08 22:39:35 +0100;Fix doc examples: name '...' is not defined (#14687)
* Fix doc examples: name '...' is not defined

* remove >>> and ... in some docstrings in visual_bert

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>
==

src/transformers/models/longformer/modeling_longformer.py
src/transformers/models/megatron_bert/modeling_megatron_bert.py
src/transformers/models/visual_bert/modeling_visual_bert.py
src/transformers/models/wav2vec2/modeling_wav2vec2.py
==================
e6219320b;Sylvain Gugger;2021-12-08 15:59:57 -0500;Make MLuke tokenizer tests slow (#14690)

==

tests/test_tokenization_mluke.py
==================
13186d715;Sylvain Gugger;2021-12-08 15:41:58 -0500;Move pyctcdecode (#14686)
* Move pyctcdecode dep

* Fix doc and last objects

* Quality

* Style

* Ignore this black
==

docs/source/model_doc/wav2vec2.rst
src/transformers/__init__.py
src/transformers/models/wav2vec2_with_lm/__init__.py
src/transformers/models/wav2vec2_with_lm/processing_wav2vec2_with_lm.py
src/transformers/utils/dummy_pyctcdecode_objects.py
==================
d104dd46d;Stas Bekman;2021-12-08 12:21:43 -0800;[trainer] support UserDict inputs (torch-nightly) (#14688)

==

src/transformers/trainer.py
==================
122866128;Stas Bekman;2021-12-08 11:33:24 -0800;[bf16 support] tweaks (#14580)
* [bf16 support] tweaks

* corrections

Co-authored-by: Manuel R. Ciosici <manuelrciosici@gmail.com>
==

docs/source/performance.md
src/transformers/training_args.py
==================
16870d114;Yih-Dar;2021-12-08 20:25:48 +0100;Fix wrong checkpoint paths in doc examples (#14685)
Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>
==

src/transformers/models/big_bird/modeling_big_bird.py
src/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py
src/transformers/models/rembert/modeling_rembert.py
src/transformers/models/segformer/modeling_segformer.py
==================
01b8cd593;Sylvain Gugger;2021-12-08 13:52:31 -0500;Revert open-in-colab and add perceiver (#14683)

==

docs/source/_toctree.yml
docs/source/benchmarks.mdx
docs/source/custom_datasets.mdx
docs/source/multilingual.mdx
docs/source/perplexity.mdx
docs/source/preprocessing.mdx
docs/source/quicktour.mdx
docs/source/task_summary.mdx
docs/source/tokenizer_summary.mdx
docs/source/training.mdx
==================
f6b87c5f3;Sylvain Gugger;2021-12-08 13:42:22 -0500;Fixes in init (#14681)
* Fixes in init

* Style
==

src/transformers/__init__.py
src/transformers/models/wav2vec2_with_lm/__init__.py
utils/check_inits.py
==================
fe06f8dca;Dhruv Nair;2021-12-09 00:09:10 +0530;Improvements to Comet Integration (#14680)
* change args to address overwriting issue

* remove project name from args

* remove passing args as kwargs to experiment object

* remove passing args as kwargs to offline experiment

* fix offline directory assignment in experiment kwargs

* log checkpoint folder on training end

* log entire output_dir as asset folder

* log asset folder  recursively

* end experiment at the end of training

* clean up

* clean up

* Default to always log training assets to Comet when using CometCallback

* change logging training assets to be true when running callback setup

* fix so that experiment always ends when training ends

* styling and quality fixes

* update docstring for COMET_LOG_ASSETS environment variable

* run styling and quality checks

* clean up to docstring

* remove merge markers

* change asset logging to false to avoid hitting max assets per experiment limit

* update training asset description

* fix styling
==

src/transformers/integrations.py
==================
4ea19de80;Gaurang Tandon;2021-12-08 18:25:30 +0000; fix: verify jsonlines file in run_translation (#14660) (#14661)
* fix: verify jsonl in run_translation (#14660)

* fix(run_translation.py): json/jsonl validation

Both json and jsonl are to be accepted as valid jsonlines file extension

* fix(run_translation.py): make black happy

* Ran make style
==

examples/pytorch/translation/run_translation.py
==================
cf36f4d7a;Sylvain Gugger;2021-12-08 13:19:46 -0500;Convert tutorials (#14665)
* Convert a few docs

* And another

* Last tutorials

* New syntax for colab links

* Convert a few docs

* And another

* Last tutorials

* New syntax for colab links
==

docs/source/benchmarks.mdx
docs/source/benchmarks.rst
docs/source/custom_datasets.mdx
docs/source/custom_datasets.rst
docs/source/multilingual.mdx
docs/source/multilingual.rst
docs/source/perplexity.mdx
docs/source/perplexity.rst
docs/source/preprocessing.mdx
docs/source/preprocessing.rst
docs/source/quicktour.mdx
docs/source/quicktour.rst
docs/source/task_summary.mdx
docs/source/task_summary.rst
docs/source/tokenizer_summary.mdx
docs/source/tokenizer_summary.rst
docs/source/training.mdx
docs/source/training.rst
==================
0f4e39c55;lewtun;2021-12-08 19:04:40 +0100;Revert "Added support for other features for already supported models (#14358)" (#14679)
This reverts commit 0c70f145d1ba79773f7fa532a5f05486e260200a.
==

src/transformers/models/albert/configuration_albert.py
src/transformers/models/bart/configuration_bart.py
src/transformers/models/bert/configuration_bert.py
src/transformers/models/distilbert/configuration_distilbert.py
src/transformers/models/gpt2/configuration_gpt2.py
src/transformers/models/gpt_neo/configuration_gpt_neo.py
src/transformers/models/mbart/configuration_mbart.py
src/transformers/models/roberta/configuration_roberta.py
src/transformers/models/t5/configuration_t5.py
src/transformers/models/xlm_roberta/configuration_xlm_roberta.py
src/transformers/onnx/__init__.py
src/transformers/onnx/__main__.py
src/transformers/onnx/config.py
src/transformers/onnx/convert.py
src/transformers/onnx/features.py
src/transformers/test.onnx/model.onnx
tests/test_onnx_v2.py
==================
0c70f145d;Michael Benayoun;2021-12-08 18:39:56 +0100;Added support for other features for already supported models (#14358)
* Added support for other features for already supported models

* Partial support for causal and seq2seq models

* Partial support for causal and seq2seq models

* OnnxSeq2SeqConfigWithPast to support seq2seq models

* Parameterized the onnx tests

* Restored run_mlm.py

* Restored run_mlm.py

* [WIP] BART update

* BART and MBART

* Added comments

* Another sequence length of the past_key_values
==

src/transformers/models/albert/configuration_albert.py
src/transformers/models/bart/configuration_bart.py
src/transformers/models/bert/configuration_bert.py
src/transformers/models/distilbert/configuration_distilbert.py
src/transformers/models/gpt2/configuration_gpt2.py
src/transformers/models/gpt_neo/configuration_gpt_neo.py
src/transformers/models/mbart/configuration_mbart.py
src/transformers/models/roberta/configuration_roberta.py
src/transformers/models/t5/configuration_t5.py
src/transformers/models/xlm_roberta/configuration_xlm_roberta.py
src/transformers/onnx/__init__.py
src/transformers/onnx/__main__.py
src/transformers/onnx/config.py
src/transformers/onnx/convert.py
src/transformers/onnx/features.py
src/transformers/test.onnx/model.onnx
tests/test_onnx_v2.py
==================
ee4fa2e46;Patrick von Platen;2021-12-08 15:51:28 +0100;[AutoProcessor] Add Wav2Vec2WithLM & small fix (#14675)
* [AutoProcessor] Add Wav2Vec2WithLM & small fix

* revert line removal

* Update src/transformers/__init__.py

* add test

* up

* up

* small fix
==

src/transformers/__init__.py
src/transformers/models/__init__.py
src/transformers/models/auto/processing_auto.py
src/transformers/models/wav2vec2/__init__.py
src/transformers/models/wav2vec2_with_lm/__init__.py
src/transformers/models/wav2vec2_with_lm/processing_wav2vec2_with_lm.py
tests/fixtures/dummy_feature_extractor_config.json
tests/test_processor_auto.py
tests/test_processor_wav2vec2_with_lm.py
utils/check_inits.py
==================
2294071a0;Lysandre Debut;2021-12-08 15:14:36 +0100;Fix doc builder (#14676)

==

.github/workflows/build_documentation.yml
==================
fab3b518e;ZOHETH;2021-12-08 21:43:21 +0800;fix deprecated tf method (#14671)
tf.matrix_band_part -> tf.linalg.band_part
==

src/transformers/models/xlnet/modeling_tf_xlnet.py
==================
65b20b739;NielsRogge;2021-12-08 14:20:34 +0100;Add Perceiver IO (#14487)
* First draft

* Style and remove mlm

* Make forward pass work

* More improvements

* More improvements

* Fix bug

* More improvements

* More improvements

* Add PerceiverTokenizer first draft

* Improve conversion script

* More improvements

* Make conversion script work for the encoder

* Make conversion script work with local pickle files

* Style & quality, fix-copies

* Add dummy input to conversion script

* Add absolute position embeddings to TextPreProcessor

* Make forward pass of encoder work

* More improvements

* Move text preprocessor to separate script

* More improvements

* More improvements

* Add post processor

* Make MLM model work

* Style

* Add PerceiverForMaskedLM

* Add PerceiverImagePreprocessor

* Make style

* Make PerceiverForImageClassification work

* More improvements

* More improvements

* Use tokenizer in conversion script

* Use PerceiverForMaskedLM in conversion script

* Define custom PerceiverModelOutput

* Improve PerceiverAttention to make it work for both MLM and image classification

* More improvements

* More improvements

* More improvements to the conversion script

* Make conversion script work for both MLM and image classification

* Add PerceiverFeatureExtractor

* More improvements

* Style and quality

* Add center cropping

* Fix bug

* Small fix

* Add print statement

* Fix bug in image preprocessor

* Fix bug with conversion script

* Make output position embeddings an nn.Parameter layer instead of nn.Embedding

* Comment out print statements

* Add position encoding classes

* More improvements

* Use position_encoding_kwargs

* Add PerceiverForImageClassificationFourier

* Make style & quality

* Add PerceiverForImageClassificationConvProcessing

* Style & quality

* Add flow model

* Move processors to modeling file

* Make position encodings modular

* Make basic decoder use modular position encodings

* Add PerceiverForOpticalFlow to conversion script

* Add AudioPreprocessor

* Make it possible for the basic decoder to use Fourier position embeddings

* Add PerceiverForMultimodalAutoencoding

* Improve model for optical flow

* Improve _build_network_inputs method

* Add print statement

* Fix device issue

* Fix device of Fourier embeddings

* Add print statements for debugging

* Add another print statement

* Add another print statement

* Add another print statement

* Add another print statement

* Improve PerceiverAudioPreprocessor

* Improve conversion script for multimodal modal

* More improvements

* More improvements

* Improve multimodal model

* Make forward pass multimodal model work

* More improvements

* Improve tests

* Fix some more tests

* Add output dataclasses

* Make more tests pass

* Add print statements for debuggin

* Add tests for image classification

* Add PerceiverClassifierOutput

* More improvements

* Make more tests pass for the optical flow model

* Make style & quality

* Small improvements

* Don't support training for optical flow model for now

* Fix _prepare_for_class for tests

* Make more tests pass, add some docs

* Add multimodal model to tests

* Minor fixes

* Fix tests

* Improve conversion script

* Make fixup

* Remove pos_dim argument

* Fix device issue

* Potential fix for OOM

* Revert previous commit

* Fix test_initialization

* Add print statements for debugging

* Fix print statement

* Add print statement

* Add print statement

* Add print statement

* Add print statement

* Add print statement

* Add print statement

* Remove need for output_shape

* Comment out output_shape

* Remove unnecessary code

* Improve docs

* Fix make fixup

* Remove PerceiverTextProcessor from init

* Improve docs

* Small improvement

* Apply first batch of suggestions from code review

* Apply more suggestions from code review

* Update docstrings

* Define dicts beforehand for readability

* Rename task to architecture in conversion script, include PerceiverModel in tests

* Add print statements for debugging

* Fix tests on GPU

* Remove preprocessors, postprocessors and decoders from main init

* Add integration test

* Fix docs

* Replace einops by torch

* Update for new docs frontend

* Rename PerceiverForImageClassification

* Improve docs

* Improve docs

* Improve docs of PerceiverModel

* Fix some more tests

* Improve center_crop

* Add PerceiverForSequenceClassification

* Small improvements

* Fix tests

* Add integration test for optical flow model

* Clean up

* Add tests for tokenizer

* Fix tokenizer by adding special tokens properly

* Fix CI
==

README.md
README_ko.md
README_zh-hans.md
README_zh-hant.md
docs/source/index.mdx
docs/source/index.rst
docs/source/model_doc/perceiver.rst
src/transformers/__init__.py
src/transformers/models/__init__.py
src/transformers/models/auto/configuration_auto.py
src/transformers/models/auto/modeling_auto.py
src/transformers/models/perceiver/__init__.py
src/transformers/models/perceiver/configuration_perceiver.py
src/transformers/models/perceiver/convert_perceiver_haiku_to_pytorch.py
src/transformers/models/perceiver/feature_extraction_perceiver.py
src/transformers/models/perceiver/modeling_perceiver.py
src/transformers/models/perceiver/tokenization_perceiver.py
src/transformers/utils/dummy_pt_objects.py
src/transformers/utils/dummy_vision_objects.py
tests/test_modeling_perceiver.py
tests/test_tokenization_perceiver.py
utils/check_repo.py
==================
961732c27;Patrick von Platen;2021-12-08 12:07:54 +0100;[Wav2Vec2] PyCTCDecode Integration to support language model boosted decoding (#14339)
* up

* up

* up

* make it cleaner

* correct

* make styhahalal

* add more tests

* finish

* small fix

* make style

* up

* tryout to solve cicrle ci

* up

* fix more tests

* fix more tests

* apply sylvains suggestions

* fix import

* correct docs

* add pyctcdecode only to speech tests

* fix more tests

* add tf, flax and pt tests

* add pt

* fix last tests

* fix more tests

* Apply suggestions from code review

* change lines

* Apply suggestions from code review

Co-authored-by: Anton Lozhkov <aglozhkov@gmail.com>

* correct tests

* correct tests

* add doc string

Co-authored-by: Anton Lozhkov <aglozhkov@gmail.com>
==

.circleci/config.yml
.github/workflows/self-push.yml
.github/workflows/self-scheduled.yml
docs/source/model_doc/wav2vec2.rst
setup.py
src/transformers/__init__.py
src/transformers/dependency_versions_table.py
src/transformers/file_utils.py
src/transformers/models/wav2vec2/__init__.py
src/transformers/models/wav2vec2/processing_wav2vec2_with_lm.py
src/transformers/testing_utils.py
src/transformers/utils/dummy_pyctcdecode_objects.py
tests/test_modeling_flax_wav2vec2.py
tests/test_modeling_tf_wav2vec2.py
tests/test_modeling_wav2vec2.py
tests/test_processor_wav2vec2_with_lm.py
==================
2e12d90b9;Nicolas Patry;2021-12-08 09:54:24 +0100;Fixing Dataset for TQA + token-classification. (#14658)
* Fixing Dataset for TQA + token-classification.

* Fixing the tests.

* Making sure `offset_mappings` is a valid argument.
==

src/transformers/pipelines/table_question_answering.py
src/transformers/pipelines/token_classification.py
tests/test_pipelines_common.py
tests/test_pipelines_table_question_answering.py
tests/test_pipelines_token_classification.py
==================
fae0b9fae;Stas Bekman;2021-12-07 13:04:18 -0800;[trainer] conditional ctx managers into one wrapper (#14663)
* [trainer] conditional ctx managers into one wrapper

* workaround for contextlib.nullcontext for py<3.7

* Update src/transformers/trainer.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* one more autocast

* style

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

src/transformers/trainer.py
src/transformers/trainer_seq2seq.py
==================
39f1dff5a;TranSirius;2021-12-08 01:09:18 +0800;Fix a Bug, trainer_seq2seq.py, in the else branch at Line 172, generation_inputs should be a dict (#14546)
* fix bug, trainer_seq2seq.py, Line 172, generation_inputs must be a dict before feeding into self.model.generation()

* fix bug, trainer_seq2seq.py, Line 172, generation_inputs must be a dict before feeding into self.model.generation()
==

src/transformers/trainer_seq2seq.py
==================
2171695cc;Nouamane Tazi;2021-12-07 16:44:28 +0100;quick fix SummarizationPipeline error messages (#14618)
* quick fix SummarizationPipeline error messages

Fix error messages to avoid spam errors, and errors of type:
`Your max_length is set to 50, but you input_length is only 46. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)`

* correcto SummarizationPipeline error messages fixes
==

src/transformers/pipelines/text2text_generation.py
==================
b66c5ab20;Stas Bekman;2021-12-06 21:57:47 -0800;[deepspeed] fix --load_best_model_at_end (#14652)
* [deepspeed] fix load_best_model_at_end

* try with pull_request_target

* revert: try with pull_request_target

* style

* add test

* cleanup
==

src/transformers/deepspeed.py
src/transformers/trainer.py
tests/deepspeed/test_deepspeed.py
==================
30646a0a3;Ryokan RI;2021-12-07 14:25:28 +0900;Add mLUKE (#14640)
* implement MLukeTokenizer and LukeForMaskedLM

* update tests

* update docs

* add LukeForMaskedLM to check_repo.py

* update README

* fix test and specify the entity pad id in tokenization_(m)luke

* fix EntityPredictionHeadTransform
==

README.md
README_ko.md
README_zh-hans.md
README_zh-hant.md
docs/source/index.mdx
docs/source/main_classes/tokenizer.rst
docs/source/model_doc/luke.rst
docs/source/model_doc/mluke.rst
docs/source/multilingual.rst
src/transformers/__init__.py
src/transformers/models/__init__.py
src/transformers/models/auto/tokenization_auto.py
src/transformers/models/luke/__init__.py
src/transformers/models/luke/modeling_luke.py
src/transformers/models/luke/tokenization_luke.py
src/transformers/models/mluke/__init__.py
src/transformers/models/mluke/convert_mluke_original_pytorch_checkpoint_to_pytorch.py
src/transformers/models/mluke/tokenization_mluke.py
src/transformers/utils/dummy_pt_objects.py
src/transformers/utils/dummy_sentencepiece_objects.py
tests/test_modeling_luke.py
tests/test_tokenization_luke.py
tests/test_tokenization_mluke.py
utils/check_repo.py
==================
4cdb67cab;Yih-Dar;2021-12-07 00:27:32 +0100;Use cross_attention_hidden_size in Encoder-Decoder models (#14378)
* add cross_attention_hidden_size to text-2-text encoder-decoder models (PT/Flax)

* for TFEncoderDecoderModel

* add equivalence test for TFEncoderDecoderModel

* fix

* fix failed equivalence tests

* remove unused import

* add detailed comment

* Fix check_equivalence_tf_to_pt by using encoder/decoder

* cleaning

* Use cross_attention_hidden_size in speech-to-text

* clean fast init logging msg in encoder decoder models

* increase tol from 1e-5 to 1e-3 for tf test

* style

* style

* make sure projection layer can run

* remove type conversion + add check

* fix conflict (config.output_hidden_size)

* Remove TF -> PT in check_pt_tf_equivalence for TFEncoderDecoderModel

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>
==

src/transformers/models/encoder_decoder/modeling_encoder_decoder.py
src/transformers/models/encoder_decoder/modeling_flax_encoder_decoder.py
src/transformers/models/encoder_decoder/modeling_tf_encoder_decoder.py
src/transformers/models/speech_encoder_decoder/modeling_speech_encoder_decoder.py
src/transformers/models/vision_encoder_decoder/modeling_flax_vision_encoder_decoder.py
src/transformers/models/vision_encoder_decoder/modeling_vision_encoder_decoder.py
tests/test_modeling_flax_encoder_decoder.py
tests/test_modeling_tf_encoder_decoder.py
==================
381b05a3f;Sylvain Gugger;2021-12-06 17:25:28 -0500;Remove nonworking workflow for now

==

.github/workflows/build_doc_test.yml
==================
75ae287ae;Suraj Patil;2021-12-07 00:34:27 +0530;fix flax examples tests (#14646)
* make tensorboard optional

* update test_fetcher for flax examples

* make the tests slow
==

examples/flax/question-answering/run_qa.py
examples/flax/test_examples.py
examples/flax/text-classification/run_flax_glue.py
examples/flax/token-classification/run_flax_ner.py
utils/tests_fetcher.py
==================
03fda7b74;Sylvain Gugger;2021-12-06 13:55:59 -0500;Add a job to test the documentation build (#14645)
* Add a job to the documentation build

* Add caching

* Test cache
==

.github/workflows/build_doc_test.yml
.github/workflows/build_documentation.yml
==================
e513c16e8;Sylvain Gugger;2021-12-06 13:31:27 -0500;Fix syntax for class references (#14644)

==

src/transformers/generation_flax_utils.py
src/transformers/generation_utils.py
src/transformers/modeling_tf_utils.py
src/transformers/models/tapas/tokenization_tapas.py
src/transformers/pipelines/__init__.py
src/transformers/pipelines/audio_classification.py
src/transformers/pipelines/automatic_speech_recognition.py
src/transformers/pipelines/base.py
src/transformers/pipelines/feature_extraction.py
src/transformers/pipelines/zero_shot_classification.py
src/transformers/trainer.py
==================
e9688875b;Lysandre Debut;2021-12-06 18:49:50 +0100;Auto processor fix (#14623)
* Add AutoProcessor class
Init and tests
Add doc
Fix init
Update src/transformers/models/auto/processing_auto.py

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
Reverts to tokenizer or feature extractor when available
Adapt test

* Revert "Adapt test"

This reverts commit bbdde5fab02465f24b54b227390073082cb32093.

* Revert "Reverts to tokenizer or feature extractor when available"

This reverts commit 77659ff5d21b6cc0baf6f443017e35e056a525bb.

* Don't revert everything Lysandre!

Co-authored-by: Sylvain Gugger <sylvain.gugger@gmail.com>
==

src/transformers/models/auto/processing_auto.py
tests/test_processor_auto.py
==================
cbe602653;Suraj Patil;2021-12-06 23:14:37 +0530;fix flax example tests (#14643)

==

examples/flax/conftest.py
==================
df085d8ea;guhur;2021-12-06 17:51:53 +0100;doc: mismatch between pooler/d_output (#14641)
The model outputs a pooler_output whereas the doctype examples were using a pooled_output.
==

src/transformers/models/clip/modeling_clip.py
==================
0f3f045eb;tucan9389;2021-12-07 01:44:10 +0900;Add GPTJForQuestionAnswering (#14503)
* Add GPTJForQuestionAnswering

* Reformat for GPTJForQuestionAnswering

* Fix isort error

* make style for GPTJForQA

* Add _keys_to_ignore_on_load_missing

* Change the sequence of qa and classification

Co-authored-by: Suraj Patil <surajp815@gmail.com>
==

docs/source/model_doc/gptj.rst
src/transformers/__init__.py
src/transformers/models/auto/modeling_auto.py
src/transformers/models/gptj/__init__.py
src/transformers/models/gptj/modeling_gptj.py
src/transformers/utils/dummy_pt_objects.py
tests/test_modeling_gptj.py
==================
1ccc033c5;Jay Zhang;2021-12-06 21:01:51 +0800;Update the example of exporting Bart + BeamSearch to ONNX module to resolve comments. (#14310)
* Update code to resolve comments left in previous PR.

* Add README.md file for this example.

* Update examples/onnx/pytorch/translation/README.md

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>

* Update examples/onnx/pytorch/translation/README.md

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>

* Update examples/onnx/pytorch/translation/README.md

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>

* Update README.md file to resolve comments.

* Add a section name.

* Update examples/onnx/pytorch/translation/README.md

Co-authored-by: Gary Miguel <garymm@garymm.org>

* Add more comments for _convert_past_list_to_tuple().

* Change the default file name to a consistent one.

* Fix a format issue.

* Update examples/onnx/pytorch/translation/README.md

Co-authored-by: Gary Miguel <garymm@garymm.org>

* Update examples/onnx/pytorch/translation/run_onnx_exporter.py

Co-authored-by: Gary Miguel <garymm@garymm.org>

* Update examples/onnx/pytorch/translation/README.md

Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>

* Change the folder to summarization and address some other coments.

* Update the torch version.

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>
Co-authored-by: Gary Miguel <garymm@garymm.org>
Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>
==

examples/onnx/pytorch/summarization/README.md
examples/onnx/pytorch/summarization/bart_onnx/generation_onnx.py
examples/onnx/pytorch/summarization/bart_onnx/reduce_onnx_size.py
examples/onnx/pytorch/summarization/requirements.txt
examples/onnx/pytorch/summarization/run_onnx_exporter.py
examples/onnx/pytorch/translation/requirements.txt
==================
6cdc3a784;Julien Chaumond;2021-12-06 10:35:01 +0100;[urls to hub] Replace outdated model tags with their now-canonical pipeline types (#14617)
* Replace outdated model tags with their now-canonical pipeline types

* spam the CI till it's green
==

examples/flax/language-modeling/run_clm_flax.py
examples/flax/language-modeling/run_mlm_flax.py
examples/pytorch/language-modeling/run_clm.py
examples/pytorch/language-modeling/run_clm_no_trainer.py
examples/pytorch/language-modeling/run_mlm.py
examples/pytorch/language-modeling/run_mlm_no_trainer.py
examples/research_projects/jax-projects/dataset-streaming/run_mlm_flax_stream.py
examples/research_projects/jax-projects/hybrid_clip/run_hybrid_clip.py
examples/research_projects/mlm_wwm/run_mlm_wwm.py
examples/research_projects/performer/run_mlm_performer.py
examples/tensorflow/language-modeling/run_clm.py
examples/tensorflow/language-modeling/run_mlm.py
src/transformers/pipelines/fill_mask.py
src/transformers/pipelines/text2text_generation.py
src/transformers/pipelines/text_generation.py
==================
c824d7ed4;Suraj Patil;2021-12-06 14:50:43 +0530;add flax example tests in CI workflow (#14637)

==

.circleci/config.yml
==================
bc8a9f415;Suraj Patil;2021-12-06 10:52:43 +0530;fix typo (#14635)

==

README.md
==================
c5bd732ac;Suraj Patil;2021-12-06 10:48:58 +0530;Add Flax example tests (#14599)
* add test for glue

* add tests for clm

* fix clm test

* add summrization tests

* more tests

* fix few tests

* add test for t5 mlm

* fix t5 mlm test

* fix tests for multi device

* cleanup

* ci job

* fix metric file name

* make t5 more robust
==

.circleci/config.yml
examples/flax/_tests_requirements.txt
examples/flax/language-modeling/run_clm_flax.py
examples/flax/language-modeling/run_mlm_flax.py
examples/flax/language-modeling/run_t5_mlm_flax.py
examples/flax/question-answering/run_qa.py
examples/flax/summarization/run_summarization_flax.py
examples/flax/test_examples.py
examples/flax/text-classification/run_flax_glue.py
examples/flax/token-classification/run_flax_ner.py
src/transformers/testing_utils.py
==================
803a8cd18;Kamal Raj;2021-12-06 08:42:51 +0530;updated readme with proper arguments (#14624)

==

examples/pytorch/token-classification/README.md
==================
3977b5843;(Bill) Yuchen Lin;2021-12-04 22:01:23 -0800;fix a typo (#14626)

==

examples/pytorch/summarization/README.md
==================
73ec4340e;Matt;2021-12-03 20:15:09 +0000;Make DefaultDataCollator importable from root (#14588)
* Make DefaultDataCollator importable from root

* Add documentation for DefaultDataCollator and add return_tensors argument to all class docstrings

* make style

* Add DefaultDataCollator to data_collator.rst

* Add DefaultDataCollator to data_collator.rst
==

docs/source/main_classes/data_collator.rst
src/transformers/__init__.py
src/transformers/data/__init__.py
src/transformers/data/data_collator.py
==================
71b1bf7ea;Stas Bekman;2021-12-03 10:08:58 -0800;[trainer] add tf32-mode control (#14606)
* [trainer] add --tf32 support

* it's pt>=.17

* it's pt>=.17

* flip the default to True

* add experimental note

* simplify logic

* style

* switch to 3-state logic

* doc

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* re-style code

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

docs/source/performance.md
src/transformers/file_utils.py
src/transformers/testing_utils.py
src/transformers/training_args.py
tests/test_trainer.py
==================
aada989ad;Lysandre Debut;2021-12-03 18:09:25 +0100;Fix doc builder (#14616)
* Fix doc builder

* Fix doc builder

* Fix doc builder
==

.github/workflows/build_documentation.yml
==================
ec47baeba;Lysandre Debut;2021-12-03 17:35:44 +0100;2022 is the year of multi-modality (#14610)
* 2022 is the year of multi-modality

* Small fix

* Apply suggestions from code review

Co-authored-by: Suraj Patil <surajp815@gmail.com>
Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>
Co-authored-by: Anton Lozhkov <aglozhkov@gmail.com>

* Apply suggestions from code review

* Apply to documentation index

* Apply suggestions from code review

Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>

* Update README.md

Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>

* Apply suggestions from code review

* Apply suggestions from code review

Co-authored-by: Suraj Patil <surajp815@gmail.com>
Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>
Co-authored-by: Anton Lozhkov <aglozhkov@gmail.com>
Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>
==

README.md
docs/source/index.mdx
==================
e62091d5a;Stas Bekman;2021-12-03 05:18:36 -0800;[CI] move env print to util, add pt, nccl versions (#14607)
* move env print to util, add pt, nccl versions

* style

* version

* align
==

.github/workflows/self-nightly-scheduled.yml
.github/workflows/self-push.yml
.github/workflows/self-scheduled.yml
utils/print_env_pt.py
==================
66ea73916;Li-Huai (Allan) Lin;2021-12-03 15:39:10 +0800;Improve tokenizer tests (#13594)
* Use new method to acquire tokenizers

* Resolve TODOs.

* Style

* Fix

* Enable do_lower_case in test_tokenize_special_tokens

* Apply suggestion from code review

* Fix mask token handling

* Revert "Fix mask token handling"

This reverts commit daaa3f5291b1f71e5bc3604ca281c000000c4648.

* Fix FNet mask token tokenization

* Complete everything

* Apply suggestions from code review
==

tests/test_tokenization_common.py
==================
6645eb61f;Nik;2021-12-02 15:05:31 +0100;fix #14524 (IndexError when mask prob is too low) (#14525)
* fix #14524 (IndexError when mask prob is too low)

* fix formatting

* correct documentation, add option for setting min_num_masks

* change the semantic meaning of `mask_prob` in _compute_mask_indices

With this commit the meaing of `mask_prob` actually adhered to the probability for each
vector to be the start of a masked span of length.

* fix check_copies test

* fix documentation to semantic meaning of `upper bound of overall masking percentage`, revert changes to _compute_mask_indices

* fix typo
==

src/transformers/models/hubert/configuration_hubert.py
src/transformers/models/hubert/modeling_hubert.py
src/transformers/models/sew/configuration_sew.py
src/transformers/models/sew/modeling_sew.py
src/transformers/models/sew_d/configuration_sew_d.py
src/transformers/models/sew_d/modeling_sew_d.py
src/transformers/models/unispeech/configuration_unispeech.py
src/transformers/models/unispeech/modeling_unispeech.py
src/transformers/models/unispeech_sat/configuration_unispeech_sat.py
src/transformers/models/unispeech_sat/modeling_unispeech_sat.py
src/transformers/models/wav2vec2/configuration_wav2vec2.py
src/transformers/models/wav2vec2/modeling_wav2vec2.py
tests/test_modeling_wav2vec2.py
==================
96cc02b51;yis11178;2021-12-02 13:13:42 +0000;change tf.math.divide with int(/) to remove dim_per_head from the TF graph (#14600)
Co-authored-by: yis <yis@graphcore.ai>
==

src/transformers/models/distilbert/modeling_tf_distilbert.py
==================
43f953cc2;Leandro von Werra;2021-12-02 10:41:35 +0100;Add CodeParrot ü¶ú codebase (#14536)
* add readme skeleton

* update readme

* add initialization script

* add deduplication script

* add codeparrot training script

* add code generation evaluation

* add validation loss script

* add requirements

* update readme

* tweak readme

* make style

* add highlights to readme

* add CLIs to scripts

* add tokenizer training script

* add docstring to constant length dataset

* fix defaults in arguments

* update readme with cli

* move image to hub

* tweaks of readme

* fix cli commands

* add author

* explain env variables

* fix formatting

* Update examples/research_projects/codeparrot/README.md

Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>

* Apply suggestions from code review

Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>

* replace generic with gpt2 tokenizer

Co-authored-by: lewtun <lewis.c.tunstall@gmail.com>
==

examples/research_projects/codeparrot/README.md
examples/research_projects/codeparrot/requirements.txt
examples/research_projects/codeparrot/scripts/arguments.py
examples/research_projects/codeparrot/scripts/bpe_training.py
examples/research_projects/codeparrot/scripts/codeparrot_training.py
examples/research_projects/codeparrot/scripts/human_eval.py
examples/research_projects/codeparrot/scripts/initialize_model.py
examples/research_projects/codeparrot/scripts/preprocessing.py
examples/research_projects/codeparrot/scripts/validation_loss.py
==================
e4c67d60e;Lysandre Debut;2021-12-02 10:09:17 +0100;Python 3.6 -> Python 3.7 for TF runs (#14598)

==

.circleci/config.yml
==================
50d909be2;Daniel Stancl;2021-12-02 09:51:48 +0100;[Flax] Add FlaxBlenderbotSmall (#14576)
* [WIP] Add FlaxBlenderbotSmall

* Revert some unintentionally changed files

Revert some unintentionally files changed by improperly filled cookiecutter instructions.

* Fix repo consistency

* Fix Flax-PT equivalence

* Apply suggestions from code review

* Update index.mdx

* Apply suggestions from code review

Co-authored-by: Suraj Patil <surajp815@gmail.com>
==

docs/source/index.mdx
docs/source/model_doc/blenderbot_small.rst
src/transformers/__init__.py
src/transformers/models/auto/modeling_flax_auto.py
src/transformers/models/blenderbot_small/__init__.py
src/transformers/models/blenderbot_small/modeling_flax_blenderbot_small.py
src/transformers/utils/dummy_flax_objects.py
tests/test_modeling_flax_blenderbot_small.py
==================
77d87e732;Lysandre Debut;2021-12-02 09:32:38 +0100;Adds a git pull instruction to the documentation builder (#14597)
* Adds a git pull instruction

* master -> main
==

.github/workflows/build_documentation.yml
==================
275402bf2;Mishig Davaadorj;2021-12-02 09:01:35 +0100;Update doc img links (#14593)
* Update doc img links

* Rename toctree.yml -> _toctree.yml (#14594)

* Update doc img links

* Update performance.md img link
==

docs/source/add_new_model.rst
docs/source/model_summary.rst
docs/source/parallelism.md
docs/source/performance.md
docs/source/perplexity.rst
==================
4f68de625;Mishig Davaadorj;2021-12-02 08:58:39 +0100;Rename toctree.yml -> _toctree.yml (#14594)

==

docs/source/_toctree.yml
==================
fbe278c76;Stas Bekman;2021-12-01 14:18:58 -0800;[doc] bf16/tf32 guide (#14579)
* [doc] bf16/tf32 guide

* expand

* expand

* Update docs/source/performance.md

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

docs/source/imgs/tf32-bf16-fp16-fp32.png
docs/source/performance.md
==================
934e2799d;Li-Huai (Allan) Lin;2021-12-02 03:16:52 +0800;Fix mask token handling (#14364)
* Fix mask token handling

* Revert "Fix mask token handling"

This reverts commit daaa3f5291b1f71e5bc3604ca281c000000c4648.

* Fix FNet mask token tokenization
==

src/transformers/models/fnet/tokenization_fnet.py
src/transformers/models/fnet/tokenization_fnet_fast.py
==================
4df7d05a8;Sylvain Gugger;2021-12-01 14:13:02 -0500;Doc new front (#14590)
* Convert PretrainedConfig doc to Markdown

* Use syntax

* Add necessary doc files (#14496)

* Doc fixes (#14499)

* Fixes for the new front

* Convert DETR file for table

* Title is needed

* Simplify a bit

* Even simpler

* Remove imports

* Fix typo in toctree (#14516)

* Fix checkpoints badge

* Update versions.yml format (#14517)

* Doc new front github actions (#14512)

* Doc new front github actions

* Fix docstring

* Fix feature extraction utils import (#14515)

* Address Julien's comments

* Push to doc-builder

* Ready for merge

* Remove old build and deploy

* Doc misc fixes (#14583)

* Rm versions.yml from doc

* Fix converting.rst

* Rm pretrained_models from toctree

* Fix index links (#14567)

* Fix links in README

* Localized READMEs

* Fix copy script

* Fix find doc script

* Update README_ko.md

Co-authored-by: Julien Chaumond <julien@huggingface.co>

Co-authored-by: Julien Chaumond <julien@huggingface.co>

* Adapt build command to new CLI tools (#14578)

* Fix typo

* Fix doc interlinks (#14589)

* Convert PretrainedConfig doc to Markdown

* Use syntax

* Rm pattern <[a-z]+(.html).*>

* Rm huggingface.co/transformers/master

* Rm .html

* Rm .html from index.mdx

* Rm .html from model_summary.rst

* Update index.mdx rm html

* Update remove .html

* Fix inner doc links

* Fix interlink in preprocssing.rst

* Update pr_checks

Co-authored-by: Sylvain Gugger <sylvain.gugger@gmail.com>
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Convert PretrainedConfig doc to Markdown

* Use syntax

* Add necessary doc files (#14496)

* Doc fixes (#14499)

* Fixes for the new front

* Convert DETR file for table

* Title is needed

* Simplify a bit

* Even simpler

* Remove imports

* Fix checkpoints badge

* Fix typo in toctree (#14516)

* Update versions.yml format (#14517)

* Doc new front github actions (#14512)

* Doc new front github actions

* Fix docstring

* Fix feature extraction utils import (#14515)

* Address Julien's comments

* Push to doc-builder

* Ready for merge

* Remove old build and deploy

* Doc misc fixes (#14583)

* Rm versions.yml from doc

* Fix converting.rst

* Rm pretrained_models from toctree

* Fix index links (#14567)

* Fix links in README

* Localized READMEs

* Fix copy script

* Fix find doc script

* Update README_ko.md

Co-authored-by: Julien Chaumond <julien@huggingface.co>

Co-authored-by: Julien Chaumond <julien@huggingface.co>

* Adapt build command to new CLI tools (#14578)

* Fix typo

* Fix doc interlinks (#14589)

* Convert PretrainedConfig doc to Markdown

* Use syntax

* Rm pattern <[a-z]+(.html).*>

* Rm huggingface.co/transformers/master

* Rm .html

* Rm .html from index.mdx

* Rm .html from model_summary.rst

* Update index.mdx rm html

* Update remove .html

* Fix inner doc links

* Fix interlink in preprocssing.rst

* Update pr_checks

Co-authored-by: Sylvain Gugger <sylvain.gugger@gmail.com>
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Styling

Co-authored-by: Mishig Davaadorj <mishig.davaadorj@coloradocollege.edu>
Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
Co-authored-by: Julien Chaumond <julien@huggingface.co>
==

.circleci/config.yml
.circleci/deploy.sh
.github/workflows/build_documentation.yml
CONTRIBUTING.md
README.md
README_ko.md
README_zh-hans.md
README_zh-hant.md
docs/source/_static/css/Calibre-Light.ttf
docs/source/_static/css/Calibre-Medium.otf
docs/source/_static/css/Calibre-Regular.otf
docs/source/_static/css/Calibre-Thin.otf
docs/source/_static/css/code-snippets.css
docs/source/_static/css/huggingface.css
docs/source/_static/js/custom.js
docs/source/_static/js/huggingface_logo.svg
docs/source/community.md
docs/source/conf.py
docs/source/converting_tensorflow_models.rst
docs/source/favicon.ico
docs/source/index.mdx
docs/source/index.rst
docs/source/migration.md
docs/source/model_doc/blenderbot.rst
docs/source/model_doc/deit.rst
docs/source/model_doc/detr.mdx
docs/source/model_doc/detr.rst
docs/source/model_doc/layoutlmv2.rst
docs/source/model_summary.rst
docs/source/parallelism.md
docs/source/preprocessing.rst
docs/source/pretrained_models.rst
docs/source/quicktour.rst
docs/source/toctree.yml
docs/source/training.rst
docs/source/troubleshooting.md
examples/README.md
notebooks/README.md
src/transformers/configuration_utils.py
src/transformers/models/beit/feature_extraction_beit.py
utils/check_copies.py
utils/check_repo.py
utils/check_table.py
==================
14cc50d08;Stas Bekman;2021-12-01 09:32:52 -0800;fix autocast for older pytorch

==

src/transformers/trainer.py
==================
4c0dd199c;Suraj Patil;2021-12-01 10:57:39 +0530;FlaxGPTJ (#14396)
* add flax gptj

* no bias in attention dense

* no wpe

* fix rotary embeddings

* fix rotary embeds

* fix rotray embeds

* quality

* doc and quality

* fix equivalence tests
==

docs/source/index.rst
docs/source/model_doc/gptj.rst
src/transformers/__init__.py
src/transformers/models/auto/modeling_flax_auto.py
src/transformers/models/gptj/__init__.py
src/transformers/models/gptj/modeling_flax_gptj.py
src/transformers/utils/dummy_flax_objects.py
tests/test_modeling_flax_gptj.py
tests/test_modeling_gptj.py
==================
70996a542;Jamie DeAntonis;2021-11-30 21:00:47 -0500;WIP: Support for Training with BF16 (#13207)
* started bf16 integration

* minor changes

* code now runs

* style

* lay foundation for bf16 testing

* lay foundation for bf16 testing

* start the tests

* better bf16 check

* style

* 2 separate checkers - one for bf16 support, another for bf16+autocast

* Update src/transformers/training_args.py

Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>

* a couple of comment resolutions

* more comment resolutions

* resolved a small bug

* just some print statemtns

* added todo marking

* added a todo

* adjust for API change s/fast_dtype/dtype/

* fix style

* merge 2 bf16 util functions

* bf16 now does scaling too

* Add support for bfloat16

* Revert T5 layernorm to float32

This is based on the comment at https://github.com/huggingface/transformers/pull/14448/files#r752660929 and the PyTorch PR https://github.com/pytorch/pytorch/pull/66920 .

* Add comment about conversion to float32 before returning the numpy data

* Add comment about AMP-bfloat16 incompatibility

* Fix formatting

* typo

* reformer / bf16

* cleanup

* require at least pt-1.10

* fix

* will deal with deepspeed separately

* cleanup

* revert

* cleanup

* fp16_full_eval and bf16_full_eval are separate modes

* proper deprecation

* cleanup

* test and fixes

* spelling

* cleanup

* add a note that this API is experimental

Co-authored-by: jamie <jamie@cortx.com>
Co-authored-by: Stas Bekman <stas@stason.org>
Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>
Co-authored-by: suriya <suriya@cortx.com>
Co-authored-by: Manuel R. Ciosici <manuelrciosici@gmail.com>
==

src/transformers/file_utils.py
src/transformers/modeling_utils.py
src/transformers/models/t5/modeling_t5.py
src/transformers/testing_utils.py
src/transformers/trainer.py
src/transformers/trainer_pt_utils.py
src/transformers/training_args.py
tests/test_trainer.py
==================
fc1d97f29;Suraj Patil;2021-11-30 22:21:48 +0530;VisionTextDualEncoder (#13511)
* init vision_text_dual_encoder

* fix merge

* remove extra heads

* fix tests

* remove VISION_TEXT_DUAL_ENCODER_PRETRAINED_CONFIG_ARCHIVE_MAP

* remove archive map

* fix imports

* fix more imports

* fix init

* delete tokenizers

* fix imports

* clean

* support clip's vision model

* handle None config

* begin tests

* more test and few fixes

* warn about newly init weights

* more tests

* add loss to model

* remove extra classes from doc

* add processor

* doc and small fixes

* add start docstr

* update flax model

* flax tests

* more flax tests

* doc

* quality

* doc and quality

* fix doc

* doc

* remove comments

* update warning

* quality

* fix docs

* Apply suggestions from code review

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* replace asserts, fix imports

* update imports

* fix import

* address some review comments

* fix check

* reduce tolerance

* fix test

* add flax integration test

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* address Sylvain's comments

* fix style

* add pt_flax_equivalence test in PT tests

* add pt integration test

* update test

* use pre-trained checkpoint in examples

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

docs/source/index.rst
docs/source/model_doc/vision_text_dual_encoder.rst
src/transformers/__init__.py
src/transformers/models/__init__.py
src/transformers/models/auto/configuration_auto.py
src/transformers/models/auto/modeling_auto.py
src/transformers/models/auto/modeling_flax_auto.py
src/transformers/models/auto/processing_auto.py
src/transformers/models/vision_text_dual_encoder/__init__.py
src/transformers/models/vision_text_dual_encoder/configuration_vision_text_dual_encoder.py
src/transformers/models/vision_text_dual_encoder/modeling_flax_vision_text_dual_encoder.py
src/transformers/models/vision_text_dual_encoder/modeling_vision_text_dual_encoder.py
src/transformers/models/vision_text_dual_encoder/processing_vision_text_dual_encoder.py
src/transformers/utils/dummy_flax_objects.py
src/transformers/utils/dummy_pt_objects.py
tests/test_modeling_flax_vision_text_dual_encoder.py
tests/test_modeling_vision_text_dual_encoder.py
tests/test_processor_vision_text_dual_encoder.py
utils/check_repo.py
==================
6ed9882dd;Thomas Viehmann;2021-11-30 17:47:33 +0100;use functional interface for softmax in attention (#14198)
* use functional interface instead of instantiating module and immediately calling it

* fix torch.nn.functional to nn.functional. Thank you Stas!
==

examples/research_projects/movement-pruning/emmental/modeling_bert_masked.py
src/transformers/models/albert/modeling_albert.py
src/transformers/models/beit/modeling_beit.py
src/transformers/models/bert/modeling_bert.py
src/transformers/models/canine/modeling_canine.py
src/transformers/models/deit/modeling_deit.py
src/transformers/models/distilbert/modeling_distilbert.py
src/transformers/models/electra/modeling_electra.py
src/transformers/models/gpt2/modeling_gpt2.py
src/transformers/models/gpt_neo/modeling_gpt_neo.py
src/transformers/models/gptj/modeling_gptj.py
src/transformers/models/ibert/quant_modules.py
src/transformers/models/layoutlm/modeling_layoutlm.py
src/transformers/models/luke/modeling_luke.py
src/transformers/models/lxmert/modeling_lxmert.py
src/transformers/models/megatron_bert/modeling_megatron_bert.py
src/transformers/models/mobilebert/modeling_mobilebert.py
src/transformers/models/mpnet/modeling_mpnet.py
src/transformers/models/openai/modeling_openai.py
src/transformers/models/rembert/modeling_rembert.py
src/transformers/models/roberta/modeling_roberta.py
src/transformers/models/roformer/modeling_roformer.py
src/transformers/models/segformer/modeling_segformer.py
src/transformers/models/splinter/modeling_splinter.py
src/transformers/models/tapas/modeling_tapas.py
src/transformers/models/visual_bert/modeling_visual_bert.py
src/transformers/models/vit/modeling_vit.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_{{cookiecutter.lowercase_modelname}}.py
==================
4176bc161;giacomo snidero;2021-11-30 17:34:41 +0100;Add documentation for multi-label classification (#14168)
* "update example docstring multilabel example

* update example docstring multilabel example
==

src/transformers/file_utils.py
==================
faacd7472;Daniel Stancl;2021-11-30 13:06:54 +0100;[Flax] Add FlaxBlenderbot (#13633)
* Init Flax implementation for Blenderbot

* Add a majority of stuff except for tests

* make style quality

* Add tests and fix some bugs

* Add tests

* Clean source code and fix some bugs

* Fix copies and docs

* Fix jax device condition for tests

* Fix layer norm in the encoder

* Fix a few typos in the test file

* make fix-copies

* make fix-copies

* fix layer norm

* Fix Flax params dtype (#13090)

* Fix PR reference (#13098)

* make fix-copies

* Update tests/test_modeling_flax_blenderbot.py

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
Co-authored-by: Suraj Patil <surajp815@gmail.com>
==

docs/source/index.rst
docs/source/model_doc/blenderbot.rst
src/transformers/__init__.py
src/transformers/models/auto/modeling_flax_auto.py
src/transformers/models/bart/modeling_flax_bart.py
src/transformers/models/blenderbot/__init__.py
src/transformers/models/blenderbot/modeling_flax_blenderbot.py
src/transformers/models/marian/modeling_flax_marian.py
src/transformers/models/mbart/modeling_flax_mbart.py
src/transformers/models/pegasus/modeling_flax_pegasus.py
src/transformers/testing_utils.py
src/transformers/utils/dummy_flax_objects.py
tests/test_modeling_flax_blenderbot.py
==================
254fef67c;Sylvain Gugger;2021-11-30 05:32:20 -0500;Fix backend regex (#14566)

==

utils/check_inits.py
==================
c468a87a6;Kamal Raj;2021-11-30 15:37:55 +0530;Tapas tf (#13393)
* TF Tapas first commit

* updated docs

* updated logger message

* updated pytorch weight conversion
script to support scalar array

* added use_cache to tapas model config to
work properly with tf input_processing

* 1. rm embeddings_sum
2. added # Copied
3. + TFTapasMLMHead
4. and lot other small fixes

* updated docs

* + test for tapas

* updated testing_utils to check
is_tensorflow_probability_available

* converted model logits post processing using
numpy to work with both PT and TF models

* + TFAutoModelForTableQuestionAnswering

* added TF support

* added test for
TFAutoModelForTableQuestionAnswering

* added test for
TFAutoModelForTableQuestionAnswering pipeline

* updated auto model docs

* fixed typo in import

* added tensorflow_probability to run tests

* updated MLM head

* updated tapas.rst with TF  model docs

* fixed optimizer import in docs

* updated convert to np
data from pt model is not
`transformers.tokenization_utils_base.BatchEncoding`
after pipeline upgrade

* updated pipeline:
1. with torch.no_gard removed, pipeline forward handles
2. token_type_ids converted to numpy

* updated docs.

* removed `use_cache` from config

* removed floats_tensor

* updated code comment

* updated Copyright Year and
logits_aggregation Optional

* updated docs and comments

* updated docstring

* fixed model weight loading

* make fixup

* fix indentation

* added tf slow pipeline test

* pip upgrade

* upgrade python to 3.7

* removed from_pt from tests

* revert commit f18cfa9
==

.circleci/config.yml
docs/source/index.rst
docs/source/model_doc/auto.rst
docs/source/model_doc/tapas.rst
src/transformers/__init__.py
src/transformers/file_utils.py
src/transformers/modeling_tf_pytorch_utils.py
src/transformers/models/auto/__init__.py
src/transformers/models/auto/modeling_tf_auto.py
src/transformers/models/tapas/__init__.py
src/transformers/models/tapas/modeling_tf_tapas.py
src/transformers/models/tapas/tokenization_tapas.py
src/transformers/pipelines/__init__.py
src/transformers/pipelines/table_question_answering.py
src/transformers/testing_utils.py
src/transformers/utils/dummy_tf_objects.py
tests/test_modeling_tf_auto.py
tests/test_modeling_tf_tapas.py
tests/test_pipelines_table_question_answering.py
==================
6fc38adff;Matt;2021-11-29 17:36:19 +0000;Add model checkpointing to push_to_hub and PushToHubCallback (#14492)
* Add checkpointing to push_to_hub and PushToHubCallback

* Add checkpoint loading

* Add missing default value

* Correct method name

* make style

* Moving everything to the right location

* make style

* Revert changes to file_utils.py

* Update src/transformers/keras_callbacks.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/keras_callbacks.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Adding docstrings and comments to clarify code

* make style

* Fix organization positional arg

* Fix load_repo_checkpoint to no longer accidentally create empty repos

* make style

* Remove unnecessary 'organization' argument in load_repo_checkpoint

* Avoid private `_create_or_get_repo` method

* make style

* Update src/transformers/modeling_tf_utils.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

src/transformers/keras_callbacks.py
src/transformers/modeling_tf_utils.py
==================
8332327dc;Rahul Nadkarni;2021-11-29 08:30:17 -0800;Fix sentinel token IDs in data collator for Flax T5 pretraining script (#14477)

==

examples/flax/language-modeling/run_t5_mlm_flax.py
==================
2bd950ca4;Kamal Raj;2021-11-29 21:55:59 +0530;[Flax] token-classification model steps enumerate start from 1 (#14547)
* step start from 1

* Updated cur_step calcualtion
==

examples/flax/token-classification/run_flax_ner.py
==================
cea17acd8;Patrick von Platen;2021-11-29 17:10:19 +0200;[Generate] Fix generate with inputs_embeds on GPU (#14564)

==

src/transformers/generation_utils.py
==================
25156eb29;NielsRogge;2021-11-29 10:19:11 +0100;Rename ImageGPT (#14526)
* Rename

* Add MODEL_FOR_CAUSAL_IMAGE_MODELING_MAPPING
==

docs/source/model_doc/imagegpt.rst
src/transformers/__init__.py
src/transformers/models/auto/__init__.py
src/transformers/models/auto/modeling_auto.py
src/transformers/models/imagegpt/__init__.py
src/transformers/models/imagegpt/modeling_imagegpt.py
src/transformers/utils/dummy_pt_objects.py
tests/test_modeling_common.py
tests/test_modeling_imagegpt.py
==================
4ee0b755b;≈†tƒõp√°n M≈±ller;2021-11-29 10:15:08 +0100;LayoutLMv2FeatureExtractor now supports non-English languages when applying Tesseract OCR. (#14514)
* Added the lang argument to apply_tesseract in feature_extraction_layoutlmv2.py, which is used in pytesseract.image_to_data.

* Added ocr_lang argument to LayoutLMv2FeatureExtractor.__init__, which is used when calling apply_tesseract

* Updated the documentation of the LayoutLMv2FeatureExtractor

* Specified in the documentation of the LayoutLMv2FeatureExtractor that the ocr_lang argument should be a language code.

* Update src/transformers/models/layoutlmv2/feature_extraction_layoutlmv2.py

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>

* Split comment into two lines to adhere to the max line size limit.

* Update src/transformers/models/layoutlmv2/feature_extraction_layoutlmv2.py

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>
==

src/transformers/models/layoutlmv2/feature_extraction_layoutlmv2.py
==================
ebbe8cc3f;Xing Han Lu;2021-11-28 18:55:38 -0500;Tokenizers docs: Specify which class contains `__call__` method (#14379)
* Update tokenizer.rst

* Apply `make fixup`
==

docs/source/main_classes/tokenizer.rst
==================
69511cdca;Suraj Patil;2021-11-26 18:21:47 +0530;unfreeze initial cache in gpt models (#14535)

==

src/transformers/models/gpt2/modeling_flax_gpt2.py
src/transformers/models/gpt_neo/modeling_flax_gpt_neo.py
==================
2318bf77e;Lysandre Debut;2021-11-26 04:35:08 -0500;Fixes (#14534)

==

docs/source/quicktour.rst
==================
c15f4f203;Lysandre Debut;2021-11-26 04:09:31 -0500;Quicktour updates (#14533)

==

docs/source/quicktour.rst
==================
1bbd6fcde;Chris Fregly;2021-11-26 03:46:07 -0500;     added save_directories for _psave_pretrained_pt and _tf, changed model to tf_model and pt_model, enable the notebook to run cleanly from top to bottom without error (#14529)
* added save_directories for _psave_pretrained_pt and _tf, changed model to tf_model and pt_model, enable the notebook to run cleanly from top to bottom without error

* Update quicktour.rst

* added >>>

* dependencies

* added space
==

docs/source/quicktour.rst
==================
04683c065;Nicolas Patry;2021-11-25 18:59:33 +0100;Fix a slow test. (#14527)

==

tests/test_pipelines_audio_classification.py
==================
d1fd64e7a;Stas Bekman;2021-11-25 00:15:35 -0800;clear ~/.cache/torch_extensions between builds (#14520)

==

.github/workflows/self-nightly-scheduled.yml
.github/workflows/self-push.yml
.github/workflows/self-scheduled.yml
==================
3772af49c;NielsRogge;2021-11-24 15:22:20 +0100;[Tests] Improve vision tests (#14458)
* Improve tests

* Install vision for tf tests
==

.circleci/config.yml
.github/workflows/self-scheduled.yml
tests/test_modeling_beit.py
tests/test_modeling_deit.py
tests/test_modeling_tf_vit.py
tests/test_modeling_vit.py
==================
f2e90bcb8;Lysandre Debut;2021-11-24 09:03:21 -0500;Fix feature extraction utils import (#14515)

==

src/transformers/__init__.py
==================
6c4d688ff;Vladimir Maryasin;2021-11-24 12:22:03 +0100;add cache_dir for tokenizer verification loading (#14508)
When loading a pretrained tokenizer, a verification is done to ensure
that the actual tokenizer class matches the class it was called from.
If the tokenizer is absent, its config file is loaded from the repo.

However, the cache_dir for downloading is not provided, which leads to
ignoring of the user-specified cache_dir, storing files in several
places and and may result in incorrect warnings when the default
cache_dir is unreachsble.

This commit fixes that.
==

src/transformers/tokenization_utils_base.py
==================
956a48317;Stas Bekman;2021-11-23 14:09:15 -0800;[deepspeed] zero inference (#14253)
* [deepspeed] zero inference

* only z3 makes sense for inference

* fix and style

* docs

* rework

* fix test

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* responding to suggestions

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

docs/source/main_classes/deepspeed.rst
setup.py
src/transformers/deepspeed.py
src/transformers/dependency_versions_table.py
src/transformers/trainer.py
tests/deepspeed/test_deepspeed.py
==================
69e16abf9;Nicholas Broad;2021-11-22 16:17:26 -0500;Switch from using sum for flattening lists of lists in group_texts (#14472)
* remove sum for list flattening

* change to chain(*)

* make chain object a list

* delete empty lines

per sgugger's suggestions

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

Co-authored-by: Nicholas Broad <nicholas@nmbroad.com>
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

examples/flax/language-modeling/run_clm_flax.py
examples/flax/language-modeling/run_mlm_flax.py
examples/flax/language-modeling/run_t5_mlm_flax.py
examples/pytorch/language-modeling/run_clm.py
examples/pytorch/language-modeling/run_clm_no_trainer.py
examples/pytorch/language-modeling/run_mlm.py
examples/pytorch/language-modeling/run_mlm_no_trainer.py
examples/pytorch/language-modeling/run_plm.py
examples/pytorch/multiple-choice/run_swag.py
examples/pytorch/multiple-choice/run_swag_no_trainer.py
examples/research_projects/jax-projects/model_parallel/run_clm_mp.py
examples/tensorflow/language-modeling/run_clm.py
examples/tensorflow/language-modeling/run_mlm.py
examples/tensorflow/multiple-choice/run_swag.py
src/transformers/file_utils.py
==================
0b7d053c1;Valentin;2021-11-22 22:00:43 +0100;fixes some key names for in LayoutLMv2 / LayoutXLM tokenizers (#14493)
in case of left padding_side there was a copy/paste error
assigning the bbox data to the labels
==

src/transformers/models/layoutlmv2/tokenization_layoutlmv2.py
src/transformers/models/layoutlmv2/tokenization_layoutlmv2_fast.py
src/transformers/models/layoutxlm/tokenization_layoutxlm.py
src/transformers/models/layoutxlm/tokenization_layoutxlm_fast.py
==================
204d25131;Sylvain Gugger;2021-11-22 12:17:38 -0500;Auto processor (#14465)
* Add AutoProcessor class

* Init and tests

* Add doc

* Fix init

* Update src/transformers/models/auto/processing_auto.py

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* Reverts to tokenizer or feature extractor when available

* Adapt test

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
==

docs/source/model_doc/auto.rst
src/transformers/__init__.py
src/transformers/models/auto/__init__.py
src/transformers/models/auto/feature_extraction_auto.py
src/transformers/models/auto/processing_auto.py
tests/fixtures/preprocessor_config.json
tests/test_processor_auto.py
==================
11f65d415;Stas Bekman;2021-11-22 08:33:43 -0800;[test] add test for --config_overrides (#14466)
* add test for --config_overrides

* remove unneeded parts of the test
==

examples/pytorch/language-modeling/run_clm.py
examples/pytorch/language-modeling/run_mlm.py
examples/pytorch/language-modeling/run_plm.py
examples/pytorch/test_examples.py
==================
e0e2da119;Daniel Stancl;2021-11-22 16:35:49 +0100;Improve a add-new-pipeline docs a bit (#14485)

==

docs/source/add_new_pipeline.rst
==================
a4553e6c6;Nicolas Patry;2021-11-22 10:40:45 +0100;Moving pipeline tests from `Narsil` to `hf-internal-testing`. (#14463)
* Moving everything to `hf-internal-testing`.

* Fixing test values.

* Moving to other repo.

* Last touch?
==

tests/test_pipelines_common.py
tests/test_pipelines_image_classification.py
tests/test_pipelines_image_segmentation.py
tests/test_pipelines_object_detection.py
tests/test_pipelines_text_classification.py
tests/test_pipelines_token_classification.py
==================
1a92bc578;Sylvain Gugger;2021-11-21 17:39:20 -0500;Fix dummy objects for quantization (#14478)
* Fix dummy objects for quantization

* Add more models
==

src/transformers/utils/dummy_flax_objects.py
src/transformers/utils/dummy_pt_objects.py
src/transformers/utils/dummy_pytorch_quantization_and_torch_objects.py
src/transformers/utils/dummy_tf_objects.py
utils/check_dummies.py
==================
c9d2cf855;Alexander Measure;2021-11-21 10:31:09 -0500;add Tuple as possible type hint for EvalPredictions label_ids (#14473)
* Update trainer_utils.py

* add Tuple type hints to all label_ids outputs

affects EvalLoopOutput and PredicctionOutput
==

src/transformers/trainer_utils.py
==================
a59e7c1ed;Shang Zhang;2021-11-19 10:33:39 -0800;Add QDQBert model and quantization examples of SQUAD task (#14066)
* clean up branch for add-qdqbert-model

* README update for QAT example; update docstrings in modeling_qdqbert.py

* Update qdqbert.rst

* Update README.md

* Update README.md

* calibration data using traning set; QAT example runs in fp32

* re-use BERTtokenizer for qdqbert

* Update docs/source/model_doc/qdqbert.rst

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update docs/source/model_doc/qdqbert.rst

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update docs/source/model_doc/qdqbert.rst

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* remove qdqbert tokenizer

* Update qdqbert.rst

* update evaluate-hf-trt-qa.py

* update configuration_qdqbert.py

* update modeling_qdqbert.py: add copied statement; replace assert with ValueError

* update copied from statement

* add is_quantization_available; run make fix-copies

* unittest add require_quantization

* add backend dependency to qdqbert model

* update README; update evaluate script; make style

* lint

* docs qdqbert update

* circleci build_doc add pytorch-quantization for qdqbert

* update README

* update example readme with instructions to upgrade TensorRT to 8.2

* Update src/transformers/models/qdqbert/configuration_qdqbert.py

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* Update src/transformers/models/qdqbert/configuration_qdqbert.py

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* Update src/transformers/models/qdqbert/configuration_qdqbert.py

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* Update src/transformers/models/qdqbert/configuration_qdqbert.py

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* change quantization to pytorch_quantization for backend requirement

* feed_forward_chunking not supported in QDQBert

* make style

* update model docstrings and comments in testing scripts

* rename example to quantization-qdqbert; rename example scripts from qat to quant

* Update src/transformers/models/qdqbert/modeling_qdqbert.py

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* rm experimental functions in quant_trainer

* qa cleanup

* make fix-copies for docs index.rst

* fix doctree; use post_init() for qdqbert

* fix early device assignment for qdqbert

* fix CI:Model templates runner

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
==

.circleci/config.yml
README.md
README_ko.md
README_zh-hans.md
README_zh-hant.md
docs/source/index.rst
docs/source/model_doc/qdqbert.rst
examples/research_projects/quantization-qdqbert/Dockerfile
examples/research_projects/quantization-qdqbert/README.md
examples/research_projects/quantization-qdqbert/evaluate-hf-trt-qa.py
examples/research_projects/quantization-qdqbert/quant_trainer.py
examples/research_projects/quantization-qdqbert/run_quant_qa.py
examples/research_projects/quantization-qdqbert/trainer_quant_qa.py
examples/research_projects/quantization-qdqbert/utils_qa.py
src/transformers/__init__.py
src/transformers/file_utils.py
src/transformers/models/__init__.py
src/transformers/models/auto/configuration_auto.py
src/transformers/models/auto/modeling_auto.py
src/transformers/models/auto/tokenization_auto.py
src/transformers/models/qdqbert/__init__.py
src/transformers/models/qdqbert/configuration_qdqbert.py
src/transformers/models/qdqbert/modeling_qdqbert.py
src/transformers/testing_utils.py
src/transformers/utils/dummy_pytorch_quantization_and_torch_objects.py
tests/test_modeling_qdqbert.py
==================
81fe8afaa;Nicolas Patry;2021-11-19 15:37:52 +0100;Adding support for `hidden_states` and `attentions` in unbatching (#14420)
support.
==

src/transformers/pipelines/base.py
tests/test_pipelines_common.py
==================
f25a9332e;Patrick von Platen;2021-11-19 15:35:06 +0100;[Generation] Allow `inputs_embeds` as an input (#14443)
* up

* finalize

* finalize

* finish

* Update src/transformers/generation_utils.py

* apply feedback
==

src/transformers/generation_utils.py
tests/test_generation_utils.py
==================
0490b9887;NielsRogge;2021-11-19 15:15:02 +0100;[ImageGPT] Small fixes (#14460)
* Add integration test

* Fix typo
==

README.md
README_ko.md
README_zh-hans.md
README_zh-hant.md
docs/source/imgs/ImageGPT.png
docs/source/index.rst
src/transformers/models/auto/modeling_auto.py
tests/test_modeling_imagegpt.py
utils/check_repo.py
==================
331c3d2aa;Lysandre Debut;2021-11-19 08:43:48 -0500;Add GitPython to quality tools (#14459)
* Update setup.py

* Update setup.py

* Update setup.py

* Remove GitPython install
==

.circleci/config.yml
setup.py
==================
efea0f868;Patrick von Platen;2021-11-18 23:42:02 +0100;[Speech Recognition] More examples
Add more XLS-R training runs to the official examples
==

examples/pytorch/speech-recognition/README.md
==================
72a6bf33c;Stas Bekman;2021-11-18 11:47:49 -0800;[Bert, et al] fix early device assignment (#14447)
* fix early device assignment

* more models
==

src/transformers/models/albert/modeling_albert.py
src/transformers/models/bert/modeling_bert.py
src/transformers/models/big_bird/modeling_big_bird.py
src/transformers/models/convbert/modeling_convbert.py
src/transformers/models/electra/modeling_electra.py
src/transformers/models/fnet/modeling_fnet.py
src/transformers/models/roberta/modeling_roberta.py
==================
83ef8bcac;Sylvain Gugger;2021-11-18 10:25:06 -0500;Fix finite IterableDataset test on multiple GPUs (#14445)

==

tests/test_trainer.py
==================
da36c557f;NielsRogge;2021-11-18 16:24:34 +0100;Add ImageGPT (#14240)
* First draft

* More improvements

* Improve conversion script

* Fix init weights for layer norm

* Fix correct model for conversion script

* Don't tie input and output embeddings

* Add print statements for debugging

* Add print statements for debugging

* Fix vocab size of model

* Improve documentation, remove fast tokenizer

* Add ImageGPTForImageClassification, improve docs

* Fix docs issue

* Set verbosity level back to info

* Improve tests

* Fix tests and add figure

* Delete tokenizer file

* Remove ImageGPTTokenizer from init files

* Remove ImageGPTLayer from init files

* Remove ImageGPT tokenizer from docs

* First draft of ImageGPTFeatureExtractor

* Fix typo

* Fix bug

* More improvements

* Apply suggestions from code review, add tests for feature extractor

* Fix layernorm

* Update save_pretrained method

* Fix issue

* Make all tests of ImageGPTFeatureExtractor pass

* Update code examples

* Rename model inputs to pixel_values

* Improve code examples

* Update init_weights to post_init

* Fix post_init
==

README.md
README_ko.md
README_zh-hans.md
README_zh-hant.md
docs/source/imgs/ImageGPT.png
docs/source/index.rst
docs/source/model_doc/imagegpt.rst
src/transformers/__init__.py
src/transformers/feature_extraction_utils.py
src/transformers/models/__init__.py
src/transformers/models/auto/configuration_auto.py
src/transformers/models/auto/modeling_auto.py
src/transformers/models/imagegpt/__init__.py
src/transformers/models/imagegpt/configuration_imagegpt.py
src/transformers/models/imagegpt/convert_imagegpt_original_tf2_to_pytorch.py
src/transformers/models/imagegpt/feature_extraction_imagegpt.py
src/transformers/models/imagegpt/modeling_imagegpt.py
src/transformers/utils/dummy_pt_objects.py
src/transformers/utils/dummy_vision_objects.py
tests/test_feature_extraction_imagegpt.py
tests/test_modeling_imagegpt.py
utils/check_repo.py
==================
d83b0e0c0;Sylvain Gugger;2021-11-18 08:38:09 -0500;Add a post init method to all models (#14431)
* Add a post init method to all models

* Fix tests

* Fix last tests

* Fix templates

* Add comment

* Forgot to save
==

src/transformers/modeling_utils.py
src/transformers/models/albert/modeling_albert.py
src/transformers/models/bart/modeling_bart.py
src/transformers/models/beit/modeling_beit.py
src/transformers/models/bert/modeling_bert.py
src/transformers/models/bert_generation/modeling_bert_generation.py
src/transformers/models/big_bird/modeling_big_bird.py
src/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py
src/transformers/models/blenderbot/modeling_blenderbot.py
src/transformers/models/blenderbot_small/modeling_blenderbot_small.py
src/transformers/models/canine/modeling_canine.py
src/transformers/models/clip/modeling_clip.py
src/transformers/models/convbert/modeling_convbert.py
src/transformers/models/ctrl/modeling_ctrl.py
src/transformers/models/deberta/modeling_deberta.py
src/transformers/models/deberta_v2/modeling_deberta_v2.py
src/transformers/models/deit/modeling_deit.py
src/transformers/models/detr/modeling_detr.py
src/transformers/models/distilbert/modeling_distilbert.py
src/transformers/models/dpr/modeling_dpr.py
src/transformers/models/electra/modeling_electra.py
src/transformers/models/flaubert/modeling_flaubert.py
src/transformers/models/fnet/modeling_fnet.py
src/transformers/models/fsmt/modeling_fsmt.py
src/transformers/models/funnel/modeling_funnel.py
src/transformers/models/gpt2/modeling_gpt2.py
src/transformers/models/gpt_neo/modeling_gpt_neo.py
src/transformers/models/gptj/modeling_gptj.py
src/transformers/models/hubert/modeling_hubert.py
src/transformers/models/ibert/modeling_ibert.py
src/transformers/models/layoutlm/modeling_layoutlm.py
src/transformers/models/layoutlmv2/modeling_layoutlmv2.py
src/transformers/models/led/modeling_led.py
src/transformers/models/longformer/modeling_longformer.py
src/transformers/models/luke/modeling_luke.py
src/transformers/models/lxmert/modeling_lxmert.py
src/transformers/models/m2m_100/modeling_m2m_100.py
src/transformers/models/marian/modeling_marian.py
src/transformers/models/mbart/modeling_mbart.py
src/transformers/models/megatron_bert/modeling_megatron_bert.py
src/transformers/models/mobilebert/modeling_mobilebert.py
src/transformers/models/mpnet/modeling_mpnet.py
src/transformers/models/openai/modeling_openai.py
src/transformers/models/pegasus/modeling_pegasus.py
src/transformers/models/prophetnet/modeling_prophetnet.py
src/transformers/models/reformer/modeling_reformer.py
src/transformers/models/rembert/modeling_rembert.py
src/transformers/models/retribert/modeling_retribert.py
src/transformers/models/roberta/modeling_roberta.py
src/transformers/models/roformer/modeling_roformer.py
src/transformers/models/segformer/modeling_segformer.py
src/transformers/models/sew/modeling_sew.py
src/transformers/models/sew_d/modeling_sew_d.py
src/transformers/models/speech_to_text/modeling_speech_to_text.py
src/transformers/models/speech_to_text_2/modeling_speech_to_text_2.py
src/transformers/models/splinter/modeling_splinter.py
src/transformers/models/squeezebert/modeling_squeezebert.py
src/transformers/models/t5/modeling_t5.py
src/transformers/models/tapas/modeling_tapas.py
src/transformers/models/transfo_xl/modeling_transfo_xl.py
src/transformers/models/trocr/modeling_trocr.py
src/transformers/models/unispeech/modeling_unispeech.py
src/transformers/models/unispeech_sat/modeling_unispeech_sat.py
src/transformers/models/visual_bert/modeling_visual_bert.py
src/transformers/models/vit/modeling_vit.py
src/transformers/models/wav2vec2/modeling_wav2vec2.py
src/transformers/models/xlm/modeling_xlm.py
src/transformers/models/xlnet/modeling_xlnet.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_{{cookiecutter.lowercase_modelname}}.py
tests/test_modeling_common.py
==================
08816de16;NielsRogge;2021-11-18 11:26:54 +0100;Fix code example (#14441)

==

src/transformers/models/encoder_decoder/modeling_encoder_decoder.py
==================
01f8e639d;William Held;2021-11-18 01:16:47 +0000;Recover Deleted XNLI Instructions (#14437)

==

examples/pytorch/text-classification/README.md
==================
1991da07f;N;2021-11-17 21:24:39 +0100;[WIP] Ensure TF model configs can be converted to proper JSON (#14415)
* test: make sure model configs are jsonifiable

* fix: return python dict instead of config object

* fix: accept pretrained config and use correct class

* Re-enabling slow tests and applying them to core models only

* Re-enabling slow tests and applying them to core models only

* Add new test file to fetcher

* Remove tooslow tests from test_modeling_tf_common.py

* make style

* Style fixes

* Style fixes

* Style fixes

* Style fixes

* Adding core tests to GPT2 and BART

* Removing unused imports

Co-authored-by: niklas.fruehauf <niklas.fruehauf@sovanta.com>
Co-authored-by: matt <rocketknight1@gmail.com>
==

src/transformers/modeling_tf_utils.py
tests/test_modeling_tf_bart.py
tests/test_modeling_tf_bert.py
tests/test_modeling_tf_common.py
tests/test_modeling_tf_core.py
tests/test_modeling_tf_gpt2.py
utils/tests_fetcher.py
==================
754202de4;Patrick von Platen;2021-11-17 19:02:33 +0100;[Bart] Fix docs (#14434)

==

docs/source/model_doc/bart.rst
==================
7544efc92;Antonio Carlos Falc√£o Petri;2021-11-17 14:37:21 -0300;[Gradient checkpoining] Update Wav2Vec scripts (#14036)
Co-authored-by: Stas Bekman <stas@stason.org>
==

examples/research_projects/jax-projects/wav2vec2/run_wav2vec2_pretrain_flax.py
==================
c6c075544;Lysandre;2021-11-17 11:39:12 -0500;Docs for version v4.12.5

==

.circleci/deploy.sh
docs/source/_static/js/custom.js
==================
a2864a50e;NielsRogge;2021-11-17 15:29:58 +0100;Improve semantic segmentation models (#14355)
* Improve tests

* Improve documentation

* Add ignore_index attribute

* Add semantic_ignore_index to BEiT model

* Add segmentation maps argument to BEiTFeatureExtractor

* Simplify SegformerFeatureExtractor and corresponding tests

* Improve tests

* Apply suggestions from code review

* Minor docs improvements

* Streamline segmentation map tests of SegFormer and BEiT

* Improve reduce_labels docs and test

* Fix code quality

* Fix code quality again
==

docs/source/model_doc/segformer.rst
src/transformers/models/beit/configuration_beit.py
src/transformers/models/beit/feature_extraction_beit.py
src/transformers/models/beit/modeling_beit.py
src/transformers/models/deit/feature_extraction_deit.py
src/transformers/models/segformer/configuration_segformer.py
src/transformers/models/segformer/feature_extraction_segformer.py
src/transformers/models/segformer/modeling_segformer.py
src/transformers/models/vit/feature_extraction_vit.py
tests/test_feature_extraction_beit.py
tests/test_feature_extraction_segformer.py
==================
700a748fe;Patrick von Platen;2021-11-17 14:38:56 +0100;[Wav2Vec2] Add New Wav2Vec2 Translation (#14392)
* add new wav2vec2 translation

* correct

* up

* add tests

* correct end copy

* correct more

* up

* correct unispeech sat

* finish

* finalize

* finish

* up
==

src/transformers/models/speech_encoder_decoder/convert_mbart_wav2vec2_seq2seq_original_to_pytorch.py
src/transformers/models/speech_encoder_decoder/modeling_speech_encoder_decoder.py
src/transformers/models/unispeech/modeling_unispeech.py
src/transformers/models/unispeech_sat/modeling_unispeech_sat.py
src/transformers/models/wav2vec2/configuration_wav2vec2.py
src/transformers/models/wav2vec2/modeling_wav2vec2.py
tests/test_modeling_wav2vec2.py
tests/test_pipelines_automatic_speech_recognition.py
==================
b567510cf;Sylvain Gugger;2021-11-16 18:58:07 -0500;Debug doc (#14424)
* Create branch for tests

* Pin first upgrade

* Really pin

* Polish fix
==

setup.py
src/transformers/dependency_versions_table.py
==================
888fb2115;Lysandre;2021-11-16 17:40:58 -0500;Docs for v4.12.4

==

.circleci/deploy.sh
docs/source/_static/js/custom.js
==================
a33168aa7;Valentin;2021-11-16 22:50:04 +0100;Avoid looping when data exhausted (#14413)
* stop training when a finite IterableDataset is exhausted

when using an iterable dataset num_epochs is set to
sys.maxsize to make sure all data is consumed
likewise we want to set max_steps high enough
but still stop when all data is consumed

(cherry picked from commit 6f0e1d6363153da9051e93acffe1cbab3a3f3b12)

* fix typo flase -> false

* add test for stopping training on exhausted finite iterable dataset

* remove redundant gradient_accumulation_steps

* run make style

reformat training_args docstring
==

src/transformers/trainer.py
src/transformers/training_args.py
tests/test_trainer.py
==================
3e8d17e66;Sylvain Gugger;2021-11-16 09:24:40 -0500;Add forward method to dummy models (#14419)
* Add forward method to dummy models

* Fix quality
==

src/transformers/utils/dummy_flax_objects.py
src/transformers/utils/dummy_pt_objects.py
src/transformers/utils/dummy_scatter_objects.py
src/transformers/utils/dummy_tf_objects.py
src/transformers/utils/dummy_timm_and_vision_objects.py
utils/check_dummies.py
==================
040fd4716;Sylvain Gugger;2021-11-16 08:58:42 -0500;Fix gradient_checkpointing backward compatibility (#14408)
* Fix gradient_checkpointing backward compatibility

* Remove needless line

* make sure mask prob is big enough and length small enough

* Fix tests

Co-authored-by: patrickvonplaten <patrick.v.platen@gmail.com>
==

src/transformers/modeling_utils.py
src/transformers/models/detr/modeling_detr.py
src/transformers/models/layoutlmv2/modeling_layoutlmv2.py
tests/test_modeling_beit.py
tests/test_modeling_common.py
tests/test_modeling_deit.py
tests/test_modeling_unispeech_sat.py
tests/test_modeling_wav2vec2.py
==================
1cc453d33;Lysandre Debut;2021-11-15 16:38:02 -0500;Allow per-version configurations (#14344)
* Allow per-version configurations

* Update tests/test_configuration_common.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update tests/test_configuration_common.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

src/transformers/configuration_utils.py
tests/test_configuration_common.py
==================
76d0d41e5;Patrick von Platen;2021-11-15 21:03:10 +0100;[Wav2Vec2] Make sure that gradient checkpointing is only run if needed (#14407)
* [Wav2Vec2] Make sure that gradient checkpointing is only run if needed

* make fix-copies
==

src/transformers/models/hubert/modeling_hubert.py
src/transformers/models/sew/modeling_sew.py
src/transformers/models/sew_d/modeling_sew_d.py
src/transformers/models/unispeech/modeling_unispeech.py
src/transformers/models/unispeech_sat/modeling_unispeech_sat.py
src/transformers/models/wav2vec2/modeling_wav2vec2.py
==================
9fd937ead;Eldar Kurtic;2021-11-15 19:25:10 +0100;Replace BertLayerNorm with LayerNorm (#14385)
Running Movement pruning experiments with the newest HuggingFace would crash due to non-existing BertLayerNorm.
==

examples/research_projects/movement-pruning/emmental/modeling_bert_masked.py
==================
a67d47b40;Yih-Dar;2021-11-15 17:48:40 +0100;Fix weight loading issue (#14016)
Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>
==

src/transformers/models/encoder_decoder/modeling_tf_encoder_decoder.py
tests/test_modeling_tf_encoder_decoder.py
==================
74e6111ba;NielsRogge;2021-11-15 17:35:33 +0100;Fix test and docs (#14399)

==

src/transformers/models/vit/modeling_tf_vit.py
tests/test_modeling_tf_vit.py
==================
4ce74edf5;Patrick von Platen;2021-11-15 16:34:11 +0100;[Speech2Text2] Enable tokenizers (#14390)
* [Speech2Text2] Enable tokenizers

* minor fix

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

docs/source/model_doc/speech_to_text_2.rst
src/transformers/models/speech_to_text_2/tokenization_speech_to_text_2.py
tests/test_tokenization_speech_to_text_2.py
==================
267867e85;Matt;2021-11-15 13:45:51 +0000;Quick fix to TF summarization example (#14401)

==

examples/tensorflow/summarization/run_summarization.py
==================
29dfb2dbb;Stas Bekman;2021-11-14 17:19:15 -0800;[doc] performance and parallelism updates (#14391)
* [doc] performance and parallelism doc update

* improve

* improve
==

docs/source/parallelism.md
docs/source/performance.md
==================
790cdc2e5;nbertagnolli;2021-11-13 19:34:34 -0700;Raise exceptions instead of using asserts  in modeling_openai #12789 (#14386)
* Raise exceptions instead of using asserts for control flow in modeling_openai #12789

* reformatted file
==

src/transformers/models/openai/modeling_openai.py
==================
2e60276b3;Suraj Patil;2021-11-13 20:57:12 +0530;[M2M100Tokenizer] fix _build_translation_inputs (#14382)
* add return_tensors paramter

* fix test

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* style

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

src/transformers/models/m2m_100/tokenization_m2m_100.py
tests/test_tokenization_m2m_100.py
==================
316593040;Suraj Patil;2021-11-13 14:21:58 +0530;support wmt21 tokenizer in m2m100 tokenizer (#14376)

==

src/transformers/models/m2m_100/tokenization_m2m_100.py
==================
280a811ec;Li-Huai (Allan) Lin;2021-11-13 02:46:40 +0800;Use `AlbertConverter` for FNet instead of using FNet's own converter (#14365)
* Add normalizer to FNetConverter

* Style

* Directly use AlbertConverter
==

src/transformers/convert_slow_tokenizer.py
==================
55f49c5f4;Patrick von Platen;2021-11-12 16:35:57 +0100;[Wav2Vec2 Example] Improve fine-tuning script (#14373)
* improve some stuff

* finish

* correct last
==

examples/pytorch/speech-recognition/run_speech_recognition_ctc.py
==================
21546e59a;Suraj Patil;2021-11-12 15:56:41 +0530;fix docs (#14377)

==

src/transformers/modeling_flax_utils.py
==================
ed5d15518;Nicolas Patry;2021-11-12 09:20:40 +0100; Adding support for raw python `generator` in addition to `Dataset` for pipelines (#14352)
* Adding support for raw python `generator` in addition to `Dataset`

The main goal is to ease the create of streaming data to the pipe.

`Dataset` is more involved and pytorch specific.

This PR, provides a way to use a python iterator too.
This enabled #14250 but can be proposed as a standalone PR.

```python
from transformers import pipeline

def read_data(filename):
    with open(filename, 'r') as f:
        for line in f:
            yield f

pipe = pipeline("text-classification")
for classified in pipe(read_data("large_file.txt")):
    print("Success ! ", classified)
```

The main caveat of this, is the interaction with `DataLoader` with
`num_workers>1`. When you have multiple workers, each receive a copy
of the generator (like `IterableDataset`). That means the naive Iterator
will fail since all workers iterate on all items of the generator.

There are ways to do clever "skipping", but it could be bad still
because all workers still do have to pass through all items of the
generator (they just ignore items they don't handle), depending on
the case it might be bad.

Using `num_workers=1` is the simplest fix and if the cost of loading
your data is small enough should be good enough. In the above example
trying to do smart tricks to skip some lines is unlikely to be a net
positive for instance.

If there are better ways to do "jumps" on some data, then using
`Dataset` is more advised (since then differents workers can just jump
themselves).

* Adding iterator support for `tf` too.
==

src/transformers/pipelines/base.py
src/transformers/pipelines/text_classification.py
tests/test_pipelines_common.py
==================
77262ef75;Stas Bekman;2021-11-11 08:50:21 -0800;fix --gradient_checkpointing (#13964)

==

examples/research_projects/wav2vec2/run_asr.py
examples/research_projects/wav2vec2/run_common_voice.py
examples/research_projects/wav2vec2/run_pretrain.py
==================
3d607df8f;Suraj Patil;2021-11-11 21:20:49 +0530;fix loading flax bf16 weights in pt (#14369)
* fix loading flax bf16 weights in pt

* fix clip test

* fix t5 test

* add logging statement

* Update src/transformers/modeling_flax_pytorch_utils.py

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* switch back to native any

* fix check for bf16 weights

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
==

src/transformers/modeling_flax_pytorch_utils.py
tests/test_modeling_flax_clip.py
tests/test_modeling_flax_common.py
tests/test_modeling_flax_t5.py
==================
7f20bf0d4;Matt;2021-11-11 15:34:00 +0000;Fixing requirements for TF LM models and use correct model mappings (#14372)
* Fixing requirements for TF LM models and use correct model mappings

* make style
==

examples/tensorflow/language-modeling/requirements.txt
examples/tensorflow/language-modeling/run_clm.py
examples/tensorflow/language-modeling/run_mlm.py
==================
4c35c8d89;Matt;2021-11-11 14:21:50 +0000;Experimenting with adding proper get_config() and from_config() methods (#14361)
* Experimenting with adding proper get_config() and from_config() methods

* Adding a test for get/from config

* Fix test for get/from config
==

src/transformers/modeling_tf_utils.py
tests/test_modeling_tf_common.py
==================
b1dbdf22e;Suraj Patil;2021-11-11 17:16:24 +0530;pass params to encode (#14370)

==

src/transformers/generation_flax_utils.py
==================
e92190c0f;Suraj Patil;2021-11-11 14:45:20 +0530;Fix Flax params dtype (#13098)
* fix inits

* fix embed dtype

* fix embed dtype

* add test to check default dtype

* quality

* add type conversion methods for flax models

* more robust casting

* cast sinusoidal positions

* update pegasus

* update albert

* update test

* make sure dtype is passed to every module

* style

* fix electra dense

* fix t5

* quality

* add more tests

* better name

* use the dtype for lm head computation

* fix albert

* style

* fix albert embed dtype

* more tests

* fix vision enc-dec

* cleanup

* fix embed dtype pegasus

* fix default param test

* doc

* update template

* fix final_logits_bias dtype

* Apply suggestions from code review

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* fix doc

* fix doc

* add detailed docstring for dtype parameter

* remove un-necessary import

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
==

examples/research_projects/jax-projects/hybrid_clip/modeling_hybrid_clip.py
src/transformers/modeling_flax_utils.py
src/transformers/models/albert/modeling_flax_albert.py
src/transformers/models/bart/modeling_flax_bart.py
src/transformers/models/beit/modeling_flax_beit.py
src/transformers/models/bert/modeling_flax_bert.py
src/transformers/models/big_bird/modeling_flax_big_bird.py
src/transformers/models/clip/modeling_flax_clip.py
src/transformers/models/distilbert/modeling_flax_distilbert.py
src/transformers/models/electra/modeling_flax_electra.py
src/transformers/models/encoder_decoder/modeling_flax_encoder_decoder.py
src/transformers/models/gpt2/modeling_flax_gpt2.py
src/transformers/models/gpt_neo/modeling_flax_gpt_neo.py
src/transformers/models/marian/modeling_flax_marian.py
src/transformers/models/mbart/modeling_flax_mbart.py
src/transformers/models/pegasus/modeling_flax_pegasus.py
src/transformers/models/roberta/modeling_flax_roberta.py
src/transformers/models/t5/modeling_flax_t5.py
src/transformers/models/vision_encoder_decoder/modeling_flax_vision_encoder_decoder.py
src/transformers/models/vit/modeling_flax_vit.py
src/transformers/models/wav2vec2/modeling_flax_wav2vec2.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_flax_{{cookiecutter.lowercase_modelname}}.py
tests/test_modeling_flax_common.py
==================
1c76a5161;Stas Bekman;2021-11-10 19:11:45 -0800;solve the port conflict (#14362)

==

tests/deepspeed/test_deepspeed.py
==================
9e37c5cdf;Li-Huai (Allan) Lin;2021-11-11 04:34:52 +0800;Fix list index out of range when padding nested empty lists (#13876)
* Fix index out of range when padding

* Apply suggestions from code review

* Style
==

src/transformers/tokenization_utils_base.py
tests/test_tokenization_common.py
==================
bec02ff20;Chang Wang;2021-11-10 20:25:41 +0800;enhance rewrite state_dict missing _metadata (#14348)

==

src/transformers/modeling_utils.py
==================
2b0d9389f;Ella Charlaix;2021-11-10 12:49:43 +0100;Add notebook INC quantization for text classification tasks (#14293)
* Add notebook applying Intel Neural Compressor quantization for text classification tasks

* Add Optimum notebooks section
==

notebooks/README.md
==================
ea163d094;Li-Huai (Allan) Lin;2021-11-10 18:16:45 +0800;Fix fast tokenization problems (#13930)
* Fix albert mask token tokenization.

* Ensure special tokans sanitized.

* Style

* Fix

* Apply suggestions from code review
==

src/transformers/models/albert/tokenization_albert.py
src/transformers/models/albert/tokenization_albert_fast.py
==================
5c153079e;Nicolas Patry;2021-11-10 10:18:35 +0100;Adding some quality of life for `pipeline` function. (#14322)
* Adding some quality of life for `pipeline` function.

* Update docs/source/main_classes/pipelines.rst

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/pipelines/__init__.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Improve the tests.

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

docs/source/main_classes/pipelines.rst
src/transformers/pipelines/__init__.py
tests/test_pipelines_common.py
==================
321eb5622;Elad Segal;2021-11-10 05:23:08 +0200;`BatchFeature`: Convert `List[np.ndarray]` to `np.ndarray` before converting to pytorch tensors (#14306)
* update

* style fix

* retrigger checks

* check first element

* fix syntax error

* Update src/transformers/feature_extraction_utils.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* remove import

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

src/transformers/feature_extraction_utils.py
==================
46d0cdae4;Sylvain Gugger;2021-11-09 18:49:29 -0500;Support for TF >= 2.7 (#14345)

==

setup.py
src/transformers/dependency_versions_table.py
src/transformers/models/roformer/modeling_tf_roformer.py
==================
e81d8d7fa;Patrick von Platen;2021-11-09 20:26:58 +0100;[Bert2Bert] allow bert2bert + relative embeddings (#14324)
* [Bert2Bert] allow bert2bert + relative embeddings

* up

* Update README_ko.md

* up

* up
==

src/transformers/models/bert/modeling_bert.py
src/transformers/models/electra/modeling_electra.py
src/transformers/models/layoutlm/modeling_layoutlm.py
src/transformers/models/megatron_bert/modeling_megatron_bert.py
src/transformers/models/rembert/modeling_rembert.py
src/transformers/models/roberta/modeling_roberta.py
src/transformers/models/roformer/modeling_roformer.py
src/transformers/models/splinter/modeling_splinter.py
src/transformers/models/tapas/modeling_tapas.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_{{cookiecutter.lowercase_modelname}}.py
tests/test_modeling_encoder_decoder.py
==================
e4d8f517b;Steven Liu;2021-11-09 11:12:50 -0800;Rewrite guides for fine-tuning with Datasets (#13923)
* rewrite guides for fine-tuning with datasets

* simple qa code example

* use anonymous rST links

* style
==

docs/source/custom_datasets.rst
==================
85a4bda4f;Suraj Patil;2021-11-09 22:15:22 +0530;bump flax version (#14343)

==

examples/flax/language-modeling/requirements.txt
examples/flax/question-answering/requirements.txt
examples/flax/summarization/requirements.txt
examples/flax/text-classification/requirements.txt
examples/flax/token-classification/requirements.txt
examples/flax/vision/requirements.txt
examples/research_projects/jax-projects/hybrid_clip/requirements.txt
setup.py
src/transformers/dependency_versions_table.py
==================
babd0b9a5;Yih-Dar;2021-11-09 17:30:17 +0100;remove test_model_various_embeddings (#14341)
Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>
==

tests/test_modeling_tf_layoutlm.py
==================
4f24058c5;karthikrangasai;2021-11-09 18:34:23 +0530;Update Seq2Seq QA example script to use SQuAD metric. (#14335)
* Update postporcessing accordingly to use SQuAD metric.

* Update assets accordingly based on SQuAD metrics.

* Fix function naming error.
==

examples/pytorch/question-answering/run_seq2seq_qa.py
examples/pytorch/question-answering/trainer_seq2seq_qa.py
examples/pytorch/test_examples.py
==================
be4a6c64d;Yih-Dar;2021-11-09 13:54:37 +0100;Add TFViTModel (#13778)
* Start the work for TFViTModel

* Convert to TF code - need to check in the follow up commits

* Clean up model code

* Expose TFViTModel

* make style

* make quality

* Add test

* make style & quality

* Fix some imports

* fix wrong usage - *kwargs => ** kwargs

* Fix Conv2D weight loading (PT->TF) issue

* Add tests for images with different sizes + fix model

* Fix some common tests for TFViTModel

* Use inputs instead of input_ids in test_compile_tf_model

* Add a comment about transpose and Conv2D in convert_tf_weight_name_to_pt_weight_name

* Avoid transpose in TFViT call

* Fix Conv2D issue in load_tf2_weights_in_pytorch_model

* Use tf.keras.layers.Conv2D instead of tf.nn.conv2d

* Using simpler heuristic to detect Conv2D layer

* Change convert_tf_weight_name_to_pt_weight_name to return TransposeType

* Check tf_weight_shape is not None before using it

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* fix missing comma

* fix input dtype

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

docs/source/index.rst
docs/source/model_doc/auto.rst
docs/source/model_doc/vit.rst
src/transformers/__init__.py
src/transformers/modeling_tf_pytorch_utils.py
src/transformers/models/auto/__init__.py
src/transformers/models/auto/modeling_tf_auto.py
src/transformers/models/vit/__init__.py
src/transformers/models/vit/modeling_tf_vit.py
src/transformers/utils/dummy_tf_objects.py
tests/test_modeling_common.py
tests/test_modeling_tf_common.py
tests/test_modeling_tf_vit.py
==================
6326aa4bf;Apoorv Garg;2021-11-09 18:19:53 +0530;Correct order of overflowing tokens for LayoutLmV2 tokenizer (#13495)
* correct order of overflowing tokens for LayoutLmV2 tokenizer

* test to check order of overflowing_tokens for a seq of input_ids

* fix up quality

* added suggested changes

* check that tests the bbox sequence

* pair_input test added

* pass quality test

* check bbox sequence added

* unittest method

* comments added

* add overflowing bbox test

* improved "seq_1"

Co-authored-by: SaulLu <55560583+SaulLu@users.noreply.github.com>

* improve code quality

Co-authored-by: SaulLu <lucilesaul.com@gmail.com>
Co-authored-by: SaulLu <55560583+SaulLu@users.noreply.github.com>
==

src/transformers/models/layoutlmv2/tokenization_layoutlmv2.py
src/transformers/tokenization_utils_base.py
tests/test_tokenization_layoutlmv2.py
==================
95b3ec3bc;Yih-Dar;2021-11-09 10:44:28 +0100;Add FlaxVisionEncoderDecoderModel (#13359)
* Start the work on FlaxVisionEncoderDecoderModel

* Add FlaxVisionEncoderDecoderModel

* Add VisionEncoderDecoderConfig

* Make FlaxVisionEncoderDecoderModel visible to transformers

* Add test

* Fix wrong getattr usage

* Fix tests

* Add FlaxAutoModelForVision2Seq

* Expose FLAX_MODEL_FOR_VISION_2_SEQ_MAPPING

* clean-up

* add integration test

* update expected logits

* update expected scores

* Add ViT2GPT2ModelIntegrationTest + some cleaning

* Add projection layer + PT/Flax equivalence tests

* Fix import

* minor changes

* make test slow again

* Apply suggestions

* Add modeling_flax_vision_encoder_decoder to _ignore_modules in get_model_modules()

* fix copies

* Apply suggestions from code review

Co-authored-by: Suraj Patil <surajp815@gmail.com>

* split long strings in multiple lines

* decoder_input_ids can't be None

* Add back test_configuration_tie

* Remove attention_mask parameter

* fix test - encoder_last_hidden_state should be encoder_outputs.last_hidden_state instead of the projected vector

* Apply suggestions from code review

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* Remove more encoder_attention_mask

* remove encoder_attention_mask when calling self.decode (in FlaxVisionEncoderDecoderModule)

* Fix style + pass 1s instead of None as encoder_attention_mask

* fix init_weights

* pass None for encoder_attention_mask

* pass 1s instead of None as encoder_attention_mask

* Fix doc style

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>
Co-authored-by: Suraj Patil <surajp815@gmail.com>
Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
==

docs/source/index.rst
docs/source/model_doc/auto.rst
docs/source/model_doc/visionencoderdecoder.rst
src/transformers/__init__.py
src/transformers/models/auto/__init__.py
src/transformers/models/auto/configuration_auto.py
src/transformers/models/auto/modeling_auto.py
src/transformers/models/auto/modeling_flax_auto.py
src/transformers/models/vision_encoder_decoder/__init__.py
src/transformers/models/vision_encoder_decoder/configuration_vision_encoder_decoder.py
src/transformers/models/vision_encoder_decoder/modeling_flax_vision_encoder_decoder.py
src/transformers/models/vision_encoder_decoder/modeling_vision_encoder_decoder.py
src/transformers/utils/dummy_flax_objects.py
src/transformers/utils/dummy_pt_objects.py
tests/test_modeling_flax_encoder_decoder.py
tests/test_modeling_flax_vision_encoder_decoder.py
tests/test_modeling_vision_encoder_decoder.py
utils/check_repo.py
==================
a50301227;Reza Yazdani;2021-11-08 18:00:05 -0800;Small change to Wav2Vec2 model to support Tensor-Parallelism with DeepSpeed (#14298)
* minor modification to the wav2vec2 modeling file to support tensor-parallelism with DeepSpeed on this HuggingFace model

* refine the comments

* synch changes

* fix comments

* refine comments

* fix format
==

src/transformers/models/bart/modeling_bart.py
src/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py
src/transformers/models/blenderbot/modeling_blenderbot.py
src/transformers/models/blenderbot_small/modeling_blenderbot_small.py
src/transformers/models/hubert/modeling_hubert.py
src/transformers/models/m2m_100/modeling_m2m_100.py
src/transformers/models/marian/modeling_marian.py
src/transformers/models/mbart/modeling_mbart.py
src/transformers/models/pegasus/modeling_pegasus.py
src/transformers/models/sew/modeling_sew.py
src/transformers/models/speech_to_text/modeling_speech_to_text.py
src/transformers/models/speech_to_text_2/modeling_speech_to_text_2.py
src/transformers/models/unispeech/modeling_unispeech.py
src/transformers/models/unispeech_sat/modeling_unispeech_sat.py
src/transformers/models/wav2vec2/modeling_wav2vec2.py
==================
d0e96c6de;Jeff Rasley;2021-11-08 12:40:29 -0800;[deepspeed] Enable multiple test runs on single box, defer to DS_TEST_PORT if set (#14331)
* defer to DS_TEST_PORT if set

* style

Co-authored-by: Stas Bekman <stas@stason.org>
==

tests/deepspeed/test_deepspeed.py
tests/deepspeed/test_model_zoo.py
==================
dfb00bf64;Sylvain Gugger;2021-11-08 15:28:25 -0500;Expand dynamic supported objects to configs and tokenizers (#14296)
* Dynamic configs

* Add config test

* Better tests

* Add tokenizer and test

* Add to from_config

* With save
==

src/transformers/models/auto/auto_factory.py
src/transformers/models/auto/configuration_auto.py
src/transformers/models/auto/tokenization_auto.py
src/transformers/tokenization_utils_base.py
tests/test_configuration_common.py
tests/test_modeling_common.py
tests/test_tokenization_common.py
==================
de635af3f;nbertagnolli;2021-11-08 08:56:44 -0700;Changed relative imports to absolute to allow convert_graph_to_onnx.py to run as a script. (#14325)
* Changed relative imports to absolute to allow convert_graph_to_onnx.py to be run as a script

* isorted code
==

src/transformers/onnx/convert.py
==================
a3ded170e;Nicolas Patry;2021-11-08 16:22:28 +0100;Fixing mutable default argument in `pipeline`. (#14316)
* Fixing mutable default argument.

* XX.

* Revert "XX."

This reverts commit 61d4bb333f6d39a7fbe31d161b8bd14787ceec2e.
==

src/transformers/pipelines/__init__.py
==================
9b78b070e;Nicolas Patry;2021-11-08 14:28:26 +0100;Fixing tests on master. (#14317)
* Fixing tests on master.

* Better fix.

* Lxmert doesn't have feature extractor but is bimodal.
==

tests/test_pipelines_feature_extraction.py
==================
df1f94eb4;Anton Lozhkov;2021-11-08 15:58:28 +0300;[TFWav2Vec2Model] Fix input shapes in TFWav2Vec2WeightNormConv1D (#14319)
* Add paddings to input shapes

* Add padding comment
==

src/transformers/models/hubert/modeling_tf_hubert.py
src/transformers/models/wav2vec2/modeling_tf_wav2vec2.py
==================
e30078b54;Anton Lozhkov;2021-11-08 14:15:56 +0300;[Tests] Update audio classification tests to support torch 1.10 (#14318)

==

tests/test_pipelines_audio_classification.py
==================
b48faae36;Patrick von Platen;2021-11-08 11:42:34 +0100;[Marian Conversion] Fix eos_token_id conversion in conversion script (#14320)

==

src/transformers/models/marian/convert_marian_to_pytorch.py
==================
c016dbdbd;Junbum Lee;2021-11-06 23:33:47 +0900;Fix execution PATH for PPLM Example (#14287)

==

examples/research_projects/pplm/README.md
==================
34307bb35;NielsRogge;2021-11-06 15:08:58 +0100;Fix tests (#14289)

==

tests/test_modeling_beit.py
tests/test_modeling_segformer.py
==================
24b30d4d2;Nicolas Patry;2021-11-06 15:04:30 +0100;Handle long answer needs to be updated. (#14279)
`start_` and `end_` tensors now contain a batch_size at this point.
==

src/transformers/pipelines/question_answering.py
tests/test_pipelines_question_answering.py
==================
843c326ee;Xing Han Lu;2021-11-06 09:41:02 -0400;Update dpr.rst (#14300)

==

docs/source/model_doc/dpr.rst
==================
08a5f5756;Sylvain Gugger;2021-11-05 18:58:51 -0400;Add new LFS prune API (#14294)

==

examples/pytorch/language-modeling/run_clm_no_trainer.py
examples/pytorch/language-modeling/run_mlm_no_trainer.py
examples/pytorch/multiple-choice/run_swag_no_trainer.py
examples/pytorch/question-answering/run_qa_beam_search_no_trainer.py
examples/pytorch/question-answering/run_qa_no_trainer.py
examples/pytorch/speech-pretraining/run_wav2vec2_pretraining_no_trainer.py
examples/pytorch/summarization/run_summarization_no_trainer.py
examples/pytorch/text-classification/run_glue_no_trainer.py
examples/pytorch/token-classification/run_ner_no_trainer.py
examples/pytorch/translation/run_translation_no_trainer.py
src/transformers/trainer.py
==================
4be78c22c;Patrick von Platen;2021-11-05 14:09:57 +0100;[Hubert Docs] Make sure example uses a fine-tuned model (#14291)

==

src/transformers/models/hubert/modeling_hubert.py
==================
a14d62b0b;Sylvain Gugger;2021-11-04 21:15:42 -0400;Pin TF until tests are fixed (#14283)
* Pin TF until tests are fixed

* Also pin TF CPU
==

setup.py
src/transformers/dependency_versions_table.py
==================
b90a48f65;Matt;2021-11-04 17:58:28 +0000;Removing Keras version pinning (#14280)
* Removing Keras version pinning

* make fixup
==

setup.py
src/transformers/dependency_versions_table.py
==================
fd8136fa7;Chang Wang;2021-11-04 22:13:23 +0800;improve rewrite state_dict missing _metadata (#14276)

==

src/transformers/modeling_utils.py
==================
d29baf69b;Nicolas Patry;2021-11-04 14:47:52 +0100;Fixing mishandling of `ignore_labels`. (#14274)
Fixes #14272
==

src/transformers/pipelines/token_classification.py
tests/test_pipelines_token_classification.py
==================
68427c9be;Nicolas Patry;2021-11-04 09:49:55 +0100;Fixing slow pipeline tests (#14260)
* Fiixng slow pipeline tests

* Remove the image-segmentaiton override.

* Fixing clamping only in training.

* Wav2vec2.

* Remove last mention of `no_grad`.

* Fixing copies.

* Rename.
==

src/transformers/models/detr/modeling_detr.py
src/transformers/models/unispeech/modeling_unispeech.py
src/transformers/models/unispeech_sat/modeling_unispeech_sat.py
src/transformers/models/wav2vec2/modeling_wav2vec2.py
src/transformers/pipelines/image_segmentation.py
src/transformers/pipelines/table_question_answering.py
tests/test_pipelines_audio_classification.py
==================
1a674ce67;Sylvain Gugger;2021-11-03 17:45:41 -0400;Add more instructions to the release guide (#14263)
* Add more instructions to the release guide

* Apply suggestions from code review

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* Address review comment

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
==

setup.py
==================
f0d6e952c;Sylvain Gugger;2021-11-03 17:43:19 -0400;Quality explain (#14264)
* Start PR doc

* Cleanup the quality checks and document them

* Add reference in the contributing guide

* Apply suggestions from code review

Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>

* Rename file as per review suggestion

Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>
==

.circleci/config.yml
CONTRIBUTING.md
Makefile
docs/source/index.rst
docs/source/pr_checks.md
utils/link_tester.py
==================
a1c15ea85;Sylvain Gugger;2021-11-03 15:03:09 -0400;Pin Keras cause they messed their release (#14262)
* Pin Keras cause they messed their release

* Put != instead of <

* Try this way

* Back to the beginning but more agressive
==

setup.py
src/transformers/dependency_versions_table.py
==================
114924318;Nicolas Patry;2021-11-03 19:28:57 +0100;Fixing typo in error message. (#14226)

==

src/transformers/models/layoutlmv2/tokenization_layoutlmv2.py
src/transformers/models/layoutlmv2/tokenization_layoutlmv2_fast.py
==================
2c8957fee;Dan Shirron;2021-11-03 18:20:48 +0200;Fix of issue #13327: Wrong weight initialization for TF t5 model (#14241)
* Fix of issue #13327: Wrong weight initialization for TF t5 model

* run black formatter

* fix typo

* remove my name tag from comments

Co-authored-by: Shirron <dan.shirron@intel.com>
==

src/transformers/models/t5/modeling_tf_t5.py
==================
dec759e7e;Nicolas Patry;2021-11-03 15:48:00 +0100;Adding support for `truncation` parameter on `feature-extraction` pipeline. (#14193)
* Adding support for `truncation` parameter on `feature-extraction`
pipeline.

Fixes #14183

* Fixing tests on ibert, longformer, and roberta.

* Rebase fix.
==

src/transformers/pipelines/feature_extraction.py
tests/test_pipelines_common.py
tests/test_pipelines_feature_extraction.py
==================
27b1516d3;Dean Wyatte;2021-11-03 08:36:41 -0600;minimal fixes to run DataCollatorForWholeWordMask with return_tensors="np" and return_tensors="tf" (#13891)
* minimal fixes to run DataCollatorForWholeWordMask with return_tensors="np" and return_tensors="tf"

* more consinstent implementation for numpy_mask_tokens
==

src/transformers/data/data_collator.py
tests/test_data_collator.py
==================
671569ddf;Mishig Davaadorj;2021-11-03 14:53:05 +0100;Put `load_image` function in `image_utils.py` & fix image rotation issue (#14062)
* Fix img load rotation

* Add `load_image` to `image_utils.py`

* Implement LoadImageTester

* Use hf-internal-testing dataset

* Add img utils comments

* Refactor LoadImageTester

* Import load_image under is_vision_available
==

src/transformers/image_utils.py
src/transformers/pipelines/image_classification.py
src/transformers/pipelines/image_segmentation.py
src/transformers/pipelines/object_detection.py
tests/test_image_utils.py
==================
89766b3d4;Patrick von Platen;2021-11-03 11:31:40 +0100;up (#14258)

==

src/transformers/models/wav2vec2/convert_wav2vec2_original_pytorch_checkpoint_to_pytorch.py
==================
bd21ed409;Yih-Dar;2021-11-03 09:54:34 +0100;Add cross attentions to TFGPT2Model (#14038)
* Add cross attentions to TFGPT2Model

* change to is_pt_tf_cross_test

* A minor correction to a comment

* Remove n_ctx when creating self.crossattention

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>
Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
==

src/transformers/models/ctrl/modeling_tf_ctrl.py
src/transformers/models/gpt2/modeling_tf_gpt2.py
tests/test_modeling_tf_encoder_decoder.py
tests/test_modeling_tf_gpt2.py
==================
5f789a687;NielsRogge;2021-11-03 08:59:44 +0100;Add LayoutXLMProcessor (and LayoutXLMTokenizer, LayoutXLMTokenizerFast) (#14115)
* Add LayoutXLMTokenizer and LayoutXLMTokenizerFast

* Fix styling issues

* Fix more styling issues

* Fix more styling issues

* Fix docstring

* Fix unit tests

* Fix docs

* Fix unit tests

* Fix typos and styling issues

* Fix styling issues

* Fix docstring

* Make all tests of test_tokenization_layoutxlm pass

* Add LayoutXLMProcessor

* Make fixup

* Make all LayoutXLMProcessor tests pass

* Minor fixes

* Leave LayoutLMv2Processor tests unchanged

* Fix code quality

* Move LayoutXLM tokenizers and processor to separate folder

* Fix code quality

* Apply suggestions from code review

* Replace assertions by value errors

* Remove methods from fast tokenizer

Co-authored-by: King Yiu Suen <kingyiusuen@gmail.com>
==

docs/source/model_doc/layoutxlm.rst
src/transformers/__init__.py
src/transformers/convert_slow_tokenizer.py
src/transformers/models/__init__.py
src/transformers/models/auto/tokenization_auto.py
src/transformers/models/layoutxlm/__init__.py
src/transformers/models/layoutxlm/processing_layoutxlm.py
src/transformers/models/layoutxlm/tokenization_layoutxlm.py
src/transformers/models/layoutxlm/tokenization_layoutxlm_fast.py
src/transformers/utils/dummy_sentencepiece_objects.py
src/transformers/utils/dummy_tokenizers_objects.py
src/transformers/utils/dummy_vision_objects.py
tests/test_processor_layoutxlm.py
tests/test_tokenization_layoutxlm.py
==================
558f8543b;Sylvain Gugger;2021-11-02 18:58:42 -0400;Update Transformers to huggingface_hub >= 0.1.0 (#14251)
* Update Transformers to huggingface_hub >= 0.1.0

* Forgot to save...

* Style

* Fix test
==

docs/source/model_doc/marian.rst
examples/research_projects/seq2seq-distillation/_test_seq2seq_examples.py
setup.py
src/transformers/commands/user.py
src/transformers/dependency_versions_table.py
src/transformers/file_utils.py
src/transformers/modelcard.py
src/transformers/models/marian/convert_marian_to_pytorch.py
tests/test_configuration_common.py
tests/test_modeling_common.py
tests/test_modeling_flax_common.py
tests/test_modeling_marian.py
tests/test_modeling_tf_common.py
tests/test_tokenization_common.py
tests/test_trainer.py
==================
519a677e8;lumliolum;2021-11-02 22:59:14 +0530;Added Beit model output class (#14133)
* add Beit model ouput class

* inherting from BaseModelOuputWithPooling

* updated docs if use_mean_pooling is False

* added beit specific outputs in model docs

* changed the import path

* Fix docs

Co-authored-by: Niels Rogge <niels.rogge1@gmail.com>
==

docs/source/model_doc/beit.rst
src/transformers/models/beit/modeling_beit.py
src/transformers/models/beit/modeling_flax_beit.py
==================
bbaa3effb;Sylvain Gugger;2021-11-02 13:07:20 -0400;Fixes Beit training for PyTorch 1.10+ (#14249)

==

src/transformers/models/beit/modeling_beit.py
==================
ad3e560bc;Sylvain Gugger;2021-11-02 12:15:15 -0400;Add PushToHubCallback in main init (#14246)

==

src/transformers/__init__.py
src/transformers/utils/dummy_tf_objects.py
==================
ce01122a3;Anton Lozhkov;2021-11-02 17:53:50 +0300;[Tests] Fix DistilHubert path (#14245)
* Add audio-classification benchmarking results

* fix distilhubert path
==

tests/test_modeling_hubert.py
==================
4a394cf53;Yih-Dar;2021-11-02 11:02:41 +0100;Fix test_configuration_tie in FlaxEncoderDecoderModelTest (#14076)
* check test_configuration_tie

* Fix test_configuration_tie

* make test slow again

* Remove property and use model.module.bind

* revert to slow test

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>
==

tests/test_modeling_flax_encoder_decoder.py
==================
a767276fd;Li-Huai (Allan) Lin;2021-11-02 16:22:45 +0800;Fix generation docstring (#14216)
* Fix generation docstring

* Style
==

src/transformers/generation_utils.py
src/transformers/models/gpt2/tokenization_gpt2_fast.py
==================
e20faa6f0;NielsRogge;2021-11-01 19:55:45 +0100;Add BeitForSemanticSegmentation (#14096)
* Add first draft

* Make forward pass work

* Improve conversion script

* Add notebook that checks if it works

* Add BeitForSemanticSegmentation to the tests

* More improvements

* Make BeitForSemanticSegmentation consistent with Segformer

* Small bug fix

* Add BeitForSemanticSegmentation to docs

* Make sure model doesn't output hidden states when the user doesn't want to

* Make it possible to convert the large model

* Fix issue

* Fix conversion script for large model

* Add auxiliary_head option to semantic segmentation model

* Apply suggestions from @sgugger's review

* Apply suggestions from code review

* Fix failing test

Co-authored-by: Lysandre <lysandre.debut@reseau.eseo.fr>
==

docs/source/model_doc/beit.rst
src/transformers/__init__.py
src/transformers/models/beit/__init__.py
src/transformers/models/beit/configuration_beit.py
src/transformers/models/beit/convert_beit_unilm_to_pytorch.py
src/transformers/models/beit/modeling_beit.py
src/transformers/utils/dummy_pt_objects.py
tests/test_modeling_beit.py
tests/test_modeling_common.py
utils/check_repo.py
==================
8b3257811;Walter Martin;2021-11-01 13:46:11 -0400;improving efficiency of mlflow metric logging (#14232)
Signed-off-by: Walter Martin <wamartin@microsoft.com>
==

src/transformers/integrations.py
==================
ce91bf9a3;Suraj Patil;2021-11-01 22:38:52 +0530;[GPTJ] enable common tests and few fixes (#14190)
* enable common tests, small fixes

* don't tie word embeds

* don't ignore lm_head
==

src/transformers/models/gptj/configuration_gptj.py
src/transformers/models/gptj/modeling_gptj.py
tests/test_modeling_gptj.py
==================
70d571184;mathor;2021-11-01 21:24:03 +0800;Fix a writing issue in the comments of trainer.py (#14202)

==

src/transformers/trainer.py
==================
33fb98338;Prabhudatta Das;2021-11-01 18:23:13 +0530;Raising exceptions instead of using assertions for few models (#14219)
* raising exceptions instead of using assertions for few models

* fixed formatting issues

* fixing copy inconsistencies
==

src/transformers/models/bart/modeling_bart.py
src/transformers/models/bert/modeling_bert.py
src/transformers/models/blenderbot/modeling_blenderbot.py
src/transformers/models/blenderbot_small/modeling_blenderbot_small.py
src/transformers/models/m2m_100/modeling_m2m_100.py
src/transformers/models/marian/modeling_marian.py
src/transformers/models/pegasus/modeling_pegasus.py
src/transformers/models/speech_to_text/modeling_speech_to_text.py
==================
999540dfe;Nicolas Patry;2021-11-01 13:42:27 +0100;Tensor location is already handled (#14224)
in `base.py` not in subclasses.
==

src/transformers/pipelines/text_classification.py
==================
323f28dce;Nicolas Patry;2021-11-01 13:25:34 +0100;Fixing `image-segmentation` tests. (#14223)

==

src/transformers/pipelines/image_segmentation.py
tests/test_pipelines_image_segmentation.py
==================
7396095af;NielsRogge;2021-11-01 12:52:22 +0100;Update README of QA examples (#14172)

==

examples/legacy/question-answering/README.md
examples/pytorch/question-answering/README.md
examples/pytorch/question-answering/run_qa.py
examples/pytorch/question-answering/run_qa_beam_search.py
examples/pytorch/question-answering/run_qa_beam_search_no_trainer.py
examples/pytorch/question-answering/run_qa_no_trainer.py
examples/pytorch/question-answering/run_seq2seq_qa.py
==================
9450bfcc6;Yih-Dar;2021-11-01 11:52:36 +0100;Add more missing models to models/__init__.py (#14177)
* Add missing models to models/__init__.py

* Fix issues previously undetected

* Add UniSpeechSatForPreTraining to all_model_classes

* fix unispeech sat

* fix

* Add check_model_list() to check_repo.py

* Remove _ignore_models = ["bort"]

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>
Co-authored-by: patrickvonplaten <patrick.v.platen@gmail.com>
==

src/transformers/models/__init__.py
src/transformers/models/unispeech_sat/modeling_unispeech_sat.py
tests/test_modeling_unispeech_sat.py
utils/check_repo.py
==================
9fc195171;Lysandre;2021-10-29 14:51:05 -0400;Docs for v4.12.2

==

.circleci/deploy.sh
docs/source/_static/js/custom.js
==================
513fa30a6;Lysandre;2021-10-29 13:49:50 -0400;Docs for v4.12.1

==

.circleci/deploy.sh
docs/source/_static/js/custom.js
==================
63d91f449;Lysandre Debut;2021-10-29 13:43:43 -0400;Torch 1.10 (#14169)
* Torch 1.10

* torch scatter for 1.10

* style

* Skip tests
ok
==

.circleci/config.yml
setup.py
src/transformers/dependency_versions_table.py
tests/test_pipelines_audio_classification.py
tests/test_pipelines_image_segmentation.py
==================
e823d8198;Haram Lee;2021-10-30 02:12:10 +0900;Add a condition for checking labels (#14211)

==

src/transformers/trainer_seq2seq.py
==================
b33859634;Nicolas Patry;2021-10-29 17:24:09 +0200;Fixing image segmentation with inference mode. (#14204)
* Fixing image segmentation for inference mode.

* Update src/transformers/pipelines/base.py

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
==

src/transformers/pipelines/base.py
src/transformers/pipelines/image_segmentation.py
==================
c28bc80bb;Sylvain Gugger;2021-10-29 10:32:56 -0400;Generalize problem_type to all sequence classification models (#14180)
* Generalize problem_type to all classification models

* Missing import

* Deberta BC and fix tests

* Fix template

* Missing imports

* Revert change to reformer test

* Fix style
==

src/transformers/models/bart/modeling_bart.py
src/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py
src/transformers/models/ctrl/modeling_ctrl.py
src/transformers/models/deberta/modeling_deberta.py
src/transformers/models/deberta_v2/modeling_deberta_v2.py
src/transformers/models/fnet/modeling_fnet.py
src/transformers/models/gpt2/modeling_gpt2.py
src/transformers/models/gpt_neo/modeling_gpt_neo.py
src/transformers/models/gptj/modeling_gptj.py
src/transformers/models/ibert/modeling_ibert.py
src/transformers/models/layoutlm/modeling_layoutlm.py
src/transformers/models/layoutlmv2/modeling_layoutlmv2.py
src/transformers/models/led/modeling_led.py
src/transformers/models/mbart/modeling_mbart.py
src/transformers/models/megatron_bert/modeling_megatron_bert.py
src/transformers/models/mpnet/modeling_mpnet.py
src/transformers/models/openai/modeling_openai.py
src/transformers/models/rembert/modeling_rembert.py
src/transformers/models/roformer/modeling_roformer.py
src/transformers/models/tapas/modeling_tapas.py
src/transformers/models/transfo_xl/modeling_transfo_xl.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_{{cookiecutter.lowercase_modelname}}.py
tests/test_modeling_albert.py
tests/test_modeling_bert.py
tests/test_modeling_big_bird.py
tests/test_modeling_common.py
tests/test_modeling_convbert.py
tests/test_modeling_distilbert.py
tests/test_modeling_electra.py
tests/test_modeling_funnel.py
tests/test_modeling_longformer.py
tests/test_modeling_mobilebert.py
tests/test_modeling_openai.py
tests/test_modeling_reformer.py
tests/test_modeling_roberta.py
tests/test_modeling_squeezebert.py
tests/test_modeling_xlm.py
tests/test_modeling_xlnet.py
==================
4ab6a4a08;Sylvain Gugger;2021-10-29 09:35:05 -0400;Fix pipeline tests env and fetch (#14209)
* Fix pipeline tests env and fetch

* Fix quality
==

.circleci/config.yml
utils/tests_fetcher.py
==================
dc540dd31;Nicolas Patry;2021-10-29 15:29:28 +0200;Adding `handle_long_generation` paramters for `text-generation` pipeline. (#14118)
* Adding `handle_long_generation` paramters for `text-generation` pipeline.

* More error handling

* Fixing tests by dropping tf support on this functionality, it needs

`max_new_tokens` to make it possible to understand user's intent.
Otherwise, `max_length` == `tokenizer.model_max_length` <
input_ids.shape[0].

* Fixing doc ?

* Doc ?

* Remove link from doc.

* Catched an issue on roberta.

* Damn doc.

* Non BC proposal ?

* Cleaning the fix ?

* Finally using only a test override.

* Don't need to modify this.

* Bad print.
==

src/transformers/models/reformer/modeling_reformer.py
src/transformers/pipelines/text_generation.py
tests/test_pipelines_common.py
tests/test_pipelines_text_generation.py
==================
d37f1fb8b;Daniel Stancl;2021-10-29 15:19:01 +0200;Add `BlenderbotTokenizerFast` (#13720)
* Add the support for the fast (rust) implementation of BlenbderbotTokenizer

* Fix a converter and a typo in a doc

* Apply the patil-suraj's suggestion

* (Nitpick) Fast tokenization -> Fast Tokenization in doc

* Apply the SaulLu's suggestion

* Apply Narsil's suggestion to fix test pipelines

* Add encoder_no_repeat_ngram_size according to the Narsil's suggestion

* Revert the last (unnecessary) commit

* Override pipeline config for Blenderbot to allow for larger pos. emb.

* make fix-copies
==

docs/source/index.rst
docs/source/model_doc/blenderbot.rst
src/transformers/__init__.py
src/transformers/convert_slow_tokenizer.py
src/transformers/models/auto/tokenization_auto.py
src/transformers/models/blenderbot/__init__.py
src/transformers/models/blenderbot/tokenization_blenderbot.py
src/transformers/models/blenderbot/tokenization_blenderbot_fast.py
src/transformers/utils/dummy_tokenizers_objects.py
tests/test_modeling_blenderbot.py
tests/test_pipelines_common.py
tests/test_tokenization_blenderbot.py
==================
5b45422b5;Thomas Wang;2021-10-29 11:50:25 +0200;Remove n_ctx from configs (#14165)
* Remove n_ctx from configs

* Fix GPTJ and OpenAIGPT, both are acceptable breaking changes as there are no configs such that it breaks

* Remove unecessary n_positions from TFOpenAIGPT
==

examples/research_projects/distillation/training_configs/distilgpt2.json
src/transformers/models/clip/modeling_clip.py
src/transformers/models/clip/modeling_flax_clip.py
src/transformers/models/ctrl/configuration_ctrl.py
src/transformers/models/gpt2/configuration_gpt2.py
src/transformers/models/gpt2/modeling_tf_gpt2.py
src/transformers/models/gpt_neo/convert_gpt_neo_mesh_tf_to_pytorch.py
src/transformers/models/gptj/configuration_gptj.py
src/transformers/models/gptj/modeling_gptj.py
src/transformers/models/megatron_gpt2/convert_megatron_gpt2_checkpoint.py
src/transformers/models/openai/configuration_openai.py
src/transformers/models/openai/modeling_openai.py
src/transformers/models/openai/modeling_tf_openai.py
tests/test_modeling_ctrl.py
tests/test_modeling_flax_gpt2.py
tests/test_modeling_gpt2.py
tests/test_modeling_gptj.py
tests/test_modeling_openai.py
tests/test_modeling_tf_ctrl.py
tests/test_modeling_tf_gpt2.py
tests/test_modeling_tf_openai.py
tests/test_trainer.py
==================
be236361f;Nicolas Patry;2021-10-29 11:34:18 +0200;Adding `batch_size` support for (almost) all pipelines (#13724)
* Tentative enabling of `batch_size` for pipelines.

* Add systematic test for pipeline batching.

* Enabling batch_size on almost all pipelines

- Not `zero-shot` (it's already passing stuff as batched so trickier)
- Not `QA` (preprocess uses squad features, we need to switch to real
tensors at this boundary.

* Adding `min_length_for_response` for conversational.

* Making CTC, speech mappings avaiable regardless of framework.

* Attempt at fixing automatic tests (ffmpeg not enabled for fast tests)

* Removing ffmpeg dependency in tests.

* Small fixes.

* Slight cleanup.

* Adding docs

and adressing comments.

* Quality.

* Update docs/source/main_classes/pipelines.rst

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/pipelines/question_answering.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/pipelines/zero_shot_classification.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Improving docs.

* Update docs/source/main_classes/pipelines.rst

Co-authored-by: Philipp Schmid <32632186+philschmid@users.noreply.github.com>

* N -> oberved_batch_size

softmax trick.

* Follow `padding_side`.

* Supporting image pipeline batching (and padding).

* Rename `unbatch` -> `loader_batch`.

* unbatch_size forgot.

* Custom padding for offset mappings.

* Attempt to remove librosa.

* Adding require_audio.

* torchaudio.

* Back to using datasets librosa.

* Adding help to set a pad_token on the tokenizer.

* Update src/transformers/pipelines/base.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/pipelines/base.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/pipelines/base.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Quality.

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
Co-authored-by: Philipp Schmid <32632186+philschmid@users.noreply.github.com>
==

docs/source/main_classes/pipelines.rst
src/transformers/__init__.py
src/transformers/pipelines/automatic_speech_recognition.py
src/transformers/pipelines/base.py
src/transformers/pipelines/conversational.py
src/transformers/pipelines/fill_mask.py
src/transformers/pipelines/object_detection.py
src/transformers/pipelines/question_answering.py
src/transformers/pipelines/token_classification.py
src/transformers/pipelines/zero_shot_classification.py
src/transformers/utils/dummy_pt_objects.py
tests/test_pipelines_audio_classification.py
tests/test_pipelines_automatic_speech_recognition.py
tests/test_pipelines_common.py
tests/test_pipelines_conversational.py
tests/test_pipelines_feature_extraction.py
tests/test_pipelines_fill_mask.py
tests/test_pipelines_image_classification.py
tests/test_pipelines_object_detection.py
tests/test_pipelines_question_answering.py
tests/test_pipelines_summarization.py
tests/test_pipelines_text2text_generation.py
tests/test_pipelines_text_classification.py
tests/test_pipelines_text_generation.py
tests/test_pipelines_token_classification.py
tests/test_pipelines_translation.py
tests/test_pipelines_zero_shot.py
==================
4469010c1;David del R√≠o Medina;2021-10-28 23:17:43 +0200;Replace assertions with RuntimeError exceptions (#14186)

==

src/transformers/integrations.py
==================
ba71f1b57;Patrick von Platen;2021-10-28 19:43:05 +0200;Update README.md

==

examples/pytorch/audio-classification/README.md
==================
b8fad022a;Lysandre;2021-10-28 12:56:46 -0400;v4.13.0.dev0

==

.circleci/deploy.sh
docs/source/_static/js/custom.js
docs/source/conf.py
examples/flax/question-answering/run_qa.py
examples/flax/token-classification/run_flax_ner.py
examples/pytorch/audio-classification/run_audio_classification.py
examples/pytorch/image-classification/run_image_classification.py
examples/pytorch/language-modeling/run_clm.py
examples/pytorch/language-modeling/run_mlm.py
examples/pytorch/language-modeling/run_plm.py
examples/pytorch/multiple-choice/run_swag.py
examples/pytorch/question-answering/run_qa.py
examples/pytorch/question-answering/run_qa_beam_search.py
examples/pytorch/question-answering/run_qa_beam_search_no_trainer.py
examples/pytorch/question-answering/run_qa_no_trainer.py
examples/pytorch/question-answering/run_seq2seq_qa.py
examples/pytorch/speech-recognition/run_speech_recognition_ctc.py
examples/pytorch/summarization/run_summarization.py
examples/pytorch/text-classification/run_glue.py
examples/pytorch/text-classification/run_xnli.py
examples/pytorch/token-classification/run_ner.py
examples/pytorch/translation/run_translation.py
examples/tensorflow/multiple-choice/run_swag.py
examples/tensorflow/question-answering/run_qa.py
examples/tensorflow/summarization/run_summarization.py
examples/tensorflow/text-classification/run_glue.py
examples/tensorflow/translation/run_translation.py
setup.py
src/transformers/__init__.py
==================
62bf53663;Lysandre;2021-10-28 12:09:45 -0400;Release v4.12.0

==

README.md
README_ko.md
README_zh-hans.md
README_zh-hant.md
docs/source/conf.py
docs/source/index.rst
examples/flax/question-answering/run_qa.py
examples/flax/token-classification/run_flax_ner.py
examples/pytorch/audio-classification/run_audio_classification.py
examples/pytorch/image-classification/run_image_classification.py
examples/pytorch/language-modeling/run_clm.py
examples/pytorch/language-modeling/run_mlm.py
examples/pytorch/language-modeling/run_plm.py
examples/pytorch/multiple-choice/run_swag.py
examples/pytorch/question-answering/run_qa.py
examples/pytorch/question-answering/run_qa_beam_search.py
examples/pytorch/question-answering/run_qa_beam_search_no_trainer.py
examples/pytorch/question-answering/run_qa_no_trainer.py
examples/pytorch/question-answering/run_seq2seq_qa.py
examples/pytorch/speech-recognition/run_speech_recognition_ctc.py
examples/pytorch/summarization/run_summarization.py
examples/pytorch/text-classification/run_glue.py
examples/pytorch/text-classification/run_xnli.py
examples/pytorch/token-classification/run_ner.py
examples/pytorch/translation/run_translation.py
examples/tensorflow/multiple-choice/run_swag.py
examples/tensorflow/question-answering/run_qa.py
examples/tensorflow/summarization/run_summarization.py
examples/tensorflow/text-classification/run_glue.py
examples/tensorflow/translation/run_translation.py
setup.py
src/transformers/__init__.py
==================
5f3bf6511;NielsRogge;2021-10-28 18:01:00 +0200;Fix EncoderDecoderModel docs (#14197)
* Fix docs

* Apply suggestions from review + fix bug
==

src/transformers/models/encoder_decoder/modeling_encoder_decoder.py
src/transformers/models/speech_encoder_decoder/modeling_speech_encoder_decoder.py
src/transformers/models/vision_encoder_decoder/modeling_vision_encoder_decoder.py
==================
ac12a5ae4;NielsRogge;2021-10-28 15:29:04 +0200;Fix EncoderDecoderModel classes to be more like BART and T5 (#14139)
* First draft

* Make tuple output more readable

* Replace assertions by value errors

* Make it possible to predict_with_generate for vision and speech models

* Adapt Seq2SeqTrainer to work with VisionEncoderDecoder/SpeechEncoderDecoder

* Add deprecation warning

* Add copied from statements to vision and speech encoder decoders

* Fix failing test

* Apply @patrickvonplaten's suggestion

* Use reshape instead of view for consistency
==

src/transformers/models/encoder_decoder/modeling_encoder_decoder.py
src/transformers/models/speech_encoder_decoder/modeling_speech_encoder_decoder.py
src/transformers/models/vision_encoder_decoder/modeling_vision_encoder_decoder.py
src/transformers/trainer_seq2seq.py
==================
1251072f4;Anton Lozhkov;2021-10-28 16:22:18 +0300;Fix SEW-D implementation differences (#14191)
* Fix SEW-D

* Update tests

* isort
==

src/transformers/activations.py
src/transformers/models/sew_d/configuration_sew_d.py
src/transformers/models/sew_d/modeling_sew_d.py
tests/test_activations.py
tests/test_modeling_sew_d.py
==================
78b6a2ecb;Anton Lozhkov;2021-10-28 15:59:18 +0300;Add audio-classification benchmarking results (#14192)

==

examples/pytorch/audio-classification/README.md
==================
1dc96a760;NielsRogge;2021-10-28 14:23:52 +0200;Add SegFormer (#14019)
* First draft

* Make style & quality

* Improve conversion script

* Add print statement to see actual slice

* Make absolute tolerance smaller

* Fix image classification models

* Add post_process_semantic method

* Disable padding

* Improve conversion script

* Rename to ForSemanticSegmentation, add integration test, remove post_process methods

* Improve docs

* Fix code quality

* Fix feature extractor tests

* Fix tests for image classification model

* Delete file

* Add is_torch_available to feature extractor

* Improve documentation of feature extractor methods

* Apply suggestions from @sgugger's code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Apply some more suggestions of code review

* Rebase with master

* Fix rebase issues

* Make sure model only outputs hidden states when the user wants to

* Apply suggestions from code review

* Add pad method

* Support padding of 2d images

* Add print statement

* Add print statement

* Move padding method to SegformerFeatureExtractor

* Fix issue

* Add casting of segmentation maps

* Add test for padding

* Add small note about padding

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

README.md
README_ko.md
README_zh-hans.md
README_zh-hant.md
docs/source/index.rst
docs/source/model_doc/segformer.rst
src/transformers/__init__.py
src/transformers/image_utils.py
src/transformers/models/__init__.py
src/transformers/models/auto/configuration_auto.py
src/transformers/models/auto/modeling_auto.py
src/transformers/models/segformer/__init__.py
src/transformers/models/segformer/configuration_segformer.py
src/transformers/models/segformer/convert_segformer_original_to_pytorch.py
src/transformers/models/segformer/feature_extraction_segformer.py
src/transformers/models/segformer/modeling_segformer.py
src/transformers/utils/dummy_pt_objects.py
src/transformers/utils/dummy_vision_objects.py
tests/test_feature_extraction_segformer.py
tests/test_modeling_segformer.py
utils/check_repo.py
==================
123cce6ff;Stas Bekman;2021-10-27 19:01:50 -0700;[modeling_utils] respect original dtype in _get_resized_lm_head (#14181)
* respect dtype in _get_resized_lm_head

* Update src/transformers/modeling_utils.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* consistency

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

src/transformers/modeling_utils.py
==================
88cd82e80;Patrick von Platen;2021-10-28 02:35:01 +0200;Update README.md

==

examples/pytorch/speech-recognition/README.md
==================
e118db15d;Patrick von Platen;2021-10-28 01:59:27 +0200;Update README.md

==

examples/pytorch/speech-recognition/README.md
==================
01b146698;Patrick von Platen;2021-10-28 01:22:28 +0200;[TPU tests] Enable first TPU examples pytorch (#14121)
* up

* up

* fix

* up

* Update examples/pytorch/test_xla_examples.py

* correct labels

* up

* up

* up

* up

* up

* up
==

.github/workflows/self-scheduled.yml
examples/pytorch/test_xla_examples.py
tests/test_trainer_tpu.py
==================
232822f36;Anton Lozhkov;2021-10-27 20:17:31 +0300;Add DistilHuBERT  (#14174)
* Add conversion

* Rename

* Add an integration test and remove layer_norm

* Remove layer_norm from the converter

* wording

* Fix imports
==

src/transformers/models/hubert/configuration_hubert.py
src/transformers/models/hubert/convert_distilhubert_original_s3prl_checkpoint_to_pytorch.py
src/transformers/models/hubert/modeling_hubert.py
tests/test_modeling_hubert.py
==================
e5b8ffb84;Lahfa Samy;2021-10-27 18:19:10 +0200;Replace assert of data/data_collator.py by ValueError (#14131)
* Replace assert of data_collator.py by ValueError

* Replace assert of data_collator.py by ValueError
==

src/transformers/data/data_collator.py
==================
25ceb8187;Anton Lozhkov;2021-10-27 17:17:47 +0300;[Pipelines] Fix ASR model types check (#14178)

==

src/transformers/pipelines/automatic_speech_recognition.py
==================
6200fd7bb;Patrick von Platen;2021-10-27 15:47:20 +0200;[Gradient checkpointing] Enable for Deberta + DebertaV2 + SEW-D (#14175)
* up

* up

* finish

* up

* final changes
==

src/transformers/models/deberta/modeling_deberta.py
src/transformers/models/deberta_v2/modeling_deberta_v2.py
src/transformers/models/sew_d/modeling_sew_d.py
==================
e1dc5afd2;Anton Lozhkov;2021-10-27 12:21:09 +0300;Add SEW CTC models (#14158)
* Add SEW CTC models

* Update paths

* Update paths
==

src/transformers/models/sew/configuration_sew.py
src/transformers/models/sew/convert_sew_original_pytorch_checkpoint_to_pytorch.py
src/transformers/models/sew/modeling_sew.py
src/transformers/models/sew_d/configuration_sew_d.py
src/transformers/models/sew_d/convert_sew_d_original_pytorch_checkpoint_to_pytorch.py
src/transformers/models/sew_d/modeling_sew_d.py
tests/test_modeling_sew.py
tests/test_modeling_sew_d.py
==================
1e53faeb2;Lysandre Debut;2021-10-26 22:20:51 -0400;Fix gelu test for torch 1.10 (#14167)

==

tests/test_activations.py
==================
8ddbfe975;Kamal Raj;2021-10-27 03:32:58 +0530;switch to inference_mode from no_gard (#13667)
* switch to inference_mode from no_gard
faster inference

* added switch to support older version of pytorch
==

src/transformers/pipelines/base.py
==================
ebd48c6de;Emanuel Huber;2021-10-26 18:14:29 -0300;Replace assertions with ValueError exception (#14142)
Updated masked-language modeling examples in pytorch
with convention defined by #12789
==

examples/pytorch/language-modeling/run_mlm.py
examples/pytorch/language-modeling/run_mlm_no_trainer.py
==================
42bfb83d7;Matthew Goldey;2021-10-26 16:36:26 -0400;fix typos in error messages in speech recognition example and modelcard.py (#14166)
* specify the text column name in the error message

* pluralize the word fields
==

examples/pytorch/speech-recognition/run_speech_recognition_ctc.py
src/transformers/modelcard.py
==================
41dad89f7;Jangwon Park;2021-10-27 05:23:41 +0900;chore: typo on ner accelerate example code (#14150)

==

examples/pytorch/token-classification/run_ner_no_trainer.py
==================
27c888db6;Lysandre;2021-10-26 15:48:28 -0400;Fix copies

==

src/transformers/models/sew_d/modeling_sew_d.py
==================
3f23634a1;Jay Zhang;2021-10-27 03:25:02 +0800;[ONNX] Add symbolic function for XSoftmax op for exporting to ONNX. (#14013)
* Add symbolic function for XSoftmax op for exporting to ONNX.

* Fix format issues.

* Fix a CI issue relative to copies.
==

src/transformers/models/deberta/modeling_deberta.py
src/transformers/models/deberta_v2/modeling_deberta_v2.py
==================
9f3aa46f4;Patrick von Platen;2021-10-26 18:59:58 +0200;Add Unispeech & Unispeech-SAT (#13963)
* unispeech

* add copy from

* remove hubert copy from

* finish for today

* add unispeech-sat

* adapt more

* up

* up

* up

* up

* add modeling

* add tests

* up

* up

* finish

* up

* Apply suggestions from code review

* up

* up

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* up

* up

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

README.md
README_ko.md
README_zh-hans.md
README_zh-hant.md
docs/source/index.rst
docs/source/model_doc/unispeech.rst
docs/source/model_doc/unispeech_sat.rst
src/transformers/__init__.py
src/transformers/models/auto/configuration_auto.py
src/transformers/models/auto/modeling_auto.py
src/transformers/models/bart/modeling_bart.py
src/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py
src/transformers/models/blenderbot/modeling_blenderbot.py
src/transformers/models/blenderbot_small/modeling_blenderbot_small.py
src/transformers/models/hubert/modeling_hubert.py
src/transformers/models/m2m_100/modeling_m2m_100.py
src/transformers/models/marian/modeling_marian.py
src/transformers/models/mbart/modeling_mbart.py
src/transformers/models/pegasus/modeling_pegasus.py
src/transformers/models/sew/modeling_sew.py
src/transformers/models/sew_d/modeling_sew_d.py
src/transformers/models/speech_to_text/modeling_speech_to_text.py
src/transformers/models/speech_to_text_2/modeling_speech_to_text_2.py
src/transformers/models/unispeech/__init__.py
src/transformers/models/unispeech/configuration_unispeech.py
src/transformers/models/unispeech/convert_unispeech_original_pytorch_checkpoint_to_pytorch.py
src/transformers/models/unispeech/modeling_unispeech.py
src/transformers/models/unispeech_sat/__init__.py
src/transformers/models/unispeech_sat/configuration_unispeech_sat.py
src/transformers/models/unispeech_sat/convert_unispeech_sat_original_pytorch_checkpoint_to_pytorch.py
src/transformers/models/unispeech_sat/modeling_unispeech_sat.py
src/transformers/models/wav2vec2/modeling_wav2vec2.py
src/transformers/utils/dummy_pt_objects.py
tests/fixtures/dummy_feature_extractor_config.json
tests/test_modeling_unispeech.py
tests/test_modeling_unispeech_sat.py
==================
9799f4e15;Patrick von Platen;2021-10-26 18:59:25 +0200;Update README.md

==

examples/pytorch/speech-recognition/README.md
==================
bfd817663;Stas Bekman;2021-10-26 09:09:54 -0700;[megatron_gpt2] dynamic gelu, add tokenizer, save config (#13928)
* [megatron_gpt2] dynamic gelu, add tokenizer, save config

* cleanup

* Update src/transformers/models/megatron_gpt2/convert_megatron_gpt2_checkpoint.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* apply suggestions

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

src/transformers/models/megatron_gpt2/convert_megatron_gpt2_checkpoint.py
==================
919a964b8;Sergio Valcarcel Macua;2021-10-26 15:08:59 +0100;Include Keras tensor in the allowed types (#14155)
* Include KerasTensor in allowed types

- This allows propagating symbolic tensors through TFBert models and layers' call(),
  which allows converting the subclass models to functional models.

* Style pass

Co-authored-by: Sergio Valcarcel Macua <sergiov@graphcore.ai>
Co-authored-by: matt <rocketknight1@gmail.com>
==

src/transformers/modeling_tf_utils.py
==================
f5ed19f57;Patrick von Platen;2021-10-26 15:59:33 +0200;[Speech Recognition] - Distributed training: Make sure vocab file removal and creation don't interfer  (#14161)
* up

* better
==

examples/pytorch/speech-recognition/run_speech_recognition_ctc.py
==================
840fc8dbc;Yih-Dar;2021-10-26 13:36:17 +0200;Add vision_encoder_decoder to models/__init__.py (#14151)
* Add vision_encoder_decoder

* Update _ignore_modules in get_model_modules()

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>
==

src/transformers/models/__init__.py
utils/check_repo.py
==================
e248e9b04;Patrick von Platen;2021-10-26 13:08:18 +0200;up (#14154)

==

examples/pytorch/speech-recognition/run_speech_recognition_ctc.py
==================
1f60df81b;Thomas Chaigneau;2021-10-26 11:22:22 +0200;Add Camembert to models exportable with ONNX (#14059)
Add Camembert to models exportable with ONNX

Co-authored-by: Thomas.Chaigneau <thomas.chaigneau@arkea.com>
Co-authored-by: Michael Benayoun <mickbenayoun@gmail.com>
==

docs/source/serialization.rst
src/transformers/models/camembert/__init__.py
src/transformers/models/camembert/configuration_camembert.py
src/transformers/onnx/features.py
==================
0c3174c75;Patrick von Platen;2021-10-25 23:55:08 +0200;Add TF<>PT and Flax<>PT everywhere (#14047)
* up

* up

* up

* up

* up

* up

* up

* add clip

* fix clip PyTorch

* fix clip PyTorch

* up

* up

* up

* up

* up

* up

* up
==

src/transformers/models/albert/modeling_flax_albert.py
src/transformers/models/hubert/modeling_tf_hubert.py
tests/test_modeling_big_bird.py
tests/test_modeling_clip.py
tests/test_modeling_common.py
tests/test_modeling_longformer.py
tests/test_modeling_lxmert.py
tests/test_modeling_tf_common.py
tests/test_modeling_wav2vec2.py
==================
8560b55b5;Sylvain Gugger;2021-10-25 16:53:47 -0400;Fix lazy init to stop hiding errors in import (#14124)

==

src/transformers/file_utils.py
==================
c99a2832e;Patrick von Platen;2021-10-25 19:50:36 +0200;Update README.md

==

examples/pytorch/speech-pretraining/README.md
==================
1a9381c60;Patrick von Platen;2021-10-25 19:49:51 +0200;Update README.md

==

examples/pytorch/speech-pretraining/README.md
==================
3e8761ab8;Matt;2021-10-25 15:04:54 +0100;Enable DefaultDataCollator class (#14141)

==

src/transformers/data/data_collator.py
==================
84b9579da;Matt;2021-10-25 15:04:36 +0100;Remove unneeded `to_tensor()` in TF inline example (#14140)

==

docs/source/training.rst
==================
1967c43eb;Chi-Liang, Liu;2021-10-25 19:58:29 +0800;BartEnocder add set_input_embeddings (#13960)
* BartEnocder add set_input_embeddings

To unify the interface, add set_input_embeddings to BartEncoder.

* BartEnocder add get_input_embeddings
==

src/transformers/models/bart/modeling_bart.py
==================
3e04a41a9;Reza Gharibi;2021-10-25 15:18:02 +0330;Fix some writing issues in the docs (#14136)
* Fix some writing issues in the docs

* Run code quality check
==

ISSUES.md
docs/source/add_new_model.rst
docs/source/add_new_pipeline.rst
docs/source/community.md
docs/source/converting_tensorflow_models.rst
docs/source/custom_datasets.rst
docs/source/debugging.rst
docs/source/model_sharing.rst
docs/source/parallelism.md
==================
2ac65551e;Reza Gharibi;2021-10-25 15:15:44 +0330;Fix rendering of examples version links (#14134)

==

examples/README.md
==================
1b871e091;karthikrangasai;2021-10-25 17:12:53 +0530;Supporting Seq2Seq model for question answering task (#13432)
* Add seq2seq example for QnA on SQuAD Dataset.

* Changes from review - Fixing styling mistakes.

* Added how to example in README, simplified the access to dataset's preprocess function.

* Added tests for the seq2seq QA example.

* Change dataset column name to fix tests.

* Fix test command mistake.

* Add missing argument 'ignore_pad_token_for_loss' from DataTrainingArguments.

* Add missing argument 'num_beams' from DataTrainingArguments.

* Fix processing of output predicted token ids so that tokenizer decode gets appropriate input. Updated assertion conditions on the tests.
==

examples/pytorch/question-answering/README.md
examples/pytorch/question-answering/run_seq2seq_qa.py
examples/pytorch/test_examples.py
==================
6b83090e8;Reza Gharibi;2021-10-25 15:10:44 +0330;Fix some typos in the docs (#14126)
* Fix some typos in the docs

* Fix a styling issue

* Fix code quality check error
==

docs/README.md
docs/source/installation.md
docs/source/model_sharing.rst
docs/source/tokenizer_summary.rst
docs/source/training.rst
==================
95bab5386;Kevin Ko;2021-10-23 04:57:48 +0900;Update TP parallel GEMM image (#14112)
* Update TP parallel GEMM image

* Delete parallelism-tp-parallel_gemm.png

* Update parallelism-tp-parallel_gemm.png
==

docs/source/imgs/parallelism-tp-parallel_gemm.png
==================
62ccbe096;Li-Huai (Allan) Lin;2021-10-23 01:05:45 +0800;Rename variables with unclear naming (#14122)
* Rename var

* Add comments
==

src/transformers/modeling_utils.py
==================
05a2afc25;Antonio Carlos Falc√£o Petri;2021-10-22 14:04:54 -0300;Add missing --validation_split_percentage data args (#14119)

==

examples/research_projects/wav2vec2/run_pretrain.py
==================
c7ccb2e77;Baizhou Huang;2021-10-22 22:03:09 +0800;Fix assertion in models (#14090)
* replace assertions in src/transformers/models/luke/convert_luke_original_pytorch_checkpoint_to_pytorch.py

* replace assertions in src/transformers/models/marian/convert_marian_to_pytorch.py

* Update src/transformers/models/luke/convert_luke_original_pytorch_checkpoint_to_pytorch.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/models/marian/convert_marian_to_pytorch.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/models/marian/convert_marian_to_pytorch.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/models/marian/convert_marian_to_pytorch.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/models/marian/convert_marian_to_pytorch.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/models/marian/convert_marian_to_pytorch.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/models/marian/convert_marian_to_pytorch.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/models/marian/convert_marian_to_pytorch.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/models/marian/convert_marian_to_pytorch.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

Co-authored-by: skpig <1900012999@pku.edu.cn>
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

src/transformers/models/luke/convert_luke_original_pytorch_checkpoint_to_pytorch.py
src/transformers/models/marian/convert_marian_to_pytorch.py
==================
16d7b70b8;Sylvain Gugger;2021-10-22 08:13:04 -0400;Update Korean README to master

==

README_ko.md
==================
fa4abdb3e;Jayesh Dewangan;2021-10-22 17:15:32 +0530;Replace assertions with valueError Exeptions (#14117)
* Replace assertions with valueError Exeptions

* Reformatted
==

src/transformers/models/wav2vec2/convert_wav2vec2_original_pytorch_checkpoint_to_pytorch.py
==================
9f53f049c;Yeoun Yi;2021-10-22 20:42:31 +0900;Translate README.md to Korean (#14015)
* Create README_ko.md

* Update README.md

* Update README_zh-hans.md

* Update README_zh-hant.md

* Update README_ko.md

* Update check_copies.py

* Update README_ko.md

* typo

* match with readme_ko
==

README.md
README_ko.md
README_zh-hans.md
README_zh-hant.md
utils/check_copies.py
==================
f5a49bfa4;David del R√≠o Medina;2021-10-22 13:11:40 +0200;Replace assert statements with exceptions (#13871) (#13901)
* Replace assert statements with exceptions (#13871)

* Change f-strings when not needed (flake8)

* Replace assert statements with exceptions (#13871)

* Change f-strings when not needed (flake8)

* Improve error message as suggested by reviewer

* Fix identation bug

* Fix style errors
==

src/transformers/generation_beam_search.py
==================
70f186f61;Patrick von Platen;2021-10-22 11:01:26 +0200;up (#14116)

==

tests/test_modeling_hubert.py
==================
ca2ef7dfc;Deepanshu verma;2021-10-22 03:37:18 +0530;Changed asserts to ValueError (#14091)

==

src/transformers/models/detr/feature_extraction_detr.py
==================
7888914ed;Reza Gharibi;2021-10-22 00:30:26 +0330;Fix a typo in preprocessing docs (#14108)

==

docs/source/preprocessing.rst
==================
d432a654f;lee1jun;2021-10-22 04:31:32 +0900;fix typo in license docstring (#14094)
last line: "# limitations under the License." is missing
==

examples/pytorch/audio-classification/run_audio_classification.py
==================
7af55d3a1;David del R√≠o Medina;2021-10-21 21:31:00 +0200;Replace assertion with ValueError exception (#14098)

==

src/transformers/hf_argparser.py
==================
f00bceab8;stalkermustang;2021-10-21 22:29:17 +0300;Fix typo in comment (#14102)

==

src/transformers/models/gpt2/modeling_gpt2.py
==================
234cfefbb;Li-Huai (Allan) Lin;2021-10-22 00:31:29 +0800;Fix ignore_mismatched_sizes (#14085)
* Fix

* Style

* Name

* Fix tests

* Style

* Remove embed sizes checking

* Disable some tests

* Fix

* Apply suggestion
==

src/transformers/modeling_utils.py
tests/test_modeling_canine.py
tests/test_modeling_common.py
tests/test_modeling_flax_big_bird.py
tests/test_modeling_flax_common.py
tests/test_modeling_layoutlmv2.py
tests/test_modeling_tf_common.py
tests/test_modeling_tf_transfo_xl.py
tests/test_modeling_transfo_xl.py
==================
e03544a13;Anton Lozhkov;2021-10-21 19:15:46 +0300;[Examples] Add audio classification notebooks (#14099)
* Update SEW integration test tolerance

* Add audio classification notebooks
==

examples/pytorch/README.md
notebooks/README.md
==================
0f502682f;Sylvain Gugger;2021-10-21 11:59:23 -0400;Pin PyTorch to make CI green

==

setup.py
src/transformers/dependency_versions_table.py
==================
f9c16b02e;Christopher Akiki;2021-10-21 17:19:30 +0200;Replace "Masked" with "Causal" in TF CLM example (#14014)

==

examples/tensorflow/language-modeling/run_clm.py
==================
318722820;David del R√≠o Medina;2021-10-21 13:32:27 +0200;Replace assertions with ValueError exceptions (#14061)
* Replace assertions with ValueError exceptions

* Format error messages as suggested
==

src/transformers/generation_utils.py
==================
9e4ea2517;Weston King-Leatham;2021-10-21 07:27:32 -0400;Change asserts in src/transformers/models/xlnet/ to raise ValueError (#14088)
* Change asserts in src/transformers/models/xlnet/ to raise ValueError

* Update src/transformers/models/xlnet/modeling_tf_xlnet.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

src/transformers/models/xlnet/configuration_xlnet.py
src/transformers/models/xlnet/modeling_tf_xlnet.py
==================
e9d2a639f;Patrick von Platen;2021-10-21 10:30:02 +0200;up (#14093)

==

examples/pytorch/speech-recognition/run_speech_recognition_ctc.py
==================
49155d243;Reza Gharibi;2021-10-20 22:40:57 +0330;Fix broken link in translation section (#14087)

==

docs/source/task_summary.rst
==================
0270d44f5;Leandro von Werra;2021-10-20 14:15:47 +0200;Context managers (#13900)
* add `ContextManagers` for lists of contexts

* fix import sorting

* add `ContextManagers` tests
==

src/transformers/file_utils.py
tests/test_file_utils.py
==================
f875fb0e5;Sylvain Gugger;2021-10-20 07:55:14 -0400;Fix label attribution in token classification examples (#14055)

==

examples/pytorch/token-classification/run_ner.py
examples/pytorch/token-classification/run_ner_no_trainer.py
==================
31560f639;Baizhou Huang;2021-10-20 19:54:39 +0800;Fix assert in src/transformers/data/datasets/language_modeling.py (#14077)
* replace assertion with ValueError

* fix code style

Co-authored-by: skpig <1900012999@pku.edu.cn>
==

src/transformers/data/datasets/language_modeling.py
==================
0106826a6;Kwanghee Choi;2021-10-20 20:51:30 +0900;Fix missing autocast() in Trainer.prediction_step() (#14075)
Co-authored-by: jonas <jonas@hpcnt.com>
==

src/transformers/trainer.py
==================
a43d9352a;Baizhou Huang;2021-10-20 19:43:45 +0800;replace assert with exception in src/transformers/utils/model_pararallel_utils.py (#14072)
* replace assert with exception in src/transformers/utils/model_parallel_utils.py

* fix some code style

* fix typo

Co-authored-by: skpig <1900012999@pku.edu.cn>
==

src/transformers/utils/model_parallel_utils.py
==================
53dc39d82;Patrick von Platen;2021-10-20 13:01:42 +0200;up (#14079)

==

examples/pytorch/speech-recognition/run_speech_recognition_ctc.py
==================
0bc2e54f0;Patrick von Platen;2021-10-20 11:51:41 +0200;Add ASR colabs (#14067)
* up

* Update notebooks/README.md
==

examples/pytorch/README.md
notebooks/README.md
==================
dbaf49203;Anton Lozhkov;2021-10-20 12:22:43 +0300;[Examples] Use Audio feature in speech classification (#14052)
* Update SEW integration test tolerance

* Update audio classification

* Update test

* Remove torchaudio

* Add dataset revision

* Hub branch naming

* Revert dataset revisions

* Update datasets
==

examples/pytorch/audio-classification/README.md
examples/pytorch/audio-classification/requirements.txt
examples/pytorch/audio-classification/run_audio_classification.py
examples/pytorch/speech-pretraining/run_wav2vec2_pretraining_no_trainer.py
examples/pytorch/speech-recognition/run_speech_recognition_ctc.py
examples/pytorch/test_examples.py
==================
3fefa292c;Robert Stone;2021-10-19 19:06:19 -0700;Trainer._load_rng_state() path fix (#14069) (#14071)

==

src/transformers/trainer.py
==================
3892d09f4;Meng Zhou;2021-10-20 02:30:53 +0800;update to_py_obj to support np.number (#14064)
Co-authored-by: Áú∏Êµ© <mouhao.zm@alibaba-inc.com>
==

src/transformers/file_utils.py
==================
122c2f81b;Pedro Marques;2021-10-19 13:14:21 +0200;TF Model train and eval step metrics for seq2seq models. (#14009)
* TF Model train and eval step metrics for seq2seq models.

When using a model with a seq2seq output compute metrics against logits.

* Removing vestigial code

Co-authored-by: matt <rocketknight1@gmail.com>
==

src/transformers/modeling_tf_utils.py
tests/test_modeling_tf_t5.py
==================
fde4867f9;Thomas Wang;2021-10-19 10:56:17 +0200;Fix passing None as concrete args (#14022)

==

src/transformers/utils/fx.py
==================
9eda0d156;Ihor Omelchenko;2021-10-19 01:03:39 +0300;Fix typo (#14056)

==

docs/source/custom_datasets.rst
==================
7a3147e9b;Weizhe Yuan;2021-10-19 06:03:11 +0800;fix typo (#14049)

==

examples/pytorch/summarization/run_summarization_no_trainer.py
==================
d5ff69fce;Patrick von Platen;2021-10-18 17:43:35 +0200;[Speech] Refactor Examples (#14040)
* adapt_examples

* up

* up

* up

* up

* add auto models

* finish
==

docs/source/model_doc/sew.rst
docs/source/model_doc/sew_d.rst
src/transformers/__init__.py
src/transformers/models/auto/modeling_auto.py
src/transformers/models/hubert/modeling_hubert.py
src/transformers/models/sew/__init__.py
src/transformers/models/sew/configuration_sew.py
src/transformers/models/sew/modeling_sew.py
src/transformers/models/sew_d/__init__.py
src/transformers/models/sew_d/configuration_sew_d.py
src/transformers/models/sew_d/modeling_sew_d.py
src/transformers/models/wav2vec2/modeling_wav2vec2.py
src/transformers/utils/dummy_pt_objects.py
tests/test_modeling_sew.py
tests/test_modeling_sew_d.py
==================
2024faf17;Sylvain Gugger;2021-10-18 10:22:57 -0400;Fix save when laod_best_model_at_end=True (#14054)

==

src/transformers/trainer_callback.py
==================
2c60ff2fe;Sylvain Gugger;2021-10-18 10:22:46 -0400;Add an API to register objects to Auto classes (#13989)
* Add API to register a new object in auto classes

* Fix test

* Documentation

* Add to tokenizers and test

* Add cleanup after tests

* Be more careful

* Move import

* Move import

* Cleanup in TF test too

* Add consistency check

* Add documentation

* Style

* Update docs/source/model_doc/auto.rst

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* Update src/transformers/models/auto/auto_factory.py

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
==

docs/source/model_doc/auto.rst
src/transformers/models/auto/auto_factory.py
src/transformers/models/auto/configuration_auto.py
src/transformers/models/auto/tokenization_auto.py
tests/test_configuration_auto.py
tests/test_modeling_auto.py
tests/test_modeling_tf_auto.py
tests/test_tokenization_auto.py
==================
3d587c534;Dat Quoc Nguyen;2021-10-18 21:16:46 +0700;Add BARTpho: Pre-trained Sequence-to-Sequence Models for Vietnamese (#13788)
* Add the pre-trained BARTpho model

* Add the pre-trained BARTpho model

* Add the pre-trained BARTpho model

* Fix incorrectly sorted and/or formatted imports

* Fix incorrectly sorted and/or formatted style

* Fix check_dummies

* Fix check_dummies

* Fix check_dummies

* Update docs/source/model_doc/bartpho.rst

Co-authored-by: Suraj Patil <surajp815@gmail.com>

* Update src/transformers/models/bartpho/__init__.py

Co-authored-by: Suraj Patil <surajp815@gmail.com>

* Update src/transformers/models/bartpho/tokenization_bartpho.py

Co-authored-by: Suraj Patil <surajp815@gmail.com>

* Update tests/test_tokenization_bartpho.py

Co-authored-by: Suraj Patil <surajp815@gmail.com>

* Update src/transformers/models/bartpho/tokenization_bartpho.py

Co-authored-by: Suraj Patil <surajp815@gmail.com>

* Update tests/test_tokenization_bartpho.py

Co-authored-by: Suraj Patil <surajp815@gmail.com>

* Update docs/source/model_doc/bartpho.rst

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update docs/source/model_doc/bartpho.rst

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/models/bartpho/__init__.py

Co-authored-by: Suraj Patil <surajp815@gmail.com>

* Add the pre-trained BARTpho model

* Add Tips section in doc and details of monolingual_vocab_file

* Fix conflicts

* Add another tip related to monolingual_vocab_file

* Readd dependency_versions_table.py

* Handle failing checks

* Remove test_list.txt

* Remove md5sum.saved

* Revise Readme.md

Co-authored-by: Suraj Patil <surajp815@gmail.com>
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

README.md
README_zh-hans.md
README_zh-hant.md
docs/source/index.rst
docs/source/model_doc/bartpho.rst
docs/source/model_doc/bertweet.rst
docs/source/model_doc/phobert.rst
src/transformers/__init__.py
src/transformers/models/__init__.py
src/transformers/models/auto/configuration_auto.py
src/transformers/models/auto/tokenization_auto.py
src/transformers/models/bartpho/__init__.py
src/transformers/models/bartpho/tokenization_bartpho.py
src/transformers/utils/dummy_sentencepiece_objects.py
tests/test_tokenization_bartpho.py
==================
7c6cd0ac2;Patrick von Platen;2021-10-18 12:59:18 +0200;up (#14046)

==

tests/test_modeling_flax_clip.py
==================
82b62fa60;Anton Lozhkov;2021-10-18 13:58:59 +0300;Update SEW integration test tolerance (#14048)

==

tests/test_modeling_sew.py
==================
bdf31d6e0;Patrick von Platen;2021-10-18 12:52:40 +0200;[Speech] Move all examples to new audio feature (#14045)
* up

* up

* up

* finish
==

examples/pytorch/speech-recognition/run_speech_recognition_ctc.py
setup.py
src/transformers/dependency_versions_table.py
tests/test_modeling_flax_wav2vec2.py
tests/test_modeling_hubert.py
tests/test_modeling_sew.py
tests/test_modeling_sew_d.py
tests/test_modeling_speech_to_text.py
tests/test_modeling_tf_hubert.py
tests/test_modeling_tf_wav2vec2.py
tests/test_modeling_wav2vec2.py
==================
4334095c3;Mishig Davaadorj;2021-10-18 10:24:25 +0200;Fix typo (#14044)

==

src/transformers/models/speech_to_text/modeling_speech_to_text.py
==================
37c5759cb;Patrick von Platen;2021-10-17 23:01:03 +0200;[Speech Examples] Add new audio feature (#14027)
* finish

* up

* finish all

* up
==

examples/pytorch/_tests_requirements.txt
examples/pytorch/speech-pretraining/README.md
examples/pytorch/speech-pretraining/requirements.txt
examples/pytorch/speech-pretraining/run_wav2vec2_pretraining_no_trainer.py
examples/pytorch/speech-recognition/README.md
examples/pytorch/speech-recognition/requirements.txt
examples/pytorch/speech-recognition/run_speech_recognition_ctc.py
examples/pytorch/test_examples.py
==================
cde0c750a;David del R√≠o Medina;2021-10-16 02:28:13 +0200;Replace assertions with ValueError exceptions (#14018)
* Replace assertions with ValueError exceptions

* Change length check for a more explicit one
==

src/transformers/generation_logits_process.py
==================
968ae57c6;Sylvain Gugger;2021-10-15 20:09:54 -0400;Don't duplicate the elements in dir (#14023)

==

src/transformers/file_utils.py
==================
84ad6af49;Suraj Patil;2021-10-16 05:38:57 +0530;minor fixes (#14026)

==

src/transformers/models/clip/modeling_clip.py
tests/test_modeling_clip.py
==================
f5af87361;Patrick von Platen;2021-10-16 00:48:37 +0200;[Docs] More general docstrings (#14028)
* up

* finish

* up

* up

* finish
==

src/transformers/file_utils.py
src/transformers/modeling_flax_utils.py
src/transformers/models/albert/modeling_albert.py
src/transformers/models/albert/modeling_tf_albert.py
src/transformers/models/bart/modeling_bart.py
src/transformers/models/bart/modeling_tf_bart.py
src/transformers/models/bert/modeling_bert.py
src/transformers/models/bert/modeling_tf_bert.py
src/transformers/models/bert_generation/modeling_bert_generation.py
src/transformers/models/big_bird/modeling_big_bird.py
src/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py
src/transformers/models/blenderbot/modeling_tf_blenderbot.py
src/transformers/models/blenderbot_small/modeling_tf_blenderbot_small.py
src/transformers/models/canine/modeling_canine.py
src/transformers/models/convbert/modeling_convbert.py
src/transformers/models/convbert/modeling_tf_convbert.py
src/transformers/models/ctrl/modeling_ctrl.py
src/transformers/models/ctrl/modeling_tf_ctrl.py
src/transformers/models/deberta/modeling_deberta.py
src/transformers/models/deberta/modeling_tf_deberta.py
src/transformers/models/deberta_v2/modeling_deberta_v2.py
src/transformers/models/deberta_v2/modeling_tf_deberta_v2.py
src/transformers/models/distilbert/modeling_distilbert.py
src/transformers/models/distilbert/modeling_tf_distilbert.py
src/transformers/models/electra/modeling_electra.py
src/transformers/models/electra/modeling_tf_electra.py
src/transformers/models/flaubert/modeling_flaubert.py
src/transformers/models/flaubert/modeling_tf_flaubert.py
src/transformers/models/fnet/modeling_fnet.py
src/transformers/models/fsmt/modeling_fsmt.py
src/transformers/models/funnel/modeling_funnel.py
src/transformers/models/funnel/modeling_tf_funnel.py
src/transformers/models/gpt2/modeling_gpt2.py
src/transformers/models/gpt2/modeling_tf_gpt2.py
src/transformers/models/gpt_neo/modeling_gpt_neo.py
src/transformers/models/gptj/modeling_gptj.py
src/transformers/models/ibert/modeling_ibert.py
src/transformers/models/led/modeling_led.py
src/transformers/models/led/modeling_tf_led.py
src/transformers/models/longformer/modeling_longformer.py
src/transformers/models/longformer/modeling_tf_longformer.py
src/transformers/models/lxmert/modeling_lxmert.py
src/transformers/models/lxmert/modeling_tf_lxmert.py
src/transformers/models/m2m_100/modeling_m2m_100.py
src/transformers/models/marian/modeling_tf_marian.py
src/transformers/models/mbart/modeling_mbart.py
src/transformers/models/mbart/modeling_tf_mbart.py
src/transformers/models/megatron_bert/modeling_megatron_bert.py
src/transformers/models/mobilebert/modeling_mobilebert.py
src/transformers/models/mobilebert/modeling_tf_mobilebert.py
src/transformers/models/mpnet/modeling_mpnet.py
src/transformers/models/mpnet/modeling_tf_mpnet.py
src/transformers/models/openai/modeling_openai.py
src/transformers/models/openai/modeling_tf_openai.py
src/transformers/models/pegasus/modeling_tf_pegasus.py
src/transformers/models/reformer/modeling_reformer.py
src/transformers/models/rembert/modeling_rembert.py
src/transformers/models/rembert/modeling_tf_rembert.py
src/transformers/models/roberta/modeling_roberta.py
src/transformers/models/roberta/modeling_tf_roberta.py
src/transformers/models/roformer/modeling_roformer.py
src/transformers/models/roformer/modeling_tf_roformer.py
src/transformers/models/speech_to_text/modeling_speech_to_text.py
src/transformers/models/splinter/modeling_splinter.py
src/transformers/models/squeezebert/modeling_squeezebert.py
src/transformers/models/transfo_xl/modeling_tf_transfo_xl.py
src/transformers/models/transfo_xl/modeling_transfo_xl.py
src/transformers/models/wav2vec2/modeling_wav2vec2.py
src/transformers/models/xlm/modeling_tf_xlm.py
src/transformers/models/xlm/modeling_xlm.py
src/transformers/models/xlnet/modeling_tf_xlnet.py
src/transformers/models/xlnet/modeling_xlnet.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_tf_{{cookiecutter.lowercase_modelname}}.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_{{cookiecutter.lowercase_modelname}}.py
==================
47489a697;Murilo Gon√ßalves;2021-10-15 16:56:07 -0300;Fix: replace asserts statements with exception (#14029)

==

src/transformers/models/lxmert/modeling_lxmert.py
==================
cd3166a8e;Anton Lozhkov;2021-10-15 18:26:26 +0300;Add the SEW and SEW-D speech models (#13962)
* Working encoder

* SEW-D and tests

* Further conv fixes

* Automodels and conv inits

* Update integration tests, add docs

* Docs cleanup, resolve todos

* Conf fix

* Fix docs

* Fix tests, apply suggestions

* Update src/transformers/models/sew/modeling_sew.py

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* Model conversion and updated no-mask tests

* Remove copy of feature_proj

* Style

* Update src/transformers/models/auto/feature_extraction_auto.py

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* Update src/transformers/models/auto/feature_extraction_auto.py

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* Move orgs

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
==

README.md
README_zh-hans.md
README_zh-hant.md
docs/source/index.rst
docs/source/model_doc/sew.rst
docs/source/model_doc/sew_d.rst
src/transformers/__init__.py
src/transformers/models/__init__.py
src/transformers/models/auto/configuration_auto.py
src/transformers/models/auto/modeling_auto.py
src/transformers/models/hubert/modeling_hubert.py
src/transformers/models/sew/__init__.py
src/transformers/models/sew/configuration_sew.py
src/transformers/models/sew/convert_sew_original_pytorch_checkpoint_to_pytorch.py
src/transformers/models/sew/modeling_sew.py
src/transformers/models/sew_d/__init__.py
src/transformers/models/sew_d/configuration_sew_d.py
src/transformers/models/sew_d/convert_sew_d_original_pytorch_checkpoint_to_pytorch.py
src/transformers/models/sew_d/modeling_sew_d.py
src/transformers/models/wav2vec2/modeling_wav2vec2.py
src/transformers/utils/dummy_pt_objects.py
tests/test_modeling_sew.py
tests/test_modeling_sew_d.py
utils/check_repo.py
==================
d5b82bb70;jacksukk;2021-10-15 09:46:09 +0800;Fixed horizon_length for PPLM (#13886)
* fixed horizon_length

* fixed horizon_length

* fix style
==

examples/research_projects/pplm/run_pplm.py
==================
5b317f7ea;Lysandre Debut;2021-10-14 15:30:27 -0400;Scatter dummies + skip pipeline tests (#13996)
* Scatter dummies + skip pipeline tests

* Add torch scatter to build docs
==

.circleci/config.yml
src/transformers/__init__.py
src/transformers/utils/dummy_pt_objects.py
src/transformers/utils/dummy_scatter_objects.py
tests/test_pipelines_common.py
==================
b65c38976;Lukas Weiner;2021-10-14 16:12:32 +0200;Raise exceptions instead of asserts in src/transformers/models/bart/modeling_flax_[bart, marian, mbart, pegasus].py (#13939)
* Raise exceptions instead of asserts

* fix: fixed failing quality check with copies

* fix: fixed max line length

* rerun github ci, failed to install dependencies
==

src/transformers/models/bart/modeling_flax_bart.py
src/transformers/models/marian/modeling_flax_marian.py
src/transformers/models/mbart/modeling_flax_mbart.py
src/transformers/models/pegasus/modeling_flax_pegasus.py
==================
7fb2a8b3d;Patrick von Platen;2021-10-14 15:46:22 +0200;up (#14008)

==

docs/source/model_doc/speech_to_text.rst
docs/source/model_doc/speech_to_text_2.rst
examples/pytorch/test_examples.py
examples/research_projects/wav2vec2/README.md
examples/research_projects/wav2vec2/test_wav2vec2_deepspeed.py
src/transformers/models/hubert/modeling_hubert.py
src/transformers/models/hubert/modeling_tf_hubert.py
src/transformers/models/speech_encoder_decoder/modeling_speech_encoder_decoder.py
src/transformers/models/speech_to_text/modeling_speech_to_text.py
src/transformers/models/wav2vec2/modeling_flax_wav2vec2.py
src/transformers/models/wav2vec2/modeling_tf_wav2vec2.py
src/transformers/models/wav2vec2/modeling_wav2vec2.py
tests/test_modeling_flax_wav2vec2.py
tests/test_modeling_hubert.py
tests/test_modeling_speech_to_text.py
tests/test_modeling_tf_hubert.py
tests/test_modeling_tf_wav2vec2.py
tests/test_modeling_wav2vec2.py
tests/test_pipelines_audio_classification.py
tests/test_pipelines_automatic_speech_recognition.py
==================
7604557e4;Lysandre Debut;2021-10-14 09:07:51 -0400;Fix FNet tokenizer tests (#13995)

==

tests/test_tokenization_fnet.py
==================
f2002fea1;Sylvain Gugger;2021-10-14 09:07:08 -0400;Add strong test for configuration attributes (#14000)
* Add strong test for configuration attributes

* Add fake modif to trigger all tests

* Add a better fake modif

* Ignore is_encoder_decoder

* Fix faulty configs

* Remove fake modif
==

src/transformers/models/fsmt/configuration_fsmt.py
src/transformers/models/lxmert/configuration_lxmert.py
tests/test_configuration_common.py
==================
0ef61d392;Sylvain Gugger;2021-10-13 22:04:40 -0400;Revert "Skip faulty test"
This reverts commit 5b6bd4e7880cd51375c2d6c33bbd8173acfd920b.
==

examples/pytorch/test_examples.py
==================
a5be95413;David del R√≠o Medina;2021-10-14 14:57:12 +0200;Replace assertion with ValueError exception (#14006)

==

src/transformers/generation_flax_logits_process.py
==================
cc3606496;Patrick von Platen;2021-10-14 10:54:20 +0200;up (#13988)

==

src/transformers/models/byt5/tokenization_byt5.py
tests/test_tokenization_byt5.py
==================
5b6bd4e78;Sylvain Gugger;2021-10-13 22:04:40 -0400;Skip faulty test

==

examples/pytorch/test_examples.py
==================
51ee20fc2;Li-Huai (Allan) Lin;2021-10-14 09:28:11 +0800;Remove wrong model_args supplied (#13937)
* Remove wrong model_args of config.from_pretrained

* Fix tf & flax
==

src/transformers/modeling_flax_utils.py
src/transformers/modeling_tf_utils.py
src/transformers/modeling_utils.py
==================
408b2d2bd;NielsRogge;2021-10-13 10:28:56 +0200;Add TrOCR + VisionEncoderDecoderModel (#13874)
* First draft

* Update self-attention of RoBERTa as proposition

* Improve conversion script

* Add TrOCR decoder-only model

* More improvements

* Make forward pass with pretrained weights work

* More improvements

* Some more improvements

* More improvements

* Make conversion work

* Clean up print statements

* Add documentation, processor

* Add test files

* Small improvements

* Some more improvements

* Make fix-copies, improve docs

* Make all vision encoder decoder model tests pass

* Make conversion script support other models

* Update URL for OCR image

* Update conversion script

* Fix style & quality

* Add support for the large-printed model

* Fix some issues

* Add print statement for debugging

* Add print statements for debugging

* Make possible fix for sinusoidal embedding

* Further debugging

* Potential fix v2

* Add more print statements for debugging

* Add more print statements for debugging

* Deubg more

* Comment out print statements

* Make conversion of large printed model possible, address review comments

* Make it possible to convert the stage1 checkpoints

* Clean up code, apply suggestions from code review

* Apply suggestions from code review, use Microsoft models in tests

* Rename encoder_hidden_size to cross_attention_hidden_size

* Improve docs
==

README.md
README_zh-hans.md
README_zh-hant.md
docs/source/index.rst
docs/source/model_doc/trocr.rst
docs/source/model_doc/visionencoderdecoder.rst
src/transformers/__init__.py
src/transformers/configuration_utils.py
src/transformers/models/auto/configuration_auto.py
src/transformers/models/auto/modeling_auto.py
src/transformers/models/deit/configuration_deit.py
src/transformers/models/deit/modeling_deit.py
src/transformers/models/trocr/__init__.py
src/transformers/models/trocr/configuration_trocr.py
src/transformers/models/trocr/modeling_trocr.py
src/transformers/models/trocr/processing_trocr.py
src/transformers/models/vision_encoder_decoder/__init__.py
src/transformers/models/vision_encoder_decoder/configuration_vision_encoder_decoder.py
src/transformers/models/vision_encoder_decoder/convert_trocr_unilm_to_pytorch.py
src/transformers/models/vision_encoder_decoder/modeling_vision_encoder_decoder.py
src/transformers/models/vit/configuration_vit.py
src/transformers/models/vit/modeling_flax_vit.py
src/transformers/models/vit/modeling_vit.py
src/transformers/utils/dummy_pt_objects.py
tests/test_modeling_trocr.py
tests/test_modeling_vision_encoder_decoder.py
==================
61f642626;Stas Bekman;2021-10-12 15:37:55 -0700;[parallel doc] dealing with layers larger than one gpu (#13980)

==

docs/source/parallelism.md
==================
8b240a066;Yih-Dar;2021-10-13 00:10:34 +0200;Add TFEncoderDecoderModel + Add cross-attention to some TF models (#13222)
* Add cross attentions to TFGPT2Model

* Add TFEncoderDecoderModel

* Add TFBaseModelOutputWithPoolingAndCrossAttentions

* Add cross attentions to TFBertModel

* Fix past or past_key_values argument issue

* Fix generation

* Fix save and load

* Add some checks and comments

* Clean the code that deals with past keys/values

* Add kwargs to processing_inputs

* Add serving_output to TFEncoderDecoderModel

* Some cleaning + fix use_cache value issue

* Fix tests + add bert2bert/bert2gpt2 tests

* Fix more tests

* Ignore crossattention.bias when loading GPT2 weights into TFGPT2

* Fix return_dict_in_generate in tf generation

* Fix is_token_logit_eos_token bug in tf generation

* Finalize the tests after fixing some bugs

* Fix another is_token_logit_eos_token bug in tf generation

* Add/Update docs

* Add TFBertEncoderDecoderModelTest

* Clean test script

* Add TFEncoderDecoderModel to the library

* Add cross attentions to TFRobertaModel

* Add TFRobertaEncoderDecoderModelTest

* make style

* Change the way of position_ids computation

* bug fix

* Fix copies in tf_albert

* Remove some copied from and apply some fix-copies

* Remove some copied

* Add cross attentions to some other TF models

* Remove encoder_hidden_states from TFLayoutLMModel.call for now

* Make style

* Fix TFRemBertForCausalLM

* Revert the change to longformer + Remove copies

* Revert the change to albert and convbert + Remove copies

* make quality

* make style

* Add TFRembertEncoderDecoderModelTest

* make quality and fix-copies

* test TFRobertaForCausalLM

* Fixes for failed tests

* Fixes for failed tests

* fix more tests

* Fixes for failed tests

* Fix Auto mapping order

* Fix TFRemBertEncoder return value

* fix tf_rembert

* Check copies are OK

* Fix missing TFBaseModelOutputWithPastAndCrossAttentions is not defined

* Add TFEncoderDecoderModelSaveLoadTests

* fix tf weight loading

* check the change of use_cache

* Revert the change

* Add missing test_for_causal_lm for TFRobertaModelTest

* Try cleaning past

* fix _reorder_cache

* Revert some files to original versions

* Keep as many copies as possible

* Apply suggested changes - Use raise ValueError instead of assert

* Move import to top

* Fix wrong require_torch

* Replace more assert by raise ValueError

* Add test_pt_tf_model_equivalence (the test won't pass for now)

* add test for loading/saving

* finish

* finish

* Remove test_pt_tf_model_equivalence

* Update tf modeling template

* Remove pooling, added in the prev. commit, from MainLayer

* Update tf modeling test template

* Move inputs["use_cache"] = False to modeling_tf_utils.py

* Fix torch.Tensor in the comment

* fix use_cache

* Fix missing use_cache in ElectraConfig

* Add a note to from_pretrained

* Fix style

* Change test_encoder_decoder_save_load_from_encoder_decoder_from_pt

* Fix TFMLP (in TFGPT2) activation issue

* Fix None past_key_values value in serving_output

* Don't call get_encoderdecoder_model in TFEncoderDecoderModelTest.test_configuration_tie until we have a TF checkpoint on Hub

* Apply review suggestions - style for cross_attns in serving_output

* Apply review suggestions - change assert + docstrings

* break the error message to respect the char limit

* deprecate the argument past

* fix docstring style

* Update the encoder-decoder rst file

* fix Unknown interpreted text role "method"

* fix typo

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>
Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
==

docs/source/index.rst
docs/source/main_classes/output.rst
docs/source/model_doc/encoderdecoder.rst
docs/source/model_doc/roberta.rst
src/transformers/__init__.py
src/transformers/convert_pytorch_checkpoint_to_tf2.py
src/transformers/generation_tf_utils.py
src/transformers/modeling_tf_outputs.py
src/transformers/modeling_tf_utils.py
src/transformers/models/albert/modeling_tf_albert.py
src/transformers/models/auto/modeling_tf_auto.py
src/transformers/models/bert/modeling_tf_bert.py
src/transformers/models/convbert/modeling_tf_convbert.py
src/transformers/models/electra/configuration_electra.py
src/transformers/models/electra/modeling_tf_electra.py
src/transformers/models/encoder_decoder/__init__.py
src/transformers/models/encoder_decoder/modeling_tf_encoder_decoder.py
src/transformers/models/layoutlm/modeling_tf_layoutlm.py
src/transformers/models/longformer/modeling_tf_longformer.py
src/transformers/models/mobilebert/modeling_tf_mobilebert.py
src/transformers/models/mpnet/modeling_tf_mpnet.py
src/transformers/models/openai/modeling_tf_openai.py
src/transformers/models/rembert/modeling_tf_rembert.py
src/transformers/models/roberta/__init__.py
src/transformers/models/roberta/modeling_tf_roberta.py
src/transformers/models/xlm_roberta/modeling_tf_xlm_roberta.py
src/transformers/utils/dummy_tf_objects.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_tf_{{cookiecutter.lowercase_modelname}}.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/test_modeling_tf_{{cookiecutter.lowercase_modelname}}.py
tests/test_modeling_tf_bert.py
tests/test_modeling_tf_common.py
tests/test_modeling_tf_encoder_decoder.py
tests/test_modeling_tf_rembert.py
tests/test_modeling_tf_roberta.py
utils/check_repo.py
==================
26b6ef79d;Nicolas Patry;2021-10-12 18:18:19 +0200;Fixing the lecture values by making sure defaults are not changed (#13976)
384 // 4 < 128 would break `doc_stride`.
==

src/transformers/pipelines/question_answering.py
==================
58bf88257;Patrick von Platen;2021-10-12 18:17:06 +0200;[Wav2Vec2] Make sure tensors are always bool for mask_indices (#13977)
* correct long to bool

* up

* correct code
==

src/transformers/models/hubert/modeling_hubert.py
src/transformers/models/wav2vec2/modeling_wav2vec2.py
tests/test_modeling_wav2vec2.py
==================
11c043d27;Mishig Davaadorj;2021-10-12 16:26:18 +0200;Specify im-seg mask greyscole mode (#13974)

==

src/transformers/pipelines/image_segmentation.py
tests/test_pipelines_image_segmentation.py
==================
85d69a7dd;Hardian Lawi;2021-10-12 11:30:03 +0800;Fix missing tpu variable in benchmark_args_tf.py (#13968)

==

src/transformers/benchmark/benchmark_args_tf.py
==================
990de2c17;Lysandre Debut;2021-10-11 23:21:37 -0400;Remove pip 21.3 from installation candidates for model templates

==

.github/workflows/model-templates.yml
==================
d45fc7da3;Patrick von Platen;2021-10-12 00:46:32 +0200;[Speech Examples] Add pytorch speech pretraining (#13877)
* adapt wav2vec2

* add example

* add files

* adapt

* remove bogus file

* Apply suggestions from code review

* adapt files more

* upload changes

* del old files

* up

* up

* up

* up

* up

* correct gradient checkpoitning

* add readme

* finish

* finish

* up

* more fixes

* up

* up

* add demo run to readme

* up
==

examples/pytorch/_tests_requirements.txt
examples/pytorch/speech-pretraining/README.md
examples/pytorch/speech-pretraining/requirements.txt
examples/pytorch/speech-pretraining/run_wav2vec2_pretraining_no_trainer.py
examples/pytorch/test_examples.py
src/transformers/models/hubert/modeling_hubert.py
src/transformers/models/wav2vec2/modeling_wav2vec2.py
tests/test_modeling_hubert.py
tests/test_modeling_wav2vec2.py
==================
3499728dc;Lahfa Samy;2021-10-11 19:58:09 +0200;Replace assert by ValueError of src/transformers/models/electra/modeling_{electra,tf_electra}.py and all other models that had copies (#13955)
* Replace all assert by ValueError in src/transformers/models/electra

* Reformat with black to pass check_code_quality test

* Change some assert to ValueError of modeling_bert & modeling_tf_albert

* Change some assert in multiples models

* Change multiples models assertion to ValueError in order to validate
  check_code_style test and models template test.

* Black reformat

* Change some more asserts in multiples models

* Change assert to ValueError in modeling_layoutlm.py to fix copy error in code_style_check

* Add proper message to ValueError in modeling_tf_albert.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Simplify logic in models/bert/modeling_bert.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Add ValueError message to models/convbert/modeling_tf_convbert.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Add error message for ValueError to modeling_tf_electra.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Simplify logic in models/tapas/modeling_tapas.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Simplify logic in models/electra/modeling_electra.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Add ValueError message in src/transformers/models/bert/modeling_tf_bert.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Simplify logic in src/transformers/models/rembert/modeling_rembert.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Simplify logic in src/transformers/models/albert/modeling_albert.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

src/transformers/models/albert/modeling_albert.py
src/transformers/models/albert/modeling_tf_albert.py
src/transformers/models/bert/modeling_bert.py
src/transformers/models/bert/modeling_tf_bert.py
src/transformers/models/convbert/modeling_tf_convbert.py
src/transformers/models/electra/modeling_electra.py
src/transformers/models/electra/modeling_tf_electra.py
src/transformers/models/layoutlm/modeling_layoutlm.py
src/transformers/models/rembert/modeling_rembert.py
src/transformers/models/roberta/modeling_roberta.py
src/transformers/models/roformer/modeling_roformer.py
src/transformers/models/splinter/modeling_splinter.py
src/transformers/models/tapas/modeling_tapas.py
==================
64743d0ab;Lukas Weiner;2021-10-11 19:21:49 +0300;Raise exceptions instead of asserts (#13938)

==

src/transformers/data/processors/utils.py
==================
32634bce3;Sylvain Gugger;2021-10-11 12:03:58 -0400;Make username optional in hub_model_id (#13940)

==

src/transformers/keras_callbacks.py
src/transformers/trainer.py
src/transformers/training_args.py
==================
708ffff66;Midhun R Nair;2021-10-11 19:52:35 +0530;Raise exceptions instead of asserts in xnli.py (#13945)

==

src/transformers/data/processors/xnli.py
==================
e1bb2ebd9;Luis F. Talavera R;2021-10-11 09:21:46 -0500;Replace assert with unittest assertions (#13957)

==

tests/test_cli.py
==================
6e4c8f683;Jungwoo Park;2021-10-11 22:35:20 +0900;change to apply `pad_to_multiple_of` to labels (#13949)

==

src/transformers/data/data_collator.py
==================
dca679687;Patrick von Platen;2021-10-11 15:34:01 +0200;[Gradient checkpoining] Correct disabling `find_unused_parameters` in Trainer when gradient checkpointing is enabled (#13961)
* up

* correct test
==

src/transformers/modeling_utils.py
src/transformers/trainer.py
tests/test_modeling_common.py
==================
4a18337ba;Sylvain Gugger;2021-10-11 09:12:09 -0400;Honor existing attention mask in tokenzier.pad (#13926)
* Honor existing attention mask in tokenzier.pad

* Fix initialization of attention mask

* Roll the implem on all subclasses

* Fix tests
==

src/transformers/feature_extraction_sequence_utils.py
src/transformers/models/layoutlmv2/tokenization_layoutlmv2.py
src/transformers/models/layoutlmv2/tokenization_layoutlmv2_fast.py
src/transformers/models/luke/tokenization_luke.py
src/transformers/models/tapas/tokenization_tapas.py
src/transformers/tokenization_utils_base.py
tests/test_tokenization_common.py
==================
3c0c699ff;Lahfa Samy;2021-10-11 10:59:16 +0200;Raise ValueError instead of asserts in src/transformers/benchmark/benchmark.py (#13951)
* Raise ValueError exception instead of assert

* Remove f unnecessary f-strings

* Remove unused f-strings
==

src/transformers/benchmark/benchmark.py
==================
91758e399;oraby8;2021-10-09 15:07:39 +0200;fix issue 13904 -attribute does not exist- by change self_.mapping to self._model_mapping (#13942)

==

src/transformers/models/auto/auto_factory.py
==================
239bd61b9;Lysandre Debut;2021-10-08 14:41:51 -0400;Update bug-report.md (#13934)
* Update bug-report.md

* Update .github/ISSUE_TEMPLATE/bug-report.md

Co-authored-by: Suraj Patil <surajp815@gmail.com>

* Update .github/ISSUE_TEMPLATE/bug-report.md

Co-authored-by: Suraj Patil <surajp815@gmail.com>

* Update .github/ISSUE_TEMPLATE/bug-report.md

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>

* Update .github/ISSUE_TEMPLATE/bug-report.md

Co-authored-by: Anton Lozhkov <aglozhkov@gmail.com>

Co-authored-by: Suraj Patil <surajp815@gmail.com>
Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>
Co-authored-by: Anton Lozhkov <aglozhkov@gmail.com>
==

.github/ISSUE_TEMPLATE/bug-report.md
==================
46dfe99e4;Chungman Lee;2021-10-09 03:25:32 +0900;Fix typo in README.md (#13883)

==

examples/pytorch/README.md
==================
3e218523e;Sylvain Gugger;2021-10-08 11:30:39 -0400;Merge remote-tracking branch 'origin/master'

==
==================
9e15b511c;Sylvain Gugger;2021-10-08 11:30:29 -0400;Move to TF only

==

src/transformers/__init__.py
==================
cb911e5bc;Sylvain Gugger;2021-10-08 11:29:10 -0400;Style

==

src/transformers/__init__.py
==================
c8b07612a;Patrick von Platen;2021-10-08 17:28:18 +0200;[Generation] Fix max_new_tokens (#13919)
* up

* Update src/transformers/generation_stopping_criteria.py

* finish
==

src/transformers/generation_stopping_criteria.py
src/transformers/generation_utils.py
tests/test_generation_utils.py
==================
5a1b5e4b1;Sylvain Gugger;2021-10-08 11:00:48 -0400;Register `keras_callbacks` as a submodule

==

src/transformers/__init__.py
==================
23ee06ed5;Adam Kaczmarek;2021-10-08 16:27:32 +0200;Fixed typo: herBERT -> HerBERT (#13936)

==

docs/source/model_doc/herbert.rst
==================
de344815e;Stella Biderman;2021-10-08 10:07:09 -0400;Adds `PreTrainedModel.framework` attribute (#13817)
* Added `framework` attribute

* Update modeling_utils.py

* Update modeling_flax_utils.py

* Update modeling_tf_utils.py

* Update modeling_utils.py

* Update modeling_tf_utils.py

* Update modeling_tf_utils.py

* Update modeling_flax_utils.py

* Update modeling_tf_utils.py

* Update modeling_utils.py

* Update modeling_utils.py

* Update modeling_tf_utils.py

* Update modeling_flax_utils.py

* string -> str

* Update modeling_tf_utils.py

* string -> str

* fixup

* make flake happy

Co-authored-by: patil-suraj <surajp815@gmail.com>
==

src/transformers/modeling_flax_utils.py
src/transformers/modeling_tf_utils.py
src/transformers/modeling_utils.py
==================
d70919e6d;Nicolas Patry;2021-10-08 10:10:38 +0200;Adding support for tokens being suffixes or part of each other. (#13918)
* Adding support for tokens being suffixes or part of each other.

* Better test name.
==

src/transformers/tokenization_utils.py
tests/test_tokenization_common.py
==================
026866df9;Mishig Davaadorj;2021-10-08 09:59:53 +0200;Image Segmentation pipeline (#13828)
* Implement img seg pipeline

* Update src/transformers/pipelines/image_segmentation.py

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>

* Update src/transformers/pipelines/image_segmentation.py

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>

* Update output shape with individual masks

* Rm dev change

* Remove loops in test

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>
==

docs/source/main_classes/pipelines.rst
docs/source/model_doc/auto.rst
src/transformers/__init__.py
src/transformers/modelcard.py
src/transformers/models/auto/__init__.py
src/transformers/models/auto/modeling_auto.py
src/transformers/models/detr/feature_extraction_detr.py
src/transformers/pipelines/__init__.py
src/transformers/pipelines/image_segmentation.py
src/transformers/utils/dummy_pt_objects.py
tests/test_pipelines_image_segmentation.py
==================
be71ac3bc;Stas Bekman;2021-10-07 10:29:01 -0700;[trainer] memory metrics: add memory at the start report (#13915)
* [trainer] memory metrics: add memory at start

* fix for no-gpu
==

src/transformers/trainer_utils.py
==================
61cf2ea9c;Matt;2021-10-07 17:30:15 +0100;Fix incorrect output shapes for TF/PT LED (#13882)
* Fix issues with LED model

* Style pass

* Bugfixes

* correct attentions as well

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
==

src/transformers/models/led/modeling_led.py
src/transformers/models/led/modeling_tf_led.py
tests/test_modeling_led.py
==================
5f34163b8;Mishig Davaadorj;2021-10-07 18:10:19 +0200;Add missing character (#13922)

==

src/transformers/modeling_flax_utils.py
src/transformers/modeling_utils.py
src/transformers/models/auto/auto_factory.py
src/transformers/tokenization_utils_base.py
==================
0f5488f79;Patrick von Platen;2021-10-07 18:07:32 +0200;[Wav2Vec2] Fix mask_feature_prob (#13921)
* up

* overwrite hubert
==

src/transformers/models/hubert/modeling_hubert.py
src/transformers/models/wav2vec2/modeling_wav2vec2.py
tests/test_modeling_wav2vec2.py
==================
57420b103;Alex Hedges;2021-10-07 09:22:11 -0400;Add missing whitespace to multiline strings (#13916)

==

src/transformers/benchmark/benchmark_args.py
src/transformers/benchmark/benchmark_tf.py
src/transformers/benchmark/benchmark_utils.py
src/transformers/configuration_utils.py
src/transformers/convert_pytorch_checkpoint_to_tf2.py
src/transformers/data/data_collator.py
src/transformers/feature_extraction_sequence_utils.py
src/transformers/generation_beam_search.py
src/transformers/generation_logits_process.py
src/transformers/generation_tf_utils.py
src/transformers/generation_utils.py
src/transformers/hf_argparser.py
src/transformers/modeling_flax_pytorch_utils.py
src/transformers/modeling_tf_utils.py
src/transformers/modeling_utils.py
src/transformers/models/bart/configuration_bart.py
src/transformers/models/beit/feature_extraction_beit.py
src/transformers/models/bert_japanese/tokenization_bert_japanese.py
src/transformers/models/big_bird/modeling_big_bird.py
src/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py
src/transformers/models/canine/modeling_canine.py
src/transformers/models/clip/feature_extraction_clip.py
src/transformers/models/cpm/tokenization_cpm.py
src/transformers/models/cpm/tokenization_cpm_fast.py
src/transformers/models/deit/feature_extraction_deit.py
src/transformers/models/detr/feature_extraction_detr.py
src/transformers/models/electra/modeling_electra.py
src/transformers/models/electra/modeling_tf_electra.py
src/transformers/models/encoder_decoder/modeling_encoder_decoder.py
src/transformers/models/gpt_neo/configuration_gpt_neo.py
src/transformers/models/hubert/configuration_hubert.py
src/transformers/models/layoutlmv2/feature_extraction_layoutlmv2.py
src/transformers/models/layoutlmv2/tokenization_layoutlmv2.py
src/transformers/models/layoutlmv2/tokenization_layoutlmv2_fast.py
src/transformers/models/luke/tokenization_luke.py
src/transformers/models/rag/retrieval_rag.py
src/transformers/models/roformer/tokenization_roformer.py
src/transformers/models/roformer/tokenization_utils.py
src/transformers/models/speech_encoder_decoder/modeling_speech_encoder_decoder.py
src/transformers/models/speech_to_text/configuration_speech_to_text.py
src/transformers/models/speech_to_text/feature_extraction_speech_to_text.py
src/transformers/models/squeezebert/modeling_squeezebert.py
src/transformers/models/tapas/modeling_tapas.py
src/transformers/models/tapas/tokenization_tapas.py
src/transformers/models/transfo_xl/tokenization_transfo_xl.py
src/transformers/models/visual_bert/modeling_visual_bert.py
src/transformers/models/vit/feature_extraction_vit.py
src/transformers/models/wav2vec2/configuration_wav2vec2.py
src/transformers/models/wav2vec2/feature_extraction_wav2vec2.py
src/transformers/models/wav2vec2/modeling_wav2vec2.py
src/transformers/models/xlm_prophetnet/tokenization_xlm_prophetnet.py
src/transformers/onnx/features.py
src/transformers/tokenization_utils.py
src/transformers/tokenization_utils_base.py
src/transformers/trainer.py
src/transformers/training_args.py
==================
319beb64e;Dhananjay Shettigar;2021-10-07 18:39:01 +0530;#12789 Replace assert statements with exceptions (#13909)
* #12789 Replace assert statements with exceptions

* fix-copies: made copy changes to utils_qa.py in examples/pytorch/question-answering and examples/tensorflow/question-answering

* minor refactor for clarity
==

examples/flax/question-answering/utils_qa.py
examples/pytorch/question-answering/utils_qa.py
examples/tensorflow/question-answering/utils_qa.py
==================
279ce5b70;Jay Zhang;2021-10-07 18:07:02 +0800;Add an example of exporting BartModel + BeamSearch to ONNX module. (#13765)
* Add all example files.

* Reformat files by black.

* Style.

* Remove unused imports.

Co-authored-by: Morgan Funtowicz <funtowiczmo@gmail.com>
==

examples/onnx/pytorch/translation/bart_onnx/generation_onnx.py
examples/onnx/pytorch/translation/bart_onnx/reduce_onnx_size.py
examples/onnx/pytorch/translation/requirements.txt
examples/onnx/pytorch/translation/run_onnx_exporter.py
==================
0d309ce39;–ú–∞–∫—Å–∏–º –ó–∞—è–∫–∏–Ω;2021-10-07 12:14:23 +0500;Raise exceptions instead of asserts (#13907)

==

utils/download_glue_data.py
==================
5be59a364;Lysandre;2021-10-06 12:58:47 -0400;Deploy docs for v4.11.3

==

.circleci/deploy.sh
docs/source/_static/js/custom.js
==================
5d390e9ee;Anton Lozhkov;2021-10-06 19:40:51 +0300;Fix nan-loss condition (#13911)

==

src/transformers/trainer.py
==================
8f2c07d3c;Sylvain Gugger;2021-10-06 11:52:28 -0400;Fix hp search for non sigopt backends (#13897)

==

src/transformers/trainer.py
==================
77770ec79;Yanming Wang;2021-10-06 04:54:54 -0700;Fix trainer logging_nan_inf_filter in torch_xla mode (#13896)
* Fix logging_nan_inf_filter in torch_xla mode

* Update src/transformers/trainer.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Fix format

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

src/transformers/trainer.py
==================
aea7c5b0c;yssjtu;2021-10-06 15:20:41 +0800;T5ForConditionalGeneration: enabling using past_key_values and labels in training (#13805)
* enabling using past_key_values together with labels when training in T5ForConditionalGeneration

* test

* Enable past_key_values in T5ForconditionalGeneration while training.

* delete comments
==

src/transformers/models/t5/modeling_t5.py
==================
dac779814;Akul Agrawal;2021-10-06 08:40:24 +0530;Update run_qa.py (#13857)

==

examples/pytorch/question-answering/run_qa.py
==================
013bdc6d6;Nicolas Patry;2021-10-06 05:06:47 +0200;Fixing Backward compatiblity for zero-shot (#13855)
Fixes #13846
==

src/transformers/pipelines/zero_shot_classification.py
tests/test_pipelines_zero_shot.py
==================
9f58becc8;David del R√≠o Medina;2021-10-06 05:02:44 +0200;Replace assert statements with exceptions (#13871)

==

src/transformers/file_utils.py
==================
155b23008;Md Saiful Islam Sayef;2021-10-06 04:47:11 +0200;Update FSNER code in examples->research_projects->fsner (#13864)
* Add example use of few-shot named entity recognition model in research_projects folder.

* Apply suggestions from code review

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>

* Update fsner example README.md.

- Change wrong import FSNERTokenizerWrapper to FSNERTokenizerUtils in the example code
- Add a link to the model identifier

* Update examples/research_projects/fsner/src/fsner/model.py

Fix spelling mistake in the default parameter of pretrained model name.

Co-authored-by: Stefan Schweter <stefan@schweter.it>

* Add example use of few-shot named entity recognition model in research_projects folder.

* Apply suggestions from code review

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>

* Update fsner example README.md.

- Change wrong import FSNERTokenizerWrapper to FSNERTokenizerUtils in the example code
- Add a link to the model identifier

* Update examples/research_projects/fsner/src/fsner/model.py

Fix spelling mistake in the default parameter of pretrained model name.

Co-authored-by: Stefan Schweter <stefan@schweter.it>

* Run Checking/fixing examples/flax/language-modeling/run_clm_flax.py examples/flax/question-answering/run_qa.py examples/flax/question-answering/utils_qa.py examples/flax/token-classification/run_flax_ner.py examples/legacy/multiple_choice/utils_multiple_choice.py examples/legacy/seq2seq/seq2seq_trainer.py examples/legacy/token-classification/utils_ner.py examples/pytorch/image-classification/run_image_classification.py examples/pytorch/language-modeling/run_clm.py examples/pytorch/language-modeling/run_clm_no_trainer.py examples/pytorch/language-modeling/run_mlm.py examples/pytorch/language-modeling/run_mlm_no_trainer.py examples/pytorch/language-modeling/run_plm.py examples/pytorch/multiple-choice/run_swag.py examples/pytorch/multiple-choice/run_swag_no_trainer.py examples/pytorch/question-answering/run_qa.py examples/pytorch/question-answering/run_qa_beam_search.py examples/pytorch/question-answering/run_qa_beam_search_no_trainer.py examples/pytorch/question-answering/run_qa_no_trainer.py examples/pytorch/summarization/run_summarization.py examples/pytorch/summarization/run_summarization_no_trainer.py examples/pytorch/test_examples.py examples/pytorch/text-classification/run_glue.py examples/pytorch/text-classification/run_glue_no_trainer.py examples/pytorch/text-classification/run_xnli.py examples/pytorch/token-classification/run_ner.py examples/pytorch/token-classification/run_ner_no_trainer.py examples/pytorch/translation/run_translation.py examples/pytorch/translation/run_translation_no_trainer.py examples/research_projects/adversarial/utils_hans.py examples/research_projects/distillation/grouped_batch_sampler.py examples/research_projects/fsner/setup.py examples/research_projects/fsner/src/fsner/__init__.py examples/research_projects/fsner/src/fsner/model.py examples/research_projects/fsner/src/fsner/tokenizer_utils.py examples/research_projects/jax-projects/big_bird/evaluate.py examples/research_projects/jax-projects/hybrid_clip/run_hybrid_clip.py examples/tensorflow/language-modeling/run_clm.py examples/tensorflow/multiple-choice/run_swag.py examples/tensorflow/question-answering/run_qa.py examples/tensorflow/summarization/run_summarization.py examples/tensorflow/text-classification/run_glue.py examples/tensorflow/translation/run_translation.py src/transformers/__init__.py src/transformers/commands/add_new_model.py src/transformers/configuration_utils.py src/transformers/convert_slow_tokenizer.py src/transformers/data/__init__.py src/transformers/data/data_collator.py src/transformers/data/datasets/glue.py src/transformers/data/datasets/language_modeling.py src/transformers/data/datasets/squad.py src/transformers/deepspeed.py src/transformers/dependency_versions_table.py src/transformers/feature_extraction_sequence_utils.py src/transformers/file_utils.py src/transformers/generation_flax_utils.py src/transformers/generation_logits_process.py src/transformers/generation_tf_utils.py src/transformers/generation_utils.py src/transformers/integrations.py src/transformers/modelcard.py src/transformers/modeling_flax_utils.py src/transformers/modeling_outputs.py src/transformers/modeling_tf_utils.py src/transformers/modeling_utils.py src/transformers/models/__init__.py src/transformers/models/albert/__init__.py src/transformers/models/albert/modeling_albert.py src/transformers/models/albert/modeling_flax_albert.py src/transformers/models/albert/tokenization_albert_fast.py src/transformers/models/auto/__init__.py src/transformers/models/auto/auto_factory.py src/transformers/models/auto/configuration_auto.py src/transformers/models/auto/dynamic.py src/transformers/models/auto/feature_extraction_auto.py src/transformers/models/auto/modeling_auto.py src/transformers/models/auto/modeling_flax_auto.py src/transformers/models/auto/modeling_tf_auto.py src/transformers/models/auto/tokenization_auto.py src/transformers/models/bart/configuration_bart.py src/transformers/models/bart/modeling_bart.py src/transformers/models/bart/modeling_flax_bart.py src/transformers/models/bart/modeling_tf_bart.py src/transformers/models/barthez/tokenization_barthez_fast.py src/transformers/models/beit/__init__.py src/transformers/models/beit/configuration_beit.py src/transformers/models/beit/modeling_beit.py src/transformers/models/beit/modeling_flax_beit.py src/transformers/models/bert/configuration_bert.py src/transformers/models/bert/modeling_bert.py src/transformers/models/bert/modeling_flax_bert.py src/transformers/models/bert_generation/configuration_bert_generation.py src/transformers/models/bert_generation/modeling_bert_generation.py src/transformers/models/big_bird/configuration_big_bird.py src/transformers/models/big_bird/modeling_big_bird.py src/transformers/models/big_bird/modeling_flax_big_bird.py src/transformers/models/big_bird/tokenization_big_bird_fast.py src/transformers/models/bigbird_pegasus/configuration_bigbird_pegasus.py src/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py src/transformers/models/blenderbot/configuration_blenderbot.py src/transformers/models/blenderbot/modeling_blenderbot.py src/transformers/models/blenderbot/modeling_tf_blenderbot.py src/transformers/models/blenderbot_small/configuration_blenderbot_small.py src/transformers/models/blenderbot_small/modeling_blenderbot_small.py src/transformers/models/blenderbot_small/modeling_tf_blenderbot_small.py src/transformers/models/byt5/tokenization_byt5.py src/transformers/models/camembert/tokenization_camembert_fast.py src/transformers/models/canine/configuration_canine.py src/transformers/models/canine/modeling_canine.py src/transformers/models/clip/configuration_clip.py src/transformers/models/clip/convert_clip_original_pytorch_to_hf.py src/transformers/models/clip/modeling_clip.py src/transformers/models/clip/modeling_flax_clip.py src/transformers/models/clip/tokenization_clip.py src/transformers/models/convbert/modeling_convbert.py src/transformers/models/ctrl/configuration_ctrl.py src/transformers/models/deberta/modeling_tf_deberta.py src/transformers/models/deberta_v2/__init__.py src/transformers/models/deberta_v2/modeling_deberta_v2.py src/transformers/models/deberta_v2/modeling_tf_deberta_v2.py src/transformers/models/deit/configuration_deit.py src/transformers/models/deit/modeling_deit.py src/transformers/models/detr/configuration_detr.py src/transformers/models/detr/modeling_detr.py src/transformers/models/distilbert/__init__.py src/transformers/models/distilbert/configuration_distilbert.py src/transformers/models/distilbert/modeling_distilbert.py src/transformers/models/distilbert/modeling_flax_distilbert.py src/transformers/models/dpr/configuration_dpr.py src/transformers/models/dpr/modeling_dpr.py src/transformers/models/electra/modeling_electra.py src/transformers/models/electra/modeling_flax_electra.py src/transformers/models/encoder_decoder/__init__.py src/transformers/models/encoder_decoder/modeling_encoder_decoder.py src/transformers/models/encoder_decoder/modeling_flax_encoder_decoder.py src/transformers/models/flaubert/configuration_flaubert.py src/transformers/models/flaubert/modeling_flaubert.py src/transformers/models/fnet/__init__.py src/transformers/models/fnet/configuration_fnet.py src/transformers/models/fnet/convert_fnet_original_flax_checkpoint_to_pytorch.py src/transformers/models/fnet/modeling_fnet.py src/transformers/models/fnet/tokenization_fnet.py src/transformers/models/fnet/tokenization_fnet_fast.py src/transformers/models/fsmt/configuration_fsmt.py src/transformers/models/fsmt/modeling_fsmt.py src/transformers/models/funnel/configuration_funnel.py src/transformers/models/gpt2/__init__.py src/transformers/models/gpt2/configuration_gpt2.py src/transformers/models/gpt2/modeling_flax_gpt2.py src/transformers/models/gpt2/modeling_gpt2.py src/transformers/models/gpt2/modeling_tf_gpt2.py src/transformers/models/gpt_neo/configuration_gpt_neo.py src/transformers/models/gpt_neo/modeling_gpt_neo.py src/transformers/models/gptj/__init__.py src/transformers/models/gptj/configuration_gptj.py src/transformers/models/gptj/modeling_gptj.py src/transformers/models/herbert/tokenization_herbert_fast.py src/transformers/models/hubert/__init__.py src/transformers/models/hubert/configuration_hubert.py src/transformers/models/hubert/convert_hubert_original_s3prl_checkpoint_to_pytorch.py src/transformers/models/hubert/modeling_hubert.py src/transformers/models/hubert/modeling_tf_hubert.py src/transformers/models/ibert/modeling_ibert.py src/transformers/models/layoutlm/__init__.py src/transformers/models/layoutlm/configuration_layoutlm.py src/transformers/models/layoutlm/modeling_layoutlm.py src/transformers/models/layoutlmv2/__init__.py src/transformers/models/layoutlmv2/configuration_layoutlmv2.py src/transformers/models/layoutlmv2/feature_extraction_layoutlmv2.py src/transformers/models/layoutlmv2/modeling_layoutlmv2.py src/transformers/models/layoutlmv2/processing_layoutlmv2.py src/transformers/models/layoutlmv2/tokenization_layoutlmv2.py src/transformers/models/layoutlmv2/tokenization_layoutlmv2_fast.py src/transformers/models/led/configuration_led.py src/transformers/models/led/modeling_led.py src/transformers/models/longformer/modeling_longformer.py src/transformers/models/luke/configuration_luke.py src/transformers/models/luke/modeling_luke.py src/transformers/models/luke/tokenization_luke.py src/transformers/models/lxmert/configuration_lxmert.py src/transformers/models/m2m_100/configuration_m2m_100.py src/transformers/models/m2m_100/modeling_m2m_100.py src/transformers/models/m2m_100/tokenization_m2m_100.py src/transformers/models/marian/configuration_marian.py src/transformers/models/marian/modeling_flax_marian.py src/transformers/models/marian/modeling_marian.py src/transformers/models/marian/modeling_tf_marian.py src/transformers/models/mbart/configuration_mbart.py src/transformers/models/mbart/modeling_flax_mbart.py src/transformers/models/mbart/modeling_mbart.py src/transformers/models/mbart/tokenization_mbart.py src/transformers/models/mbart/tokenization_mbart_fast.py src/transformers/models/mbart50/tokenization_mbart50.py src/transformers/models/mbart50/tokenization_mbart50_fast.py src/transformers/models/megatron_bert/configuration_megatron_bert.py src/transformers/models/megatron_bert/convert_megatron_bert_checkpoint.py src/transformers/models/megatron_bert/modeling_megatron_bert.py src/transformers/models/megatron_gpt2/convert_megatron_gpt2_checkpoint.py src/transformers/models/openai/configuration_openai.py src/transformers/models/pegasus/__init__.py src/transformers/models/pegasus/configuration_pegasus.py src/transformers/models/pegasus/modeling_flax_pegasus.py src/transformers/models/pegasus/modeling_pegasus.py src/transformers/models/pegasus/modeling_tf_pegasus.py src/transformers/models/pegasus/tokenization_pegasus_fast.py src/transformers/models/prophetnet/configuration_prophetnet.py src/transformers/models/prophetnet/modeling_prophetnet.py src/transformers/models/rag/modeling_rag.py src/transformers/models/rag/modeling_tf_rag.py src/transformers/models/reformer/configuration_reformer.py src/transformers/models/reformer/tokenization_reformer_fast.py src/transformers/models/rembert/configuration_rembert.py src/transformers/models/rembert/modeling_rembert.py src/transformers/models/rembert/tokenization_rembert_fast.py src/transformers/models/roberta/modeling_flax_roberta.py src/transformers/models/roberta/modeling_roberta.py src/transformers/models/roberta/modeling_tf_roberta.py src/transformers/models/roformer/configuration_roformer.py src/transformers/models/roformer/modeling_roformer.py src/transformers/models/speech_encoder_decoder/__init__.py src/transformers/models/speech_encoder_decoder/configuration_speech_encoder_decoder.py src/transformers/models/speech_encoder_decoder/convert_speech_to_text_wav2vec2_seq2seq_original_to_pytorch.py src/transformers/models/speech_encoder_decoder/modeling_speech_encoder_decoder.py src/transformers/models/speech_to_text/configuration_speech_to_text.py src/transformers/models/speech_to_text/feature_extraction_speech_to_text.py src/transformers/models/speech_to_text/modeling_speech_to_text.py src/transformers/models/speech_to_text_2/__init__.py src/transformers/models/speech_to_text_2/configuration_speech_to_text_2.py src/transformers/models/speech_to_text_2/modeling_speech_to_text_2.py src/transformers/models/speech_to_text_2/processing_speech_to_text_2.py src/transformers/models/speech_to_text_2/tokenization_speech_to_text_2.py src/transformers/models/splinter/configuration_splinter.py src/transformers/models/splinter/modeling_splinter.py src/transformers/models/t5/configuration_t5.py src/transformers/models/t5/modeling_flax_t5.py src/transformers/models/t5/modeling_t5.py src/transformers/models/t5/modeling_tf_t5.py src/transformers/models/t5/tokenization_t5_fast.py src/transformers/models/tapas/__init__.py src/transformers/models/tapas/configuration_tapas.py src/transformers/models/tapas/convert_tapas_original_tf_checkpoint_to_pytorch.py src/transformers/models/tapas/modeling_tapas.py src/transformers/models/tapas/tokenization_tapas.py src/transformers/models/transfo_xl/configuration_transfo_xl.py src/transformers/models/visual_bert/modeling_visual_bert.py src/transformers/models/vit/configuration_vit.py src/transformers/models/vit/convert_dino_to_pytorch.py src/transformers/models/vit/modeling_flax_vit.py src/transformers/models/vit/modeling_vit.py src/transformers/models/wav2vec2/__init__.py src/transformers/models/wav2vec2/configuration_wav2vec2.py src/transformers/models/wav2vec2/convert_wav2vec2_original_s3prl_checkpoint_to_pytorch.py src/transformers/models/wav2vec2/feature_extraction_wav2vec2.py src/transformers/models/wav2vec2/modeling_flax_wav2vec2.py src/transformers/models/wav2vec2/modeling_wav2vec2.py src/transformers/models/wav2vec2/tokenization_wav2vec2.py src/transformers/models/xlm/configuration_xlm.py src/transformers/models/xlm_roberta/tokenization_xlm_roberta.py src/transformers/models/xlm_roberta/tokenization_xlm_roberta_fast.py src/transformers/models/xlnet/configuration_xlnet.py src/transformers/models/xlnet/tokenization_xlnet_fast.py src/transformers/onnx/convert.py src/transformers/onnx/features.py src/transformers/optimization.py src/transformers/pipelines/__init__.py src/transformers/pipelines/audio_classification.py src/transformers/pipelines/automatic_speech_recognition.py src/transformers/pipelines/base.py src/transformers/pipelines/conversational.py src/transformers/pipelines/feature_extraction.py src/transformers/pipelines/fill_mask.py src/transformers/pipelines/image_classification.py src/transformers/pipelines/object_detection.py src/transformers/pipelines/question_answering.py src/transformers/pipelines/table_question_answering.py src/transformers/pipelines/text2text_generation.py src/transformers/pipelines/text_classification.py src/transformers/pipelines/text_generation.py src/transformers/pipelines/token_classification.py src/transformers/pipelines/zero_shot_classification.py src/transformers/testing_utils.py src/transformers/tokenization_utils.py src/transformers/tokenization_utils_base.py src/transformers/tokenization_utils_fast.py src/transformers/trainer.py src/transformers/trainer_callback.py src/transformers/trainer_pt_utils.py src/transformers/trainer_seq2seq.py src/transformers/trainer_utils.py src/transformers/training_args.py src/transformers/training_args_seq2seq.py src/transformers/utils/dummy_detectron2_objects.py src/transformers/utils/dummy_flax_objects.py src/transformers/utils/dummy_pt_objects.py src/transformers/utils/dummy_tf_objects.py src/transformers/utils/dummy_tokenizers_objects.py src/transformers/utils/dummy_vision_objects.py tests/deepspeed/test_deepspeed.py tests/sagemaker/conftest.py tests/sagemaker/test_multi_node_data_parallel.py tests/test_configuration_auto.py tests/test_configuration_common.py tests/test_data_collator.py tests/test_feature_extraction_auto.py tests/test_feature_extraction_layoutlmv2.py tests/test_feature_extraction_speech_to_text.py tests/test_feature_extraction_wav2vec2.py tests/test_file_utils.py tests/test_modeling_auto.py tests/test_modeling_bart.py tests/test_modeling_beit.py tests/test_modeling_bert.py tests/test_modeling_clip.py tests/test_modeling_common.py tests/test_modeling_convbert.py tests/test_modeling_deit.py tests/test_modeling_distilbert.py tests/test_modeling_encoder_decoder.py tests/test_modeling_flaubert.py tests/test_modeling_flax_albert.py tests/test_modeling_flax_bart.py tests/test_modeling_flax_beit.py tests/test_modeling_flax_distilbert.py tests/test_modeling_flax_encoder_decoder.py tests/test_modeling_flax_gpt2.py tests/test_modeling_flax_gpt_neo.py tests/test_modeling_flax_mt5.py tests/test_modeling_flax_pegasus.py tests/test_modeling_fnet.py tests/test_modeling_gpt2.py tests/test_modeling_gpt_neo.py tests/test_modeling_gptj.py tests/test_modeling_hubert.py tests/test_modeling_layoutlmv2.py tests/test_modeling_pegasus.py tests/test_modeling_rag.py tests/test_modeling_reformer.py tests/test_modeling_speech_encoder_decoder.py tests/test_modeling_speech_to_text.py tests/test_modeling_speech_to_text_2.py tests/test_modeling_tf_auto.py tests/test_modeling_tf_deberta_v2.py tests/test_modeling_tf_hubert.py tests/test_modeling_tf_pytorch.py tests/test_modeling_tf_wav2vec2.py tests/test_modeling_wav2vec2.py tests/test_onnx_v2.py tests/test_pipelines_audio_classification.py tests/test_pipelines_automatic_speech_recognition.py tests/test_pipelines_common.py tests/test_pipelines_conversational.py tests/test_pipelines_feature_extraction.py tests/test_pipelines_fill_mask.py tests/test_pipelines_image_classification.py tests/test_pipelines_object_detection.py tests/test_pipelines_question_answering.py tests/test_pipelines_summarization.py tests/test_pipelines_table_question_answering.py tests/test_pipelines_text2text_generation.py tests/test_pipelines_text_classification.py tests/test_pipelines_text_generation.py tests/test_pipelines_token_classification.py tests/test_pipelines_translation.py tests/test_pipelines_zero_shot.py tests/test_processor_layoutlmv2.py tests/test_processor_wav2vec2.py tests/test_sequence_feature_extraction_common.py tests/test_tokenization_auto.py tests/test_tokenization_byt5.py tests/test_tokenization_canine.py tests/test_tokenization_common.py tests/test_tokenization_fnet.py tests/test_tokenization_layoutlmv2.py tests/test_tokenization_luke.py tests/test_tokenization_mbart.py tests/test_tokenization_mbart50.py tests/test_tokenization_speech_to_text_2.py tests/test_tokenization_t5.py tests/test_tokenization_tapas.py tests/test_tokenization_xlm_roberta.py tests/test_trainer.py tests/test_trainer_distributed.py tests/test_trainer_tpu.py tests/test_utils_check_copies.py utils/check_copies.py utils/check_repo.py utils/notification_service.py utils/release.py utils/tests_fetcher.py
python utils/custom_init_isort.py
python utils/style_doc.py src/transformers docs/source --max_len 119
running deps_table_update
updating src/transformers/dependency_versions_table.py
python utils/check_copies.py
python utils/check_table.py
python utils/check_dummies.py
python utils/check_repo.py
Checking all models are public.
Checking all models are properly tested.
Checking all objects are properly documented.
Checking all models are in at least one auto class.
python utils/check_inits.py
python utils/tests_fetcher.py --sanity_check and fix suggested changes.

* Run black examples tests src utils
isort examples tests src utils
Skipped 1 files
make autogenerate_code
make[1]: Entering directory '/mnt/c/Users/Admin/Desktop/Home/Projects/transformers'
running deps_table_update
updating src/transformers/dependency_versions_table.py
make[1]: Leaving directory '/mnt/c/Users/Admin/Desktop/Home/Projects/transformers'
make extra_style_checks
make[1]: Entering directory '/mnt/c/Users/Admin/Desktop/Home/Projects/transformers'
python utils/custom_init_isort.py
python utils/style_doc.py src/transformers docs/source --max_len 119
make[1]: Leaving directory '/mnt/c/Users/Admin/Desktop/Home/Projects/transformers' for reformatting code.

* Add installation dependencies for examples/research_projects/fsner.

* Add support to pass in variable numbers of examples to FSNER model.

* Retrieve start_token_id and end_token_id from tokenizer instead of hardcoding in the FSNER model.

* Run black examples tests src utils
isort examples tests src utils
Skipped 1 files
make autogenerate_code
make[1]: Entering directory '/home/saif/transformers'
running deps_table_update
updating src/transformers/dependency_versions_table.py
make[1]: Leaving directory '/home/saif/transformers'
make extra_style_checks
make[1]: Entering directory '/home/saif/transformers'
python utils/custom_init_isort.py
python utils/style_doc.py src/transformers docs/source --max_len 119
make[1]: Leaving directory '/home/saif/transformers' for FSNER

* Update FSNER readme.md with a header image.

* Update FSNER readme

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>
Co-authored-by: Stefan Schweter <stefan@schweter.it>
==

examples/research_projects/fsner/README.md
examples/research_projects/fsner/src/fsner/model.py
examples/research_projects/fsner/src/fsner/tokenizer_utils.py
==================
e7b16f33a;Nicolas Patry;2021-10-06 04:44:31 +0200;Fixing GPU for token-classification in a better way. (#13856)
Co-authored-by:  Pierre Snell <pierre.snell@botpress.com>

Co-authored-by: Pierre Snell <pierre.snell@botpress.com>
==

src/transformers/pipelines/base.py
src/transformers/pipelines/token_classification.py
tests/test_pipelines_token_classification.py
==================
7d83655da;Sylvain Gugger;2021-10-05 22:43:16 -0400;Autodocument the list of ONNX-supported models (#13884)

==

docs/source/serialization.rst
utils/check_table.py
==================
36fc40162;Hyunwoong Ko;2021-10-06 09:42:12 +0900;Update parallelism.md (#13892)
* Update parallelism.md

* Update docs/source/parallelism.md

Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>

* Update docs/source/parallelism.md

Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>

* Update docs/source/parallelism.md

Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>

* Update docs/source/parallelism.md

Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>

* Update docs/source/parallelism.md

Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>

* Update docs/source/parallelism.md

Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>

Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>
==

docs/source/parallelism.md
==================
7af7d7ce0;Siarhei Melnik;2021-10-06 01:08:48 +0300;fix: replace asserts by error (#13894)

==

src/transformers/models/distilbert/modeling_flax_distilbert.py
==================
f099249cf;Boris Dayma;2021-10-05 15:27:22 -0500;fix(integrations): consider test metrics (#13888)

==

src/transformers/integrations.py
==================
0ddadbf0a;Nicolas Patry;2021-10-05 16:08:58 +0200;Fixing question-answering with long contexts  (#13873)
* Tmp.

* Fixing BC for question answering with long context.

* Capping model_max_length to avoid tf overflow.

* Bad workaround bugged roberta.

* Fixing name.
==

src/transformers/pipelines/question_answering.py
tests/test_modeling_led.py
tests/test_modeling_reformer.py
tests/test_pipelines_question_answering.py
==================
1b74af76b;Zhaofeng Wu;2021-10-05 06:04:39 -0700;Allow dataset to be an optional argument for (Distributed)LengthGroupedSampler (#13820)
* Allow dataset to be an optional argument for (Distributed)LengthGroupedSampler

* Fix
==

src/transformers/trainer.py
src/transformers/trainer_pt_utils.py
tests/test_trainer_utils.py
==================
d4e4efce6;Michael Benayoun;2021-10-05 14:19:47 +0200;Initial support for symbolic tracing with torch.fx allowing dynamic axes (#13579)
* Symbolic trace dynamic axes support for BERT like models (albert, bert, distilbert, mobilebert, electra, megatron-bert)
* Sanity checks before tracing that make sure the model to trace is supported
* Adapted to PyTorch 1.9

Co-authored-by: Michael Benayoun <michael@huggingface.co>
==

src/transformers/file_utils.py
src/transformers/models/distilbert/modeling_distilbert.py
src/transformers/utils/fx.py
src/transformers/utils/fx_transformations.py
tests/test_modeling_albert.py
tests/test_modeling_bert.py
tests/test_modeling_common.py
tests/test_modeling_distilbert.py
tests/test_modeling_electra.py
tests/test_modeling_megatron_bert.py
tests/test_modeling_mobilebert.py
==================
46efc5802;Alex Hedges;2021-10-05 08:09:10 -0400;Improve error message when loading models from Hub (#13836)
* Improve error message when loading models from Hub

* Adjust error message wording
==

src/transformers/configuration_utils.py
src/transformers/feature_extraction_utils.py
src/transformers/modeling_flax_utils.py
src/transformers/modeling_tf_utils.py
src/transformers/modeling_utils.py
src/transformers/tokenization_utils_base.py
==================
3a9c0f23b;Nicolas Patry;2021-10-05 13:46:10 +0200;Fixing empty prompts for text-generation when BOS exists. (#13859)
* Fixing empty prompts for text-generation when BOS exists.

* Fixing odd case with Pegasus.

* Fixing Bert is Assertion Error.
==

src/transformers/pipelines/text_generation.py
tests/test_pipelines_text_generation.py
==================
a6ea244f9;Yih-Dar;2021-10-05 13:00:13 +0200;Fix: save checkpoint after each epoch and push checkpoint to the hub (#13872)
Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>
==

examples/flax/summarization/run_summarization_flax.py
==================
7079a99e7;Nicolas Patry;2021-10-05 12:26:54 +0200;Fixing 1-length special tokens cut. (#13862)

==

src/transformers/tokenization_utils.py
tests/test_tokenization_common.py
==================
7051b8926;Sam Hardwick;2021-10-05 12:15:18 +0300;Update Tatoeba conversion (#13757)
* Update Tatoeba conversion
==

src/transformers/models/marian/convert_marian_tatoeba_to_pytorch.py
==================
12b4d66a8;Bram Vanroy;2021-10-04 22:28:52 +0200;Update no_* argument (HfArgumentParser) (#13865)
* update no_* argument

Changes the order so that the no_* argument is created after the original argument AND sets the default for this no_* argument to False

* import copy

* update test

* make style

* Use kwargs to set default=False

* make style
==

src/transformers/hf_argparser.py
tests/test_hf_argparser.py
==================
cc0a415e2;Nathan Raw;2021-10-04 12:49:51 -0600;:sparkles: update image classification example (#13824)
* :sparkles: update image classification example

* :pushpin: update reqs
==

examples/pytorch/image-classification/requirements.txt
examples/pytorch/image-classification/run_image_classification.py
==================
6c0884062;Evgeniy Zheltonozhskiy;2021-10-04 18:57:54 +0300;Fix broken link to distill models in docs (#13848)
* Fix broken link to distill models

* Missing symbol

* Fix spaces
==

docs/source/pretrained_models.rst
==================
3a8de58c5;Sidd Karamcheti;2021-10-04 04:37:09 -0700;Add Mistral GPT-2 Stability Tweaks (#13573)
* Add layer-wise scaling

* Add reorder & upcasting argument

* Add OpenAI GPT-2 weight initialization scheme

* start `layer_idx` count at zero for consistency

* disentangle attn and reordered and upscaled attn function

* rename `scale_attn_by_layer` to `scale_attn_by_layer_id`

* make autocast from amp compatible with pytorch<1.6

* fix docstring

* style fixes

* Add fixes from PR feedback, style tweaks

* Fix doc whitespace

* Reformat

* First pass scale_attn_by_layer_idx and reorder_and_upcast_attn tests

* Rename scale_attn_by_layer_idx, add tip

* Remove extra newline

* add test for weight initialization

* update code format

* add assert check weights are fp32

* remove assert

* Fix incorrect merge

* Fix shape mismatch in baddbmm

* Add generation test for Mistral flags

Co-authored-by: leandro <leandro.vonwerra@spoud.io>
Co-authored-by: Keshav Santhanam <keshav2@stanford.edu>
Co-authored-by: J38 <jebolton@stanford.edu>
==

docs/source/model_doc/gpt2.rst
src/transformers/models/gpt2/configuration_gpt2.py
src/transformers/models/gpt2/modeling_gpt2.py
tests/test_modeling_gpt2.py
==================
955fd4fea;Yaser Abdelaziz;2021-10-04 12:30:50 +0200;[docs/gpt-j] fix typo (#13851)

==

docs/source/model_doc/gptj.rst
==================
de948350c;Gunjan Chhablani;2021-10-04 16:00:21 +0530;Delete convert_multiberts_checkpoint_to_pytorch.py (#13852)

==

src/transformers/models/bert/convert_multiberts_checkpoint_to_pytorch.py
==================
bcc3f7b65;Stas Bekman;2021-10-01 11:42:08 -0700;include megatron_gpt2 in installed modules (#13834)

==

src/transformers/models/megatron_gpt2/__init__.py
==================
707f7eb18;Silviu Oprea;2021-10-01 18:36:57 +0100;Bart: check if decoder_inputs_embeds is set (#13800)
In BartForConditionalGeneration.forward, if labels are provided,
   decoder_input_ids are set to the labels shifted to the right.
   This is problematic: if decoder_inputs_embeds is also set,
   the call to self.model, which eventually gets to BartDecoder.forward,
   will raise an error.
   The fix is quite simple, similar to what is there already in
   BartModel.forward. Mainly, we should not
   compute decoder_input_ids if decoder_inputs_embeds is provided.

Co-authored-by: Silviu Vlad Oprea <silviuvo@amazon.co.uk>
==

src/transformers/models/bart/modeling_bart.py
src/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py
==================
421372806;Anton Lozhkov;2021-10-01 19:52:45 +0300;[Examples] Add an official audio classification example (#13722)
* Restore broken merge

* Additional args, DDP, remove CommonLanguage

* Update examples for V100, add training results

* Style

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Remove custom datasets for simplicity, apply suggestions from code review

* Add the attention_mask flag, reorganize README

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

examples/pytorch/audio-classification/README.md
examples/pytorch/audio-classification/requirements.txt
examples/pytorch/audio-classification/run_audio_classification.py
examples/pytorch/test_examples.py
==================
c4113721f;Arfon Smith;2021-10-01 15:41:27 +0100;Update CITATION.cff (#13833)

==

CITATION.cff
==================
90f980ed3;Yuta Hayashibe;2021-10-01 22:29:08 +0900;Fix warning situation: UserWarning: max_length is ignored when padding=True" (#13829)
* Removed wrong warning

* Raise a warning when `max_length` is given with wrong `truncation`

* Update the error message

* Update the warning message

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

src/transformers/tokenization_utils_base.py
==================
8bbb53e20;Suraj Patil;2021-10-01 01:14:33 +0530;skip gptj slow generate tests for now (#13809)

==

tests/test_modeling_gptj.py
==================
41436d3df;Patrick von Platen;2021-09-30 18:55:20 +0200;[DPR] Correct init (#13796)
* update

* add to docs and init

* make fix-copies
==

docs/source/model_doc/dpr.rst
src/transformers/__init__.py
src/transformers/models/dpr/__init__.py
src/transformers/models/dpr/modeling_dpr.py
src/transformers/utils/dummy_pt_objects.py
tests/test_modeling_dpr.py
==================
44eb8bdee;Patrick von Platen;2021-09-30 18:52:53 +0200;map only on one process (#13810)

==

examples/pytorch/language-modeling/run_clm_no_trainer.py
examples/pytorch/language-modeling/run_mlm_no_trainer.py
examples/pytorch/multiple-choice/run_swag_no_trainer.py
examples/pytorch/question-answering/run_qa_beam_search_no_trainer.py
examples/pytorch/question-answering/run_qa_no_trainer.py
examples/pytorch/summarization/run_summarization_no_trainer.py
examples/pytorch/text-classification/run_glue_no_trainer.py
examples/pytorch/token-classification/run_ner_no_trainer.py
examples/pytorch/translation/run_translation_no_trainer.py
==================
9a9805fcc;Gunjan Chhablani;2021-09-30 22:18:56 +0530;Add MultiBERTs conversion script (#13077)
* Init multibert checkpoint conversion script

* Rename conversion script

* Fix MultiBerts Conversion Script

* Apply suggestions from code review

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>
==

src/transformers/models/bert/convert_multiberts_checkpoint_to_pytorch.py
==================
e1d1c7c08;Stas Bekman;2021-09-30 09:26:49 -0700;[testing] auto-replay captured streams (#13803)

==

docs/source/testing.rst
src/transformers/testing_utils.py
==================
5f25855b3;Sylvain Gugger;2021-09-30 11:58:33 -0400;Update doc for v4.11.2

==

.circleci/deploy.sh
docs/source/_static/js/custom.js
==================
269c3d140;Sylvain Gugger;2021-09-30 11:32:40 -0400;Fix gather for TPU (#13813)

==

src/transformers/trainer_pt_utils.py
==================
7db2a79b3;Suraj Patil;2021-09-30 16:38:07 +0530;[examples/flax] use Repository API for push_to_hub (#13672)
* use Repository for push_to_hub

* update readme

* update other flax scripts

* update readme

* update qa example

* fix push_to_hub call

* fix typo

* fix more typos

* update readme

* use abosolute path to get repo name

* fix glue script
==

examples/flax/README.md
examples/flax/language-modeling/README.md
examples/flax/language-modeling/run_clm_flax.py
examples/flax/language-modeling/run_mlm_flax.py
examples/flax/language-modeling/run_t5_mlm_flax.py
examples/flax/question-answering/README.md
examples/flax/question-answering/run_qa.py
examples/flax/summarization/README.md
examples/flax/summarization/run_summarization_flax.py
examples/flax/text-classification/README.md
examples/flax/text-classification/run_flax_glue.py
examples/flax/token-classification/README.md
examples/flax/token-classification/run_flax_ner.py
examples/flax/vision/README.md
examples/flax/vision/run_image_classification.py
==================
b90096fe1;Stas Bekman;2021-09-29 13:45:19 -0700;[examples `run_glue.py`] missing requirements `scipy`, `sklearn` (#13768)
* missing requirement

* list both
==

examples/pytorch/text-classification/requirements.txt
==================
bf6118e70;Suraj Patil;2021-09-29 23:43:46 +0530;[docs/gpt-j] addd instructions for how minimize CPU RAM usage (#13795)
* add a note about tokenizer

* add  tips to load model is less RAM

* fix link

* fix more links
==

docs/source/model_doc/gptj.rst
==================
55695df0f;Sylvain Gugger;2021-09-29 12:09:54 -0400;Merge remote-tracking branch 'origin/master'

==
==================
cf4aa3597;Sylvain Gugger;2021-09-29 12:09:40 -0400;Update doc for v4.11.1

==

.circleci/deploy.sh
docs/source/_static/js/custom.js
==================
2a51b1551;Matt;2021-09-29 17:07:10 +0100;Add TF notebooks (#13793)

==

notebooks/README.md
==================
63cc5bda6;Sylvain Gugger;2021-09-29 11:48:48 -0400;Fix length of IterableDatasetShard and add test (#13792)
* Fix length of IterableDatasetShard and add test

* Add comments
==

src/transformers/trainer_pt_utils.py
tests/test_trainer_utils.py
utils/tests_fetcher.py
==================
7d84c3a48;Li-Huai (Allan) Lin;2021-09-29 23:18:59 +0800;Enable readme link synchronization (#13785)
* Enable readme link synchronization

* Style

* Reuse regex pattern

* Apply suggestions

* Update
==

README_zh-hans.md
README_zh-hant.md
tests/test_utils_check_copies.py
utils/check_copies.py
==================
a1ea3adb2;Nishant Prabhu;2021-09-29 19:20:15 +0530;Fix LayoutLM ONNX test error (#13710)
Fix LayoutLM ONNX test error
==

docs/source/serialization.rst
src/transformers/models/layoutlm/configuration_layoutlm.py
==================
3a8a8013a;Matt;2021-09-29 12:47:35 +0100;Keras callback to push to hub each epoch, or after N steps (#13773)
* Keras callback to push to hub each epoch, or after N steps

* Reworked the callback to use Repository

* Use an Enum for save_strategy

* Style pass

* Correct type for tokenizer

* Update src/transformers/keras_callbacks.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/keras_callbacks.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/keras_callbacks.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/keras_callbacks.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/keras_callbacks.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/keras_callbacks.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Adding print message to the final upload

* Adding print message to the final upload

* Change how we wait for the last process to finish

* is_done is a property, not a method, derp

* Docstrings and documentation

* Style pass

* Style edit

* Docstring reformat

* Docstring rewrite

* Replacing print with internal logger

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

docs/source/index.rst
docs/source/main_classes/keras_callbacks.rst
src/transformers/keras_callbacks.py
==================
aa018a795;Patrick von Platen;2021-09-29 10:30:00 +0200;up (#13777)

==

src/transformers/models/hubert/modeling_hubert.py
src/transformers/models/wav2vec2/modeling_wav2vec2.py
==================
a21ee1f99;Sylvain Gugger;2021-09-28 18:22:37 -0400;Implement len in IterableDatasetShard (#13780)

==

src/transformers/trainer_pt_utils.py
==================
83d3dc0f6;Sylvain Gugger;2021-09-28 14:21:17 -0400;Fix warning for gradient_checkpointing (#13767)

==

src/transformers/configuration_utils.py
==================
5e3b4a70d;Sylvain Gugger;2021-09-27 15:26:54 -0400;Fix filtering in test fetcher utils (#13766)

==

utils/tests_fetcher.py
==================
11c69b804;Lysandre;2021-09-27 14:19:38 -0400;Docs for version v4.11.0

==

.circleci/deploy.sh
docs/source/_static/js/custom.js
docs/source/conf.py
examples/flax/question-answering/run_qa.py
examples/flax/token-classification/run_flax_ner.py
examples/pytorch/image-classification/run_image_classification.py
examples/pytorch/language-modeling/run_clm.py
examples/pytorch/language-modeling/run_mlm.py
examples/pytorch/language-modeling/run_plm.py
examples/pytorch/multiple-choice/run_swag.py
examples/pytorch/question-answering/run_qa.py
examples/pytorch/question-answering/run_qa_beam_search.py
examples/pytorch/question-answering/run_qa_beam_search_no_trainer.py
examples/pytorch/question-answering/run_qa_no_trainer.py
examples/pytorch/speech-recognition/run_speech_recognition_ctc.py
examples/pytorch/summarization/run_summarization.py
examples/pytorch/text-classification/run_glue.py
examples/pytorch/text-classification/run_xnli.py
examples/pytorch/token-classification/run_ner.py
examples/pytorch/translation/run_translation.py
examples/tensorflow/multiple-choice/run_swag.py
examples/tensorflow/question-answering/run_qa.py
examples/tensorflow/summarization/run_summarization.py
examples/tensorflow/text-classification/run_glue.py
examples/tensorflow/translation/run_translation.py
setup.py
src/transformers/__init__.py
==================
dc193c906;Lysandre;2021-09-27 14:14:09 -0400;Release: v4.11.0

==

README.md
docs/source/conf.py
docs/source/index.rst
examples/flax/question-answering/run_qa.py
examples/flax/token-classification/run_flax_ner.py
examples/pytorch/image-classification/run_image_classification.py
examples/pytorch/language-modeling/run_clm.py
examples/pytorch/language-modeling/run_mlm.py
examples/pytorch/language-modeling/run_plm.py
examples/pytorch/multiple-choice/run_swag.py
examples/pytorch/question-answering/run_qa.py
examples/pytorch/question-answering/run_qa_beam_search.py
examples/pytorch/question-answering/run_qa_beam_search_no_trainer.py
examples/pytorch/question-answering/run_qa_no_trainer.py
examples/pytorch/speech-recognition/run_speech_recognition_ctc.py
examples/pytorch/summarization/run_summarization.py
examples/pytorch/text-classification/run_glue.py
examples/pytorch/text-classification/run_xnli.py
examples/pytorch/token-classification/run_ner.py
examples/pytorch/translation/run_translation.py
examples/tensorflow/multiple-choice/run_swag.py
examples/tensorflow/question-answering/run_qa.py
examples/tensorflow/summarization/run_summarization.py
examples/tensorflow/text-classification/run_glue.py
examples/tensorflow/translation/run_translation.py
setup.py
src/transformers/__init__.py
==================
1c9650008;Sylvain Gugger;2021-09-27 13:11:58 -0400;Fix gather for SageMaker model parallel

==

src/transformers/trainer_pt_utils.py
==================
4e0410e92;Sylvain Gugger;2021-09-27 11:57:18 -0400;Fix in gather for SM distributed

==

src/transformers/trainer_pt_utils.py
==================
367c2ef53;Matt;2021-09-27 14:47:07 +0100;Modified TF train_step (#13678)
Allows models to be compiled without a loss, and to use the internal loss computations for training with fit()
==

src/transformers/modeling_tf_utils.py
==================
e00bc7cd2;Sylvain Gugger;2021-09-27 07:43:38 -0400;Silence warning in gradient checkpointing when it's False (#13734)

==

src/transformers/configuration_utils.py
==================
3ffd18a61;Sylvain Gugger;2021-09-27 07:33:08 -0400;Fix loss computation in Trainer (#13760)
Co-authored-by: quantitative-technologies <james.hirschorn@quantitative-technologies.com>

Co-authored-by: quantitative-technologies <james.hirschorn@quantitative-technologies.com>
==

src/transformers/trainer.py
==================
3ccc27019;Xiaohan Zou;2021-09-27 18:29:12 +0800;Fix type annotations for `distributed_concat()` (#13746)
* Fix type annotations for `distributed_concat()`

* Use Any
==

src/transformers/trainer_pt_utils.py
==================
e0d31a898;Anton Lozhkov;2021-09-26 22:58:23 +0300;[Tests] Cast Hubert test models to fp16 (#13755)

==

tests/test_modeling_hubert.py
==================
400c5a158;Stas Bekman;2021-09-26 09:51:40 -0700;[megatron gpt checkpoint conversion] causal mask requires pos_embed dimension (#13735)

==

src/transformers/models/megatron_gpt2/convert_megatron_gpt2_checkpoint.py
==================
91df45516;Patrick von Platen;2021-09-26 09:03:45 +0200;[Trainer] Make sure shown loss in distributed training is correctly averaged over all workers (#13681)
* push

* improve tr loss gather
==

src/transformers/trainer.py
==================
044eff5bf;Sylvain Gugger;2021-09-26 03:02:45 -0400;Update requirements for speech example (#13745)

==

examples/pytorch/_tests_requirements.txt
==================
067413fb7;Patrick von Platen;2021-09-25 21:20:21 +0200;finish (#13743)

==

tests/test_modeling_flax_beit.py
==================
a8ec00292;Sylvain Gugger;2021-09-25 12:47:39 -0400;Update test dependence for torch examples (#13738)

==

.circleci/config.yml
==================
469b80d4e;Patrick von Platen;2021-09-24 18:53:58 +0200;Update README.md

==

examples/pytorch/speech-recognition/README.md
==================
493643fff;Patrick von Platen;2021-09-24 18:32:35 +0200;up (#13733)

==

examples/pytorch/speech-recognition/README.md
examples/pytorch/speech-recognition/run_speech_recognition_ctc.py
==================
38580455d;Gunjan Chhablani;2021-09-24 19:21:46 +0530;Add model card creation snippet to example scripts (#13730)
* Update run_glue.py

* Update run_glue.py

* Add model creation snippet to other scripts

* Fix style
==

examples/pytorch/language-modeling/run_clm.py
examples/pytorch/language-modeling/run_mlm.py
examples/pytorch/language-modeling/run_plm.py
examples/pytorch/multiple-choice/run_swag.py
examples/pytorch/question-answering/run_qa.py
examples/pytorch/question-answering/run_qa_beam_search.py
examples/pytorch/summarization/run_summarization.py
examples/pytorch/text-classification/run_glue.py
examples/pytorch/token-classification/run_ner.py
examples/pytorch/translation/run_translation.py
==================
66b01ce86;Yuta Hayashibe;2021-09-24 22:14:23 +0900;Warn for unexpected argument combinations (#13509)
* Warn for unexpected argument combinations

* Updated the waning message for pad_to_max_length
==

src/transformers/tokenization_utils_base.py
==================
e579f855f;Patrick von Platen;2021-09-24 14:57:49 +0200;up (#13729)

==

tests/test_tokenization_fnet.py
==================
0eabe4920;Nicolas Patry;2021-09-24 13:38:17 +0200;Fixing zero-shot backward compatiblity (#13725)
Fixes #13697
==

src/transformers/pipelines/zero_shot_classification.py
tests/test_pipelines_zero_shot.py
==================
a2ef9c544;Tommy Chiang;2021-09-24 16:31:23 +0800;Use torch.unique_consecutive to check same element (#13637)
We use `torch.unique` here only to check whether every elements have
the same value.
Therefore, we can use `torch.unique_consecutive` here.

This function eliminates all but the first element from every consecutive
group of equivalent elements.
Like, if we apply this function to `[1, 2, 2, 1]`, it will result in
`[1, 2, 1]`.

As you could see, this is enough for checking whether every elements
have the same value.

Since `torch.unique_consecutive` do less thing, it is much more faster.
On my computer, it is 25x faster on GPU and 15x faster on CPU.
==

src/transformers/models/bart/modeling_bart.py
src/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py
src/transformers/models/led/modeling_led.py
src/transformers/models/mbart/modeling_mbart.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_{{cookiecutter.lowercase_modelname}}.py
==================
95f888fd6;Patrick von Platen;2021-09-24 09:53:37 +0200;Update README.md

==

examples/pytorch/speech-recognition/README.md
==================
678bb248d;Josh Devins;2021-09-24 08:52:15 +0200;Make assertions only if actually chunking forward (#13598)
This moves the assertion on checking input dimensions into a block that will only be called if the function is actually going to do chunking forward. This is often not the case at inference time and PyTorch tracing a model with this assertion in it leads to a tracing warning.

TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  input_tensor.shape[chunk_dim] == tensor_shape for input_tensor in input_tensors
==

src/transformers/modeling_utils.py
==================
4a320f6c9;Patrick von Platen;2021-09-24 07:01:11 +0200;[ASR] Add official ASR CTC example to `examples/pytorch/speech-recognition` (#13620)
* up

* rename

* add asr example

* add auto feature extractor

* some more fixes

* correct layerdrop

* correct for multi-gpu dist

* clean up

* refactor

* refactor

* more fixes

* more fixes

* clean-up

* finish

* up

* Apply suggestions from code review

* fix isort

* update

* up

* add note

* apply surajs suggestions

* Apply suggestions from code review

Co-authored-by: Suraj Patil <surajp815@gmail.com>

* isort

* small change

* Apply suggestions from code review

Co-authored-by: Anton Lozhkov <aglozhkov@gmail.com>

* Apply suggestions from code review

Co-authored-by: Anton Lozhkov <aglozhkov@gmail.com>

* add hubert

* Update examples/pytorch/speech-recognition/run_speech_recognition_ctc.py

Co-authored-by: Suraj Patil <surajp815@gmail.com>
Co-authored-by: Anton Lozhkov <aglozhkov@gmail.com>
==

examples/pytorch/speech-recognition/README.md
examples/pytorch/speech-recognition/requirements.txt
examples/pytorch/speech-recognition/run_speech_recognition_ctc.py
examples/pytorch/test_examples.py
src/transformers/models/hubert/configuration_hubert.py
src/transformers/models/wav2vec2/configuration_wav2vec2.py
==================
41c186d2a;Lysandre Debut;2021-09-23 17:08:29 -0400;Replace torch.set_grad_enabled by torch.no_grad (#13703)

==

src/transformers/onnx/convert.py
==================
f888e5c37;Md Saiful Islam Sayef;2021-09-23 23:04:15 +0200;Add FSNER example in research_projects (#13712)
* Add example use of few-shot named entity recognition model in research_projects folder.

* Apply suggestions from code review

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>

* Update fsner example README.md.

- Change wrong import FSNERTokenizerWrapper to FSNERTokenizerUtils in the example code
- Add a link to the model identifier

* Update examples/research_projects/fsner/src/fsner/model.py

Fix spelling mistake in the default parameter of pretrained model name.

Co-authored-by: Stefan Schweter <stefan@schweter.it>

* Add example use of few-shot named entity recognition model in research_projects folder.

* Apply suggestions from code review

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>

* Update fsner example README.md.

- Change wrong import FSNERTokenizerWrapper to FSNERTokenizerUtils in the example code
- Add a link to the model identifier

* Update examples/research_projects/fsner/src/fsner/model.py

Fix spelling mistake in the default parameter of pretrained model name.

Co-authored-by: Stefan Schweter <stefan@schweter.it>

* Run Checking/fixing examples/flax/language-modeling/run_clm_flax.py examples/flax/question-answering/run_qa.py examples/flax/question-answering/utils_qa.py examples/flax/token-classification/run_flax_ner.py examples/legacy/multiple_choice/utils_multiple_choice.py examples/legacy/seq2seq/seq2seq_trainer.py examples/legacy/token-classification/utils_ner.py examples/pytorch/image-classification/run_image_classification.py examples/pytorch/language-modeling/run_clm.py examples/pytorch/language-modeling/run_clm_no_trainer.py examples/pytorch/language-modeling/run_mlm.py examples/pytorch/language-modeling/run_mlm_no_trainer.py examples/pytorch/language-modeling/run_plm.py examples/pytorch/multiple-choice/run_swag.py examples/pytorch/multiple-choice/run_swag_no_trainer.py examples/pytorch/question-answering/run_qa.py examples/pytorch/question-answering/run_qa_beam_search.py examples/pytorch/question-answering/run_qa_beam_search_no_trainer.py examples/pytorch/question-answering/run_qa_no_trainer.py examples/pytorch/summarization/run_summarization.py examples/pytorch/summarization/run_summarization_no_trainer.py examples/pytorch/test_examples.py examples/pytorch/text-classification/run_glue.py examples/pytorch/text-classification/run_glue_no_trainer.py examples/pytorch/text-classification/run_xnli.py examples/pytorch/token-classification/run_ner.py examples/pytorch/token-classification/run_ner_no_trainer.py examples/pytorch/translation/run_translation.py examples/pytorch/translation/run_translation_no_trainer.py examples/research_projects/adversarial/utils_hans.py examples/research_projects/distillation/grouped_batch_sampler.py examples/research_projects/fsner/setup.py examples/research_projects/fsner/src/fsner/__init__.py examples/research_projects/fsner/src/fsner/model.py examples/research_projects/fsner/src/fsner/tokenizer_utils.py examples/research_projects/jax-projects/big_bird/evaluate.py examples/research_projects/jax-projects/hybrid_clip/run_hybrid_clip.py examples/tensorflow/language-modeling/run_clm.py examples/tensorflow/multiple-choice/run_swag.py examples/tensorflow/question-answering/run_qa.py examples/tensorflow/summarization/run_summarization.py examples/tensorflow/text-classification/run_glue.py examples/tensorflow/translation/run_translation.py src/transformers/__init__.py src/transformers/commands/add_new_model.py src/transformers/configuration_utils.py src/transformers/convert_slow_tokenizer.py src/transformers/data/__init__.py src/transformers/data/data_collator.py src/transformers/data/datasets/glue.py src/transformers/data/datasets/language_modeling.py src/transformers/data/datasets/squad.py src/transformers/deepspeed.py src/transformers/dependency_versions_table.py src/transformers/feature_extraction_sequence_utils.py src/transformers/file_utils.py src/transformers/generation_flax_utils.py src/transformers/generation_logits_process.py src/transformers/generation_tf_utils.py src/transformers/generation_utils.py src/transformers/integrations.py src/transformers/modelcard.py src/transformers/modeling_flax_utils.py src/transformers/modeling_outputs.py src/transformers/modeling_tf_utils.py src/transformers/modeling_utils.py src/transformers/models/__init__.py src/transformers/models/albert/__init__.py src/transformers/models/albert/modeling_albert.py src/transformers/models/albert/modeling_flax_albert.py src/transformers/models/albert/tokenization_albert_fast.py src/transformers/models/auto/__init__.py src/transformers/models/auto/auto_factory.py src/transformers/models/auto/configuration_auto.py src/transformers/models/auto/dynamic.py src/transformers/models/auto/feature_extraction_auto.py src/transformers/models/auto/modeling_auto.py src/transformers/models/auto/modeling_flax_auto.py src/transformers/models/auto/modeling_tf_auto.py src/transformers/models/auto/tokenization_auto.py src/transformers/models/bart/configuration_bart.py src/transformers/models/bart/modeling_bart.py src/transformers/models/bart/modeling_flax_bart.py src/transformers/models/bart/modeling_tf_bart.py src/transformers/models/barthez/tokenization_barthez_fast.py src/transformers/models/beit/__init__.py src/transformers/models/beit/configuration_beit.py src/transformers/models/beit/modeling_beit.py src/transformers/models/beit/modeling_flax_beit.py src/transformers/models/bert/configuration_bert.py src/transformers/models/bert/modeling_bert.py src/transformers/models/bert/modeling_flax_bert.py src/transformers/models/bert_generation/configuration_bert_generation.py src/transformers/models/bert_generation/modeling_bert_generation.py src/transformers/models/big_bird/configuration_big_bird.py src/transformers/models/big_bird/modeling_big_bird.py src/transformers/models/big_bird/modeling_flax_big_bird.py src/transformers/models/big_bird/tokenization_big_bird_fast.py src/transformers/models/bigbird_pegasus/configuration_bigbird_pegasus.py src/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py src/transformers/models/blenderbot/configuration_blenderbot.py src/transformers/models/blenderbot/modeling_blenderbot.py src/transformers/models/blenderbot/modeling_tf_blenderbot.py src/transformers/models/blenderbot_small/configuration_blenderbot_small.py src/transformers/models/blenderbot_small/modeling_blenderbot_small.py src/transformers/models/blenderbot_small/modeling_tf_blenderbot_small.py src/transformers/models/byt5/tokenization_byt5.py src/transformers/models/camembert/tokenization_camembert_fast.py src/transformers/models/canine/configuration_canine.py src/transformers/models/canine/modeling_canine.py src/transformers/models/clip/configuration_clip.py src/transformers/models/clip/convert_clip_original_pytorch_to_hf.py src/transformers/models/clip/modeling_clip.py src/transformers/models/clip/modeling_flax_clip.py src/transformers/models/clip/tokenization_clip.py src/transformers/models/convbert/modeling_convbert.py src/transformers/models/ctrl/configuration_ctrl.py src/transformers/models/deberta/modeling_tf_deberta.py src/transformers/models/deberta_v2/__init__.py src/transformers/models/deberta_v2/modeling_deberta_v2.py src/transformers/models/deberta_v2/modeling_tf_deberta_v2.py src/transformers/models/deit/configuration_deit.py src/transformers/models/deit/modeling_deit.py src/transformers/models/detr/configuration_detr.py src/transformers/models/detr/modeling_detr.py src/transformers/models/distilbert/__init__.py src/transformers/models/distilbert/configuration_distilbert.py src/transformers/models/distilbert/modeling_distilbert.py src/transformers/models/distilbert/modeling_flax_distilbert.py src/transformers/models/dpr/configuration_dpr.py src/transformers/models/dpr/modeling_dpr.py src/transformers/models/electra/modeling_electra.py src/transformers/models/electra/modeling_flax_electra.py src/transformers/models/encoder_decoder/__init__.py src/transformers/models/encoder_decoder/modeling_encoder_decoder.py src/transformers/models/encoder_decoder/modeling_flax_encoder_decoder.py src/transformers/models/flaubert/configuration_flaubert.py src/transformers/models/flaubert/modeling_flaubert.py src/transformers/models/fnet/__init__.py src/transformers/models/fnet/configuration_fnet.py src/transformers/models/fnet/convert_fnet_original_flax_checkpoint_to_pytorch.py src/transformers/models/fnet/modeling_fnet.py src/transformers/models/fnet/tokenization_fnet.py src/transformers/models/fnet/tokenization_fnet_fast.py src/transformers/models/fsmt/configuration_fsmt.py src/transformers/models/fsmt/modeling_fsmt.py src/transformers/models/funnel/configuration_funnel.py src/transformers/models/gpt2/__init__.py src/transformers/models/gpt2/configuration_gpt2.py src/transformers/models/gpt2/modeling_flax_gpt2.py src/transformers/models/gpt2/modeling_gpt2.py src/transformers/models/gpt2/modeling_tf_gpt2.py src/transformers/models/gpt_neo/configuration_gpt_neo.py src/transformers/models/gpt_neo/modeling_gpt_neo.py src/transformers/models/gptj/__init__.py src/transformers/models/gptj/configuration_gptj.py src/transformers/models/gptj/modeling_gptj.py src/transformers/models/herbert/tokenization_herbert_fast.py src/transformers/models/hubert/__init__.py src/transformers/models/hubert/configuration_hubert.py src/transformers/models/hubert/convert_hubert_original_s3prl_checkpoint_to_pytorch.py src/transformers/models/hubert/modeling_hubert.py src/transformers/models/hubert/modeling_tf_hubert.py src/transformers/models/ibert/modeling_ibert.py src/transformers/models/layoutlm/__init__.py src/transformers/models/layoutlm/configuration_layoutlm.py src/transformers/models/layoutlm/modeling_layoutlm.py src/transformers/models/layoutlmv2/__init__.py src/transformers/models/layoutlmv2/configuration_layoutlmv2.py src/transformers/models/layoutlmv2/feature_extraction_layoutlmv2.py src/transformers/models/layoutlmv2/modeling_layoutlmv2.py src/transformers/models/layoutlmv2/processing_layoutlmv2.py src/transformers/models/layoutlmv2/tokenization_layoutlmv2.py src/transformers/models/layoutlmv2/tokenization_layoutlmv2_fast.py src/transformers/models/led/configuration_led.py src/transformers/models/led/modeling_led.py src/transformers/models/longformer/modeling_longformer.py src/transformers/models/luke/configuration_luke.py src/transformers/models/luke/modeling_luke.py src/transformers/models/luke/tokenization_luke.py src/transformers/models/lxmert/configuration_lxmert.py src/transformers/models/m2m_100/configuration_m2m_100.py src/transformers/models/m2m_100/modeling_m2m_100.py src/transformers/models/m2m_100/tokenization_m2m_100.py src/transformers/models/marian/configuration_marian.py src/transformers/models/marian/modeling_flax_marian.py src/transformers/models/marian/modeling_marian.py src/transformers/models/marian/modeling_tf_marian.py src/transformers/models/mbart/configuration_mbart.py src/transformers/models/mbart/modeling_flax_mbart.py src/transformers/models/mbart/modeling_mbart.py src/transformers/models/mbart/tokenization_mbart.py src/transformers/models/mbart/tokenization_mbart_fast.py src/transformers/models/mbart50/tokenization_mbart50.py src/transformers/models/mbart50/tokenization_mbart50_fast.py src/transformers/models/megatron_bert/configuration_megatron_bert.py src/transformers/models/megatron_bert/convert_megatron_bert_checkpoint.py src/transformers/models/megatron_bert/modeling_megatron_bert.py src/transformers/models/megatron_gpt2/convert_megatron_gpt2_checkpoint.py src/transformers/models/openai/configuration_openai.py src/transformers/models/pegasus/__init__.py src/transformers/models/pegasus/configuration_pegasus.py src/transformers/models/pegasus/modeling_flax_pegasus.py src/transformers/models/pegasus/modeling_pegasus.py src/transformers/models/pegasus/modeling_tf_pegasus.py src/transformers/models/pegasus/tokenization_pegasus_fast.py src/transformers/models/prophetnet/configuration_prophetnet.py src/transformers/models/prophetnet/modeling_prophetnet.py src/transformers/models/rag/modeling_rag.py src/transformers/models/rag/modeling_tf_rag.py src/transformers/models/reformer/configuration_reformer.py src/transformers/models/reformer/tokenization_reformer_fast.py src/transformers/models/rembert/configuration_rembert.py src/transformers/models/rembert/modeling_rembert.py src/transformers/models/rembert/tokenization_rembert_fast.py src/transformers/models/roberta/modeling_flax_roberta.py src/transformers/models/roberta/modeling_roberta.py src/transformers/models/roberta/modeling_tf_roberta.py src/transformers/models/roformer/configuration_roformer.py src/transformers/models/roformer/modeling_roformer.py src/transformers/models/speech_encoder_decoder/__init__.py src/transformers/models/speech_encoder_decoder/configuration_speech_encoder_decoder.py src/transformers/models/speech_encoder_decoder/convert_speech_to_text_wav2vec2_seq2seq_original_to_pytorch.py src/transformers/models/speech_encoder_decoder/modeling_speech_encoder_decoder.py src/transformers/models/speech_to_text/configuration_speech_to_text.py src/transformers/models/speech_to_text/feature_extraction_speech_to_text.py src/transformers/models/speech_to_text/modeling_speech_to_text.py src/transformers/models/speech_to_text_2/__init__.py src/transformers/models/speech_to_text_2/configuration_speech_to_text_2.py src/transformers/models/speech_to_text_2/modeling_speech_to_text_2.py src/transformers/models/speech_to_text_2/processing_speech_to_text_2.py src/transformers/models/speech_to_text_2/tokenization_speech_to_text_2.py src/transformers/models/splinter/configuration_splinter.py src/transformers/models/splinter/modeling_splinter.py src/transformers/models/t5/configuration_t5.py src/transformers/models/t5/modeling_flax_t5.py src/transformers/models/t5/modeling_t5.py src/transformers/models/t5/modeling_tf_t5.py src/transformers/models/t5/tokenization_t5_fast.py src/transformers/models/tapas/__init__.py src/transformers/models/tapas/configuration_tapas.py src/transformers/models/tapas/convert_tapas_original_tf_checkpoint_to_pytorch.py src/transformers/models/tapas/modeling_tapas.py src/transformers/models/tapas/tokenization_tapas.py src/transformers/models/transfo_xl/configuration_transfo_xl.py src/transformers/models/visual_bert/modeling_visual_bert.py src/transformers/models/vit/configuration_vit.py src/transformers/models/vit/convert_dino_to_pytorch.py src/transformers/models/vit/modeling_flax_vit.py src/transformers/models/vit/modeling_vit.py src/transformers/models/wav2vec2/__init__.py src/transformers/models/wav2vec2/configuration_wav2vec2.py src/transformers/models/wav2vec2/convert_wav2vec2_original_s3prl_checkpoint_to_pytorch.py src/transformers/models/wav2vec2/feature_extraction_wav2vec2.py src/transformers/models/wav2vec2/modeling_flax_wav2vec2.py src/transformers/models/wav2vec2/modeling_wav2vec2.py src/transformers/models/wav2vec2/tokenization_wav2vec2.py src/transformers/models/xlm/configuration_xlm.py src/transformers/models/xlm_roberta/tokenization_xlm_roberta.py src/transformers/models/xlm_roberta/tokenization_xlm_roberta_fast.py src/transformers/models/xlnet/configuration_xlnet.py src/transformers/models/xlnet/tokenization_xlnet_fast.py src/transformers/onnx/convert.py src/transformers/onnx/features.py src/transformers/optimization.py src/transformers/pipelines/__init__.py src/transformers/pipelines/audio_classification.py src/transformers/pipelines/automatic_speech_recognition.py src/transformers/pipelines/base.py src/transformers/pipelines/conversational.py src/transformers/pipelines/feature_extraction.py src/transformers/pipelines/fill_mask.py src/transformers/pipelines/image_classification.py src/transformers/pipelines/object_detection.py src/transformers/pipelines/question_answering.py src/transformers/pipelines/table_question_answering.py src/transformers/pipelines/text2text_generation.py src/transformers/pipelines/text_classification.py src/transformers/pipelines/text_generation.py src/transformers/pipelines/token_classification.py src/transformers/pipelines/zero_shot_classification.py src/transformers/testing_utils.py src/transformers/tokenization_utils.py src/transformers/tokenization_utils_base.py src/transformers/tokenization_utils_fast.py src/transformers/trainer.py src/transformers/trainer_callback.py src/transformers/trainer_pt_utils.py src/transformers/trainer_seq2seq.py src/transformers/trainer_utils.py src/transformers/training_args.py src/transformers/training_args_seq2seq.py src/transformers/utils/dummy_detectron2_objects.py src/transformers/utils/dummy_flax_objects.py src/transformers/utils/dummy_pt_objects.py src/transformers/utils/dummy_tf_objects.py src/transformers/utils/dummy_tokenizers_objects.py src/transformers/utils/dummy_vision_objects.py tests/deepspeed/test_deepspeed.py tests/sagemaker/conftest.py tests/sagemaker/test_multi_node_data_parallel.py tests/test_configuration_auto.py tests/test_configuration_common.py tests/test_data_collator.py tests/test_feature_extraction_auto.py tests/test_feature_extraction_layoutlmv2.py tests/test_feature_extraction_speech_to_text.py tests/test_feature_extraction_wav2vec2.py tests/test_file_utils.py tests/test_modeling_auto.py tests/test_modeling_bart.py tests/test_modeling_beit.py tests/test_modeling_bert.py tests/test_modeling_clip.py tests/test_modeling_common.py tests/test_modeling_convbert.py tests/test_modeling_deit.py tests/test_modeling_distilbert.py tests/test_modeling_encoder_decoder.py tests/test_modeling_flaubert.py tests/test_modeling_flax_albert.py tests/test_modeling_flax_bart.py tests/test_modeling_flax_beit.py tests/test_modeling_flax_distilbert.py tests/test_modeling_flax_encoder_decoder.py tests/test_modeling_flax_gpt2.py tests/test_modeling_flax_gpt_neo.py tests/test_modeling_flax_mt5.py tests/test_modeling_flax_pegasus.py tests/test_modeling_fnet.py tests/test_modeling_gpt2.py tests/test_modeling_gpt_neo.py tests/test_modeling_gptj.py tests/test_modeling_hubert.py tests/test_modeling_layoutlmv2.py tests/test_modeling_pegasus.py tests/test_modeling_rag.py tests/test_modeling_reformer.py tests/test_modeling_speech_encoder_decoder.py tests/test_modeling_speech_to_text.py tests/test_modeling_speech_to_text_2.py tests/test_modeling_tf_auto.py tests/test_modeling_tf_deberta_v2.py tests/test_modeling_tf_hubert.py tests/test_modeling_tf_pytorch.py tests/test_modeling_tf_wav2vec2.py tests/test_modeling_wav2vec2.py tests/test_onnx_v2.py tests/test_pipelines_audio_classification.py tests/test_pipelines_automatic_speech_recognition.py tests/test_pipelines_common.py tests/test_pipelines_conversational.py tests/test_pipelines_feature_extraction.py tests/test_pipelines_fill_mask.py tests/test_pipelines_image_classification.py tests/test_pipelines_object_detection.py tests/test_pipelines_question_answering.py tests/test_pipelines_summarization.py tests/test_pipelines_table_question_answering.py tests/test_pipelines_text2text_generation.py tests/test_pipelines_text_classification.py tests/test_pipelines_text_generation.py tests/test_pipelines_token_classification.py tests/test_pipelines_translation.py tests/test_pipelines_zero_shot.py tests/test_processor_layoutlmv2.py tests/test_processor_wav2vec2.py tests/test_sequence_feature_extraction_common.py tests/test_tokenization_auto.py tests/test_tokenization_byt5.py tests/test_tokenization_canine.py tests/test_tokenization_common.py tests/test_tokenization_fnet.py tests/test_tokenization_layoutlmv2.py tests/test_tokenization_luke.py tests/test_tokenization_mbart.py tests/test_tokenization_mbart50.py tests/test_tokenization_speech_to_text_2.py tests/test_tokenization_t5.py tests/test_tokenization_tapas.py tests/test_tokenization_xlm_roberta.py tests/test_trainer.py tests/test_trainer_distributed.py tests/test_trainer_tpu.py tests/test_utils_check_copies.py utils/check_copies.py utils/check_repo.py utils/notification_service.py utils/release.py utils/tests_fetcher.py
python utils/custom_init_isort.py
python utils/style_doc.py src/transformers docs/source --max_len 119
running deps_table_update
updating src/transformers/dependency_versions_table.py
python utils/check_copies.py
python utils/check_table.py
python utils/check_dummies.py
python utils/check_repo.py
Checking all models are public.
Checking all models are properly tested.
Checking all objects are properly documented.
Checking all models are in at least one auto class.
python utils/check_inits.py
python utils/tests_fetcher.py --sanity_check and fix suggested changes.

* Run black examples tests src utils
isort examples tests src utils
Skipped 1 files
make autogenerate_code
make[1]: Entering directory '/mnt/c/Users/Admin/Desktop/Home/Projects/transformers'
running deps_table_update
updating src/transformers/dependency_versions_table.py
make[1]: Leaving directory '/mnt/c/Users/Admin/Desktop/Home/Projects/transformers'
make extra_style_checks
make[1]: Entering directory '/mnt/c/Users/Admin/Desktop/Home/Projects/transformers'
python utils/custom_init_isort.py
python utils/style_doc.py src/transformers docs/source --max_len 119
make[1]: Leaving directory '/mnt/c/Users/Admin/Desktop/Home/Projects/transformers' for reformatting code.

* Add installation dependencies for examples/research_projects/fsner.

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>
Co-authored-by: Stefan Schweter <stefan@schweter.it>
==

examples/research_projects/fsner/README.md
examples/research_projects/fsner/pyproject.toml
examples/research_projects/fsner/requirements.txt
examples/research_projects/fsner/setup.py
examples/research_projects/fsner/src/fsner/__init__.py
examples/research_projects/fsner/src/fsner/model.py
examples/research_projects/fsner/src/fsner/tokenizer_utils.py
==================
1988849bb;Li-Huai (Allan) Lin;2021-09-24 04:56:34 +0800;Handle `UnicodeDecodeError` (#13717)

==

src/transformers/configuration_utils.py
==================
8632a60d3;kding1;2021-09-23 09:15:27 -0700;Add cpu distributed fine-tuning support for transformers Trainer API (#13574)
* update trainer with cpu distributed fine-tuning support.

Signed-off-by: Ding, Ke <ke.ding@intel.com>

* Style.

* refinement on cpu dist training check.

Signed-off-by: Ding, Ke <ke.ding@intel.com>

* style.

Signed-off-by: Ding, Ke <ke.ding@intel.com>

* Test over private field not public one.

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

Co-authored-by: Morgan Funtowicz <funtowiczmo@gmail.com>
Co-authored-by: Funtowicz Morgan <mfuntowicz@users.noreply.github.com>
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

src/transformers/trainer.py
src/transformers/trainer_pt_utils.py
src/transformers/training_args.py
==================
6a3a197fc;kding1;2021-09-23 08:01:51 -0700;Add SigOpt HPO to transformers trainer api (#13572)
* add sigopt hpo to transformers.

Signed-off-by: Ding, Ke <ke.ding@intel.com>

* extend sigopt changes to test code and others..

Signed-off-by: Ding, Ke <ke.ding@intel.com>

* Style.

* fix style for sigopt integration.

Signed-off-by: Ding, Ke <ke.ding@intel.com>

* Add necessary information to run unittests on SigOpt.

Co-authored-by: Morgan Funtowicz <funtowiczmo@gmail.com>
==

.github/workflows/self-nightly-scheduled.yml
.github/workflows/self-scheduled.yml
setup.py
src/transformers/__init__.py
src/transformers/dependency_versions_table.py
src/transformers/integrations.py
src/transformers/testing_utils.py
src/transformers/trainer.py
src/transformers/trainer_utils.py
tests/test_trainer.py
==================
62832c962;Stas Bekman;2021-09-22 19:33:09 -0700;1x model size CPU memory usage for `from_pretrained`  (#13466)
* one possible solution

* low mem from_pretrained

* edge cases

* solve the persistent buffers

* style

* parametrize

* for later

* proper solution

* cleanup

* refactor; rework based on suggestions

* revert splitting into 2 parts, move checks into main func
==

src/transformers/modeling_utils.py
==================
ca257a06c;Lysandre Debut;2021-09-22 19:02:54 -0400;Fix torchscript tests (#13701)

==

tests/test_modeling_convbert.py
tests/test_modeling_distilbert.py
tests/test_modeling_flaubert.py
==================
5b5707544;Lysandre Debut;2021-09-22 19:00:47 -0400;Add BlenderBot small tokenizer to the init (#13367)
* Add BlenderBot small tokenizer to the init

* Update src/transformers/__init__.py

Co-authored-by: Suraj Patil <surajp815@gmail.com>

* Style

* Bugfix

Co-authored-by: Suraj Patil <surajp815@gmail.com>
==

docs/source/index.rst
docs/source/model_doc/blenderbot_small.rst
src/transformers/__init__.py
src/transformers/models/blenderbot_small/__init__.py
src/transformers/models/blenderbot_small/tokenization_blenderbot_small_fast.py
src/transformers/utils/dummy_tokenizers_objects.py
==================
9e0fd7805;Gunjan Chhablani;2021-09-23 04:06:24 +0530;Fix reference to tpu short seq length (#13686)

==

src/transformers/models/fnet/modeling_fnet.py
==================
6dc41d9f8;Suraj Patil;2021-09-23 02:48:13 +0530;add a note about tokenizer (#13696)

==

docs/source/model_doc/gptj.rst
==================
7c7d2ec95;Anton Lozhkov;2021-09-22 23:17:57 +0300;[GPT-J] Use the `float16` checkpoints in integration tests (#13676)
* Use fp16 checkpoints

* Style

* Fix outputs and disable OOM tests

* Correct another output

* Use a random smaller model for generation tests

* repo quickfix

* fix gradient checkpointing
==

tests/test_modeling_gptj.py
==================
0ecdf6de0;Lysandre Debut;2021-09-22 15:33:18 -0400;Patch training arguments issue (#13700)
* Patch training arguments issue

* Update src/transformers/training_args.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

src/transformers/training_args.py
==================
50c746eeb;Gunjan Chhablani;2021-09-22 21:21:53 +0530;Allow only textual inputs to VisualBert (#13687)

==

src/transformers/models/visual_bert/modeling_visual_bert.py
==================
93624bfee;Yih-Dar;2021-09-22 15:14:55 +0200;Fix non-negligible difference between GPT2 and TFGP2 (#13679)
Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>
==

src/transformers/models/gpt2/modeling_tf_gpt2.py
==================
a0c08aa36;MocktaiLEngineer;2021-09-22 18:44:29 +0530;Assertions to exceptions (#13692)
* Raise exceptions instead of using assertions for control flow #12789

* # coding=utf-8

* Raise exceptions instead of using assertions for control flow

* Raise exceptions instead of using assertions for control flow

* Update src/transformers/tokenization_utils.py

Raise exceptions instead of using assertions for control flow

Co-authored-by: Suraj Patil <surajp815@gmail.com>

* Update src/transformers/tokenization_utils.py

Raise exceptions instead of using assertions for control flow

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Raise exceptions instead of using assertions for control flow

* test

* Raise exceptions instead of using assertions for control flow

Co-authored-by: MocktaiLEngineer <kavinarasu22@gmail.com>
Co-authored-by: Suraj Patil <surajp815@gmail.com>
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

src/transformers/tokenization_utils.py
==================
27d463977;Sylvain Gugger;2021-09-22 07:51:38 -0400;Make gradient_checkpointing a training argument (#13657)
* Make gradient_checkpointing a training argument

* Update src/transformers/modeling_utils.py

Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>

* Update src/transformers/configuration_utils.py

Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>

* Fix tests

* Style

* document Gradient Checkpointing as a performance feature

* Small rename

* PoC for not using the config

* Adapt BC to new PoC

* Forgot to save

* Rollout changes to all other models

* Fix typo

Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>
Co-authored-by: Stas Bekman <stas@stason.org>
==

docs/source/model_doc/led.rst
docs/source/performance.md
examples/pytorch/language-modeling/README.md
src/transformers/configuration_utils.py
src/transformers/modeling_utils.py
src/transformers/models/bart/configuration_bart.py
src/transformers/models/bart/modeling_bart.py
src/transformers/models/beit/configuration_beit.py
src/transformers/models/beit/modeling_beit.py
src/transformers/models/bert/configuration_bert.py
src/transformers/models/bert/modeling_bert.py
src/transformers/models/bert_generation/configuration_bert_generation.py
src/transformers/models/big_bird/configuration_big_bird.py
src/transformers/models/big_bird/modeling_big_bird.py
src/transformers/models/bigbird_pegasus/configuration_bigbird_pegasus.py
src/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py
src/transformers/models/blenderbot/configuration_blenderbot.py
src/transformers/models/blenderbot/modeling_blenderbot.py
src/transformers/models/blenderbot_small/configuration_blenderbot_small.py
src/transformers/models/blenderbot_small/modeling_blenderbot_small.py
src/transformers/models/canine/configuration_canine.py
src/transformers/models/canine/modeling_canine.py
src/transformers/models/clip/configuration_clip.py
src/transformers/models/clip/modeling_clip.py
src/transformers/models/convbert/modeling_convbert.py
src/transformers/models/deit/configuration_deit.py
src/transformers/models/deit/modeling_deit.py
src/transformers/models/detr/modeling_detr.py
src/transformers/models/dpr/configuration_dpr.py
src/transformers/models/dpr/modeling_dpr.py
src/transformers/models/electra/modeling_electra.py
src/transformers/models/fnet/configuration_fnet.py
src/transformers/models/fnet/modeling_fnet.py
src/transformers/models/gpt2/configuration_gpt2.py
src/transformers/models/gpt2/modeling_gpt2.py
src/transformers/models/gpt_neo/configuration_gpt_neo.py
src/transformers/models/gpt_neo/modeling_gpt_neo.py
src/transformers/models/gptj/configuration_gptj.py
src/transformers/models/gptj/modeling_gptj.py
src/transformers/models/hubert/configuration_hubert.py
src/transformers/models/hubert/modeling_hubert.py
src/transformers/models/ibert/modeling_ibert.py
src/transformers/models/layoutlm/configuration_layoutlm.py
src/transformers/models/layoutlm/modeling_layoutlm.py
src/transformers/models/layoutlmv2/modeling_layoutlmv2.py
src/transformers/models/led/configuration_led.py
src/transformers/models/led/modeling_led.py
src/transformers/models/longformer/modeling_longformer.py
src/transformers/models/luke/configuration_luke.py
src/transformers/models/luke/modeling_luke.py
src/transformers/models/m2m_100/configuration_m2m_100.py
src/transformers/models/m2m_100/modeling_m2m_100.py
src/transformers/models/marian/configuration_marian.py
src/transformers/models/marian/modeling_marian.py
src/transformers/models/mbart/configuration_mbart.py
src/transformers/models/mbart/modeling_mbart.py
src/transformers/models/megatron_bert/configuration_megatron_bert.py
src/transformers/models/megatron_bert/convert_megatron_bert_checkpoint.py
src/transformers/models/megatron_bert/modeling_megatron_bert.py
src/transformers/models/megatron_gpt2/convert_megatron_gpt2_checkpoint.py
src/transformers/models/pegasus/configuration_pegasus.py
src/transformers/models/pegasus/modeling_pegasus.py
src/transformers/models/prophetnet/configuration_prophetnet.py
src/transformers/models/prophetnet/modeling_prophetnet.py
src/transformers/models/rembert/configuration_rembert.py
src/transformers/models/rembert/modeling_rembert.py
src/transformers/models/roberta/modeling_roberta.py
src/transformers/models/roformer/configuration_roformer.py
src/transformers/models/roformer/modeling_roformer.py
src/transformers/models/speech_to_text/configuration_speech_to_text.py
src/transformers/models/speech_to_text/modeling_speech_to_text.py
src/transformers/models/speech_to_text_2/configuration_speech_to_text_2.py
src/transformers/models/speech_to_text_2/modeling_speech_to_text_2.py
src/transformers/models/splinter/configuration_splinter.py
src/transformers/models/splinter/modeling_splinter.py
src/transformers/models/t5/configuration_t5.py
src/transformers/models/t5/modeling_t5.py
src/transformers/models/tapas/configuration_tapas.py
src/transformers/models/tapas/modeling_tapas.py
src/transformers/models/visual_bert/modeling_visual_bert.py
src/transformers/models/vit/configuration_vit.py
src/transformers/models/vit/modeling_vit.py
src/transformers/models/wav2vec2/configuration_wav2vec2.py
src/transformers/models/wav2vec2/modeling_wav2vec2.py
src/transformers/trainer.py
src/transformers/training_args.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/configuration_{{cookiecutter.lowercase_modelname}}.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_{{cookiecutter.lowercase_modelname}}.py
tests/test_modeling_beit.py
tests/test_modeling_common.py
tests/test_modeling_deit.py
tests/test_modeling_flax_gpt2.py
tests/test_modeling_flax_gpt_neo.py
tests/test_modeling_gpt2.py
tests/test_modeling_gpt_neo.py
tests/test_modeling_gptj.py
==================
75f6641ea;Anton Lozhkov;2021-09-22 12:02:54 +0300;[Wav2Vec2FeatureExtractor] Fix `extractor.pad()` dtype backwards compatibility (#13693)
* Force dtype, add tests

* Local torch imports

* Remove unused logic (always ndarray)
==

src/transformers/feature_extraction_sequence_utils.py
tests/test_feature_extraction_speech_to_text.py
tests/test_feature_extraction_wav2vec2.py
==================
8e908c8c7;Patrick von Platen;2021-09-22 00:29:38 +0200;[AutoTokenizer] Allow creation of tokenizers by tokenizer type (#13668)
* up

* up
==

src/transformers/models/auto/tokenization_auto.py
tests/fixtures/merges.txt
tests/fixtures/vocab.json
tests/fixtures/vocab.txt
tests/test_tokenization_auto.py
==================
2608944dc;Patrick von Platen;2021-09-22 00:28:43 +0200;up (#13688)

==

tests/test_modeling_flax_wav2vec2.py
==================
8565d38f3;Kamal Raj;2021-09-22 03:06:13 +0530;Update modeling_flax_wav2vec2.py (#13680)
conv kernel_size to Tuple,
Flax Version 0.3.5 breaking change, https://github.com/google/flax/releases/tag/v0.3.5
==

src/transformers/models/wav2vec2/modeling_flax_wav2vec2.py
==================
d16bec953;Sylvain Gugger;2021-09-21 16:17:01 -0400;Skip FlaxWav2Vec2 test until fixed

==

tests/test_modeling_flax_wav2vec2.py
==================
ddd4d02f3;Nishant Prabhu;2021-09-22 01:09:37 +0530;Layoutlm onnx support (Issue #13300) (#13562)
* Add support for exporting PyTorch LayoutLM to ONNX

* Added tests for converting LayoutLM to ONNX

* Add support for exporting PyTorch LayoutLM to ONNX

* Added tests for converting LayoutLM to ONNX

* cleanup

* Removed regression/ folder

* Add support for exporting PyTorch LayoutLM to ONNX

* Added tests for converting LayoutLM to ONNX

* cleanup

* Fixed import error

* Remove unnecessary import statements

* Changed max_2d_positions from class variable to instance variable of the config class

* Add support for exporting PyTorch LayoutLM to ONNX

* Added tests for converting LayoutLM to ONNX

* cleanup

* Add support for exporting PyTorch LayoutLM to ONNX

* cleanup

* Fixed import error

* Changed max_2d_positions from class variable to instance variable of the config class

* Use super class generate_dummy_inputs method

Co-authored-by: Michael Benayoun <mickbenayoun@gmail.com>

* Add support for Masked LM, sequence classification and token classification

Co-authored-by: Michael Benayoun <mickbenayoun@gmail.com>

* Removed uncessary import and method

* Fixed code styling

* Raise error if PyTorch is not installed

* Remove unnecessary import statement

Co-authored-by: Michael Benayoun <mickbenayoun@gmail.com>
==

src/transformers/models/layoutlm/__init__.py
src/transformers/models/layoutlm/configuration_layoutlm.py
src/transformers/onnx/features.py
tests/test_onnx_v2.py
==================
b7d264be0;Sylvain Gugger;2021-09-21 13:13:30 -0400;Add push_to_hub to no_trainer examples (#13659)
* Add push_to_hub to no_trainer examples

* Quality

* Document integration

* Roll out to other examples
==

examples/pytorch/README.md
examples/pytorch/language-modeling/run_clm_no_trainer.py
examples/pytorch/language-modeling/run_mlm_no_trainer.py
examples/pytorch/multiple-choice/run_swag_no_trainer.py
examples/pytorch/question-answering/run_qa_beam_search_no_trainer.py
examples/pytorch/question-answering/run_qa_no_trainer.py
examples/pytorch/summarization/run_summarization_no_trainer.py
examples/pytorch/text-classification/run_glue_no_trainer.py
examples/pytorch/token-classification/run_ner_no_trainer.py
examples/pytorch/translation/run_translation_no_trainer.py
==================
a722c301b;Stas Bekman;2021-09-21 09:05:05 -0700;[SinusoidalPositionalEmbedding] incorrect dtype when make_weights in forward (#13665)

==

src/transformers/models/fsmt/modeling_fsmt.py
src/transformers/models/m2m_100/modeling_m2m_100.py
src/transformers/models/speech_to_text/modeling_speech_to_text.py
src/transformers/models/speech_to_text_2/modeling_speech_to_text_2.py
==================
1417978cd;Anton Lozhkov;2021-09-21 17:10:13 +0300;[SequenceFeatureExtractor] Rewrite padding logic from pure python to numpy (#13650)
* Test np padding

* Pass feature extraction tests

* Update type hints

* Fix flaky integration tests

* Try a more stable waveform

* Add to_numpy jax support

* int32 attention masks

* Refactor normalization tests
==

src/transformers/feature_extraction_sequence_utils.py
src/transformers/file_utils.py
src/transformers/models/speech_to_text/feature_extraction_speech_to_text.py
tests/test_feature_extraction_speech_to_text.py
tests/test_feature_extraction_wav2vec2.py
tests/test_modeling_speech_to_text.py
tests/test_pipelines_automatic_speech_recognition.py
tests/test_sequence_feature_extraction_common.py
==================
8d533e6ad;Kamal Raj;2021-09-21 18:41:26 +0530;Typo "UNKWOWN" -> "UNKNOWN" (#13675)

==

src/transformers/testing_utils.py
tests/test_configuration_auto.py
tests/test_file_utils.py
tests/test_modeling_auto.py
tests/test_modeling_tf_auto.py
tests/test_modeling_tf_pytorch.py
tests/test_tokenization_auto.py
==================
78807d86e;Kamal Raj;2021-09-21 18:34:48 +0530;[FLAX] Question Answering Example  (#13649)
* flax qa example

* Updated README:  Added Large model

* added utils_qa.py FULL_COPIES

* Updates:
1. Copyright Year updated
2. added dtype arg
3. passing seed and dtype to load model
4. Check eval flag before running eval

* updated README

* updated code comment
==

examples/flax/question-answering/README.md
examples/flax/question-answering/requirements.txt
examples/flax/question-answering/run_qa.py
examples/flax/question-answering/utils_qa.py
utils/check_copies.py
==================
a2dec768a;Kamal Raj;2021-09-21 17:04:19 +0530;beit-flax (#13515)
* beit-flax

* updated FLAX_BEIT_MLM_DOCSTRING

* removed bool_masked_pos from classification

* updated Copyright

* code refactoring: x -> embeddings

* updated test: rm from_pt

* Update docs/source/model_doc/beit.rst

* model code dtype updates and
other changes according to review

* relative_position_bias
revert back to pytorch design
==

docs/source/index.rst
docs/source/model_doc/beit.rst
src/transformers/__init__.py
src/transformers/models/auto/modeling_flax_auto.py
src/transformers/models/beit/__init__.py
src/transformers/models/beit/modeling_flax_beit.py
src/transformers/utils/dummy_flax_objects.py
tests/test_modeling_flax_beit.py
utils/check_repo.py
==================
48fa42e5d;Patrick von Platen;2021-09-21 08:50:33 +0200;Add Speech AutoModels (#13655)
* upload

* correct

* correct

* correct

* finish

* up

* up

* up again
==

docs/source/model_doc/auto.rst
src/transformers/__init__.py
src/transformers/models/auto/__init__.py
src/transformers/models/auto/modeling_auto.py
src/transformers/pipelines/__init__.py
src/transformers/pipelines/automatic_speech_recognition.py
src/transformers/utils/dummy_pt_objects.py
tests/test_pipelines_automatic_speech_recognition.py
==================
ea9213659;flozi00;2021-09-20 21:10:33 +0200;Fix typo distilbert doc (#13643)

==

docs/source/model_doc/distilbert.rst
==================
28d5700aa;Lowin;2021-09-21 03:01:35 +0800;fix research_projects/mlm_wwm readme.md examples (#13646)
the variables of run example is not correct
==

examples/research_projects/mlm_wwm/README.md
==================
002a078af;Sylvain Gugger;2021-09-20 13:59:21 -0400;Dynamically load model code from the Hub (#13467)
* Dynamic model

* Use defensive flag

* Style

* Doc and arg rename

* Arg rename

* Add tests

* Apply suggestions from code review

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* Apply suggestions from code review

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* Address review comments

* Apply suggestions from code review

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
==

src/transformers/file_utils.py
src/transformers/models/auto/auto_factory.py
src/transformers/models/auto/dynamic.py
tests/test_modeling_auto.py
tests/test_modeling_common.py
==================
aeb2dac04;flozi00;2021-09-20 18:31:46 +0200;Change https:/ to https:// (#13644)

==

docs/source/training.rst
==================
0af901e83;Stas Bekman;2021-09-20 08:50:54 -0700;[megatron_gpt2] checkpoint v3 (#13508)
* [megatron_gpt2] checkpoint v3

* bug fix

* fixes

* switch to default  from  - which is what the current megatron-lm uses

* cleanup

* back compat
==

src/transformers/models/megatron_gpt2/convert_megatron_gpt2_checkpoint.py
==================
936b3fdea;Kamal Raj;2021-09-20 20:41:04 +0530;Update modeling_tf_deberta.py (#13654)
Fixed expand_dims axis
==

src/transformers/models/deberta/modeling_tf_deberta.py
==================
04976a32d;Ayaka Mikazuki;2021-09-20 19:53:31 +0800;Fix mT5 documentation (#13639)
* Fix MT5 documentation

The abstract is incomplete

* MT5 -> mT5
==

docs/source/model_doc/mt5.rst
==================
fe379f856;Chengjiang Li;2021-09-20 19:50:03 +0800;[Fix]Make sure the args tb_writer passed to the TensorBoardCallback works (#13636)

==

src/transformers/integrations.py
==================
d8049331d;Gunjan Chhablani;2021-09-20 16:54:30 +0530;Add FNet (#13045)
* Init FNet

* Update config

* Fix config

* Update model classes

* Update tokenizers to use sentencepiece

* Fix errors in model

* Fix defaults in config

* Remove position embedding type completely

* Fix typo and take only real numbers

* Fix type vocab size in configuration

* Add projection layer to embeddings

* Fix position ids bug in embeddings

* Add minor changes

* Add conversion script and remove CausalLM vestiges

* Fix conversion script

* Fix conversion script

* Remove CausalLM Test

* Update checkpoint names to dummy checkpoints

* Add tokenizer mapping

* Fix modeling file and corresponding tests

* Add tokenization test file

* Add PreTraining model test

* Make style and quality

* Make tokenization base tests work

* Update docs

* Add FastTokenizer tests

* Fix fast tokenizer special tokens

* Fix style and quality

* Remove load_tf_weights vestiges

* Add FNet to  main README

* Fix configuration example indentation

* Comment tokenization slow test

* Fix style

* Add changes from review

* Fix style

* Remove bos and eos tokens from tokenizers

* Add tokenizer slow test, TPU transforms, NSP

* Add scipy check

* Add scipy availabilty check to test

* Fix tokenizer and use correct inputs

* Remove remaining TODOs

* Fix tests

* Fix tests

* Comment Fourier Test

* Uncomment Fourier Test

* Change to google checkpoint

* Add changes from review

* Fix activation function

* Fix model integration test

* Add more integration tests

* Add comparison steps to MLM integration test

* Fix style

* Add masked tokenization fix

* Improve mask tokenization fix

* Fix index docs

* Add changes from review

* Fix issue

* Fix failing import in test

* some more fixes

* correct fast tokenizer

* finalize

* make style

* Remove additional tokenization logic

* Set do_lower_case to False

* Allow keeping accents

* Fix tokenization test

* Fix FNet Tokenizer Fast

* fix tests

* make style

* Add tips to FNet docs

Co-authored-by: patrickvonplaten <patrick.v.platen@gmail.com>
==

README.md
README_zh-hans.md
README_zh-hant.md
docs/source/index.rst
docs/source/model_doc/fnet.rst
src/transformers/__init__.py
src/transformers/convert_slow_tokenizer.py
src/transformers/models/__init__.py
src/transformers/models/auto/configuration_auto.py
src/transformers/models/auto/modeling_auto.py
src/transformers/models/auto/tokenization_auto.py
src/transformers/models/fnet/__init__.py
src/transformers/models/fnet/configuration_fnet.py
src/transformers/models/fnet/convert_fnet_original_flax_checkpoint_to_pytorch.py
src/transformers/models/fnet/modeling_fnet.py
src/transformers/models/fnet/tokenization_fnet.py
src/transformers/models/fnet/tokenization_fnet_fast.py
src/transformers/pipelines/question_answering.py
src/transformers/utils/dummy_pt_objects.py
src/transformers/utils/dummy_tokenizers_objects.py
tests/test_modeling_fnet.py
tests/test_tokenization_fnet.py
==================
87d5057d8;Suraj Patil;2021-09-20 13:22:26 +0530;fix typo (#13647)

==

examples/pytorch/summarization/run_summarization.py
==================
b518aaf19;calpt;2021-09-17 21:36:23 +0200;Fix GPT2Config parameters in GPT2ModelTester (#13630)

==

tests/test_modeling_gpt2.py
==================
300ee0c7b;Lysandre Debut;2021-09-17 15:35:34 -0400;Updated tiny distilbert models (#13631)

==

tests/test_pipelines_feature_extraction.py
==================
afb07a79a;Yih-Dar;2021-09-17 17:39:35 +0200;fix some docstring in encoder-decoder models (#13611)
Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>
==

src/transformers/models/encoder_decoder/modeling_encoder_decoder.py
src/transformers/models/speech_encoder_decoder/modeling_speech_encoder_decoder.py
==================
19b7acdd6;Alessandro Suglia;2021-09-17 16:05:49 +0100;Cloned tensors after indexing in _compute_attn_output_with_global_indices (#13613)
Co-authored-by: Alessandro Suglia <asuglia@fb.com>
==

src/transformers/models/led/modeling_led.py
src/transformers/models/longformer/modeling_longformer.py
==================
ce32c69c0;Alex Hedges;2021-09-17 10:57:27 -0400;Use `config_dict_or_path` for deepspeed.zero.Init (#13614)

==

src/transformers/modeling_utils.py
==================
0eb02871d;Matt;2021-09-17 15:44:33 +0100;Removed console spam from misfiring warnings (#13625)
* Removed misfiring warnings

* Revert "Removed misfiring warnings"

This reverts commit cea90de325056b9c1cbcda2bd2613a785c1639ce.

* Retain the warning, but only when the user actually overrides things

* Fix accidentally breaking just about every model on the hub simultaneously

* Style pass
==

src/transformers/modeling_tf_utils.py
==================
da8beaaf7;Li-Huai (Allan) Lin;2021-09-17 22:28:28 +0800;Fix special tokens not correctly tokenized (#13489)
* Fix special tokens not correctly tokenized

* Add testing

* Fix

* Fix

* Use user workflows instead of directly assigning variables

* Enable test of fast tokenizers

* Update test of canine tokenizer
==

src/transformers/tokenization_utils.py
tests/test_tokenization_canine.py
tests/test_tokenization_common.py
==================
1f9dcfc1e;Patrick von Platen;2021-09-17 16:21:59 +0200;[Trainer] Add nan/inf logging filter (#13619)
* finish

* add test

* push

* remove unnecessary code

* up

* correct test

* Update src/transformers/training_args.py
==

src/transformers/trainer.py
src/transformers/training_args.py
tests/test_trainer.py
==================
eae7a96b7;Ibraheem Moosa;2021-09-17 20:07:52 +0600;Optimize Token Classification models for TPU (#13096)
* Optimize Token Classification models for TPU

As per the XLA document XLA cannot handle masked indexing well. So token classification
models for BERT and others use an implementation based on `torch.where`. This implementation
works well on TPU. 

ALBERT token classification model uses the masked indexing which causes performance issues
on TPU. This PR fixes this issue by following the BERT implementation.

* Same fix for ELECTRA

* Same fix for LayoutLM
==

src/transformers/models/albert/modeling_albert.py
src/transformers/models/electra/modeling_electra.py
src/transformers/models/layoutlm/modeling_layoutlm.py
==================
e02ed0ee7;Benjamin Davidson;2021-09-16 21:30:05 +0100;XLMR tokenizer is fully picklable (#13577)
* made tokenizer fully picklable

* remove whitespace

* added testcase
==

src/transformers/models/xlm_roberta/tokenization_xlm_roberta.py
tests/test_tokenization_xlm_roberta.py
==================
af5c6ae5e;Sylvain Gugger;2021-09-16 15:13:00 -0400;Properly use test_fetcher for examples (#13604)
* Properly use test_fetcher for examples

* Fake example modification

* Fake modeling file modification

* Clean fake modifications

* Run example tests for any modification.
==

.circleci/config.yml
utils/tests_fetcher.py
==================
bec2e3f55;Stas Bekman;2021-09-16 12:12:16 -0700;[deepspeed] replaced deprecated init arg (#13587)
* [deepspeed] replaced deprecated init arg

* Trigger CI
==

setup.py
src/transformers/dependency_versions_table.py
src/transformers/modeling_utils.py
==================
4d5b4c786;Patrick von Platen;2021-09-16 20:02:54 +0200;Feature Extractor: Wav2Vec2 & Speech2Text - Allow truncation + padding=longest (#13600)
* correct

* add tests

* Update src/transformers/feature_extraction_sequence_utils.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

src/transformers/feature_extraction_sequence_utils.py
src/transformers/models/speech_to_text/feature_extraction_speech_to_text.py
src/transformers/models/wav2vec2/feature_extraction_wav2vec2.py
tests/test_feature_extraction_speech_to_text.py
tests/test_feature_extraction_wav2vec2.py
==================
e59041684;Matt;2021-09-16 18:00:59 +0100;DataCollatorForTokenClassification numpy fix (#13609)
* Fix issue when labels are supplied as Numpy array instead of list

* Fix issue when labels are supplied as Numpy array instead of list

* Fix same issue in the `TokenClassification` data collator

* Style pass
==

src/transformers/data/data_collator.py
==================
88dbbfb2d;Sylvain Gugger;2021-09-16 11:55:37 -0400;Fix make fix-copies with type annotations (#13586)

==

src/transformers/models/hubert/modeling_hubert.py
src/transformers/models/hubert/modeling_tf_hubert.py
src/transformers/models/roberta/modeling_tf_roberta.py
utils/check_copies.py
==================
cec1c6364;Lysandre Debut;2021-09-16 11:33:08 -0400;Fix test (#13608)

==

tests/test_pipelines_feature_extraction.py
==================
5c5937182;Matt;2021-09-16 15:35:57 +0100;Fix DataCollatorForSeq2Seq when labels are supplied as Numpy array instead of list (#13582)
* Fix issue when labels are supplied as Numpy array instead of list

* Fix issue when labels are supplied as Numpy array instead of list
==

src/transformers/data/data_collator.py
==================
421929b55;Patrick von Platen;2021-09-16 10:07:47 +0200;finish (#13593)

==

src/transformers/models/distilbert/modeling_distilbert.py
src/transformers/models/pegasus/modeling_pegasus.py
==================
b5bab710f;Patrick von Platen;2021-09-16 09:07:20 +0200;correct (#13585)

==

tests/test_feature_extraction_speech_to_text.py
==================
89da1bfea;Stas Bekman;2021-09-15 17:18:34 -0700;[ci] nightly: add deepspeed master (#13589)

==

.github/workflows/self-nightly-scheduled.yml
==================
95f933ea8;Patrick von Platen;2021-09-15 19:03:56 +0200;[Pretrained Model] Add resize_position_embeddings (#13559)
* finish

* delete bogus file

* correct some stuff

* finish

* finish
==

examples/pytorch/summarization/run_summarization.py
src/transformers/modeling_utils.py
src/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py
src/transformers/models/distilbert/modeling_distilbert.py
src/transformers/models/pegasus/modeling_pegasus.py
tests/test_modeling_common.py
tests/test_modeling_distilbert.py
tests/test_modeling_pegasus.py
==================
c783e1488;elishowk;2021-09-15 15:25:03 +0200;upgrade sentencepiece version (#13564)

==

examples/research_projects/lxmert/demo.ipynb
examples/research_projects/movement-pruning/Saving_PruneBERT.ipynb
examples/research_projects/visual_bert/demo.ipynb
setup.py
src/transformers/dependency_versions_table.py
==================
e86c02ea9;Suraj Patil;2021-09-15 16:38:41 +0530;Fix GPTNeo onnx export (#13524)
Update GPT Neo ONNX config to match the changes implied by the simplification of the local attention

Co-authored-by: Michael Benayoun <michael@huggingface.co>
==

src/transformers/models/gpt_neo/configuration_gpt_neo.py
==================
3fbb55c75;Bhadresh Savani;2021-09-15 11:03:52 +0530;[Flax] Fixes typo in Bart based Flax Models (#13565)

==

src/transformers/models/bart/modeling_flax_bart.py
src/transformers/models/marian/modeling_flax_marian.py
src/transformers/models/mbart/modeling_flax_mbart.py
src/transformers/models/pegasus/modeling_flax_pegasus.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_flax_{{cookiecutter.lowercase_modelname}}.py
==================
7bd16b877;Sylvain Gugger;2021-09-14 13:33:41 -0400;Fix test_fetcher when setup is updated (#13566)
* Fix test_fetcher when setup is updated

* Remove example
==

utils/tests_fetcher.py
==================
054b6013c;elishowk;2021-09-14 18:07:36 +0200;separate model card git push from the rest (#13514)
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

docs/source/model_sharing.rst
src/transformers/trainer.py
==================
9f318be3d;Sylvain Gugger;2021-09-14 11:31:17 -0400;Fix yml syntax error

==

.github/workflows/self-nightly-scheduled.yml
==================
801ec115c;Sylvain Gugger;2021-09-14 11:27:32 -0400;Add checks to build cleaner model cards (#13542)
* Add checks to build cleaner model cards

* Address review comments
==

src/transformers/modelcard.py
==================
c1e47bf4f;Bhadresh Savani;2021-09-14 20:45:19 +0530;[Flax] Addition of FlaxPegasus (#13420)
* added initial files

* fixes pipeline

* fixes style and quality

* fixes doc issue and positional encoding

* fixes layer norm and test

* fixes quality issue

* fixes code quality

* removed extra layer norm

* added layer norm back in encoder and decoder

* added more code copy quality checks

* update tests

* Apply suggestions from code review

* fix import

* fix test

Co-authored-by: patil-suraj <surajp815@gmail.com>
==

docs/source/index.rst
docs/source/model_doc/pegasus.rst
src/transformers/__init__.py
src/transformers/models/auto/configuration_auto.py
src/transformers/models/auto/modeling_flax_auto.py
src/transformers/models/pegasus/__init__.py
src/transformers/models/pegasus/modeling_flax_pegasus.py
src/transformers/utils/dummy_flax_objects.py
tests/test_modeling_flax_pegasus.py
==================
fc3551a6d;Suraj Patil;2021-09-14 19:06:41 +0530;add flax mbart in auto seq2seq lm (#13560)

==

src/transformers/models/auto/modeling_flax_auto.py
==================
3081d3868;Sylvain Gugger;2021-09-14 08:02:15 -0400;Push to hub when saving checkpoints (#13503)
* Push to hub when saving checkpoints

* Add model card

* Revert partial model card

* Small fix for checkpoint

* Add tests

* Add documentation

* Fix tests

* Bump huggingface_hub

* Fix test
==

docs/source/main_classes/trainer.rst
setup.py
src/transformers/dependency_versions_table.py
src/transformers/trainer.py
src/transformers/trainer_utils.py
src/transformers/training_args.py
tests/test_trainer.py
==================
51e5eca61;Avital Oliver;2021-09-14 10:11:55 +0200;Add long overdue link to the Google TRC project (#13501)
* Add long-overdue link to the Google TRC project

* Apply suggestions from code review

Co-authored-by: Suraj Patil <surajp815@gmail.com>
Co-authored-by: Stefan Schweter <stefan@schweter.it>
==

examples/flax/README.md
==================
3ab0185b0;Lysandre Debut;2021-09-13 16:17:29 -0400;Nightly torch ci (#13550)
* Nightly CI torch

* Version

* Reformat

* Only subset
Fix

* Revert

* Better formatting

* New channel
==

.github/workflows/self-nightly-scheduled.yml
utils/notification_service.py
==================
5c14fceac;Patrick von Platen;2021-09-13 14:02:23 +0200;return attention mask in int32 (#13543)

==

src/transformers/models/speech_to_text/feature_extraction_speech_to_text.py
src/transformers/models/wav2vec2/feature_extraction_wav2vec2.py
==================
149c833b7;SaulLu;2021-09-13 13:32:32 +0200;Small changes in `perplexity.rst`to make the notebook executable on google collaboratory (#13541)
* add imports

* Update docs/source/perplexity.rst

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

docs/source/perplexity.rst
==================
f1c22dae7;Stas Bekman;2021-09-13 04:31:35 -0700;[tokenizer] use use_auth_token for config (#13523)
* [tokenizer] use use_auth_token for config

* args order
==

src/transformers/tokenization_utils_base.py
==================
d2904264a;Patrick von Platen;2021-09-13 13:07:59 +0200;up (#13538)

==

tests/test_feature_extraction_speech_to_text.py
==================
65ee1a43e;Nicolas Patry;2021-09-13 12:48:54 +0200;fixing BC in `fill-mask` (wasn't tested in theses test suites (#13540)
apparently).
==

src/transformers/pipelines/fill_mask.py
tests/test_pipelines_fill_mask.py
==================
9d60eebeb;Patrick von Platen;2021-09-13 11:30:10 +0200;up (#13536)

==

tests/test_tokenization_speech_to_text_2.py
==================
a2045067c;Xiaohan Zou;2021-09-13 16:08:38 +0800;Fix attention mask size checking for CLIP (#13535)

==

src/transformers/models/clip/modeling_clip.py
==================
68b0baeed;Alex Hedges;2021-09-13 03:06:07 -0400;Ignore past_key_values during GPT-Neo inference (#13521)

==

src/transformers/models/gpt_neo/configuration_gpt_neo.py
==================
07c2607d4;holazzer;2021-09-13 13:48:50 +0800;fix use_cache value assign (#13532)
fix use_cache value assign
==

src/transformers/models/prophetnet/modeling_prophetnet.py
==================
010965dcd;Suraj Patil;2021-09-10 22:52:20 +0530;[GPT-Neo] Simplify local attention (#13491)
* simplify local attention

* update tests

* add a comment and use torch.bitwise_xor
==

src/transformers/models/gpt_neo/modeling_gpt_neo.py
tests/test_modeling_gpt_neo.py
==================
a57d784df;Patrick von Platen;2021-09-10 18:19:10 +0200;[Wav2Vec2] Fix dtype 64 bug (#13517)
* fix

* 2nd fix
==

src/transformers/models/speech_to_text/feature_extraction_speech_to_text.py
src/transformers/models/wav2vec2/feature_extraction_wav2vec2.py
==================
72ec2f3eb;patrickvonplaten;2021-09-10 16:45:19 +0200;Docs for v4.10.1

==

.circleci/deploy.sh
docs/source/_static/js/custom.js
==================
26d9212e3;Matt;2021-09-10 14:49:17 +0100;TF multiple choice loss fix (#13513)
Fix issues with `TFMultipleChoiceLoss` if the choices dimension is None when `build()` is called.
==

src/transformers/modeling_tf_utils.py
==================
d7b3b709d;Patrick von Platen;2021-09-10 15:27:16 +0200;[Wav2Vec2] Fix normalization for non-padded tensors (#13512)
* finalize

* Apply suggestions from code review

* finish cleaner implementation

* more tests

* small fix

* finish

* up
==

src/transformers/feature_extraction_sequence_utils.py
src/transformers/models/speech_to_text/feature_extraction_speech_to_text.py
src/transformers/models/wav2vec2/feature_extraction_wav2vec2.py
tests/test_feature_extraction_speech_to_text.py
tests/test_feature_extraction_wav2vec2.py
==================
c63fcabfe;Nicolas Patry;2021-09-10 14:47:48 +0200;[Large PR] Entire rework of pipelines. (#13308)
* Enabling dataset iteration on pipelines.

Enabling dataset iteration on pipelines.

Unifying parameters under `set_parameters` function.

Small fix.

Last fixes after rebase

Remove print.

Fixing text2text `generate_kwargs`

No more `self.max_length`.

Fixing tf only conversational.

Consistency in start/stop index over TF/PT.

Speeding up drastically on TF (nasty bug where max_length would increase
a ton.)

Adding test for support for non fast tokenizers.

Fixign GPU usage on zero-shot.

Fix working on Tf.

Update src/transformers/pipelines/base.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

Update src/transformers/pipelines/base.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

Small cleanup.

Remove all asserts + simple format.

* Fixing audio-classification for large PR.

* Overly explicity null checking.

* Encapsulating GPU/CPU pytorch manipulation directly within `base.py`.

* Removed internal state for parameters of the  pipeline.

Instead of overriding implicitly internal state, we moved
to real named arguments on every `preprocess`, `_forward`,
`postprocess` function.

Instead `_sanitize_parameters` will be used to split all kwargs
of both __init__ and __call__ into the 3 kinds of named parameters.

* Move import warnings.

* Small fixes.

* Quality.

* Another small fix, using the CI to debug faster.

* Last fixes.

* Last fix.

* Small cleanup of tensor moving.

* is not None.

* Adding a bunch of docs + a iteration test.

* Fixing doc style.

* KeyDataset = None guard.

* RRemoving the Cuda test for pipelines (was testing).

* Even more simple iteration test.

* Correct import .

* Long day.

* Fixes in docs.

* [WIP] migrating object detection.

* Fixed the target_size bug.

* Fixup.

* Bad variable name.

* Fixing `ensure_on_device` respects original ModelOutput.
==

docs/source/add_new_pipeline.rst
docs/source/index.rst
docs/source/main_classes/pipelines.rst
docs/source/quicktour.rst
src/transformers/pipelines/__init__.py
src/transformers/pipelines/audio_classification.py
src/transformers/pipelines/automatic_speech_recognition.py
src/transformers/pipelines/base.py
src/transformers/pipelines/conversational.py
src/transformers/pipelines/feature_extraction.py
src/transformers/pipelines/fill_mask.py
src/transformers/pipelines/image_classification.py
src/transformers/pipelines/object_detection.py
src/transformers/pipelines/question_answering.py
src/transformers/pipelines/table_question_answering.py
src/transformers/pipelines/text2text_generation.py
src/transformers/pipelines/text_classification.py
src/transformers/pipelines/text_generation.py
src/transformers/pipelines/token_classification.py
src/transformers/pipelines/zero_shot_classification.py
src/transformers/testing_utils.py
tests/test_pipelines_common.py
tests/test_pipelines_conversational.py
tests/test_pipelines_feature_extraction.py
tests/test_pipelines_fill_mask.py
tests/test_pipelines_object_detection.py
tests/test_pipelines_token_classification.py
tests/test_pipelines_translation.py
==================
09549aa18;Stefan Schweter;2021-09-10 08:15:57 +0200;examples: minor fixes in flax example readme (#13502)

==

examples/flax/language-modeling/README.md
==================
aacd2123e;Nicolas Patry;2021-09-09 20:23:52 +0200;Fixing #13381 (#13400)
* Fixing #13381

* Enabling automatic LED models.
==

src/transformers/pipelines/zero_shot_classification.py
tests/test_pipelines_zero_shot.py
==================
db514a75d;Nicolas Patry;2021-09-09 19:36:09 +0200;Fixing backward compatiblity for non prefixed tokens (B-, I-). (#13493)

==

src/transformers/pipelines/token_classification.py
tests/test_pipelines_token_classification.py
==================
e59d4d014;Sylvain Gugger;2021-09-09 13:04:37 -0400;Refactor internals for Trainer push_to_hub (#13486)

==

src/transformers/file_utils.py
src/transformers/trainer.py
src/transformers/training_args.py
tests/test_trainer.py
==================
3dd538c4d;Nicolas Patry;2021-09-09 17:26:16 +0200;[Tentative] Moving slow tokenizer to the Trie world. (#13220)
* Moving slow tokenizer to the Trie world.

* Adding more docstrings to the Trie.

* Fixing doctest (incompatible wiht our format? )

* Update src/transformers/tokenization_utils.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Adding a lot more comment into the internals of this algorithm.

* Cleaner doc.

* Fixing the namings.

* Update src/transformers/tokenization_utils.py

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* quality.

* Fixing longest first match.

* Small improvements to cuts + more test + canine resistant test.

* Fixing fast test.

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
==

src/transformers/models/wav2vec2/tokenization_wav2vec2.py
src/transformers/tokenization_utils.py
tests/test_tokenization_common.py
==================
b8385d8a1;Matt;2021-09-09 15:54:08 +0100;TF Seq2Seq int dtype fix (#13496)
Fixes problems with passing int64 input to TF Seq2Seq models.
==

src/transformers/models/bart/modeling_tf_bart.py
src/transformers/models/blenderbot/modeling_tf_blenderbot.py
src/transformers/models/blenderbot_small/modeling_tf_blenderbot_small.py
src/transformers/models/marian/modeling_tf_marian.py
src/transformers/models/pegasus/modeling_tf_pegasus.py
==================
008c2d0b7;Aleksander Smywi≈Ñski-Pohl;2021-09-09 14:00:05 +0200;Fix typo in documentation (#13494)
* Fix typo in deepspeed documentation

* Add missing import in deepspeed configuration

* Fix path in translation examples
==

examples/pytorch/translation/README.md
==================
1c191efc3;Kamal Raj;2021-09-09 10:12:57 +0530;flax ner example (#13365)
* flax ner example

* added task to README

* updated readme

* 1. ArgumentParser -> HfArgumentParser
2. step-wise logging,eval and save

* added requirements.txt

* added progress bar

* updated README

* added check_min_version

* updated training data permuattion with JAX

* added metric lib to requirements

* updated readme table

* fixed imports
==

examples/flax/token-classification/README.md
examples/flax/token-classification/requirements.txt
examples/flax/token-classification/run_flax_ner.py
==================
c37573806;Aleksander Smywi≈Ñski-Pohl;2021-09-08 20:24:10 +0200;Fix typo in deepspeed documentation (#13482)
* Fix typo in deepspeed documentation

* Add missing import in deepspeed configuration
==

docs/source/main_classes/deepspeed.rst
==================
e1f6e4903;Anton Lozhkov;2021-09-08 19:51:51 +0300;Fix integration tests for TFWav2Vec2 and TFHubert

==

tests/test_modeling_tf_hubert.py
tests/test_modeling_tf_wav2vec2.py
==================
41cd52a76;Mohan Zhang;2021-09-08 11:48:00 -0400;fixed document (#13414)

==

docs/source/main_classes/trainer.rst
==================
330d83fdb;Koichi Yasuoka;2021-09-09 00:26:07 +0900;Typo in "end_of_word_suffix" (#13477)
But does it really work?
==

src/transformers/tokenization_utils_fast.py
==================
2a15e8ccf;Mishig Davaadorj;2021-09-08 17:17:32 +0200;Object detection pipeline (#12886)
* Implement object-detection pipeline

* Define threshold const

* Add `threshold` argument

* Refactor

* Uncomment test inputs

* `rm

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>

* Fix typo

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>

* Fix typo

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>

* Chore better doc

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>

* Rm unnecessary lines

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>

* Chore better naming

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>

* Update src/transformers/pipelines/object_detection.py

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>

* Update src/transformers/pipelines/object_detection.py

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>

* Fix typo

* Add `detr-tiny` for tests

* Add `ObjectDetectionPipeline` to `trnsfrmrs/init`

* Implement new bbox format

* Update detr post_process

* Update `load_img` method obj det pipeline

* make style

* Implement new testing format for obj det pipeln

* Add guard pytorch specific code in pipeline

* Add doc

* Make pipeline_obj_tet tests deterministic

* Revert some changes to `post_process` COCO api

* Chore

* Update src/transformers/pipelines/object_detection.py

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>

* Update src/transformers/pipelines/object_detection.py

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>

* Update src/transformers/pipelines/object_detection.py

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>

* Update src/transformers/pipelines/object_detection.py

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>

* Update src/transformers/pipelines/object_detection.py

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>

* Update src/transformers/pipelines/object_detection.py

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>

* Rm timm requirement

* make fixup

* Add timm requirement to test

* Make fixup

* Guard torch.Tensor

* Chore

* Delete unnecessary comment

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>
==

docs/source/main_classes/pipelines.rst
docs/source/model_doc/auto.rst
src/transformers/__init__.py
src/transformers/models/auto/__init__.py
src/transformers/models/auto/feature_extraction_auto.py
src/transformers/models/auto/modeling_auto.py
src/transformers/pipelines/__init__.py
src/transformers/pipelines/object_detection.py
src/transformers/utils/dummy_pt_objects.py
tests/test_pipelines_object_detection.py
==================
707105290;Matt;2021-09-08 15:06:04 +0100;Fix Tensorflow T5 with int64 input (#13479)
* Fix Tensorflow T5 with int64 input

* Style pass
==

src/transformers/models/t5/modeling_tf_t5.py
==================
361b6df36;Kevin Canwen Xu;2021-09-08 21:09:22 +0800;Throw ValueError for mirror downloads (#13478)

==

src/transformers/file_utils.py
==================
99029ab6b;Lysandre Debut;2021-09-08 08:28:22 -0400;Better error raised when cloned without lfs (#13401)
* Better error raised when cloned without lfs

* add from e
==

src/transformers/modeling_flax_utils.py
src/transformers/modeling_tf_utils.py
src/transformers/modeling_utils.py
==================
18447c206;Li-Huai (Allan) Lin;2021-09-08 20:03:35 +0800;Enable automated model list copying for localized READMEs (#13465)
* Complete basic mechanism

* Save

* Complete everything

* Style & Quality

* Update READMEs

* Add testing

* Fix README.md format

* Apply suggestions

* Fix format

* Update utils/check_copies.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

README.md
README_zh-hans.md
README_zh-hant.md
docs/source/index.rst
tests/test_utils_check_copies.py
utils/check_copies.py
==================
cd6653966;Sylvain Gugger;2021-09-08 07:45:36 -0400;Don't modify labels inplace in `LabelSmoother` (#13464)

==

src/transformers/trainer_pt_utils.py
==================
c164c651d;Suraj Patil;2021-09-08 14:21:13 +0530;[CLIP] fix logit_scale init (#13436)
* fix logit_scale init

* add logit_scale_init_value as config param
==

src/transformers/models/clip/configuration_clip.py
src/transformers/models/clip/modeling_clip.py
src/transformers/models/clip/modeling_flax_clip.py
tests/test_modeling_clip.py
==================
f667d5b26;Kevin Canwen Xu;2021-09-08 16:09:44 +0800;Deprecate Mirror for Downloading (#13470)
* Deprecated Mirror

* revert

* revert

* revert

* fix
==

src/transformers/file_utils.py
==================
f5d3bb1dd;Suraj Patil;2021-09-08 12:57:18 +0530;fix CLIP conversion script (#13474)

==

src/transformers/models/clip/convert_clip_original_pytorch_to_hf.py
==================
4be082ce3;shabie;2021-09-07 22:57:58 +0200;[docs] update dead quickstart link on resuing past for GPT2 (#13455)
* [docs] update dead quickstart link on resuing past for GPT2

Thed dead link have been replaced by two links of forward and call methods of the GPT2 class for torch and tensorflow respectively.

* [docs] fix formatting for gpt2 page update
==

docs/source/model_doc/gpt2.rst
==================
214683376;Anton Lozhkov;2021-09-07 23:47:52 +0300;Add unit_divisor to downloads (#13468)

==

src/transformers/file_utils.py
==================
63b90a51a;guillaume-be;2021-09-07 16:51:04 +0200;Optimized bad word ids (#13433)
* Optimized bad word ids generation

* Fixed optimized bad token ids

* Updated style
==

src/transformers/generation_logits_process.py
==================
5c7789d41;Nicolas Patry;2021-09-07 16:45:45 +0200;Fixing by correctly raising UnicodeDecodeError. (#13449)

==

src/transformers/models/byt5/tokenization_byt5.py
tests/test_tokenization_byt5.py
==================
79815090e;Nathan Raw;2021-09-07 03:58:45 -0600;Fix img classification tests (#13456)
* :white_check_mark: Update image-classification example's tests

* :fire: remove cats_and_dogs test samples

* :lipstick: fix flake8
==

examples/pytorch/test_examples.py
tests/fixtures/tests_samples/cats_and_dogs/Cat/1.jpg
tests/fixtures/tests_samples/cats_and_dogs/Cat/2.jpg
tests/fixtures/tests_samples/cats_and_dogs/Cat/3.jpg
tests/fixtures/tests_samples/cats_and_dogs/Cat/4.jpg
tests/fixtures/tests_samples/cats_and_dogs/Cat/5.jpg
tests/fixtures/tests_samples/cats_and_dogs/Dog/1.jpg
tests/fixtures/tests_samples/cats_and_dogs/Dog/2.jpg
tests/fixtures/tests_samples/cats_and_dogs/Dog/3.jpg
tests/fixtures/tests_samples/cats_and_dogs/Dog/4.jpg
tests/fixtures/tests_samples/cats_and_dogs/Dog/5.jpg
==================
92d4ef9ab;Anurag Kumar;2021-09-07 03:02:24 +0530;Update setup.py (#13421)

==

setup.py
==================
75858ca15;Shiv Dhar;2021-09-07 02:49:02 +0530;Update version of `packaging` package (#13454)

==

setup.py
src/transformers/dependency_versions_table.py
==================
f8363e49f;Anton Lozhkov;2021-09-07 00:12:43 +0300;Install libsndfile (#13403)

==

.github/workflows/self-scheduled.yml
==================
5642a555a;NielsRogge;2021-09-06 19:19:30 +0200;Add TAPAS MLM-only models (#13408)
* Add conversion of TapasForMaskedLM

* Add copied from statements
==

src/transformers/__init__.py
src/transformers/models/tapas/__init__.py
src/transformers/models/tapas/convert_tapas_original_tf_checkpoint_to_pytorch.py
src/transformers/models/tapas/modeling_tapas.py
src/transformers/utils/dummy_pt_objects.py
==================
2dd975b23;Suraj Patil;2021-09-06 21:46:25 +0530;skip image classification test (#13451)

==

examples/pytorch/test_examples.py
==================
c8be8a9ad;Nils Reimers;2021-09-06 16:30:13 +0200;Update model configs - Allow setters for common properties  (#13026)
* refactor GPT Config to allow dyn. properties

* make attribute_map a class attribute

* remove old code

* update unit test to test config: Add test for common properties setter

* update unit test to test config: Add test for common properties passed as parameters to __init__

* update to black code format

* Allow that setters are not defined for certain config classes

* update config classes to implement attribute_map

* bugfix lxmert config - id2labels was not defined when num_labels was set

* update broken configs - add attribute_maps

* update bart config

* update black codestyle

* update documentation on common config attributes

* update GPTJ config to new attribute map

* update docs on common attributes

* gptj config: add max_position_embeddings

* gptj config: format with black

* update speech to text 2 config

* format doc file to max_len 119

* update config template

==

docs/source/main_classes/configuration.rst
src/transformers/configuration_utils.py
src/transformers/models/bart/configuration_bart.py
src/transformers/models/bigbird_pegasus/configuration_bigbird_pegasus.py
src/transformers/models/blenderbot/configuration_blenderbot.py
src/transformers/models/blenderbot_small/configuration_blenderbot_small.py
src/transformers/models/ctrl/configuration_ctrl.py
src/transformers/models/detr/configuration_detr.py
src/transformers/models/distilbert/configuration_distilbert.py
src/transformers/models/flaubert/configuration_flaubert.py
src/transformers/models/fsmt/configuration_fsmt.py
src/transformers/models/funnel/configuration_funnel.py
src/transformers/models/gpt2/configuration_gpt2.py
src/transformers/models/gpt_neo/configuration_gpt_neo.py
src/transformers/models/gptj/configuration_gptj.py
src/transformers/models/led/configuration_led.py
src/transformers/models/lxmert/configuration_lxmert.py
src/transformers/models/m2m_100/configuration_m2m_100.py
src/transformers/models/marian/configuration_marian.py
src/transformers/models/mbart/configuration_mbart.py
src/transformers/models/openai/configuration_openai.py
src/transformers/models/pegasus/configuration_pegasus.py
src/transformers/models/prophetnet/configuration_prophetnet.py
src/transformers/models/reformer/configuration_reformer.py
src/transformers/models/speech_to_text/configuration_speech_to_text.py
src/transformers/models/speech_to_text_2/configuration_speech_to_text_2.py
src/transformers/models/t5/configuration_t5.py
src/transformers/models/transfo_xl/configuration_transfo_xl.py
src/transformers/models/xlm/configuration_xlm.py
src/transformers/models/xlnet/configuration_xlnet.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/configuration_{{cookiecutter.lowercase_modelname}}.py
tests/test_configuration_common.py
==================
cf4eb8b3f;Nicolas Patry;2021-09-06 16:11:23 +0200;Adding a test for multibytes unicode. (#13447)
* Adding a test for multibytes unicode.

* Adding some accents.

* Making sure decoding works.

* Make tests passing by being cheesy.
==

src/transformers/models/byt5/tokenization_byt5.py
tests/test_tokenization_byt5.py
==================
607611f24;Patrick von Platen;2021-09-06 16:09:24 +0200;up (#13448)

==

tests/test_modeling_encoder_decoder.py
==================
6b29bff85;Suraj Patil;2021-09-06 18:47:54 +0530;add torchvision in example test requirements (#13438)

==

examples/pytorch/_tests_requirements.txt
==================
26700a951;Anton Lozhkov;2021-09-06 15:55:13 +0300;Fix scheduled tests for `SpeechEncoderDecoderModel` (#13422)
* Add inputs to pretrained tests

* Make style
==

tests/test_modeling_speech_encoder_decoder.py
==================
73ad25880;Yih-Dar;2021-09-06 14:51:45 +0200;Fix tests without any real effect (#13406)
Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>
==

tests/test_modeling_encoder_decoder.py
==================
76c4d8bf2;Nathan Raw;2021-09-02 13:29:42 -0600;‚ú® Add PyTorch image classification example (#13134)
* :sparkles: add pytorch image classification example

* :fire: remove utils.py

* :lipstick: fix flake8 style issues

* :fire: remove unnecessary line

* :sparkles: limit dataset sizes

* :pushpin: update reqs

* :art: restructure - use datasets lib

* :art: import transforms directly

* :memo: add comments

* :lipstick: style

* :fire: remove flag

* :pushpin: update requirement warning

* :memo: add vision README.md

* :memo: update README.md

* :memo: update README.md

* :art: add image-classification tag to model card

* :truck: rename vision ‚û°Ô∏è image-classification

* :memo: update image-classification README.md
==

examples/pytorch/image-classification/README.md
examples/pytorch/image-classification/requirements.txt
examples/pytorch/image-classification/run_image_classification.py
examples/pytorch/test_examples.py
tests/fixtures/tests_samples/cats_and_dogs/Cat/1.jpg
tests/fixtures/tests_samples/cats_and_dogs/Cat/2.jpg
tests/fixtures/tests_samples/cats_and_dogs/Cat/3.jpg
tests/fixtures/tests_samples/cats_and_dogs/Cat/4.jpg
tests/fixtures/tests_samples/cats_and_dogs/Cat/5.jpg
tests/fixtures/tests_samples/cats_and_dogs/Dog/1.jpg
tests/fixtures/tests_samples/cats_and_dogs/Dog/2.jpg
tests/fixtures/tests_samples/cats_and_dogs/Dog/3.jpg
tests/fixtures/tests_samples/cats_and_dogs/Dog/4.jpg
tests/fixtures/tests_samples/cats_and_dogs/Dog/5.jpg
==================
9bd5d97cd;Patrick von Platen;2021-09-02 18:47:09 +0200;up (#13396)

==

tests/test_modeling_speech_encoder_decoder.py
==================
efa4f5f0e;Patrick von Platen;2021-09-02 18:11:26 +0200;fix (#13395)

==

tests/test_modeling_speech_encoder_decoder.py
==================
596bb85f2;Aman Madaan;2021-09-02 07:49:12 -0400;[docs] Update perplexity.rst to use negative log likelihood (#13386)
* [docs] Update perplexity.rst to use negative log likelihood

Model `forward` returns the negative log likelihood. The document correctly defines and calculates perplexity, but the description and variable names are inconsistent, which might cause confusion.

* [docs] restyle perplexity.rst
==

docs/source/perplexity.rst
==================
b91e65afe;Apoorv Garg;2021-09-02 15:28:23 +0530;Correct order of overflowing_tokens for slow tokenizer (#13179)
* correct order of overflowing_tokens for slow tokenizer (issue fix #13148)

* python 3.9 requires sentencepiece version 0.1.94 or above

* slicing of ids fixed in truncated_sequence()

* Update setup.py

* Correct order of overflowing tokens for pair of sentences

* code reformatted

* Update tokenization_utils_base.py

* reformatting file

* test to check single_input added

* missing function restored

* test to check pair_input overflowing tokens order

* test to check pair_input overflowing tokens order

* test to check pair_input overflowing tokens order

* added an error message for pair of seq and longest_first strategy

* test for pair_input modified

* variable name corrected

* fixed a typo in error message

* requested changes implemented

* required test added

* Corrected the message to match test message

* added error message for Luke Tokenizer

* lost test recovered

* docstring for truncate_sequences and prepare_for_model updated

* docstring for luke tokenizer updated

* updated ENCODE_PLUS_ADDITIONAL_KWARGS_DOCSTRING

* aligned text and fixed puncuatations

* improved style and quality of code

* fixed error_msg in truncate_sequences

* replaced encode_plus method with regular call method

* clean up

* rephrased the docstring
==

src/transformers/models/luke/tokenization_luke.py
src/transformers/tokenization_utils_base.py
tests/test_tokenization_common.py
==================
c9184a2e0;Nicolas Patry;2021-09-02 11:37:42 +0200;Enabling automatic loading of tokenizer with `pipeline` for (#13376)
`audio-classification`.
==

src/transformers/pipelines/__init__.py
tests/test_pipelines_audio_classification.py
==================
e92140c56;Suraj Patil;2021-09-02 15:02:18 +0530;fix example (#13387)

==

docs/source/model_doc/mbart.rst
==================
4114c9a75;NielsRogge;2021-09-02 09:46:05 +0200;Add tokenizer docs (#13373)

==

docs/source/model_doc/layoutxlm.rst
==================
872e6be03;Sachin Abeywardana;2021-09-02 16:45:56 +1000;Update clip loss calculation (#13217)
* Update clip loss calculation

Hello, I'm the author of the blog you took the snippet from. I think this way of calculating is possibly slightly more accurate for calculation.

* Apply suggestions from code review

Co-authored-by: Suraj Patil <surajp815@gmail.com>
==

src/transformers/models/clip/modeling_clip.py
==================
0a22335e6;Eduardo Gonzalez Ponferrada;2021-09-01 22:49:49 -0700;[Flax/run_hybrid_clip] Fix duplicating images when captions_per_image exceeds the number of captions, enable truncation

==

examples/research_projects/jax-projects/hybrid_clip/run_hybrid_clip.py
==================
c1c2d68d3;Sylvain Gugger;2021-09-01 20:54:49 -0400;Fix name and get_class method in AutoFeatureExtractor (#13385)

==

src/transformers/models/auto/feature_extraction_auto.py
==================
a105c9b77;Patrick von Platen;2021-09-01 23:12:01 +0200;fix (#13383)

==

tests/test_modeling_speech_encoder_decoder.py
==================
4475f1dc2;Patrick von Platen;2021-09-01 18:33:54 +0200;[Flax] Fix BigBird (#13380)
* finish

* finish
==

src/transformers/models/big_bird/modeling_flax_big_bird.py
==================
ecd539710;Lysandre Debut;2021-09-01 17:11:32 +0200;Fix RemBERT (#13375)

==

src/transformers/models/rembert/modeling_rembert.py
src/transformers/models/rembert/tokenization_rembert_fast.py
==================
33b7c9a8a;Lysandre Debut;2021-09-01 17:10:49 +0200;Add missing feature extractors (#13374)

==

src/transformers/models/auto/feature_extraction_auto.py
==================
2406892a2;Anton Lozhkov;2021-09-01 18:09:02 +0300;Add `Hubert` to the `AutoFeatureExtractor` (#13366)
* Add Hubert to the auto feature extractor

* Fix import structure
==

src/transformers/models/auto/feature_extraction_auto.py
src/transformers/models/hubert/__init__.py
==================
6b3532643;Sylvain Gugger;2021-09-01 10:57:43 -0400;Properly register missing submodules in main init (#13372)

==

src/transformers/__init__.py
==================
4b7988eb4;NielsRogge;2021-09-01 16:42:59 +0200;Fix assertion (#13369)

==

src/transformers/models/vit/modeling_vit.py
==================
c4d78f01d;SaulLu;2021-09-01 16:32:56 +0200;Fix tokenizer saving during training with `Trainer` (#12806)
* add test in trainer and test tokenizer saving wi
th trainer

* quality

* reverse trainer changes

* replace test in test_trainer by a test for all the tokenizers

* format

* add can_save_slow_tokenizer attribute to all tokenizers

* fix Herbert

* format

* Change comment in error

* add comments and a new assert

* Update src/transformers/models/albert/tokenization_albert_fast.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* change ValueError barthez

* change ValueError BigBird

* change ValueError Camembert

* change ValueError Mbart50

* change ValueError Pegasus

* change ValueError ReFormer

* change ValueError T5

* change ValueError RoBERTa

* XLNET fast

* Update tests/test_tokenization_common.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* change `assert` into `self.assertIn`

* format

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

src/transformers/models/albert/tokenization_albert_fast.py
src/transformers/models/barthez/tokenization_barthez_fast.py
src/transformers/models/big_bird/tokenization_big_bird_fast.py
src/transformers/models/camembert/tokenization_camembert_fast.py
src/transformers/models/herbert/tokenization_herbert_fast.py
src/transformers/models/mbart50/tokenization_mbart50_fast.py
src/transformers/models/pegasus/tokenization_pegasus_fast.py
src/transformers/models/reformer/tokenization_reformer_fast.py
src/transformers/models/t5/tokenization_t5_fast.py
src/transformers/models/xlm_roberta/tokenization_xlm_roberta_fast.py
src/transformers/models/xlnet/tokenization_xlnet_fast.py
src/transformers/tokenization_utils_fast.py
tests/test_tokenization_common.py
==================
c1b20e42f;Sylvain Gugger;2021-09-01 09:21:50 -0400;Redeploy stable documentation

==

.circleci/deploy.sh
==================
85cb44776;Li-Huai (Allan) Lin;2021-08-30 23:40:25 +0800;Revert "Correct wrong function signatures on the docs website (#13198)"
This reverts commit ffecfea9495d4aa788e1c05d0612a40bc4b460fc.
==

.circleci/config.yml
setup.py
src/transformers/dependency_versions_table.py
==================
4766e009b;NielsRogge;2021-09-01 15:05:40 +0200;Improve T5 docs  (#13240)
* Remove disclaimer

* First draft

* Fix rebase

* Improve docs some more

* Add inference section

* Improve example scripts section

* Improve code examples of modeling files

* Add docs regarding task prefix

* Address @craffel's comments

* Apply suggestions from @patrickvonplaten's review

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* Add suggestions from code review

* Apply @sgugger's suggestions

* Fix Flax code examples

* Fix index.rst

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
==

README.md
docs/source/index.rst
docs/source/model_doc/byt5.rst
docs/source/model_doc/mt5.rst
docs/source/model_doc/t5.rst
docs/source/model_doc/t5v1.1.rst
src/transformers/models/t5/modeling_flax_t5.py
src/transformers/models/t5/modeling_t5.py
src/transformers/models/t5/modeling_tf_t5.py
==================
ba1b3db70;donggyukimc;2021-09-01 21:03:16 +0900;fix wrong 'cls' masking for bigbird qa model output (#13143)

==

src/transformers/models/big_bird/modeling_big_bird.py
==================
7a26307e3;Sylvain Gugger;2021-09-01 07:54:28 -0400;Fixes for the documentation (#13361)

==

src/transformers/configuration_utils.py
src/transformers/data/__init__.py
src/transformers/deepspeed.py
src/transformers/generation_tf_utils.py
src/transformers/generation_utils.py
src/transformers/models/speech_to_text/configuration_speech_to_text.py
src/transformers/pipelines/__init__.py
==================
0b8c84e11;Patrick von Platen;2021-09-01 13:33:31 +0200;Add SpeechEncoderDecoder & Speech2Text2 (#13186)
* fix_torch_device_generate_test

* remove @

* up

* correct some bugs

* correct model

* finish speech2text extension

* up

* up

* up

* up

* Update utils/custom_init_isort.py

* up

* up

* update with tokenizer

* correct old tok

* correct old tok

* fix bug

* up

* up

* add more tests

* up

* fix docs

* up

* fix some more tests

* add better config

* correct some more things
"

* fix tests

* improve docs

* Apply suggestions from code review

* Apply suggestions from code review

* final fixes

* finalize

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* apply suggestions Lysandre and Sylvain

* apply nicos suggestions

* upload everything

* finish

Co-authored-by: Patrick von Platen <patrick@huggingface.co>
Co-authored-by: your_github_username <your_github_email>
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
==

README.md
docs/source/index.rst
docs/source/model_doc/speech_to_text_2.rst
docs/source/model_doc/speechencoderdecoder.rst
src/transformers/__init__.py
src/transformers/models/auto/configuration_auto.py
src/transformers/models/auto/feature_extraction_auto.py
src/transformers/models/auto/modeling_auto.py
src/transformers/models/auto/tokenization_auto.py
src/transformers/models/encoder_decoder/modeling_encoder_decoder.py
src/transformers/models/speech_encoder_decoder/__init__.py
src/transformers/models/speech_encoder_decoder/configuration_speech_encoder_decoder.py
src/transformers/models/speech_encoder_decoder/convert_speech_to_text_wav2vec2_seq2seq_original_to_pytorch.py
src/transformers/models/speech_encoder_decoder/modeling_speech_encoder_decoder.py
src/transformers/models/speech_to_text/modeling_speech_to_text.py
src/transformers/models/speech_to_text_2/__init__.py
src/transformers/models/speech_to_text_2/configuration_speech_to_text_2.py
src/transformers/models/speech_to_text_2/modeling_speech_to_text_2.py
src/transformers/models/speech_to_text_2/processing_speech_to_text_2.py
src/transformers/models/speech_to_text_2/tokenization_speech_to_text_2.py
src/transformers/models/wav2vec2/configuration_wav2vec2.py
src/transformers/pipelines/__init__.py
src/transformers/pipelines/automatic_speech_recognition.py
src/transformers/utils/dummy_pt_objects.py
tests/test_modeling_speech_encoder_decoder.py
tests/test_modeling_speech_to_text.py
tests/test_modeling_speech_to_text_2.py
tests/test_pipelines_audio_classification.py
tests/test_pipelines_automatic_speech_recognition.py
tests/test_tokenization_speech_to_text_2.py
==================
9396b4043;Lysandre Debut;2021-09-01 12:57:43 +0200;Fix GPT-J _CHECKPOINT_FOR_DOC typo (#13368)

==

src/transformers/models/gptj/modeling_gptj.py
==================
53ee995ac;Hamid Shojanazeri;2021-09-01 01:47:58 -0700;Fix for the issue of device-id getting hardcoded for token_type_ids during Tracing for ConvBert (#12287)
* added token_type_ids buffer to fix the issue #5664

* Handling the case that position_id buffer is not registered

* added token_type_ids buffer to fix the issue #5664

* modified to support device conversion when the model is traced
==

src/transformers/models/convbert/modeling_convbert.py
==================
5adf5cab2;Hamid Shojanazeri;2021-09-01 01:47:25 -0700;Fix for the issue of device-id getting hardcoded for position-ids during Tracing for Distillbert (#12290)
* registered buffer for position-ids to address issues similar to issue#5664

* added comment

* added the flag to prevent from adding the buffer into the state_dict
==

src/transformers/models/distilbert/modeling_distilbert.py
==================
5d1a3d135;Hamid Shojanazeri;2021-09-01 01:46:58 -0700;Fix for the issue of device-id getting hardcoded for position-ids during Tracing for Flaubert (#12292)
* adding position_ids buffer to fix the issue simialr to #5664

* adding position-id buffer to address similar issues to #5664
==

src/transformers/models/flaubert/modeling_flaubert.py
==================
58e999b7e;Lysandre Debut;2021-09-01 10:44:31 +0200;Torchscript test for Flaubert (#13353)
* Torchscript test for Flaubert

* Update tests/test_modeling_flaubert.py

* Update tests/test_modeling_flaubert.py
==

tests/test_modeling_flaubert.py
==================
d07c771dd;Lysandre Debut;2021-09-01 10:43:09 +0200;Torchscript test for ConvBERT (#13352)
* Torchscript test for ConvBERT

* Apply suggestions from code review
==

tests/test_modeling_convbert.py
==================
680733a7c;Lysandre Debut;2021-09-01 10:42:21 +0200;Torchscript test for DistilBERT (#13351)
* Torchscript test for DistilBERT

* Update tests/test_modeling_distilbert.py
==

tests/test_modeling_distilbert.py
==================
73a038128;Lysandre Debut;2021-09-01 10:41:46 +0200;Torchscript test (#13350)
* Torchscript test

* Remove print statement
==

tests/test_modeling_bert.py
==================
b9c6a9769;Anton Lozhkov;2021-09-01 11:03:48 +0300;Add the `AudioClassificationPipeline` (#13342)
* Add the audio classification pipeline

* Remove autoconfig exception

* Mark ffmpeg test as slow

* Rearrange pipeline tests

* Add small test

* Replace asserts with ValueError
==

docs/source/main_classes/pipelines.rst
docs/source/model_doc/auto.rst
src/transformers/__init__.py
src/transformers/modelcard.py
src/transformers/models/auto/__init__.py
src/transformers/models/auto/modeling_auto.py
src/transformers/models/wav2vec2/modeling_wav2vec2.py
src/transformers/pipelines/__init__.py
src/transformers/pipelines/audio_classification.py
src/transformers/utils/dummy_pt_objects.py
tests/test_pipelines_audio_classification.py
utils/check_repo.py
==================
02039352b;Patrick von Platen;2021-09-01 09:50:21 +0200;Update README.md

==

templates/adding_a_new_model/README.md
==================
d160782a5;Jonathan Chang;2021-09-01 15:49:03 +0800;Add template for adding flax models (#12441)
* Add option to add flax

* Add flax template for __init__.py

* Add flax template for .rst

* Copy TF modeling template

* Add a missing line in modeling_tf_... template

* Update first half of modeling_flax_..

* Update encoder flax template

* Copy test_modeling_tf... as test_modeling_flax...

* Replace some TF to Flax in test_modeling_flax_...

* Replace tf to np

some function might not work, like _assert_tensors_equal

* Replace remaining tf to np (might not work)

* Fix cookiecutter

* Add Flax in to_replace_... template

* Update transformers-cli add-new-model

* Save generate_flax in configuration.json

This will be read by transformers-cli

* Fix to_replace_... and cli

* Fix replace cli

* Fix cookiecutter name

* Move docstring earlier to avoid not defined error

* Fix a missing Module

* Add encoder-decoder flax template from bart

* Fix flax test

* Make style

* Fix endif

* Fix replace all "utf-8 -> unp-8"

* Update comment

* Fix flax template (add missing ..._DOCSTRING)

* Use flax_bart imports in template (was t5)

* Fix unp

* Update templates/adding_a_new_model/tests

* Revert "Fix unp"

This reverts commit dc9002a41d902c4f9b07343eab1cb350c8b7fd57.

* Remove one line of copied from to suppress CI error

* Use generate_tensorflow_pytorch_and_flax

* Add a missing part

* fix typo

* fix flax config

* add examples for flax

* small rename

* correct modeling imports

* correct auto loading

* corrects some flax tests

* correct small typo

* correct as type

* finish modif

* correct more templates

* final fixes

* add file testers

* up

* make sure tests match template regex

* correct pytorch

* correct tf

* correct more tf

* correct imports

* minor error

* minor error

* correct init

* more fixes

* correct more flax tests

* correct flax test

* more fixes

* correct docs

* update

* fix

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
==

.github/workflows/model-templates.yml
src/transformers/__init__.py
src/transformers/commands/add_new_model.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/__init__.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/configuration.json
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_flax_{{cookiecutter.lowercase_modelname}}.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_tf_{{cookiecutter.lowercase_modelname}}.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_{{cookiecutter.lowercase_modelname}}.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/test_modeling_flax_{{cookiecutter.lowercase_modelname}}.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/to_replace_{{cookiecutter.lowercase_modelname}}.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/{{cookiecutter.lowercase_modelname}}.rst
templates/adding_a_new_model/cookiecutter.json
templates/adding_a_new_model/tests/encoder-bert-tokenizer.json
templates/adding_a_new_model/tests/flax-encoder-bert-tokenizer.json
templates/adding_a_new_model/tests/flax-seq-2-seq-bart-tokenizer.json
templates/adding_a_new_model/tests/pt-encoder-bert-tokenizer.json
templates/adding_a_new_model/tests/pt-seq-2-seq-bart-tokenizer.json
templates/adding_a_new_model/tests/standalone.json
templates/adding_a_new_model/tests/tf-encoder-bert-tokenizer.json
templates/adding_a_new_model/tests/tf-seq-2-seq-bart-tokenizer.json
==================
8e2088788;Patrick von Platen;2021-09-01 09:37:51 +0200;Update self-push.yml (#13364)

==

.github/workflows/self-push.yml
==================
c02cd95c5;Stella Biderman;2021-08-31 11:53:02 -0400;GPT-J-6B (#13022)
* Test GPTJ implementation

* Fixed conflicts

* Update __init__.py

* Update __init__.py

* change GPT_J to GPTJ

* fix missing imports and typos

* use einops for now
(need to change to torch ops later)

* Use torch ops instead of einsum

* remove einops deps

* Update configuration_auto.py

* Added GPT J

* Update gptj.rst

* Update __init__.py

* Update test_modeling_gptj.py

* Added GPT J

* Changed configs to match GPT2 instead of GPT Neo

* Removed non-existent sequence model

* Update configuration_auto.py

* Update configuration_auto.py

* Update configuration_auto.py

* Update modeling_gptj.py

* Update modeling_gptj.py

* Progress on updating configs to agree with GPT2

* Update modeling_gptj.py

* num_layers -> n_layer

* layer_norm_eps -> layer_norm_epsilon

* attention_layers -> num_hidden_layers

* Update modeling_gptj.py

* attention_pdrop -> attn_pdrop

* hidden_act -> activation_function

* Update configuration_gptj.py

* Update configuration_gptj.py

* Update configuration_gptj.py

* Update configuration_gptj.py

* Update configuration_gptj.py

* Update modeling_gptj.py

* Update modeling_gptj.py

* Update modeling_gptj.py

* Update modeling_gptj.py

* Update modeling_gptj.py

* Update modeling_gptj.py

* fix layernorm and lm_head size
delete attn_type

* Update docs/source/model_doc/gptj.rst

Co-authored-by: Suraj Patil <surajp815@gmail.com>

* removed claim that GPT J uses local attention

* Removed GPTJForSequenceClassification

* Update src/transformers/models/gptj/configuration_gptj.py

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* Removed unsupported boilerplate

* Update tests/test_modeling_gptj.py

Co-authored-by: Suraj Patil <surajp815@gmail.com>

* Update src/transformers/models/gptj/modeling_gptj.py

Co-authored-by: Suraj Patil <surajp815@gmail.com>

* Update src/transformers/models/gptj/modeling_gptj.py

Co-authored-by: Suraj Patil <surajp815@gmail.com>

* Update src/transformers/models/gptj/modeling_gptj.py

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* Update tests/test_modeling_gptj.py

Co-authored-by: Eric Hallahan <eric@hallahans.name>

* Update tests/test_modeling_gptj.py

Co-authored-by: Eric Hallahan <eric@hallahans.name>

* Update tests/test_modeling_gptj.py

Co-authored-by: Eric Hallahan <eric@hallahans.name>

* Update src/transformers/models/gptj/modeling_gptj.py

Co-authored-by: Suraj Patil <surajp815@gmail.com>

* Update __init__.py

* Update configuration_gptj.py

* Update modeling_gptj.py

* Corrected indentation

* Remove stray backslash

* Delete .DS_Store

* Delete .DS_Store

* Delete .DS_Store

* Delete .DS_Store

* Delete .DS_Store

* Update docs to match

* Remove tf loading

* Remove config.jax

* Remove stray `else:` statement

* Remove references to `load_tf_weights_in_gptj`

* Adapt tests to match output from GPT-J 6B

* Apply suggestions from code review

Co-authored-by: Suraj Patil <surajp815@gmail.com>

* Default `activation_function` to `gelu_new`

- Specify the approximate formulation of GELU to ensure parity with the default setting of `jax.nn.gelu()`

* Fix part of the config documentation

* Revert "Update configuration_auto.py"

This reverts commit e9860e9c043b6ebf57a0e705044e9ec9ba2263bb.

* Revert "Update configuration_auto.py"

This reverts commit cfaaae4c4dc70f1fbe9abd60fc8bd0b863b8c011.

* Revert "Update configuration_auto.py"

This reverts commit 687788954fd0cfbc567fa1202d56a4ff9271944f.

* Revert "Update configuration_auto.py"

This reverts commit 194d024ea87d4fcef0dcb08e57f52c47511a9fc6.

* Hyphenate GPT-J

* Undid sorting of the models alphabetically

* Reverting previous commit

* fix style and quality issues

* Update docs/source/model_doc/gptj.rst

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/__init__.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update tests/test_modeling_gptj.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/models/gptj/modeling_gptj.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/__init__.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/models/gptj/modeling_gptj.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/models/gptj/modeling_gptj.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/models/gptj/configuration_gptj.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/models/gptj/configuration_gptj.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/models/gptj/configuration_gptj.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/models/gptj/modeling_gptj.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/models/gptj/modeling_gptj.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/models/gptj/modeling_gptj.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/models/gptj/modeling_gptj.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/models/gptj/modeling_gptj.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Replaced GPTJ-specific code with generic code

* Update src/transformers/models/gptj/modeling_gptj.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Made the code always use rotary positional encodings

* Update index.rst

* Fix documentation

* Combine attention classes

- Condense all attention operations into `GPTJAttention`
- Replicate GPT-2 and improve code clarity by renaming `GPTJAttention.attn_pdrop` and `GPTJAttention.resid_pdrop` to `GPTJAttention.attn_dropout` and `GPTJAttention.resid_dropout`

* Removed `config.rotary_dim` from tests

* Update test_modeling_gptj.py

* Update test_modeling_gptj.py

* Fix formatting

* Removed depreciated argument `layer_id` to `GPTJAttention`

* Update modeling_gptj.py

* Update modeling_gptj.py

* Fix code quality

* Restore model functionality

* Save `lm_head.weight` in checkpoints

* Fix crashes when loading with reduced precision

* refactor self._attn(...)` and rename layer weights"

* make sure logits are in fp32 for sampling

* improve docs

* Add `GPTJForCausalLM` to `TextGenerationPipeline` whitelist

* Added GPT-J to the README

* Fix doc/readme consistency

* Add rough parallelization support

- Remove unused imports and variables
- Clean up docstrings
- Port experimental parallelization code from GPT-2 into GPT-J

* Clean up loose ends

* Fix index.rst

Co-authored-by: kurumuz <kurumuz1@gmail.com>
Co-authored-by: Suraj Patil <surajp815@gmail.com>
Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
Co-authored-by: Eric Hallahan <eric@hallahans.name>
Co-authored-by: Leo Gao <54557097+leogao2@users.noreply.github.com>
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
Co-authored-by: your_github_username <your_github_email>
Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
==

README.md
docs/source/index.rst
docs/source/model_doc/gptj.rst
src/transformers/__init__.py
src/transformers/models/__init__.py
src/transformers/models/auto/configuration_auto.py
src/transformers/models/auto/modeling_auto.py
src/transformers/models/gptj/__init__.py
src/transformers/models/gptj/configuration_gptj.py
src/transformers/models/gptj/modeling_gptj.py
src/transformers/pipelines/text_generation.py
src/transformers/utils/dummy_pt_objects.py
tests/test_modeling_gptj.py
==================
e53af030c;Lysandre;2021-08-31 16:18:14 +0200;Re-deploy documentation

==

.circleci/deploy.sh
==================
20677b22f;Lysandre;2021-08-31 16:15:49 +0200;Adjust documentation index

==

docs/source/index.rst
==================
5ee67a441;Lysandre;2021-08-31 16:02:31 +0200;Docs for v4.10.0

==

.circleci/deploy.sh
docs/source/_static/js/custom.js
docs/source/conf.py
examples/pytorch/language-modeling/run_clm.py
examples/pytorch/language-modeling/run_mlm.py
examples/pytorch/language-modeling/run_plm.py
examples/pytorch/multiple-choice/run_swag.py
examples/pytorch/question-answering/run_qa.py
examples/pytorch/question-answering/run_qa_beam_search.py
examples/pytorch/question-answering/run_qa_beam_search_no_trainer.py
examples/pytorch/question-answering/run_qa_no_trainer.py
examples/pytorch/summarization/run_summarization.py
examples/pytorch/text-classification/run_glue.py
examples/pytorch/text-classification/run_xnli.py
examples/pytorch/token-classification/run_ner.py
examples/pytorch/translation/run_translation.py
examples/tensorflow/multiple-choice/run_swag.py
examples/tensorflow/question-answering/run_qa.py
examples/tensorflow/summarization/run_summarization.py
examples/tensorflow/text-classification/run_glue.py
examples/tensorflow/translation/run_translation.py
setup.py
src/transformers/__init__.py
==================
d12bbe494;Lysandre;2021-08-31 15:53:10 +0200;Release: v4.10.0

==

README.md
docs/source/conf.py
examples/pytorch/language-modeling/run_clm.py
examples/pytorch/language-modeling/run_mlm.py
examples/pytorch/language-modeling/run_plm.py
examples/pytorch/multiple-choice/run_swag.py
examples/pytorch/question-answering/run_qa.py
examples/pytorch/question-answering/run_qa_beam_search.py
examples/pytorch/question-answering/run_qa_beam_search_no_trainer.py
examples/pytorch/question-answering/run_qa_no_trainer.py
examples/pytorch/summarization/run_summarization.py
examples/pytorch/text-classification/run_glue.py
examples/pytorch/text-classification/run_xnli.py
examples/pytorch/token-classification/run_ner.py
examples/pytorch/translation/run_translation.py
examples/tensorflow/multiple-choice/run_swag.py
examples/tensorflow/question-answering/run_qa.py
examples/tensorflow/summarization/run_summarization.py
examples/tensorflow/text-classification/run_glue.py
examples/tensorflow/translation/run_translation.py
setup.py
src/transformers/__init__.py
==================
642e1936e;Patrick von Platen;2021-08-31 15:01:35 +0200;[GitHub Runner] Fix flax runner (#13357)
* correct

* also comment out multi-gpu test push
==

.github/workflows/self-push.yml
.github/workflows/self-scheduled.yml
==================
c76de1053;Sylvain Gugger;2021-08-31 08:42:00 -0400;Add generate kwargs to Seq2SeqTrainingArguments (#13339)
* Add generate kwargs to Seq2SeqTrainingArguments

* typo

* Address review comments + doc

* Style
==

examples/pytorch/summarization/run_summarization.py
examples/pytorch/translation/run_translation.py
src/transformers/trainer_seq2seq.py
src/transformers/training_args_seq2seq.py
==================
702f4a49c;Matt;2021-08-31 13:21:39 +0100;Fixed CLM model still using MODEL_FOR_MASKED_LM_MAPPING (#13002)

==

examples/tensorflow/language-modeling/run_clm.py
==================
aa08a3466;Lysandre;2021-08-31 14:18:20 +0200;[Flax tests] NVIDIA-SMI failure should continue

==

.github/workflows/self-push.yml
.github/workflows/self-scheduled.yml
==================
854260ca4;Matt;2021-08-31 13:06:48 +0100;TF/Numpy variants for all DataCollator classes (#13105)
* Adding a TF variant of the DataCollatorForTokenClassification to get feedback

* Added a Numpy variant and a post_init check to fail early if a missing import is found

* Fixed call to Numpy variant

* Added a couple more of the collators

* Update src/transformers/data/data_collator.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Fixes, style pass, finished DataCollatorForSeqToSeq

* Added all the LanguageModeling DataCollators, except SOP and PermutationLanguageModeling

* Adding DataCollatorForPermutationLanguageModeling

* Style pass

* Add missing `__call__` for PLM

* Remove `post_init` checks for frameworks because the imports inside them were making us fail code quality checks

* Remove unused imports

* First attempt at some TF tests

* A second attempt to make any of those tests actually work

* TF tests, round three

* TF tests, round four

* TF tests, round five

* TF tests, all enabled!

* Style pass

* Merging tests into `test_data_collator.py`

* Merging tests into `test_data_collator.py`

* Fixing up test imports

* Fixing up test imports

* Trying shuffling the conditionals around

* Commenting out non-functional old tests

* Completed all tests for all three frameworks

* Style pass

* Fixed test typo

* Style pass

* Move standard `__call__` method to mixin

* Rearranged imports for `test_data_collator`

* Fix data collator typo "torch" -> "pt"

* Fixed the most embarrassingly obvious bug

* Update src/transformers/data/data_collator.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Renaming mixin

* Updating docs

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
Co-authored-by: Dalton Walker <dalton_walker@icloud.com>
Co-authored-by: Andrew Romans <andrew.romans@hotmail.com>
==

docs/source/main_classes/data_collator.rst
src/transformers/__init__.py
src/transformers/data/data_collator.py
src/transformers/utils/dummy_pt_objects.py
tests/test_data_collator.py
==================
74b3344fb;Sylvain Gugger;2021-08-31 07:06:49 -0400;Clean up test file

==

tests/test_modeling_common.py
==================
ef8d6f2b4;Jongheon Kim;2021-08-31 19:51:25 +0900;Set missing seq_length variable when using inputs_embeds with ALBERT & Remove code duplication (#13152)
* Set seq_length variable when using inputs_embeds

* remove code duplication
==

src/transformers/models/albert/modeling_albert.py
src/transformers/models/bert/modeling_bert.py
src/transformers/models/bert_generation/modeling_bert_generation.py
src/transformers/models/big_bird/modeling_big_bird.py
src/transformers/models/canine/modeling_canine.py
src/transformers/models/ibert/modeling_ibert.py
src/transformers/models/luke/modeling_luke.py
src/transformers/models/megatron_bert/modeling_megatron_bert.py
src/transformers/models/rembert/modeling_rembert.py
src/transformers/models/roberta/modeling_roberta.py
src/transformers/models/roformer/modeling_roformer.py
src/transformers/models/splinter/modeling_splinter.py
src/transformers/models/visual_bert/modeling_visual_bert.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_{{cookiecutter.lowercase_modelname}}.py
==================
180c6de6a;Jake Tae;2021-08-31 06:49:05 -0400;docs: fix minor typo (#13289)
`at` should be `a1`
==

docs/source/parallelism.md
==================
066fd047c;Stas Bekman;2021-08-31 03:47:23 -0700;correct TP implementation resources (#13248)
fix a few implementation links
==

docs/source/parallelism.md
==================
4d10474fa;Sylvain Gugger;2021-08-31 06:34:31 -0400;Handle nested dict/lists of tensors as inputs in the Trainer (#13338)

==

src/transformers/trainer.py
==================
3efcfeab6;Kamal Raj;2021-08-31 16:02:47 +0530;Deberta_v2 tf (#13120)
* Deberta_v2 tf

* added new line at the end of file, make style

* +V2, typo

* remove never executed branch of code

* rm cmnt and fixed typo in url filter

* cleanup according to review comments

* added #Copied from
==

docs/source/index.rst
docs/source/model_doc/deberta_v2.rst
src/transformers/__init__.py
src/transformers/models/auto/modeling_tf_auto.py
src/transformers/models/deberta_v2/__init__.py
src/transformers/models/deberta_v2/modeling_tf_deberta_v2.py
src/transformers/utils/dummy_tf_objects.py
tests/test_modeling_tf_deberta_v2.py
==================
286ccefb4;Apoorv Garg;2021-08-31 15:58:37 +0530;doc mismatch fixed (#13345)

==

docs/source/preprocessing.rst
==================
41c559415;tucan9389;2021-08-31 19:19:04 +0900;Add GPT2ForTokenClassification (#13290)
* Add GPT2ForTokenClassification

* Fix dropout exception for GPT2 NER

* Remove sequence label in test

* Change TokenClassifierOutput to TokenClassifierOutputWithPast

* Fix for black formatter

* Remove dummy

* Update docs for GPT2ForTokenClassification

* Fix check_inits ci fail

* Update dummy_pt_objects after make fix-copies

* Remove TokenClassifierOutputWithPast

* Fix tuple input issue

Co-authored-by: danielsejong55@gmail.com <danielsejong55@gmail.com>
==

docs/source/model_doc/gpt2.rst
src/transformers/__init__.py
src/transformers/models/auto/modeling_auto.py
src/transformers/models/gpt2/__init__.py
src/transformers/models/gpt2/modeling_gpt2.py
src/transformers/utils/dummy_pt_objects.py
tests/test_modeling_gpt2.py
==================
11fbc32e3;Serhiy-Shekhovtsov;2021-08-31 10:01:12 +0000;Fixing a typo in the data_collator documentation (#13309)

==

docs/source/main_classes/data_collator.rst
==================
062300ba7;Patrick von Platen;2021-08-31 11:08:22 +0200;[Testing] Add Flax Tests on GPU, Add Speech and Vision to Flax & TF tests (#13313)
* up

* finish

* Apply suggestions from code review

* apply Lysandres suggestions

* adapt circle ci as well

* finish

* Update setup.py
==

.circleci/config.yml
.github/workflows/self-push.yml
.github/workflows/self-scheduled.yml
setup.py
==================
8b2de0e48;Sylvain Gugger;2021-08-31 03:57:01 -0400;Tests fetcher tests (#13340)
* Incorporate tests dependencies in tests_fetcher

* Harder modif

* Debug

* Loop through all files

* Last modules

* Remove debug statement
==

tests/test_modeling_common.py
utils/tests_fetcher.py
==================
42f359d01;Olatunji Ruwase;2021-08-30 10:01:06 -0700;Use DS callable API to allow hf_scheduler + ds_optimizer (#13216)
* Use DS callable API to allow hf_scheduler + ds_optimizer

* Preserve backward-compatibility

* Restore backward compatibility

* Tweak arg positioning

* Tweak arg positioning

* bump the required version

* Undo indent

* Update src/transformers/trainer.py

* style

Co-authored-by: Stas Bekman <stas@stason.org>
Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>
==

setup.py
src/transformers/deepspeed.py
src/transformers/dependency_versions_table.py
src/transformers/trainer.py
tests/deepspeed/test_deepspeed.py
==================
35236b870;Laura Hanu;2021-08-30 17:39:05 +0100;Add missing module __spec__ (#13321)
* added missing __spec__ to _LazyModule

* test __spec__ is not None after module import

* changed module_spec arg to be optional in _LazyModule

* fix style issue

* added module spec test to test_file_utils
==

src/transformers/__init__.py
src/transformers/file_utils.py
tests/test_file_utils.py
==================
4ebe798ff;Sylvain Gugger;2021-08-30 12:09:14 -0400;Fix release utils (#13337)
* Fix release utils

* Update docs/source/conf.py

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
==

docs/source/conf.py
utils/release.py
==================
c4ecd234f;Sylvain Gugger;2021-08-30 11:55:18 -0400;Fix AutoTokenizer when no fast tokenizer is available (#13336)
* Fix AutoTokenizer when a tokenizer has no fast version

* Add test
==

src/transformers/models/auto/tokenization_auto.py
tests/test_tokenization_auto.py
==================
ffecfea94;Li-Huai (Allan) Lin;2021-08-30 23:40:25 +0800;Correct wrong function signatures on the docs website (#13198)
* Correct outdated function signatures on website.

* Upgrade sphinx to 3.5.4 (latest 3.x)

* Test

* Test

* Test

* Test

* Test

* Test

* Revert unnecessary changes.

* Change sphinx version to 3.5.4"

* Test python 3.7.11
==

.circleci/config.yml
setup.py
src/transformers/dependency_versions_table.py
==================
98e409abb;Kamal Raj;2021-08-30 20:59:27 +0530;albert flax (#13294)
* albert flax

* year -> 2021

* docstring updated for flax

* removed head_mask

* removed from_pt

* removed passing attention_mask to embedding layer
==

docs/source/index.rst
docs/source/model_doc/albert.rst
src/transformers/__init__.py
src/transformers/models/albert/__init__.py
src/transformers/models/albert/modeling_flax_albert.py
src/transformers/models/auto/modeling_flax_auto.py
src/transformers/utils/dummy_flax_objects.py
tests/test_modeling_flax_albert.py
==================
ee5b24573;Ben Nimmo;2021-08-30 16:19:50 +0100;the use_auth_token has not been set up early enough in the model_kwargs. Fixes #12941 (#13205)

==

src/transformers/pipelines/__init__.py
==================
030567309;Maxwell Forbes;2021-08-30 08:12:35 -0700;Fall back to `observed_batch_size` when the `dataloader` does not know the `batch_size`. (#13188)

==

src/transformers/trainer.py
==================
ce6add8ec;Nathan Raw;2021-08-30 08:45:57 -0600;:bug: fix small model card bugs (#13310)
* :bug: fix small model card bugs

* :lipstick: style
==

src/transformers/modelcard.py
==================
139e83015;Sylvain Gugger;2021-08-30 10:35:09 -0400;Update label2id in the model config for run_glue (#13334)

==

examples/pytorch/text-classification/run_glue.py
examples/pytorch/text-classification/run_glue_no_trainer.py
examples/tensorflow/text-classification/run_glue.py
==================
6f3c99acc;fcakyon;2021-08-30 16:59:17 +0300;add ability to connect a neptune.ai run (#13319)
when `NEPTUNE_RUN_ID` environmetnt variable is set, neptune will log into the previous run with id `NEPTUNE_RUN_ID`
==

src/transformers/integrations.py
==================
f4f4e6b2d;Sylvain Gugger;2021-08-30 09:43:23 -0400;Use existing functionality for #13251 (#13333)

==

src/transformers/models/auto/tokenization_auto.py
==================
d50649531;Li-Huai (Allan) Lin;2021-08-30 20:18:51 +0800;Check None before going through iteration (#13250)
* Check None before going through iteration

* Format
==

src/transformers/trainer.py
==================
774760e6f;Kamal Raj;2021-08-30 17:46:18 +0530;distilbert-flax (#13324)
* distilbert-flax

* added missing self

* docs fix

* removed tied kernal extra init

* updated docs

* x -> hidden states

* removed head_mask

* removed from_pt, +FLAX

* updated year
==

docs/source/index.rst
docs/source/model_doc/distilbert.rst
src/transformers/__init__.py
src/transformers/models/auto/modeling_flax_auto.py
src/transformers/models/distilbert/__init__.py
src/transformers/models/distilbert/modeling_flax_distilbert.py
src/transformers/utils/dummy_flax_objects.py
tests/test_modeling_flax_distilbert.py
==================
01977466f;arfy slowy;2021-08-30 19:09:14 +0700;fix: typo spelling grammar (#13212)
* fix: typo spelling grammar

* fix: make fixup
==

docs/source/main_classes/trainer.rst
docs/source/model_doc/deberta_v2.rst
docs/source/model_doc/speech_to_text.rst
docs/source/training.rst
src/transformers/deepspeed.py
src/transformers/modelcard.py
src/transformers/modeling_flax_utils.py
src/transformers/modeling_tf_utils.py
src/transformers/modeling_utils.py
src/transformers/models/big_bird/modeling_big_bird.py
src/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py
src/transformers/models/clip/tokenization_clip.py
src/transformers/models/detr/modeling_detr.py
src/transformers/models/encoder_decoder/modeling_encoder_decoder.py
src/transformers/models/gpt_neo/configuration_gpt_neo.py
src/transformers/models/hubert/modeling_hubert.py
src/transformers/models/rag/modeling_rag.py
src/transformers/models/rag/modeling_tf_rag.py
src/transformers/models/wav2vec2/modeling_flax_wav2vec2.py
src/transformers/models/wav2vec2/modeling_wav2vec2.py
src/transformers/onnx/convert.py
src/transformers/onnx/features.py
src/transformers/tokenization_utils_base.py
src/transformers/tokenization_utils_fast.py
==================
ef83dc4f0;Navjot;2021-08-30 05:08:16 -0700;Improve documentation of pooler_output in ModelOutput (#13228)
* update documentation of pooler_output in modeling_outputs, making it more clear and available for generic usage

* Update src/transformers/modeling_outputs.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/modeling_outputs.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* run make style

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

src/transformers/modeling_outputs.py
==================
7828194eb;Falk Puschner;2021-08-30 13:46:55 +0200;:sparkles: add citation file (#13214)

==

CITATION.cff
==================
b6ddb08a6;NielsRogge;2021-08-30 12:35:42 +0200;Add LayoutLMv2 + LayoutXLM (#12604)
* First commit

* Make style

* Fix dummy objects

* Add Detectron2 config

* Add LayoutLMv2 pooler

* More improvements, add documentation

* More improvements

* Add model tests

* Add clarification regarding image input

* Improve integration test

* Fix bug

* Fix another bug

* Fix another bug

* Fix another bug

* More improvements

* Make more tests pass

* Make more tests pass

* Improve integration test

* Remove gradient checkpointing and add head masking

* Add integration test

* Add LayoutLMv2ForSequenceClassification to the tests

* Add LayoutLMv2ForQuestionAnswering

* More improvements

* More improvements

* Small improvements

* Fix _LazyModule

* Fix fast tokenizer

* Move sync_batch_norm to a separate method

* Replace dummies by requires_backends

* Move calculation of visual bounding boxes to separate method + update README

* Add models to main init

* First draft

* More improvements

* More improvements

* More improvements

* More improvements

* More improvements

* Remove is_split_into_words

* More improvements

* Simply tesseract - no use of pandas anymore

* Add LayoutLMv2Processor

* Update is_pytesseract_available

* Fix bugs

* Improve feature extractor

* Fix bug

* Add print statement

* Add truncation of bounding boxes

* Add tests for LayoutLMv2FeatureExtractor and LayoutLMv2Tokenizer

* Improve tokenizer tests

* Make more tokenizer tests pass

* Make more tests pass, add integration tests

* Finish integration tests

* More improvements

* More improvements - update API of the tokenizer

* More improvements

* Remove support for VQA training

* Remove some files

* Improve feature extractor

* Improve documentation and one more tokenizer test

* Make quality and small docs improvements

* Add batched tests for LayoutLMv2Processor, remove fast tokenizer

* Add truncation of labels

* Apply suggestions from code review

* Improve processor tests

* Fix failing tests and add suggestion from code review

* Fix tokenizer test

* Add detectron2 CI job

* Simplify CI job

* Comment out non-detectron2 jobs and specify number of processes

* Add pip install torchvision

* Add durations to see which tests are slow

* Fix tokenizer test and make model tests smaller

* Frist draft

* Use setattr

* Possible fix

* Proposal with configuration

* First draft of fast tokenizer

* More improvements

* Enable fast tokenizer tests

* Make more tests pass

* Make more tests pass

* More improvements

* Addd padding to fast tokenizer

* Mkae more tests pass

* Make more tests pass

* Make all tests pass for fast tokenizer

* Make fast tokenizer support overflowing boxes and labels

* Add support for overflowing_labels to slow tokenizer

* Add support for fast tokenizer to the processor

* Update processor tests for both slow and fast tokenizers

* Add head models to model mappings

* Make style & quality

* Remove Detectron2 config file

* Add configurable option to label all subwords

* Fix test

* Skip visual segment embeddings in test

* Use ResNet-18 backbone in tests instead of ResNet-101

* Proposal

* Re-enable all jobs on CI

* Fix installation of tesseract

* Fix failing test

* Fix index table

* Add LayoutXLM doc page, first draft of code examples

* Improve documentation a lot

* Update expected boxes for Tesseract 4.0.0 beta

* Use offsets to create labels instead of checking if they start with ##

* Update expected boxes for Tesseract 4.1.1

* Fix conflict

* Make variable names cleaner, add docstring, add link to notebooks

* Revert "Fix conflict"

This reverts commit a9b46ce9afe47ebfcfe7b45e6a121d49e74ef2c5.

* Revert to make integration test pass

* Apply suggestions from @LysandreJik's review

* Address @patrickvonplaten's comments

* Remove fixtures DocVQA in favor of dataset on the hub

Co-authored-by: Lysandre <lysandre.debut@reseau.eseo.fr>
==

.circleci/config.yml
README.md
docs/source/index.rst
docs/source/model_doc/layoutlmv2.rst
docs/source/model_doc/layoutxlm.rst
src/transformers/__init__.py
src/transformers/convert_slow_tokenizer.py
src/transformers/file_utils.py
src/transformers/models/__init__.py
src/transformers/models/auto/configuration_auto.py
src/transformers/models/auto/modeling_auto.py
src/transformers/models/auto/tokenization_auto.py
src/transformers/models/layoutlmv2/__init__.py
src/transformers/models/layoutlmv2/configuration_layoutlmv2.py
src/transformers/models/layoutlmv2/feature_extraction_layoutlmv2.py
src/transformers/models/layoutlmv2/modeling_layoutlmv2.py
src/transformers/models/layoutlmv2/processing_layoutlmv2.py
src/transformers/models/layoutlmv2/tokenization_layoutlmv2.py
src/transformers/models/layoutlmv2/tokenization_layoutlmv2_fast.py
src/transformers/testing_utils.py
src/transformers/utils/dummy_detectron2_objects.py
src/transformers/utils/dummy_pt_objects.py
src/transformers/utils/dummy_tokenizers_objects.py
src/transformers/utils/dummy_vision_objects.py
tests/test_feature_extraction_layoutlmv2.py
tests/test_modeling_layoutlmv2.py
tests/test_processor_layoutlmv2.py
tests/test_tokenization_layoutlmv2.py
==================
439e7abd2;Hwijeen Ahn;2021-08-30 19:09:24 +0900;use float 16 in causal mask and masked bias (#13194)

==

src/transformers/models/megatron_gpt2/convert_megatron_gpt2_checkpoint.py
==================
8be921f9d;Nicolas Patry;2021-08-30 12:04:30 +0200;Announcing the default model used by the pipeline (with a link). (#13276)

==

src/transformers/pipelines/__init__.py
==================
a75db353c;Patrick von Platen;2021-08-30 12:03:02 +0200;[Slow tests] Disable Wav2Vec2 pretraining test for now (#13303)
* fix_torch_device_generate_test

* remove @

* wav2vec2 pretraining

Co-authored-by: Patrick von Platen <patrick@huggingface.co>
==

tests/test_modeling_wav2vec2.py
==================
4362ee298;Patrick von Platen;2021-08-30 12:02:08 +0200;correct (#13304)

==

tests/test_modeling_rag.py
==================
4046e66e4;Stefan Schweter;2021-08-28 16:22:29 +0200;examples: only use keep_linebreaks when reading TXT files (#13320)
* examples: only use keep_linebreaks when reading TXT files for all CLM examples

* examples: only use keep_linebreaks when reading TXT files for all CLM examples

* examples: only use keep_linebreaks when reading TXT files for all CLM examples
==

examples/flax/language-modeling/run_clm_flax.py
examples/pytorch/language-modeling/run_clm.py
examples/pytorch/language-modeling/run_clm_no_trainer.py
examples/tensorflow/language-modeling/run_clm.py
==================
b6f332eca;Anton Lozhkov;2021-08-27 20:52:51 +0300;Add Wav2Vec2 & Hubert ForSequenceClassification (#13153)
* Add hubert classifier + tests

* Add hubert classifier + tests

* Dummies for all classification tests

* Wav2Vec2 classifier + ER test

* Fix hubert integration tests

* Add hubert IC

* Pass tests for all classification tasks on Hubert

* Pass all tests + copies

* Move models to the SUPERB org
==

docs/source/model_doc/hubert.rst
docs/source/model_doc/wav2vec2.rst
src/transformers/__init__.py
src/transformers/models/hubert/__init__.py
src/transformers/models/hubert/configuration_hubert.py
src/transformers/models/hubert/convert_hubert_original_s3prl_checkpoint_to_pytorch.py
src/transformers/models/hubert/modeling_hubert.py
src/transformers/models/wav2vec2/__init__.py
src/transformers/models/wav2vec2/configuration_wav2vec2.py
src/transformers/models/wav2vec2/convert_wav2vec2_original_s3prl_checkpoint_to_pytorch.py
src/transformers/models/wav2vec2/feature_extraction_wav2vec2.py
src/transformers/models/wav2vec2/modeling_wav2vec2.py
src/transformers/utils/dummy_pt_objects.py
tests/test_modeling_hubert.py
tests/test_modeling_wav2vec2.py
utils/check_repo.py
==================
2bef3433e;Patrick von Platen;2021-08-27 17:38:34 +0200;[Flax] Correct all return tensors to numpy (#13307)
* fix_torch_device_generate_test

* remove @

* finish find and replace
==

examples/research_projects/jax-projects/big_bird/evaluate.py
src/transformers/file_utils.py
src/transformers/generation_flax_utils.py
src/transformers/models/bert/modeling_flax_bert.py
src/transformers/models/big_bird/modeling_flax_big_bird.py
src/transformers/models/electra/modeling_flax_electra.py
src/transformers/models/vit/modeling_flax_vit.py
tests/test_modeling_flax_bart.py
tests/test_modeling_flax_gpt2.py
tests/test_modeling_flax_gpt_neo.py
==================
8aa67fc19;Nicolas Patry;2021-08-27 17:22:06 +0200;Fixing mbart50 with `return_tensors` argument too. (#13301)
* Fixing mbart50 with `return_tensors` argument too.

* Adding mbart50 tokenization tests.
==

src/transformers/models/mbart50/tokenization_mbart50.py
src/transformers/models/mbart50/tokenization_mbart50_fast.py
tests/test_tokenization_mbart50.py
==================
b89a964d3;Nicolas Patry;2021-08-27 15:46:11 +0200;Moving `zero-shot-classification` pipeline to new testing. (#13299)
* Moving `zero-shot-classification` pipeline to new testing.

* Cleaning up old mixins.

* Fixing tests
`sshleifer/tiny-distilbert-base-uncased-finetuned-sst-2-english` is
corrupted in PT.

* Adding warning.
==

src/transformers/pipelines/zero_shot_classification.py
tests/test_pipelines_common.py
tests/test_pipelines_zero_shot.py
==================
cc27ac1a8;NielsRogge;2021-08-27 15:09:57 +0200;Fix BeitForMaskedImageModeling (#13275)
* First pass

* Fix docs of bool_masked_pos

* Add integration script

* Fix docstring

* Add integration test for BeitForMaskedImageModeling

* Remove file

* Fix docs
==

src/transformers/models/beit/modeling_beit.py
tests/test_modeling_beit.py
==================
a3f96f366;Nicolas Patry;2021-08-27 12:26:17 +0200;Moving `translation` pipeline to new testing scheme. (#13297)
* Moving `translation` pipeline to new testing scheme.

* Update tokenization mbart tests.
==

src/transformers/models/mbart/tokenization_mbart.py
src/transformers/models/mbart/tokenization_mbart_fast.py
tests/test_pipelines_translation.py
tests/test_tokenization_mbart.py
==================
319d840b4;Stefan Schweter;2021-08-27 11:35:45 +0200;examples: add keep_linebreaks option to CLM examples (#13150)
* examples: add keep_linebreaks option to text dataset loader for all CLM examples

* examples: introduce new keep_linebreaks option as data argument in CLM examples
==

examples/flax/language-modeling/run_clm_flax.py
examples/pytorch/language-modeling/run_clm.py
examples/pytorch/language-modeling/run_clm_no_trainer.py
examples/tensorflow/language-modeling/run_clm.py
==================
45a8eb66b;Nicolas Patry;2021-08-27 11:24:56 +0200;Moving `token-classification` pipeline to new testing. (#13286)
* Moving `token-classification` pipeline to new testing.

* Fix tests.
==

src/transformers/testing_utils.py
tests/test_pipelines_common.py
tests/test_pipelines_token_classification.py
==================
a6e36558e;Nicolas Patry;2021-08-26 17:30:03 +0200;Moving `text-generation` pipeline to new testing framework. (#13285)
* Moving `text-generation` pipeline to new testing framework.

* Keep check_model_type but log instead of raise Exception.

* warning -> error.
==

src/transformers/pipelines/base.py
src/transformers/pipelines/text_generation.py
tests/test_pipelines_text_generation.py
==================
0759f2510;NielsRogge;2021-08-26 17:25:20 +0200;Add DINO conversion script (#13265)
* First commit

* Add interpolation of patch embeddings

* Comment out code

* Fix bug

* Fix another bug

* Fix bug

* Fix another bug

* Remove print statements

* Update conversion script

* Use the official vit implementation

* Add support for converting dino_vits8

* Add DINO to docs of ViT

* Remove assertion

* Add interpolation of position encodings

* Fix bug

* Add align_corners

* Add interpolate_pos_encoding option to forward pass of ViTModel

* Improve interpolate_pos_encoding method

* Add docstring
==

docs/source/model_doc/vit.rst
src/transformers/models/deit/modeling_deit.py
src/transformers/models/vit/convert_dino_to_pytorch.py
src/transformers/models/vit/modeling_vit.py
==================
14e52783f;Nicolas Patry;2021-08-26 16:26:58 +0200;Moving `text2text-generation` to new pipeline testing mecanism. (#13283)

==
==================
662b143b7;Nicolas Patry;2021-08-26 16:09:53 +0200;Hotfixing master tests. (#13282)

==

tests/test_pipelines_text_classification.py
==================
59c378d06;Nicolas Patry;2021-08-26 16:09:48 +0200;Moving `text2text-generation` to new pipeline testing mecanism. (#13281)

==

tests/test_pipelines_text2text_generation.py
==================
0ebda5382;Nicolas Patry;2021-08-26 15:09:57 +0200;Moving `table-question-answering` pipeline to new testing. (#13280)

==

src/transformers/models/tapas/modeling_tapas.py
tests/test_pipelines_table_question_answering.py
==================
879fe8fa7;Nicolas Patry;2021-08-26 14:47:11 +0200;Moving `summarization` pipeline to new testing format. (#13279)
* Moving `summarization` pipeline to new testing format.

* Remove generate_kwargs from __init__ args.
==

src/transformers/pipelines/text2text_generation.py
tests/test_pipelines_summarization.py
==================
55fb88d36;Nicolas Patry;2021-08-26 12:37:55 +0200;Moving question_answering tests to the new testing scheme. Had to tweak a little some ModelTesterConfig for pipelines. (#13277)
* Moving question_answering tests to the new testing scheme. Had to tweak
a little some ModelTesterConfig for pipelines.

* Removing commented code.
==

src/transformers/pipelines/question_answering.py
tests/test_modeling_bart.py
tests/test_modeling_reformer.py
tests/test_pipelines_question_answering.py
==================
4fa1cd995;Nicolas Patry;2021-08-26 12:13:48 +0200;Fixing the test (warnings was incorrect.) (#13278)

==

src/transformers/pipelines/token_classification.py
==================
6b586ed18;Nicolas Patry;2021-08-26 11:52:49 +0200;Move `image-classification` pipeline to new testing (#13272)
- Enforce `test_small_models_{tf,pt}` methods to exist (enforce checking
actual values in small tests)
- Add support for non RGB image for the pipeline.
==

src/transformers/pipelines/image_classification.py
tests/test_pipelines_common.py
tests/test_pipelines_conversational.py
tests/test_pipelines_feature_extraction.py
tests/test_pipelines_fill_mask.py
tests/test_pipelines_image_classification.py
==================
401377e67;Bram Vanroy;2021-08-26 10:32:57 +0200;Add error message concerning revision (#13266)
* add error message concerning revision

* Update src/transformers/configuration_utils.py

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* re-add double line endings

* is not None instead of implicit bool casting

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
==

src/transformers/configuration_utils.py
src/transformers/modeling_utils.py
src/transformers/tokenization_utils_base.py
==================
40d60e153;Stas Bekman;2021-08-26 01:29:14 -0700;fix `tokenizer_class_from_name` for models with `-` in the name (#13251)
* fix tokenizer_class_from_name

* Update src/transformers/models/auto/tokenization_auto.py

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* add test

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
==

src/transformers/models/auto/tokenization_auto.py
tests/test_tokenization_auto.py
==================
83bfdbdd7;Nicolas Patry;2021-08-26 09:50:43 +0200;Migrating conversational pipeline tests to new testing format (#13114)
* New test format for conversational.

* Putting back old mixin.

* Re-enabling auto tests with LazyLoading.

* Feature extraction tests.

* Remove feature-extraction.

* Feature extraction with feature_extractor (No pun intended).

* Update check_model_type for fill-mask.
==

src/transformers/models/auto/auto_factory.py
src/transformers/pipelines/base.py
src/transformers/pipelines/conversational.py
src/transformers/pipelines/feature_extraction.py
src/transformers/pipelines/fill_mask.py
tests/test_pipelines_common.py
tests/test_pipelines_conversational.py
tests/test_pipelines_feature_extraction.py
tests/test_pipelines_fill_mask.py
tests/test_pipelines_text_classification.py
==================
72eefb34a;Lysandre Debut;2021-08-25 18:56:25 +0200;Add require flax to test (#13260)

==

tests/test_modeling_flax_mt5.py
==================
5af8df5af;Lysandre Debut;2021-08-25 18:56:16 +0200;Some `model_type`s cannot be in the mapping (#13259)
* Some tokenizers cannot be in the mapping

* Style
==

src/transformers/tokenization_utils_base.py
==================
68b690729;Lysandre Debut;2021-08-25 18:56:07 +0200;Add CLIP tokenizer to AutoTokenizer (#13258)

==

src/transformers/models/auto/tokenization_auto.py
==================
3bbe68f83;Lysandre Debut;2021-08-25 18:41:26 +0200;Hubert test fix (#13261)

==

tests/test_modeling_hubert.py
==================
3bb446626;Lysandre Debut;2021-08-25 18:14:44 +0200;Better notification service (#13267)

==

.github/workflows/self-scheduled.yml
utils/notification_service.py
==================
225de5ccb;Nishant Prabhu;2021-08-25 21:44:03 +0530;Replace assert statement with if condition and ValueError (#13263)

==

src/transformers/optimization.py
==================
46554fc12;Lysandre;2021-08-25 11:39:45 +0200;Grad enabled typo

==

src/transformers/onnx/convert.py
==================
0e4f72706;Lysandre Debut;2021-08-25 11:32:51 +0200;Remove side effects of disabling gradient computaiton (#13257)

==

src/transformers/onnx/convert.py
==================
b1198a844;Will Frey;2021-08-24 14:34:05 -0400;Update generation_logits_process.py (#12671)
If you're using type hints, then passing an `int` where a `float` is annotated is acceptable as per [PEP 484](https://www.python.org/dev/peps/pep-0484/#the-numeric-tower).

This makes life a little nicer.
==

src/transformers/generation_logits_process.py
==================
0245cee46;dependabot[bot];2021-08-24 09:52:39 -0400;Bump notebook from 6.1.5 to 6.4.1 in /examples/research_projects/lxmert (#13226)
Bumps [notebook](http://jupyter.org) from 6.1.5 to 6.4.1.

---
updated-dependencies:
- dependency-name: notebook
  dependency-type: direct:production
...

Signed-off-by: dependabot[bot] <support@github.com>

Co-authored-by: dependabot[bot] <49699333+dependabot[bot]@users.noreply.github.com>
==

examples/research_projects/lxmert/requirements.txt
==================
0512bfe79;Ambesh Shekhar;2021-08-24 18:31:01 +0530;Custom errors and BatchSizeError (#13184)
* Adding custom errors and BatchSizeError for GPT2

* Adding custom errors and BatchSizeError for GPT2

* Changing Exception to BaseException

* Exception

* Adding args to Custom Exception

* Adding args to Custom Exception

* Changing from BaseException to Exception

* Changing Conditional loop syntax

* Adding Copyright info

* Handling check_code_quality

* Handling check_code_quality pt2

* Handling check_code_quality pt3

* Handling check_code_quality pt4

* Handling check_code_quality pt5

* Handling check_code_quality pt6

* Handling check_code_quality pt6

* Using black for check_code_quality

* sorting import style

* Changing

* Changing

* verified through style_doc.py

* verified through style_doc.py

* applying isort

* Removing indentation

* Changing

* Changing

* Changing

* Used ValueError

* Using ValueError

* Reformatted Style doc

* Using style doc on modeling_gp2.py

* Adding indentation

* Changing
==

src/transformers/models/gpt2/modeling_gpt2.py
==================
cf5744764;Ori Ram;2021-08-24 14:55:21 +0300;Fix broken links in Splinter documentation (#13237)

==

README.md
docs/source/index.rst
==================
5c6eca71a;Stas Bekman;2021-08-24 02:43:41 -0700;fix `AutoModel.from_pretrained(..., torch_dtype=...)` (#13209)
* fix AutoModel.from_pretrained(..., torch_dtype=...)

* fix to_diff_dict

* add better test

* torch is not always available when a model has self.torch_dtype
==

src/transformers/configuration_utils.py
tests/test_modeling_common.py
==================
39db2f3c1;Bram Vanroy;2021-08-24 09:05:33 +0200;Allow local_files_only for fast pretrained tokenizers (#13225)
* allow local_files_only for fast pretrained tokenizers

* make style
==

src/transformers/file_utils.py
src/transformers/tokenization_utils_base.py
==================
2772d3e79;Lysandre Debut;2021-08-23 19:16:48 +0200;Add RemBert to AutoTokenizer (#13224)

==

src/transformers/models/auto/tokenization_auto.py
==================
f1bb6f083;Allan Lin;2021-08-24 00:08:33 +0800;Fix load tf alias in Albert. (#13159)

==

src/transformers/models/albert/modeling_albert.py
==================
0b54046ff;Kamal Raj;2021-08-23 21:37:41 +0530;remove unwanted code (#13145)

==

src/transformers/models/deberta_v2/modeling_deberta_v2.py
==================
2e20c0f34;Yih-Dar;2021-08-23 17:57:29 +0200;Make Flax GPT2 working with cross attention (#13008)
* make flax gpt2 working with cross attention

* Remove encoder->decoder projection layer

* A draft (incomplete) for FlaxEncoderDecoderModel

* Add the method from_encoder_decoder_pretrained + the docstrings

* Fix the mistakes of using EncoderDecoderModel

* Fix style

* Add FlaxEncoderDecoderModel to the library

* Fix cyclic imports

* Add FlaxEncoderDecoderModel to modeling_flax_auto.py

* Remove question comments

* add tests for FlaxEncoderDecoderModel

* add flax_encoder_decoder to the lists of ignored entries in check_repo.py

* fix missing required positional arguments

* Remove **kwargs when creating FlaxEncoderDecoderModel in from_encoder_decoder_pretrained()

Also fix generation eos/pad tokens issue

* Fix: Use sequences from the generated_output

* Change a check from assert to raise ValueError

* Fix examples and token ids issues

* Fix missing all_cross_attentions when outputting tuple in modeling_gpt2

* Remove the changes in configuration docstrings.

* allow for bert 2 gpt2

* make fix-copies

* Apply suggestions from code review

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* Change remaining examples to bert2gpt2

* Change the test to Bert2GPT2

* Fix examples

* Fix import

* Fix unpack bug

* Rename to FlaxEncoderDecoderModelTest and change the test to bert2gpt2

* Apply suggestions from code review

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* Fix: NotImplentedError -> NotImplementedError

* Apply suggestions from code review

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* up

* finalize

Co-authored-by: ydshieh <ydshieh@user.noreply>
Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>
Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
==

docs/source/index.rst
docs/source/model_doc/encoderdecoder.rst
src/transformers/__init__.py
src/transformers/models/auto/modeling_flax_auto.py
src/transformers/models/bert/modeling_flax_bert.py
src/transformers/models/encoder_decoder/__init__.py
src/transformers/models/encoder_decoder/modeling_flax_encoder_decoder.py
src/transformers/models/gpt2/modeling_flax_gpt2.py
src/transformers/models/gpt2/modeling_gpt2.py
src/transformers/models/roberta/modeling_flax_roberta.py
src/transformers/utils/dummy_flax_objects.py
tests/test_modeling_encoder_decoder.py
tests/test_modeling_flax_encoder_decoder.py
tests/test_modeling_flax_gpt2.py
utils/check_repo.py
==================
7223844df;SaulLu;2021-08-23 14:35:18 +0200;Change how "additional_special_tokens" argument in the ".from_pretrained" method of the tokenizer is taken into account (#13056)
* add test

* add change in PretrainedTokenizerBase

* change Luke

* deactivate

* add the possibility to add additional special tokens for M2M100

* format

* add special test for canine

* proposed changes for mbart

* proposed changes for mbart50

* proposed changes for byt5

* proposed changes for canine

* proposed changes for t5

* test fast and slow

* remove comment

* remove comment

* add fast version for all tests

* replace break by continue

* add more comments

* add check to avoid duplicates

* remove comment

* format

* proposed change for wave2vec2

* reverse changes mbart

* uncomment

* format
==

src/transformers/models/luke/tokenization_luke.py
src/transformers/models/m2m_100/tokenization_m2m_100.py
src/transformers/models/mbart50/tokenization_mbart50.py
src/transformers/models/mbart50/tokenization_mbart50_fast.py
src/transformers/tokenization_utils_base.py
tests/test_processor_wav2vec2.py
tests/test_tokenization_byt5.py
tests/test_tokenization_canine.py
tests/test_tokenization_common.py
tests/test_tokenization_t5.py
==================
b13c6c18d;sourabh112;2021-08-23 16:57:24 +0530;correcting group beam search function output score bug (#13211)

==

src/transformers/generation_utils.py
==================
f689743e7;Philipp Schmid;2021-08-23 10:18:07 +0200;SageMaker: Fix sagemaker DDP & metric logs (#13181)
* Barrier -> barrier

* added logger for metrics

* removed stream handler in trainer

* moved handler

* removed streamhandler from trainer

* updated test image and instance type added datasets version to test

* Update tests/sagemaker/scripts/pytorch/requirements.txt

Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>

Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>
==

src/transformers/trainer.py
src/transformers/trainer_pt_utils.py
src/transformers/training_args.py
tests/sagemaker/conftest.py
tests/sagemaker/scripts/pytorch/requirements.txt
tests/sagemaker/test_multi_node_data_parallel.py
==================
8679bd714;NielsRogge;2021-08-23 09:44:42 +0200;Add min and max question length options to TapasTokenizer (#12803)
* Add min and max question length option to the tokenizer

* Add corresponding test
==

src/transformers/models/tapas/tokenization_tapas.py
tests/test_tokenization_tapas.py
==================
588e6caa1;NielsRogge;2021-08-23 09:41:35 +0200;Overwrite get_clean_sequence as this was causing a bottleneck (#13183)

==

tests/test_tokenization_luke.py
==================
143738214;StevenTang1998;2021-08-20 17:01:54 +0800;Fix the loss calculation of ProphetNet (#13132)
* Fix the loss calculation of ProphetNet

* Fix the loss calculation of ProphetNet

Fix the loss calculation of ProphetNet and remove warning
==

src/transformers/models/prophetnet/modeling_prophetnet.py
==================
91ff480e2;Allan Lin;2021-08-19 20:29:51 +0800;Update namespaces inside torch.utils.data to the latest. (#13167)
* Update torch.utils.data namespaces to the latest.

* Format

* Update Dataloader.

* Style
==

examples/legacy/multiple_choice/utils_multiple_choice.py
examples/legacy/seq2seq/seq2seq_trainer.py
examples/legacy/token-classification/utils_ner.py
examples/pytorch/language-modeling/run_clm_no_trainer.py
examples/pytorch/language-modeling/run_mlm_no_trainer.py
examples/pytorch/multiple-choice/run_swag_no_trainer.py
examples/pytorch/question-answering/run_qa_beam_search_no_trainer.py
examples/pytorch/question-answering/run_qa_no_trainer.py
examples/pytorch/summarization/run_summarization_no_trainer.py
examples/pytorch/text-classification/run_glue_no_trainer.py
examples/pytorch/token-classification/run_ner_no_trainer.py
examples/pytorch/translation/run_translation_no_trainer.py
examples/research_projects/adversarial/utils_hans.py
examples/research_projects/distillation/grouped_batch_sampler.py
src/transformers/data/datasets/glue.py
src/transformers/data/datasets/language_modeling.py
src/transformers/data/datasets/squad.py
src/transformers/trainer.py
src/transformers/trainer_callback.py
src/transformers/trainer_pt_utils.py
src/transformers/trainer_seq2seq.py
templates/adding_a_new_example_script/{{cookiecutter.directory_name}}/run_{{cookiecutter.example_shortcut}}.py
tests/test_trainer_distributed.py
tests/test_trainer_tpu.py
==================
1fec32adc;Jannis Vamvas;2021-08-18 16:51:54 +0200;Fix generation docstrings regarding input_ids=None (#12823)

==

src/transformers/generation_flax_utils.py
src/transformers/generation_tf_utils.py
src/transformers/generation_utils.py
==================
ecfa7eb26;Patrick von Platen;2021-08-18 16:18:13 +0200;[AutoFeatureExtractor] Fix loading of local folders if config.json exists (#13166)
* up

* up
==

src/transformers/models/auto/configuration_auto.py
src/transformers/models/auto/feature_extraction_auto.py
tests/test_feature_extraction_auto.py
==================
439a43b6b;Ori Ram;2021-08-17 15:29:01 +0300;Add splinter (#12955)
* splinter template

* initialize splinter classes

* Splinter Tokenizer

* splinter.rst

* tokenization fixes

* Documentation & some minor variable name changes

* bug fix (added back question_token_id to config) + variable names

* Minor bug fixes + variable name changes

* Fix Splinter references after merge with new transformers

* changes after running make style & quality

* Fix documentation unindent

* Fix doc indentation in tokenization_splinter

* Fix also SplinterTokenizerFast

* Add Splinter to index.rst and README

* Fixdouble whitespace from index.rst

* Fixed index.rst with 'make fix-copies'

* Update docs/source/model_doc/splinter.rst

Co-authored-by: Suraj Patil <surajp815@gmail.com>

* Update docs/source/model_doc/splinter.rst

Co-authored-by: Suraj Patil <surajp815@gmail.com>

* Update docs/source/model_doc/splinter.rst

Co-authored-by: Suraj Patil <surajp815@gmail.com>

* Update docs/source/model_doc/splinter.rst

Co-authored-by: Suraj Patil <surajp815@gmail.com>

* Update src/transformers/models/splinter/__init__.py

Co-authored-by: Suraj Patil <surajp815@gmail.com>

* Added "copied from BERT" comments

* Removing unnexessary code from modeling_splinter

* Update README.md

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/models/splinter/configuration_splinter.py

Co-authored-by: Suraj Patil <surajp815@gmail.com>

* Remove references to TF modeling from splinter

* Update src/transformers/models/splinter/modeling_splinter.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Remove unnecessary check

* Update src/transformers/models/splinter/modeling_splinter.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Add differences between Splinter and Bert tokenizers

* Update src/transformers/models/splinter/modeling_splinter.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/models/splinter/tokenization_splinter_fast.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Remove unnecessary check

* Doc formatting

* Update src/transformers/models/splinter/tokenization_splinter.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/models/splinter/tokenization_splinter.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* bug fix: remove load_tf_weights attribute

* Some minor quality changes

* Update docs/source/model_doc/splinter.rst

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* Update src/transformers/models/splinter/configuration_splinter.py

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* Change FullyConnectedLayer to SplinterFullyConnectedLayer

* Variable naming

* Reove gather_positions function

* Remove ClassificationHead as it's outdated

* Update src/transformers/models/splinter/modeling_splinter.py

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* Remove hardcoded 102 token id

* Minor style change

* Added "tau" organization to all model identifiers & URLS

* Added tau to the tests as well

* Copy-from comments

* Removed all unnecessary classes (e.g. SplinterForMaskedLM)

* Running make fix-copies

* Bug fix: Further removed unnecessary classes

* Add Splinter to AutoTokenization

* Add an integration test for Splinter

* Removed initialize_new_qass from config - It will be done through different checkpoints

* Removed `initialize_new_qass` from documentation as well

* Added new checkpoint names (`tau/splinter-base-qass` and same for large) in the code

* Minor change to test

* SplinterTokenizer now doesn't abstract from BertTokenizer

* SplinterTokenizerFast also dosn't abstract from Bert

* style and quality

* bug fix: import ing torch in tests only if it's available

* Auto mappings

* Changed copyrights in Splinter's files

* Update src/transformers/models/splinter/configuration_splinter.py

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

Co-authored-by: yuvalkirstain <kirstain.yuval@gmail.com>
Co-authored-by: Suraj Patil <surajp815@gmail.com>
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
Co-authored-by: Lysandre <lysandre.debut@reseau.eseo.fr>
Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
==

README.md
docs/source/index.rst
docs/source/model_doc/splinter.rst
src/transformers/__init__.py
src/transformers/convert_slow_tokenizer.py
src/transformers/models/__init__.py
src/transformers/models/auto/configuration_auto.py
src/transformers/models/auto/modeling_auto.py
src/transformers/models/auto/tokenization_auto.py
src/transformers/models/splinter/__init__.py
src/transformers/models/splinter/configuration_splinter.py
src/transformers/models/splinter/modeling_splinter.py
src/transformers/models/splinter/tokenization_splinter.py
src/transformers/models/splinter/tokenization_splinter_fast.py
src/transformers/utils/dummy_pt_objects.py
src/transformers/utils/dummy_tokenizers_objects.py
tests/test_modeling_splinter.py
==================
6626d8a62;Nicolas Patry;2021-08-17 10:11:58 +0200;Optimizes ByT5 tokenizer (#13119)
* Starting to optimize ByT5.

* Making ByT5Tokenizer faster.

* Even faster.

* Cleaning up.
==

src/transformers/models/byt5/tokenization_byt5.py
==================
14e9d2954;sararb;2021-08-16 12:36:08 -0400;compute seq_len from inputs_embeds (#13128)

==

src/transformers/models/electra/modeling_electra.py
==================
e2f07c01e;Lysandre Debut;2021-08-16 17:40:38 +0200;Ci continue through smi failure (#13140)
* Continue on error

* Specific

* Temporary patch
==

.github/workflows/self-push.yml
.github/workflows/self-scheduled.yml
==================
73caccde3;Patrick von Platen;2021-08-16 16:02:34 +0200;fix bug (#13051)

==

src/transformers/feature_extraction_sequence_utils.py
==================
c066598c2;Omar Sanseviero;2021-08-16 15:45:19 +0200;Fix frameworks table so it's alphabetical (#13118)
* Fix frameworks table so it's alphabetical

* Update index.rst

* Don't differentiate when sorting between upper and lower case
==

docs/source/index.rst
utils/check_table.py
==================
62ba3b6b4;Lysandre;2021-08-16 10:52:28 +0200;Depend on hidden_dropout_prob

==

src/transformers/models/bert/modeling_bert.py
src/transformers/models/mobilebert/modeling_mobilebert.py
==================
3c6d73bc5;Lysandre;2021-08-16 10:43:59 +0200;Fix BERT/MobileBERT classifier dropout

==

src/transformers/models/bert/modeling_bert.py
src/transformers/models/mobilebert/modeling_mobilebert.py
==================
7d2feb3a3;weierstrass_walker;2021-08-16 14:17:37 +0600;Update modeling_bert.py (#13129)

==

src/transformers/models/bert/modeling_bert.py
==================
a13c8145b;Omar Sanseviero;2021-08-13 17:38:02 +0200;Fix docstring of train_new_from_iterator

==

src/transformers/tokenization_utils_fast.py
==================
86a154722;Minwoo Lee;2021-08-13 19:24:53 +0900;Fix omitted lazy import for xlm-prophetnet (#13052)
* Fix omitted lazy import for xlm-prophetnet

* Update src/transformers/models/xlm_prophetnet/__init__.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Fix style using black

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

src/transformers/models/xlm_prophetnet/__init__.py
==================
d58926ab1;Nicolas Patry;2021-08-13 12:04:18 +0200;Moving fill-mask pipeline to new testing scheme (#12943)
* Fill mask pipelines test updates.

* Model eval !!

* Adding slow test with actual values.

* Making all tests pass (skipping quite a bit.)

* Doc styling.

* Better doc cleanup.

* Making an explicit test with no pad token tokenizer.

* Typo.
==

src/transformers/pipelines/base.py
src/transformers/pipelines/fill_mask.py
tests/test_modeling_reformer.py
tests/test_pipelines_common.py
tests/test_pipelines_fill_mask.py
==================
a04d4bf2d;Yih-Dar;2021-08-13 10:45:53 +0200;Fix flax gpt2 hidden states (#13109)
* Fix inconsistency of the last element in hidden_states between PyTorch/Flax GPT2(Neo) (#13102)

* Fix missing elements in outputs tuple

* Apply suggestions from code review

Co-authored-by: Suraj Patil <surajp815@gmail.com>

* Fix local variable 'all_hidden_states' referenced before assignment

* Fix by returning tuple containing None values

* Fix quality

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>
Co-authored-by: Suraj Patil <surajp815@gmail.com>
==

src/transformers/models/gpt2/modeling_flax_gpt2.py
src/transformers/models/gpt_neo/modeling_flax_gpt_neo.py
==================
d8fb278a2;Will Frey;2021-08-13 04:12:59 -0400;Create py.typed (#12893)
* Create py.typed

This creates a [py.typed as per PEP 561](https://www.python.org/dev/peps/pep-0561/#packaging-type-information) that should be distributed to mark that the package includes (inline) type annotations.

* Update setup.py

Include py.typed as package data

* Update setup.py

Call `setup(...)` with `zip_safe=False`.
==

setup.py
src/transformers/py.typed
==================
b0a917c48;Sylvain Gugger;2021-08-13 08:57:30 +0200;Fix CircleCI nightly tests (#13113)

==

.circleci/config.yml
==================
bda1cb023;Gunjan Chhablani;2021-08-13 11:44:04 +0530;Fix VisualBERT docs (#13106)
* Fix VisualBERT docs

* Show example notebooks as lists

* Fix style
==

docs/source/model_doc/visual_bert.rst
src/transformers/models/visual_bert/modeling_visual_bert.py
==================
e46ad22cd;Bill Schnurr;2021-08-12 09:45:54 -0700;Improve type checker performance (#13094)
* conditional declare `TOKENIZER_MAPPING_NAMES` within a `if TYPE_CHECKING` block so that type checkers dont need to evaluate the RHS of the assignment.

this improves performance of the pylance/pyright type checkers

* Update src/transformers/models/auto/tokenization_auto.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* adding missing import

* format

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

src/transformers/models/auto/tokenization_auto.py
==================
b9962b865;Sylvain Gugger;2021-08-12 16:45:06 +0200;Ci last fix (#13103)
* Only report failures on failures

* Fix typo

* Put it everywhere
==

.github/workflows/self-push.yml
==================
f5cd27694;Suraj Patil;2021-08-12 18:35:01 +0530;[FlaxCLIP] allow passing params to image and text feature methods (#13099)
* allow passing params to image and text feature method

* ifx for hybrid clip as well
==

examples/research_projects/jax-projects/hybrid_clip/modeling_hybrid_clip.py
src/transformers/models/clip/modeling_flax_clip.py
==================
9a498c37a;Sylvain Gugger;2021-08-12 14:59:02 +0200;Rely on huggingface_hub for common tools (#13100)
* Remove hf_api module and use hugginface_hub

* Style

* Fix to test_fetcher

* Quality
==

docs/source/model_doc/marian.rst
examples/research_projects/seq2seq-distillation/_test_seq2seq_examples.py
src/transformers/commands/user.py
src/transformers/hf_api.py
src/transformers/models/marian/convert_marian_to_pytorch.py
tests/test_hf_api.py
tests/test_modeling_marian.py
utils/tests_fetcher.py
==================
6900dded4;Patrick von Platen;2021-08-12 14:49:46 +0200;[Flax/JAX] Run jitted tests at every commit (#13090)
* up

* up

* up
==

tests/test_modeling_flax_big_bird.py
tests/test_modeling_flax_clip.py
tests/test_modeling_flax_common.py
tests/test_modeling_flax_vit.py
tests/test_modeling_flax_wav2vec2.py
==================
773d38604;Yih-Dar;2021-08-12 14:19:48 +0200;Change a parameter name in FlaxBartForConditionalGeneration.decode() (#13074)
* Change FlaxBartForConditionalGeneration.decode() argument: deterministic -> train

* Also change the parameter name to train for flax marian and mbart

Co-authored-by: ydshieh <ydshieh@users.noreply.github.com>
==

src/transformers/models/bart/modeling_flax_bart.py
src/transformers/models/marian/modeling_flax_marian.py
src/transformers/models/mbart/modeling_flax_mbart.py
==================
f176fbf58;Sylvain Gugger;2021-08-12 05:49:02 -0400;Fix doc building error

==

docs/source/internal/modeling_utils.rst
==================
be323d515;Sylvain Gugger;2021-08-12 11:38:14 +0200;Reactive test fecthers on scheduled test with proper git install (#13097)
* Reactive test fecthers on scheduled test with proper git install

* Proper fetch-depth
==

.github/workflows/self-push.yml
utils/tests_fetcher.py
==================
ea8ffe36d;Sylvain Gugger;2021-08-12 11:23:00 +0200;Proper import for unittest.mock.patch (#13085)

==

tests/test_trainer_callback.py
==================
d329b6336;Kamal Raj;2021-08-12 14:31:26 +0530;Deberta tf (#12972)
* TFDeberta

moved weights to build and fixed name scope

added missing ,

bug fixes to enable graph mode execution

updated setup.py

fixing typo

fix imports

embedding mask fix

added layer names avoid autmatic incremental names

+XSoftmax

cleanup

added names to layer

disable keras_serializable
Distangled attention output shape hidden_size==None
using symbolic inputs

test for Deberta tf

make style

Update src/transformers/models/deberta/modeling_tf_deberta.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

Update src/transformers/models/deberta/modeling_tf_deberta.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

Update src/transformers/models/deberta/modeling_tf_deberta.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

Update src/transformers/models/deberta/modeling_tf_deberta.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

Update src/transformers/models/deberta/modeling_tf_deberta.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

Update src/transformers/models/deberta/modeling_tf_deberta.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

Update src/transformers/models/deberta/modeling_tf_deberta.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

removed tensorflow-probability

removed blank line

* removed tf experimental api
+torch_gather tf implementation from @Rocketknight1

* layername DeBERTa --> deberta

* copyright fix

* added docs for TFDeberta & make style

* layer_name change to fix load from pt model

* layer_name change as pt model

* SequenceClassification layername change,
to same as pt model

* switched to keras built-in LayerNormalization

* added `TFDeberta` prefix most layer classes

* updated to tf.Tensor in the docstring
==

docs/source/index.rst
docs/source/model_doc/deberta.rst
src/transformers/__init__.py
src/transformers/models/auto/modeling_tf_auto.py
src/transformers/models/deberta/__init__.py
src/transformers/models/deberta/modeling_tf_deberta.py
src/transformers/utils/dummy_tf_objects.py
tests/test_modeling_tf_deberta.py
==================
c4e1586db;Gunjan Chhablani;2021-08-12 13:27:34 +0530;Fix VisualBert Embeddings (#13017)

==

src/transformers/models/visual_bert/modeling_visual_bert.py
==================
53b38d626;Lysandre Debut;2021-08-12 09:42:25 +0200;Doctests job (#13088)
* Doctests

* Limit to 4 decimals

* Try with separate PT/TF tests

* Remove test for TF

* Ellips the predictions

* Doctest continue on failure

Co-authored-by: Sylvain Gugger <sylvain.gugger@gmail.com>
==

.github/workflows/doctests.yml
docs/source/quicktour.rst
docs/source/task_summary.rst
utils/documentation_tests.txt
==================
3f52c685c;Ibraheem Moosa;2021-08-12 13:37:31 +0600;Fix classifier dropout in AlbertForMultipleChoice (#13087)
Classification head of AlbertForMultipleChoice uses `hidden_dropout_prob` instead of `classifier_dropout_prob`.  This
is not desirable as we cannot change classifer head dropout probability without changing the dropout probabilities of
the whole model.
==

src/transformers/models/albert/modeling_albert.py
==================
c89180a9d;Lysandre Debut;2021-08-11 18:09:41 +0200;Install git (#13091)
* Install git

* Add TF tests

* And last TF test

* Add in commented code too

Co-authored-by: Sylvain Gugger <sylvain.gugger@gmail.com>
==

.github/workflows/self-push.yml
.github/workflows/self-scheduled.yml
==================
c71f73f43;Gunjan Chhablani;2021-08-11 19:40:59 +0530;Add VisualBERT demo notebook (#12263)
* Initialize VisualBERT demo

* Update demo

* Add commented URL

* Update README

* Update README
==

examples/research_projects/visual_bert/README.md
examples/research_projects/visual_bert/demo.ipynb
examples/research_projects/visual_bert/extracting_data.py
examples/research_projects/visual_bert/modeling_frcnn.py
examples/research_projects/visual_bert/processing_image.py
examples/research_projects/visual_bert/requirements.txt
examples/research_projects/visual_bert/utils.py
examples/research_projects/visual_bert/visualizing_image.py
==================
83424ade1;Sylvain Gugger;2021-08-11 13:45:25 +0200;[Doctest] Setup, quicktour and task_summary (#13078)
* Fix doctests for quicktour

* Adapt causal LM exemple

* Remove space

* Fix until summarization

* End of task summary

* Style

* With last changes in quicktour
==

docs/source/quicktour.rst
docs/source/task_summary.rst
setup.cfg
==================
bfc885091;Sylvain Gugger;2021-08-10 13:48:26 -0400;Fix last one

==

.github/workflows/self-push.yml
==================
29dada00c;Ibraheem Moosa;2021-08-10 22:39:48 +0600;Use original key for label in DataCollatorForTokenClassification (#13057)
* Use original key for label in DataCollatorForTokenClassification

DataCollatorForTokenClassification accepts either `label` or `labels` as key for label in it's input. However after padding the label it assigns the padded labels to key `labels`. If originally `label` was used as key than the original upadded labels still remains in the batch. Then at line 192 when we try to convert the batch elements to torch tensor than these original unpadded labels cannot be converted as the labels for different samples have different lengths.

* Fixed style.
==

src/transformers/data/data_collator.py
==================
95e2e14f9;Sylvain Gugger;2021-08-10 18:37:01 +0200;Revert to all tests whil we debug what's wrong (#13072)

==

.github/workflows/self-push.yml
==================
477480ce2;Sylvain Gugger;2021-08-10 10:26:06 -0400;Trigger GPU tests

==

.github/workflows/self-push.yml
==================
0dad5d825;Sylvain Gugger;2021-08-10 16:17:06 +0200;Fix fallback of test_fetcher (#13071)

==

utils/tests_fetcher.py
==================
4dd857244;Sylvain Gugger;2021-08-10 09:40:38 -0400;Merge branch 'master' of github.com:huggingface/transformers

==
==================
bd5593b6c;Sylvain Gugger;2021-08-10 09:40:16 -0400;Try fecthing the last two commits

==

.github/workflows/self-push.yml
==================
9e9b8f1d9;Sylvain Gugger;2021-08-10 14:54:52 +0200;Roll out the test fetcher on push tests (#13055)
* Use test fetcher for push tests as well

* Force diff with last commit for circleCI on master

* Fix syntax error

* Style

* Schedule nightly tests
==

.circleci/config.yml
.github/workflows/self-push.yml
utils/tests_fetcher.py
==================
2e0d767ab;Sylvain Gugger;2021-08-10 06:27:49 -0400;Pin sacrebleu

==

setup.py
src/transformers/dependency_versions_table.py
==================
0454e4bd8;Sylvain Gugger;2021-08-10 12:20:04 +0200;Fix ModelOutput instantiation form dictionaries (#13067)
* Fix ModelOutput instantiation form dictionaries

* Style
==

src/transformers/file_utils.py
tests/test_model_output.py
==================
3157fa3c5;Aleksey Korshuk;2021-08-10 10:36:44 +0300;docs: add HuggingArtists to community notebooks (#13050)
* Adding HuggingArtists to Community Notebooks

* Adding HuggingArtists to Community Notebooks

* Adding HuggingArtists to Community Notebooks

* docs: add HuggingArtists to community notebooks

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

docs/source/community.md
==================
ab7551cd7;Kevin Canwen Xu;2021-08-10 15:29:35 +0800;Add try-except for torch_scatter (#13040)
* Add try-catch for torch_scatter

* Update modeling_tapas.py
==

src/transformers/models/tapas/modeling_tapas.py
==================
76cadb794;SaulLu;2021-08-09 19:17:05 +0200;replace tgt_lang by tgt_text (#13061)

==

docs/source/model_doc/m2m_100.rst
==================
a8bf2fa76;Lysandre;2021-08-09 16:14:17 +0200;Documentation for patch v4.9.2

==

.circleci/deploy.sh
docs/source/_static/js/custom.js
==================
5008e0888;Lysandre Debut;2021-08-09 15:51:49 +0200;Add to ONNX docs (#13048)
* Add to ONNX docs

* Add MBART example

* Update docs/source/serialization.rst

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

docs/source/serialization.rst
==================
6f5ab9daf;Lysandre Debut;2021-08-09 14:56:04 +0200;Add MBART to models exportable with ONNX (#13049)
* Add MBART to models exportable with ONNX

* unittest mock

* Add tests

* Misc fixes
==

src/transformers/models/mbart/__init__.py
src/transformers/models/mbart/configuration_mbart.py
src/transformers/onnx/features.py
src/transformers/testing_utils.py
tests/test_onnx_v2.py
==================
13a9c9a35;Patrick von Platen;2021-08-09 13:37:50 +0200;[Flax] Refactor gpt2 & bert example docs (#13024)
* fix_torch_device_generate_test

* remove @

* improve docs for clm

* speed-ups

* correct t5 example as well

* push final touches

* Update examples/flax/language-modeling/README.md

* correct docs for mlm

* Update examples/flax/language-modeling/README.md

Co-authored-by: Patrick von Platen <patrick@huggingface.co>
==

examples/flax/language-modeling/README.md
examples/flax/language-modeling/run_clm_flax.py
examples/flax/language-modeling/run_mlm_flax.py
==================
3ff2cde5c;abhishek thakur;2021-08-09 08:11:17 +0200;tfhub.de -> tfhub.dev (#12565)

==

examples/flax/language-modeling/README.md
==================
24cbf6bc5;Patrick von Platen;2021-08-08 17:11:19 +0200;Update README.md

==

examples/research_projects/wav2vec2/README.md
==================
7390d9de6;lewtun;2021-08-08 09:06:05 -0500;Use min version for huggingface-hub dependency (#12961)
* Use min version for huggingface-hub dependency

* Update dependency version table
==

setup.py
src/transformers/dependency_versions_table.py
==================
7fcee113c;Sylvain Gugger;2021-08-06 20:41:39 +0200;Tpu tie weights (#13030)
* Fix tied weights on TPU

* Manually tie weights in no trainer examples

* Fix for test

* One last missing

* Gettning owned by my scripts

* Address review comments

* Fix test

* Fix tests

* Fix reformer tests
==

examples/pytorch/language-modeling/run_clm_no_trainer.py
examples/pytorch/language-modeling/run_mlm_no_trainer.py
src/transformers/modeling_utils.py
src/transformers/models/albert/modeling_albert.py
src/transformers/models/bert_generation/modeling_bert_generation.py
src/transformers/models/ibert/modeling_ibert.py
src/transformers/models/longformer/modeling_longformer.py
src/transformers/models/reformer/modeling_reformer.py
src/transformers/models/roberta/modeling_roberta.py
src/transformers/trainer.py
==================
1bf38611a;Lysandre Debut;2021-08-06 18:41:33 +0200;Put smaller ALBERT model (#13028)

==

tests/test_onnx_v2.py
==================
dc420b0eb;Michael Benayoun;2021-08-06 15:46:26 +0200;T5 with past ONNX export (#13014)
T5 with past ONNX export, and more explicit past_key_values inputs and outputs names for ONNX model

Authored-by: Michael Benayoun <michael@huggingface.co>
==

src/transformers/models/gpt_neo/configuration_gpt_neo.py
src/transformers/models/t5/configuration_t5.py
src/transformers/models/t5/modeling_t5.py
src/transformers/onnx/config.py
src/transformers/onnx/convert.py
src/transformers/onnx/features.py
src/transformers/onnx/utils.py
tests/test_onnx_v2.py
==================
ee1122461;Michael Benayoun;2021-08-06 15:37:29 +0200;FX submodule naming fix (#13016)
Changed the way dynamically inserted submodules are named and the method used to insert them

Authored-by: Michael Benayoun <michael@huggingface.co>
==

src/transformers/utils/fx.py
==================
9870093f7;Sylvain Gugger;2021-08-06 13:12:30 +0200;[WIP] Disentangle auto modules from other modeling files (#13023)
* Initial work

* All auto models

* All tf auto models

* All flax auto models

* Tokenizers

* Add feature extractors

* Fix typos

* Fix other typo

* Use the right config

* Remove old mapping names and update logic in AutoTokenizer

* Update check_table

* Fix copies and check_repo script

* Fix last test

* Add back name

* clean up

* Update template

* Update template

* Forgot a )

* Use alternative to fixup

* Fix TF model template

* Address review comments

* Address review comments

* Style
==

.github/workflows/model-templates.yml
Makefile
src/transformers/__init__.py
src/transformers/modelcard.py
src/transformers/models/__init__.py
src/transformers/models/auto/auto_factory.py
src/transformers/models/auto/configuration_auto.py
src/transformers/models/auto/feature_extraction_auto.py
src/transformers/models/auto/modeling_auto.py
src/transformers/models/auto/modeling_flax_auto.py
src/transformers/models/auto/modeling_tf_auto.py
src/transformers/models/auto/tokenization_auto.py
src/transformers/models/mbart/__init__.py
src/transformers/models/mbart50/__init__.py
src/transformers/models/mbart50/tokenization_mbart50.py
src/transformers/models/mbart50/tokenization_mbart50_fast.py
src/transformers/tokenization_utils_base.py
src/transformers/trainer.py
src/transformers/utils/dummy_tokenizers_objects.py
src/transformers/utils/modeling_auto_mapping.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_tf_{{cookiecutter.lowercase_modelname}}.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/to_replace_{{cookiecutter.lowercase_modelname}}.py
tests/test_pipelines_translation.py
utils/check_repo.py
utils/check_table.py
utils/class_mapping_update.py
==================
2e4082364;Patrick von Platen;2021-08-06 11:21:37 +0200;[Flax T5] Speed up t5 training (#13012)
* fix_torch_device_generate_test

* remove @

* update

* up

* fix

* remove f-stings

* correct readme

* up

Co-authored-by: Patrick von Platen <patrick@huggingface.co>
==

examples/flax/language-modeling/README.md
examples/flax/language-modeling/run_t5_mlm_flax.py
==================
60e448c87;Patrick von Platen;2021-08-05 18:38:50 +0200;[Flax] Correct pt to flax conversion if from base to head (#13006)
* finish PR

* add tests

* correct tests

* finish

* correct other flax tests

* better naming

* correct naming

* finish

* apply sylvains suggestions
==

src/transformers/modeling_flax_pytorch_utils.py
tests/test_modeling_flax_clip.py
tests/test_modeling_flax_common.py
tests/test_modeling_flax_t5.py
==================
33929448a;Nils Reimers;2021-08-05 15:55:14 +0200;Replace // operator with / operator + long() (#13013)

==

src/transformers/generation_utils.py
==================
a6d62aaba;Michael Benayoun;2021-08-05 10:12:13 +0200;GPT-Neo ONNX export (#12911)
GPT-Neo ONNX export and task / feature refactoring

Authored-by: Michael Benayoun <michael@huggingface.co>
==

src/transformers/models/gpt_neo/__init__.py
src/transformers/models/gpt_neo/configuration_gpt_neo.py
src/transformers/models/gpt_neo/modeling_gpt_neo.py
src/transformers/onnx/__init__.py
src/transformers/onnx/__main__.py
src/transformers/onnx/config.py
src/transformers/onnx/convert.py
src/transformers/onnx/features.py
tests/test_onnx_v2.py
==================
8aa01d2a6;Sasha Luccioni;2021-08-05 02:56:13 -0400;Create perplexity.rst (#13004)
Updating the import for load_dataset
==

docs/source/perplexity.rst
==================
83e5a1060;NielsRogge;2021-08-04 18:29:23 +0200;Add BEiT (#12994)
* First pass

* Make conversion script work

* Improve conversion script

* Fix bug, conversion script working

* Improve conversion script, implement BEiTFeatureExtractor

* Make conversion script work based on URL

* Improve conversion script

* Add tests, add documentation

* Fix bug in conversion script

* Fix another bug

* Add support for converting masked image modeling model

* Add support for converting masked image modeling

* Fix bug

* Add print statement for debugging

* Fix another bug

* Make conversion script finally work for masked image modeling models

* Move id2label for datasets to JSON files on the hub

* Make sure id's are read in as integers

* Add integration tests

* Make style & quality

* Fix test, add BEiT to README

* Apply suggestions from @sgugger's review

* Apply suggestions from code review

* Make quality

* Replace nielsr by microsoft in tests, add docs

* Rename BEiT to Beit

* Minor fix

* Fix docs of BeitForMaskedImageModeling

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

README.md
docs/source/index.rst
docs/source/model_doc/beit.rst
src/transformers/__init__.py
src/transformers/image_utils.py
src/transformers/models/__init__.py
src/transformers/models/auto/configuration_auto.py
src/transformers/models/auto/feature_extraction_auto.py
src/transformers/models/auto/modeling_auto.py
src/transformers/models/beit/__init__.py
src/transformers/models/beit/configuration_beit.py
src/transformers/models/beit/convert_beit_unilm_to_pytorch.py
src/transformers/models/beit/feature_extraction_beit.py
src/transformers/models/beit/modeling_beit.py
src/transformers/models/deit/convert_deit_timm_to_pytorch.py
src/transformers/models/detr/convert_detr_original_pytorch_checkpoint_to_pytorch.py
src/transformers/models/vit/convert_vit_timm_to_pytorch.py
src/transformers/models/vit/feature_extraction_vit.py
src/transformers/utils/coco_classes.py
src/transformers/utils/dummy_pt_objects.py
src/transformers/utils/dummy_vision_objects.py
src/transformers/utils/imagenet_classes.py
src/transformers/utils/modeling_auto_mapping.py
tests/test_feature_extraction_beit.py
tests/test_modeling_beit.py
utils/check_repo.py
==================
0dd1152c1;Lysandre Debut;2021-08-04 18:24:54 +0200;Skip ProphetNet test (#12462)

==

tests/test_modeling_prophetnet.py
==================
f82653874;Arman Cohan;2021-08-04 08:58:30 -0700;create tensors on device (#12846)

==

src/transformers/models/t5/modeling_t5.py
==================
fbf468b05;Patrick von Platen;2021-08-04 16:31:23 +0200;[Flax] Correct flax docs (#12782)
* fix_torch_device_generate_test

* remove @

* fix flax docs

* correct more docs in flax

* another correction

* fix flax docs

* Apply suggestions from code review
==

docs/source/main_classes/output.rst
docs/source/model_doc/bert.rst
docs/source/model_doc/wav2vec2.rst
src/transformers/file_utils.py
src/transformers/models/bart/modeling_flax_bart.py
src/transformers/models/clip/modeling_flax_clip.py
src/transformers/models/marian/modeling_flax_marian.py
src/transformers/models/mbart/modeling_flax_mbart.py
src/transformers/models/t5/modeling_flax_t5.py
src/transformers/models/vit/modeling_flax_vit.py
src/transformers/models/wav2vec2/modeling_flax_wav2vec2.py
src/transformers/models/wav2vec2/modeling_wav2vec2.py
==================
a317e6c3b;Patrick von Platen;2021-08-04 16:03:13 +0200;[Flax] Correctly Add MT5 (#12988)
* finish PR

* finish mt5

* push

* up

* Update tests/test_modeling_flax_mt5.py

Co-authored-by: Suraj Patil <surajp815@gmail.com>

Co-authored-by: Suraj Patil <surajp815@gmail.com>
==

docs/source/index.rst
docs/source/model_doc/mt5.rst
src/transformers/__init__.py
src/transformers/models/auto/modeling_flax_auto.py
src/transformers/models/mt5/__init__.py
src/transformers/models/mt5/modeling_flax_mt5.py
src/transformers/utils/dummy_flax_objects.py
tests/test_modeling_flax_mt5.py
utils/check_repo.py
==================
da9754a3a;Patrick von Platen;2021-08-04 16:00:09 +0200;[Flax] Align jax flax device name (#12987)
* [Flax] Align device name in docs

* make style

* fix import error
==

examples/research_projects/jax-projects/hybrid_clip/modeling_hybrid_clip.py
src/transformers/generation_flax_logits_process.py
src/transformers/generation_flax_utils.py
src/transformers/modeling_flax_outputs.py
src/transformers/models/bert/modeling_flax_bert.py
src/transformers/models/big_bird/modeling_flax_big_bird.py
src/transformers/models/clip/modeling_flax_clip.py
src/transformers/models/electra/modeling_flax_electra.py
tests/test_modeling_flax_common.py
==================
07df5578d;Aktsvigun;2021-08-04 16:49:21 +0300;pad_to_multiple_of added to DataCollatorForWholeWordMask (#12999)
* pad_to_multiple_of added to DataCollatorForWholeWordMask

* pad_to_multiple_of added to DataCollatorForWholeWordMask

Co-authored-by: –¶–≤–∏–≥—É–Ω –ê–∫–∏–º –û–ª–µ–≥–æ–≤–∏—á <AOTsvigun@sberbank.ru>
==

src/transformers/data/data_collator.py
==================
3f44a66cb;Lysandre Debut;2021-08-04 14:42:47 +0200;Return raw outputs in TextClassificationPipeline (#8328)
* Return raw outputs in TextClassificationPipeline

* Style

* Support for problem type

* Update src/transformers/pipelines/text_classification.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Apply Nicolas' comments

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

src/transformers/configuration_utils.py
src/transformers/pipelines/text_classification.py
==================
d4c834d2e;Sylvain Gugger;2021-08-04 11:48:39 +0200;Fix from_pretrained with corrupted state_dict (#12939)
* Fix from_pretrained with corrupted state_dict

* Adapt test

* Use better checkpoint

* Style

* Clean up
==

src/transformers/modeling_utils.py
tests/test_benchmark.py
tests/test_benchmark_tf.py
tests/test_pipelines_zero_shot.py
==================
a28da4c49;NielsRogge;2021-08-04 09:29:34 +0200;Replace nielsr by google namespace in tests (#12453)

==

tests/test_modeling_canine.py
==================
f064e0a43;Michal Szutenberg;2021-08-03 21:02:59 +0200;Cast logits to fp32 at the end of TF_T5 (#12332)
This change enables tf.keras.mixed_precision with bf16
==

src/transformers/models/t5/modeling_tf_t5.py
==================
b7439675b;Philip May;2021-08-03 10:10:33 +0200;fix `Trainer.train(resume_from_checkpoint=False)` is causing an exception (#12981)
* fix #12970

* Update tests/test_trainer.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update tests/test_trainer.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update tests/test_trainer.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* remove unnecessary issue link

* fix test formatting

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

src/transformers/trainer.py
tests/test_trainer.py
==================
790f1c954;Sylvain Gugger;2021-08-03 08:28:25 +0200;Fix template for inputs docstrings (#12976)

==

src/transformers/models/big_bird/modeling_big_bird.py
src/transformers/models/canine/modeling_canine.py
src/transformers/models/convbert/modeling_convbert.py
src/transformers/models/deberta/modeling_deberta.py
src/transformers/models/deberta_v2/modeling_deberta_v2.py
src/transformers/models/deit/modeling_deit.py
src/transformers/models/ibert/modeling_ibert.py
src/transformers/models/mpnet/modeling_mpnet.py
src/transformers/models/rembert/modeling_rembert.py
src/transformers/models/roberta/modeling_roberta.py
src/transformers/models/roformer/modeling_roformer.py
src/transformers/models/squeezebert/modeling_squeezebert.py
src/transformers/models/vit/modeling_vit.py
src/transformers/models/xlnet/modeling_xlnet.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_{{cookiecutter.lowercase_modelname}}.py
==================
75b8990d9;Chungman Lee;2021-08-02 19:58:43 +0900;fix typo in example/text-classification README (#12974)
* fix typo in example/text-classification README

* add space to align the table
==

examples/flax/text-classification/README.md
examples/pytorch/text-classification/README.md
==================
c1a65385a;Sylvain Gugger;2021-08-02 08:26:38 +0200;Place BigBirdTokenizer in sentencepiece-only objects (#12975)

==

src/transformers/__init__.py
src/transformers/utils/dummy_sentencepiece_objects.py
==================
b5995badc;Tadej Svetina;2021-08-02 08:08:57 +0200;Fix typo in example of DPRReader (#12954)

==

src/transformers/models/dpr/modeling_dpr.py
==================
a4340d3b8;Alex Hedges;2021-08-01 02:35:47 -0400;Set tb_writer to None in TensorBoardCallback.on_train_end() (#12963)

==

src/transformers/integrations.py
==================
3d4b3bc3f;Stefan Schweter;2021-07-30 18:27:53 +0200;examples: use correct way to get vocab size in flax lm readme (#12947)

==

examples/flax/language-modeling/README.md
==================
23d6761f3;Sylvain Gugger;2021-07-30 09:31:29 -0400;Fix division by zero in NotebookProgressPar (#12953)

==

src/transformers/utils/notebook.py
==================
8ff619d95;Kevin Canwen Xu;2021-07-30 20:56:14 +0800;Add multilingual documentation support (#12952)
* Add multilingual documentation support

* Add multilingual documentation support

* make style

* make style

* revert
==

docs/source/conf.py
setup.py
src/transformers/dependency_versions_table.py
==================
fe6ff4a92;wulu473;2021-07-30 13:20:38 +0100;Add substep callbacks (#12951)
Co-authored-by: Lukas Wutschitz <lukas.wutschitz@microsoft.com>
==

src/transformers/trainer.py
src/transformers/trainer_callback.py
==================
f84226b7a;harshithapv;2021-07-30 00:11:31 -0700;Log Azure ML metrics only for rank 0 (#12766)
* minor change to log azureml only for rank 0

* fix typo
==

src/transformers/integrations.py
==================
5c673efad;21jun;2021-07-30 16:06:33 +0900;fix typo in gradient_checkpointing arg (#12855)
help for `ModelArguments.gradient_checkpointing` should be
"If True, use gradient checkpointing to save memory
at the expense of slower backward pass."
not "Whether to freeze the feature extractor layers of the model."
(which is duplicated from `freeze_feature_extractor` arg)
==

examples/research_projects/wav2vec2/run_asr.py
examples/research_projects/wav2vec2/run_pretrain.py
==================
fd0255b41;Kevin Canwen Xu;2021-07-30 03:05:16 +0800;Add CpmTokenizerFast (#12938)
* Add CpmTokenizerFast

* Fix isort

* Overwrite _batch_encode_plus
==

src/transformers/models/auto/tokenization_auto.py
src/transformers/models/cpm/__init__.py
src/transformers/models/cpm/tokenization_cpm.py
src/transformers/models/cpm/tokenization_cpm_fast.py
==================
e2d22eef1;Nicolas Patry;2021-07-29 19:35:55 +0200;Moving feature-extraction pipeline to new testing scheme (#12843)
* Update feature extraction pipelilne.

* Leaving 1 small model for actual values check.

* Fixes tests

- Better support for tokenizer with no pad token
- Increasing PegasusModelTesterConfig for pipelines
- Test of feature extraction are more permissive + don't test Multimodel
models + encoder-decoder.

* Fixing model loading with incorrect shape (+ model with HEAD).

* Update tests/test_pipelines_common.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Revert modeling_utils modification.

* Some corrections.

* Update tests/test_pipelines_common.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update tests/test_pipelines_feature_extraction.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Syntax.

* Fixing text-classification tests.

* Don't modify this file.

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

src/transformers/models/pegasus/modeling_pegasus.py
src/transformers/pipelines/base.py
src/transformers/pipelines/feature_extraction.py
tests/test_modeling_pegasus.py
tests/test_modeling_t5.py
tests/test_modeling_tf_pegasus.py
tests/test_pipelines_common.py
tests/test_pipelines_feature_extraction.py
tests/test_pipelines_text_classification.py
==================
640421c0e;Funtowicz Morgan;2021-07-29 18:02:29 +0200;ONNX v2 raises an Exception when using PyTorch < 1.8.0  (#12933)
* Raise an issue if the pytorch version is < 1.8.0

* Attempt to add a test to ensure it correctly raises.

* Missing docstring.

* Second attempt, patch with string absolute import.

* Let's do the call before checking it was called ...

* use the correct function ... :facepalm:

* Raise ImportError and AssertionError respectively when unable to find torch and torch version is not sufficient.

* Correct path mock patching

* relax constraint for torch_onnx_dict_inputs to ge instead of eq.

* Style.

* Split each version requirements for torch.

* Let's compare version directly.

* Import torch_version after checking pytorch is installed.

* @require_torch
==

src/transformers/file_utils.py
src/transformers/onnx/convert.py
tests/test_onnx_v2.py
==================
9160d81c9;Will Frey;2021-07-28 14:19:34 -0400;Fix docstring typo in tokenization_auto.py (#12891)
Change `PreTrainedConfig` -> `PretrainedConfig` in the docstring for `AutoTokenizer.from_pretrained(...)`.
==

src/transformers/models/auto/tokenization_auto.py
==================
0d00c08da;Will Frey;2021-07-28 14:17:57 -0400;Fix typo in tokenization_auto.py (#12896)
Fix `config.decoder.__class` -> `config.decoder.__class__`
==

src/transformers/models/auto/tokenization_auto.py
==================
c3287ebd3;Will Frey;2021-07-28 14:17:20 -0400;Update typing in generation_logits_process.py (#12900)
Change `torch.Tensor` -> `torch.FloatTensor` in `TemperatureLogitsWarper` to be consistent with the `LogitsWarper` ABC signature annotation.
==

src/transformers/generation_logits_process.py
==================
df55c2b9b;Will Frey;2021-07-28 14:16:34 -0400;Update typing in generation_logits_process.py (#12901)
While `Iterable[Iterable[int]]` is a nicer annotation (it's covariant!), the defensive statements parsing out `bad_words_ids` in `__init__(...)` force the caller to pass in `List[List[int]]`. I've changed the annotation to make that clear.
==

src/transformers/generation_logits_process.py
==================
c164064ee;chutaklee;2021-07-29 02:11:38 +0800;Fix distiller.py (#12910)
* fix distiller

* fix style
==

examples/research_projects/distillation/distiller.py
==================
1da782cb2;Will Frey;2021-07-28 13:01:38 -0400;Add missing classmethod decorators (#12927)
`_BaseAutoModelClass` was missing `classmethod` decorators on the `from_config(...)` and `from_pretrained(...)` methods.
==

src/transformers/models/auto/auto_factory.py
==================
bf78f523a;Will Frey;2021-07-28 12:47:15 -0400;Fix StoppingCriteria ABC signature (#12918)
Change `score` -> `scores` because the argument is not positional-only, so you need consistently named parameters for the subclasses. The subclasses appear to favor `scores` over `score`.
==

src/transformers/generation_stopping_criteria.py
==================
63f2b9ab3;Sylvain Gugger;2021-07-28 11:37:20 -0400;Print defaults when using --help for scripts (#12930)

==

src/transformers/hf_argparser.py
==================
3ec851dc5;Sylvain Gugger;2021-07-28 09:47:49 -0400;Fix QA examples for roberta tokenizer (#12928)

==

examples/pytorch/question-answering/run_qa.py
examples/pytorch/question-answering/run_qa_beam_search.py
examples/pytorch/question-answering/run_qa_beam_search_no_trainer.py
examples/pytorch/question-answering/run_qa_no_trainer.py
examples/tensorflow/question-answering/run_qa.py
==================
fd85734e0;Sylvain Gugger;2021-07-28 09:38:12 -0400;Add option to set max_len in run_ner (#12929)

==

examples/pytorch/token-classification/run_ner.py
==================
1486fb810;Buddhi Chathuranga Senarathna;2021-07-28 17:15:30 +0530;Fix typo in the example of MobileBertForPreTraining (#12919)

==

src/transformers/models/mobilebert/modeling_mobilebert.py
==================
f3d0866ed;Elysium1436;2021-07-27 22:01:40 -0300;Correct validation_split_percentage argument from int (ex:5) to float (0.05) (#12897)
* Fixed train_test_split test_size argument

* `Seq2SeqTrainer` set max_length and num_beams only when non None  (#12899)

* set max_length and num_beams only when non None

* fix instance variables

* fix code style

* [FLAX] Minor fixes in CLM example (#12914)

* readme: fix retrieval of vocab size for flax clm example

* examples: fix flax clm example when using training/evaluation files

* Fix module path for symbolic_trace example

Co-authored-by: cchen-dialpad <47165889+cchen-dialpad@users.noreply.github.com>
Co-authored-by: Stefan Schweter <stefan@schweter.it>
Co-authored-by: Sylvain Gugger <sylvain.gugger@gmail.com>
==

examples/tensorflow/language-modeling/run_clm.py
examples/tensorflow/language-modeling/run_mlm.py
==================
68a441fa4;Sylvain Gugger;2021-07-27 13:47:22 -0400;Fix module path for symbolic_trace example

==

src/transformers/utils/fx.py
==================
d3c3e722d;Stefan Schweter;2021-07-27 16:18:04 +0200;[FLAX] Minor fixes in CLM example (#12914)
* readme: fix retrieval of vocab size for flax clm example

* examples: fix flax clm example when using training/evaluation files
==

examples/flax/language-modeling/README.md
examples/flax/language-modeling/run_clm_flax.py
==================
12e02e339;cchen-dialpad;2021-07-27 05:37:46 -0700;`Seq2SeqTrainer` set max_length and num_beams only when non None  (#12899)
* set max_length and num_beams only when non None

* fix instance variables

* fix code style
==

src/transformers/trainer_seq2seq.py
==================
ba15fe799;Sylvain Gugger;2021-07-26 17:10:34 -0400;Fix push_to_hub for TPUs (#12895)

==

src/transformers/trainer.py
==================
b3f95dcec;Sylvain Gugger;2021-07-26 10:27:25 -0400;Merge remote-tracking branch 'origin/master'

==
==================
a492aec82;Sylvain Gugger;2021-07-26 10:27:14 -0400;Update doc

==

.circleci/deploy.sh
docs/source/_static/js/custom.js
==================
a3bd76373;Nicolas Patry;2021-07-26 16:21:26 +0200;Better heuristic for token-classification pipeline. (#12611)
* Better heuristic for token-classification pipeline.

Relooking at the problem makes thing actually much simpler,
when we look at ids from a tokenizer, we have no way in **general**
to recover if some substring is part of a word or not.

However, within the pipeline, with offsets we still have access to the
original string, so we can simply look if previous character (if it
exists) of a token, is actually a space. This will obviously be wrong
for tokenizers that contain spaces within tokens, tokenizers where
offsets include spaces too (Don't think there are a lot).

This heuristic hopefully is fully bc and still can handle non-word based
tokenizers.

* Updating test with real values.

* We still need the older "correct" heuristic to prevent fusing
punctuation.

* Adding a real warning when important.
==

src/transformers/pipelines/token_classification.py
tests/test_pipelines_token_classification.py
==================
569f61a76;Matt;2021-07-26 15:15:51 +0100;Add TF multiple choice example (#12865)
* Add new multiple-choice example, remove old one
==

examples/tensorflow/multiple-choice/README.md
examples/tensorflow/multiple-choice/run_swag.py
examples/tensorflow/multiple-choice/run_tf_multiple_choice.py
examples/tensorflow/multiple-choice/utils_multiple_choice.py
==================
4f19881f8;Sylvain Gugger;2021-07-26 10:11:25 -0400;Fix documentation of BigBird tokenizer (#12889)

==

src/transformers/models/big_bird/tokenization_big_bird_fast.py
==================
303989de0;Sylvain Gugger;2021-07-26 09:57:34 -0400;Add accelerate to examples requirements (#12888)

==

examples/pytorch/language-modeling/requirements.txt
examples/pytorch/multiple-choice/requirements.txt
examples/pytorch/question-answering/requirements.txt
examples/pytorch/summarization/requirements.txt
examples/pytorch/token-classification/requirements.txt
examples/pytorch/translation/requirements.txt
==================
5f4362384;Sylvain Gugger;2021-07-26 09:48:19 -0400;Add possibility to ignore imports in test_fecther (#12801)
* Add possibility to ignore imports in test_fecther

* Style
==

src/transformers/tokenization_utils_base.py
utils/tests_fetcher.py
==================
7c300d6d4;Sylvain Gugger;2021-07-26 08:30:53 -0400;Fix barrier for SM distributed (#12853)

==

src/transformers/training_args.py
==================
0c1c42c12;Philip May;2021-07-26 14:30:05 +0200;add `classifier_dropout` to classification heads (#12794)
* add classifier_dropout to Electra

* no type annotations yet

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* add classifier_dropout to Electra

* add classifier_dropout to Electra ForTokenClass.

* add classifier_dropout to bert

* add classifier_dropout to roberta

* add classifier_dropout to big_bird

* add classifier_dropout to mobilebert

* empty commit to trigger CI

* add classifier_dropout to reformer

* add classifier_dropout to ConvBERT

* add classifier_dropout to Albert

* add classifier_dropout to Albert

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

src/transformers/models/albert/modeling_albert.py
src/transformers/models/albert/modeling_tf_albert.py
src/transformers/models/bert/configuration_bert.py
src/transformers/models/bert/modeling_bert.py
src/transformers/models/bert/modeling_flax_bert.py
src/transformers/models/bert/modeling_tf_bert.py
src/transformers/models/big_bird/configuration_big_bird.py
src/transformers/models/big_bird/modeling_big_bird.py
src/transformers/models/big_bird/modeling_flax_big_bird.py
src/transformers/models/convbert/configuration_convbert.py
src/transformers/models/convbert/modeling_convbert.py
src/transformers/models/convbert/modeling_tf_convbert.py
src/transformers/models/electra/configuration_electra.py
src/transformers/models/electra/modeling_electra.py
src/transformers/models/electra/modeling_flax_electra.py
src/transformers/models/electra/modeling_tf_electra.py
src/transformers/models/mobilebert/configuration_mobilebert.py
src/transformers/models/mobilebert/modeling_mobilebert.py
src/transformers/models/mobilebert/modeling_tf_mobilebert.py
src/transformers/models/reformer/configuration_reformer.py
src/transformers/models/reformer/modeling_reformer.py
src/transformers/models/roberta/modeling_flax_roberta.py
src/transformers/models/roberta/modeling_roberta.py
src/transformers/models/roberta/modeling_tf_roberta.py
==================
9ff672fc4;Lysandre;2021-07-24 17:37:58 +0200;BaseLazyModule -> LazyModule in RemBERT

==

src/transformers/models/rembert/__init__.py
==================
434022ada;Thibault FEVRY;2021-07-24 11:31:42 -0400;Add RemBERT model code to huggingface (#10692)
* Faster list concat for trainer_pt_utils.get_length_grouped_indices() (#11825)

get_length_grouped_indices() in LengthGroupedSampler and DistributedLengthGroupedSampler
is prohibitively slow for large number of megabatches (in test case takes hours for ~270k
megabatches with 100 items each) due to slow list concatenation with sum(megabatches, []).

Resolves: #11795

Co-authored-by: ctheodoris <cvtheodo@ds.dfci.harvard.edu>

* Replace double occurrences as the last step (#11367)

* [Flax] Fix PyTorch import error (#11839)

* fix_torch_device_generate_test

* remove @

* change pytorch import to flax import

* Fix reference to XLNet (#11846)

* Switch mem metrics flag (#11851)

* Switch mem metrics flag

* Update src/transformers/training_args.py

Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>

Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>

* Fix flos single node (#11844)

* fixing flos bug/typo in non-distributed setting

* storing flos every logging_interval

* Fix two typos in docs (#11852)

* typo2

* fix typo

* [Trainer] Report both steps and num samples per second (#11818)

* [Trainer] Report both steps and num samples per second

* Fix batch number

* Update src/transformers/trainer_utils.py

Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>

* Address review comments

Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>

* Add some tests to the slow suite #11860

* Enable memory metrics in tests that need it (#11859)

* fixed a small typo in the doc (#11856)

* typo (#11858)

* Add option to log only once in multinode training (#11819)

* Add option to long only once in multinode training

* Use an alternate property

* [Wav2Vec2] SpecAugment Fast (#11764)

* first try

* finish

* [lm examples] fix overflow in perplexity calc (#11855)

* fix overflow in perplexity calc

* use inf

* fix

* [Examples] create model with custom config on the fly (#11798)

* create custom model on the flight

* better wording

* add update_from_string

* cleanup

* cleanup

* Update src/transformers/configuration_utils.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* more bool options

* style

* fix logger

* add test

* add the doc

* assert on conflict of options

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* [Wav2Vec2ForCTC] example typo fixed (#11878)

* Ensure input tensor are on device. (#11874)

The feature extractor does not create tensors on the appropriate device,
so we call `ensure_tensor_on_device` before feeding the processed inputs
to the model.

* Fix usage of head masks by TF encoder-decoder models' `generate()` function (#11775)

* Fix Bart

* Fix Blenderbot{,_small}

* Fix LED

* Fix Marian

* Fix MBart

* Fix Pegasus

* Fix T5

* Add test for generation with head_mask

* Add a common TF test

* Override a test for the LED model as head masking is not yet properly implemented

* Remove all head_masks from input preparation for LED

* Drop masking for T5 as it needs a bit of refactor

* Correcting comments in T5Stack to reflect correct tuple order  (#11330)

* Correcting comments to reflect correct tuple order

In order to match the actual order (line 513 and 516, and as accessed in 968), I've changed the order mentioned in comments L962 and L966-967.

* Update modeling_t5.py

Updating another comment as well

* Removing extra space

* Fixing style and quality

* style & quality

* Update src/transformers/models/t5/modeling_t5.py

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* [Flax] Allow dataclasses to be jitted (#11886)

* fix_torch_device_generate_test

* remove @

* change dataclasses to flax ones

* fix typo

* fix jitted tests

* fix bert & electra

* changing find_batch_size to work with tokenizer outputs (#11890)

* changing find_batch_size to work with tokenizer outputs

trainer_pt_utils.find_batch_size does not recognize the batch size of BatchEncoding objects. This can cause an error when a trainer relies on find_batch_size to report the number of observed examples in the evaluation loop.

* Trigger CI

Co-authored-by: jrenner <joseph.renner@inria.fr>

* Link official Cloud TPU JAX docs (#11892)

* Flax Generate (#11777)

* fix_torch_device_generate_test

* remove @

* add

* indexing

* correct a couple of tests

* fix tests

* add logits processor

* finish top_k, top_p, temp

* add docs

* correct flax prng key default

* improve generate

* add generation docs

* add docs

* make style

* revert model outputs change

* make style

* correct typo

* fix tests

* fix slow test

* add raise

* finish generation

Co-authored-by: Patrick von Platen <patrick@huggingface.co>

* Add Emotion Speech Noteboook (#11900)

* Update deepspeed config to reflect hyperparameter search parameters (#11896)

* rebuild deepspeed config for hyperparameter search

* reformat code to fix style issues

* Adding new argument `max_new_tokens` for generate. (#11476)

* Adding new argument `max_new_tokens` for generate.

This is a proposal to add a new argument `max_new_tokens` to `generate`.
This include a `MaxNewTokensCriteria` that enables callers that don't
know about the token length ahead (like pipelines callers) to manage
more easily the length of their generated output.

* Adding a test for the user warning when both`max_length` and
`max_new_tokens` are used together.

* Removed redundant `no_grad`.

* Added Sequence Classification class in GPTNeo (#11906)

* seq classification changes

* fix tests

* [Flax] Return Attention from BERT, ELECTRA, RoBERTa and GPT2 (#11918)

* Added logic to return attention from flax-bert model and added test cases to check that

* Added new line at the end of file to test_modeling_flax_common.py

* fixing code style

* Fixing Roberta and Elextra models too from cpoying bert

* Added temporary hack to not run test_attention_outputs for FlaxGPT2

* Returning attention weights from GPT2 and changed the tests accordingly.

* last fixes

* bump flax dependency

Co-authored-by: jayendra <jayendra@infocusp.in>
Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* Test optuna and ray (#11924)

* Remove `datasets` submodule

* fix assert (#11935)

* Remove redundant `nn.log_softmax` in `run_flax_glue.py` (#11920)

* Remove redundant `nn.log_softmax` in `run_flax_glue.py`

`optax.softmax_cross_entropy` expects unnormalized logits, and so it already calls `nn.log_softmax`, so I believe it is not needed here. `nn.log_softmax` is idempotent so mathematically it shouldn't have made a difference.

* Remove unused 'flax.linen' import

* Add MT5ForConditionalGeneration as supported arch. to summarization README (#11961)

* Add MT5ForConditionalGeneration as supported arch.

* Update README.md

* Add FlaxCLIP (#11883)

* add flax CLIP

* default input_shape

* add tests

* fix test

* fix name

* fix docs

* fix shapes

* attend at least 1 token

* flax conv to torch conv

* return floats

* fix equivalence tests

* fix import

* return attention_weights and update tests

* fix dosctrings

* address patricks comments

* input_shape arg

* add tests for get_image_features and get_text_features methods

* fix tests

* RAG-2nd2end-revamp (#11893)

* initial

* code quality test

* code quality

* added test functions in test_modeling_rag.py and test_retrieval_rag.py to test end2end retreiver

* minor change in test_modeling_rag

* fixed tests

* Update examples/research_projects/rag-end2end-retriever/README.md

typo corrected as suggested by lhoestq

Co-authored-by: Quentin Lhoest <42851186+lhoestq@users.noreply.github.com>

* Update examples/research_projects/rag-end2end-retriever/finetune_rag.py

type change suggested by lhoestq

Co-authored-by: Quentin Lhoest <42851186+lhoestq@users.noreply.github.com>

* Update src/transformers/models/rag/retrieval_rag.py

Adding this change as mentioned by lhoestq.

Co-authored-by: Quentin Lhoest <42851186+lhoestq@users.noreply.github.com>

* completed the minor changes suggested by the reviewers

Co-authored-by: Quentin Lhoest <42851186+lhoestq@users.noreply.github.com>

* modify qa-trainer (#11872)

* modify qa-trainer

* fix flax model

* bugfixes training_args.py (#11922)

modified according to:
https://pytorch.org/xla/release/1.8.1/_modules/torch_xla/core/xla_model.html

* reinitialize wandb config for each hyperparameter search run (#11945)

* Add regression tests for slow sentencepiece tokenizers.  (#11737)

* add test_vocab_size for sentencepiece tok.

* add test_get_vocab for sentencepiece tok.

* add test_convert_token_and_id for sentencepiece tok.

* add test_tokenize_and_convert_tokens_to_string for all tok.

* improve test_tokenize_and_convert_tokens_to_string for sp. tok.

* add common tokenizer integration tests
- for albert
- for barthez

* add tokenizer integration tests to bert gen.

* add most tokenizer integration tests

* fix camembert tokenizer integration test

* add tokenizer integration test to marian

* add tokenizer integration test to reformer

* add typing and doc to tokenizer_integration_test_util

* fix tokenizer integration test of reformer

* improve test_sentencepiece_tokenize_and_convert_tokens_to_string

* empty commit to trigger CI

* fix tokenizer integration test of reformer

* remove code not needed anymore

* empty commit to trigger CI

* empty commit to trigger CI

* Authorize args when instantiating an AutoModel (#11956)

* Neptune.ai integration (#11937)

An option that turns on neptune.ai logging
--report_to 'neptune'

Additional ENV variables:
	NEPTUNE_PROJECT
	NEPTUNE_API_TOKEN
	NEPTUNE_RUN_NAME (optional)
	NEPTUNE_STOP_TIMEOUT (optional)

* Run the integration tests on schedule tests instead of master tests

* [deepspeed] docs (#11940)

* deepspeed docs

* cleanup

* cleanup

* typo correction (#11973)

* typo correction

* type corrections

* ByT5 model (#11971)

* allow tf to use uneven num of layers

* add tokenizer

* finish docs

* finish docs

* Apply suggestions from code review

* include in index

* finish

* Update docs/source/model_doc/byt5.rst

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>

* apply sylvais suggestions

* make style

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>

* Typo in usage example, changed to device instead of torch_device (#11979)

* [DeepSpeed] decouple `DeepSpeedConfigHF` from `Trainer` (#11966)

* decouple DeepSpeedConfigHF from Trainer

* add LoggingLevel ctx manager; add new test

* cleanup

* add docs

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* implemented suggested renames

* formatter workaround

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* [Trainer] add train loss and flops metrics reports (#11980)

* add train loss and flops metrics reports

* consistency

* add train_loss to skip keys

* restore on_train_end call timing

* Bump urllib3 from 1.25.8 to 1.26.5 in /examples/research_projects/lxmert (#11983)

Bumps [urllib3](https://github.com/urllib3/urllib3) from 1.25.8 to 1.26.5.
- [Release notes](https://github.com/urllib3/urllib3/releases)
- [Changelog](https://github.com/urllib3/urllib3/blob/main/CHANGES.rst)
- [Commits](https://github.com/urllib3/urllib3/compare/1.25.8...1.26.5)

---
updated-dependencies:
- dependency-name: urllib3
  dependency-type: direct:production
...

Signed-off-by: dependabot[bot] <support@github.com>

Co-authored-by: dependabot[bot] <49699333+dependabot[bot]@users.noreply.github.com>

* [RAG] Fix rag from pretrained question encoder generator behavior (#11962)

* fix_torch_device_generate_test

* remove @

* fix rag from pretrained loading

* add test

* uplaod

* finish

* VisualBERT (#10534)

* Init VisualBERT

* Add cookie-cutter, Config, and Embeddings

* Add preliminary Model

* Add Bert analogous classes

* Add basic code for NLVR, VQA, Flickr

* Update Init

* Fix VisualBert Downstream Models

* Rename classifier to cls

* Comment position_ids buffer

* Remove sentence image predictor output

* Update output dicts

* Remove unnecessary files

* Fix Auto Modeling

* Fix transformers init

* Add conversion script

* Add conversion script

* Fix docs

* Update visualbert modelling

* Update configuration

* Style fixes

* Add model and integration tests

* Add all tests

* Update model mapping

* Add simple detector from original repository

* Update docs and configs

* Fix style

* Fix style

* Update docs

* Fix style

* Fix import issues in style

* Fix style

* Add changes from review

* Fix style

* Fix style

* Update docs

* Fix style

* Fix style

* Update docs/source/model_doc/visual_bert.rst

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/models/visual_bert/modeling_visual_bert.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update tests/test_modeling_visual_bert.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/models/visual_bert/modeling_visual_bert.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/models/visual_bert/modeling_visual_bert.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/models/visual_bert/modeling_visual_bert.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Add changes from review

* Remove convert run script

* Add changes from review

* Update src/transformers/models/visual_bert/modeling_visual_bert.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/models/visual_bert/modeling_visual_bert.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/models/visual_bert/modeling_visual_bert.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/models/visual_bert/modeling_visual_bert.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/models/visual_bert/modeling_visual_bert.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Add changes from review

* Add changes from review

* Add visual embedding example in docs

* Fix "copied from" comments

* Add changes from review

* Fix error, style, checkpoints

* Update docs

* Fix integration tests

* Fix style

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Fix examples (#11990)

* [docs] fix xref to `PreTrainedModel.generate` (#11049)

* fix xref to generate

* do the same for search methods

* style

* style

* Update return introduction (#11976)

Make it clear that the `forward` method now returns a dict instead of tuple.

Fix style

* [deepspeed] Move code and doc into standalone files (#11984)

* move code and docs

* style

* moved

* restore

* [deepspeed] add nvme test skip rule (#11997)

* add nvme skip rule

* fix

* Fix weight decay masking in `run_flax_glue.py` (#11964)

* Fix weight decay masking in `run_flax_glue.py`

Issues with the previous implementation:
- The `dict` from `traverse_util.flatten_dict` has keys which are tuples of strings, not one long string with the path separated by periods.
- `optax.masked` applies the transformation wherever the mask is True, so the masks are flipped.
- Flax's LayerNorm calls the scale parameter `scale` not `weight`

* Fix formatting with black

* adapt results

Co-authored-by: Patrick von Platen <patrick@huggingface.co>

* [Flax] Refactor MLM  (#12013)

* fix_torch_device_generate_test

* remove @

* finish refactor

Co-authored-by: Patrick von Platen <patrick@huggingface.co>

* [Deepspeed] Assert on mismatches between ds and hf args (#12021)

* wip

* add mismatch validation + test

* renames

* Update docs/source/main_classes/deepspeed.rst

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* renames

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* [TrainerArguments] format and sort __repr__, add __str__ (#12018)

* format and sort __repr__, add __str__

* typo

* use __str__ directly

* alias __repr__ = __str__

* Fixed Typo in modeling_bart.py (#12035)

* Fixed Typo in modeling_bart.py - Issue #11895

* Fixed Typo in modeling_bart.py

* fix deberta 2 tokenizer integration test (#12017)

* fix docs of past_key_values (#12049)

* [JAX] Bump jax lib (#12053)

* fix_torch_device_generate_test

* remove @

* bump up jax lib

* Fixes bug that appears when using QA bert and distilation. (#12026)

* Fixing bug that appears when using distilation (and potentially other uses).
During backward pass Pytorch complains with:
RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation
This happens because the QA model code modifies the start_positions and end_positions input tensors, using clamp_ function: as a consequence the teacher and the student both modifies the inputs, and backward pass fails.

* Fixing all models QA clamp_ bug.

* Extend pipelines for automodel tupels (#12025)

* fix_torch_device_generate_test

* remove @

* finish

* refactor

* add test

* fix test

* Attempt at simplification.

* Small fix.

* Fixing non existing AutoModel for TF.

* Naming.

* Remove extra condition.

Co-authored-by: patrickvonplaten <patrick.v.platen@gmail.com>

* Add optional grouped parsers description to HfArgumentParser (#12042)

* Adding optional argument group to HfArgumentParser

* Minor

* remove whitespace

* Minor styling

* adds metric prefix. (#12057)

* adds metric prefix.

* update tests to include prefix

* skip failing test (#12059)

* Fix integration tests (#12066)

* Fix tapas issue (#12063)

* Fix scatter function to be compatible with torch-scatter 2.7.0

* Allow test again

* updated the original RAG implementation to be compatible with latest Pytorch-Lightning (#11806)

* updated the original RAG implementation to be compatible with the latest PL version

* updated the requirements.txt file

* execute make style

* code quality test

* code quality

* conflix resolved in requirement.txt

* code quality

* changed the MyDDP class name to CustomDDP

* Replace legacy tensor.Tensor with torch.tensor/torch.empty (#12027)

* Replace legacy torch.Tensor constructor with torch.{tensor, empty}

* Remove torch.Tensor in examples

* Add torch to requirements.txt in language-modeling (#12040)

* Add torch to requirements.txt in language-modeling

* Update examples/pytorch/language-modeling/requirements.txt

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Properly indent block_size (#12070)

* [Deepspeed] various fixes (#12058)

* replace deprecated config

* sub_group_size was too big

* complete deprecation removal

* [Deepspeed Wav2vec2] integration (#11638)

* wip

* wip - but working with https://github.com/microsoft/DeepSpeed/pull/1044

* cleanup

* workaround

* working 5/8 modes

* solve fp32 distributed zero3

* style

* sync

* sync

* rework

* deprecation

* cleanup

* https://github.com/microsoft/DeepSpeed/pull/1044 pr was merged

* clean up

* add a guide

* more prose

* more prose

* fix

* more prose

* sub_group_size was too big

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* refactor

* bug fix

* make the true check explicit

* new deepspeed release

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* typo

* Update run_ner.py with id2label config (#12001)

* sync LayerDrop for Wav2Vec2Encoder + tests (#12076)

* Add DETR (#11653)

* Squash all commits of modeling_detr_v7 branch into one

* Improve docs

* Fix tests

* Style

* Improve docs some more and fix most tests

* Fix slow tests of ViT, DeiT and DETR

* Improve replacement of batch norm

* Restructure timm backbone forward

* Make DetrForSegmentation support any timm backbone

* Fix name of output

* Address most comments by @LysandreJik

* Give better names for variables

* Conditional imports + timm in setup.py

* Address additional comments by @sgugger

* Make style, add require_timm and require_vision to tests√©

* Remove train_backbone attribute of DetrConfig, add methods to freeze/unfreeze backbone

* Add png files to fixtures

* Fix type hint

* Add timm to workflows

* Add `BatchNorm2d` to the weight initialization

* Fix retain_grad test

* Replace model checkpoints by Facebook namespace

* Fix name of checkpoint in test

* Add user-friendly message when scipy is not available

* Address most comments by @patrickvonplaten

* Remove return_intermediate_layers attribute of DetrConfig and simplify Joiner

* Better initialization

* Scipy is necessary to get sklearn metrics

* Rename TimmBackbone to DetrTimmConvEncoder and rename DetrJoiner to DetrConvModel

* Make style

* Improve docs and add 2 community notebooks

Co-authored-by: Lysandre <lysandre.debut@reseau.eseo.fr>

* [test] support more than 2 gpus (#12074)

* support more than 2 gpus

* style

* Wav2Vec2 Pretraining (#11306)

* Working quantizer forward

* Working quantizer forward

* Clean up unused model parts, test reproducibility

* Working quantizer forward

* Clean up unused model parts, test reproducibility

* Remove custom outputs from the shared ones

* correct conversion

* correct bug

* add first pretrain script

* save intermediate

* static shapes

* save intermediate

* finish first pretrain script version

* more refactor

* remove wanddb

* refactor more

* improve test

* correct perplexity compute bug

* finish model implementation

* add to docs

* finish docs

* finish pretraining script

* finish pretraining script

* remove wandb

* finish PR for merge

* finish config

* finish

* make deepspeed work

* Apply suggestions from code review

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* apply suggestions

* fix flaky test

Co-authored-by: patrickvonplaten <patrick.v.platen@gmail.com>
Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* pass decay_mask fn to optimizer (#12087)

* rm require_version_examples (#12088)

* [Wav2Vec2ForPretraining] Correct checkpoints wav2vec2 & fix tests (#12089)

* fix_torch_device_generate_test

* remove @

* fix tests

* Add text_column_name and label_column_name to run_ner and run_ner_no_trainer args (#12083)

* Add text_column_name and label_column_name to run_ner args

* Minor fix: grouping for text and label column name

* CLIPFeatureExtractor should resize images with kept aspect ratio (#11994)

* Resize with kept aspect ratio

* Fixed failed test

* Overload center_crop and resize methods instead

* resize should handle non-PIL images

* update slow test

* Tensor => tensor

Co-authored-by: patil-suraj <surajp815@gmail.com>

* New TF GLUE example (#12028)

* Pushing partially-complete new GLUE example

* First draft of the new TF GLUE example! Needs a little more testing to be sure but it's almost ready.

* Fix to the fit() call

* Bugfixes, making sure TPU and multi-GPU support is ready

* Remove logger line that depends on Pytorch

* Style pass

* Deleting old TF GLUE example

* Include label2id and id2label in the saved model config

* Don't clobber the existing model.config.label2id

* Style fixes

* Update examples/tensorflow/text-classification/run_glue.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Fix quality

* Update README.md to cover the TF GLUE example.

* Minor style edits

* Appending label2id and id2label to models to ensure inference works properly (#12102)

* Fix a condition in test_generate_with_head_masking (#11911)

* Fix a condition in test_generate_with_head_masking

* Fix usage of head_mask in bigbirg_pegasus

* Fix head masking for speech2text

* Resolve copy mismatch + drop unwanted print statement

* Fix the condition

* Flax VisionTransformer (#11951)

* adding vit for flax

* added test for Flax-vit and some bug-fixes

* overrided methods where variable changes were necessary for flax_vit test

* added FlaxViTForImageClassification for test

* Update src/transformers/models/vit/modeling_flax_vit.py

Co-authored-by: Suraj Patil <surajp815@gmail.com>

* made changes suggested in PR

* Adding jax-vit models for autoimport

* swapping num_channels and height,width dimension

* fixing the docstring for torch-like inputs for VIT

* add model to main init

* add docs

* doc, fix-copies

* docstrings

* small test fixes

* fix docs

* fix docstr

* Apply suggestions from code review

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* style

Co-authored-by: jayendra <jayendra@infocusp.in>
Co-authored-by: Suraj Patil <surajp815@gmail.com>
Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* add relevant description to tqdm in examples (#11927)

* add relevant `desc` in examples

* require_version datasets>=1.8.0

* Fix head masking generate tests (#12110)

* fix_torch_device_generate_test

* remove @

* fix tests

* Flax CLM script (#12023)

* first draft

* max_seq_length => block_size

* fix arg names

* fix typos

* fix loss calculation

* add max examples, fix  train eval steps, metrics

* optimizer mask

* fix perpelexity, metric logging

* fix logging

* data_collator = > data_loader

* refactor loss_fn

* support single GPU

* pass distributed to write_metric

* fix jitting

* fix single device training

* fix single device metrics

* close inner progress bars once finished

* add overwrite_cache arg

* ifx dataset caching issue

* add more logs

* few small fixes,

* address nicholas suggestions

* fix docstr

* address patricks suggestions

* make flake happy

* pass new new_dropout_rng to apply_gradients

* reset train metrics after every epoc

* remove distributed logis, small fixes

* Add from_pretrained to dummy timm objects (#12097)

* Add from_pretrained to dummy timm

* Fix at the source

* Update utils/check_dummies.py

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* Missing pretrained dummies

* Style

Co-authored-by: Sylvain Gugger <sylvain.gugger@gmail.com>
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Fix t5 error message (#12136)

* Fix t5 error message

* Fix again

* Fix megatron_gpt2 attention block's causal mask (#12007)

* Fix megatron_gpt2 attention block's causal mask.

* compatibility with checkpoints created with recent versions of Megatron-LM

* added integration test for the released Megatron-GPT2 model

* code style changes

* added option to megatron conversion script to read from config file

Co-authored-by: Guido Novati <gnovati@nvidia.com>

* Add mlm pretraining xla torch readme (#12011)

* fix_torch_device_generate_test

* remove @

* upload

* Apply suggestions from code review

* Apply suggestions from code review

* Apply suggestions from code review

* Update examples/flax/language-modeling/README.md

* add more info

* finish

* fix

Co-authored-by: Patrick von Platen <patrick@huggingface.co>

* add readme for flax clm (#12111)

* add readme for flax clm

* use section link for tokenizer

* Apply suggestions from code review

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* update metrics

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* FlaxBart (#11537)

* Start working on FlaxBart

* Create modeling_flax_bart.py

* Write FlaxBartAttention

* Add FlaxBartEncoderLayer

* Add FlaxBartDecoderLayer and some typing

* Add helepr function for FlaxBart

* shift_tokens_right

* _make_causal_mask

* _expand_mask

* Add PositionalEmbedding and fix init_std naming

* Add FlaxBartPretrainedModel

* Add FlaxBartEncoder

* Add FlaxBartEncoder

* Add FlaxBartEncoder among modules to be imported

* YET WE CANNOT INITIALIZE THAT!! :(

* Make BartEncoder working

Change BartEncoder to instance of nn.Module so far

* Add FlaxBartDecoder

* Add FlaxBartModel

* TODO to make model run -> Prepapre model inputs

* Resolve padding

* Add FlaxBartModel

* Add FlaxBartModel into importable modules

* Remove FlaxBartEncoder and FlaxBartDecoder from importable modules

* make style; not properly working

* make style; make quality not pass due to some import I left

* Remove TODO for padding_idx in nn.Embed so far

* Add FlaxBartForConditionalGeneration

* Incorporate Flax model output classes, i.e. return_dict

* Add another models and incorporate use_cache arg

* Add FlaxBartForSequenceClassification and FlaxBartForQuestionAnswering

* Incorporate use_cache arg from PyTorch implementation

* Add all necessary Flax output utils

* Add FlaxBartForCausalLM; not working yet'

* Add minor improvements; still lacks some functionality

* Update docs, src and tests

* Add support of FlaxBart to docs/source

* Fix some bugs in FlaxBart souce code

* Add some neccessary tests for FlaxBart models - jit_compilation not passing

* Fix tests and add test_head_masking

* Fix tests for @jax.jit computation

* Add test_head_masking

* Migrate FlaxBart tests from jax.numpy to numpy

* Remove FlaxBartForCausalLM

* Clean repo

* fix bart model weight structure

* Fix FlaxBartForSequenceClassification

Slicing is not possible to use below jit, therefore, selecting sentence
representation from hidden_states must be changed.

* Allow FlaxBartForSequenceClassification for testing pt_flax equivalence

* Allow testing for FlaxBartForQA for pt_flax equivalence

* Add a comment to FlaxBartForSequenceClassification + change noise from 1e-3 to 1e-6

* remove past_key_values

* remove inputs_mebeds and make input_ids required

* add position ids

* re-write attention layer

* fix dataclass

* fix pos embeds and attention output

* fix pos embeds

* expose encode method

* expose decode method

* move docstring to top

* add cache for causal attn layer

* remove head masking for now

* s2s greedy search first pass

* boom boom

* fix typos

* fix greedy generate for bart

* use encoder, decoder layers instead of num_hidden_layers

* handle encoder_outputs

* cleanup

* simplify decoding

* more clean-up

* typos

* Change header + add {decoder_,}position_ids into 2 models

* add BartConfig

* fix existing tests

* add encode, decode methods

* Fix shift_tokens_right for JIT compilation + clarify one condition

* fix decode

* encoder => encode

* simplify generate

* add tests for encode and decode

* style

* add tests for cache

* fix equivalence tests

* sample generate now works with seq2seq

* generation tests

* initialize dense layers

* docstring and cleanup

* quality

* remove get/set input_embeddings

* address Patricks suggestions

* decode for every model, remove encoder_outputs from call

* update tests accordingly

* decode returns only decoder outputs and logits

* fix arguments

* doc encode, decode methods

* correct base_model_prefix

* fix test for seq classif model

* fix docs

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
Co-authored-by: Suraj Patil <surajp815@gmail.com>

* Feature to use the PreTrainedTokenizerFast class as a stand-alone tokenizer (#11810)

* feature for tokenizer without slow/legacy version

* format

* modify common test

* add tests

* add PreTrainedTokenizerFast to AutoTokenizer

* format

* change tokenizer common test in order to be able to run test without a slow version

* update tokenizer fast test in order to use `rust_tokenizer_class` attribute instead of `tokenizer_class`

* add autokenizer test

* replace  `if self.tokenizer_class is not None` with ` if self.tokenizer_class is None`

* remove obsolete change in comment

* Update src/transformers/tokenization_utils_base.py

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* Update src/transformers/tokenization_utils_fast.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* change `get_main_tokenizer` into `get_tokenizers`

* clarify `get_tokenizers` method

* homogenize with `test_slow_tokenizer` and `test_rust_tokenizer`

* add `test_rust_tokenizer = False` to tokenizer which don't define a fast version

* `test_rust_tokenizer = False` for BertJapaneseTokenizer

* `test_rust_tokenizer = False` for BertJapaneseCharacterTokenizationTest

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* [Flax] Add links to google colabs (#12146)

* fix_torch_device_generate_test

* remove @

* add colab links

* Don't log anything before logging is setup in examples (#12121)

* Don't log anything before logging is setup in examples

* Last example

* Use text_column_name variable instead of "text" (#12132)

* Use text_column_name variable instead of "text"

`text_column_name` was already defined above where I made the changes and it was also used below where I made changes.

This is a very minor change. If a dataset does not use "text" as the column name, then the `tokenize_function` will now use whatever column is assigned to `text_column_name`. `text_column_name` is just the first column name if "text" is not a column name. It makes the function a little more robust, though I would assume that 90% + of datasets use "text" anyway.

* black formatting

* make style

Co-authored-by: Nicholas Broad <nicholas@nmbroad.com>

* [lm examples] Replicate --config_overrides addition to other LM examples (#12135)

* [lm examples] Replicate --config_overrides addition to other LM examples

* Removing no trainer files changes

* Update README

Co-authored-by: Kumar Abhishek <kabhishek@expedia.com>

* fix error message (#12148)

* [optim] implement AdafactorSchedule (#12123)

* implement AdafactorSchedule

* typo

* fix

* Update src/transformers/optimization.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* [style] consistent nn. and nn.functional (#12124)

* consistent nn. and nn.functional

* fix glitch

* fix glitch #2

* Adding TFWav2Vec2Model (#11617)

* [WIP] Add TFWav2Vec2Model

Work in progress for adding a tensorflow version of Wav2Vec2

* feedback changes

* small fix

* Test Feedback Round 1

* Add SpecAugment and CTC Loss

* correct spec augment mask creation

* docstring and correct copyright

* correct bugs

* remove bogus file

* finish tests correction

* del unnecessary layers

* Update src/transformers/models/wav2vec2/modeling_tf_wav2vec2.py

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* make style

* correct final bug

* Feedback Changes

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* [Flax] Fix flax pt equivalence tests (#12154)

* fix_torch_device_generate_test

* remove @

* upload

* consistent nn. and nn.functional: p2 templates (#12153)

* Flax Big Bird (#11967)

* add flax bert

* bert -> bigbird

* original_full ported

* add debugger

* init block sparse

* fix copies ; gelu_fast -> gelu_new

* block sparse port

* fix block sparse

* block sparse working

* all ckpts working

* fix-copies

* make quality

* init tests

* temporary fix for FlaxBigBirdForMultipleChoice

* skip test_attention_outputs

* fix

* gelu_fast -> gelu_new ; fix multiple choice model

* remove nsp

* fix sequence classifier

* fix

* make quality

* make fix-copies

* finish

* Delete debugger.ipynb

* Update src/transformers/models/big_bird/modeling_flax_big_bird.py

* make style

* finish

* bye bye jit flax tests

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* [style] consistent nn. and nn.functional: part 3 `tests` (#12155)

* consistent nn. and nn.functional: p3 templates

* restore

* [style] consistent nn. and nn.functional: part 4 `examples` (#12156)

* consistent nn. and nn.functional: p4 examples

* restore

* consistent nn. and nn.functional: part 5 docs (#12161)

* Add video links to the documentation (#12162)

* [Flax generate] Add params to generate (#12171)

* fix_torch_device_generate_test

* remove @

* add params as input

* finish

* Use a released version of optax rather than installing from Git. (#12173)

Use a released version of optax rather than installing from Git

* Have dummy processors have a `from_pretrained` method (#12145)

* Add course banner (#12157)

* Add course banner

* Update course banner

* Adjust banner width

* Enable add_prefix_space if model_type is roberta or gpt2 (#12116)

* Update AutoModel classes in summarization example (#12178)

- Convert use of deprecated AutoModelWithLMHead to AutoModelForSeq2SeqLM
- Add newly required `truncation=True` to `tokenizer.encode` with `max_length`

This silences all warnings.

* Ray Tune Integration Updates (#12134)

* fix

* fixes

* add back to scheduled tests

* formatting

* Update integrations.py

* [testing] ensure concurrent pytest workers use a unique port for torch.dist (#12166)

* ensure concurrent pytest workers use a unique port for torch.distributed.launch

* reword

* Model card defaults (#12122)

* [WIP] Model card defaults

* finetuned_from default value

* Add all mappings to the mapping file

* Be more defensive on finetuned_from arg

* Add default task tag

* Separate tags from tasks

* Edge case for dataset

* Apply suggestions from code review

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* Temporarily deactivate torch-scatter while we wait for new release (#12181)

* Temporarily deactivate torch-scatter while we wait for new release

* torch-1.8.1 binary for scatter

* Revert to 1.8.0

* Pin torch dependency

* torchaudio and torchvision

* Temporarily deactivate torchhub test (#12184)

* [Flax] Add Beam Search (#12131)

* fix_torch_device_generate_test

* remove @

* push new logit processors

* add processors

* save first working version

* save intermediate

* finish

* make style

* make fix-copies

* finish

* Update tests/test_modeling_flax_bart.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Apply suggestions from code review

Co-authored-by: Suraj Patil <surajp815@gmail.com>

Co-authored-by: Patrick von Platen <patrick@huggingface.co>
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
Co-authored-by: Suraj Patil <surajp815@gmail.com>

* Hubert (#11889)

* fix_torch_device_generate_test

* remove @

* add hubert

* add first test file

* more docs

* fix bugs

* fix bug

* finish

* finish

* finish docstring

* fix

* fix

* finalize

* add to ignored

* finish

* Apply suggestions from code review

* correct naming

* finish

* fix auto config

* finish

* correct convert script

* Apply suggestions from code review

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
Co-authored-by: Suraj Patil <surajp815@gmail.com>

* apply suggestions lysandre & suraj

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
Co-authored-by: Suraj Patil <surajp815@gmail.com>

* updated DLC images and sample notebooks (#12191)

* Enabling AutoTokenizer for HubertConfig. (#12198)

* Use yaml to create metadata (#12185)

* Use yaml to create metadata

* Fix typo

* Remove pin

* [Docs] fixed broken link (#12205)

* fixed broken link

* Update docs/source/benchmarks.rst

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update docs/source/benchmarks.rst

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Pipeline update & tests (#12207)

* Improve detr (#12147)

* Remove unused variables

* Improve docs

* Fix docs of segmentation masks

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* Add link to the course (#12229)

* Support for torch 1.9.0 (#12224)

* Support for torch 1.9.0

* Torch scatter for 1.9.0

* Github Actions run on 1.9.0

* fix pt-1.9.0 `add_` deprecation (#12217)

* fix pt-1.9.0 add_ deprecation

* add () for clarity

* Trigger CI

* require_version(torch

* Release: v4.7.0

* Docs for v4.8.0

* AutoTokenizer: infer the class from the tokenizer config if possible (#12208)

* AutoTokenizer: infer the class from the tokenizer config if possible

* Add tests

* Update src/transformers/models/auto/tokenization_auto.py

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* update desc for map in all examples (#12226)

* update desc for map in all examples

* added plm

* suggestions

* [Flax] FlaxAutoModelForSeq2SeqLM (#12228)

* add FlaxAutoModelForSeq2SeqLM

* [FlaxBart] few small fixes (#12247)

* boom boom

* remove flax clip example

* few small fixes

* Depreciate pythonic Mish and support PyTorch 1.9 version of Mish (#12240)

* Moved Mish to Torch 1.9 version

* Run black formatting

* [t5 doc] make the example work out of the box (#12239)

* [run_clm.py] restore caching

* style

* [t5 doc] make the example work out of the box

This PR expands the training example to include the correct model type for the example to work, e.g. with `T5Model` this example will break.

* Update docs/source/model_doc/t5.rst

Co-authored-by: Suraj Patil <surajp815@gmail.com>

* expand the other example

Co-authored-by: Suraj Patil <surajp815@gmail.com>

* Fix the scheduled CI

* Better CI feedback (#12279)

* Better run ID

* Only part of CI

* Revert "Only part of CI"

This reverts commit 29f7f248d21e0f5792e0670ba8705b31ad8967b7.

* Fix for making student ProphetNet for Seq2Seq Distillation (#12130)

* make_student.py: fix to make student ProphetNet

* reformat

* [FlaxClip] fix test from/save pretrained test (#12284)

* boom boom

* remove flax clip example

* fix from_save_pretrained

* [Flax] [WIP] allow loading head model with base model weights (#12255)

* boom boom

* remove flax clip example

* allow loading head model with base model weights

* add test

* fix imports

* disable save, load test for clip

* add test_save_load_to_base

* [DeepSpeed] don't ignore --adafactor (#12257)

* [Flax] Fix flax test save pretrained (#12256)

* fix_torch_device_generate_test

* remove @

* fix flax save pretrained test

* Tensorflow QA example (#12252)

* New Tensorflow QA example!

* Style pass

* Updating README.md for the new example

* flake8 fixes

* Update examples/tensorflow/question-answering/README.md

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* [Flax] Add jax flax to env command (#12251)

* fix_torch_device_generate_test

* remove @

* add commands for flax/jax

* reset report_to to none, avoid deprecation warning (#12293)

* [trainer + examples] set log level from CLI (#12276)

* set log level from CLI

* add log_level_replica + test + extended docs

* cleanup

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* rename datasets objects to allow datasets module

* improve the doc

* style

* doc improve

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* [tests] multiple improvements (#12294)

* [tests] multiple improvements

* cleanup

* style

* todo to investigate

* fix

* Fix for the issue of device-id getting hardcoded for token_type_ids during Tracing [WIP] (#11252)

* registering a buffer for token_type_ids, to pass the error of device-id getting hardcoded when tracing

* sytle format

* adding persistent flag to the resgitered buffers that prevent from adding them to the state_dict and addresses the Backward compatibility issue

* adding the try catch to the fix as persistent flag is only available from PT >1.6

* adding version check

* added the condition to only use the token_type_ids buffer when its autogenerated not passed by user

* adding comments and making the conidtion where token_type_ids are None to use the registered buffer

* taking out position-embeddding from the if block

* adding comments

* handling the case if buffer for position_ids was not registered

* reverted the changes on position_ids, fix the issue with size of token_type_ids buffer, moved the modification for generated token_type_ids to Bertmodel, instead of Embeddings

* reverting the token_type_ids in case of None to the previous version

* reverting changes on position_ids adding back the if block

* changes added by running make fix-copies

* changes added by running make fix-copies and added the import version as it was getting used

* changes added by running make fix-copies

* changes added by running make fix-copies

* fixing the import format

* fixing the import format

* modified to use temp tensor for trimed and expanded token_type_ids buffer

* changes made by fix-copies after temp tensor modifications

* changes made by fix-copies after temp tensor modifications

* changes made by fix-copies after temp tensor modifications

* clean up

* clean up

* clean up

* clean up

* Nit

* Nit

* Nit

* modified according to support device conversion on traced models

* modified according to support device conversion on traced models

* modified according to support device conversion on traced models

* modified according to support device conversion on traced models

* changes based on latest in master

* Adapt templates

* Add version import

Co-authored-by: Ubuntu <ubuntu@ip-172-31-32-81.us-west-2.compute.internal>
Co-authored-by: Lysandre <lysandre.debut@reseau.eseo.fr>

* trainer_tf: adjust wandb installation command (#12291)

* add FlaxAutoModelForImageClassification in main init (#12298)

* Fix and improve documentation for LEDForConditionalGeneration (#12303)

* Replace conditional generation example (fixes #12268)

* Replace model in summarization example with finetuned checkpoint, adapt example text

* Fix typo in new summarization example

* Fix docstring formatting, add missing import statement to example

* [Flax] Main doc for event orga (#12305)

* fix_torch_device_generate_test

* remove @

* push

* finish

* some typos

* add more info on communication

* add suggestions

* [trainer] 2 bug fixes and a rename (#12309)

* bug fixes and a rename

* add extended DDP test

* FlaxBartPretrainedModel -> FlaxBartPreTrainedModel (#12313)

* [docs]  performance  (#12258)

* initial performance document

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* rewrites based on suggestions

* 8x multiple is for AMP only

* add contribute section

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* Add CodeCarbon Integration (#12304)

* Add optional dependency

* Add CodeCarbon integration

* Add CodeCarbon integration

* Add CodeCarbon integration

* typo

* Optimizing away the `fill-mask` pipeline. (#12113)

* Optimizing away the `fill-mask` pipeline.

- Don't send anything to the tokenizer unless needed. Vocab check is
much faster
- Keep BC by sending data to the tokenizer when needed. User handling warning messages will see performance benefits again
- Make `targets` and `top_k` work together better `top_k` cannot be
higher than `len(targets)` but can be smaller still.
- Actually simplify the `target_ids` in case of duplicate (it can happen
because we're parsing raw strings)
- Removed useless code to fail on empty strings. It works only if empty
string is in first position, moved to ignoring them instead.
- Changed the related tests as only the tests would fail correctly
(having incorrect value in first position)

* Make tests compatible for 2 different vocabs... (at the price of a
warning).

Co-authored-by: @EtaoinWu

* ValueError working globally

* Update src/transformers/pipelines/fill_mask.py

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* `tokenizer.vocab` -> `tokenizer.get_vocab()` for more compatiblity +
fallback.

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* Add output in a dictionary for TF `generate` method (#12139)

* Add output args to greedy search

* Fix critical typo + make style quality

* Handle generate_beam_search

* Add dict_specific tests and fix the placement of encoder outputs

* Add  specific outputs

* Update doc

* Fix typo

* Adjust handling encoder_outputs + Fix generating for T5

* Fix generate for RAG

* Fix handling ouptut_attentions when target_mapping is not None

Take care of situations when target_mapping is provided
as there are 2-tuple of attentions

Change from:
if inputs["output_attentions"]:
    attentions = tuple(tf.transpose(t, perm(2, 3, 0, 1)) for t in attentions)

to:
if inputs["output_attentions"]:
    if inputs["target_mapping"] is not None:
        # when target_mapping is provided, there are 2-tuple of attentions
         attentions = tuple(
             tuple(tf.transpose(attn_stream, perm=(2, 3, 0, 1)) for attn_stream in t) for t in attentions
        )
    else:
        attentions = tuple(tf.transpose(t, perm=(2, 3, 0, 1)) for t in attentions)

* Rename kwargs to model_kwargs

* make style quality

* Move imports in test_modeling_tf_common.py

Move ModelOutput-related imports in test_modeling_tf_common.py
into the `is_tf_available():` statement.

* Rewrite nested if-statements

* Fix added tests

* Flax summarization script  (#12230)

* add summrization script

* fix arguments, preprocessing, metrics

* add generation and metrics

* auto model, prediction loop

* prettify

* label smoothing

* adress Sylvain and Patricks suggestions

* dynamically import shift_tokens_right

* fix shift_tokens_right_fn call

* Rewrite ProphetNet to adapt converting ONNX friendly (#11981)

* Rewrite

* [ONNX] rewrite

* Flax T5 (#12150)

* copy pytorch-t5

* init

* boom boom

* forward pass same

* make generation work

* add more tests

* make test work

* finish normal tests

* make fix-copies

* finish quality

* correct slow example

* correct slow test

* version table

* upload models

* Update tests/test_modeling_flax_t5.py

* correct incorrectly deleted line

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
Co-authored-by: Patrick von Platen <patrick@huggingface.co>

* Add mention of the huggingface_hub methods for offline mode (#12320)

* [Flax/JAX] Add how to propose projects markdown (#12311)

* fix_torch_device_generate_test

* remove @

* finish

* make style

* [TFWav2Vec2] Fix docs (#12283)

* fix error

* make style check happy

Co-authored-by: chenhaitao <chenhaitao@qiyi.com>

* Clean push to hub API (#12187)

* Clean push to hub API

* Create working dir if it does not exist

* Different tweak

* New API + all models + test Flax

* Adds the Trainer clean up

* Update src/transformers/file_utils.py

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* Address review comments

* (nit) output types

* No need to set clone_from when folder exists

* Update src/transformers/trainer.py

Co-authored-by: Julien Chaumond <julien@huggingface.co>

* Add generated_from_trainer tag

* Update to new version

* Fixes

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
Co-authored-by: Julien Chaumond <julien@huggingface.co>
Co-authored-by: Lysandre <lysandre.debut@reseau.eseo.fr>

* Add all XxxPreTrainedModel to the main init (#12314)

* Add all XxxPreTrainedModel to the main init

* Add to template

* Add to template bis

* Add FlaxT5

* Conda build (#12323)

* Temporarily revert the `fill-mask` improvements.

* changed modeling_fx_utils.py to utils/fx.py for clarity (#12326)

Co-authored-by: Michael Benayoun <michael@huggingface.co>

* Pin good version of huggingface_hub

* [Flax T5] Fix weight initialization and fix docs (#12327)

* finish t5 flax fixes

* improve naming

* Release: v4.8.0

* v4.9.0.dev0

* Update training_args.py (#12328)

mention in `save_strategy` param description that `load_best_model_at_end` can override

* [Deepspeed] new docs (#12077)

* document sub_group_size

* style

* install + issues reporting

* style

* style

* Update docs/source/main_classes/deepspeed.rst

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* indent 4

* restore

* style

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Fix default to logging_dir lost in merge conflict

* try-this (#12338)

Signed-off-by: Richard Liaw <rliaw@berkeley.edu>

* [examples/Flax] move the examples table up (#12341)

* Fix torchscript tests (#12336)

* Fix torchscript tests

* Better test

* Remove bogus print

* Document patch release v4.8.1

* Add flax/jax quickstart (#12342)

* Update README.md

* fixed typo (#12356)

* Fix exception in prediction loop occurring for certain batch sizes (#12350)

* fix distributed_concat for scalar outputs

* Update README.md

* fixed typo (#12356)

* simplify fix with terser syntax

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Trigger CI

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
Co-authored-by: michal pitr <21157924+MichalPitr@users.noreply.github.com>
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Add FlaxBigBird QuestionAnswering script (#12233)

* port bigbird script

* adapt script a bit

* change location

* adapt more

* save progress

* init commit

* style

* dataset script tested

* readme add

* Replace NotebookProgressReporter by ProgressReporter in Ray Tune run (#12357)

* Replace NotebookProgressReporter by ProgressReporter in Ray Tune run

* Move to local import

* Style

* remove extra white space from log format (#12360)

* fixed multiplechoice tokenization (#12362)

* fixed multiplechoice tokenization

The model would have seen two sequences:
1. [CLS]prompt[SEP]prompt[SEP]
2. [CLS]choice0[SEP]choice1[SEP]
that is not correct as we want a contextualized embedding of prompt and choice

* removed outer brackets for proper sequence generation

* [trainer] add main_process_first context manager (#12351)

* main_process_first context manager

* handle multi-node, add context description

* sync desc

* [Examples] Replicates the new --log_level feature to all trainer-based pytorch (#12359)

* added log_level

* fix comment

* fixed log_level

* Trigger CI

* Unfied logging

* simplified args for log_level

* updated example template (#12365)

* replace print with logger (#12368)

* [Documentation] Warn that DataCollatorForWholeWordMask is limited to BertTokenizer-like tokenizers (#12371)

* Notify users that DataCollatorForWholeWordMask is limited to BertTokenier-like tokenizers

* Fix code formatting

* Update run_mlm.py (#12344)

Before the code could not be used for validation only because of this line:
extension = data_args.train_file.split(".")[-1]
was assuming that extension must be extracted from the training dataset. This line would run regardless of the training or validation options of the user. This would lead to an error if the user only wants to run an evaluation only and does not want to do train (because the training file does not exist). I modified it to extract extension from the training file if the user wants to do train and extract it from the validation file if the user wants to run eval. This way the code can be used for both training and validation separately.

* Add possibility to maintain full copies of files (#12312)

* [CI] add dependency table sync verification (#12364)

* add dependency table sync verification

* improve the message

* improve the message

* revert

* ready to merge

* [Examples] Added context manager to datasets map (#12367)

* added cotext manager to datasets map

* fixed style and spaces

* fixed warning of deprecation

* changed desc

* [Flax community event] Add more description to readme (#12398)

* fix_torch_device_generate_test

* remove @

* boom boom

* correct typos

* Apply suggestions from code review

Co-authored-by: Suraj Patil <surajp815@gmail.com>

* Apply suggestions from code review

Co-authored-by: Suzana Iliƒá <io.suzanai@gmail.com>

* Apply suggestions from code review

Co-authored-by: Suraj Patil <surajp815@gmail.com>
Co-authored-by: Suzana Iliƒá <io.suzanai@gmail.com>

* Update README.md

* Fix copies

* Remove the need for `einsum` in Albert's attention computation (#12394)

* debug albert einsum

* Fix matmul computation

* Let's use torch linear layer.

* Style.

* [Flax] Adapt flax examples to include `push_to_hub` (#12391)

* fix_torch_device_generate_test

* remove @

* finish

* correct summary writer

* correct push to hub

* fix indent

* finish

* finish

* finish

* finish

* finish

Co-authored-by: Patrick von Platen <patrick@huggingface.co>

* Tensorflow LM examples (#12358)

* Tensorflow MLM example

* Add CLM example

* Style fixes, adding missing checkpoint code from the CLM example

* Fix TPU training, avoid massive dataset warnings

* Fix incorrect training length calculation for multi-GPU training

* Fix incorrect training length calculation for multi-GPU training

* Refactors and nitpicks from the review

* Style pass

* Adding README

* pass the matching trainer log level to deepspeed (#12401)

* [Flax] Add T5 pretraining script (#12355)

* fix_torch_device_generate_test

* remove @

* add length computatan

* finish masking

* finish

* upload

* fix some bugs

* finish

* fix dependency table

* correct tensorboard

* Apply suggestions from code review

* correct processing

* slight change init

* correct some more mistakes

* apply suggestions

* improve readme

* fix indent

* Apply suggestions from code review

Co-authored-by: SaulLu <55560583+SaulLu@users.noreply.github.com>

* correct tokenizer

* finish

* finish

* finish

* finish

Co-authored-by: Patrick von Platen <patrick@huggingface.co>
Co-authored-by: SaulLu <55560583+SaulLu@users.noreply.github.com>

* [models] respect dtype of the model when instantiating it (#12316)

* [models] respect dtype of the model when instantiating it

* cleanup

* cleanup

* rework to handle non-float dtype

* fix

* switch to fp32 tiny model

* improve

* use dtype.is_floating_point

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* fix the doc

* recode to use explicit torch_dtype_auto_detect, torch_dtype args

* docs and tweaks

* docs and tweaks

* docs and tweaks

* merge 2 args, add docs

* fix

* fix

* better doc

* better doc

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Rename detr targets to labels (#12280)

* Rename target to labels in DetrFeatureExtractor

* Update DetrFeatureExtractor tests accordingly

* Improve docs of DetrFeatureExtractor

* Improve docs

* Make style

* Add out of vocabulary error to ASR models (#12288)

* Add OOV error to ASR models

* Feedback changes

* Fix TFWav2Vec2 SpecAugment (#12289)

* Fix TFWav2Vec2 SpecAugment

* Invert masks

* Feedback changes

* [example/flax] add summarization readme (#12393)

* add readme

* update readme and add requirements

* Update examples/flax/summarization/README.md

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* [Flax] Example scripts - correct weight decay  (#12409)

* fix_torch_device_generate_test

* remove @

* finish

* finish

* correct style

* fix ids_to_tokens naming error in tokenizer of deberta v2 (#12412)

Co-authored-by: Jipeng Huang <jihuan@microsoft.com>

* minor fixes in original RAG training (#12395)

* Added talks (#12415)

* Easily train a new fast tokenizer from a given one (#12361)

* [WIP] Easily train a new fast tokenizer from a given one

* Fix test

* Roll out to other tokenizers and add tests

* Fix bug with unk id and add emoji to test

* Really use something different in test

* Implement special tokens map

* Map special tokens in the Transformers tokenizers

* Fix test

* Make test more robust

* Fix test for BPE

* More robust map and test

Co-authored-by SaulLu

* Test file

* Stronger tests

Co-authored-by: SaulLu <lucilesaul.com@gmail.com>

* Map unk token for Wordpiece and address review comment

* Fix lowercase test and address review comment

* Fix all tests

* Simplify test

* Fix tests for realsies

* Easily train a new fast tokenizer from a given one - tackle the special tokens format (str or AddedToken) (#12420)

* Propose change in tests regarding lower case

* add new test for special tokens types

* put back the test part about decoding

* add feature: the AddedToken is re-build with the different mapped content

* Address review comment: simplify AddedToken building

Co-authored-by: sgugger <sylvain.gugger@gmail.com>

* Update src/transformers/tokenization_utils_fast.py

Co-authored-by: sgugger <sylvain.gugger@gmail.com>
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

Co-authored-by: SaulLu <lucilesaul.com@gmail.com>
Co-authored-by: SaulLu <55560583+SaulLu@users.noreply.github.com>

* [modelcard] fix (#12422)

this PR is fixing an incorrect attribute - probably some tests are needed?

* Add option to save on each training node (#12421)

* Add option to save on each training node

* Apply suggestions from code review

Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>

* Address review comments

Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>

* Added to talks section (#12433)

Added one more confirmed speaker, zoom links and gcal event links

* Fix default bool in argparser (#12424)

* Fix default bool in argparser

* Add more to test

* Add default bos_token and eos_token for tokenizer of deberta_v2 (#12429)

* fix ids_to_tokens naming error in tokenizer of deberta v2

* Update tokenization_deberta_v2.py

Add bos_token and eos_token.

* format code

Co-authored-by: Jipeng Huang <jihuan@microsoft.com>

* Add CANINE (#12024)

* First pass

* More progress

* Add support for local attention

* More improvements

* More improvements

* Conversion script working

* Add CanineTokenizer

* Make style & quality

* First draft of integration test

* Remove decoder test

* Improve tests

* Add documentation

* Mostly docs improvements

* Add CanineTokenizer tests

* Fix most tests on GPU, improve upsampling projection

* Address most comments by @dhgarrette

* Remove decoder logic

* Improve Canine tests, improve docs of CanineConfig

* All tokenizer tests passing

* Make fix-copies and fix tokenizer tests

* Fix test_model_outputs_equivalence test

* Apply suggestions from @sgugger's review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Address some more comments

* Add support for hidden_states and attentions of shallow encoders

* Define custom CanineModelOutputWithPooling, tests pass

* First pass

* More progress

* Add support for local attention

* More improvements

* More improvements

* Conversion script working

* Add CanineTokenizer

* Make style & quality

* First draft of integration test

* Remove decoder test

* Improve tests

* Add documentation

* Mostly docs improvements

* Add CanineTokenizer tests

* Fix most tests on GPU, improve upsampling projection

* Address most comments by @dhgarrette

* Remove decoder logic

* Improve Canine tests, improve docs of CanineConfig

* All tokenizer tests passing

* Make fix-copies and fix tokenizer tests

* Fix test_model_outputs_equivalence test

* Apply suggestions from @sgugger's review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Address some more comments

* Make conversion script work for Canine-c too

* Fix tokenizer tests

* Remove file

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Document patch release v4.8.2

* fix typo in mt5 configuration docstring (#12432)

* Add to talks section (#12442)

* [JAX/Flax readme] add philosophy doc (#12419)

* add philosophy doc

* fix typos

* update doc

* Apply suggestions from code review

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* address Patricks suggestions

* add a training example and fix typos

* jit the training step

* jit train step

* fix example code

* typo

* Apply suggestions from code review

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* [Flax] Add wav2vec2 (#12271)

* fix_torch_device_generate_test

* remove @

* start flax wav2vec2

* save intermediate

* forward pass has correct shape

* add weight norm

* add files

* finish ctc

* make style

* finish gumbel quantizer

* correct docstrings

* correct some more files

* fix vit

* finish quality

* correct tests

* correct docstring

* correct tests

* start wav2vec2 pretraining script

* save intermediate

* start pretraining script

* finalize pretraining script

* finish

* finish

* small typo

* finish

* correct

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
Co-authored-by: Suraj Patil <surajp815@gmail.com>

* make style

* push

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
Co-authored-by: Suraj Patil <surajp815@gmail.com>

* Add missing Copied from statements

* Reference model uploaded under Google org

* Fix various duplicates from merging

* Rembert-large -> rembert, fix overeager Copied from, return type

* Incorporate PR comments from Patrick and Sylvain

Co-authored-by: ctheodoris <seanymphoceana@yahoo.com>
Co-authored-by: ctheodoris <cvtheodo@ds.dfci.harvard.edu>
Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>
Co-authored-by: Teven <teven.lescao@gmail.com>
Co-authored-by: Nick Lane-Smith <nlanesmith@gmail.com>
Co-authored-by: Shiro T <stsuchi@users.noreply.github.com>
Co-authored-by: Wang Ran (Ê±™ÁÑ∂) <wrran@outlook.com>
Co-authored-by: Ahmet Akko√ß <themadprogramer@gmail.com>
Co-authored-by: francescorubbo <francescorubbo@users.noreply.github.com>
Co-authored-by: Daniel Stancl <46073029+stancld@users.noreply.github.com>
Co-authored-by: talkhaldi <tareq.alkhaldi@gmail.com>
Co-authored-by: joerenner <joepeterrenner@gmail.com>
Co-authored-by: jrenner <joseph.renner@inria.fr>
Co-authored-by: Avital Oliver <avitalo@google.com>
Co-authored-by: Patrick von Platen <patrick@huggingface.co>
Co-authored-by: Josh Tanner <mindful.jt@gmail.com>
Co-authored-by: Nicolas Patry <patry.nicolas@protonmail.com>
Co-authored-by: Bhadresh Savani <bhadreshpsavani@gmail.com>
Co-authored-by: Jayendra <jayendra0parmar@gmail.com>
Co-authored-by: jayendra <jayendra@infocusp.in>
Co-authored-by: Lysandre <lysandre.debut@reseau.eseo.fr>
Co-authored-by: Philip May <philip@may.la>
Co-authored-by: Nicholas Vadivelu <nicholas.vadivelu@gmail.com>
Co-authored-by: Suraj Patil <surajp815@gmail.com>
Co-authored-by: Shamane Siri <shamane@ahlab.org>
Co-authored-by: Quentin Lhoest <42851186+lhoestq@users.noreply.github.com>
Co-authored-by: Fan Zhang <zhangfan.tju@gmail.com>
Co-authored-by: Riccardo Bassani <48254418+BassaniRiccardo@users.noreply.github.com>
Co-authored-by: Volodymyr Byno <volodymyr.byno@gmail.com>
Co-authored-by: Jeoung-Minju <51041861+JminJ@users.noreply.github.com>
Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>
Co-authored-by: Alberto Villa <a.villa.diez@gmail.com>
Co-authored-by: dependabot[bot] <49699333+dependabot[bot]@users.noreply.github.com>
Co-authored-by: Gunjan Chhablani <chhablani.gunjan@gmail.com>
Co-authored-by: Kou Yong Kang <kou.yongkang@dhs.sg>
Co-authored-by: Shiva Pundir <36535845+ceevaaa@users.noreply.github.com>
Co-authored-by: Fran√ßois Lagunas <francois.lagunas@gmail.com>
Co-authored-by: Peter Izsak <232524+peteriz@users.noreply.github.com>
Co-authored-by: Russell Klopfer <russell@klopfer.us>
Co-authored-by: Mario ≈†a≈°ko <mariosasko777@gmail.com>
Co-authored-by: cdleong <4109253+cdleong@users.noreply.github.com>
Co-authored-by: Koichi Yasuoka <yasuoka@kanji.zinbun.kyoto-u.ac.jp>
Co-authored-by: Anton Lozhkov <aglozhkov@gmail.com>
Co-authored-by: kumapo <kumapo@users.noreply.github.com>
Co-authored-by: Tobias Norlund <tobias@norlund.se>
Co-authored-by: Matt <Rocketknight1@users.noreply.github.com>
Co-authored-by: Sylvain Gugger <sylvain.gugger@gmail.com>
Co-authored-by: Bhavitvya Malik <bhavitvya.malik@gmail.com>
Co-authored-by: Jonathan Chang <31893406+cccntu@users.noreply.github.com>
Co-authored-by: Guido Novati <16716298+novatig@users.noreply.github.com>
Co-authored-by: Guido Novati <gnovati@nvidia.com>
Co-authored-by: SaulLu <55560583+SaulLu@users.noreply.github.com>
Co-authored-by: Nicholas Broad <nbroad94@gmail.com>
Co-authored-by: Nicholas Broad <nicholas@nmbroad.com>
Co-authored-by: Kumar Abhishek <kr.abhish@gmail.com>
Co-authored-by: Kumar Abhishek <kabhishek@expedia.com>
Co-authored-by: Will Rice <will@spokestack.io>
Co-authored-by: Vasudev Gupta <7vasudevgupta@gmail.com>
Co-authored-by: Kilian Kluge <32523967+ionicsolutions@users.noreply.github.com>
Co-authored-by: Amog Kamsetty <amogkam@users.noreply.github.com>
Co-authored-by: Philipp Schmid <32632186+philschmid@users.noreply.github.com>
Co-authored-by: Xa9aX „ÉÑ <mishradiganta91@gmail.com>
Co-authored-by: Vishal Burman <vishal.a.burman23@gmail.com>
Co-authored-by: Hamid Shojanazeri <hamid.nazeri2010@gmail.com>
Co-authored-by: Ubuntu <ubuntu@ip-172-31-32-81.us-west-2.compute.internal>
Co-authored-by: Stefan Schweter <stefan@schweter.it>
Co-authored-by: Kevin Canwen Xu <canwenxu@126.com>
Co-authored-by: David Fan <30608893+jiafatom@users.noreply.github.com>
Co-authored-by: chenht2010 <chenht2010@yahoo.com>
Co-authored-by: chenhaitao <chenhaitao@qiyi.com>
Co-authored-by: Julien Chaumond <julien@huggingface.co>
Co-authored-by: Michael Benayoun <mickbenayoun@gmail.com>
Co-authored-by: Michael Benayoun <michael@huggingface.co>
Co-authored-by: Sam Havens <47401552+sam-qordoba@users.noreply.github.com>
Co-authored-by: Richard Liaw <rliaw@berkeley.edu>
Co-authored-by: Marc van Zee <marcvanzee@gmail.com>
Co-authored-by: michal pitr <21157924+MichalPitr@users.noreply.github.com>
Co-authored-by: jglaser <glaserj@ornl.gov>
Co-authored-by: Kai Fricke <krfricke@users.noreply.github.com>
Co-authored-by: cronoik <johannes.schaffrath@mail.de>
Co-authored-by: Taha ValizadehAslani <47432410+TahaAslani@users.noreply.github.com>
Co-authored-by: Suzana Iliƒá <io.suzanai@gmail.com>
Co-authored-by: Funtowicz Morgan <mfuntowicz@users.noreply.github.com>
Co-authored-by: Will Rice <wrice20@gmail.com>
Co-authored-by: Jabin Huang <huangjipengnju@gmail.com>
Co-authored-by: Jipeng Huang <jihuan@microsoft.com>
Co-authored-by: SaulLu <lucilesaul.com@gmail.com>
Co-authored-by: fcakyon <34196005+fcakyon@users.noreply.github.com>
==

README.md
docs/source/index.rst
docs/source/model_doc/rembert.rst
src/transformers/__init__.py
src/transformers/commands/convert.py
src/transformers/convert_slow_tokenizer.py
src/transformers/modeling_tf_utils.py
src/transformers/models/__init__.py
src/transformers/models/auto/configuration_auto.py
src/transformers/models/auto/modeling_auto.py
src/transformers/models/auto/modeling_tf_auto.py
src/transformers/models/rembert/__init__.py
src/transformers/models/rembert/configuration_rembert.py
src/transformers/models/rembert/convert_rembert_tf_checkpoint_to_pytorch.py
src/transformers/models/rembert/modeling_rembert.py
src/transformers/models/rembert/modeling_tf_rembert.py
src/transformers/models/rembert/tokenization_rembert.py
src/transformers/models/rembert/tokenization_rembert_fast.py
src/transformers/utils/dummy_pt_objects.py
src/transformers/utils/dummy_sentencepiece_objects.py
src/transformers/utils/dummy_tf_objects.py
src/transformers/utils/dummy_tokenizers_objects.py
src/transformers/utils/modeling_auto_mapping.py
tests/test_modeling_rembert.py
tests/test_modeling_tf_rembert.py
==================
f6e254474;Patrick von Platen;2021-07-23 17:53:30 +0200;[Sequence Feature Extraction] Add truncation (#12804)
* fix_torch_device_generate_test

* remove @

* add truncate

* finish

* correct test

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* clean tests

* correct normalization for truncation

* remove casting

* up

* save intermed

* finish

* finish

* correct

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

src/transformers/feature_extraction_sequence_utils.py
src/transformers/models/speech_to_text/feature_extraction_speech_to_text.py
src/transformers/models/speech_to_text/modeling_speech_to_text.py
src/transformers/models/wav2vec2/feature_extraction_wav2vec2.py
tests/test_feature_extraction_speech_to_text.py
tests/test_feature_extraction_wav2vec2.py
tests/test_modeling_wav2vec2.py
tests/test_sequence_feature_extraction_common.py
==================
98364ea74;Stas Bekman;2021-07-23 08:05:48 -0700;[tests] fix logging_steps requirements (#12860)

==

examples/research_projects/wav2vec2/test_wav2vec2_deepspeed.py
tests/deepspeed/test_deepspeed.py
==================
e218249b0;Patrick von Platen;2021-07-23 14:16:04 +0200;Pin git python to <3.10.0 (#12858)
* fix_torch_device_generate_test

* remove @

* pin git python

* make style

* typo
==

setup.py
src/transformers/dependency_versions_table.py
==================
795c1444e;Nicolas Patry;2021-07-22 15:19:35 +0200;Improving pipeline tests (#12784)
* Proposal

* Testing pipelines slightly better.

- Overall same design
- Metaclass to get proper different tests instead of subTest (not well
supported by Pytest)
- Added ANY meta object to make output checking more readable.
- Skipping architectures either without tiny_config or without
architecture.

* Small fix.

* Fixing the tests in case of None value.

* Oups.

* Rebased with more architectures.

* Fixing reformer tests (no override anymore).

* Adding more options for model tester config.

Co-authored-by: Lysandre <lysandre.debut@reseau.eseo.fr>
==

src/transformers/models/led/modeling_led.py
src/transformers/pipelines/base.py
tests/test_modeling_mbart.py
tests/test_modeling_reformer.py
tests/test_pipelines_common.py
tests/test_pipelines_text_classification.py
==================
40de2d5a4;Lysandre;2021-07-22 12:52:25 +0200;Docs for v4.10.0dev0

==

.circleci/deploy.sh
docs/source/_static/js/custom.js
examples/pytorch/language-modeling/run_clm.py
examples/pytorch/language-modeling/run_mlm.py
examples/pytorch/language-modeling/run_plm.py
examples/pytorch/multiple-choice/run_swag.py
examples/pytorch/question-answering/run_qa.py
examples/pytorch/question-answering/run_qa_beam_search.py
examples/pytorch/question-answering/run_qa_beam_search_no_trainer.py
examples/pytorch/question-answering/run_qa_no_trainer.py
examples/pytorch/summarization/run_summarization.py
examples/pytorch/text-classification/run_glue.py
examples/pytorch/text-classification/run_xnli.py
examples/pytorch/token-classification/run_ner.py
examples/pytorch/translation/run_translation.py
examples/tensorflow/question-answering/run_qa.py
examples/tensorflow/summarization/run_summarization.py
examples/tensorflow/text-classification/run_glue.py
examples/tensorflow/translation/run_translation.py
setup.py
src/transformers/__init__.py
==================
72aee83ce;Lysandre;2021-07-22 12:11:55 +0200;Release: v4.9.0

==

examples/pytorch/language-modeling/run_clm.py
examples/pytorch/language-modeling/run_mlm.py
examples/pytorch/language-modeling/run_plm.py
examples/pytorch/multiple-choice/run_swag.py
examples/pytorch/question-answering/run_qa.py
examples/pytorch/question-answering/run_qa_beam_search.py
examples/pytorch/question-answering/run_qa_beam_search_no_trainer.py
examples/pytorch/question-answering/run_qa_no_trainer.py
examples/pytorch/summarization/run_summarization.py
examples/pytorch/text-classification/run_glue.py
examples/pytorch/text-classification/run_xnli.py
examples/pytorch/token-classification/run_ner.py
examples/pytorch/translation/run_translation.py
examples/tensorflow/question-answering/run_qa.py
examples/tensorflow/summarization/run_summarization.py
examples/tensorflow/text-classification/run_glue.py
examples/tensorflow/translation/run_translation.py
setup.py
src/transformers/__init__.py
==================
fcf83011d;Maxwell Forbes;2021-07-21 23:14:14 -0700;Fix type of max_seq_length arg in run_swag.py (#12832)

==

examples/pytorch/multiple-choice/run_swag.py
==================
27a8c9e4f;Stas Bekman;2021-07-21 15:11:02 -0700;[parallelism doc] document Deepspeed-Inference and parallelformers (#12836)
* document Deepspeed-Inference and parallelformers

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

docs/source/parallelism.md
==================
807b6bd16;Stas Bekman;2021-07-21 10:49:29 -0700;[Deepspeed] warmup_ratio docs (#12830)
* [Deepspeed] warmup_ratio docs

* Update docs/source/main_classes/deepspeed.rst

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* style

* Update docs/source/main_classes/deepspeed.rst

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* style

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

docs/source/main_classes/deepspeed.rst
==================
8c2384d8e;Sylvain Gugger;2021-07-21 12:44:41 -0400;Raise warning in HP search when hp is not in args (#12831)

==

src/transformers/trainer.py
==================
cf0755aa6;Stas Bekman;2021-07-21 09:36:02 -0700;[debug] DebugUnderflowOverflow doesn't work with DP (#12816)

==

docs/source/debugging.rst
src/transformers/trainer.py
src/transformers/trainer_utils.py
==================
ac3cb660c;Lysandre Debut;2021-07-21 14:29:43 +0200;Add _CHECKPOINT_FOR_DOC to all models (#12811)
* Add _CHECKPOINT_FOR_DOC

* Update src/transformers/models/funnel/modeling_funnel.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

src/transformers/models/blenderbot/modeling_blenderbot.py
src/transformers/models/blenderbot_small/modeling_blenderbot_small.py
src/transformers/models/clip/modeling_clip.py
src/transformers/models/deit/modeling_deit.py
src/transformers/models/detr/modeling_detr.py
src/transformers/models/dpr/modeling_dpr.py
src/transformers/models/funnel/modeling_funnel.py
src/transformers/models/hubert/modeling_hubert.py
src/transformers/models/layoutlm/modeling_layoutlm.py
src/transformers/models/luke/modeling_luke.py
src/transformers/models/m2m_100/modeling_m2m_100.py
src/transformers/models/marian/modeling_marian.py
src/transformers/models/prophetnet/modeling_prophetnet.py
src/transformers/models/speech_to_text/modeling_speech_to_text.py
src/transformers/models/t5/modeling_t5.py
src/transformers/models/tapas/modeling_tapas.py
src/transformers/models/visual_bert/modeling_visual_bert.py
src/transformers/models/vit/modeling_vit.py
src/transformers/models/wav2vec2/modeling_wav2vec2.py
==================
786ced363;Sylvain Gugger;2021-07-21 08:24:36 -0400;Add versioning system to fast tokenizer files (#12713)
* Add versioning system to fast tokenizer files

* Deal with offline mode

* Use staging env in tests

* Style

* Apply suggestions from code review

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* Style

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
==

src/transformers/file_utils.py
src/transformers/tokenization_utils_base.py
src/transformers/training_args.py
tests/test_tokenization_fast.py
==================
037bdf82d;Masatoshi TSUCHIYA;2021-07-21 19:37:49 +0900;Refer warmup_ratio when setting warmup_num_steps. (#12818)
* Refer warmup_ratio when setting warmup_num_steps.

* Add a method to get number of warmup steps to TrainerArguments class.

* Fix.

* Fix.
==

src/transformers/deepspeed.py
src/transformers/trainer.py
src/transformers/training_args.py
==================
15d19ecfd;Philip May;2021-07-21 10:28:30 +0200;fix convert_tokens_to_string calls (#11716)

==

src/transformers/models/albert/tokenization_albert.py
src/transformers/models/barthez/tokenization_barthez.py
src/transformers/models/camembert/tokenization_camembert.py
src/transformers/models/m2m_100/tokenization_m2m_100.py
src/transformers/models/mbart/tokenization_mbart50.py
src/transformers/models/speech_to_text/tokenization_speech_to_text.py
==================
c3d9ac760;Lysandre Debut;2021-07-21 10:13:11 +0200;Expose get_config() on ModelTesters (#12812)
* Expose get_config() on ModelTesters

* Typo
==

templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/test_modeling_{{cookiecutter.lowercase_modelname}}.py
tests/test_modeling_albert.py
tests/test_modeling_bart.py
tests/test_modeling_bert.py
tests/test_modeling_bert_generation.py
tests/test_modeling_big_bird.py
tests/test_modeling_bigbird_pegasus.py
tests/test_modeling_blenderbot.py
tests/test_modeling_blenderbot_small.py
tests/test_modeling_canine.py
tests/test_modeling_clip.py
tests/test_modeling_convbert.py
tests/test_modeling_ctrl.py
tests/test_modeling_deberta.py
tests/test_modeling_deberta_v2.py
tests/test_modeling_deit.py
tests/test_modeling_detr.py
tests/test_modeling_distilbert.py
tests/test_modeling_dpr.py
tests/test_modeling_electra.py
tests/test_modeling_flaubert.py
tests/test_modeling_fsmt.py
tests/test_modeling_funnel.py
tests/test_modeling_gpt2.py
tests/test_modeling_gpt_neo.py
tests/test_modeling_hubert.py
tests/test_modeling_ibert.py
tests/test_modeling_layoutlm.py
tests/test_modeling_led.py
tests/test_modeling_longformer.py
tests/test_modeling_luke.py
tests/test_modeling_lxmert.py
tests/test_modeling_m2m_100.py
tests/test_modeling_marian.py
tests/test_modeling_mbart.py
tests/test_modeling_megatron_bert.py
tests/test_modeling_mobilebert.py
tests/test_modeling_mpnet.py
tests/test_modeling_pegasus.py
tests/test_modeling_prophetnet.py
tests/test_modeling_reformer.py
tests/test_modeling_roberta.py
tests/test_modeling_roformer.py
tests/test_modeling_speech_to_text.py
tests/test_modeling_squeezebert.py
tests/test_modeling_t5.py
tests/test_modeling_tapas.py
tests/test_modeling_transfo_xl.py
tests/test_modeling_visual_bert.py
tests/test_modeling_vit.py
tests/test_modeling_wav2vec2.py
tests/test_modeling_xlm.py
tests/test_modeling_xlnet.py
==================
cabcc7517;Stas Bekman;2021-07-20 09:05:26 -0700;[trainer] sanity checks for `save_steps=0|None` and `logging_steps=0` (#12796)
* [trainer] fix % 0

* sanity checks

* fix logging_strategy

* correction

* Update src/transformers/training_args.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

src/transformers/trainer_callback.py
src/transformers/training_args.py
tests/extended/test_trainer_ext.py
==================
acdd78db0;Patrick von Platen;2021-07-20 16:48:37 +0200;Update README.md

==

examples/research_projects/jax-projects/README.md
==================
b5b4e5492;Suraj Patil;2021-07-20 18:58:50 +0530;add and fix examples (#12810)

==

docs/source/model_doc/clip.rst
src/transformers/models/clip/modeling_clip.py
src/transformers/models/clip/modeling_flax_clip.py
==================
31d06729f;Patrick von Platen;2021-07-20 14:19:37 +0200;Update README.md

==

examples/research_projects/jax-projects/README.md
==================
2955d50e0;Patrick von Platen;2021-07-20 14:17:21 +0200;[Longformer] Correct longformer docs (#12809)
* fix_torch_device_generate_test

* remove @

* correct longformer docs

Co-authored-by: Patrick von Platen <patrick@huggingface.co>
==

src/transformers/models/longformer/modeling_longformer.py
==================
13fefdf34;Patrick von Platen;2021-07-20 13:51:15 +0200;Update README.md
cc @patil-suraj
==

examples/flax/language-modeling/README.md
==================
66197adc9;fgaim;2021-07-20 13:38:25 +0200;Flax MLM: Allow validation split when loading dataset from local file (#12689)
* Allow validation split when loading dataset from local file

* Flax clm & t5, enable validation split for datasets loaded from local file
==

examples/flax/language-modeling/run_clm_flax.py
examples/flax/language-modeling/run_mlm_flax.py
examples/flax/language-modeling/run_t5_mlm_flax.py
==================
6f8e367ae;Will Rice;2021-07-20 07:36:47 -0400;Fix Padded Batch Error 12282 (#12487)
This fixes the padded batch [issue](https://github.com/huggingface/transformers/issues/12282). The error was generated due to the maximum sequence length of the attention mask not matching the padded sequence length of the hidden_states. `np.allclose` now passes with a 1e-2 absolute tolerance.

This change fixes
==

src/transformers/models/wav2vec2/modeling_tf_wav2vec2.py
==================
7fae53505;Stas Bekman;2021-07-20 00:32:02 -0700;add troubleshooting docs (#12791)

==

.circleci/TROUBLESHOOT.md
.github/workflows/TROUBLESHOOT.md
==================
0118ef89e;Sylvain Gugger;2021-07-19 19:50:47 +0200;Enforce eval and save strategies are compatible when --load_best_model_at_end (#12786)
* Enforce eval and save strategies are compatible when --load_best_model_at_end

* Update doc

* Fix typos

* Fix tests
==

src/transformers/trainer_callback.py
src/transformers/training_args.py
tests/test_trainer.py
==================
546dc24e0;Lysandre Debut;2021-07-19 10:55:40 +0200;Longer timeout for slow tests (#12779)

==

.github/workflows/self-scheduled.yml
==================
cab3b8689;Antoni Baum;2021-07-19 10:32:40 +0200;[ray] Fix `datasets_modules` ImportError with Ray Tune (#12749)
* Fix dynamic_modules ImportError with Ray Tune

* Nit
==

src/transformers/integrations.py
==================
534f6eb9f;Patrick von Platen;2021-07-17 19:22:37 +0200;Create README.md

==
==================
c6b9095cb;Patrick von Platen;2021-07-17 19:22:26 +0200;Update README.md

==

examples/research_projects/jax-projects/README.md
==================
da72ac6e2;Sylvain Gugger;2021-07-17 15:52:33 +0200;Fix push_to_hub docstring and make it appear in doc (#12770)

==

docs/source/main_classes/configuration.rst
docs/source/main_classes/model.rst
docs/source/main_classes/tokenizer.rst
src/transformers/configuration_utils.py
src/transformers/file_utils.py
src/transformers/modeling_flax_utils.py
src/transformers/modeling_tf_utils.py
src/transformers/modeling_utils.py
src/transformers/tokenization_utils_base.py
==================
08d609bfb;Tomohiro Endo;2021-07-17 22:52:21 +0900;Add tokenizers class mismatch detection between `cls` and checkpoint (#12619)
* Detect mismatch by analyzing config

* Fix comment

* Fix import

* Update src/transformers/tokenization_utils_base.py

Co-authored-by: SaulLu <55560583+SaulLu@users.noreply.github.com>

* Revise based on reviews

* remove kwargs

* Fix exception

* Fix handling exception again

* Disable mismatch test in PreTrainedTokenizerFast

Co-authored-by: SaulLu <55560583+SaulLu@users.noreply.github.com>
==

src/transformers/models/bert_japanese/tokenization_bert_japanese.py
src/transformers/tokenization_utils_base.py
tests/test_tokenization_bert_japanese.py
tests/test_tokenization_common.py
tests/test_tokenization_fast.py
==================
b4b562d83;Patrick von Platen;2021-07-16 18:07:08 +0100;[Wav2Vec2] Padded vectors should not allowed to be sampled (#12764)
* fix_torch_device_generate_test

* remove @

* finish

* correct script

* correct script
==

examples/research_projects/jax-projects/wav2vec2/run_wav2vec2_pretrain_flax.py
src/transformers/models/wav2vec2/modeling_flax_wav2vec2.py
src/transformers/models/wav2vec2/modeling_wav2vec2.py
tests/test_modeling_flax_wav2vec2.py
tests/test_modeling_wav2vec2.py
==================
6e8701006;SaulLu;2021-07-16 18:26:54 +0200;Preserve `list` type of `additional_special_tokens` in `special_token_map` (#12759)
* preserve type of `additional_special_tokens` in `special_token_map`

* format

* Update src/transformers/tokenization_utils_base.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

src/transformers/tokenization_utils_base.py
tests/test_tokenization_common.py
==================
fbf1397bf;Funtowicz Morgan;2021-07-16 15:09:15 +0200;Turn on eval mode when exporting to ONNX (#12758)
* Set model in eval mode when exporting to ONNX.

* Disable t5 for now.

* Disable T5 with past too.

* Style.
==

src/transformers/onnx/convert.py
tests/test_onnx_v2.py
==================
8ef3f3656;Suraj Patil;2021-07-16 16:44:59 +0530;fix typos (#12757)

==

examples/research_projects/jax-projects/model_parallel/README.md
examples/research_projects/jax-projects/model_parallel/partitions.py
examples/research_projects/jax-projects/model_parallel/run_clm_mp.py
==================
c07334c12;Nathan Zhou;2021-07-16 05:54:49 -0400;add intel-tensorflow-avx512 to the candidates (#12751)

==

src/transformers/file_utils.py
==================
698926496;Stas Bekman;2021-07-15 16:18:56 -0700;[doc] testing: how to trigger a self-push workflow (#12724)
* [testing] details of how to start self-push workflow

* style

* fix

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

docs/source/testing.rst
==================
a76dd7ee8;Patrick von Platen;2021-07-16 00:16:30 +0100;Update README.md

==

examples/research_projects/jax-projects/README.md
==================
2e9fb13fb;Patrick von Platen;2021-07-15 21:40:25 +0100;[Wav2Vec2] Correctly pad mask indices for PreTraining (#12748)
* fix_torch_device_generate_test

* remove @

* start adding tests

* correct wav2vec2 pretraining

* up

* up

Co-authored-by: Patrick von Platen <patrick@huggingface.co>
==

examples/research_projects/jax-projects/wav2vec2/run_wav2vec2_pretrain_flax.py
examples/research_projects/wav2vec2/run_pretrain.py
src/transformers/models/hubert/modeling_hubert.py
src/transformers/models/wav2vec2/modeling_flax_wav2vec2.py
src/transformers/models/wav2vec2/modeling_wav2vec2.py
tests/test_modeling_flax_wav2vec2.py
tests/test_modeling_wav2vec2.py
==================
5f2791c7c;SaulLu;2021-07-15 18:59:48 +0200;Replace specific tokenizer in log message by AutoTokenizer (#12745)

==

src/transformers/models/deberta_v2/tokenization_deberta_v2.py
src/transformers/models/mpnet/tokenization_mpnet.py
src/transformers/models/prophetnet/tokenization_prophetnet.py
src/transformers/models/roformer/tokenization_roformer.py
==================
31cfcbd3e;Stas Bekman;2021-07-15 09:39:34 -0700;[doc] performance: batch sizes (#12725)

==

docs/source/performance.md
==================
68605e9db;Stas Bekman;2021-07-15 09:38:51 -0700;[doc] parallelism: Which Strategy To Use When (#12712)

==

docs/source/parallelism.md
==================
eb4d7ef97;Lysandre Debut;2021-07-15 17:49:02 +0200;Remove framework mention (#12731)

==

docs/source/serialization.rst
==================
959d448b3;Lysandre Debut;2021-07-15 17:48:50 +0200;Fix led torchscript (#12735)
* Don't test LED on torchscript

* Typo
==

tests/test_modeling_led.py
==================
f03580fb0;Lysandre Debut;2021-07-15 17:48:37 +0200;Fix DETR integration test (#12734)

==

tests/test_modeling_detr.py
==================
f42d9dcc0;Lysandre Debut;2021-07-15 17:40:17 +0200;Patch T5 device test (#12742)

==

tests/test_modeling_t5.py
==================
370be9cc3;Lysandre Debut;2021-07-15 17:39:35 +0200;Fix MBart failing test (#12737)

==

tests/test_modeling_mbart.py
==================
2349ac58c;qqaatw;2021-07-15 23:35:39 +0800;Translate README.md to Traditional Chinese (#12701)
* Add README_zh-tw.md

* Add links to each README.

* Fix a mismatched term.

* Minor improvements.

* Rename language code to be more inclusive.

* Polish terms to make them fluent.

* Remove redundant spaces.

* Fix typo.
==

README.md
README_zh-hans.md
README_zh-hant.md
==================
eb2e006b3;Lysandre Debut;2021-07-15 15:14:12 +0200;Skip test while the model is not available (#12740)

==

tests/test_modeling_megatron_gpt2.py
==================
8c7bd1b97;Lysandre Debut;2021-07-15 15:06:47 +0200;Skip test while the model is not available (#12739)

==

tests/test_modeling_megatron_bert.py
==================
3290315a2;Lysandre Debut;2021-07-15 15:06:12 +0200;Fix AutoModel tests (#12733)

==

tests/test_modeling_auto.py
tests/test_modeling_common.py
==================
01cb2f25e;Lysandre Debut;2021-07-15 14:29:49 +0200;LXMERT integration test typo (#12736)

==

tests/test_modeling_lxmert.py
==================
199b4c526;Sylvain Gugger;2021-07-15 04:17:47 -0400;Init adds its own files as impacted (#12709)

==

utils/tests_fetcher.py
==================
6fb58d30b;Will Rice;2021-07-15 02:44:03 -0400;Fix typo in example (#12716)

==

src/transformers/models/speech_to_text/modeling_speech_to_text.py
==================
8244c5ad4;Patrick von Platen;2021-07-15 07:42:36 +0100;[Flax] Correct shift labels for seq2seq models in Flax (#12720)
* fix_torch_device_generate_test

* remove @

* push

* fix marian

* fix

* up
==

src/transformers/models/bart/modeling_flax_bart.py
src/transformers/models/marian/modeling_flax_marian.py
src/transformers/models/mbart/modeling_flax_mbart.py
src/transformers/models/t5/modeling_flax_t5.py
==================
1a3deae82;Stas Bekman;2021-07-14 15:18:02 -0700;[trainer] release tmp memory in checkpoint load (#12718)
* [trainer] release tmp memory in checkpoint load

* Update src/transformers/trainer.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

src/transformers/trainer.py
==================
a18a17d2b;Stas Bekman;2021-07-14 13:04:58 -0700;[test] split test into 4 sub-tests to avoid timeout (#12710)
* split the test into 4 sub-tests to avoid timeout

* fix decorator order
==

tests/extended/test_trainer_ext.py
==================
44f5b260f;Suraj Patil;2021-07-14 22:55:44 +0530;flax model parallel training (#12590)
* update scripts

* add copyright

* add logging

* cleanup

* add z loss

* add readme

* shard description

* update readme
==

examples/research_projects/jax-projects/model parallel/README.md
examples/research_projects/jax-projects/model parallel/partitions.py
examples/research_projects/jax-projects/model parallel/run_clm_mp.py
==================
79c57e1a0;Matt;2021-07-14 15:59:14 +0100;Deprecate TFTrainer (#12706)
* Deprecate TFTrainer

* Style pass
==

src/transformers/trainer_tf.py
==================
084873b02;Sylvain Gugger;2021-07-14 10:56:55 -0400;Only test the files impacted by changes in the diff (#12644)
* Base test

* More test

* Fix mistake

* Add a docstring change

* Add doc ignore

* Add changes

* Add recursive dep search

* Add recursive dep search

* save

* Finalize test mapping

* Fix bug

* Print prettier

* Ignore comments and empty lines

* Make script runnable from anywhere

* Need dev install

* Like that

* Adapt

* Add as artifact

* Try on torch tests

* Fix yaml error

* Install GitPython

* Apply everywhere

* Be more defensive

* Revert to all tests if something is wrong

* Install GitPython

* Test if there are tests before launching.

* Fixes

* Fixes

* Fixes

* Fixes

* Bash syntax is horrible

* Be less stupid

* Try differently

* Typo

* Typo

* Typo

* Style

* Better name

* Escape quotes

* Ignore black unhelpful re-formatting

* Not a docstring

* Deal with inits in dependency map

* Run all tests once PR is merged.

* Add last job

* Apply suggestions from code review

Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>

* Stronger dependencies gather

* Ignore empty lines too!

* Clean up

* Fix quality

Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>
==

.circleci/config.yml
Makefile
setup.py
src/transformers/dependency_versions_table.py
tests/conftest.py
utils/style_doc.py
utils/tests_fetcher.py
==================
11edecd75;Funtowicz Morgan;2021-07-14 16:30:19 +0200;Fix uninitialized variables when `config.mask_feature_prob > 0` (#12705)

==

src/transformers/models/hubert/modeling_hubert.py
src/transformers/models/wav2vec2/modeling_wav2vec2.py
==================
f9ac677eb;Matt;2021-07-14 15:15:25 +0100;Update TF examples README (#12703)
* Update Transformers README, rename token_classification example to token-classification to be consistent with the others

* Update examples/tensorflow/README.md

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Add README for TF token classification

* Update examples/tensorflow/token-classification/README.md

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update examples/tensorflow/token-classification/README.md

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

examples/tensorflow/README.md
examples/tensorflow/token-classification/README.md
examples/tensorflow/token-classification/run_ner.py
==================
f4399ec57;Patrick von Platen;2021-07-14 12:54:31 +0100;Update README.md

==

examples/flax/language-modeling/README.md
==================
d94773e68;Funtowicz Morgan;2021-07-14 13:17:33 +0200;Provide mask_time_indices to `_mask_hidden_states` to avoid double masking (#12692)
* We need to provide mask_time_indices to `_mask_hidden_states` to avoid applying the mask two times

* apply the same to wav2vec2

* Uniformize the style between hubert and wav2vec2

* fix tf as well

Co-authored-by: patrickvonplaten <patrick.v.platen@gmail.com>
==

src/transformers/models/hubert/modeling_hubert.py
src/transformers/models/hubert/modeling_tf_hubert.py
src/transformers/models/wav2vec2/modeling_tf_wav2vec2.py
src/transformers/models/wav2vec2/modeling_wav2vec2.py
==================
144cea253;Sylvain Gugger;2021-07-14 03:35:18 -0400;Fix multiple choice doc examples (#12679)

==

src/transformers/file_utils.py
==================
5dd0c956a;Stas Bekman;2021-07-13 20:18:51 -0700;non-native optimizers are mostly ok with zero-offload (#12690)

==

docs/source/main_classes/deepspeed.rst
src/transformers/deepspeed.py
tests/deepspeed/test_deepspeed.py
==================
4cdb7ee51;yujun;2021-07-14 05:18:54 +0800;fix #11724 (#11897)

==

src/transformers/models/marian/modeling_tf_marian.py
src/transformers/models/pegasus/modeling_tf_pegasus.py
==================
83f025125;Lysandre Debut;2021-07-13 21:13:18 +0200;Add timeout to CI. (#12684)
* Global 60-300 seconds timeout

* Add verbose option

* [skip ci] typo
==

.github/workflows/self-push.yml
.github/workflows/self-scheduled.yml
setup.py
src/transformers/dependency_versions_table.py
utils/notification_service.py
==================
78f5fe141;Stas Bekman;2021-07-13 12:07:32 -0700;[Deepspeed] adapt multiple models, add zero_to_fp32 tests (#12477)
* zero_to_fp32 tests

* args change

* remove unnecessary work

* use transformers.trainer_utils.get_last_checkpoint

* document the new features

* cleanup

* wip

* fix fsmt

* add bert

* cleanup

* add xlm-roberta

* electra works

* cleanup

* sync

* split off the model zoo tests

* cleanup

* cleanup

* cleanup

* cleanup

* reformat

* cleanup

* casing

* deepspeed>=0.4.3

* adjust distilbert

* Update docs/source/main_classes/deepspeed.rst

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* style

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

docs/source/main_classes/deepspeed.rst
setup.py
src/transformers/dependency_versions_table.py
src/transformers/modeling_utils.py
src/transformers/models/distilbert/modeling_distilbert.py
src/transformers/models/fsmt/modeling_fsmt.py
src/transformers/models/wav2vec2/modeling_wav2vec2.py
src/transformers/training_args.py
tests/deepspeed/test_deepspeed.py
tests/deepspeed/test_model_zoo.py
==================
65bf05cd1;Matt;2021-07-13 19:08:25 +0100;Adding TF translation example (#12667)
* Adding TF translation example

* Fixes and style pass for TF translation example

* Remove unused postprocess_text copied from run_summarization

* Adding README

* Review fixes

* Move changes to model.config to after we've initialized the model
==

examples/tensorflow/translation/README.md
examples/tensorflow/translation/run_translation.py
==================
cee2d2135;Patrick von Platen;2021-07-13 18:53:30 +0100;[Flax Generation] Correct inconsistencies PyTorch/Flax (#12662)
* fix_torch_device_generate_test

* remove @

* correct greedy search

* save intertmed

* add final logits bias

* correct

* up

* add more tests

* fix another bug

* finish tests

* finish marian tests

* up

Co-authored-by: Patrick von Platen <patrick@huggingface.co>
==

docs/source/model_doc/marian.rst
src/transformers/generation_flax_utils.py
src/transformers/generation_utils.py
src/transformers/models/marian/modeling_flax_marian.py
tests/test_generation_flax_utils.py
tests/test_modeling_flax_marian.py
tests/test_modeling_flax_t5.py
==================
7a22a02a7;Stas Bekman;2021-07-13 09:19:04 -0700;[tokenizer.prepare_seq2seq_batch] change deprecation to be easily actionable (#12669)
* change deprecation to be easily actionable

* Update src/transformers/tokenization_utils_base.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* rework as suggested

* one warning together

* fix format

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

src/transformers/tokenization_utils_base.py
==================
711d901c4;qqaatw;2021-07-14 00:08:15 +0800;Fix minor docstring typos. (#12682)

==

src/transformers/modeling_flax_utils.py
src/transformers/modeling_tf_utils.py
src/transformers/modeling_utils.py
==================
90178b0ce;Sylvain Gugger;2021-07-13 10:15:15 -0400;Add option to load a pretrained model with mismatched shapes (#12664)
* Add option to load a pretrained model with mismatched shapes

* Fail at loading when mismatched shapes in Flax

* Fix tests

* Update src/transformers/modeling_flax_utils.py

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* Address review comments

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
==

src/transformers/modeling_flax_utils.py
src/transformers/modeling_tf_utils.py
src/transformers/modeling_utils.py
src/transformers/models/auto/configuration_auto.py
src/transformers/models/deberta/modeling_deberta.py
src/transformers/models/deberta_v2/modeling_deberta_v2.py
tests/test_modeling_common.py
tests/test_modeling_flax_common.py
tests/test_modeling_tf_common.py
==================
7f6d37502;Patrick von Platen;2021-07-13 14:17:31 +0100;[Blenderbot] Fix docs (#12227)
* fix_torch_device_generate_test

* remove @

* fix docs
==

src/transformers/models/blenderbot/configuration_blenderbot.py
==================
9519f0cd6;Jeroen Steggink;2021-07-13 14:40:27 +0200;Wrong model is used in example, should be character instead of subword model (#12676)
* Wrong model is used, should be character instead of subword

In the original Google repo for CANINE there was mixup in the model names in the README.md, which was fixed 2 weeks ago. Since this transformer model was created before, it probably resulted in wrong use in this example.

s = subword, c = character

* canine.rst style fix

* Update docs/source/model_doc/canine.rst

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Styling canine.rst

* Added links to model cards.

* Fixed links to model cards.

Co-authored-by: Jeroen Steggink <978411+jsteggink@users.noreply.github.com>
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

docs/source/model_doc/canine.rst
==================
5803a2a7a;Nick Doiron;2021-07-13 08:39:57 -0400;Add ByT5 option to example run_t5_mlm_flax.py (#12634)
* Allow ByT5 type in Flax T5 script

* use T5TokenizerFast

* change up tokenizer config

* model_args

* reorder imports

* Update run_t5_mlm_flax.py
==

examples/flax/language-modeling/run_t5_mlm_flax.py
==================
9da1acaea;Lysandre Debut;2021-07-13 12:31:56 +0200;**encode_plus() shouldn't run for  W2V2CTC (#12655)
* **encode_plus() shouldn't run for  W2V2CTC

* Typo
==

tests/test_tokenization_wav2vec2.py
==================
a6938c472;Lysandre Debut;2021-07-13 08:53:06 +0200;Patch BigBird tokenization test (#12653)

==

tests/test_tokenization_big_bird.py
==================
c523b241c;Omar Sanseviero;2021-07-12 21:24:58 +0200;Update timeline for Flax event evaluation

==

examples/research_projects/jax-projects/README.md
==================
dc06e4358;Kevin Canwen Xu;2021-07-13 01:50:12 +0800;Fix typo in README_zh-hans.md (#12663)

==

README_zh-hans.md
==================
9d771c547;Kevin Canwen Xu;2021-07-13 01:19:54 +0800;Translate README.md to Simplified Chinese (#12596)
* README Translation for Chinese (Simplified)

* update link

* h3->h4

* html refactor

* update model list

* fix

* Add a translation guide

* format

* update

* typo

* Refine wording
==

README.md
README_zh-hans.md
==================
21a81c1e3;Philip May;2021-07-12 18:24:32 +0200;fix typo in modeling_t5.py docstring (#12640)

==

src/transformers/models/t5/modeling_t5.py
==================
b90d49937;Ahmed Khaled;2021-07-12 18:03:13 +0200;fixed docs (#12646)

==

docs/source/training.rst
==================
da0e9ee69;Philipp Schmid;2021-07-12 18:02:51 +0200;remove documentation (#12657)

==

docs/source/sagemaker.md
==================
b189226e8;Lysandre Debut;2021-07-12 17:51:35 +0200;Fix transfo xl integration test (#12652)
* Cleanup test

* Skip TF TransfoXL test
==

tests/test_modeling_tf_transfo_xl.py
tests/test_modeling_transfo_xl.py
==================
fd41e2daf;Lysandre Debut;2021-07-12 17:42:59 +0200;Pipeline should be agnostic (#12656)

==

tests/test_pipelines_question_answering.py
==================
9b3aab2cc;Sylvain Gugger;2021-07-12 11:15:54 -0400;Pickle auto models (#12654)
* PoC, it pickles!

* Remove old method.

* Apply to every auto object
==

src/transformers/file_utils.py
src/transformers/models/auto/auto_factory.py
src/transformers/models/auto/modeling_auto.py
src/transformers/models/auto/modeling_flax_auto.py
src/transformers/models/auto/modeling_tf_auto.py
==================
379f64943;Matt;2021-07-12 15:58:38 +0100;TF summarization example (#12617)
* Adding a TF summarization example

* Style pass

* Style fixes

* Updates for review comments

* Adding README

* Style pass

* Remove unused import
==

examples/tensorflow/summarization/README.md
examples/tensorflow/summarization/run_summarization.py
==================
0f43e742d;Sylvain Gugger;2021-07-12 10:32:51 -0400;Fix typo

==

src/transformers/tokenization_utils_base.py
==================
9adff7a0f;Sylvain Gugger;2021-07-12 09:57:54 -0400;Fix syntax in conda file

==

.github/conda/meta.yaml
==================
ad4205427;Sylvain Gugger;2021-07-12 09:55:36 -0400;Minimum requirement for pyyaml

==

.github/conda/meta.yaml
setup.py
src/transformers/dependency_versions_table.py
==================
fb5665b5a;Lysandre Debut;2021-07-12 15:47:05 +0200;The extended trainer tests should require torch (#12650)

==

tests/extended/test_trainer_ext.py
==================
0af8579bb;Lysandre Debut;2021-07-12 15:11:32 +0200;Skip TestMarian_MT_EN (#12649)
* Skip TestMarian_MT_EN

* Skip EN_ZH and EN_ROMANCE

* Skip EN_ROMANCE pipeline
==

tests/test_modeling_tf_marian.py
==================
a882b9fac;Lewis Bails;2021-07-12 13:51:58 +0200;Add tokenizer_file parameter to PreTrainedTokenizerFast docstring (#12624)
Co-authored-by: Lewis Bails <Lewis.Bails@infomedia.dk>
==

src/transformers/tokenization_utils_fast.py
==================
f8f9a679a;Suraj Patil;2021-07-12 15:18:43 +0530;fix type check (#12638)

==

src/transformers/file_utils.py
==================
2dd9440d0;Eduardo Gonzalez Ponferrada;2021-07-11 23:46:22 -0700;Point to the right file for hybrid CLIP (#12599)

==

examples/research_projects/jax-projects/hybrid_clip/README.md
==================
de23ecea3;Bhadresh Savani;2021-07-12 12:15:14 +0530;added test file (#12630)

==

examples/flax/summarization/run_summarization_flax.py
==================
9ee66adad;Stas Bekman;2021-07-09 18:48:28 -0700;fix anchor (#12620)

==

docs/source/parallelism.md
==================
0dcc3c86e;Stas Bekman;2021-07-09 17:39:09 -0700;[doc] DP/PP/TP/etc parallelism (#12524)
* wip

* complete the doc

* missing img

* improve

* correction

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

docs/source/imgs/parallelism-deepspeed-3d.png
docs/source/imgs/parallelism-flexflow.jpeg
docs/source/imgs/parallelism-gpipe-bubble.png
docs/source/imgs/parallelism-sagemaker-interleaved-pipeline.png
docs/source/imgs/parallelism-tp-independent-gelu.png
docs/source/imgs/parallelism-tp-parallel_gemm.png
docs/source/imgs/parallelism-tp-parallel_self_attention.png
docs/source/imgs/parallelism-tp-parallel_shard_processing.png
docs/source/imgs/parallelism-zero-dp-pp.png
docs/source/imgs/parallelism-zero.png
docs/source/index.rst
docs/source/parallelism.md
==================
4cdbf63c0;Stas Bekman;2021-07-09 17:38:28 -0700;[debugging utils] minor doc improvements (#12525)

==

src/transformers/debug_utils.py
==================
fb65f65ea;Will Rice;2021-07-09 13:55:25 -0400;Add TFHubertModel (#12206)
* TFHubert

* Update with TFWav2Vec Bug Fixes

* Add OOV Error

* Feedback changes

* Fix kwargs call
==

docs/source/index.rst
docs/source/model_doc/hubert.rst
src/transformers/__init__.py
src/transformers/models/auto/modeling_tf_auto.py
src/transformers/models/hubert/__init__.py
src/transformers/models/hubert/modeling_tf_hubert.py
src/transformers/models/wav2vec2/modeling_tf_wav2vec2.py
src/transformers/utils/dummy_tf_objects.py
tests/test_modeling_tf_hubert.py
utils/check_repo.py
==================
934222e3c;Patrick von Platen;2021-07-09 18:28:57 +0100;[FLax] Fix marian docs 2 (#12615)
* fix_torch_device_generate_test

* remove @

* up
==

docs/source/model_doc/marian.rst
src/transformers/models/marian/modeling_flax_marian.py
==================
165606e5b;Patrick von Platen;2021-07-09 18:01:58 +0100;[Flax Marian] Add marian flax example (#12614)
* fix_torch_device_generate_test

* remove @

* finish better examples for marian flax
==

docs/source/model_doc/marian.rst
src/transformers/models/marian/modeling_flax_marian.py
==================
51eb6d345;Patrick von Platen;2021-07-09 17:33:04 +0100;[Flax] Fix mt5 auto (#12612)
* fix_torch_device_generate_test

* remove @

* fix mt5 auto
==

src/transformers/models/auto/modeling_flax_auto.py
==================
e7f33e8cb;Alex Hedges;2021-07-09 09:24:55 -0400;Pass `model_kwargs` when loading a model in `pipeline()` (#12449)
* Pass model_kwargs when loading a model in pipeline

* Add test for model_kwargs parameter of pipeline()

* Rewrite test to not download model

* Fix failing style checks
==

src/transformers/pipelines/__init__.py
tests/test_pipelines_token_classification.py
==================
18ca59e1d;Sylvain Gugger;2021-07-09 09:24:42 -0400;Fix arg count for partial functions (#12609)

==

src/transformers/trainer.py
src/transformers/trainer_utils.py
==================
0cc2dc245;Sylvain Gugger;2021-07-09 09:02:34 -0400;Simplify unk token (#12582)
* Base test

* More test

* Fix mistake

* Add a docstring change

* Add doc ignore

* Simplify logic for unk token in Unigram tokenizers

* Remove changes from otehr branch
==

src/transformers/tokenization_utils_fast.py
==================
deecdd493;Patrick von Platen;2021-07-09 13:51:28 +0100;[Flax] Fix cur step flax examples (#12608)
* fix_torch_device_generate_test

* remove @

* fix save problem
==

examples/flax/language-modeling/run_clm_flax.py
examples/flax/language-modeling/run_mlm_flax.py
examples/flax/language-modeling/run_t5_mlm_flax.py
==================
65e27215b;Patrick von Platen;2021-07-09 11:42:13 +0100;[Flax] Add flax marian (#12595)
* fix_torch_device_generate_test

* remove @

* add marian

* finish make style

* add model

* add docs

* add test

* add integration tests

* up

* solve bug

* correct tests

* correct some tests

* Apply suggestions from code review

Co-authored-by: Suraj Patil <surajp815@gmail.com>
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* correct adapt marian

* finish

Co-authored-by: Patrick von Platen <patrick@huggingface.co>
Co-authored-by: Suraj Patil <surajp815@gmail.com>
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

docs/source/index.rst
docs/source/model_doc/marian.rst
src/transformers/__init__.py
src/transformers/generation_flax_utils.py
src/transformers/models/auto/modeling_flax_auto.py
src/transformers/models/marian/__init__.py
src/transformers/models/marian/modeling_flax_marian.py
src/transformers/utils/dummy_flax_objects.py
tests/test_modeling_flax_marian.py
==================
cc12e1dbf;Nicolas Patry;2021-07-09 09:36:05 +0200;This will reduce "Already borrowed error": (#12550)
* This will reduce "Already borrowed error":

Original issue https://github.com/huggingface/tokenizers/issues/537

The original issue is caused by transformers calling many times
mutable functions on the rust tokenizers.
Rust needs to guarantee that only 1 agent has a mutable reference
to memory at a given time (for many reasons which don't need explaining
here). Usually, the rust compiler can guarantee that this property is
true at compile time.

Unfortunately, this is impossible for Python to do that, so PyO3, the
bridge between rust and python used by `tokenizers`, will change the
compile guarantee for a dynamic guarantee, so if multiple agents try
to have multiple mutable borrows at the same time, then the runtime will
yell with "Already borrowed".

The proposed fix here in transformers, is simply to reduce the actual
number of calls that really need mutable borrows. By reducing them,
we reduce the risk of running into "Already borrowed" error.
The caveat is now we add a call to read the current configuration of the
`_tokenizer`, so worst case we have 2 calls instead of 1, and best case
we simply have 1 + a Python comparison of a dict (should be negligible).

* Adding a test.

* trivial error :(.

* Update tests/test_tokenization_fast.py

Co-authored-by: SaulLu <55560583+SaulLu@users.noreply.github.com>

* Adding reference to original issues in the tests.

* Update the tests with fast tokenizer.

Co-authored-by: SaulLu <55560583+SaulLu@users.noreply.github.com>
==

src/transformers/tokenization_utils_fast.py
tests/test_tokenization_fast.py
==================
8fe836af5;Omar Sanseviero;2021-07-09 08:52:30 +0200;Add Flax sprint project evaluation section (#12592)

==

examples/research_projects/jax-projects/README.md
==================
ce111feed;Stas Bekman;2021-07-08 14:11:01 -0700;[doc] fix broken ref (#12597)

==

src/transformers/models/xlm_prophetnet/tokenization_xlm_prophetnet.py
src/transformers/models/xlm_roberta/tokenization_xlm_roberta.py
src/transformers/models/xlm_roberta/tokenization_xlm_roberta_fast.py
==================
f0dde6012;Stas Bekman;2021-07-08 08:17:51 -0700;[model.from_pretrained] raise exception early on failed load (#12574)
* [model.from_pretrained] raise exception early on failed load

Currently if `load` pretrained weights fails in `from_pretrained`, we first print a whole bunch of successful messages and then fail - this PR puts the exception first to avoid all the misleading messages.

* style

Co-authored-by: Suraj Patil <surajp815@gmail.com>
==

src/transformers/modeling_utils.py
==================
75e63dbf7;Sylvain Gugger;2021-07-08 11:12:18 -0400;Fix MT5 init (#12591)

==

src/transformers/models/mt5/__init__.py
==================
4da568c15;Nicolas Patry;2021-07-08 16:58:15 +0200;Fixing the pipeline optimization by reindexing targets (V2) (#12330)
* Fixing the pipeline optimization by rescaling the logits first.

* Add test for target equivalence

Co-authored-by: Lysandre <lysandre.debut@reseau.eseo.fr>
==

src/transformers/pipelines/fill_mask.py
tests/test_pipelines_fill_mask.py
==================
2aa3cd935;Funtowicz Morgan;2021-07-08 16:54:42 +0200;[RFC] Laying down building stone for more flexible ONNX export capabilities (#11786)
* Laying down building stone for more flexible ONNX export capabilities

* Ability to provide a map of config key to override before exporting.

* Makes it possible to export BART with/without past keys.

* Supports simple mathematical syntax for OnnxVariable.repeated

* Effectively apply value override from onnx config for model

* Supports export with additional features such as with-past for seq2seq

* Store the output path directly in the args for uniform usage across.

* Make BART_ONNX_CONFIG_* constants and fix imports.

* Support BERT model.

* Use tokenizer for more flexibility in defining the inputs of a model.

* Add TODO as remainder to provide the batch/sequence_length as CLI args

* Enable optimizations to be done on the model.

* Enable GPT2 + past

* Improve model validation with outputs containing nested structures

* Enable Roberta

* Enable Albert

* Albert requires opset >= 12

* BERT-like models requires opset >= 12

* Remove double printing.

* Enable XLM-Roberta

* Enable DistilBERT

* Disable optimization by default

* Fix missing setattr when applying optimizer_features

* Add value field to OnnxVariable to define constant input (not from tokenizers)

* Add T5 support.

* Simplify model type retrieval

* Example exporting token_classification pipeline for DistilBERT.

* Refactoring to package `transformers.onnx`

* Solve circular dependency & __main__

* Remove unnecessary imports in `__init__`

* Licences

* Use @Narsil's suggestion to forward the model's configuration to the ONNXConfig to avoid interpolation.

* Onnx export v2 fixes (#12388)

* Tiny fixes
Remove `convert_pytorch` from onnxruntime-less runtimes
Correct reference to model

* Style

* Fix Copied from

* LongFormer ONNX config.

* Removed optimizations

* Remvoe bad merge relicas.

* Remove unused constants.

* Remove some deleted constants from imports.

* Fix unittest to remove usage of PyTorch model for onnx.utils.

* Fix distilbert export

* Enable ONNX export test for supported model.

* Style.

* Fix lint.

* Enable all supported default models.

* GPT2 only has one output

* Fix bad property name when overriding config.

* Added unittests and docstrings.

* Disable with_past tests for now.

* Enable outputs validation for default export.

* Remove graph opt lvls.

* Last commit with on-going past commented.

* Style.

* Disabled `with_past` for now

* Remove unused imports.

* Remove framework argument

* Remove TFPreTrainedModel reference

* Add documentation

* Add onnxruntime tests to CircleCI

* Add test

* Rename `convert_pytorch` to `export`

* Use OrderedDict for dummy inputs

* WIP Wav2Vec2

* Revert "WIP Wav2Vec2"

This reverts commit f665efb04c92525c3530e589029f0ae7afdf603e.

* Style

* Use OrderedDict for I/O

* Style.

* Specify OrderedDict documentation.

* Style :)

Co-authored-by: Lysandre <lysandre.debut@reseau.eseo.fr>
Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
==

.circleci/config.yml
docs/source/serialization.rst
src/transformers/file_utils.py
src/transformers/models/albert/__init__.py
src/transformers/models/albert/configuration_albert.py
src/transformers/models/bart/__init__.py
src/transformers/models/bart/configuration_bart.py
src/transformers/models/bert/__init__.py
src/transformers/models/bert/configuration_bert.py
src/transformers/models/distilbert/__init__.py
src/transformers/models/distilbert/configuration_distilbert.py
src/transformers/models/gpt2/__init__.py
src/transformers/models/gpt2/configuration_gpt2.py
src/transformers/models/longformer/__init__.py
src/transformers/models/longformer/configuration_longformer.py
src/transformers/models/roberta/__init__.py
src/transformers/models/roberta/configuration_roberta.py
src/transformers/models/t5/__init__.py
src/transformers/models/t5/configuration_t5.py
src/transformers/models/xlm_roberta/__init__.py
src/transformers/models/xlm_roberta/configuration_xlm_roberta.py
src/transformers/onnx/__init__.py
src/transformers/onnx/__main__.py
src/transformers/onnx/config.py
src/transformers/onnx/convert.py
src/transformers/onnx/utils.py
src/transformers/testing_utils.py
tests/test_modeling_tf_common.py
tests/test_onnx_v2.py
==================
0085e712d;Sylvain Gugger;2021-07-08 07:24:46 -0400;Don't stop at num_epochs when using IterableDataset (#12561)

==

src/transformers/trainer.py
==================
6f1adc433;Sylvain Gugger;2021-07-08 07:23:41 -0400;Fix group_lengths for short datasets (#12558)

==

examples/flax/language-modeling/run_clm_flax.py
examples/flax/language-modeling/run_mlm_flax.py
examples/flax/language-modeling/run_t5_mlm_flax.py
examples/pytorch/language-modeling/run_clm.py
examples/pytorch/language-modeling/run_clm_no_trainer.py
examples/pytorch/language-modeling/run_mlm.py
examples/pytorch/language-modeling/run_mlm_no_trainer.py
examples/pytorch/language-modeling/run_plm.py
examples/tensorflow/language-modeling/run_clm.py
examples/tensorflow/language-modeling/run_mlm.py
==================
0a6b9048d;Sylvain Gugger;2021-07-08 07:20:46 -0400;Init pickle (#12567)
* Try to pickle transformers

* Deal with special objs better

* Make picklable
==

docs/source/internal/file_utils.rst
src/transformers/__init__.py
src/transformers/file_utils.py
src/transformers/models/albert/__init__.py
src/transformers/models/auto/__init__.py
src/transformers/models/bart/__init__.py
src/transformers/models/barthez/__init__.py
src/transformers/models/bert/__init__.py
src/transformers/models/bert_generation/__init__.py
src/transformers/models/bert_japanese/__init__.py
src/transformers/models/bertweet/__init__.py
src/transformers/models/big_bird/__init__.py
src/transformers/models/bigbird_pegasus/__init__.py
src/transformers/models/blenderbot/__init__.py
src/transformers/models/blenderbot_small/__init__.py
src/transformers/models/byt5/__init__.py
src/transformers/models/camembert/__init__.py
src/transformers/models/canine/__init__.py
src/transformers/models/clip/__init__.py
src/transformers/models/convbert/__init__.py
src/transformers/models/cpm/__init__.py
src/transformers/models/ctrl/__init__.py
src/transformers/models/deberta/__init__.py
src/transformers/models/deberta_v2/__init__.py
src/transformers/models/deit/__init__.py
src/transformers/models/detr/__init__.py
src/transformers/models/distilbert/__init__.py
src/transformers/models/dpr/__init__.py
src/transformers/models/electra/__init__.py
src/transformers/models/encoder_decoder/__init__.py
src/transformers/models/flaubert/__init__.py
src/transformers/models/fsmt/__init__.py
src/transformers/models/funnel/__init__.py
src/transformers/models/gpt2/__init__.py
src/transformers/models/gpt_neo/__init__.py
src/transformers/models/herbert/__init__.py
src/transformers/models/hubert/__init__.py
src/transformers/models/ibert/__init__.py
src/transformers/models/layoutlm/__init__.py
src/transformers/models/led/__init__.py
src/transformers/models/longformer/__init__.py
src/transformers/models/luke/__init__.py
src/transformers/models/lxmert/__init__.py
src/transformers/models/m2m_100/__init__.py
src/transformers/models/marian/__init__.py
src/transformers/models/mbart/__init__.py
src/transformers/models/megatron_bert/__init__.py
src/transformers/models/mmbt/__init__.py
src/transformers/models/mobilebert/__init__.py
src/transformers/models/mpnet/__init__.py
src/transformers/models/mt5/__init__.py
src/transformers/models/openai/__init__.py
src/transformers/models/pegasus/__init__.py
src/transformers/models/phobert/__init__.py
src/transformers/models/prophetnet/__init__.py
src/transformers/models/rag/__init__.py
src/transformers/models/reformer/__init__.py
src/transformers/models/retribert/__init__.py
src/transformers/models/roberta/__init__.py
src/transformers/models/roformer/__init__.py
src/transformers/models/speech_to_text/__init__.py
src/transformers/models/squeezebert/__init__.py
src/transformers/models/t5/__init__.py
src/transformers/models/tapas/__init__.py
src/transformers/models/transfo_xl/__init__.py
src/transformers/models/visual_bert/__init__.py
src/transformers/models/vit/__init__.py
src/transformers/models/wav2vec2/__init__.py
src/transformers/models/xlm/__init__.py
src/transformers/models/xlm_roberta/__init__.py
src/transformers/models/xlnet/__init__.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/__init__.py
==================
b29c39458;Hwijeen Ahn;2021-07-08 17:17:34 +0900;raise exception when arguments to pipeline are incomplete (#12548)
* raise exception when arguments are incomplete

* change exception to runtime error
==

src/transformers/pipelines/__init__.py
==================
122d7dc34;Ibraheem Moosa;2021-07-08 04:05:47 +0600;Remove logging of GPU count etc logging. (#12569)
Successfully logging this requires Pytorch. For the purposes of this script we are not using Pytorch.
==

examples/flax/language-modeling/run_t5_mlm_flax.py
==================
d7e156bd1;Suraj Patil;2021-07-07 22:50:27 +0530;fix loading clip vision model (#12566)

==

examples/research_projects/jax-projects/hybrid_clip/configuration_hybrid_clip.py
==================
b86826099;Sylvain Gugger;2021-07-07 12:50:41 -0400;Double check for attribute num_examples (#12562)
* Double check for attribute

* Use right name
==

src/transformers/trainer.py
==================
0d2bffad3;Michal Szutenberg;2021-07-07 17:17:30 +0200;Remove tf.roll wherever not needed (#12512)
It was used in shift_right.
After this change TF code is more similar to Pytorch implementations
Also, TF graphs are optimized (one node less)
==

src/transformers/generation_tf_utils.py
src/transformers/models/bart/modeling_tf_bart.py
src/transformers/models/blenderbot/modeling_tf_blenderbot.py
src/transformers/models/blenderbot_small/modeling_tf_blenderbot_small.py
src/transformers/models/led/modeling_tf_led.py
src/transformers/models/marian/modeling_tf_marian.py
src/transformers/models/pegasus/modeling_tf_pegasus.py
src/transformers/models/rag/modeling_tf_rag.py
src/transformers/models/t5/modeling_tf_t5.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_tf_{{cookiecutter.lowercase_modelname}}.py
==================
95425d546;Matt;2021-07-07 15:30:47 +0100;Adding prepare_decoder_input_ids_from_labels methods to all ConditionalGeneration TF models (#12560)

==

src/transformers/models/bart/modeling_tf_bart.py
src/transformers/models/led/modeling_tf_led.py
src/transformers/models/marian/modeling_tf_marian.py
src/transformers/models/mbart/modeling_tf_mbart.py
src/transformers/models/pegasus/modeling_tf_pegasus.py
src/transformers/models/t5/modeling_tf_t5.py
==================
ebc69afc3;Nicolas Patry;2021-07-07 16:06:48 +0200;Adding support for `pipeline("automatic-speech-recognition")`. (#11525)
* Adding support for `pipeline("automatic-speech-recognition")`.

- Ugly `"config"` choice for AutoModel. It would be great to have the
possibility to have something like `AutoModelFor` that would implement
the same logic (Load the config, check Architectures and load the first
one)

* Remove `model_id` was not needed in the end.

* Rebased !

* Remove old code.

* Rename `nlp`.
==

src/transformers/pipelines/__init__.py
src/transformers/pipelines/base.py
tests/test_pipelines_automatic_speech_recognition.py
==================
7d321b768;Patrick von Platen;2021-07-07 14:43:43 +0100;[Flax] Allow retraining from save checkpoint (#12559)
* fix_torch_device_generate_test

* remove @

* finish
==

examples/flax/language-modeling/run_mlm_flax.py
examples/flax/language-modeling/run_t5_mlm_flax.py
examples/research_projects/jax-projects/dataset-streaming/run_mlm_flax_stream.py
==================
1d6623c6a;Souvic Chakraborty;2021-07-07 18:35:44 +0530;MLM training fails with no validation file(same as #12406 for pytorch now) (#12517)
* Validation split percentage to be used for custom data files also

Issue same as https://github.com/huggingface/transformers/issues/12406 fixed for pytorch branch run_mlm.py

* Validation split added in the right place

* Update run_clm.py

* validation split added for custom files

* Validation split added for custom files

* Update run_plm.py

* fixed validation split for custom files as input for pytorch examples in lm

* Update run_clm_no_trainer.py

* args modified
==

examples/pytorch/language-modeling/run_clm.py
examples/pytorch/language-modeling/run_clm_no_trainer.py
examples/pytorch/language-modeling/run_mlm.py
examples/pytorch/language-modeling/run_mlm_no_trainer.py
examples/pytorch/language-modeling/run_plm.py
==================
3488ef5a9;shabie;2021-07-07 14:07:46 +0200;[trainer] add option to ignore keys for the train function too (#11719) (#12551)

==

src/transformers/trainer.py
==================
45dcfdec5;Kevin Canwen Xu;2021-07-07 16:32:48 +0800;Add a warning for broken ProphetNet fine-tuning (#12511)

==

src/transformers/models/prophetnet/modeling_prophetnet.py
==================
61400e1ec;Daniel Stancl;2021-07-07 08:50:38 +0200;[Flax] Add FlaxMBart (#12236)
* Copy BART to MBart and rename some stuff

* Add copy statements pointing to FlaxBart

* Update/add some common files

* Update shift_tokens_rigth + fix imports

* Fix shift_tokens_right method according to MBart implementation

* Update shift_tokens_right in tests accordingly

* Fix the import issue and update docs file
* make style quality

* Do some minor changes according to patil-suraj suggestions

* Change the order of normalization layer and attention

* Add some copu statementes

* Update generate method and add integration test for mBart

* Make a few updates after a review

Besides, add `lang_code_to_id` to MBartTokenizeFast

* fix-copies; make style quality

* Apply suggestions from code review

* Apply suggestions from code review

* Apply suggestions from code review

* fix output type, style

* add copied from

* resolve conflicts

Co-authored-by: Suraj Patil <surajp815@gmail.com>
==

docs/source/index.rst
docs/source/model_doc/mbart.rst
src/transformers/__init__.py
src/transformers/models/auto/modeling_flax_auto.py
src/transformers/models/mbart/__init__.py
src/transformers/models/mbart/modeling_flax_mbart.py
src/transformers/models/mbart/tokenization_mbart_fast.py
src/transformers/utils/dummy_flax_objects.py
tests/test_modeling_flax_mbart.py
==================
2d42915ab;Suraj Patil;2021-07-07 11:50:30 +0530;[examples/flax] add adafactor optimizer (#12544)
* add adafactor

* Update examples/flax/language-modeling/run_mlm_flax.py

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
==

examples/flax/language-modeling/requirements.txt
examples/flax/language-modeling/run_clm_flax.py
examples/flax/language-modeling/run_mlm_flax.py
examples/flax/language-modeling/run_t5_mlm_flax.py
==================
208df208b;Patrick von Platen;2021-07-06 19:41:51 +0100;[Flax] Adapt examples to be able to use eval_steps and save_steps (#12543)
* fix_torch_device_generate_test

* remove @

* up

* up

* correct

* upload

Co-authored-by: Patrick von Platen <patrick@huggingface.co>
==

examples/flax/language-modeling/README.md
examples/flax/language-modeling/run_clm_flax.py
examples/flax/language-modeling/run_mlm_flax.py
examples/flax/language-modeling/run_t5_mlm_flax.py
==================
2870fd198;Lysandre;2021-07-06 17:46:39 +0200;Bump CircleCI machine sizes

==

.circleci/config.yml
==================
3fd85777e;sadakmed;2021-07-06 17:44:47 +0200;implementing tflxmertmodel integration test (#12497)
* implementing tflxmertmodel integration test

* move import

* revert and fix
==

tests/test_modeling_tf_lxmert.py
==================
09af5bdea;SaulLu;2021-07-06 17:31:45 +0200;Replace `nn.Moudle` by `nn.Module` (#12541)

==

examples/research_projects/jax-projects/README.md
==================
f42a0abf4;Patrick von Platen;2021-07-06 15:14:48 +0100;Update README.md

==

examples/research_projects/jax-projects/wav2vec2/README.md
==================
029b9d3f4;Suzana Iliƒá;2021-07-06 16:12:16 +0200;Update README (#12540)

==

examples/research_projects/jax-projects/README.md
==================
7a259c190;Suraj Patil;2021-07-06 18:55:18 +0530;FlaxGPTNeo (#12493)
* flax gpt neo

* fix query scaling

* update generation test

* use flax model for test
==

docs/source/index.rst
docs/source/model_doc/gpt_neo.rst
src/transformers/__init__.py
src/transformers/models/auto/modeling_flax_auto.py
src/transformers/models/gpt_neo/__init__.py
src/transformers/models/gpt_neo/modeling_flax_gpt_neo.py
src/transformers/utils/dummy_flax_objects.py
tests/test_modeling_flax_gpt_neo.py
==================
626a0a014;yujun;2021-07-06 15:31:57 +0800;[RoFormer] Fix some issues (#12397)
* add RoFormerTokenizerFast into AutoTokenizer

* fix typo in roformer docs

* make onnx export happy

* update RoFormerConfig embedding_size

* use jieba not rjieba

* fix 12244 and make test_alignement passed

* update ARCHIVE_MAP

* make style & quality & fixup

* update

* make style & quality & fixup

* make style quality fixup

* update

* suggestion from LysandreJik

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* make style

* use rjieba

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
==

docs/source/model_doc/roformer.rst
src/transformers/file_utils.py
src/transformers/models/auto/tokenization_auto.py
src/transformers/models/roformer/configuration_roformer.py
src/transformers/models/roformer/modeling_roformer.py
src/transformers/models/roformer/modeling_tf_roformer.py
src/transformers/models/roformer/tokenization_roformer.py
src/transformers/models/roformer/tokenization_roformer_fast.py
src/transformers/models/roformer/tokenization_utils.py
src/transformers/testing_utils.py
tests/test_tokenization_roformer.py
==================
f5b0c1ecf;Suraj Patil;2021-07-06 11:12:47 +0530;[Flax] Fix hybrid clip (#12519)
* fix saving and loading

* update readme
==

examples/research_projects/jax-projects/hybrid_clip/README.md
examples/research_projects/jax-projects/hybrid_clip/configuration_hybrid_clip.py
examples/research_projects/jax-projects/hybrid_clip/modeling_hybrid_clip.py
examples/research_projects/jax-projects/hybrid_clip/run_hybrid_clip.py
==================
7d6285a92;Patrick von Platen;2021-07-05 23:49:47 +0100;[Wav2Vec2] Flax - Adapt wav2vec2 script (#12520)
* fix_torch_device_generate_test

* remove @

* adapt flax pretrain script
==

examples/research_projects/jax-projects/wav2vec2/run_wav2vec2_pretrain_flax.py
==================
4605b2b8e;Patrick von Platen;2021-07-05 18:35:22 +0100;[Flax] Fix another bug in logging steps (#12516)
* fix_torch_device_generate_test

* remove @

* up
==

examples/flax/language-modeling/run_mlm_flax.py
examples/flax/language-modeling/run_t5_mlm_flax.py
==================
d0f7508ab;Patrick von Platen;2021-07-05 18:21:00 +0100;[Flax] Correct logging steps flax (#12515)
* fix_torch_device_generate_test

* remove @

* push
==

examples/flax/language-modeling/run_clm_flax.py
examples/flax/language-modeling/run_mlm_flax.py
examples/flax/language-modeling/run_t5_mlm_flax.py
==================
bb4ac2b5a;Patrick von Platen;2021-07-05 18:14:50 +0100;[Flax] Correct flax training scripts (#12514)
* fix_torch_device_generate_test

* remove @

* add logging steps

* correct training scripts

* correct readme

* correct
==

examples/flax/language-modeling/README.md
examples/flax/language-modeling/run_clm_flax.py
examples/flax/language-modeling/run_mlm_flax.py
examples/flax/language-modeling/run_t5_mlm_flax.py
==================
ea5567502;Matt;2021-07-05 15:42:18 +0100;NER example for Tensorflow (#12469)
* NER example for Tensorflow

* Style pass

* Style pass

* Added metric computation on the evaluation set

* Style pass

* Fixed label masking

* Style pass

* Style pass
==

examples/tensorflow/token_classification/run_ner.py
==================
9b9081055;Patrick von Platen;2021-07-05 15:13:10 +0100;[Flax] Dataset streaming example (#12470)
* fix_torch_device_generate_test

* remove @

* upload

* finish dataset streaming

* adapt readme

* finish

* up

* up

* up

* up

* Apply suggestions from code review

* finish

* make style

* make style2

* finish

Co-authored-by: Patrick von Platen <patrick@huggingface.co>
==

examples/research_projects/jax-projects/dataset-streaming/README.md
examples/research_projects/jax-projects/dataset-streaming/run_mlm_flax_stream.py
==================
eceb1042c;Navjot;2021-07-05 07:03:14 -0700;flax.linen.apply takes state as the first param, followed by the input (#12510)

==

examples/research_projects/jax-projects/README.md
==================
f1c81d6b9;Suraj Patil;2021-07-05 18:23:03 +0530;[Flax] ViT training example (#12300)
* begin script

* clean example, add readme

* update readme

* remove decay mask

* remove masking

* update readme & make flake happy
==

examples/flax/vision/README.md
examples/flax/vision/requirements.txt
examples/flax/vision/run_image_classification.py
==================
e799e0f1e;Akmal;2021-07-05 19:35:20 +0700;[Flax] Fix wav2vec2 pretrain arguments (#12498)

==

examples/research_projects/jax-projects/wav2vec2/run_wav2vec2_pretrain_flax.py
==================
0e1718afb;sadakmed;2021-07-05 11:21:25 +0200;create LxmertModelIntegrationTest Pytorch (#9989)
* create LxmertModelIntegrationTest

* implementation using numpy seeding to fix inputs params.

* fix code quality

* isort check
==

tests/test_modeling_lxmert.py
==================
23ab0b698;Suraj Patil;2021-07-05 13:26:44 +0530;[examples/flax] clip style image-text training example (#12491)
* clip style example

* fix post init

* add requirements

* update readme, few small fixes
==

examples/research_projects/jax-projects/hybrid_clip/README.md
examples/research_projects/jax-projects/hybrid_clip/configuration_hybrid_clip.py
examples/research_projects/jax-projects/hybrid_clip/modeling_hybrid_clip.py
examples/research_projects/jax-projects/hybrid_clip/requirements.txt
examples/research_projects/jax-projects/hybrid_clip/run_hybrid_clip.py
==================
89a8739f0;Lysandre Debut;2021-07-05 09:51:11 +0200;Add `Repository` import to the FLAX example script (#12501)

==

examples/research_projects/jax-projects/README.md
==================
2df63282e;Patrick von Platen;2021-07-04 13:16:29 +0100;Update README.md

==

examples/research_projects/jax-projects/README.md
==================
a76eebfc8;Omar Sanseviero;2021-07-02 20:35:17 +0200;Add guide on how to build demos for the Flax sprint (#12468)

==

examples/research_projects/jax-projects/README.md
==================
b21905e03;Patrick von Platen;2021-07-02 14:12:47 +0100;Update README.md

==

examples/research_projects/jax-projects/README.md
==================
d24a52313;Patrick von Platen;2021-07-02 13:41:14 +0100;Update README.md

==

examples/research_projects/jax-projects/README.md
==================
e3fce2f86;Patrick von Platen;2021-07-02 12:12:54 +0100;Update README.md
Thanks a lot @BirgerMoell
==

examples/research_projects/jax-projects/README.md
==================
b889d3f6c;Lysandre Debut;2021-07-02 10:35:10 +0200;Fix TAPAS test uncovered by #12446 (#12480)

==

tests/test_modeling_tapas.py
==================
b4ecc6bef;Matthew LeMay;2021-07-02 02:57:39 -0400;fixed typo in flax-projects readme (#12466)

==

examples/research_projects/jax-projects/README.md
==================
e52288a14;Sylvain Gugger;2021-07-02 02:29:51 -0400;Rework notebooks and move them to the Notebooks repo (#12471)

==

notebooks/01-training-tokenizers.ipynb
notebooks/02-transformers.ipynb
notebooks/03-pipelines.ipynb
notebooks/04-onnx-export.ipynb
notebooks/05-benchmark.ipynb
notebooks/README.md
==================
2d1d92181;Stas Bekman;2021-07-01 10:31:19 -0700;[roberta] fix lm_head.decoder.weight ignore_key handling (#12446)
* fix lm_head.decoder.weight ignore_key handling

* fix the mutable class variable

* Update src/transformers/models/roberta/modeling_roberta.py

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* replicate the comment

* make deterministic

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
==

src/transformers/modeling_utils.py
src/transformers/models/roberta/modeling_roberta.py
tests/test_modeling_common.py
tests/test_modeling_roberta.py
==================
7f0027db3;Teven;2021-07-01 19:25:40 +0200;Fixing bug with param count without embeddings (#12461)
* fixing bug with param count without embeddings

* Update src/transformers/modeling_utils.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* style

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

src/transformers/modeling_utils.py
==================
d5b8fe3b9;Souvic Chakraborty;2021-07-01 22:52:42 +0530;Validation split added: custom data files @sgugger, @patil-suraj (#12407)
* Validation split added: custom data files

Validation split added in case of no validation file and loading custom data

* Updated documentation with custom file usage

Updated documentation with custom file usage

* Update README.md

* Update README.md

* Update README.md

* Made some suggested stylistic changes

* Used logger instead of print.

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Made similar changes to add validation split

In case of a missing validation file, a validation split will be used now.

* max_train_samples to be used for training only

max_train_samples got misplaced, now corrected so that it is applied on training data only, not whole data.

* styled

* changed ordering

* Improved language of documentation

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Improved language of documentation

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Fixed styling issue

* Update run_mlm.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

examples/tensorflow/language-modeling/README.md
examples/tensorflow/language-modeling/run_clm.py
examples/tensorflow/language-modeling/run_mlm.py
==================
f929462b2;Thibault FEVRY;2021-07-01 12:52:00 -0400;Import check_inits handling of duplicate definitions. (#12467)
* Import fix_inits handling of duplicate definitions.

* Style fix
==

utils/check_inits.py
==================
7f87bfc91;Patrick von Platen;2021-07-01 17:11:54 +0100;Add TPU README (#12463)
* Add TPU README

* Apply suggestions from code review

* Update examples/research_projects/jax-projects/README.md

* Update examples/research_projects/jax-projects/README.md

Co-authored-by: Stefan Schweter <stefan@schweter.it>

Co-authored-by: Stefan Schweter <stefan@schweter.it>
==

examples/research_projects/jax-projects/README.md
==================
1457839fc;Patrick von Platen;2021-07-01 15:52:11 +0100;Update README.md

==

examples/research_projects/jax-projects/README.md
==================
c18af5d40;Suzana Iliƒá;2021-07-01 16:19:23 +0200;Added talk details (#12465)

==

examples/research_projects/jax-projects/README.md
==================
6c5b20aa0;Jin Young (Daniel) Sohn;2021-07-01 10:17:38 -0400;Fix training_args.py barrier for torch_xla (#12464)
torch_xla currently has its own synchronization primitives, so use
xm.rendezvous(tag) instead.
==

src/transformers/training_args.py
==================
2a501ac95;Lysandre Debut;2021-07-01 15:26:46 +0200;Comment fast GPU TF tests (#12452)

==

.github/workflows/self-push.yml
==================
27d348f2f;Patrick von Platen;2021-07-01 13:59:32 +0100;[Wav2Vec2, Hubert] Fix ctc loss test (#12458)
* fix_torch_device_generate_test

* remove @

* fix test
==

tests/test_modeling_hubert.py
tests/test_modeling_wav2vec2.py
==================
b655f16d4;Patrick von Platen;2021-07-01 11:41:22 +0100;[Flax community event] How to use hub during training (#12447)
* fix_torch_device_generate_test

* remove @

* upload

* finish doc

* Apply suggestions from code review

Co-authored-by: Omar Sanseviero <osanseviero@users.noreply.github.com>
Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
Co-authored-by: Julien Chaumond <chaumond@gmail.com>

* finish

Co-authored-by: Omar Sanseviero <osanseviero@users.noreply.github.com>
Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
Co-authored-by: Julien Chaumond <chaumond@gmail.com>
==

examples/research_projects/jax-projects/README.md
==================
3aa37b945;SaulLu;2021-07-01 12:37:07 +0200;Add test for a WordLevel tokenizer model (#12437)
* add a test for a WordLevel tokenizer

* adapt common test to new tokenizer
==

tests/test_tokenization_common.py
tests/test_tokenization_fast.py
==================
0d1f67e65;Patrick von Platen;2021-06-30 18:44:23 +0100;[Flax] Add wav2vec2 (#12271)
* fix_torch_device_generate_test

* remove @

* start flax wav2vec2

* save intermediate

* forward pass has correct shape

* add weight norm

* add files

* finish ctc

* make style

* finish gumbel quantizer

* correct docstrings

* correct some more files

* fix vit

* finish quality

* correct tests

* correct docstring

* correct tests

* start wav2vec2 pretraining script

* save intermediate

* start pretraining script

* finalize pretraining script

* finish

* finish

* small typo

* finish

* correct

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
Co-authored-by: Suraj Patil <surajp815@gmail.com>

* make style

* push

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
Co-authored-by: Suraj Patil <surajp815@gmail.com>
==

docs/source/index.rst
docs/source/model_doc/wav2vec2.rst
examples/research_projects/jax-projects/wav2vec2/README.md
examples/research_projects/jax-projects/wav2vec2/run_wav2vec2_pretrain_flax.py
src/transformers/__init__.py
src/transformers/models/auto/modeling_flax_auto.py
src/transformers/models/wav2vec2/__init__.py
src/transformers/models/wav2vec2/modeling_flax_wav2vec2.py
src/transformers/utils/dummy_flax_objects.py
tests/test_modeling_flax_wav2vec2.py
utils/check_repo.py
==================
3f36a2c06;Suraj Patil;2021-06-30 21:40:12 +0530;[JAX/Flax readme] add philosophy doc (#12419)
* add philosophy doc

* fix typos

* update doc

* Apply suggestions from code review

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* address Patricks suggestions

* add a training example and fix typos

* jit the training step

* jit train step

* fix example code

* typo

* Apply suggestions from code review

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
==

examples/research_projects/jax-projects/README.md
==================
1ad1c4a86;Suzana Iliƒá;2021-06-30 16:58:03 +0200;Add to talks section (#12442)

==

examples/research_projects/jax-projects/README.md
==================
42477d68f;fcakyon;2021-06-30 17:24:06 +0300;fix typo in mt5 configuration docstring (#12432)

==

src/transformers/models/mt5/configuration_mt5.py
==================
89073a95b;Lysandre;2021-06-30 14:39:27 +0200;Document patch release v4.8.2

==

.circleci/deploy.sh
docs/source/_static/js/custom.js
==================
6e6859787;NielsRogge;2021-06-30 14:05:44 +0200;Add CANINE (#12024)
* First pass

* More progress

* Add support for local attention

* More improvements

* More improvements

* Conversion script working

* Add CanineTokenizer

* Make style & quality

* First draft of integration test

* Remove decoder test

* Improve tests

* Add documentation

* Mostly docs improvements

* Add CanineTokenizer tests

* Fix most tests on GPU, improve upsampling projection

* Address most comments by @dhgarrette

* Remove decoder logic

* Improve Canine tests, improve docs of CanineConfig

* All tokenizer tests passing

* Make fix-copies and fix tokenizer tests

* Fix test_model_outputs_equivalence test

* Apply suggestions from @sgugger's review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Address some more comments

* Add support for hidden_states and attentions of shallow encoders

* Define custom CanineModelOutputWithPooling, tests pass

* First pass

* More progress

* Add support for local attention

* More improvements

* More improvements

* Conversion script working

* Add CanineTokenizer

* Make style & quality

* First draft of integration test

* Remove decoder test

* Improve tests

* Add documentation

* Mostly docs improvements

* Add CanineTokenizer tests

* Fix most tests on GPU, improve upsampling projection

* Address most comments by @dhgarrette

* Remove decoder logic

* Improve Canine tests, improve docs of CanineConfig

* All tokenizer tests passing

* Make fix-copies and fix tokenizer tests

* Fix test_model_outputs_equivalence test

* Apply suggestions from @sgugger's review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Address some more comments

* Make conversion script work for Canine-c too

* Fix tokenizer tests

* Remove file

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

README.md
docs/source/index.rst
docs/source/model_doc/canine.rst
src/transformers/__init__.py
src/transformers/models/__init__.py
src/transformers/models/auto/configuration_auto.py
src/transformers/models/auto/modeling_auto.py
src/transformers/models/auto/tokenization_auto.py
src/transformers/models/canine/__init__.py
src/transformers/models/canine/configuration_canine.py
src/transformers/models/canine/convert_canine_original_tf_checkpoint_to_pytorch.py
src/transformers/models/canine/modeling_canine.py
src/transformers/models/canine/tokenization_canine.py
src/transformers/utils/dummy_pt_objects.py
src/transformers/utils/modeling_auto_mapping.py
tests/test_modeling_canine.py
tests/test_tokenization_canine.py
==================
69f570156;Jabin Huang;2021-06-30 20:03:58 +0800;Add default bos_token and eos_token for tokenizer of deberta_v2 (#12429)
* fix ids_to_tokens naming error in tokenizer of deberta v2

* Update tokenization_deberta_v2.py

Add bos_token and eos_token.

* format code

Co-authored-by: Jipeng Huang <jihuan@microsoft.com>
==

src/transformers/models/deberta_v2/tokenization_deberta_v2.py
==================
c9486fd0f;Sylvain Gugger;2021-06-30 07:57:05 -0400;Fix default bool in argparser (#12424)
* Fix default bool in argparser

* Add more to test
==

src/transformers/hf_argparser.py
tests/test_hf_argparser.py
==================
90d69456e;Suzana Iliƒá;2021-06-30 13:14:11 +0200;Added to talks section (#12433)
Added one more confirmed speaker, zoom links and gcal event links
==

examples/research_projects/jax-projects/README.md
==================
31a811091;Sylvain Gugger;2021-06-30 02:41:47 -0400;Add option to save on each training node (#12421)
* Add option to save on each training node

* Apply suggestions from code review

Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>

* Address review comments

Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>
==

src/transformers/trainer.py
src/transformers/training_args.py
==================
990540b72;Stas Bekman;2021-06-29 14:59:03 -0700;[modelcard] fix (#12422)
this PR is fixing an incorrect attribute - probably some tests are needed?
==

src/transformers/modelcard.py
==================
dc42e770b;Sylvain Gugger;2021-06-29 15:00:08 -0400;Easily train a new fast tokenizer from a given one (#12361)
* [WIP] Easily train a new fast tokenizer from a given one

* Fix test

* Roll out to other tokenizers and add tests

* Fix bug with unk id and add emoji to test

* Really use something different in test

* Implement special tokens map

* Map special tokens in the Transformers tokenizers

* Fix test

* Make test more robust

* Fix test for BPE

* More robust map and test

Co-authored-by SaulLu

* Test file

* Stronger tests

Co-authored-by: SaulLu <lucilesaul.com@gmail.com>

* Map unk token for Wordpiece and address review comment

* Fix lowercase test and address review comment

* Fix all tests

* Simplify test

* Fix tests for realsies

* Easily train a new fast tokenizer from a given one - tackle the special tokens format (str or AddedToken) (#12420)

* Propose change in tests regarding lower case

* add new test for special tokens types

* put back the test part about decoding

* add feature: the AddedToken is re-build with the different mapped content

* Address review comment: simplify AddedToken building

Co-authored-by: sgugger <sylvain.gugger@gmail.com>

* Update src/transformers/tokenization_utils_fast.py

Co-authored-by: sgugger <sylvain.gugger@gmail.com>
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

Co-authored-by: SaulLu <lucilesaul.com@gmail.com>
Co-authored-by: SaulLu <55560583+SaulLu@users.noreply.github.com>
==

src/transformers/models/albert/tokenization_albert_fast.py
src/transformers/models/barthez/tokenization_barthez_fast.py
src/transformers/models/bert/tokenization_bert_fast.py
src/transformers/models/big_bird/tokenization_big_bird_fast.py
src/transformers/models/blenderbot_small/tokenization_blenderbot_small_fast.py
src/transformers/models/camembert/tokenization_camembert_fast.py
src/transformers/models/clip/tokenization_clip_fast.py
src/transformers/models/deberta/tokenization_deberta_fast.py
src/transformers/models/funnel/tokenization_funnel_fast.py
src/transformers/models/gpt2/tokenization_gpt2_fast.py
src/transformers/models/herbert/tokenization_herbert_fast.py
src/transformers/models/mbart/tokenization_mbart.py
src/transformers/models/mbart/tokenization_mbart50_fast.py
src/transformers/models/mbart/tokenization_mbart_fast.py
src/transformers/models/mpnet/tokenization_mpnet_fast.py
src/transformers/models/openai/tokenization_openai_fast.py
src/transformers/models/pegasus/tokenization_pegasus_fast.py
src/transformers/models/reformer/tokenization_reformer_fast.py
src/transformers/models/roberta/tokenization_roberta_fast.py
src/transformers/models/roformer/tokenization_roformer_fast.py
src/transformers/models/t5/tokenization_t5_fast.py
src/transformers/models/xlm_roberta/tokenization_xlm_roberta_fast.py
src/transformers/models/xlnet/tokenization_xlnet_fast.py
src/transformers/tokenization_utils_fast.py
tests/test_tokenization_common.py
==================
b440b8d1c;Suzana Iliƒá;2021-06-29 16:01:16 +0100;Added talks (#12415)

==

examples/research_projects/jax-projects/README.md
==================
5257818e6;Shamane Siri;2021-06-30 00:39:48 +1200;minor fixes in original RAG training (#12395)

==

examples/research_projects/rag/callbacks_rag.py
examples/research_projects/rag/finetune_rag.py
==================
e3f39a295;Jabin Huang;2021-06-29 20:15:35 +0800;fix ids_to_tokens naming error in tokenizer of deberta v2 (#12412)
Co-authored-by: Jipeng Huang <jihuan@microsoft.com>
==

src/transformers/models/deberta_v2/tokenization_deberta_v2.py
==================
813328682;Patrick von Platen;2021-06-29 12:01:08 +0100;[Flax] Example scripts - correct weight decay  (#12409)
* fix_torch_device_generate_test

* remove @

* finish

* finish

* correct style
==

examples/flax/language-modeling/run_clm_flax.py
examples/flax/language-modeling/run_mlm_flax.py
examples/flax/language-modeling/run_t5_mlm_flax.py
examples/flax/summarization/run_summarization_flax.py
==================
aecae5337;Suraj Patil;2021-06-29 14:02:33 +0530;[example/flax] add summarization readme (#12393)
* add readme

* update readme and add requirements

* Update examples/flax/summarization/README.md

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
==

examples/flax/summarization/README.md
examples/flax/summarization/requirements.txt
==================
388610457;Will Rice;2021-06-29 04:15:57 -0400;Fix TFWav2Vec2 SpecAugment (#12289)
* Fix TFWav2Vec2 SpecAugment

* Invert masks

* Feedback changes
==

src/transformers/models/wav2vec2/modeling_tf_wav2vec2.py
==================
bc084938f;Will Rice;2021-06-29 03:57:46 -0400;Add out of vocabulary error to ASR models (#12288)
* Add OOV error to ASR models

* Feedback changes
==

src/transformers/models/hubert/modeling_hubert.py
src/transformers/models/wav2vec2/modeling_tf_wav2vec2.py
src/transformers/models/wav2vec2/modeling_wav2vec2.py
tests/test_modeling_hubert.py
tests/test_modeling_tf_wav2vec2.py
tests/test_modeling_wav2vec2.py
==================
1fc6817a3;NielsRogge;2021-06-29 09:07:46 +0200;Rename detr targets to labels (#12280)
* Rename target to labels in DetrFeatureExtractor

* Update DetrFeatureExtractor tests accordingly

* Improve docs of DetrFeatureExtractor

* Improve docs

* Make style
==

src/transformers/models/detr/configuration_detr.py
src/transformers/models/detr/feature_extraction_detr.py
src/transformers/models/detr/modeling_detr.py
tests/test_feature_extraction_detr.py
==================
7682e9770;Stas Bekman;2021-06-28 20:11:21 -0700;[models] respect dtype of the model when instantiating it (#12316)
* [models] respect dtype of the model when instantiating it

* cleanup

* cleanup

* rework to handle non-float dtype

* fix

* switch to fp32 tiny model

* improve

* use dtype.is_floating_point

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* fix the doc

* recode to use explicit torch_dtype_auto_detect, torch_dtype args

* docs and tweaks

* docs and tweaks

* docs and tweaks

* merge 2 args, add docs

* fix

* fix

* better doc

* better doc

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

docs/source/main_classes/deepspeed.rst
docs/source/main_classes/model.rst
src/transformers/configuration_utils.py
src/transformers/modeling_flax_utils.py
src/transformers/modeling_tf_utils.py
src/transformers/modeling_utils.py
src/transformers/models/auto/auto_factory.py
tests/test_modeling_common.py
==================
31c3e7e75;Patrick von Platen;2021-06-28 20:11:29 +0100;[Flax] Add T5 pretraining script (#12355)
* fix_torch_device_generate_test

* remove @

* add length computatan

* finish masking

* finish

* upload

* fix some bugs

* finish

* fix dependency table

* correct tensorboard

* Apply suggestions from code review

* correct processing

* slight change init

* correct some more mistakes

* apply suggestions

* improve readme

* fix indent

* Apply suggestions from code review

Co-authored-by: SaulLu <55560583+SaulLu@users.noreply.github.com>

* correct tokenizer

* finish

* finish

* finish

* finish

Co-authored-by: Patrick von Platen <patrick@huggingface.co>
Co-authored-by: SaulLu <55560583+SaulLu@users.noreply.github.com>
==

examples/flax/language-modeling/README.md
examples/flax/language-modeling/run_mlm_flax.py
examples/flax/language-modeling/run_t5_mlm_flax.py
examples/flax/language-modeling/t5_tokenizer_model.py
examples/research_projects/jax-projects/README.md
src/transformers/models/auto/modeling_flax_auto.py
src/transformers/models/t5/modeling_flax_t5.py
==================
e27707488;Stas Bekman;2021-06-28 11:43:24 -0700;pass the matching trainer log level to deepspeed (#12401)

==

src/transformers/deepspeed.py
==================
7e22609e0;Matt;2021-06-28 19:31:44 +0100;Tensorflow LM examples (#12358)
* Tensorflow MLM example

* Add CLM example

* Style fixes, adding missing checkpoint code from the CLM example

* Fix TPU training, avoid massive dataset warnings

* Fix incorrect training length calculation for multi-GPU training

* Fix incorrect training length calculation for multi-GPU training

* Refactors and nitpicks from the review

* Style pass

* Adding README
==

examples/tensorflow/language-modeling/README.md
examples/tensorflow/language-modeling/run_clm.py
examples/tensorflow/language-modeling/run_mlm.py
==================
2d70c9120;Patrick von Platen;2021-06-28 19:23:35 +0100;[Flax] Adapt flax examples to include `push_to_hub` (#12391)
* fix_torch_device_generate_test

* remove @

* finish

* correct summary writer

* correct push to hub

* fix indent

* finish

* finish

* finish

* finish

* finish

Co-authored-by: Patrick von Platen <patrick@huggingface.co>
==

examples/flax/language-modeling/README.md
examples/flax/language-modeling/run_clm_flax.py
examples/flax/language-modeling/run_mlm_flax.py
examples/flax/summarization/run_summarization_flax.py
examples/flax/text-classification/README.md
examples/flax/text-classification/run_flax_glue.py
==================
a7d0b288f;Funtowicz Morgan;2021-06-28 18:30:05 +0200;Remove the need for `einsum` in Albert's attention computation (#12394)
* debug albert einsum

* Fix matmul computation

* Let's use torch linear layer.

* Style.
==

src/transformers/models/albert/modeling_albert.py
==================
276bc149d;Sylvain Gugger;2021-06-28 12:26:40 -0400;Fix copies

==

examples/tensorflow/question-answering/utils_qa.py
==================
27b6ac461;Patrick von Platen;2021-06-28 17:22:10 +0100;Update README.md

==

examples/research_projects/jax-projects/README.md
==================
89b57a666;Patrick von Platen;2021-06-28 17:18:42 +0100;[Flax community event] Add more description to readme (#12398)
* fix_torch_device_generate_test

* remove @

* boom boom

* correct typos

* Apply suggestions from code review

Co-authored-by: Suraj Patil <surajp815@gmail.com>

* Apply suggestions from code review

Co-authored-by: Suzana Iliƒá <io.suzanai@gmail.com>

* Apply suggestions from code review

Co-authored-by: Suraj Patil <surajp815@gmail.com>
Co-authored-by: Suzana Iliƒá <io.suzanai@gmail.com>
==

examples/research_projects/jax-projects/README.md
==================
04dbea31a;Bhadresh Savani;2021-06-28 21:44:00 +0530;[Examples] Added context manager to datasets map (#12367)
* added cotext manager to datasets map

* fixed style and spaces

* fixed warning of deprecation

* changed desc
==

examples/pytorch/language-modeling/run_clm.py
examples/pytorch/language-modeling/run_mlm.py
examples/pytorch/language-modeling/run_plm.py
examples/pytorch/multiple-choice/run_swag.py
examples/pytorch/question-answering/run_qa.py
examples/pytorch/question-answering/run_qa_beam_search.py
examples/pytorch/summarization/run_summarization.py
examples/pytorch/text-classification/run_glue.py
examples/pytorch/text-classification/run_xnli.py
examples/pytorch/token-classification/run_ner.py
templates/adding_a_new_example_script/{{cookiecutter.directory_name}}/run_{{cookiecutter.example_shortcut}}.py
tests/sagemaker/scripts/pytorch/run_glue_model_parallelism.py
==================
d25ad34c8;Stas Bekman;2021-06-28 08:55:59 -0700;[CI] add dependency table sync verification (#12364)
* add dependency table sync verification

* improve the message

* improve the message

* revert

* ready to merge
==

.circleci/config.yml
Makefile
==================
57461ac0b;Sylvain Gugger;2021-06-28 10:02:53 -0400;Add possibility to maintain full copies of files (#12312)

==

examples/tensorflow/question-answering/utils_qa.py
utils/check_copies.py
==================
9490d668d;Taha ValizadehAslani;2021-06-28 06:49:22 -0500;Update run_mlm.py (#12344)
Before the code could not be used for validation only because of this line:
extension = data_args.train_file.split(".")[-1]
was assuming that extension must be extracted from the training dataset. This line would run regardless of the training or validation options of the user. This would lead to an error if the user only wants to run an evaluation only and does not want to do train (because the training file does not exist). I modified it to extract extension from the training file if the user wants to do train and extract it from the validation file if the user wants to run eval. This way the code can be used for both training and validation separately.
==

examples/pytorch/language-modeling/run_mlm.py
==================
c7faf2ccc;Kilian Kluge;2021-06-28 13:39:56 +0200;[Documentation] Warn that DataCollatorForWholeWordMask is limited to BertTokenizer-like tokenizers (#12371)
* Notify users that DataCollatorForWholeWordMask is limited to BertTokenier-like tokenizers

* Fix code formatting
==

src/transformers/data/data_collator.py
==================
ff5cdc086;Bhadresh Savani;2021-06-26 17:31:25 +0100;replace print with logger (#12368)

==

examples/pytorch/question-answering/utils_qa.py
==================
9a7545943;Bhadresh Savani;2021-06-26 04:50:30 +0100;updated example template (#12365)

==

templates/adding_a_new_example_script/{{cookiecutter.directory_name}}/run_{{cookiecutter.example_shortcut}}.py
==================
539ee456d;Bhadresh Savani;2021-06-25 22:58:42 +0100;[Examples] Replicates the new --log_level feature to all trainer-based pytorch (#12359)
* added log_level

* fix comment

* fixed log_level

* Trigger CI

* Unfied logging

* simplified args for log_level
==

examples/pytorch/language-modeling/run_clm.py
examples/pytorch/language-modeling/run_mlm.py
examples/pytorch/language-modeling/run_plm.py
examples/pytorch/multiple-choice/run_swag.py
examples/pytorch/question-answering/run_qa.py
examples/pytorch/question-answering/run_qa_beam_search.py
examples/pytorch/question-answering/utils_qa.py
examples/pytorch/summarization/run_summarization.py
examples/pytorch/text-classification/run_glue.py
examples/pytorch/text-classification/run_xnli.py
examples/pytorch/token-classification/run_ner.py
examples/pytorch/token-classification/run_ner_no_trainer.py
examples/pytorch/translation/run_translation.py
==================
64e609809;Stas Bekman;2021-06-25 14:58:03 -0700;[trainer] add main_process_first context manager (#12351)
* main_process_first context manager

* handle multi-node, add context description

* sync desc
==

examples/pytorch/translation/run_translation.py
src/transformers/training_args.py
==================
f86642589;cronoik;2021-06-25 23:41:08 +0200;fixed multiplechoice tokenization (#12362)
* fixed multiplechoice tokenization

The model would have seen two sequences:
1. [CLS]prompt[SEP]prompt[SEP]
2. [CLS]choice0[SEP]choice1[SEP]
that is not correct as we want a contextualized embedding of prompt and choice

* removed outer brackets for proper sequence generation
==

src/transformers/file_utils.py
==================
4a872caef;Stas Bekman;2021-06-25 13:20:14 -0700;remove extra white space from log format (#12360)

==

docs/source/main_classes/trainer.rst
examples/flax/language-modeling/run_clm_flax.py
examples/flax/language-modeling/run_mlm_flax.py
examples/flax/summarization/run_summarization_flax.py
examples/flax/text-classification/run_flax_glue.py
examples/legacy/multiple_choice/run_multiple_choice.py
examples/legacy/question-answering/run_squad.py
examples/legacy/question-answering/run_squad_trainer.py
examples/legacy/run_language_modeling.py
examples/legacy/run_openai_gpt.py
examples/legacy/run_swag.py
examples/legacy/run_transfo_xl.py
examples/legacy/seq2seq/finetune_trainer.py
examples/legacy/text-classification/run_tf_text_classification.py
examples/legacy/token-classification/run_ner.py
examples/legacy/token-classification/run_tf_ner.py
examples/pytorch/language-modeling/run_clm.py
examples/pytorch/language-modeling/run_clm_no_trainer.py
examples/pytorch/language-modeling/run_mlm.py
examples/pytorch/language-modeling/run_mlm_no_trainer.py
examples/pytorch/language-modeling/run_plm.py
examples/pytorch/multiple-choice/run_swag.py
examples/pytorch/multiple-choice/run_swag_no_trainer.py
examples/pytorch/question-answering/run_qa.py
examples/pytorch/question-answering/run_qa_beam_search.py
examples/pytorch/question-answering/run_qa_beam_search_no_trainer.py
examples/pytorch/question-answering/run_qa_no_trainer.py
examples/pytorch/summarization/run_summarization.py
examples/pytorch/summarization/run_summarization_no_trainer.py
examples/pytorch/text-classification/run_glue.py
examples/pytorch/text-classification/run_glue_no_trainer.py
examples/pytorch/text-classification/run_xnli.py
examples/pytorch/text-generation/run_generation.py
examples/pytorch/token-classification/run_ner.py
examples/pytorch/token-classification/run_ner_no_trainer.py
examples/pytorch/translation/run_translation.py
examples/pytorch/translation/run_translation_no_trainer.py
examples/research_projects/adversarial/run_hans.py
examples/research_projects/bert-loses-patience/run_glue_with_pabee.py
examples/research_projects/deebert/run_glue_deebert.py
examples/research_projects/distillation/run_squad_w_distillation.py
examples/research_projects/distillation/scripts/binarized_data.py
examples/research_projects/distillation/scripts/token_counts.py
examples/research_projects/mlm_wwm/run_mlm_wwm.py
examples/research_projects/mm-imdb/run_mmimdb.py
examples/research_projects/movement-pruning/masked_run_glue.py
examples/research_projects/movement-pruning/masked_run_squad.py
examples/research_projects/performer/run_mlm_performer.py
examples/research_projects/wav2vec2/run_asr.py
examples/research_projects/wav2vec2/run_common_voice.py
examples/research_projects/wav2vec2/run_pretrain.py
examples/research_projects/zero-shot-distillation/distill_classifier.py
examples/tensorflow/multiple-choice/run_tf_multiple_choice.py
examples/tensorflow/question-answering/run_qa.py
examples/tensorflow/text-classification/run_glue.py
examples/tensorflow/text-classification/run_text_classification.py
templates/adding_a_new_example_script/{{cookiecutter.directory_name}}/run_{{cookiecutter.example_shortcut}}.py
tests/sagemaker/scripts/pytorch/run_glue_model_parallelism.py
==================
a3daabfe1;Sylvain Gugger;2021-06-25 15:54:31 -0400;Style

==

src/transformers/dependency_versions_table.py
==================
238521b0b;Kai Fricke;2021-06-25 20:12:03 +0200;Replace NotebookProgressReporter by ProgressReporter in Ray Tune run (#12357)
* Replace NotebookProgressReporter by ProgressReporter in Ray Tune run

* Move to local import
==

src/transformers/integrations.py
==================
332a24586;Vasudev Gupta;2021-06-25 22:35:48 +0530;Add FlaxBigBird QuestionAnswering script (#12233)
* port bigbird script

* adapt script a bit

* change location

* adapt more

* save progress

* init commit

* style

* dataset script tested

* readme add
==

examples/research_projects/jax-projects/big_bird/README.md
examples/research_projects/jax-projects/big_bird/bigbird_flax.py
examples/research_projects/jax-projects/big_bird/evaluate.py
examples/research_projects/jax-projects/big_bird/prepare_natural_questions.py
examples/research_projects/jax-projects/big_bird/requirements.txt
examples/research_projects/jax-projects/big_bird/sweep_flax.yaml
examples/research_projects/jax-projects/big_bird/train.py
==================
55bb4c06f;jglaser;2021-06-25 10:55:15 -0400;Fix exception in prediction loop occurring for certain batch sizes (#12350)
* fix distributed_concat for scalar outputs

* Update README.md

* fixed typo (#12356)

* simplify fix with terser syntax

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Trigger CI

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
Co-authored-by: michal pitr <21157924+MichalPitr@users.noreply.github.com>
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

src/transformers/trainer_pt_utils.py
==================
d4ce31e83;michal pitr;2021-06-25 12:49:29 +0100;fixed typo (#12356)

==

examples/pytorch/token-classification/README.md
==================
aa550c4a1;Patrick von Platen;2021-06-25 11:55:51 +0100;Update README.md

==

examples/research_projects/jax-projects/README.md
==================
f2c4ce7e3;Marc van Zee;2021-06-24 18:04:18 +0200;Add flax/jax quickstart (#12342)

==

examples/research_projects/jax-projects/README.md
==================
5b1b5635d;Sylvain Gugger;2021-06-24 10:15:15 -0400;Document patch release v4.8.1

==

.circleci/deploy.sh
docs/source/_static/js/custom.js
==================
8ef62ec9e;Lysandre Debut;2021-06-24 15:52:28 +0200;Fix torchscript tests (#12336)
* Fix torchscript tests

* Better test

* Remove bogus print
==

tests/test_modeling_common.py
==================
aef3823e1;Suraj Patil;2021-06-24 16:03:37 +0530;[examples/Flax] move the examples table up (#12341)

==

examples/flax/README.md
==================
7875b638c;Richard Liaw;2021-06-24 01:13:17 -0700;try-this (#12338)
Signed-off-by: Richard Liaw <rliaw@berkeley.edu>
==

setup.py
==================
cf3c9198a;Sylvain Gugger;2021-06-23 16:22:29 -0400;Fix default to logging_dir lost in merge conflict

==

src/transformers/training_args.py
==================
07ae6103c;Stas Bekman;2021-06-23 11:07:37 -0700;[Deepspeed] new docs (#12077)
* document sub_group_size

* style

* install + issues reporting

* style

* style

* Update docs/source/main_classes/deepspeed.rst

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* indent 4

* restore

* style

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

docs/source/main_classes/deepspeed.rst
==================
3694484d0;Sam Havens;2021-06-23 10:39:43 -0700;Update training_args.py (#12328)
mention in `save_strategy` param description that `load_best_model_at_end` can override
==

src/transformers/training_args.py
==================
2150dfed3;Sylvain Gugger;2021-06-23 13:31:19 -0400;v4.9.0.dev0

==

.circleci/deploy.sh
docs/source/_static/js/custom.js
examples/pytorch/language-modeling/run_clm.py
examples/pytorch/language-modeling/run_mlm.py
examples/pytorch/language-modeling/run_plm.py
examples/pytorch/multiple-choice/run_swag.py
examples/pytorch/question-answering/run_qa.py
examples/pytorch/question-answering/run_qa_beam_search.py
examples/pytorch/question-answering/run_qa_beam_search_no_trainer.py
examples/pytorch/question-answering/run_qa_no_trainer.py
examples/pytorch/summarization/run_summarization.py
examples/pytorch/text-classification/run_glue.py
examples/pytorch/text-classification/run_xnli.py
examples/pytorch/token-classification/run_ner.py
examples/pytorch/translation/run_translation.py
examples/tensorflow/question-answering/run_qa.py
examples/tensorflow/text-classification/run_glue.py
setup.py
src/transformers/__init__.py
==================
9252a5127;Sylvain Gugger;2021-06-23 13:25:56 -0400;Release: v4.8.0

==

examples/pytorch/language-modeling/run_clm.py
examples/pytorch/language-modeling/run_mlm.py
examples/pytorch/language-modeling/run_plm.py
examples/pytorch/multiple-choice/run_swag.py
examples/pytorch/question-answering/run_qa.py
examples/pytorch/question-answering/run_qa_beam_search.py
examples/pytorch/question-answering/run_qa_beam_search_no_trainer.py
examples/pytorch/question-answering/run_qa_no_trainer.py
examples/pytorch/summarization/run_summarization.py
examples/pytorch/text-classification/run_glue.py
examples/pytorch/text-classification/run_xnli.py
examples/pytorch/token-classification/run_ner.py
examples/pytorch/translation/run_translation.py
examples/tensorflow/question-answering/run_qa.py
examples/tensorflow/text-classification/run_glue.py
setup.py
src/transformers/__init__.py
==================
468cda20f;Patrick von Platen;2021-06-23 17:39:21 +0100;[Flax T5] Fix weight initialization and fix docs (#12327)
* finish t5 flax fixes

* improve naming
==

src/transformers/models/t5/modeling_flax_t5.py
==================
12a4457c5;Sylvain Gugger;2021-06-23 12:30:15 -0400;Pin good version of huggingface_hub

==

setup.py
src/transformers/dependency_versions_table.py
==================
986ac03e3;Michael Benayoun;2021-06-23 18:16:24 +0200;changed modeling_fx_utils.py to utils/fx.py for clarity (#12326)
Co-authored-by: Michael Benayoun <michael@huggingface.co>
==

src/transformers/utils/fx.py
tests/test_modeling_common.py
==================
941b4442b;Lysandre;2021-06-23 17:46:24 +0200;Temporarily revert the `fill-mask` improvements.

==

src/transformers/pipelines/fill_mask.py
tests/test_pipelines_fill_mask.py
==================
4bdff2cdb;Lysandre Debut;2021-06-23 17:07:07 +0200;Conda build (#12323)

==

.github/conda/meta.yaml
.github/workflows/release-conda.yml
==================
9eda6b52e;Sylvain Gugger;2021-06-23 10:40:54 -0400;Add all XxxPreTrainedModel to the main init (#12314)
* Add all XxxPreTrainedModel to the main init

* Add to template

* Add to template bis

* Add FlaxT5
==

src/transformers/__init__.py
src/transformers/models/bart/__init__.py
src/transformers/models/bert_generation/__init__.py
src/transformers/models/blenderbot/__init__.py
src/transformers/models/blenderbot_small/__init__.py
src/transformers/models/clip/__init__.py
src/transformers/models/flaubert/__init__.py
src/transformers/models/funnel/__init__.py
src/transformers/models/gpt2/__init__.py
src/transformers/models/layoutlm/__init__.py
src/transformers/models/longformer/__init__.py
src/transformers/models/marian/__init__.py
src/transformers/models/mbart/__init__.py
src/transformers/models/megatron_bert/__init__.py
src/transformers/models/pegasus/__init__.py
src/transformers/models/rag/__init__.py
src/transformers/models/reformer/__init__.py
src/transformers/models/roberta/__init__.py
src/transformers/models/tapas/__init__.py
src/transformers/models/vit/__init__.py
src/transformers/utils/dummy_flax_objects.py
src/transformers/utils/dummy_pt_objects.py
src/transformers/utils/dummy_tf_objects.py
src/transformers/utils/dummy_timm_and_vision_objects.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/to_replace_{{cookiecutter.lowercase_modelname}}.py
utils/check_repo.py
==================
53c60babe;Sylvain Gugger;2021-06-23 10:11:19 -0400;Clean push to hub API (#12187)
* Clean push to hub API

* Create working dir if it does not exist

* Different tweak

* New API + all models + test Flax

* Adds the Trainer clean up

* Update src/transformers/file_utils.py

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* Address review comments

* (nit) output types

* No need to set clone_from when folder exists

* Update src/transformers/trainer.py

Co-authored-by: Julien Chaumond <julien@huggingface.co>

* Add generated_from_trainer tag

* Update to new version

* Fixes

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
Co-authored-by: Julien Chaumond <julien@huggingface.co>
Co-authored-by: Lysandre <lysandre.debut@reseau.eseo.fr>
==

setup.py
src/transformers/configuration_utils.py
src/transformers/dependency_versions_table.py
src/transformers/file_utils.py
src/transformers/modelcard.py
src/transformers/modeling_flax_utils.py
src/transformers/modeling_tf_utils.py
src/transformers/modeling_utils.py
src/transformers/tokenization_utils_base.py
src/transformers/trainer.py
src/transformers/training_args.py
tests/test_configuration_common.py
tests/test_modeling_common.py
tests/test_modeling_flax_common.py
tests/test_modeling_tf_common.py
tests/test_tokenization_common.py
tests/test_trainer.py
==================
625f512d5;chenht2010;2021-06-23 21:51:31 +0800;[TFWav2Vec2] Fix docs (#12283)
* fix error

* make style check happy

Co-authored-by: chenhaitao <chenhaitao@qiyi.com>
==

src/transformers/models/wav2vec2/modeling_tf_wav2vec2.py
==================
44739c818;Patrick von Platen;2021-06-23 14:50:35 +0100;[Flax/JAX] Add how to propose projects markdown (#12311)
* fix_torch_device_generate_test

* remove @

* finish

* make style
==

examples/research_projects/jax-projects/HOW_TO_PROPOSE_PROJECT.md
examples/research_projects/jax-projects/README.md
==================
ef3dceff4;Lysandre Debut;2021-06-23 15:45:30 +0200;Add mention of the huggingface_hub methods for offline mode (#12320)

==

docs/source/installation.md
==================
e98233dde;Vasudev Gupta;2021-06-23 17:43:32 +0530;Flax T5 (#12150)
* copy pytorch-t5

* init

* boom boom

* forward pass same

* make generation work

* add more tests

* make test work

* finish normal tests

* make fix-copies

* finish quality

* correct slow example

* correct slow test

* version table

* upload models

* Update tests/test_modeling_flax_t5.py

* correct incorrectly deleted line

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
Co-authored-by: Patrick von Platen <patrick@huggingface.co>
==

docs/source/index.rst
docs/source/model_doc/t5.rst
setup.py
src/transformers/__init__.py
src/transformers/dependency_versions_table.py
src/transformers/models/auto/modeling_flax_auto.py
src/transformers/models/bart/modeling_flax_bart.py
src/transformers/models/t5/__init__.py
src/transformers/models/t5/modeling_flax_t5.py
src/transformers/utils/dummy_flax_objects.py
tests/test_modeling_flax_bart.py
tests/test_modeling_flax_t5.py
tests/test_modeling_t5.py
==================
7d4cfa3b4;David Fan;2021-06-23 03:34:18 -0700;Rewrite ProphetNet to adapt converting ONNX friendly (#11981)
* Rewrite

* [ONNX] rewrite
==

src/transformers/models/prophetnet/modeling_prophetnet.py
==================
c0fe3c9a7;Suraj Patil;2021-06-23 15:49:30 +0530;Flax summarization script  (#12230)
* add summrization script

* fix arguments, preprocessing, metrics

* add generation and metrics

* auto model, prediction loop

* prettify

* label smoothing

* adress Sylvain and Patricks suggestions

* dynamically import shift_tokens_right

* fix shift_tokens_right_fn call
==

examples/flax/summarization/run_summarization_flax.py
==================
26a2e3659;Daniel Stancl;2021-06-23 11:52:11 +0200;Add output in a dictionary for TF `generate` method (#12139)
* Add output args to greedy search

* Fix critical typo + make style quality

* Handle generate_beam_search

* Add dict_specific tests and fix the placement of encoder outputs

* Add  specific outputs

* Update doc

* Fix typo

* Adjust handling encoder_outputs + Fix generating for T5

* Fix generate for RAG

* Fix handling ouptut_attentions when target_mapping is not None

Take care of situations when target_mapping is provided
as there are 2-tuple of attentions

Change from:
if inputs["output_attentions"]:
    attentions = tuple(tf.transpose(t, perm(2, 3, 0, 1)) for t in attentions)

to:
if inputs["output_attentions"]:
    if inputs["target_mapping"] is not None:
        # when target_mapping is provided, there are 2-tuple of attentions
         attentions = tuple(
             tuple(tf.transpose(attn_stream, perm=(2, 3, 0, 1)) for attn_stream in t) for t in attentions
        )
    else:
        attentions = tuple(tf.transpose(t, perm=(2, 3, 0, 1)) for t in attentions)

* Rename kwargs to model_kwargs

* make style quality

* Move imports in test_modeling_tf_common.py

Move ModelOutput-related imports in test_modeling_tf_common.py
into the `is_tf_available():` statement.

* Rewrite nested if-statements

* Fix added tests
==

src/transformers/generation_tf_utils.py
src/transformers/models/rag/modeling_tf_rag.py
src/transformers/models/t5/modeling_tf_t5.py
src/transformers/models/xlnet/modeling_tf_xlnet.py
tests/test_modeling_tf_common.py
==================
d4be49844;Nicolas Patry;2021-06-23 10:38:04 +0200;Optimizing away the `fill-mask` pipeline. (#12113)
* Optimizing away the `fill-mask` pipeline.

- Don't send anything to the tokenizer unless needed. Vocab check is
much faster
- Keep BC by sending data to the tokenizer when needed. User handling warning messages will see performance benefits again
- Make `targets` and `top_k` work together better `top_k` cannot be
higher than `len(targets)` but can be smaller still.
- Actually simplify the `target_ids` in case of duplicate (it can happen
because we're parsing raw strings)
- Removed useless code to fail on empty strings. It works only if empty
string is in first position, moved to ignoring them instead.
- Changed the related tests as only the tests would fail correctly
(having incorrect value in first position)

* Make tests compatible for 2 different vocabs... (at the price of a
warning).

Co-authored-by: @EtaoinWu

* ValueError working globally

* Update src/transformers/pipelines/fill_mask.py

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* `tokenizer.vocab` -> `tokenizer.get_vocab()` for more compatiblity +
fallback.

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
==

src/transformers/pipelines/fill_mask.py
tests/test_pipelines_fill_mask.py
==================
037e466b1;Kevin Canwen Xu;2021-06-23 14:53:09 +0800;Add CodeCarbon Integration (#12304)
* Add optional dependency

* Add CodeCarbon integration

* Add CodeCarbon integration

* Add CodeCarbon integration

* typo
==

setup.py
src/transformers/dependency_versions_table.py
src/transformers/integrations.py
==================
bfd5da8e2;Stas Bekman;2021-06-22 15:34:19 -0700;[docs]  performance  (#12258)
* initial performance document

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* rewrites based on suggestions

* 8x multiple is for AMP only

* add contribute section

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
==

docs/source/index.rst
docs/source/performance.md
==================
1562c04e4;Sylvain Gugger;2021-06-22 16:37:05 -0400;FlaxBartPretrainedModel -> FlaxBartPreTrainedModel (#12313)

==

src/transformers/models/bart/modeling_flax_bart.py
==================
ebe541358;Stas Bekman;2021-06-22 11:13:23 -0700;[trainer] 2 bug fixes and a rename (#12309)
* bug fixes and a rename

* add extended DDP test
==

docs/source/main_classes/trainer.rst
examples/pytorch/translation/run_translation.py
src/transformers/trainer.py
src/transformers/training_args.py
tests/deepspeed/test_deepspeed.py
tests/extended/test_trainer_ext.py
tests/test_trainer.py
==================
64029abe4;Patrick von Platen;2021-06-22 18:02:52 +0100;[Flax] Main doc for event orga (#12305)
* fix_torch_device_generate_test

* remove @

* push

* finish

* some typos

* add more info on communication

* add suggestions
==

examples/research_projects/jax-projects/README.md
==================
032d56a43;Kilian Kluge;2021-06-22 15:58:13 +0200;Fix and improve documentation for LEDForConditionalGeneration (#12303)
* Replace conditional generation example (fixes #12268)

* Replace model in summarization example with finetuned checkpoint, adapt example text

* Fix typo in new summarization example

* Fix docstring formatting, add missing import statement to example
==

src/transformers/models/led/modeling_led.py
==================
1498eb988;Suraj Patil;2021-06-22 18:26:05 +0530;add FlaxAutoModelForImageClassification in main init (#12298)

==

docs/source/model_doc/auto.rst
src/transformers/__init__.py
src/transformers/models/auto/__init__.py
src/transformers/models/auto/modeling_flax_auto.py
src/transformers/utils/dummy_flax_objects.py
==================
2affeb290;Stefan Schweter;2021-06-22 14:47:31 +0200;trainer_tf: adjust wandb installation command (#12291)

==

src/transformers/trainer_tf.py
==================
af6e01c5b;Hamid Shojanazeri;2021-06-22 02:21:30 -0700;Fix for the issue of device-id getting hardcoded for token_type_ids during Tracing [WIP] (#11252)
* registering a buffer for token_type_ids, to pass the error of device-id getting hardcoded when tracing

* sytle format

* adding persistent flag to the resgitered buffers that prevent from adding them to the state_dict and addresses the Backward compatibility issue

* adding the try catch to the fix as persistent flag is only available from PT >1.6

* adding version check

* added the condition to only use the token_type_ids buffer when its autogenerated not passed by user

* adding comments and making the conidtion where token_type_ids are None to use the registered buffer

* taking out position-embeddding from the if block

* adding comments

* handling the case if buffer for position_ids was not registered

* reverted the changes on position_ids, fix the issue with size of token_type_ids buffer, moved the modification for generated token_type_ids to Bertmodel, instead of Embeddings

* reverting the token_type_ids in case of None to the previous version

* reverting changes on position_ids adding back the if block

* changes added by running make fix-copies

* changes added by running make fix-copies and added the import version as it was getting used

* changes added by running make fix-copies

* changes added by running make fix-copies

* fixing the import format

* fixing the import format

* modified to use temp tensor for trimed and expanded token_type_ids buffer

* changes made by fix-copies after temp tensor modifications

* changes made by fix-copies after temp tensor modifications

* changes made by fix-copies after temp tensor modifications

* clean up

* clean up

* clean up

* clean up

* Nit

* Nit

* Nit

* modified according to support device conversion on traced models

* modified according to support device conversion on traced models

* modified according to support device conversion on traced models

* modified according to support device conversion on traced models

* changes based on latest in master

* Adapt templates

* Add version import

Co-authored-by: Ubuntu <ubuntu@ip-172-31-32-81.us-west-2.compute.internal>
Co-authored-by: Lysandre <lysandre.debut@reseau.eseo.fr>
==

src/transformers/models/albert/modeling_albert.py
src/transformers/models/bert/modeling_bert.py
src/transformers/models/big_bird/modeling_big_bird.py
src/transformers/models/electra/modeling_electra.py
src/transformers/models/roberta/modeling_roberta.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_{{cookiecutter.lowercase_modelname}}.py
==================
0d97ba8a9;Stas Bekman;2021-06-21 19:51:36 -0700;[tests] multiple improvements (#12294)
* [tests] multiple improvements

* cleanup

* style

* todo to investigate

* fix
==

docs/source/testing.rst
src/transformers/testing_utils.py
tests/test_trainer.py
==================
dad414d5f;Stas Bekman;2021-06-21 19:30:50 -0700;[trainer + examples] set log level from CLI (#12276)
* set log level from CLI

* add log_level_replica + test + extended docs

* cleanup

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* rename datasets objects to allow datasets module

* improve the doc

* style

* doc improve

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

docs/source/main_classes/trainer.rst
examples/pytorch/translation/run_translation.py
src/transformers/trainer.py
src/transformers/trainer_pt_utils.py
src/transformers/training_args.py
src/transformers/utils/logging.py
tests/test_trainer.py
==================
a4ed074d4;Stas Bekman;2021-06-21 16:50:12 -0700;reset report_to to none, avoid deprecation warning (#12293)

==

tests/test_trainer.py
==================
7ef309ca1;Patrick von Platen;2021-06-21 17:12:12 +0100;[Flax] Add jax flax to env command (#12251)
* fix_torch_device_generate_test

* remove @

* add commands for flax/jax
==

src/transformers/commands/env.py
==================
e3cb7a0b6;Matt;2021-06-21 16:37:28 +0100;Tensorflow QA example (#12252)
* New Tensorflow QA example!

* Style pass

* Updating README.md for the new example

* flake8 fixes

* Update examples/tensorflow/question-answering/README.md

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

examples/tensorflow/question-answering/README.md
examples/tensorflow/question-answering/run_qa.py
examples/tensorflow/question-answering/run_tf_squad.py
examples/tensorflow/question-answering/utils_qa.py
==================
4e9a6796c;Patrick von Platen;2021-06-21 16:37:13 +0100;[Flax] Fix flax test save pretrained (#12256)
* fix_torch_device_generate_test

* remove @

* fix flax save pretrained test
==

tests/test_modeling_flax_common.py
==================
b75b5605c;Stas Bekman;2021-06-21 08:17:00 -0700;[DeepSpeed] don't ignore --adafactor (#12257)

==

src/transformers/deepspeed.py
==================
eb881674f;Suraj Patil;2021-06-21 20:26:42 +0530;[Flax] [WIP] allow loading head model with base model weights (#12255)
* boom boom

* remove flax clip example

* allow loading head model with base model weights

* add test

* fix imports

* disable save, load test for clip

* add test_save_load_to_base
==

src/transformers/modeling_flax_utils.py
tests/test_modeling_flax_clip.py
tests/test_modeling_flax_common.py
==================
8d5b7f36e;Suraj Patil;2021-06-21 20:24:34 +0530;[FlaxClip] fix test from/save pretrained test (#12284)
* boom boom

* remove flax clip example

* fix from_save_pretrained
==

tests/test_modeling_flax_clip.py
==================
b53bc55ba;Vishal Burman;2021-06-21 19:06:44 +0530;Fix for making student ProphetNet for Seq2Seq Distillation (#12130)
* make_student.py: fix to make student ProphetNet

* reformat
==

examples/research_projects/seq2seq-distillation/make_student.py
==================
b76850a80;Lysandre Debut;2021-06-21 08:52:12 +0200;Better CI feedback (#12279)
* Better run ID

* Only part of CI

* Revert "Only part of CI"

This reverts commit 29f7f248d21e0f5792e0670ba8705b31ad8967b7.
==

utils/notification_service.py
==================
30a5521c0;Lysandre;2021-06-21 08:27:25 +0200;Fix the scheduled CI

==

.github/workflows/self-scheduled.yml
==================
2e5dbdf2d;Stas Bekman;2021-06-18 10:00:19 -0700;[t5 doc] make the example work out of the box (#12239)
* [run_clm.py] restore caching

* style

* [t5 doc] make the example work out of the box

This PR expands the training example to include the correct model type for the example to work, e.g. with `T5Model` this example will break.

* Update docs/source/model_doc/t5.rst

Co-authored-by: Suraj Patil <surajp815@gmail.com>

* expand the other example

Co-authored-by: Suraj Patil <surajp815@gmail.com>
==

docs/source/model_doc/t5.rst
==================
f3558bbcf;Xa9aX „ÉÑ;2021-06-18 18:43:45 +0530;Depreciate pythonic Mish and support PyTorch 1.9 version of Mish (#12240)
* Moved Mish to Torch 1.9 version

* Run black formatting
==

src/transformers/activations.py
src/transformers/models/mobilebert/modeling_mobilebert.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_{{cookiecutter.lowercase_modelname}}.py
==================
47a976833;Suraj Patil;2021-06-18 14:59:42 +0530;[FlaxBart] few small fixes (#12247)
* boom boom

* remove flax clip example

* few small fixes
==

src/transformers/models/bart/modeling_flax_bart.py
==================
f74655cd9;Suraj Patil;2021-06-18 13:20:09 +0530;[Flax] FlaxAutoModelForSeq2SeqLM (#12228)
* add FlaxAutoModelForSeq2SeqLM
==

docs/source/model_doc/auto.rst
src/transformers/__init__.py
src/transformers/models/auto/__init__.py
src/transformers/models/auto/modeling_flax_auto.py
src/transformers/utils/dummy_flax_objects.py
==================
e43e11260;Bhavitvya Malik;2021-06-18 01:07:31 +0530;update desc for map in all examples (#12226)
* update desc for map in all examples

* added plm

* suggestions
==

examples/pytorch/language-modeling/requirements.txt
examples/pytorch/language-modeling/run_clm.py
examples/pytorch/language-modeling/run_clm_no_trainer.py
examples/pytorch/language-modeling/run_mlm.py
examples/pytorch/language-modeling/run_mlm_no_trainer.py
examples/pytorch/language-modeling/run_plm.py
examples/pytorch/question-answering/requirements.txt
examples/pytorch/question-answering/run_qa.py
examples/pytorch/question-answering/run_qa_beam_search.py
examples/pytorch/question-answering/run_qa_beam_search_no_trainer.py
examples/pytorch/question-answering/run_qa_no_trainer.py
examples/pytorch/summarization/requirements.txt
examples/pytorch/summarization/run_summarization.py
examples/pytorch/summarization/run_summarization_no_trainer.py
examples/pytorch/token-classification/requirements.txt
examples/pytorch/token-classification/run_ner.py
examples/pytorch/token-classification/run_ner_no_trainer.py
examples/pytorch/translation/requirements.txt
examples/pytorch/translation/run_translation.py
examples/pytorch/translation/run_translation_no_trainer.py
==================
adb70eda4;Sylvain Gugger;2021-06-17 12:39:22 -0400;AutoTokenizer: infer the class from the tokenizer config if possible (#12208)
* AutoTokenizer: infer the class from the tokenizer config if possible

* Add tests

* Update src/transformers/models/auto/tokenization_auto.py

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
==

src/transformers/models/auto/tokenization_auto.py
src/transformers/tokenization_utils_base.py
tests/test_tokenization_auto.py
==================
0daadc191;Lysandre;2021-06-17 18:17:42 +0200;Docs for v4.8.0

==

.circleci/deploy.sh
docs/source/_static/js/custom.js
examples/pytorch/language-modeling/run_clm.py
examples/pytorch/language-modeling/run_mlm.py
examples/pytorch/language-modeling/run_plm.py
examples/pytorch/multiple-choice/run_swag.py
examples/pytorch/question-answering/run_qa.py
examples/pytorch/question-answering/run_qa_beam_search.py
examples/pytorch/question-answering/run_qa_beam_search_no_trainer.py
examples/pytorch/question-answering/run_qa_no_trainer.py
examples/pytorch/summarization/run_summarization.py
examples/pytorch/text-classification/run_glue.py
examples/pytorch/text-classification/run_xnli.py
examples/pytorch/token-classification/run_ner.py
examples/pytorch/translation/run_translation.py
examples/tensorflow/text-classification/run_glue.py
setup.py
src/transformers/__init__.py
==================
7a6c9fab8;Lysandre;2021-06-17 17:57:42 +0200;Release: v4.7.0

==

docs/source/conf.py
examples/pytorch/language-modeling/run_clm.py
examples/pytorch/language-modeling/run_mlm.py
examples/pytorch/language-modeling/run_plm.py
examples/pytorch/multiple-choice/run_swag.py
examples/pytorch/question-answering/run_qa.py
examples/pytorch/question-answering/run_qa_beam_search.py
examples/pytorch/question-answering/run_qa_beam_search_no_trainer.py
examples/pytorch/question-answering/run_qa_no_trainer.py
examples/pytorch/summarization/run_summarization.py
examples/pytorch/text-classification/run_glue.py
examples/pytorch/text-classification/run_xnli.py
examples/pytorch/token-classification/run_ner.py
examples/pytorch/translation/run_translation.py
examples/tensorflow/text-classification/run_glue.py
setup.py
src/transformers/__init__.py
==================
d6ea91c96;Stas Bekman;2021-06-17 08:53:59 -0700;fix pt-1.9.0 `add_` deprecation (#12217)
* fix pt-1.9.0 add_ deprecation

* add () for clarity

* Trigger CI

* require_version(torch
==

src/transformers/optimization.py
==================
3a960c485;Lysandre Debut;2021-06-17 17:29:01 +0200;Support for torch 1.9.0 (#12224)
* Support for torch 1.9.0

* Torch scatter for 1.9.0

* Github Actions run on 1.9.0
==

.circleci/config.yml
.github/workflows/self-push.yml
.github/workflows/self-scheduled.yml
src/transformers/file_utils.py
src/transformers/modeling_fx_utils.py
==================
afdd9e366;Sylvain Gugger;2021-06-17 11:14:53 -0400;Add link to the course (#12229)

==

docs/source/index.rst
==================
29b0aef87;NielsRogge;2021-06-17 16:37:54 +0200;Improve detr (#12147)
* Remove unused variables

* Improve docs

* Fix docs of segmentation masks

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
==

docs/source/model_doc/detr.rst
src/transformers/models/detr/feature_extraction_detr.py
src/transformers/models/detr/modeling_detr.py
==================
b56848c8c;Lysandre Debut;2021-06-17 09:41:16 +0200;Pipeline update & tests (#12207)

==

src/transformers/pipelines/image_classification.py
tests/test_pipelines_image_classification.py
==================
700cee344;Bhadresh Savani;2021-06-16 20:14:53 +0100;[Docs] fixed broken link (#12205)
* fixed broken link

* Update docs/source/benchmarks.rst

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update docs/source/benchmarks.rst

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

docs/source/benchmarks.rst
==================
255a17a08;Sylvain Gugger;2021-06-16 13:17:45 -0400;Use yaml to create metadata (#12185)
* Use yaml to create metadata

* Fix typo

* Remove pin
==

setup.py
src/transformers/dependency_versions_table.py
src/transformers/modelcard.py
==================
15ef0dc5c;Nicolas Patry;2021-06-16 16:28:46 +0200;Enabling AutoTokenizer for HubertConfig. (#12198)

==

src/transformers/models/auto/tokenization_auto.py
==================
afa414d06;Philipp Schmid;2021-06-16 13:24:00 +0200;updated DLC images and sample notebooks (#12191)

==

docs/source/sagemaker.md
==================
ccca51027;Patrick von Platen;2021-06-16 12:14:12 +0100;Hubert (#11889)
* fix_torch_device_generate_test

* remove @

* add hubert

* add first test file

* more docs

* fix bugs

* fix bug

* finish

* finish

* finish docstring

* fix

* fix

* finalize

* add to ignored

* finish

* Apply suggestions from code review

* correct naming

* finish

* fix auto config

* finish

* correct convert script

* Apply suggestions from code review

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
Co-authored-by: Suraj Patil <surajp815@gmail.com>

* apply suggestions lysandre & suraj

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
Co-authored-by: Suraj Patil <surajp815@gmail.com>
==

README.md
docs/source/index.rst
docs/source/model_doc/hubert.rst
src/transformers/__init__.py
src/transformers/models/auto/configuration_auto.py
src/transformers/models/auto/modeling_auto.py
src/transformers/models/hubert/__init__.py
src/transformers/models/hubert/configuration_hubert.py
src/transformers/models/hubert/convert_hubert_original_pytorch_checkpoint_to_pytorch.py
src/transformers/models/hubert/modeling_hubert.py
src/transformers/utils/dummy_pt_objects.py
src/transformers/utils/modeling_auto_mapping.py
tests/test_modeling_hubert.py
utils/check_repo.py
==================
c3c39f7e8;Patrick von Platen;2021-06-16 09:43:54 +0100;[Flax] Add Beam Search (#12131)
* fix_torch_device_generate_test

* remove @

* push new logit processors

* add processors

* save first working version

* save intermediate

* finish

* make style

* make fix-copies

* finish

* Update tests/test_modeling_flax_bart.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Apply suggestions from code review

Co-authored-by: Suraj Patil <surajp815@gmail.com>

Co-authored-by: Patrick von Platen <patrick@huggingface.co>
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
Co-authored-by: Suraj Patil <surajp815@gmail.com>
==

docs/source/internal/generation_utils.rst
src/transformers/__init__.py
src/transformers/generation_flax_logits_process.py
src/transformers/generation_flax_utils.py
src/transformers/utils/dummy_flax_objects.py
tests/test_generation_flax_logits_process.py
tests/test_generation_flax_utils.py
tests/test_modeling_flax_bart.py
==================
802ffaff0;Sylvain Gugger;2021-06-15 16:16:51 -0400;Temporarily deactivate torchhub test (#12184)

==

.github/workflows/github-torch-hub.yml
==================
52c7ca048;Lysandre Debut;2021-06-15 22:03:58 +0200;Temporarily deactivate torch-scatter while we wait for new release (#12181)
* Temporarily deactivate torch-scatter while we wait for new release

* torch-1.8.1 binary for scatter

* Revert to 1.8.0

* Pin torch dependency

* torchaudio and torchvision
==

.circleci/config.yml
==================
7d7ceca39;Sylvain Gugger;2021-06-15 16:01:37 -0400;Model card defaults (#12122)
* [WIP] Model card defaults

* finetuned_from default value

* Add all mappings to the mapping file

* Be more defensive on finetuned_from arg

* Add default task tag

* Separate tags from tasks

* Edge case for dataset

* Apply suggestions from code review

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
==

examples/pytorch/language-modeling/run_clm.py
examples/pytorch/language-modeling/run_mlm.py
examples/pytorch/language-modeling/run_plm.py
examples/pytorch/multiple-choice/run_swag.py
examples/pytorch/question-answering/run_qa.py
examples/pytorch/question-answering/run_qa_beam_search.py
examples/pytorch/summarization/run_summarization.py
examples/pytorch/text-classification/run_glue.py
examples/pytorch/token-classification/run_ner.py
examples/pytorch/translation/run_translation.py
src/transformers/modelcard.py
src/transformers/trainer.py
src/transformers/utils/modeling_auto_mapping.py
utils/class_mapping_update.py
==================
6e7cc5cc5;Stas Bekman;2021-06-15 11:12:59 -0700;[testing] ensure concurrent pytest workers use a unique port for torch.dist (#12166)
* ensure concurrent pytest workers use a unique port for torch.distributed.launch

* reword
==

src/transformers/testing_utils.py
tests/extended/test_trainer_ext.py
tests/test_trainer_distributed.py
==================
b9d66f4c4;Amog Kamsetty;2021-06-15 11:11:29 -0700;Ray Tune Integration Updates (#12134)
* fix

* fixes

* add back to scheduled tests

* formatting

* Update integrations.py
==

.github/workflows/self-scheduled.yml
src/transformers/integrations.py
tests/test_trainer.py
==================
a79585bbf;Kilian Kluge;2021-06-15 16:36:10 +0200;Update AutoModel classes in summarization example (#12178)
- Convert use of deprecated AutoModelWithLMHead to AutoModelForSeq2SeqLM
- Add newly required `truncation=True` to `tokenizer.encode` with `max_length`

This silences all warnings.
==

docs/source/task_summary.rst
==================
d6c929e20;Sylvain Gugger;2021-06-15 09:37:46 -0400;Merge remote-tracking branch 'origin/master'

==
==================
a8694b885;Sylvain Gugger;2021-06-15 09:37:15 -0400;Adjust banner width

==

docs/source/imgs/course_banner.png
==================
955b2b97a;kumapo;2021-06-15 22:33:21 +0900;Enable add_prefix_space if model_type is roberta or gpt2 (#12116)

==

examples/pytorch/token-classification/run_ner.py
examples/pytorch/token-classification/run_ner_no_trainer.py
==================
60b1d6b45;Sylvain Gugger;2021-06-15 09:25:49 -0400;Add course banner (#12157)
* Add course banner

* Update course banner
==

README.md
docs/source/imgs/course_banner.png
==================
d07b540a3;Lysandre Debut;2021-06-15 14:39:05 +0200;Have dummy processors have a `from_pretrained` method (#12145)

==

src/transformers/utils/dummy_flax_objects.py
src/transformers/utils/dummy_pt_objects.py
src/transformers/utils/dummy_sentencepiece_and_speech_objects.py
src/transformers/utils/dummy_vision_objects.py
utils/check_dummies.py
==================
9b393240a;Avital Oliver;2021-06-15 13:12:51 +0200;Use a released version of optax rather than installing from Git. (#12173)
Use a released version of optax rather than installing from Git
==

examples/flax/language-modeling/requirements.txt
examples/flax/text-classification/requirements.txt
==================
9bc9e5986;Patrick von Platen;2021-06-15 11:50:12 +0100;[Flax generate] Add params to generate (#12171)
* fix_torch_device_generate_test

* remove @

* add params as input

* finish
==

src/transformers/generation_flax_utils.py
==================
a55dc157e;Sylvain Gugger;2021-06-15 06:37:37 -0400;Add video links to the documentation (#12162)

==

docs/source/glossary.rst
docs/source/model_sharing.rst
docs/source/model_summary.rst
docs/source/preprocessing.rst
docs/source/quicktour.rst
docs/source/tokenizer_summary.rst
docs/source/training.rst
==================
040283170;Stas Bekman;2021-06-14 13:34:32 -0700;consistent nn. and nn.functional: part 5 docs (#12161)

==

docs/source/add_new_model.rst
docs/source/main_classes/trainer.rst
docs/source/migration.md
docs/source/quicktour.rst
docs/source/task_summary.rst
==================
88e84186e;Stas Bekman;2021-06-14 12:28:24 -0700;[style] consistent nn. and nn.functional: part 4 `examples` (#12156)
* consistent nn. and nn.functional: p4 examples

* restore
==

examples/research_projects/bert-loses-patience/pabee/modeling_pabee_albert.py
examples/research_projects/bert-loses-patience/pabee/modeling_pabee_bert.py
examples/research_projects/bert-loses-patience/run_glue_with_pabee.py
examples/research_projects/bertology/run_bertology.py
examples/research_projects/bertology/run_prune_gpt.py
examples/research_projects/deebert/run_glue_deebert.py
examples/research_projects/deebert/src/modeling_highway_roberta.py
examples/research_projects/distillation/distiller.py
examples/research_projects/distillation/run_squad_w_distillation.py
examples/research_projects/longform-qa/eli5_utils.py
examples/research_projects/lxmert/modeling_frcnn.py
examples/research_projects/lxmert/processing_image.py
examples/research_projects/mm-imdb/run_mmimdb.py
examples/research_projects/mm-imdb/utils_mmimdb.py
examples/research_projects/movement-pruning/Saving_PruneBERT.ipynb
examples/research_projects/movement-pruning/emmental/modules/masked_nn.py
examples/research_projects/movement-pruning/masked_run_glue.py
examples/research_projects/movement-pruning/masked_run_squad.py
examples/research_projects/pplm/pplm_classification_head.py
examples/research_projects/pplm/run_pplm.py
examples/research_projects/pplm/run_pplm_discrim_train.py
examples/research_projects/seq2seq-distillation/_test_seq2seq_examples.py
examples/research_projects/seq2seq-distillation/distillation.py
examples/research_projects/seq2seq-distillation/finetune.py
examples/research_projects/wav2vec2/run_asr.py
examples/research_projects/wav2vec2/run_pretrain.py
==================
372ab9cd6;Stas Bekman;2021-06-14 12:18:22 -0700;[style] consistent nn. and nn.functional: part 3 `tests` (#12155)
* consistent nn. and nn.functional: p3 templates

* restore
==

tests/test_generation_logits_process.py
tests/test_modeling_clip.py
tests/test_modeling_common.py
tests/test_modeling_deit.py
tests/test_modeling_fsmt.py
tests/test_modeling_ibert.py
tests/test_modeling_reformer.py
tests/test_modeling_transfo_xl.py
tests/test_modeling_vit.py
tests/test_optimization.py
tests/test_pipelines_conversational.py
tests/test_pipelines_summarization.py
tests/test_trainer.py
tests/test_trainer_utils.py
==================
d9c0d08f9;Vasudev Gupta;2021-06-15 00:31:03 +0530;Flax Big Bird (#11967)
* add flax bert

* bert -> bigbird

* original_full ported

* add debugger

* init block sparse

* fix copies ; gelu_fast -> gelu_new

* block sparse port

* fix block sparse

* block sparse working

* all ckpts working

* fix-copies

* make quality

* init tests

* temporary fix for FlaxBigBirdForMultipleChoice

* skip test_attention_outputs

* fix

* gelu_fast -> gelu_new ; fix multiple choice model

* remove nsp

* fix sequence classifier

* fix

* make quality

* make fix-copies

* finish

* Delete debugger.ipynb

* Update src/transformers/models/big_bird/modeling_flax_big_bird.py

* make style

* finish

* bye bye jit flax tests

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
==

docs/source/index.rst
docs/source/model_doc/bigbird.rst
src/transformers/__init__.py
src/transformers/models/auto/modeling_flax_auto.py
src/transformers/models/bert/modeling_flax_bert.py
src/transformers/models/big_bird/__init__.py
src/transformers/models/big_bird/configuration_big_bird.py
src/transformers/models/big_bird/modeling_big_bird.py
src/transformers/models/big_bird/modeling_flax_big_bird.py
src/transformers/models/bigbird_pegasus/configuration_bigbird_pegasus.py
src/transformers/models/electra/modeling_flax_electra.py
src/transformers/models/roberta/modeling_flax_roberta.py
src/transformers/utils/dummy_flax_objects.py
tests/test_modeling_flax_big_bird.py
tests/test_modeling_flax_clip.py
tests/test_modeling_flax_common.py
tests/test_modeling_flax_vit.py
==================
a156da9a2;Stas Bekman;2021-06-14 11:41:24 -0700;consistent nn. and nn.functional: p2 templates (#12153)

==

templates/adding_a_new_model/ADD_NEW_MODEL_PROPOSAL_TEMPLATE.md
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_{{cookiecutter.lowercase_modelname}}.py
templates/adding_a_new_model/open_model_proposals/ADD_BIG_BIRD.md
==================
007be9e40;Patrick von Platen;2021-06-14 19:19:10 +0100;[Flax] Fix flax pt equivalence tests (#12154)
* fix_torch_device_generate_test

* remove @

* upload
==

tests/test_modeling_flax_common.py
==================
d438eee03;Will Rice;2021-06-14 13:58:54 -0400;Adding TFWav2Vec2Model (#11617)
* [WIP] Add TFWav2Vec2Model

Work in progress for adding a tensorflow version of Wav2Vec2

* feedback changes

* small fix

* Test Feedback Round 1

* Add SpecAugment and CTC Loss

* correct spec augment mask creation

* docstring and correct copyright

* correct bugs

* remove bogus file

* finish tests correction

* del unnecessary layers

* Update src/transformers/models/wav2vec2/modeling_tf_wav2vec2.py

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* make style

* correct final bug

* Feedback Changes

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
==

docs/source/index.rst
docs/source/model_doc/wav2vec2.rst
src/transformers/__init__.py
src/transformers/convert_pytorch_checkpoint_to_tf2.py
src/transformers/models/auto/modeling_tf_auto.py
src/transformers/models/wav2vec2/__init__.py
src/transformers/models/wav2vec2/modeling_tf_wav2vec2.py
src/transformers/models/wav2vec2/modeling_wav2vec2.py
src/transformers/utils/dummy_tf_objects.py
tests/test_modeling_tf_common.py
tests/test_modeling_tf_wav2vec2.py
utils/check_repo.py
==================
1ed2ebf60;Stas Bekman;2021-06-14 09:44:28 -0700;[style] consistent nn. and nn.functional (#12124)
* consistent nn. and nn.functional

* fix glitch

* fix glitch #2
==

src/transformers/activations.py
src/transformers/generation_utils.py
src/transformers/modeling_fx_utils.py
src/transformers/modeling_utils.py
src/transformers/models/albert/modeling_albert.py
src/transformers/models/bart/modeling_bart.py
src/transformers/models/bert_generation/modeling_bert_generation.py
src/transformers/models/big_bird/modeling_big_bird.py
src/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py
src/transformers/models/blenderbot/modeling_blenderbot.py
src/transformers/models/blenderbot_small/modeling_blenderbot_small.py
src/transformers/models/bort/convert_bort_original_gluonnlp_checkpoint_to_pytorch.py
src/transformers/models/clip/modeling_clip.py
src/transformers/models/convbert/modeling_convbert.py
src/transformers/models/ctrl/modeling_ctrl.py
src/transformers/models/deberta/modeling_deberta.py
src/transformers/models/deberta_v2/modeling_deberta_v2.py
src/transformers/models/detr/feature_extraction_detr.py
src/transformers/models/detr/modeling_detr.py
src/transformers/models/distilbert/modeling_distilbert.py
src/transformers/models/electra/modeling_electra.py
src/transformers/models/flaubert/modeling_flaubert.py
src/transformers/models/flaubert/modeling_tf_flaubert.py
src/transformers/models/fsmt/modeling_fsmt.py
src/transformers/models/funnel/modeling_funnel.py
src/transformers/models/gpt2/modeling_gpt2.py
src/transformers/models/gpt_neo/modeling_gpt_neo.py
src/transformers/models/ibert/modeling_ibert.py
src/transformers/models/ibert/quant_modules.py
src/transformers/models/layoutlm/modeling_layoutlm.py
src/transformers/models/led/modeling_led.py
src/transformers/models/longformer/convert_longformer_original_pytorch_lightning_to_pytorch.py
src/transformers/models/longformer/modeling_longformer.py
src/transformers/models/luke/modeling_luke.py
src/transformers/models/m2m_100/modeling_m2m_100.py
src/transformers/models/marian/convert_marian_to_pytorch.py
src/transformers/models/marian/modeling_marian.py
src/transformers/models/mbart/modeling_mbart.py
src/transformers/models/mmbt/modeling_mmbt.py
src/transformers/models/mobilebert/modeling_mobilebert.py
src/transformers/models/openai/modeling_openai.py
src/transformers/models/pegasus/modeling_pegasus.py
src/transformers/models/prophetnet/convert_prophetnet_original_pytorch_checkpoint_to_pytorch.py
src/transformers/models/prophetnet/modeling_prophetnet.py
src/transformers/models/rag/modeling_rag.py
src/transformers/models/reformer/convert_reformer_trax_checkpoint_to_pytorch.py
src/transformers/models/reformer/modeling_reformer.py
src/transformers/models/retribert/modeling_retribert.py
src/transformers/models/roberta/modeling_roberta.py
src/transformers/models/speech_to_text/modeling_speech_to_text.py
src/transformers/models/squeezebert/modeling_squeezebert.py
src/transformers/models/t5/modeling_t5.py
src/transformers/models/tapas/modeling_tapas.py
src/transformers/models/transfo_xl/modeling_transfo_xl.py
src/transformers/models/transfo_xl/modeling_transfo_xl_utilities.py
src/transformers/models/visual_bert/modeling_visual_bert.py
src/transformers/models/wav2vec2/modeling_wav2vec2.py
src/transformers/models/xlm/modeling_tf_xlm.py
src/transformers/models/xlm/modeling_xlm.py
src/transformers/models/xlnet/modeling_xlnet.py
src/transformers/optimization.py
src/transformers/trainer.py
src/transformers/trainer_pt_utils.py
==================
ff7c81687;Stas Bekman;2021-06-14 09:43:48 -0700;[optim] implement AdafactorSchedule (#12123)
* implement AdafactorSchedule

* typo

* fix

* Update src/transformers/optimization.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

src/transformers/optimization.py
tests/test_trainer.py
==================
fe3576488;Suraj Patil;2021-06-14 18:42:18 +0530;fix error message (#12148)

==

src/transformers/modeling_flax_pytorch_utils.py
==================
9de62cfbc;Kumar Abhishek;2021-06-14 05:12:22 -0700;[lm examples] Replicate --config_overrides addition to other LM examples (#12135)
* [lm examples] Replicate --config_overrides addition to other LM examples

* Removing no trainer files changes

* Update README

Co-authored-by: Kumar Abhishek <kabhishek@expedia.com>
==

examples/pytorch/language-modeling/README.md
examples/pytorch/language-modeling/run_mlm.py
examples/pytorch/language-modeling/run_plm.py
==================
cd7961b63;Nicholas Broad;2021-06-14 08:11:13 -0400;Use text_column_name variable instead of "text" (#12132)
* Use text_column_name variable instead of "text"

`text_column_name` was already defined above where I made the changes and it was also used below where I made changes.

This is a very minor change. If a dataset does not use "text" as the column name, then the `tokenize_function` will now use whatever column is assigned to `text_column_name`. `text_column_name` is just the first column name if "text" is not a column name. It makes the function a little more robust, though I would assume that 90% + of datasets use "text" anyway.

* black formatting

* make style

Co-authored-by: Nicholas Broad <nicholas@nmbroad.com>
==

examples/pytorch/language-modeling/run_mlm.py
examples/pytorch/language-modeling/run_mlm_no_trainer.py
==================
b8ab54134;Sylvain Gugger;2021-06-14 08:03:33 -0400;Don't log anything before logging is setup in examples (#12121)
* Don't log anything before logging is setup in examples

* Last example
==

examples/pytorch/language-modeling/run_clm.py
examples/pytorch/language-modeling/run_mlm.py
examples/pytorch/language-modeling/run_plm.py
examples/pytorch/multiple-choice/run_swag.py
examples/pytorch/question-answering/run_qa.py
examples/pytorch/question-answering/run_qa_beam_search.py
examples/pytorch/summarization/run_summarization.py
examples/pytorch/text-classification/run_glue.py
examples/pytorch/text-classification/run_xnli.py
examples/pytorch/token-classification/run_ner.py
examples/pytorch/translation/run_translation.py
==================
7566fefa6;Patrick von Platen;2021-06-14 11:00:29 +0100;[Flax] Add links to google colabs (#12146)
* fix_torch_device_generate_test

* remove @

* add colab links
==

examples/flax/README.md
examples/flax/language-modeling/README.md
==================
476ba679d;SaulLu;2021-06-14 11:58:44 +0200;Feature to use the PreTrainedTokenizerFast class as a stand-alone tokenizer (#11810)
* feature for tokenizer without slow/legacy version

* format

* modify common test

* add tests

* add PreTrainedTokenizerFast to AutoTokenizer

* format

* change tokenizer common test in order to be able to run test without a slow version

* update tokenizer fast test in order to use `rust_tokenizer_class` attribute instead of `tokenizer_class`

* add autokenizer test

* replace  `if self.tokenizer_class is not None` with ` if self.tokenizer_class is None`

* remove obsolete change in comment

* Update src/transformers/tokenization_utils_base.py

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* Update src/transformers/tokenization_utils_fast.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* change `get_main_tokenizer` into `get_tokenizers`

* clarify `get_tokenizers` method

* homogenize with `test_slow_tokenizer` and `test_rust_tokenizer`

* add `test_rust_tokenizer = False` to tokenizer which don't define a fast version

* `test_rust_tokenizer = False` for BertJapaneseTokenizer

* `test_rust_tokenizer = False` for BertJapaneseCharacterTokenizationTest

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

src/transformers/models/auto/tokenization_auto.py
src/transformers/tokenization_utils_base.py
src/transformers/tokenization_utils_fast.py
tests/test_tokenization_auto.py
tests/test_tokenization_bert_generation.py
tests/test_tokenization_bert_japanese.py
tests/test_tokenization_bertweet.py
tests/test_tokenization_clip.py
tests/test_tokenization_common.py
tests/test_tokenization_fast.py
tests/test_tokenization_fsmt.py
tests/test_tokenization_luke.py
tests/test_tokenization_phobert.py
tests/test_tokenization_small_blenderbot.py
==================
4a51b1dd9;Daniel Stancl;2021-06-14 11:46:08 +0200;FlaxBart (#11537)
* Start working on FlaxBart

* Create modeling_flax_bart.py

* Write FlaxBartAttention

* Add FlaxBartEncoderLayer

* Add FlaxBartDecoderLayer and some typing

* Add helepr function for FlaxBart

* shift_tokens_right

* _make_causal_mask

* _expand_mask

* Add PositionalEmbedding and fix init_std naming

* Add FlaxBartPretrainedModel

* Add FlaxBartEncoder

* Add FlaxBartEncoder

* Add FlaxBartEncoder among modules to be imported

* YET WE CANNOT INITIALIZE THAT!! :(

* Make BartEncoder working

Change BartEncoder to instance of nn.Module so far

* Add FlaxBartDecoder

* Add FlaxBartModel

* TODO to make model run -> Prepapre model inputs

* Resolve padding

* Add FlaxBartModel

* Add FlaxBartModel into importable modules

* Remove FlaxBartEncoder and FlaxBartDecoder from importable modules

* make style; not properly working

* make style; make quality not pass due to some import I left

* Remove TODO for padding_idx in nn.Embed so far

* Add FlaxBartForConditionalGeneration

* Incorporate Flax model output classes, i.e. return_dict

* Add another models and incorporate use_cache arg

* Add FlaxBartForSequenceClassification and FlaxBartForQuestionAnswering

* Incorporate use_cache arg from PyTorch implementation

* Add all necessary Flax output utils

* Add FlaxBartForCausalLM; not working yet'

* Add minor improvements; still lacks some functionality

* Update docs, src and tests

* Add support of FlaxBart to docs/source

* Fix some bugs in FlaxBart souce code

* Add some neccessary tests for FlaxBart models - jit_compilation not passing

* Fix tests and add test_head_masking

* Fix tests for @jax.jit computation

* Add test_head_masking

* Migrate FlaxBart tests from jax.numpy to numpy

* Remove FlaxBartForCausalLM

* Clean repo

* fix bart model weight structure

* Fix FlaxBartForSequenceClassification

Slicing is not possible to use below jit, therefore, selecting sentence
representation from hidden_states must be changed.

* Allow FlaxBartForSequenceClassification for testing pt_flax equivalence

* Allow testing for FlaxBartForQA for pt_flax equivalence

* Add a comment to FlaxBartForSequenceClassification + change noise from 1e-3 to 1e-6

* remove past_key_values

* remove inputs_mebeds and make input_ids required

* add position ids

* re-write attention layer

* fix dataclass

* fix pos embeds and attention output

* fix pos embeds

* expose encode method

* expose decode method

* move docstring to top

* add cache for causal attn layer

* remove head masking for now

* s2s greedy search first pass

* boom boom

* fix typos

* fix greedy generate for bart

* use encoder, decoder layers instead of num_hidden_layers

* handle encoder_outputs

* cleanup

* simplify decoding

* more clean-up

* typos

* Change header + add {decoder_,}position_ids into 2 models

* add BartConfig

* fix existing tests

* add encode, decode methods

* Fix shift_tokens_right for JIT compilation + clarify one condition

* fix decode

* encoder => encode

* simplify generate

* add tests for encode and decode

* style

* add tests for cache

* fix equivalence tests

* sample generate now works with seq2seq

* generation tests

* initialize dense layers

* docstring and cleanup

* quality

* remove get/set input_embeddings

* address Patricks suggestions

* decode for every model, remove encoder_outputs from call

* update tests accordingly

* decode returns only decoder outputs and logits

* fix arguments

* doc encode, decode methods

* correct base_model_prefix

* fix test for seq classif model

* fix docs

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
Co-authored-by: Suraj Patil <surajp815@gmail.com>
==

docs/source/index.rst
docs/source/model_doc/bart.rst
src/transformers/__init__.py
src/transformers/generation_flax_utils.py
src/transformers/modeling_flax_outputs.py
src/transformers/models/auto/modeling_flax_auto.py
src/transformers/models/bart/__init__.py
src/transformers/models/bart/modeling_flax_bart.py
src/transformers/utils/dummy_flax_objects.py
tests/test_modeling_flax_bart.py
tests/test_modeling_flax_common.py
==================
d36fce823;Suraj Patil;2021-06-14 15:03:55 +0530;add readme for flax clm (#12111)
* add readme for flax clm

* use section link for tokenizer

* Apply suggestions from code review

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* update metrics

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
==

examples/flax/language-modeling/README.md
==================
16c0efca2;Patrick von Platen;2021-06-14 10:31:21 +0100;Add mlm pretraining xla torch readme (#12011)
* fix_torch_device_generate_test

* remove @

* upload

* Apply suggestions from code review

* Apply suggestions from code review

* Apply suggestions from code review

* Update examples/flax/language-modeling/README.md

* add more info

* finish

* fix

Co-authored-by: Patrick von Platen <patrick@huggingface.co>
==

examples/flax/language-modeling/README.md
examples/flax/text-classification/README.md
==================
ecd6efe7c;Guido Novati;2021-06-14 10:57:55 +0200;Fix megatron_gpt2 attention block's causal mask (#12007)
* Fix megatron_gpt2 attention block's causal mask.

* compatibility with checkpoints created with recent versions of Megatron-LM

* added integration test for the released Megatron-GPT2 model

* code style changes

* added option to megatron conversion script to read from config file

Co-authored-by: Guido Novati <gnovati@nvidia.com>
==

src/transformers/models/megatron_gpt2/convert_megatron_gpt2_checkpoint.py
tests/test_modeling_megatron_gpt2.py
==================
783b0dd58;Jonathan Chang;2021-06-13 19:02:57 +0800;Fix t5 error message (#12136)
* Fix t5 error message

* Fix again
==

src/transformers/models/t5/modeling_t5.py
src/transformers/models/t5/modeling_tf_t5.py
==================
3b1f5caff;Lysandre Debut;2021-06-11 18:27:10 +0200;Add from_pretrained to dummy timm objects (#12097)
* Add from_pretrained to dummy timm

* Fix at the source

* Update utils/check_dummies.py

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* Missing pretrained dummies

* Style

Co-authored-by: Sylvain Gugger <sylvain.gugger@gmail.com>
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

src/transformers/utils/dummy_flax_objects.py
src/transformers/utils/dummy_pt_objects.py
src/transformers/utils/dummy_sentencepiece_objects.py
src/transformers/utils/dummy_tf_objects.py
src/transformers/utils/dummy_timm_and_vision_objects.py
src/transformers/utils/dummy_timm_objects.py
src/transformers/utils/dummy_tokenizers_objects.py
utils/check_dummies.py
==================
15b498f3b;Suraj Patil;2021-06-11 15:16:20 +0530;Flax CLM script (#12023)
* first draft

* max_seq_length => block_size

* fix arg names

* fix typos

* fix loss calculation

* add max examples, fix  train eval steps, metrics

* optimizer mask

* fix perpelexity, metric logging

* fix logging

* data_collator = > data_loader

* refactor loss_fn

* support single GPU

* pass distributed to write_metric

* fix jitting

* fix single device training

* fix single device metrics

* close inner progress bars once finished

* add overwrite_cache arg

* ifx dataset caching issue

* add more logs

* few small fixes,

* address nicholas suggestions

* fix docstr

* address patricks suggestions

* make flake happy

* pass new new_dropout_rng to apply_gradients

* reset train metrics after every epoc

* remove distributed logis, small fixes
==

examples/flax/language-modeling/run_clm_flax.py
==================
e47765d88;Patrick von Platen;2021-06-11 09:04:07 +0100;Fix head masking generate tests (#12110)
* fix_torch_device_generate_test

* remove @

* fix tests
==

tests/test_generation_utils.py
==================
d2753dcbe;Bhavitvya Malik;2021-06-11 01:29:55 +0530;add relevant description to tqdm in examples (#11927)
* add relevant `desc` in examples

* require_version datasets>=1.8.0
==

examples/pytorch/text-classification/requirements.txt
examples/pytorch/text-classification/run_glue.py
examples/pytorch/text-classification/run_glue_no_trainer.py
examples/pytorch/text-classification/run_xnli.py
==================
9a9314f6d;Jayendra;2021-06-10 21:17:13 +0530;Flax VisionTransformer (#11951)
* adding vit for flax

* added test for Flax-vit and some bug-fixes

* overrided methods where variable changes were necessary for flax_vit test

* added FlaxViTForImageClassification for test

* Update src/transformers/models/vit/modeling_flax_vit.py

Co-authored-by: Suraj Patil <surajp815@gmail.com>

* made changes suggested in PR

* Adding jax-vit models for autoimport

* swapping num_channels and height,width dimension

* fixing the docstring for torch-like inputs for VIT

* add model to main init

* add docs

* doc, fix-copies

* docstrings

* small test fixes

* fix docs

* fix docstr

* Apply suggestions from code review

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* style

Co-authored-by: jayendra <jayendra@infocusp.in>
Co-authored-by: Suraj Patil <surajp815@gmail.com>
Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
==

docs/source/index.rst
docs/source/model_doc/vit.rst
src/transformers/__init__.py
src/transformers/models/auto/__init__.py
src/transformers/models/auto/modeling_flax_auto.py
src/transformers/models/vit/__init__.py
src/transformers/models/vit/modeling_flax_vit.py
src/transformers/utils/dummy_flax_objects.py
tests/test_modeling_flax_vit.py
==================
0eaeae2e3;Daniel Stancl;2021-06-10 16:28:07 +0200;Fix a condition in test_generate_with_head_masking (#11911)
* Fix a condition in test_generate_with_head_masking

* Fix usage of head_mask in bigbirg_pegasus

* Fix head masking for speech2text

* Resolve copy mismatch + drop unwanted print statement

* Fix the condition
==

src/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py
src/transformers/models/speech_to_text/modeling_speech_to_text.py
tests/test_generation_utils.py
==================
bebbdd0fc;Matt;2021-06-10 15:25:04 +0100;Appending label2id and id2label to models to ensure inference works properly (#12102)

==

examples/pytorch/text-classification/run_glue.py
examples/pytorch/text-classification/run_glue_no_trainer.py
==================
4cda08dec;Matt;2021-06-10 15:10:57 +0100;Minor style edits

==

examples/tensorflow/text-classification/README.md
==================
7f08dbd10;Matt;2021-06-10 14:33:42 +0100;Update README.md to cover the TF GLUE example.

==

examples/tensorflow/text-classification/README.md
==================
d72e5a3a6;Sylvain Gugger;2021-06-10 09:27:11 -0400;Fix quality

==

examples/pytorch/token-classification/run_ner_no_trainer.py
==================
73a532651;Matt;2021-06-10 14:14:37 +0100;New TF GLUE example (#12028)
* Pushing partially-complete new GLUE example

* First draft of the new TF GLUE example! Needs a little more testing to be sure but it's almost ready.

* Fix to the fit() call

* Bugfixes, making sure TPU and multi-GPU support is ready

* Remove logger line that depends on Pytorch

* Style pass

* Deleting old TF GLUE example

* Include label2id and id2label in the saved model config

* Don't clobber the existing model.config.label2id

* Style fixes

* Update examples/tensorflow/text-classification/run_glue.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

examples/tensorflow/text-classification/run_glue.py
examples/tensorflow/text-classification/run_text_classification.py
examples/tensorflow/text-classification/run_tf_glue.py
==================
9d2cee8b4;Tobias Norlund;2021-06-10 15:10:41 +0200;CLIPFeatureExtractor should resize images with kept aspect ratio (#11994)
* Resize with kept aspect ratio

* Fixed failed test

* Overload center_crop and resize methods instead

* resize should handle non-PIL images

* update slow test

* Tensor => tensor

Co-authored-by: patil-suraj <surajp815@gmail.com>
==

src/transformers/models/clip/feature_extraction_clip.py
tests/test_modeling_clip.py
==================
472a86762;kumapo;2021-06-10 21:03:20 +0900;Add text_column_name and label_column_name to run_ner and run_ner_no_trainer args (#12083)
* Add text_column_name and label_column_name to run_ner args

* Minor fix: grouping for text and label column name
==

examples/pytorch/token-classification/run_ner.py
examples/pytorch/token-classification/run_ner_no_trainer.py
==================
bc6f51e53;Patrick von Platen;2021-06-09 20:41:59 +0100;[Wav2Vec2ForPretraining] Correct checkpoints wav2vec2 & fix tests (#12089)
* fix_torch_device_generate_test

* remove @

* fix tests
==

tests/test_modeling_wav2vec2.py
==================
61e191987;Stas Bekman;2021-06-09 11:02:52 -0700;rm require_version_examples (#12088)

==

examples/legacy/pytorch-lightning/lightning_base.py
examples/research_projects/rag-end2end-retriever/lightning_base.py
examples/research_projects/rag/lightning_base.py
examples/research_projects/seq2seq-distillation/lightning_base.py
src/transformers/utils/versions.py
tests/test_versions_utils.py
==================
d1500d915;Suraj Patil;2021-06-09 23:19:27 +0530;pass decay_mask fn to optimizer (#12087)

==

examples/flax/language-modeling/run_mlm_flax.py
==================
d472bd7b1;Anton Lozhkov;2021-06-09 19:40:56 +0200;Wav2Vec2 Pretraining (#11306)
* Working quantizer forward

* Working quantizer forward

* Clean up unused model parts, test reproducibility

* Working quantizer forward

* Clean up unused model parts, test reproducibility

* Remove custom outputs from the shared ones

* correct conversion

* correct bug

* add first pretrain script

* save intermediate

* static shapes

* save intermediate

* finish first pretrain script version

* more refactor

* remove wanddb

* refactor more

* improve test

* correct perplexity compute bug

* finish model implementation

* add to docs

* finish docs

* finish pretraining script

* finish pretraining script

* remove wandb

* finish PR for merge

* finish config

* finish

* make deepspeed work

* Apply suggestions from code review

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* apply suggestions

* fix flaky test

Co-authored-by: patrickvonplaten <patrick.v.platen@gmail.com>
Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

docs/source/model_doc/wav2vec2.rst
examples/research_projects/wav2vec2/README.md
examples/research_projects/wav2vec2/run_pretrain.py
src/transformers/__init__.py
src/transformers/models/auto/modeling_auto.py
src/transformers/models/wav2vec2/__init__.py
src/transformers/models/wav2vec2/configuration_wav2vec2.py
src/transformers/models/wav2vec2/convert_wav2vec2_original_pytorch_checkpoint_to_pytorch.py
src/transformers/models/wav2vec2/modeling_wav2vec2.py
src/transformers/utils/dummy_pt_objects.py
tests/test_modeling_wav2vec2.py
==================
b1a8aa94f;Stas Bekman;2021-06-09 09:23:47 -0700;[test] support more than 2 gpus (#12074)
* support more than 2 gpus

* style
==

tests/test_trainer.py
==================
d3eacbb82;NielsRogge;2021-06-09 17:51:13 +0200;Add DETR (#11653)
* Squash all commits of modeling_detr_v7 branch into one

* Improve docs

* Fix tests

* Style

* Improve docs some more and fix most tests

* Fix slow tests of ViT, DeiT and DETR

* Improve replacement of batch norm

* Restructure timm backbone forward

* Make DetrForSegmentation support any timm backbone

* Fix name of output

* Address most comments by @LysandreJik

* Give better names for variables

* Conditional imports + timm in setup.py

* Address additional comments by @sgugger

* Make style, add require_timm and require_vision to tests√©

* Remove train_backbone attribute of DetrConfig, add methods to freeze/unfreeze backbone

* Add png files to fixtures

* Fix type hint

* Add timm to workflows

* Add `BatchNorm2d` to the weight initialization

* Fix retain_grad test

* Replace model checkpoints by Facebook namespace

* Fix name of checkpoint in test

* Add user-friendly message when scipy is not available

* Address most comments by @patrickvonplaten

* Remove return_intermediate_layers attribute of DetrConfig and simplify Joiner

* Better initialization

* Scipy is necessary to get sklearn metrics

* Rename TimmBackbone to DetrTimmConvEncoder and rename DetrJoiner to DetrConvModel

* Make style

* Improve docs and add 2 community notebooks

Co-authored-by: Lysandre <lysandre.debut@reseau.eseo.fr>
==

.circleci/config.yml
.github/workflows/self-push.yml
.github/workflows/self-scheduled.yml
README.md
docs/source/community.md
docs/source/index.rst
docs/source/model_doc/detr.rst
setup.py
src/transformers/__init__.py
src/transformers/dependency_versions_table.py
src/transformers/file_utils.py
src/transformers/models/__init__.py
src/transformers/models/auto/__init__.py
src/transformers/models/auto/configuration_auto.py
src/transformers/models/auto/modeling_auto.py
src/transformers/models/detr/__init__.py
src/transformers/models/detr/configuration_detr.py
src/transformers/models/detr/convert_detr_original_pytorch_checkpoint_to_pytorch.py
src/transformers/models/detr/feature_extraction_detr.py
src/transformers/models/detr/modeling_detr.py
src/transformers/testing_utils.py
src/transformers/utils/coco_classes.py
src/transformers/utils/dummy_pt_objects.py
src/transformers/utils/dummy_timm_and_vision_objects.py
src/transformers/utils/dummy_timm_objects.py
src/transformers/utils/dummy_vision_objects.py
tests/fixtures/coco.jpg
tests/fixtures/tests_samples/.gitignore
tests/fixtures/tests_samples/COCO/000000039769.png
tests/fixtures/tests_samples/COCO/coco_annotations.txt
tests/fixtures/tests_samples/COCO/coco_panoptic/000000039769.png
tests/fixtures/tests_samples/COCO/coco_panoptic_annotations.txt
tests/test_feature_extraction_common.py
tests/test_feature_extraction_deit.py
tests/test_feature_extraction_detr.py
tests/test_feature_extraction_vit.py
tests/test_modeling_common.py
tests/test_modeling_deit.py
tests/test_modeling_detr.py
tests/test_modeling_vit.py
tests/test_pipelines_image_classification.py
utils/check_repo.py
==================
d14e0af27;Stas Bekman;2021-06-09 05:21:03 -0700;sync LayerDrop for Wav2Vec2Encoder + tests (#12076)

==

examples/research_projects/wav2vec2/test_wav2vec2_deepspeed.py
src/transformers/models/wav2vec2/modeling_wav2vec2.py
==================
82a2b76c9;Koichi Yasuoka;2021-06-09 20:27:05 +0900;Update run_ner.py with id2label config (#12001)

==

examples/pytorch/token-classification/run_ner.py
==================
0e82f0cbc;Stas Bekman;2021-06-08 12:55:17 -0700;typo

==

docs/source/main_classes/deepspeed.rst
==================
11d86d3de;Stas Bekman;2021-06-08 12:32:03 -0700;[Deepspeed Wav2vec2] integration (#11638)
* wip

* wip - but working with https://github.com/microsoft/DeepSpeed/pull/1044

* cleanup

* workaround

* working 5/8 modes

* solve fp32 distributed zero3

* style

* sync

* sync

* rework

* deprecation

* cleanup

* https://github.com/microsoft/DeepSpeed/pull/1044 pr was merged

* clean up

* add a guide

* more prose

* more prose

* fix

* more prose

* sub_group_size was too big

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* refactor

* bug fix

* make the true check explicit

* new deepspeed release

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

examples/research_projects/wav2vec2/README.md
examples/research_projects/wav2vec2/ds_config_wav2vec2_zero2.json
examples/research_projects/wav2vec2/ds_config_wav2vec2_zero3.json
examples/research_projects/wav2vec2/test_wav2vec2_deepspeed.py
setup.py
src/transformers/deepspeed.py
src/transformers/dependency_versions_table.py
src/transformers/models/wav2vec2/modeling_wav2vec2.py
src/transformers/testing_utils.py
src/transformers/trainer.py
tests/deepspeed/test_deepspeed.py
==================
32290d87f;Stas Bekman;2021-06-08 08:36:15 -0700;[Deepspeed] various fixes (#12058)
* replace deprecated config

* sub_group_size was too big

* complete deprecation removal
==

docs/source/main_classes/deepspeed.rst
src/transformers/deepspeed.py
tests/deepspeed/ds_config_zero2.json
tests/deepspeed/ds_config_zero3.json
tests/deepspeed/test_deepspeed.py
==================
fd6902838;Sylvain Gugger;2021-06-08 10:27:02 -0400;Properly indent block_size (#12070)

==

examples/pytorch/language-modeling/run_clm.py
==================
49bee0aea;cdleong;2021-06-08 09:02:35 -0400;Add torch to requirements.txt in language-modeling (#12040)
* Add torch to requirements.txt in language-modeling

* Update examples/pytorch/language-modeling/requirements.txt

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

examples/pytorch/language-modeling/requirements.txt
==================
f5eec0d8e;Mario ≈†a≈°ko;2021-06-08 05:58:38 -0700;Replace legacy tensor.Tensor with torch.tensor/torch.empty (#12027)
* Replace legacy torch.Tensor constructor with torch.{tensor, empty}

* Remove torch.Tensor in examples
==

examples/research_projects/lxmert/modeling_frcnn.py
examples/research_projects/lxmert/utils.py
examples/research_projects/movement-pruning/emmental/modules/masked_nn.py
examples/research_projects/rag-end2end-retriever/finetune_rag.py
examples/research_projects/rag/finetune_rag.py
src/transformers/models/convbert/modeling_convbert.py
tests/test_activations.py
tests/test_modeling_bart.py
tests/test_modeling_clip.py
tests/test_modeling_fsmt.py
tests/test_modeling_ibert.py
tests/test_modeling_mbart.py
tests/test_modeling_roberta.py
==================
e33085d64;Shamane Siri;2021-06-09 00:42:49 +1200;updated the original RAG implementation to be compatible with latest Pytorch-Lightning (#11806)
* updated the original RAG implementation to be compatible with the latest PL version

* updated the requirements.txt file

* execute make style

* code quality test

* code quality

* conflix resolved in requirement.txt

* code quality

* changed the MyDDP class name to CustomDDP
==

examples/research_projects/rag/callbacks_rag.py
examples/research_projects/rag/distributed_ray_retriever.py
examples/research_projects/rag/finetune_rag.py
examples/research_projects/rag/lightning_base.py
examples/research_projects/rag/requirements.txt
==================
70f88eecc;NielsRogge;2021-06-08 11:22:31 +0200;Fix tapas issue (#12063)
* Fix scatter function to be compatible with torch-scatter 2.7.0

* Allow test again
==

src/transformers/models/tapas/modeling_tapas.py
tests/test_modeling_tapas.py
==================
e56e3140d;NielsRogge;2021-06-08 11:21:38 +0200;Fix integration tests (#12066)

==

tests/test_modeling_luke.py
==================
4abc6dd69;Stas Bekman;2021-06-07 20:48:41 -0700;skip failing test (#12059)

==

src/transformers/dependency_versions_table.py
tests/test_modeling_tapas.py
==================
e363e1d93;Russell Klopfer;2021-06-07 22:34:10 -0400;adds metric prefix. (#12057)
* adds metric prefix.

* update tests to include prefix
==

examples/pytorch/question-answering/trainer_qa.py
examples/pytorch/test_examples.py
==================
8994c1e47;Peter Izsak;2021-06-07 18:47:12 +0300;Add optional grouped parsers description to HfArgumentParser (#12042)
* Adding optional argument group to HfArgumentParser

* Minor

* remove whitespace

* Minor styling
==

src/transformers/hf_argparser.py
==================
2056f26e8;Nicolas Patry;2021-06-07 08:41:27 -0700;Extend pipelines for automodel tupels (#12025)
* fix_torch_device_generate_test

* remove @

* finish

* refactor

* add test

* fix test

* Attempt at simplification.

* Small fix.

* Fixing non existing AutoModel for TF.

* Naming.

* Remove extra condition.

Co-authored-by: patrickvonplaten <patrick.v.platen@gmail.com>
==

src/transformers/pipelines/__init__.py
src/transformers/pipelines/base.py
tests/test_pipelines_conversational.py
==================
f8bd8c6c7;Fran√ßois Lagunas;2021-06-07 17:21:59 +0200;Fixes bug that appears when using QA bert and distilation. (#12026)
* Fixing bug that appears when using distilation (and potentially other uses).
During backward pass Pytorch complains with:
RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation
This happens because the QA model code modifies the start_positions and end_positions input tensors, using clamp_ function: as a consequence the teacher and the student both modifies the inputs, and backward pass fails.

* Fixing all models QA clamp_ bug.
==

src/transformers/models/albert/modeling_albert.py
src/transformers/models/bart/modeling_bart.py
src/transformers/models/bert/modeling_bert.py
src/transformers/models/big_bird/modeling_big_bird.py
src/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py
src/transformers/models/convbert/modeling_convbert.py
src/transformers/models/deberta/modeling_deberta.py
src/transformers/models/deberta_v2/modeling_deberta_v2.py
src/transformers/models/distilbert/modeling_distilbert.py
src/transformers/models/electra/modeling_electra.py
src/transformers/models/funnel/modeling_funnel.py
src/transformers/models/ibert/modeling_ibert.py
src/transformers/models/led/modeling_led.py
src/transformers/models/longformer/modeling_longformer.py
src/transformers/models/mbart/modeling_mbart.py
src/transformers/models/megatron_bert/modeling_megatron_bert.py
src/transformers/models/mobilebert/modeling_mobilebert.py
src/transformers/models/mpnet/modeling_mpnet.py
src/transformers/models/reformer/modeling_reformer.py
src/transformers/models/roberta/modeling_roberta.py
src/transformers/models/roformer/modeling_roformer.py
src/transformers/models/squeezebert/modeling_squeezebert.py
src/transformers/models/xlm/modeling_xlm.py
src/transformers/models/xlnet/modeling_xlnet.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_{{cookiecutter.lowercase_modelname}}.py
==================
59f75d538;Patrick von Platen;2021-06-07 13:04:18 +0100;[JAX] Bump jax lib (#12053)
* fix_torch_device_generate_test

* remove @

* bump up jax lib
==

setup.py
==================
185122ef2;Suraj Patil;2021-06-07 15:24:03 +0530;fix docs of past_key_values (#12049)

==

src/transformers/models/bart/modeling_bart.py
src/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py
src/transformers/models/blenderbot/modeling_blenderbot.py
src/transformers/models/blenderbot_small/modeling_blenderbot_small.py
src/transformers/models/led/modeling_led.py
src/transformers/models/m2m_100/modeling_m2m_100.py
src/transformers/models/marian/modeling_marian.py
src/transformers/models/mbart/modeling_mbart.py
src/transformers/models/pegasus/modeling_pegasus.py
src/transformers/models/speech_to_text/modeling_speech_to_text.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_{{cookiecutter.lowercase_modelname}}.py
==================
3857f2b4e;Philip May;2021-06-07 10:55:55 +0200;fix deberta 2 tokenizer integration test (#12017)

==

tests/test_tokenization_common.py
tests/test_tokenization_deberta_v2.py
==================
20b6f3b80;Shiva Pundir;2021-06-07 11:44:25 +0530;Fixed Typo in modeling_bart.py (#12035)
* Fixed Typo in modeling_bart.py - Issue #11895

* Fixed Typo in modeling_bart.py
==

src/transformers/models/bart/modeling_bart.py
src/transformers/models/blenderbot_small/modeling_blenderbot_small.py
src/transformers/models/marian/modeling_marian.py
==================
1f335aef3;Stas Bekman;2021-06-04 09:39:38 -0700;[TrainerArguments] format and sort __repr__, add __str__ (#12018)
* format and sort __repr__, add __str__

* typo

* use __str__ directly

* alias __repr__ = __str__
==

src/transformers/training_args.py
==================
2c73b9309;Stas Bekman;2021-06-04 08:58:23 -0700;[Deepspeed] Assert on mismatches between ds and hf args (#12021)
* wip

* add mismatch validation + test

* renames

* Update docs/source/main_classes/deepspeed.rst

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* renames

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

docs/source/main_classes/deepspeed.rst
src/transformers/deepspeed.py
tests/deepspeed/test_deepspeed.py
==================
242ec31aa;Patrick von Platen;2021-06-03 16:31:32 +0100;[Flax] Refactor MLM  (#12013)
* fix_torch_device_generate_test

* remove @

* finish refactor

Co-authored-by: Patrick von Platen <patrick@huggingface.co>
==

examples/flax/language-modeling/run_mlm_flax.py
==================
4674061b2;Nicholas Vadivelu;2021-06-03 06:35:26 -0400;Fix weight decay masking in `run_flax_glue.py` (#11964)
* Fix weight decay masking in `run_flax_glue.py`

Issues with the previous implementation:
- The `dict` from `traverse_util.flatten_dict` has keys which are tuples of strings, not one long string with the path separated by periods.
- `optax.masked` applies the transformation wherever the mask is True, so the masks are flipped.
- Flax's LayerNorm calls the scale parameter `scale` not `weight`

* Fix formatting with black

* adapt results

Co-authored-by: Patrick von Platen <patrick@huggingface.co>
==

examples/flax/text-classification/README.md
examples/flax/text-classification/run_flax_glue.py
==================
61c506349;Stas Bekman;2021-06-02 12:06:37 -0700;[deepspeed] add nvme test skip rule (#11997)
* add nvme skip rule

* fix
==

tests/deepspeed/test_deepspeed.py
==================
640318bef;Stas Bekman;2021-06-02 09:56:00 -0700;[deepspeed] Move code and doc into standalone files (#11984)
* move code and docs

* style

* moved

* restore
==

docs/source/main_classes/deepspeed.rst
docs/source/main_classes/trainer.rst
src/transformers/deepspeed.py
src/transformers/integrations.py
src/transformers/modeling_utils.py
src/transformers/models/auto/auto_factory.py
src/transformers/trainer.py
src/transformers/trainer_seq2seq.py
src/transformers/training_args.py
tests/deepspeed/test_deepspeed.py
==================
d6d747cb2;Kou Yong Kang;2021-06-03 00:53:09 +0800;Update return introduction (#11976)
Make it clear that the `forward` method now returns a dict instead of tuple.

Fix style
==

src/transformers/file_utils.py
==================
d406a2729;Stas Bekman;2021-06-02 09:21:05 -0700;[docs] fix xref to `PreTrainedModel.generate` (#11049)
* fix xref to generate

* do the same for search methods

* style

* style
==

docs/source/internal/generation_utils.rst
docs/source/model_doc/bart.rst
docs/source/model_doc/t5.rst
docs/source/task_summary.rst
src/transformers/models/rag/modeling_rag.py
src/transformers/models/rag/modeling_tf_rag.py
==================
123b597f5;Gunjan Chhablani;2021-06-02 19:42:52 +0530;Fix examples (#11990)

==

src/transformers/models/visual_bert/modeling_visual_bert.py
==================
88ca6a231;Gunjan Chhablani;2021-06-02 18:13:08 +0530;VisualBERT (#10534)
* Init VisualBERT

* Add cookie-cutter, Config, and Embeddings

* Add preliminary Model

* Add Bert analogous classes

* Add basic code for NLVR, VQA, Flickr

* Update Init

* Fix VisualBert Downstream Models

* Rename classifier to cls

* Comment position_ids buffer

* Remove sentence image predictor output

* Update output dicts

* Remove unnecessary files

* Fix Auto Modeling

* Fix transformers init

* Add conversion script

* Add conversion script

* Fix docs

* Update visualbert modelling

* Update configuration

* Style fixes

* Add model and integration tests

* Add all tests

* Update model mapping

* Add simple detector from original repository

* Update docs and configs

* Fix style

* Fix style

* Update docs

* Fix style

* Fix import issues in style

* Fix style

* Add changes from review

* Fix style

* Fix style

* Update docs

* Fix style

* Fix style

* Update docs/source/model_doc/visual_bert.rst

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/models/visual_bert/modeling_visual_bert.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update tests/test_modeling_visual_bert.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/models/visual_bert/modeling_visual_bert.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/models/visual_bert/modeling_visual_bert.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/models/visual_bert/modeling_visual_bert.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Add changes from review

* Remove convert run script

* Add changes from review

* Update src/transformers/models/visual_bert/modeling_visual_bert.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/models/visual_bert/modeling_visual_bert.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/models/visual_bert/modeling_visual_bert.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/models/visual_bert/modeling_visual_bert.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/models/visual_bert/modeling_visual_bert.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Add changes from review

* Add changes from review

* Add visual embedding example in docs

* Fix "copied from" comments

* Add changes from review

* Fix error, style, checkpoints

* Update docs

* Fix integration tests

* Fix style

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

README.md
docs/source/index.rst
docs/source/model_doc/visual_bert.rst
src/transformers/__init__.py
src/transformers/models/__init__.py
src/transformers/models/auto/configuration_auto.py
src/transformers/models/auto/modeling_auto.py
src/transformers/models/visual_bert/__init__.py
src/transformers/models/visual_bert/configuration_visual_bert.py
src/transformers/models/visual_bert/convert_visual_bert_original_pytorch_checkpoint_to_pytorch.py
src/transformers/models/visual_bert/modeling_visual_bert.py
src/transformers/utils/dummy_pt_objects.py
tests/test_modeling_visual_bert.py
utils/check_repo.py
==================
43f46aa7f;Patrick von Platen;2021-06-02 09:17:14 +0100;[RAG] Fix rag from pretrained question encoder generator behavior (#11962)
* fix_torch_device_generate_test

* remove @

* fix rag from pretrained loading

* add test

* uplaod

* finish
==

src/transformers/models/rag/modeling_rag.py
tests/test_modeling_rag.py
==================
6db3a87de;dependabot[bot];2021-06-02 03:40:20 -0400;Bump urllib3 from 1.25.8 to 1.26.5 in /examples/research_projects/lxmert (#11983)
Bumps [urllib3](https://github.com/urllib3/urllib3) from 1.25.8 to 1.26.5.
- [Release notes](https://github.com/urllib3/urllib3/releases)
- [Changelog](https://github.com/urllib3/urllib3/blob/main/CHANGES.rst)
- [Commits](https://github.com/urllib3/urllib3/compare/1.25.8...1.26.5)

---
updated-dependencies:
- dependency-name: urllib3
  dependency-type: direct:production
...

Signed-off-by: dependabot[bot] <support@github.com>

Co-authored-by: dependabot[bot] <49699333+dependabot[bot]@users.noreply.github.com>
==

examples/research_projects/lxmert/requirements.txt
==================
4ba203d9d;Stas Bekman;2021-06-01 15:58:31 -0700;[Trainer] add train loss and flops metrics reports (#11980)
* add train loss and flops metrics reports

* consistency

* add train_loss to skip keys

* restore on_train_end call timing
==

src/transformers/trainer.py
tests/test_trainer.py
==================
7ec596ecd;Stas Bekman;2021-06-01 13:24:52 -0700;[DeepSpeed] decouple `DeepSpeedConfigHF` from `Trainer` (#11966)
* decouple DeepSpeedConfigHF from Trainer

* add LoggingLevel ctx manager; add new test

* cleanup

* add docs

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* implemented suggested renames

* formatter workaround

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

docs/source/index.rst
docs/source/main_classes/deepspeed.rst
src/transformers/integrations.py
src/transformers/testing_utils.py
src/transformers/trainer.py
src/transformers/training_args.py
tests/deepspeed/test_deepspeed.py
==================
1c3ab3e5d;Alberto Villa;2021-06-01 20:58:49 +0200;Typo in usage example, changed to device instead of torch_device (#11979)

==

docs/source/model_doc/pegasus.rst
==================
47a98fc4c;Patrick von Platen;2021-06-01 19:07:37 +0100;ByT5 model (#11971)
* allow tf to use uneven num of layers

* add tokenizer

* finish docs

* finish docs

* Apply suggestions from code review

* include in index

* finish

* Update docs/source/model_doc/byt5.rst

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>

* apply sylvais suggestions

* make style

Co-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>
==

README.md
docs/source/index.rst
docs/source/model_doc/byt5.rst
src/transformers/__init__.py
src/transformers/models/auto/tokenization_auto.py
src/transformers/models/byt5/__init__.py
src/transformers/models/byt5/convert_byt5_original_tf_checkpoint_to_pytorch.py
src/transformers/models/byt5/tokenization_byt5.py
src/transformers/models/t5/modeling_tf_t5.py
tests/test_modeling_t5.py
tests/test_modeling_tf_t5.py
tests/test_tokenization_byt5.py
==================
1eb58b456;Jeoung-Minju;2021-06-02 01:24:59 +0900;typo correction (#11973)
* typo correction

* type corrections
==

src/transformers/generation_utils.py
==================
79712e7e7;Stas Bekman;2021-06-01 09:21:21 -0700;[deepspeed] docs (#11940)
* deepspeed docs

* cleanup

* cleanup
==

docs/source/main_classes/trainer.rst
==================
985d70884;Lysandre;2021-06-01 15:58:31 +0200;Run the integration tests on schedule tests instead of master tests

==

.github/workflows/self-push.yml
.github/workflows/self-scheduled.yml
==================
9996558bf;Volodymyr Byno;2021-06-01 16:40:52 +0300;Neptune.ai integration (#11937)
An option that turns on neptune.ai logging
--report_to 'neptune'

Additional ENV variables:
	NEPTUNE_PROJECT
	NEPTUNE_API_TOKEN
	NEPTUNE_RUN_NAME (optional)
	NEPTUNE_STOP_TIMEOUT (optional)
==

src/transformers/integrations.py
==================
ae6ce28f3;Lysandre Debut;2021-06-01 15:27:54 +0200;Authorize args when instantiating an AutoModel (#11956)

==

src/transformers/models/auto/auto_factory.py
==================
fcad80182;Philip May;2021-06-01 15:24:39 +0200;Add regression tests for slow sentencepiece tokenizers.  (#11737)
* add test_vocab_size for sentencepiece tok.

* add test_get_vocab for sentencepiece tok.

* add test_convert_token_and_id for sentencepiece tok.

* add test_tokenize_and_convert_tokens_to_string for all tok.

* improve test_tokenize_and_convert_tokens_to_string for sp. tok.

* add common tokenizer integration tests
- for albert
- for barthez

* add tokenizer integration tests to bert gen.

* add most tokenizer integration tests

* fix camembert tokenizer integration test

* add tokenizer integration test to marian

* add tokenizer integration test to reformer

* add typing and doc to tokenizer_integration_test_util

* fix tokenizer integration test of reformer

* improve test_sentencepiece_tokenize_and_convert_tokens_to_string

* empty commit to trigger CI

* fix tokenizer integration test of reformer

* remove code not needed anymore

* empty commit to trigger CI

* empty commit to trigger CI
==

tests/test_tokenization_albert.py
tests/test_tokenization_barthez.py
tests/test_tokenization_bert_generation.py
tests/test_tokenization_big_bird.py
tests/test_tokenization_camembert.py
tests/test_tokenization_common.py
tests/test_tokenization_deberta_v2.py
tests/test_tokenization_m2m_100.py
tests/test_tokenization_marian.py
tests/test_tokenization_mbart50.py
tests/test_tokenization_pegasus.py
tests/test_tokenization_reformer.py
tests/test_tokenization_speech_to_text.py
tests/test_tokenization_t5.py
tests/test_tokenization_xlm_prophetnet.py
tests/test_tokenization_xlm_roberta.py
tests/test_tokenization_xlnet.py
==================
c3d958b2c;Josh Tanner;2021-06-01 06:18:33 -0700;reinitialize wandb config for each hyperparameter search run (#11945)

==

src/transformers/integrations.py
==================
99dbbdb91;Riccardo Bassani;2021-06-01 15:04:51 +0200;bugfixes training_args.py (#11922)
modified according to:
https://pytorch.org/xla/release/1.8.1/_modules/torch_xla/core/xla_model.html
==

src/transformers/training_args.py
==================
7e73601f3;Fan Zhang;2021-06-01 20:28:41 +0800;modify qa-trainer (#11872)
* modify qa-trainer

* fix flax model
==

examples/pytorch/question-answering/run_qa_no_trainer.py
src/transformers/models/albert/modeling_albert.py
src/transformers/models/bart/modeling_bart.py
src/transformers/models/bert/modeling_bert.py
src/transformers/models/big_bird/modeling_big_bird.py
src/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py
src/transformers/models/convbert/modeling_convbert.py
src/transformers/models/deberta/modeling_deberta.py
src/transformers/models/deberta_v2/modeling_deberta_v2.py
src/transformers/models/distilbert/modeling_distilbert.py
src/transformers/models/dpr/modeling_dpr.py
src/transformers/models/electra/modeling_electra.py
src/transformers/models/funnel/modeling_funnel.py
src/transformers/models/ibert/modeling_ibert.py
src/transformers/models/led/modeling_led.py
src/transformers/models/longformer/modeling_longformer.py
src/transformers/models/mbart/modeling_mbart.py
src/transformers/models/megatron_bert/modeling_megatron_bert.py
src/transformers/models/mobilebert/modeling_mobilebert.py
src/transformers/models/mpnet/modeling_mpnet.py
src/transformers/models/reformer/modeling_reformer.py
src/transformers/models/roberta/modeling_roberta.py
src/transformers/models/squeezebert/modeling_squeezebert.py
src/transformers/models/xlm/modeling_xlm.py
src/transformers/models/xlnet/modeling_xlnet.py
==================
9ec0f01b6;Shamane Siri;2021-06-01 18:32:26 +1200;RAG-2nd2end-revamp (#11893)
* initial

* code quality test

* code quality

* added test functions in test_modeling_rag.py and test_retrieval_rag.py to test end2end retreiver

* minor change in test_modeling_rag

* fixed tests

* Update examples/research_projects/rag-end2end-retriever/README.md

typo corrected as suggested by lhoestq

Co-authored-by: Quentin Lhoest <42851186+lhoestq@users.noreply.github.com>

* Update examples/research_projects/rag-end2end-retriever/finetune_rag.py

type change suggested by lhoestq

Co-authored-by: Quentin Lhoest <42851186+lhoestq@users.noreply.github.com>

* Update src/transformers/models/rag/retrieval_rag.py

Adding this change as mentioned by lhoestq.

Co-authored-by: Quentin Lhoest <42851186+lhoestq@users.noreply.github.com>

* completed the minor changes suggested by the reviewers

Co-authored-by: Quentin Lhoest <42851186+lhoestq@users.noreply.github.com>
==

examples/research_projects/rag-end2end-retriever/README.md
examples/research_projects/rag-end2end-retriever/callbacks_rag.py
examples/research_projects/rag-end2end-retriever/distributed_ray_retriever.py
examples/research_projects/rag-end2end-retriever/eval_rag.py
examples/research_projects/rag-end2end-retriever/finetune_rag.py
examples/research_projects/rag-end2end-retriever/finetune_rag_ray_end2end.sh
examples/research_projects/rag-end2end-retriever/kb_encode_utils.py
examples/research_projects/rag-end2end-retriever/lightning_base.py
examples/research_projects/rag-end2end-retriever/requirements.txt
examples/research_projects/rag-end2end-retriever/test_run/dummy-kb/my_knowledge_dataset.csv
examples/research_projects/rag-end2end-retriever/test_run/dummy-train-data/train.source
examples/research_projects/rag-end2end-retriever/test_run/dummy-train-data/train.target
examples/research_projects/rag-end2end-retriever/test_run/dummy-train-data/val.source
examples/research_projects/rag-end2end-retriever/test_run/dummy-train-data/val.target
examples/research_projects/rag-end2end-retriever/test_run/test_finetune.sh
examples/research_projects/rag-end2end-retriever/test_run/test_rag_new_features.sh
examples/research_projects/rag-end2end-retriever/use_own_knowledge_dataset.py
examples/research_projects/rag-end2end-retriever/utils_rag.py
src/transformers/models/rag/modeling_rag.py
src/transformers/models/rag/retrieval_rag.py
tests/test_modeling_rag.py
tests/test_retrieval_rag.py
==================
ad25fd62b;Suraj Patil;2021-06-01 09:44:31 +0530;Add FlaxCLIP (#11883)
* add flax CLIP

* default input_shape

* add tests

* fix test

* fix name

* fix docs

* fix shapes

* attend at least 1 token

* flax conv to torch conv

* return floats

* fix equivalence tests

* fix import

* return attention_weights and update tests

* fix dosctrings

* address patricks comments

* input_shape arg

* add tests for get_image_features and get_text_features methods

* fix tests
==

docs/source/index.rst
docs/source/model_doc/clip.rst
src/transformers/__init__.py
src/transformers/modeling_flax_pytorch_utils.py
src/transformers/modeling_flax_utils.py
src/transformers/models/auto/modeling_flax_auto.py
src/transformers/models/clip/__init__.py
src/transformers/models/clip/configuration_clip.py
src/transformers/models/clip/modeling_flax_clip.py
src/transformers/utils/dummy_flax_objects.py
tests/test_modeling_flax_clip.py
tests/test_modeling_flax_common.py
utils/check_repo.py
==================
cfca638ac;Philip May;2021-05-31 17:54:33 +0200;Add MT5ForConditionalGeneration as supported arch. to summarization README (#11961)
* Add MT5ForConditionalGeneration as supported arch.

* Update README.md
==

examples/pytorch/summarization/README.md
examples/pytorch/translation/README.md
==================
1ab147d64;Nicholas Vadivelu;2021-05-31 10:29:04 -0400;Remove redundant `nn.log_softmax` in `run_flax_glue.py` (#11920)
* Remove redundant `nn.log_softmax` in `run_flax_glue.py`

`optax.softmax_cross_entropy` expects unnormalized logits, and so it already calls `nn.log_softmax`, so I believe it is not needed here. `nn.log_softmax` is idempotent so mathematically it shouldn't have made a difference.

* Remove unused 'flax.linen' import
==

examples/flax/text-classification/run_flax_glue.py
==================
fb60c309c;Philip May;2021-05-31 10:02:10 +0200;fix assert (#11935)

==

tests/test_tokenization_deberta_v2.py
==================
04a9709c2;Lysandre;2021-05-31 09:18:49 +0200;Remove `datasets` submodule

==

.github/workflows/self-push.yml
datasets
==================
8d171628f;Lysandre Debut;2021-05-28 13:52:01 +0200;Test optuna and ray (#11924)

==

setup.py
src/transformers/dependency_versions_table.py
==================
af1a10bff;Jayendra;2021-05-28 16:16:56 +0530;[Flax] Return Attention from BERT, ELECTRA, RoBERTa and GPT2 (#11918)
* Added logic to return attention from flax-bert model and added test cases to check that

* Added new line at the end of file to test_modeling_flax_common.py

* fixing code style

* Fixing Roberta and Elextra models too from cpoying bert

* Added temporary hack to not run test_attention_outputs for FlaxGPT2

* Returning attention weights from GPT2 and changed the tests accordingly.

* last fixes

* bump flax dependency

Co-authored-by: jayendra <jayendra@infocusp.in>
Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
==

setup.py
src/transformers/dependency_versions_table.py
src/transformers/models/bert/modeling_flax_bert.py
src/transformers/models/electra/modeling_flax_electra.py
src/transformers/models/gpt2/modeling_flax_gpt2.py
src/transformers/models/roberta/modeling_flax_roberta.py
tests/test_modeling_flax_common.py
==================
e1205e478;Bhadresh Savani;2021-05-28 15:57:02 +0530;Added Sequence Classification class in GPTNeo (#11906)
* seq classification changes

* fix tests
==

datasets
docs/source/model_doc/gpt_neo.rst
src/transformers/__init__.py
src/transformers/models/auto/modeling_auto.py
src/transformers/models/gpt_neo/__init__.py
src/transformers/models/gpt_neo/modeling_gpt_neo.py
src/transformers/utils/dummy_pt_objects.py
tests/test_modeling_gpt2.py
tests/test_modeling_gpt_neo.py
==================
80d712fac;Nicolas Patry;2021-05-27 14:22:58 +0200;Adding new argument `max_new_tokens` for generate. (#11476)
* Adding new argument `max_new_tokens` for generate.

This is a proposal to add a new argument `max_new_tokens` to `generate`.
This include a `MaxNewTokensCriteria` that enables callers that don't
know about the token length ahead (like pipelines callers) to manage
more easily the length of their generated output.

* Adding a test for the user warning when both`max_length` and
`max_new_tokens` are used together.

* Removed redundant `no_grad`.
==

src/transformers/generation_stopping_criteria.py
src/transformers/generation_utils.py
tests/test_generation_stopping_criteria.py
tests/test_generation_utils.py
==================
2dd6fb258;Josh Tanner;2021-05-27 04:53:33 -0700;Update deepspeed config to reflect hyperparameter search parameters (#11896)
* rebuild deepspeed config for hyperparameter search

* reformat code to fix style issues
==

src/transformers/trainer.py
==================
42fe0dc23;Patrick von Platen;2021-05-27 10:46:10 +0100;Add Emotion Speech Noteboook (#11900)

==

docs/source/community.md
==================
996a315e7;Patrick von Platen;2021-05-27 00:18:17 +0100;Flax Generate (#11777)
* fix_torch_device_generate_test

* remove @

* add

* indexing

* correct a couple of tests

* fix tests

* add logits processor

* finish top_k, top_p, temp

* add docs

* correct flax prng key default

* improve generate

* add generation docs

* add docs

* make style

* revert model outputs change

* make style

* correct typo

* fix tests

* fix slow test

* add raise

* finish generation

Co-authored-by: Patrick von Platen <patrick@huggingface.co>
==

docs/source/internal/generation_utils.rst
docs/source/main_classes/model.rst
src/transformers/__init__.py
src/transformers/generation_flax_logits_process.py
src/transformers/generation_flax_utils.py
src/transformers/modeling_flax_utils.py
src/transformers/models/gpt2/modeling_flax_gpt2.py
src/transformers/utils/dummy_flax_objects.py
tests/test_generation_flax_logits_process.py
tests/test_generation_flax_utils.py
tests/test_modeling_flax_gpt2.py
==================
2df546918;Avital Oliver;2021-05-26 21:44:40 +0200;Link official Cloud TPU JAX docs (#11892)

==

examples/flax/README.md
==================
1530384e5;joerenner;2021-05-26 17:59:06 +0200;changing find_batch_size to work with tokenizer outputs (#11890)
* changing find_batch_size to work with tokenizer outputs

trainer_pt_utils.find_batch_size does not recognize the batch size of BatchEncoding objects. This can cause an error when a trainer relies on find_batch_size to report the number of observed examples in the evaluation loop.

* Trigger CI

Co-authored-by: jrenner <joseph.renner@inria.fr>
==

src/transformers/trainer_pt_utils.py
==================
d5a72b6e1;Patrick von Platen;2021-05-26 15:01:13 +0100;[Flax] Allow dataclasses to be jitted (#11886)
* fix_torch_device_generate_test

* remove @

* change dataclasses to flax ones

* fix typo

* fix jitted tests

* fix bert & electra
==

src/transformers/modeling_flax_outputs.py
src/transformers/models/bert/modeling_flax_bert.py
src/transformers/models/electra/modeling_flax_electra.py
tests/test_modeling_flax_common.py
==================
e6126e193;talkhaldi;2021-05-26 22:07:23 +0900;Correcting comments in T5Stack to reflect correct tuple order  (#11330)
* Correcting comments to reflect correct tuple order

In order to match the actual order (line 513 and 516, and as accessed in 968), I've changed the order mentioned in comments L962 and L966-967.

* Update modeling_t5.py

Updating another comment as well

* Removing extra space

* Fixing style and quality

* style & quality

* Update src/transformers/models/t5/modeling_t5.py

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
==

src/transformers/models/t5/modeling_t5.py
==================
0b9335844;Daniel Stancl;2021-05-26 15:02:44 +0200;Fix usage of head masks by TF encoder-decoder models' `generate()` function (#11775)
* Fix Bart

* Fix Blenderbot{,_small}

* Fix LED

* Fix Marian

* Fix MBart

* Fix Pegasus

* Fix T5

* Add test for generation with head_mask

* Add a common TF test

* Override a test for the LED model as head masking is not yet properly implemented

* Remove all head_masks from input preparation for LED

* Drop masking for T5 as it needs a bit of refactor
==

src/transformers/models/bart/modeling_tf_bart.py
src/transformers/models/blenderbot/modeling_tf_blenderbot.py
src/transformers/models/blenderbot_small/modeling_tf_blenderbot_small.py
src/transformers/models/led/modeling_tf_led.py
src/transformers/models/marian/modeling_tf_marian.py
src/transformers/models/mbart/modeling_tf_mbart.py
src/transformers/models/pegasus/modeling_tf_pegasus.py
src/transformers/models/t5/modeling_tf_t5.py
tests/test_modeling_tf_common.py
tests/test_modeling_tf_led.py
tests/test_modeling_tf_t5.py
==================
0b0a59845;francescorubbo;2021-05-26 01:19:37 -0700;Ensure input tensor are on device. (#11874)
The feature extractor does not create tensors on the appropriate device,
so we call `ensure_tensor_on_device` before feeding the processed inputs
to the model.
==

src/transformers/pipelines/automatic_speech_recognition.py
==================
a9c797f93;Ahmet Akko√ß;2021-05-26 00:06:14 +0300;[Wav2Vec2ForCTC] example typo fixed (#11878)

==

src/transformers/models/wav2vec2/modeling_wav2vec2.py
==================
1b6530104;Stas Bekman;2021-05-25 10:40:49 -0700;[Examples] create model with custom config on the fly (#11798)
* create custom model on the flight

* better wording

* add update_from_string

* cleanup

* cleanup

* Update src/transformers/configuration_utils.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* more bool options

* style

* fix logger

* add test

* add the doc

* assert on conflict of options

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

examples/pytorch/language-modeling/README.md
examples/pytorch/language-modeling/run_clm.py
src/transformers/configuration_utils.py
tests/test_configuration_common.py
==================
6287c929c;Stas Bekman;2021-05-25 08:11:26 -0700;[lm examples] fix overflow in perplexity calc (#11855)
* fix overflow in perplexity calc

* use inf

* fix
==

examples/pytorch/language-modeling/run_clm.py
examples/pytorch/language-modeling/run_clm_no_trainer.py
examples/pytorch/language-modeling/run_mlm.py
examples/pytorch/language-modeling/run_mlm_no_trainer.py
examples/pytorch/language-modeling/run_plm.py
==================
7630c11f3;Patrick von Platen;2021-05-25 13:59:52 +0100;[Wav2Vec2] SpecAugment Fast (#11764)
* first try

* finish
==

src/transformers/models/wav2vec2/modeling_wav2vec2.py
tests/test_modeling_wav2vec2.py
==================
f086652b1;Sylvain Gugger;2021-05-25 08:03:43 -0400;Add option to log only once in multinode training (#11819)
* Add option to long only once in multinode training

* Use an alternate property
==

examples/pytorch/language-modeling/run_clm.py
examples/pytorch/language-modeling/run_mlm.py
examples/pytorch/language-modeling/run_plm.py
examples/pytorch/multiple-choice/run_swag.py
examples/pytorch/question-answering/run_qa.py
examples/pytorch/question-answering/run_qa_beam_search.py
examples/pytorch/summarization/run_summarization.py
examples/pytorch/text-classification/run_glue.py
examples/pytorch/text-classification/run_xnli.py
examples/pytorch/token-classification/run_ner.py
examples/pytorch/translation/run_translation.py
src/transformers/trainer.py
src/transformers/training_args.py
templates/adding_a_new_example_script/{{cookiecutter.directory_name}}/run_{{cookiecutter.example_shortcut}}.py
tests/sagemaker/scripts/pytorch/run_glue_model_parallelism.py
==================
b8344a274;Wang Ran (Ê±™ÁÑ∂);2021-05-25 16:23:46 +0800;typo (#11858)

==

examples/pytorch/text-classification/README.md
==================
f9880f62a;Shiro T;2021-05-25 17:18:55 +0900;fixed a small typo in the doc (#11856)

==

CONTRIBUTING.md
==================
6da129cb3;Lysandre Debut;2021-05-25 10:06:19 +0200;Enable memory metrics in tests that need it (#11859)

==

tests/test_trainer.py
==================
db0b2477c;Lysandre Debut;2021-05-25 10:06:06 +0200;Add some tests to the slow suite #11860

==

tests/test_modeling_bigbird_pegasus.py
tests/test_modeling_common.py
==================
afe479adb;Sylvain Gugger;2021-05-24 19:51:42 -0400;[Trainer] Report both steps and num samples per second (#11818)
* [Trainer] Report both steps and num samples per second

* Fix batch number

* Update src/transformers/trainer_utils.py

Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>

* Address review comments

Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>
==

src/transformers/modelcard.py
src/transformers/trainer.py
src/transformers/trainer_utils.py
src/transformers/utils/notebook.py
tests/test_trainer.py
==================
eaab9397c;Nick Lane-Smith;2021-05-24 11:26:02 -0700;Fix two typos in docs (#11852)
* typo2

* fix typo
==

docs/source/installation.md
docs/source/model_sharing.rst
==================
8a2a3a25a;Teven;2021-05-24 20:15:52 +0200;Fix flos single node (#11844)
* fixing flos bug/typo in non-distributed setting

* storing flos every logging_interval
==

src/transformers/trainer.py
==================
adb785b0f;Sylvain Gugger;2021-05-24 13:30:39 -0400;Switch mem metrics flag (#11851)
* Switch mem metrics flag

* Update src/transformers/training_args.py

Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>

Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>
==

src/transformers/training_args.py
==================
fcdb85e9d;Sylvain Gugger;2021-05-24 09:26:40 -0400;Fix reference to XLNet (#11846)

==

src/transformers/training_args.py
==================
f58060415;Patrick von Platen;2021-05-24 10:41:10 +0100;[Flax] Fix PyTorch import error (#11839)
* fix_torch_device_generate_test

* remove @

* change pytorch import to flax import
==

examples/flax/language-modeling/run_mlm_flax.py
==================
0cbddfb19;Lysandre Debut;2021-05-24 09:38:59 +0200;Replace double occurrences as the last step (#11367)

==

src/transformers/convert_slow_tokenizer.py
==================
73fde1def;ctheodoris;2021-05-22 10:27:20 -0400;Faster list concat for trainer_pt_utils.get_length_grouped_indices() (#11825)
get_length_grouped_indices() in LengthGroupedSampler and DistributedLengthGroupedSampler
is prohibitively slow for large number of megabatches (in test case takes hours for ~270k
megabatches with 100 items each) due to slow list concatenation with sum(megabatches, []).

Resolves: #11795

Co-authored-by: ctheodoris <cvtheodo@ds.dfci.harvard.edu>
==

src/transformers/trainer_pt_utils.py
==================
da22245ed;Patrick von Platen;2021-05-21 23:11:58 +0100;Add flax text class colab (#11824)
* fix_torch_device_generate_test

* remove @

* add flax glue link
==

examples/flax/README.md
==================
a26f4d620;Stas Bekman;2021-05-21 09:07:46 -0700;[Deepspeed] support `zero.Init` in `from_config` (#11805)
* support zero.Init in from_config

* no need for eval test
==

src/transformers/models/auto/auto_factory.py
tests/deepspeed/test_deepspeed.py
==================
82335185f;Patrick von Platen;2021-05-21 16:52:23 +0100;[Flax] Small fixes in `run_flax_glue.py` (#11820)
* fix_torch_device_generate_test

* remove @

* correct best seed for flax fine-tuning

Co-authored-by: Patrick von Platen <patrick@huggingface.co>
==

examples/flax/text-classification/README.md
examples/flax/text-classification/run_flax_glue.py
==================
b8697bc62;Sylvain Gugger;2021-05-21 09:23:31 -0400;Avoid TensorFlow import in Trainer

==

src/transformers/modelcard.py
==================
e2c1dd096;yujun;2021-05-21 20:06:11 +0800;fix roformer config doc (#11813)

==

src/transformers/models/roformer/configuration_roformer.py
==================
1b652295c;Lysandre Debut;2021-05-21 12:50:01 +0200;Patch recursive import (#11812)

==

src/transformers/convert_slow_tokenizer.py
tests/test_tokenization_utils.py
==================
bd9871657;Patrick von Platen;2021-05-21 09:36:56 +0100;[Flax] Align GLUE training script with mlm training script (#11778)
* speed up flax glue

* remove unnecessary line

* remove folder

* remove run in loop

Co-authored-by: Patrick von Platen <patrick@huggingface.co>
==

examples/flax/text-classification/README.md
examples/flax/text-classification/run_flax_glue.py
==================
223943872;Keren Fuentes;2021-05-20 16:54:23 -0700;Fix failing test on Windows Platform (#11589)
* add separator for windows

* fixes test_is_copy_consistent on Windows

* fixing writing encoding issue on extended test (for Windows)

* resolving comments
==

examples/pytorch/translation/run_translation.py
tests/test_tokenization_wav2vec2.py
tests/test_utils_check_copies.py
==================
f4a0d6ff8;Michael Benayoun;2021-05-20 18:02:29 +0200;A cleaner and more scalable implementation of symbolic tracing (#11763)
Cleaner and more scalable implementation of symbolic tracing with torch.fx, and provides support for new architectures:
- ALBERT
- DistilBERT
- MobileBERT
- MegatronBERT
- GPT2
- GPT Neo

Co-authored-by: Michael Benayoun <michael@huggingface.co>
==

src/transformers/modeling_fx_utils.py
tests/test_modeling_albert.py
tests/test_modeling_common.py
tests/test_modeling_distilbert.py
tests/test_modeling_gpt2.py
tests/test_modeling_gpt_neo.py
tests/test_modeling_megatron_bert.py
tests/test_modeling_mobilebert.py
==================
469384a77;Sylvain Gugger;2021-05-20 09:55:13 -0400;Fix regression in regression (#11785)
* Fix regression in regression

* Add test
==

src/transformers/models/albert/modeling_albert.py
src/transformers/models/bert/modeling_bert.py
src/transformers/models/big_bird/modeling_big_bird.py
src/transformers/models/convbert/modeling_convbert.py
src/transformers/models/distilbert/modeling_distilbert.py
src/transformers/models/electra/modeling_electra.py
src/transformers/models/funnel/modeling_funnel.py
src/transformers/models/longformer/modeling_longformer.py
src/transformers/models/mobilebert/modeling_mobilebert.py
src/transformers/models/reformer/modeling_reformer.py
src/transformers/models/roberta/modeling_roberta.py
src/transformers/models/squeezebert/modeling_squeezebert.py
src/transformers/models/xlm/modeling_xlm.py
src/transformers/models/xlnet/modeling_xlnet.py
tests/test_modeling_common.py
==================
5ad5cc719;Sylvain Gugger;2021-05-20 09:30:31 -0400;Fix pattern in conf.py (#11784)

==

utils/release.py
==================
206f06f2d;yujun;2021-05-20 20:00:34 +0800;Add new model RoFormer (use rotary position embedding ) (#11684)
* add roformer

* Update docs/source/model_doc/roformer.rst

Co-authored-by: Suraj Patil <surajp815@gmail.com>

* Update docs/source/model_doc/roformer.rst

Co-authored-by: Suraj Patil <surajp815@gmail.com>

* update

* add TFRoFormerSinusoidalPositionalEmbedding and fix TFMarianSinusoidalPositionalEmbedding

* update docs

* make style and make quality

* roback

* unchanged

* rm copies from , this is a error in TFMarianSinusoidalPositionalEmbedding

* update Copyright year

* move # Add modeling imports here to the correct position

* max_position_embeddings can be set to 1536

* # Copied from transformers.models.bert.modeling_bert.BertOutput with Bert->RoFormer

* # Copied from transformers.models.bert.modeling_bert.BertLayer.__init__ with Bert->RoFormer

* update tokenization_roformer

* make style

* add staticmethod apply_rotary_position_embeddings

* add TF staticmethod apply_rotary_position_embeddings

* update torch apply_rotary_position_embeddings

* fix tf apply_rotary_position_embeddings error

* make style

* add pytorch RoFormerSelfAttentionRotaryPositionEmbeddingTest

* add TF rotary_position_embeddings test

* update test_modeling_rofomer

* Update docs/source/model_doc/roformer.rst

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/__init__.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/__init__.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/__init__.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/__init__.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/models/roformer/convert_roformer_original_tf_checkpoint_to_pytorch.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/models/roformer/modeling_roformer.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/models/roformer/modeling_roformer.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/models/roformer/modeling_tf_roformer.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* refact roformer tokenizer

* add RoFormerTokenizerFast

* add RoFormerTokenizationTest

* add require_jieba

* update Copyright

* update tokenizer & add copy from

* add option rotary_value

* use rust jieba

* use rjieba

* use rust jieba

* fix test_alignement_methods

* slice normalized_string is too slow

* add config.embedding_size when embedding_size!=hidden_size

* fix pickle tokenizer

* Update docs/source/model_doc/roformer.rst

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* make style and make quality

Co-authored-by: Suraj Patil <surajp815@gmail.com>
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
==

README.md
docs/source/index.rst
docs/source/model_doc/roformer.rst
src/transformers/__init__.py
src/transformers/convert_slow_tokenizer.py
src/transformers/models/__init__.py
src/transformers/models/auto/configuration_auto.py
src/transformers/models/auto/modeling_auto.py
src/transformers/models/auto/modeling_tf_auto.py
src/transformers/models/auto/tokenization_auto.py
src/transformers/models/roformer/__init__.py
src/transformers/models/roformer/configuration_roformer.py
src/transformers/models/roformer/convert_roformer_original_tf_checkpoint_to_pytorch.py
src/transformers/models/roformer/modeling_roformer.py
src/transformers/models/roformer/modeling_tf_roformer.py
src/transformers/models/roformer/tokenization_roformer.py
src/transformers/models/roformer/tokenization_roformer_fast.py
src/transformers/models/roformer/tokenization_utils.py
src/transformers/utils/dummy_pt_objects.py
src/transformers/utils/dummy_tf_objects.py
src/transformers/utils/dummy_tokenizers_objects.py
src/transformers/utils/modeling_auto_mapping.py
tests/test_modeling_roformer.py
tests/test_modeling_tf_roformer.py
tests/test_tokenization_roformer.py
==================
075fdab4f;Lysandre Debut;2021-05-20 09:16:03 +0200;Deprecate commands from the transformers-cli that are in the hf-cli (#11779)

==

src/transformers/commands/lfs.py
src/transformers/commands/user.py
==================
2582e59a5;Albert Villanova del Moral;2021-05-19 15:48:56 +0200;Add DOI badge to README (#11771)

==

README.md
==================
00440e350;Patrick von Platen;2021-05-19 12:00:58 +0100;[Flax MLM] Refactor run mlm with optax (#11745)
* refactor

* update

* update

* update

* refactor run mlm

* finalize

* refactor more

* fix typo

* update

* finish refactor

* modify run mlm

* Apply suggestions from code review

* Apply suggestions from code review

* Apply suggestions from code review

* small fixes

* upload

* upload

* finish run mlm script

Co-authored-by: Patrick von Platen <patrick@huggingface.co>
==

examples/flax/language-modeling/README.md
examples/flax/language-modeling/requirements.txt
examples/flax/language-modeling/run_mlm_flax.py
examples/flax/text-classification/requirements.txt
==================
43891be19;Patrick von Platen;2021-05-19 10:31:17 +0100;[T5 failing CI] Fix generate test (#11770)
* fix_torch_device_generate_test

* remove @
==

tests/test_generation_utils.py
tests/test_modeling_t5.py
==================
680d181ce;Daniel Stancl;2021-05-19 01:44:53 +0200;Fix usage of head masks by PT encoder-decoder models' `generate()` function (#11621)
* Add missing head masking for generate() function

* Add head_mask, decoder_head_mask and cross_attn_head_mask
into prepare_inputs_for_generation for generate() function
for multiple encoder-decoder models.

* Add test_genereate_with_head_masking

* [WIP] Update the new test and handle special cases

* make style

* Omit ProphetNet test so far

* make fix-copies
==

src/transformers/generation_utils.py
src/transformers/models/bart/modeling_bart.py
src/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py
src/transformers/models/blenderbot/modeling_blenderbot.py
src/transformers/models/blenderbot_small/modeling_blenderbot_small.py
src/transformers/models/fsmt/modeling_fsmt.py
src/transformers/models/led/modeling_led.py
src/transformers/models/m2m_100/modeling_m2m_100.py
src/transformers/models/marian/modeling_marian.py
src/transformers/models/mbart/modeling_mbart.py
src/transformers/models/pegasus/modeling_pegasus.py
src/transformers/models/prophetnet/modeling_prophetnet.py
src/transformers/models/t5/modeling_t5.py
tests/test_generation_utils.py
tests/test_modeling_prophetnet.py
tests/test_modeling_t5.py
==================
ca33278fd;Suraj Patil;2021-05-19 03:20:51 +0530;FlaxGPT2 (#11556)
* flax gpt2

* combine masks

* handle shared embeds

* add causal LM sample

* style

* add tests

* style

* fix imports, docs, quality

* don't use cache

* add cache

* add cache 1st version

* make use cache work

* start adding test for generation

* finish generation loop compilation

* rewrite test

* finish

* update

* update

* apply sylvains suggestions

* update

* refactor

* fix typo

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
==

docs/source/index.rst
docs/source/model_doc/auto.rst
docs/source/model_doc/gpt2.rst
src/transformers/__init__.py
src/transformers/file_utils.py
src/transformers/modeling_flax_outputs.py
src/transformers/models/auto/__init__.py
src/transformers/models/auto/modeling_flax_auto.py
src/transformers/models/gpt2/__init__.py
src/transformers/models/gpt2/modeling_flax_gpt2.py
src/transformers/utils/dummy_flax_objects.py
tests/test_modeling_flax_common.py
tests/test_modeling_flax_gpt2.py
==================
eb3e072a3;Tomy Hsieh;2021-05-19 02:38:36 +0800;Fix a small error in summarization example (#11762)

==

examples/pytorch/summarization/run_summarization_no_trainer.py
==================
77f9bd18a;Avital Oliver;2021-05-18 18:45:16 +0200;Add Flax Examples and Cloud TPU README (#11753)
* Add Flax Examples README

* Apply suggestions from code review

* Update examples/flax/README.md

* add nice table

* fix

* fix

* apply suggestions

* upload

* finish flax readme.md

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
==

examples/flax/README.md
==================
04e25c628;Philipp Schmid;2021-05-18 16:27:29 +0200;add `dataset_name` to data_args and added accuracy metric (#11760)
* add `dataset_name` to data_args and added accuracy metric

* added documentation for dataset_name

* spelling correction
==

examples/pytorch/text-classification/README.md
examples/pytorch/text-classification/run_glue.py
==================
fd3b12e8c;Vyom Pathak;2021-05-18 19:17:28 +0530;Fixed: Better names for nlp variables in pipelines' tests and docs. (#11752)
* Fixed: Better names for nlp variables in pipelines' tests and docs.

* Fixed: Better variable names
==

docs/source/task_summary.rst
tests/test_onnx.py
tests/test_pipelines_common.py
tests/test_pipelines_conversational.py
tests/test_pipelines_fill_mask.py
tests/test_pipelines_question_answering.py
tests/test_pipelines_summarization.py
tests/test_pipelines_table_question_answering.py
tests/test_pipelines_text_generation.py
tests/test_pipelines_token_classification.py
tests/test_pipelines_translation.py
tests/test_pipelines_zero_shot.py
==================
cebb96f53;Patrick von Platen;2021-05-18 14:38:56 +0100;Add more subsections to main doc (#11758)
* add headers to main doc

* Apply suggestions from code review

* update

* upload
==

README.md
docs/source/index.rst
examples/pytorch/question-answering/README.md
examples/pytorch/question-answering/run_qa.py
examples/pytorch/token-classification/README.md
examples/pytorch/token-classification/run_ner.py
utils/check_copies.py
==================
da7e73b72;Tommy Chiang;2021-05-18 21:28:13 +0800;Fix incorrect newline in #11650 (#11757)

==

examples/pytorch/text-generation/README.md
==================
a515caa33;Sylvain Gugger;2021-05-18 07:42:39 -0400;Fix checkpoint deletion (#11748)

==

src/transformers/trainer.py
tests/test_trainer.py
==================
b88e0e016;Nicolas Patry;2021-05-18 09:53:20 +0200;[TokenClassification] Label realignment for subword aggregation (#11680)
* [TokenClassification] Label realignment for subword aggregation

Tentative to replace https://github.com/huggingface/transformers/pull/11622/files

- Added `AggregationStrategy`
- `ignore_subwords` and `grouped_entities` arguments are now fused
  into `aggregation_strategy`. It makes more sense anyway because
  `ignore_subwords=True` with `grouped_entities=False` did not have a
  meaning anyway.
- Added 2 new ways to aggregate which are MAX, and AVERAGE
- AVERAGE requires a bit more information than the others, for now this
case is slightly specific, we should keep that in mind for future
changes.
- Testing has been modified to reflect new argument, and to check the
correct deprecation and the new aggregation_strategy.
- Put the testing argument and testing results for aggregation_strategy,
close together, so that readers can understand what is supposed to
happen.
- `aggregate` is now only tested on a small model as it does not mean
anything to test it globally for all models.
- Previous tests are unchanged in desired output.
- Added a new test case that showcases better the difference between the
  FIRST, MAX and AVERAGE strategies.

* Wrong framework.

* Addressing three issues.

1- Tags might not follow B-, I- convention, so any tag should work now
(assumed as B-TAG)
2- Fixed an issue with average that leads to a substantial code change.
3- The testing suite was not checking for the "index" key for "none"
strategy. This is now fixed.

The issue is that "O" could not be chosen by AVERAGE strategy because
those tokens were filtered out beforehand, so their relative scores were
not counted in the average. Now filtering on
ignore_labels will happen at the very end of the pipeline fixing
that issue.
It's a bit hard to make sure this stays like that because we do
not have a end-to-end test for that behavior

* Formatting.

* Adding formatting to code + cleaner handling of B-, I- tags.

Co-authored-by: Francesco Rubbo <rubbo.francesco@gmail.com>
Co-authored-by: elk-cloner <rezakakhki.rk@gmail.com>

* Typo.

Co-authored-by: Francesco Rubbo <rubbo.francesco@gmail.com>
Co-authored-by: elk-cloner <rezakakhki.rk@gmail.com>
==

src/transformers/pipelines/__init__.py
src/transformers/pipelines/token_classification.py
src/transformers/testing_utils.py
tests/test_pipelines_token_classification.py
==================
c73e35323;Patrick von Platen;2021-05-17 19:54:33 +0100;push (#11750)

==

src/transformers/models/bert/modeling_flax_bert.py
==================
936b57158;Sylvain Gugger;2021-05-17 10:10:13 -0400;Use new evaluation loop in TrainerQA (#11746)

==

examples/pytorch/question-answering/trainer_qa.py
==================
73893fc77;Patrick von Platen;2021-05-17 11:30:53 +0100;[BigBird Pegasus] Make tests faster (#11744)
* improve tests

* remove bogus file

* make style

Co-authored-by: Patrick von Platen <patrick@huggingface.co>
==

tests/test_modeling_bigbird_pegasus.py
==================
a0531c8a2;Michael Benayoun;2021-05-17 12:17:31 +0200;fixed shape issue for T5 tracing (#11742)
Co-authored-by: Michael Benayoun <michael@huggingface.co>
==

src/transformers/modeling_fx_utils.py
==================
0fc56df5f;Julien Chaumond;2021-05-17 11:28:56 +0200;Add visual + link to Premium Support webpage (#11740)
* Update README.md

* Update index.rst
==

README.md
docs/source/index.rst
==================
2f88bd9c4;Julien Chaumond;2021-05-17 10:42:37 +0200;Remove tapas model card (#11739)

==

model_cards/google/tapas-base/README.md
==================
726e953d4;Marc van Zee;2021-05-17 10:26:33 +0200;Improvements to Flax finetuning script (#11727)
* Add Cloud details to README

* Flax script and readme updates

* Some simplifications of Flax script
==

examples/flax/text-classification/README.md
examples/flax/text-classification/run_flax_glue.py
==================
86d5fb0b3;Michael Benayoun;2021-05-14 20:57:30 +0200;Experimental symbolic tracing feature with torch.fx for BERT, ELECTRA and T5 (#11475)
Symbolic tracing feature for BERT, ELECTRA and T5

Co-authored-by: Michael Benayoun <michael@huggingface.co>
Co-authored-by: Stas Bekman <stas@stason.org>
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

src/transformers/file_utils.py
src/transformers/modeling_fx_utils.py
src/transformers/models/t5/modeling_t5.py
tests/test_modeling_bert.py
tests/test_modeling_common.py
tests/test_modeling_electra.py
tests/test_modeling_t5.py
==================
94a234870;Marc van Zee;2021-05-14 15:51:25 +0200;Add Cloud details to README (#11706)
* Add Cloud details to README

* Flax script and readme updates
==

examples/flax/text-classification/README.md
==================
113eaa757;Patrick von Platen;2021-05-14 12:02:57 +0100;correct example script (#11726)

==

examples/flax/text-classification/run_flax_glue.py
==================
bd3b599c1;Oyvind Tafjord;2021-05-14 02:44:03 -0700;Fix T5 beam search using parallelize (#11717)

==

src/transformers/models/t5/modeling_t5.py
==================
218d552f3;Volodymyr Byno;2021-05-13 23:11:12 +0300;Fix loading the best model on the last stage of training (#11718)

==

src/transformers/trainer.py
tests/test_modeling_common.py
==================
252082001;Sylvain Gugger;2021-05-13 10:45:28 -0400;Fix v4.6.0 doc

==

.circleci/deploy.sh
==================
cbbf49f64;Sylvain Gugger;2021-05-13 10:34:14 -0400;Fix doc deployment

==

.circleci/config.yml
docs/source/main_classes/pipelines.rst
==================
91cf29153;lexhuismans;2021-05-13 13:02:27 +0200;[T5] Add 3D attention mask to T5 model (2) (#9643) (#11197)
* Add 3D attention mask to T5 model (#9643)

Added code for 3D attention mask in T5 model. Similar to BERT model.

* Add test for 3D attention mask

Added test for 3D attention mask: test_decoder_model_past_with_3d_attn_mask()
3D attention mask of the shape [Batch_size, Seq_length, Seq_length] both for
attention mask and decoder attention mask. Test is passing.
==

src/transformers/models/t5/modeling_t5.py
tests/test_modeling_t5.py
==================
6ee1a4fd3;Vasudev Gupta;2021-05-13 16:21:30 +0530;add everything (#11651)

==

src/transformers/models/big_bird/modeling_big_bird.py
src/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py
==================
57b6a80de;Patrick von Platen;2021-05-13 10:58:19 +0100;[Flax] Fix BERT initialization & token_type_ids default (#11695)
* fix some stuff

* fix roberta & electra as well

* del run bug

Co-authored-by: Patrick von Platen <patrick@huggingface.co>
==

src/transformers/models/bert/modeling_flax_bert.py
src/transformers/models/electra/modeling_flax_electra.py
src/transformers/models/roberta/modeling_flax_roberta.py
==================
daf0d6a97;Lysandre Debut;2021-05-13 09:35:44 +0200;Fix gpt-2 warnings (#11709)

==

src/transformers/models/gpt2/modeling_gpt2.py
==================
37ed3ab71;Philip May;2021-05-13 08:44:55 +0200;Enable option for subword regularization in more tokenizers. (#11417)
* improve slow class tok usage at xlm rob

* add subword regularization for barthez

* improve barthez tok. test

* fix tokenizer tests

* add subword regularization for camembert

* add subword regularization for deberta v2 tokenizer

* add more doc to deberta v2 tokenizer

* add subword regularization for speech to text tok.

* fix sp_model_kwargs type in speech 2 text tok.

* add subword regularization for M2M100 tok.

* add more concrete type hints

* fix tests for m2m100 and s2t tok.

* add missing Any import

* fix syntax error in m2m100 tok.

* fix unpickle of m2m100 and s2t tok.

* fix test of m2m100 and s2t tok.

* improve unpickle of deberta v2 tok.

* add test for pickle of barthez & camembert

* fix pickle of barthez & camembert

* add test for deberta v2 tok. pickle

* fix m2m100 tok. pickle

* fix s2t tok. pickle

* add subword regularization to albert tok.

* refactor subword reg. test into TokenizerTesterMixin

improve albert tok. test

remove sample argument form albert tok.

check subword reg. using TokenizerTesterMixin

improve tok. tests

improve xlm roberta tok. tests

improve xlm roberta tok. tests

* add subword regularization for big bird t.

* improve xlm roberta tok. test

* add subword regularization for mbart50 tok.

* add subword regularization for pegasus tok.

* add subword regularization for reformer tok.

* add subword regularization for T5 tok.

* fix t5 tok. test formatting

* add subword regularization for xlm_proph. tok.

* add subword regularization for xlnet tok.

* add subword regularization for gert_gen tok.

* add typing to tokenizers

* add typing to xlm rob. tok

* add subword regularization for marian tok.

* add reverse tok. test

* fix marian tok test

* fix marian tok test

* fix casing in tok. tests

* fix style of tok. common test

* fix deberta v2 tok test

* add type annotations to tok. tests

* add type annotations to tok. __init__

* add typing to kokenizer

* add type annotations to tok. __init__

* don't specify the default when it's None

* fix barthez tok. doc

* move sentencepiece tok. tests to TokenizerTesterMixin

* fix unused imports

* fix albert tok. test

* add comment to sentencepiece test options

* fix Any import at big bird tok.

* fix Any import at xlm prophetnet tok.

* empty commit to trigger CI
==

src/transformers/models/albert/tokenization_albert.py
src/transformers/models/barthez/tokenization_barthez.py
src/transformers/models/bert_generation/tokenization_bert_generation.py
src/transformers/models/big_bird/tokenization_big_bird.py
src/transformers/models/camembert/tokenization_camembert.py
src/transformers/models/deberta_v2/tokenization_deberta_v2.py
src/transformers/models/m2m_100/tokenization_m2m_100.py
src/transformers/models/marian/tokenization_marian.py
src/transformers/models/mbart/tokenization_mbart50.py
src/transformers/models/pegasus/tokenization_pegasus.py
src/transformers/models/reformer/tokenization_reformer.py
src/transformers/models/speech_to_text/tokenization_speech_to_text.py
src/transformers/models/t5/tokenization_t5.py
src/transformers/models/xlm_prophetnet/tokenization_xlm_prophetnet.py
src/transformers/models/xlm_roberta/tokenization_xlm_roberta.py
src/transformers/models/xlnet/tokenization_xlnet.py
tests/test_tokenization_albert.py
tests/test_tokenization_barthez.py
tests/test_tokenization_bert_generation.py
tests/test_tokenization_big_bird.py
tests/test_tokenization_camembert.py
tests/test_tokenization_common.py
tests/test_tokenization_deberta_v2.py
tests/test_tokenization_m2m_100.py
tests/test_tokenization_marian.py
tests/test_tokenization_mbart50.py
tests/test_tokenization_pegasus.py
tests/test_tokenization_reformer.py
tests/test_tokenization_speech_to_text.py
tests/test_tokenization_t5.py
tests/test_tokenization_xlm_prophetnet.py
tests/test_tokenization_xlm_roberta.py
tests/test_tokenization_xlnet.py
==================
fa84540e9;NielsRogge;2021-05-12 17:46:02 +0200;Vit deit fixes (#11309)
* Improve docs of DeiT and ViT, add community notebook

* Add gitignore for test_samples

* Add notebook with Trainer

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
==

docs/source/community.md
src/transformers/models/deit/feature_extraction_deit.py
src/transformers/models/deit/modeling_deit.py
src/transformers/models/vit/feature_extraction_vit.py
src/transformers/models/vit/modeling_vit.py
tests/fixtures/tests_samples/.gitignore
tests/fixtures/tests_samples/COCO/cats.png
==================
d77eb0cf9;Lysandre;2021-05-12 17:08:35 +0200;Docs for v4.7.0.dev0

==

.circleci/deploy.sh
docs/source/_static/js/custom.js
examples/pytorch/language-modeling/run_clm.py
examples/pytorch/language-modeling/run_mlm.py
examples/pytorch/language-modeling/run_plm.py
examples/pytorch/multiple-choice/run_swag.py
examples/pytorch/question-answering/run_qa.py
examples/pytorch/question-answering/run_qa_beam_search.py
examples/pytorch/question-answering/run_qa_beam_search_no_trainer.py
examples/pytorch/question-answering/run_qa_no_trainer.py
examples/pytorch/summarization/run_summarization.py
examples/pytorch/text-classification/run_glue.py
examples/pytorch/text-classification/run_xnli.py
examples/pytorch/token-classification/run_ner.py
examples/pytorch/translation/run_translation.py
setup.py
src/transformers/__init__.py
==================
64e78564a;Lysandre;2021-05-12 17:03:03 +0200;Release: v4.6.0

==

examples/pytorch/language-modeling/run_clm.py
examples/pytorch/language-modeling/run_mlm.py
examples/pytorch/language-modeling/run_plm.py
examples/pytorch/multiple-choice/run_swag.py
examples/pytorch/question-answering/run_qa.py
examples/pytorch/question-answering/run_qa_beam_search.py
examples/pytorch/question-answering/run_qa_beam_search_no_trainer.py
examples/pytorch/question-answering/run_qa_no_trainer.py
examples/pytorch/summarization/run_summarization.py
examples/pytorch/text-classification/run_glue.py
examples/pytorch/text-classification/run_xnli.py
examples/pytorch/token-classification/run_ner.py
examples/pytorch/translation/run_translation.py
setup.py
src/transformers/__init__.py
==================
fd6204b2a;Patrick von Platen;2021-05-12 15:52:54 +0100;[Lazy init] Force fall back to slow init for composite models (#11705)
* fix encoder-decoder & RAG

* finalize

* Update src/transformers/models/encoder_decoder/modeling_encoder_decoder.py

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* Update src/transformers/models/rag/modeling_rag.py

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

Co-authored-by: Patrick von Platen <patrick@huggingface.co>
Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
==

src/transformers/modeling_utils.py
src/transformers/models/encoder_decoder/modeling_encoder_decoder.py
src/transformers/models/rag/modeling_rag.py
==================
5c1cda9d3;Suraj Patil;2021-05-12 19:18:52 +0530;fix example in config doc (#11696)

==

src/transformers/models/clip/configuration_clip.py
==================
77f4c46b5;Philip May;2021-05-12 15:11:10 +0200;remove defaults to None if optional (#11703)

==

examples/research_projects/wav2vec2/run_asr.py
src/transformers/debug_utils.py
src/transformers/modeling_tf_utils.py
src/transformers/modeling_utils.py
src/transformers/models/albert/tokenization_albert_fast.py
src/transformers/models/big_bird/tokenization_big_bird_fast.py
src/transformers/models/ibert/quant_modules.py
src/transformers/models/mpnet/modeling_mpnet.py
src/transformers/models/mpnet/tokenization_mpnet.py
src/transformers/models/xlm_prophetnet/tokenization_xlm_prophetnet.py
src/transformers/pipelines/text2text_generation.py
==================
6797cdc07;Marc van Zee;2021-05-12 14:52:52 +0200;Updates README and fixes bug (#11701)

==

examples/flax/text-classification/README.md
examples/flax/text-classification/run_flax_glue.py
==================
f063c56d9;Suraj Patil;2021-05-12 15:28:30 +0530;Fix clip docs (#11694)
* fix doc url

* fix example
==

README.md
docs/source/index.rst
src/transformers/models/clip/configuration_clip.py
==================
8719afa1a;Suraj Patil;2021-05-12 13:48:15 +0530;CLIP (#11445)
* begin second draft

* fix import, style

* add loss

* fix embeds, logits_scale, and projection

* fix imports

* add conversion script

* add feature_extractor and processor

* style

* add tests for tokenizer, extractor and processor

* add vision model tests

* add weight init

* add more tests

* fix save_load  test

* model output, dosstrings, causal mask

* config doc

* add clip model tests

* return dict

* bigin integration test

* add integration tests

* fix-copies

* fix init

* Clip => CLIP

* fix module name

* docs

* fix doc

* output_dim => projection_dim

* fix checkpoint names

* remoe fast tokenizer file

* fix conversion script

* fix tests, quality

* put causal mask on device

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* fix attribute test

* style

* address sylvains comments

* style

* fix docstrings

* add qucik_gelu in activations, docstrings

* clean-up attention test

* fix act fun

* fix config

* fix torchscript tests

* even batch_size

* remove comment

* fix ouput tu_tuple

* fix save load tests

* fix add tokens test

* add fast tokenizer

* update copyright

* new processor API

* fix docs

* docstrings

* docs

* fix doc

* fix doc

* fix tokenizer

* fix import in doc example

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* check types of config

* valhalla => openai

* load image using url

* fix test

* typo

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

README.md
docs/source/index.rst
docs/source/model_doc/clip.rst
src/transformers/__init__.py
src/transformers/activations.py
src/transformers/convert_slow_tokenizer.py
src/transformers/models/__init__.py
src/transformers/models/auto/configuration_auto.py
src/transformers/models/auto/modeling_auto.py
src/transformers/models/clip/__init__.py
src/transformers/models/clip/configuration_clip.py
src/transformers/models/clip/convert_clip_original_pytorch_to_hf.py
src/transformers/models/clip/feature_extraction_clip.py
src/transformers/models/clip/modeling_clip.py
src/transformers/models/clip/processing_clip.py
src/transformers/models/clip/tokenization_clip.py
src/transformers/models/clip/tokenization_clip_fast.py
src/transformers/utils/dummy_pt_objects.py
src/transformers/utils/dummy_tokenizers_objects.py
src/transformers/utils/dummy_vision_objects.py
tests/test_feature_extraction_clip.py
tests/test_modeling_clip.py
tests/test_processor_clip.py
tests/test_tokenization_clip.py
utils/check_repo.py
==================
4ce6bcc31;Marc van Zee;2021-05-11 20:02:59 +0200;Adds Flax BERT finetuning example on GLUE (#11564)
* Adds Flax BERT finetuning example

* fix traced jax tensor type

* Use Optax losses and learning schedulers

* Add 1GPU training results

* merge into master & make style

* fix input

* del file

* Fix bug in loss and add torch runs

* finish bert flax fine-tune

* Update examples/flax/text-classification/README.md

* Update examples/flax/text-classification/run_flax_glue.py

* add requirements

* finalize

* finalize

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
Co-authored-by: Patrick von Platen <patrick@huggingface.co>
==

examples/flax/text-classification/README.md
examples/flax/text-classification/requirements.txt
examples/flax/text-classification/run_flax_glue.py
==================
f13f1f8fb;Sylvain Gugger;2021-05-11 12:02:48 -0400;Test checkpointing (#11682)
* Add test and see where CI is unhappy

* Load with strict=False
==

src/transformers/trainer.py
tests/test_modeling_common.py
==================
d9b286272;Julien Plu;2021-05-11 18:01:03 +0200;Fix TF Roberta for mixed precision training (#11675)

==

src/transformers/models/roberta/modeling_tf_roberta.py
==================
a135f5953;Sylvain Gugger;2021-05-11 11:30:34 -0400;Auto modelcard (#11599)
* Autogenerate model cards from the Trainer

* ModelCard deprecated

* Fix test

* Style

* Apply suggestions from code review

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* Address review comments

* Quality

* With all metadata

* Metadata

* Post-merge conflict mess

* Data args and all examples

* Default license and languages when possible

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
==

examples/pytorch/language-modeling/run_clm.py
examples/pytorch/language-modeling/run_mlm.py
examples/pytorch/language-modeling/run_plm.py
examples/pytorch/multiple-choice/run_swag.py
examples/pytorch/question-answering/run_qa.py
examples/pytorch/question-answering/run_qa_beam_search.py
examples/pytorch/summarization/run_summarization.py
examples/pytorch/text-classification/run_glue.py
examples/pytorch/token-classification/run_ner.py
examples/pytorch/translation/run_translation.py
src/transformers/modelcard.py
src/transformers/pipelines/__init__.py
src/transformers/trainer.py
tests/test_trainer.py
==================
b3429ab67;Matt;2021-05-11 15:49:34 +0100;Grammar and style edits for the frontpage README (#11679)
* Grammar and style edits for the frontpage README

* Going all-in on em-dashes because you only live once

* Update README.md

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

README.md
==================
901153c61;nxznm;2021-05-11 20:12:02 +0800;Fix docstring of description about input_ids (#11672)

==

src/transformers/models/distilbert/modeling_distilbert.py
==================
64232bc0d;Jonathan Chang;2021-05-11 19:58:38 +0800;Add --text_column to run_summarization_no_trainer (#11673)

==

examples/pytorch/summarization/run_summarization_no_trainer.py
==================
024cd19bb;Julien Plu;2021-05-11 11:42:21 +0200;Add MacOS TF version (#11674)
Co-authored-by: Julien Plu <jplu@argos.local>
==

src/transformers/file_utils.py
==================
9120ae7d6;Pavel Soriano;2021-05-10 19:28:10 +0200;Fixes NoneType exception when topk is larger than one coupled with a small context in the Question-Answering pipeline (#11628)
* added fix to decode function. added test to qa pipeline tests

* completed topk docstring

* fixed formatting with black

* applied style_doc to fix line length
==

src/transformers/pipelines/question_answering.py
tests/test_pipelines_question_answering.py
==================
dcb0e6143;Patrick von Platen;2021-05-10 17:38:17 +0100;push (#11667)

==

src/transformers/models/auto/tokenization_auto.py
==================
05a930671;Sylvain Gugger;2021-05-10 10:58:30 -0400;Save scaler state dict when checkpointing (#11663)

==

src/transformers/trainer.py
==================
ef8d32c5e;Matt;2021-05-10 14:28:04 +0100;Fix suggested by @bhadreshpsavani (#11660)

==

examples/tensorflow/text-classification/run_text_classification.py
==================
575c97914;Vasudev Gupta;2021-05-10 14:18:21 +0530;Update community.md (#11654)

==

docs/source/community.md
==================
f7f872955;Tanmay Laud;2021-05-10 00:01:23 -0700;Big Bird Fast Tokenizer implementation (#11075)
* Added Big Bird Fast Tokenizer initial file

* style fixes

* flake fixes

* Added big bird fast tokenizer to init files

* Added big bird fast to Auto tokenization

* fix styles

* minor quality fixes

* Added initial test code

* Fix SpmConverter when precompiled_charsmap doesn't exist

* fixed post processor

* minor style fix

* minor fix input names

* Actually fix identity normalization

* style

* Added token type ids to fast tokenizer

* style

* flake fix

* fix copies

Co-authored-by: Anthony MOI <m.anthony.moi@gmail.com>
==

docs/source/index.rst
docs/source/model_doc/bigbird.rst
src/transformers/__init__.py
src/transformers/convert_slow_tokenizer.py
src/transformers/models/auto/tokenization_auto.py
src/transformers/models/big_bird/__init__.py
src/transformers/models/big_bird/tokenization_big_bird_fast.py
src/transformers/utils/dummy_tokenizers_objects.py
tests/test_tokenization_big_bird.py
==================
80da304a0;Bhavitvya Malik;2021-05-10 12:15:29 +0530;updated user permissions based on umask (#11119)
* updated user permissions based on umask

* updated user permissions based on umask

* changes as per suggestions

* minor changes
==

src/transformers/file_utils.py
==================
1a0b41781;Quentin Lhoest;2021-05-10 07:49:52 +0200;Update requirements.txt (#11634)

==

examples/research_projects/rag/requirements.txt
==================
f785c5169;NielsRogge;2021-05-10 07:48:43 +0200;Update code example (#11631)
* Update code example

* Code review
==

src/transformers/models/luke/modeling_luke.py
==================
7e406f4a6;Tommy Chiang;2021-05-10 13:46:48 +0800;[Examples] Fix invalid links after reorg (#11650)

==

examples/legacy/token-classification/README.md
examples/pytorch/text-classification/README.md
examples/pytorch/text-generation/README.md
examples/research_projects/mm-imdb/README.md
examples/research_projects/movement-pruning/README.md
==================
f2ffcaf49;Tommy Chiang;2021-05-10 03:42:38 +0800;[Examples] Check key exists in datasets first (#11503)

==

examples/pytorch/multiple-choice/run_swag.py
examples/pytorch/summarization/run_summarization.py
examples/pytorch/translation/run_translation.py
==================
ba0d50f21;Stas Bekman;2021-05-07 14:44:22 -0700;[examples] fix sys.path in conftest.py (#11636)
* restore conftest.py

* fix conftest and make copies

* remove unneeded parts

* remove unwanted files
==

examples/pytorch/conftest.py
==================
cd9b8d7ef;Stas Bekman;2021-05-07 14:06:33 -0700;[self-push CI] sync with self-scheduled (#11637)
forgot to add the missing `libaio-dev` to this workflow
==

.github/workflows/self-push.yml
==================
da37eb8e4;Lysandre Debut;2021-05-07 17:55:20 +0200;Reduce to 1 worker and set timeout for GPU TF tests (#11633)

==

.github/workflows/self-push.yml
==================
39084ca66;Lysandre Debut;2021-05-07 14:08:40 +0200;Add the ImageClassificationPipeline (#11598)
* Add the ImageClassificationPipeline

* Code review

Co-authored-by: patrickvonplaten <patrick.v.platen@gmail.com>

* Have `load_image` at the module level

Co-authored-by: patrickvonplaten <patrick.v.platen@gmail.com>
==

.github/workflows/model-templates.yml
docs/source/main_classes/pipelines.rst
docs/source/model_doc/auto.rst
src/transformers/__init__.py
src/transformers/feature_extraction_utils.py
src/transformers/models/auto/feature_extraction_auto.py
src/transformers/models/speech_to_text/processing_speech_to_text.py
src/transformers/models/wav2vec2/processing_wav2vec2.py
src/transformers/pipelines/__init__.py
src/transformers/pipelines/base.py
src/transformers/pipelines/image_classification.py
src/transformers/utils/dummy_pt_objects.py
tests/fixtures/coco.jpg
tests/fixtures/preprocessor_config.json
tests/test_feature_extraction_auto.py
tests/test_pipelines_image_classification.py
==================
e7bff0aab;Patrick von Platen;2021-05-07 13:48:51 +0200;make fix copy (#11627)

==

src/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py
==================
dc3f6758c;Vasudev Gupta;2021-05-07 12:57:43 +0530;Add BigBirdPegasus (#10991)
* init bigbird pegasus

* add debugging nb ; update config

* init conversion

* update conversion script

* complete conversion script

* init forward()

* complete forward()

* add tokenizer

* add some slow tests

* commit current

* fix copies

* add docs

* add conversion script for bigbird-roberta-summarization

* remove TODO

* small fixups

* correct tokenizer

* add bigbird core for now

* fix config

* fix more

* revert pegasus-tokenizer back

* make style

* everything working for pubmed; yayygit status

* complete tests finally

* remove bigbird pegasus tok

* correct tokenizer

* correct tests

* add tokenizer files

* finish make style

* fix test

* update

* make style

* fix tok utils base file

* make fix-copies

* clean a bit

* small update

* fix some suggestions

* add to readme

* fix a bit, clean tests

* fix more tests

* Update src/transformers/__init__.py

* Update src/transformers/__init__.py

* make fix-copies

* complete attn switching, auto-padding left

* make style

* fix auto-padding test

* make style

* fix batched attention tests

* put tolerance at 1e-1 for stand-alone decoder test

* fix docs

* fix tests

* correct slow tokenizer conversion

* Apply suggestions from code review

Co-authored-by: Suraj Patil <surajp815@gmail.com>
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* complete remaining suggestions

* fix test

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
Co-authored-by: Suraj Patil <surajp815@gmail.com>
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

README.md
docs/source/index.rst
docs/source/model_doc/bigbird_pegasus.rst
src/transformers/__init__.py
src/transformers/convert_slow_tokenizer.py
src/transformers/models/__init__.py
src/transformers/models/auto/configuration_auto.py
src/transformers/models/auto/modeling_auto.py
src/transformers/models/big_bird/modeling_big_bird.py
src/transformers/models/bigbird_pegasus/__init__.py
src/transformers/models/bigbird_pegasus/configuration_bigbird_pegasus.py
src/transformers/models/bigbird_pegasus/convert_bigbird_pegasus_tf_to_pytorch.py
src/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py
src/transformers/models/pegasus/tokenization_pegasus.py
src/transformers/models/pegasus/tokenization_pegasus_fast.py
src/transformers/utils/dummy_pt_objects.py
src/transformers/utils/modeling_auto_mapping.py
tests/test_generation_utils.py
tests/test_modeling_bigbird_pegasus.py
tests/test_modeling_common.py
tests/test_tokenization_pegasus.py
utils/check_repo.py
==================
6f40e3176;Jonathan Chang;2021-05-07 15:02:30 +0800;Fix comment in run_clm_no_trainer.py (#11624)

==

examples/pytorch/language-modeling/run_clm_no_trainer.py
==================
33fd83bc0;Sylvain Gugger;2021-05-06 17:14:12 -0400;Fix RNG saves in distributed mode. (#11620)
* Fix RNG saves in distributed mode.

* Update src/transformers/trainer.py

Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>

Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>
==

src/transformers/trainer.py
==================
619200cc4;Stas Bekman;2021-05-06 13:35:28 -0700;[cuda ext tests] fixing tests (#11619)
* fixing tests

* cleanup
==

.github/workflows/self-scheduled.yml
tests/deepspeed/test_deepspeed.py
tests/extended/test_trainer_ext.py
==================
44c5621db;Patrick von Platen;2021-05-06 20:42:51 +0200;fix tests (#11615)

==

src/transformers/modeling_utils.py
==================
7eee950ac;Sylvain Gugger;2021-05-06 14:24:19 -0400;Re-styling in seq2seq attention (#11613)

==

src/transformers/models/bart/modeling_bart.py
src/transformers/models/blenderbot/modeling_blenderbot.py
src/transformers/models/blenderbot_small/modeling_blenderbot_small.py
src/transformers/models/m2m_100/modeling_m2m_100.py
src/transformers/models/marian/modeling_marian.py
src/transformers/models/mbart/modeling_mbart.py
src/transformers/models/pegasus/modeling_pegasus.py
src/transformers/models/speech_to_text/modeling_speech_to_text.py
src/transformers/models/wav2vec2/modeling_wav2vec2.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_{{cookiecutter.lowercase_modelname}}.py
==================
cf409e559;Eldar Kurtic;2021-05-06 13:39:28 +0200;Fix docstring typo (#11611)

==

src/transformers/optimization.py
==================
f594090a9;Vipul Raheja;2021-05-06 00:02:54 -0700;fix typo in command (#11605)

==

examples/pytorch/translation/README.md
==================
079557c1c;Lysandre Debut;2021-05-06 08:50:11 +0200;Fix Python version (#11607)

==

.github/workflows/release-conda.yml
==================
c1780ce7a;baeseongsu;2021-05-06 15:18:02 +0900;fix head_mask for albert encoder part(`AlbertTransformer`) (#11596)
* fix head mask for albert encoder part

* fix head_mask for albert encoder part
==

src/transformers/models/albert/modeling_albert.py
==================
864c1dfe3;Mats Sj√∂berg;2021-05-05 21:44:29 +0300;Accept tensorflow-rocm package when checking TF availability (#11595)

==

src/transformers/file_utils.py
==================
3e3e41ae2;Patrick von Platen;2021-05-05 17:22:20 +0200;Pytorch - Lazy initialization of models (#11471)
* lazy_init_weights

* remove ipdb

* save int

* add necessary code

* remove unnecessary utils

* Update src/transformers/models/t5/modeling_t5.py

* clean

* add tests

* correct

* finish tests

* finish tests

* fix some more tests

* fix xlnet & transfo-xl

* fix more tests

* make sure tests are independent

* fix tests more

* finist tests

* final touches

* Update src/transformers/modeling_utils.py

* Apply suggestions from code review

* Update src/transformers/modeling_utils.py

Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>

* Update src/transformers/modeling_utils.py

Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>

* clean tests

* give arg positive name

* add more mock weights to xlnet

Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>
==

examples/pytorch/test_examples.py
src/transformers/modeling_utils.py
tests/test_modeling_common.py
tests/test_modeling_funnel.py
tests/test_modeling_transfo_xl.py
tests/test_modeling_wav2vec2.py
tests/test_modeling_xlnet.py
==================
8fa8e1942;Lysandre;2021-05-05 12:38:01 +0200;Skip Funnel test

==

tests/test_modeling_tf_funnel.py
==================
83e59d8e0;Deepali;2021-05-05 13:06:18 +0530;add importlib_metadata and huggingface_hub as dependency in the conda recipe (#11591)
* add importlib_metadata as dependency (#11490)

Co-authored-by: Deepali Chourasia <deepch23@us.ibm.com>

* add huggingface_hub dependency

Co-authored-by: Deepali Chourasia <deepch23@us.ibm.com>
==

.github/conda/meta.yaml
==================
bf0dfa98d;Stas Bekman;2021-05-05 00:35:15 -0700;copies need to be fixed too (#11585)

==

.github/workflows/model-templates.yml
==================
c065025c4;Stas Bekman;2021-05-04 14:17:11 -0700;[trainer] document resume randomness (#11588)
* document resume randomness

* fix link

* reword

* fix

* reword

* style
==

docs/source/main_classes/trainer.rst
==================
6b241e0e3;Sylvain Gugger;2021-05-04 16:20:56 -0400;Reproducible checkpoint (#11582)
* Set generator in dataloader

* Use generator in all random samplers

* Checkpoint all RNG states

* Final version

* Quality

* Test

* Address review comments

* Quality

* Remove debug util

* Add python and numpy RNGs

* Split states in different files in distributed

* Quality

* local_rank for TPUs

* Only use generator when accepted

* Add test

* Set seed to avoid flakiness

* Make test less flaky

* Quality
==

examples/pytorch/test_examples.py
src/transformers/trainer.py
src/transformers/trainer_pt_utils.py
tests/test_trainer.py
==================
0afe4a90f;Patrick Fernandes;2021-05-04 19:56:09 +0100;[Flax] Add Electra models (#11426)
* add electra model to flax

* Remove Electra Next Sentence Prediction model added by mistake

* fix parameter sharing and loosen equality threshold

* fix styling issues

* add mistaken removen imports

* fix electra table

* Add FlaxElectra to automodels and fixe docs

* fix issues pointed out the PR

* fix flax electra to comply with latest changes

* remove stale class

* add copied from

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
==

docs/source/index.rst
docs/source/model_doc/electra.rst
src/transformers/__init__.py
src/transformers/models/auto/modeling_flax_auto.py
src/transformers/models/electra/__init__.py
src/transformers/models/electra/modeling_flax_electra.py
src/transformers/utils/dummy_flax_objects.py
tests/test_modeling_flax_electra.py
==================
226e74b61;Philipp Schmid;2021-05-04 20:31:18 +0200;Removes SageMakerTrainer code but keeps class as wrapper (#11587)
* removed all old code

* make quality
==

src/transformers/sagemaker/trainer_sm.py
==================
084a187da;Patrick von Platen;2021-05-04 19:57:59 +0200;[FlaxRoberta] Add FlaxRobertaModels & adapt run_mlm_flax.py (#11470)
* add flax roberta

* make style

* correct initialiazation

* modify model to save weights

* fix copied from

* fix copied from

* correct some more code

* add more roberta models

* Apply suggestions from code review

* merge from master

* finish

* finish docs

Co-authored-by: Patrick von Platen <patrick@huggingface.co>
==

docs/source/model_doc/roberta.rst
examples/flax/language-modeling/run_mlm_flax.py
src/transformers/__init__.py
src/transformers/file_utils.py
src/transformers/modeling_flax_utils.py
src/transformers/models/auto/modeling_flax_auto.py
src/transformers/models/roberta/__init__.py
src/transformers/models/roberta/modeling_flax_roberta.py
src/transformers/utils/dummy_flax_objects.py
tests/test_modeling_flax_common.py
tests/test_modeling_flax_roberta.py
==================
2ce0fb84c;Sylvain Gugger;2021-05-04 09:53:44 -0400;Make quality scripts work when one backend is missing. (#11573)
* Make quality scripts work when one backend is missing.

* Check env variable is properly set

* Add default

* With print statements

* Fix typo

* Set env variable

* Remove debug code
==

.circleci/config.yml
utils/check_repo.py
==================
09b0bcfea;Lysandre Debut;2021-05-04 14:13:57 +0200;Enable added tokens (#11325)
* Fix tests

* Reorganize

* Update tests/test_modeling_mobilebert.py

* Remove unnecessary addition
==

src/transformers/models/herbert/tokenization_herbert.py
src/transformers/models/herbert/tokenization_herbert_fast.py
src/transformers/models/mbart/tokenization_mbart.py
src/transformers/models/mbart/tokenization_mbart_fast.py
src/transformers/models/t5/tokenization_t5.py
src/transformers/models/t5/tokenization_t5_fast.py
tests/test_tokenization_common.py
tests/test_tokenization_t5.py
==================
c40c7e213;abhishek thakur;2021-05-04 08:23:40 +0200;Add multi-class, multi-label and regression to transformers (#11012)
* add to  bert

* review comments

* Update src/transformers/configuration_utils.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/configuration_utils.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* self.config.problem_type

* fix style

* fix

* fin

* fix

* update doc

* fix

* test

* Test more problem types

* Update src/transformers/configuration_utils.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* fix

* remove

* fix

* quality

* make fix-copies

* remove test

Co-authored-by: abhishek thakur <abhishekkrthakur@users.noreply.github.com>
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
Co-authored-by: Lysandre <lysandre.debut@reseau.eseo.fr>
==

src/transformers/configuration_utils.py
src/transformers/models/albert/modeling_albert.py
src/transformers/models/bert/modeling_bert.py
src/transformers/models/big_bird/modeling_big_bird.py
src/transformers/models/convbert/modeling_convbert.py
src/transformers/models/distilbert/modeling_distilbert.py
src/transformers/models/electra/modeling_electra.py
src/transformers/models/funnel/modeling_funnel.py
src/transformers/models/longformer/modeling_longformer.py
src/transformers/models/mobilebert/modeling_mobilebert.py
src/transformers/models/reformer/modeling_reformer.py
src/transformers/models/roberta/modeling_roberta.py
src/transformers/models/squeezebert/modeling_squeezebert.py
src/transformers/models/xlm/modeling_xlm.py
src/transformers/models/xlnet/modeling_xlnet.py
tests/test_modeling_albert.py
tests/test_modeling_bert.py
tests/test_modeling_big_bird.py
tests/test_modeling_common.py
tests/test_modeling_convbert.py
tests/test_modeling_distilbert.py
tests/test_modeling_electra.py
tests/test_modeling_funnel.py
tests/test_modeling_longformer.py
tests/test_modeling_mobilebert.py
tests/test_modeling_reformer.py
tests/test_modeling_roberta.py
tests/test_modeling_squeezebert.py
tests/test_modeling_xlm.py
tests/test_modeling_xlnet.py
==================
7c622482e;Stas Bekman;2021-05-03 13:12:06 -0700;fix resize_token_embeddings (#11572)

==

src/transformers/modeling_utils.py
==================
fe82b1bfa;Sylvain Gugger;2021-05-03 13:18:46 -0400;Update training tutorial (#11533)
* Update training tutorial

* Apply suggestions from code review

Co-authored-by: Hamel Husain <hamelsmu@github.com>

* Address review comments

* Update docs/source/training.rst

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* More review comments

* Last review comments

Co-authored-by: Hamel Husain <hamelsmu@github.com>
Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
==

docs/source/training.rst
==================
f4c9a7e62;Sylvain Gugger;2021-05-03 13:18:27 -0400;Accumulate opt state dict on do_rank 0 (#11481)

==

src/transformers/trainer.py
==================
1e8e06862;Nicolas Patry;2021-05-03 18:48:13 +0200;Fixes a useless warning. (#11566)
Fixes #11525
==

src/transformers/generation_utils.py
==================
87dd1a00e;Sylvain Gugger;2021-05-03 11:42:55 -0400;Fix metric computation in `run_glue_no_trainer` (#11569)

==

examples/pytorch/text-classification/run_glue_no_trainer.py
==================
a721a5eef;Muktan;2021-05-03 20:49:12 +0530;[Wav2vec2] Fixed tokenization mistakes while adding single-char tokens to tokenizer (#11538)
* Fixed tokenization mistakes while adding single-char tokens to tokenizer

* Added tests and Removed unnecessary comments.

* finalize wav2vec2 tok

* add more aggressive tests

* Apply suggestions from code review

* fix useless import

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
==

src/transformers/models/wav2vec2/tokenization_wav2vec2.py
tests/test_tokenization_wav2vec2.py
==================
f3cf8ae7b;NielsRogge;2021-05-03 15:07:29 +0200;Add LUKE (#11223)
* Rebase with master

* Minor bug fix in docs

* Copy files from adding_luke_v2 and improve docs

* change the default value of use_entity_aware_attention to True

* remove word_hidden_states

* fix head models

* fix tests

* fix the conversion script

* add integration tests for the pretrained large model

* improve docstring

* Improve docs, make style

* fix _init_weights for pytorch 1.8

* improve docs

* fix tokenizer to construct entity sequence with [MASK] entity when entities=None

* Make fix-copies

* Make style & quality

* Bug fixes

* Add LukeTokenizer to init

* Address most comments by @patil-suraj and @LysandreJik

* rename _compute_extended_attention_mask to get_extended_attention_mask

* add comments to LukeSelfAttention

* fix the documentation of the tokenizer

* address comments by @patil-suraj, @LysandreJik, and @sgugger

* improve docs

* Make style, quality and fix-copies

* Improve docs

* fix docs

* add "entity_span_classification" task

* update example code for LukeForEntitySpanClassification

* improve docs

* improve docs

* improve the code example in luke.rst

* rename the classification layer in LukeForEntityClassification from typing to classifier

* add bias to the classifier in LukeForEntitySpanClassification

* update docs to use fine-tuned hub models in code examples of the head models

* update the example sentences

* Make style & quality

* Add require_torch to tokenizer tests

* Add require_torch to tokenizer tests

* Address comments by @sgugger and add community notebooks

* Make fix-copies

Co-authored-by: Ikuya Yamada <ikuya@ikuya.net>
==

README.md
docs/source/community.md
docs/source/index.rst
docs/source/model_doc/luke.rst
src/transformers/__init__.py
src/transformers/models/__init__.py
src/transformers/models/auto/configuration_auto.py
src/transformers/models/auto/modeling_auto.py
src/transformers/models/auto/tokenization_auto.py
src/transformers/models/luke/__init__.py
src/transformers/models/luke/configuration_luke.py
src/transformers/models/luke/convert_luke_original_pytorch_checkpoint_to_pytorch.py
src/transformers/models/luke/modeling_luke.py
src/transformers/models/luke/tokenization_luke.py
src/transformers/tokenization_utils_base.py
src/transformers/utils/dummy_pt_objects.py
tests/test_modeling_luke.py
tests/test_tokenization_luke.py
utils/check_repo.py
==================
6a11e4c2a;Frederik Bode;2021-05-03 13:43:30 +0200;fix the mlm longformer example by changing [MASK] to <mask> (#11559)

==

src/transformers/models/longformer/modeling_tf_longformer.py
==================
1c86157d9;Lysandre Debut;2021-05-03 12:02:33 +0200;Remove `datasets` submodule. (#11563)

==

datasets
==================
c448c01f2;Patrick von Platen;2021-05-03 11:53:30 +0200;[Wav2Vec2] Fix convert (#11562)
* push

* small change

* correct other typo
==

src/transformers/models/wav2vec2/convert_wav2vec2_original_pytorch_checkpoint_to_pytorch.py
==================
623281aa1;Suraj Patil;2021-05-03 14:05:06 +0530;[Flax BERT/Roberta] few small fixes (#11558)
* small fixes

* style
==

src/transformers/models/bert/modeling_flax_bert.py
src/transformers/models/roberta/modeling_flax_roberta.py
==================
a5d2967bd;lewtun;2021-05-03 07:26:31 +0200;Fix examples in M2M100 docstrings (#11540)
Replaces `tok` with `tokenizer` so examples can run with copy-paste
==

src/transformers/models/m2m_100/modeling_m2m_100.py
==================
980208650;jingyihe;2021-05-02 04:10:47 -0400;Fixed docs for the shape of `scores` in `generate()` (#10057)
* Fixed the doc for the shape of return scores tuples in generation_utils.py.

* Fix the output shape of `scores` for `DecoderOnlyOutput`.

* style fix
==

src/transformers/generation_utils.py
==================
4e7bf94e7;Stas Bekman;2021-04-30 12:51:48 -0700;[DeepSpeed] fp32 support (#11499)
* prep for deepspeed==0.3.16

* new version

* too soon

* support and test fp32 mode

* troubleshooting doc start

* workaround no longer needed

* add fp32 doc

* style

* cleanup, add tf32 note

* clarify

* release was made
==

docs/source/main_classes/trainer.rst
setup.py
src/transformers/dependency_versions_table.py
src/transformers/integrations.py
src/transformers/modeling_utils.py
tests/deepspeed/test_deepspeed.py
==================
282f3ac3e;Stas Bekman;2021-04-30 11:15:46 -0700;[debug utils] activation/weights underflow/overflow detector (#11274)
* sync

* add activation overflow debug utility

* cleanup

* document detect_overflow

* import torch

* add deprecation warning

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* convert to rst, add note

* add class

* fix docs

* improve the doc

* rework to dump a lot more info about each frame

* complete expansion

* cleanup

* format

* cleanup

* doesn't have to be transformers

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* wrap long line

* style

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

docs/source/debugging.rst
docs/source/index.rst
docs/source/internal/trainer_utils.rst
src/transformers/debug_utils.py
src/transformers/tokenization_utils_base.py
src/transformers/trainer.py
src/transformers/training_args.py
==================
804c2974d;Hamel Husain;2021-04-30 06:06:47 -0700;Improve task summary docs (#11513)
* fix task summary docs

* refactor to use model.config.id2label instead of list

* fix nit

* Update docs/source/task_summary.rst

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

docs/source/task_summary.rst
==================
bc80f8bc3;Sylvain Gugger;2021-04-30 09:03:13 -0400;Add Stas and Suraj as authors (#11526)

==

setup.py
==================
84326a28f;Bhadresh Savani;2021-04-30 18:32:50 +0530;[Examples] Added support for test-file in QA examples with no trainer (#11510)
* added support for test-file

* fixed typo

* added suggested changes

* reformatted code

* modifed files

* fix post processing error

* Trigger CI

* removed extra lines
==

datasets
examples/pytorch/question-answering/README.md
examples/pytorch/question-answering/run_qa_beam_search_no_trainer.py
examples/pytorch/question-answering/run_qa_no_trainer.py
==================
af0692a2c;Lysandre Debut;2021-04-30 14:47:12 +0200;Run model templates on master (#11527)

==

.github/workflows/model-templates.yml
==================
57c8e822f;Suraj Patil;2021-04-30 18:17:01 +0530;reszie token embeds (#11524)

==

examples/pytorch/summarization/run_summarization.py
examples/pytorch/translation/run_translation.py
==================
20d6931e3;Matt;2021-04-30 13:45:33 +0100;Update TF text classification example (#11496)
Big refactor, fixes and multi-GPU/TPU support
==

examples/tensorflow/text-classification/README.md
examples/tensorflow/text-classification/run_text_classification.py
src/transformers/training_args_tf.py
==================
8b945ef03;bonniehyeon;2021-04-30 21:35:12 +0900;Fix do_eval default value in training_args.py (#11511)
* Fix do_eval default value in training_args.py

* Update PULL_REQUEST_TEMPLATE.md
==

src/transformers/training_args.py
==================
c2cd02ac6;Takuya Makino;2021-04-30 21:27:46 +0900;Accepts BatchEncoding in LengthSampler (#11431)

==

src/transformers/trainer_pt_utils.py
tests/test_trainer_utils.py
==================
30ede8994;Shubham Sanghavi;2021-04-30 07:08:15 -0500;Implement Fast Tokenization for Deberta (#11387)

==

docs/source/index.rst
docs/source/model_doc/deberta.rst
src/transformers/__init__.py
src/transformers/convert_slow_tokenizer.py
src/transformers/models/auto/tokenization_auto.py
src/transformers/models/deberta/__init__.py
src/transformers/models/deberta/tokenization_deberta_fast.py
src/transformers/utils/dummy_tokenizers_objects.py
tests/test_tokenization_deberta.py
==================
db9dd09cf;Nicolas Patry;2021-04-30 11:54:08 +0200;Adding `AutomaticSpeechRecognitionPipeline`. (#11337)
* Adding `AutomaticSpeechRecognitionPipeline`.

- Because we added everything to enable this pipeline, we probably
should add it to `transformers`.
- This PR tries to limit the scope and focuses only on the pipeline part
(what should go in, and out).
- The tests are very specific for S2T and Wav2vec2 to make sure both
architectures are supported by the pipeline. We don't use the mixin for
tests right now, because that requires more work in the `pipeline`
function (will be done in a follow up PR).
- Unsure about the "helper" function `ffmpeg_read`. It makes a lot of
  sense from a user perspective, it does not add any additional
dependencies (as in hard dependency, because users can always use their
own load mechanism). Meanwhile, it feels slightly clunky to have so much
optional preprocessing.
- The pipeline is not done to support streaming audio right now.

Future work:

- Add `automatic-speech-recognition` as a `task`. And add the
FeatureExtractor.from_pretrained within `pipeline` function.
- Add small models within tests
- Add the Mixin to tests.
- Make the logic between ForCTC vs ForConditionalGeneration better.

* Update tests/test_pipelines_automatic_speech_recognition.py

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* Adding docs + main import + type checking + LICENSE.

* Doc style !.

* Fixing TYPE_HINT.

* Specifying waveform shape in the docs.

* Adding asserts + specify in the documentation the shape of the input
np.ndarray.

* Update src/transformers/pipelines/automatic_speech_recognition.py

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* Adding require to tests + move the `feature_extractor` doc.

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
==

docs/source/main_classes/pipelines.rst
src/transformers/__init__.py
src/transformers/pipelines/__init__.py
src/transformers/pipelines/automatic_speech_recognition.py
tests/test_pipelines_automatic_speech_recognition.py
==================
76116f479;CeShine Lee;2021-04-30 16:43:55 +0800; T5 Gradient Checkpointing (#11353)
* Implement gradient checkpoinging for T5Stack

* A bit more robust type checking

* Add `gradient_checkpointing` to T5Config

* Formatting

* Set requires_grad only when training

* None return value will only cause problems when training

* Change the output tuple according to `use_cache`

* Enable gradient checkpointing for the decoder

Squashed commit of the following:

commit 658bdd0bd1215353a8770f558bda2ea69a0ad0c7
Author: Ceshine Lee <shuanck@gmail.com>
Date:   Sat Apr 24 14:08:17 2021 +0800

    Only set `require_grad` for gradient checkpointing

commit acaeee6b2e675045fb28ce2176444c1d63e908bd
Author: Ceshine Lee <shuanck@gmail.com>
Date:   Sat Apr 24 13:59:35 2021 +0800

    Make gradient checkpointing work with the decoder

* Formatting
==

src/transformers/models/t5/configuration_t5.py
src/transformers/models/t5/modeling_t5.py
==================
58c789e3d;Manuel Romero;2021-04-30 10:29:59 +0200;Update README.md (#11489)
Add link to code
==

examples/research_projects/zero-shot-distillation/README.md
==================
022a1e9e6;Patrick von Platen;2021-04-30 09:54:58 +0200;make style (#11520)

==

src/transformers/models/led/modeling_led.py
==================
e0db8276a;Philip May;2021-04-30 09:44:58 +0200;add sp_model_kwargs to unpickle of xlm roberta tok (#11430)
add test for pickle

simplify test

fix test code style

add missing pickle import

fix test

fix test

fix test
==

src/transformers/models/xlm_roberta/tokenization_xlm_roberta.py
tests/test_tokenization_xlm_roberta.py
==================
b43e3f93a;Frederik Bode;2021-04-30 09:42:13 +0200;correct the dimension comment of matrix multiplication (#11494)
Co-authored-by: Frederik Bode <frederik@paperbox.ai>
==

src/transformers/models/longformer/modeling_longformer.py
==================
f37f2adb6;Lysandre Debut;2021-04-30 08:57:50 +0200;Pin HuggingFace Hub dependency (#11502)

==

setup.py
src/transformers/dependency_versions_table.py
==================
60d5bda4f;Lysandre;2021-04-30 08:55:58 +0200;Patch notification service

==

utils/notification_service.py
==================
b29eb247d;Sylvain Gugger;2021-04-29 18:33:47 -0400;Split checkpoint from model_name_or_path in examples (#11492)
* Split checkpoint from model_name_or_path in examples

* Address review comments

* Address review comments
==

examples/pytorch/README.md
examples/pytorch/language-modeling/run_clm.py
examples/pytorch/language-modeling/run_mlm.py
examples/pytorch/language-modeling/run_plm.py
examples/pytorch/multiple-choice/run_swag.py
examples/pytorch/question-answering/run_qa.py
examples/pytorch/question-answering/run_qa_beam_search.py
examples/pytorch/summarization/run_summarization.py
examples/pytorch/text-classification/run_glue.py
examples/pytorch/text-classification/run_xnli.py
examples/pytorch/token-classification/run_ner.py
examples/pytorch/translation/run_translation.py
src/transformers/training_args.py
==================
d6ec54ba3;Michael Benayoun;2021-04-29 21:47:26 +0200;solved coefficient issue for the TF version of gelu_fast (#11514)
Co-authored-by: Michael Benayoun <michael@huggingface.co>
==

src/transformers/activations_tf.py
==================
ad1f7bef1;Sylvain Gugger;2021-04-29 07:51:09 -0400;Reformat to make code clearer in tokenizer call (#11497)
* Reformat to make code clearer

* Reformat to make code clearer
==

src/transformers/tokenization_utils_base.py
==================
f748bd424;Patrick von Platen;2021-04-29 12:04:51 +0200;[Flax] Add docstrings & model outputs (#11498)
* add attentions & hidden states

* add model outputs + docs

* finish docs

* finish tests

* finish impl

* del @

* finish

* finish

* correct test

* apply sylvains suggestions

* Update src/transformers/models/bert/modeling_flax_bert.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* simplify more

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

src/transformers/file_utils.py
src/transformers/modeling_flax_outputs.py
src/transformers/modeling_flax_utils.py
src/transformers/models/bert/modeling_flax_bert.py
src/transformers/models/roberta/modeling_flax_roberta.py
tests/test_modeling_common.py
tests/test_modeling_flax_common.py
==================
3f6add8ba;Hamel Husain;2021-04-28 08:16:41 -0700;fix #1149 (#11493)

==

docs/source/installation.md
==================
c0eb218a5;Hamel Husain;2021-04-28 07:11:17 -0700;Update `PreTrainedTokenizerBase` to check/handle batch length for `text_pair` parameter (#11486)
* Update tokenization_utils_base.py

* add assertion

* check batch len

* Update src/transformers/tokenization_utils_base.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* add error message

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

src/transformers/tokenization_utils_base.py
==================
2d27900b5;Sylvain Gugger;2021-04-28 09:10:06 -0400;Update min versions in README and add Flax (#11472)
* Update min versions in README and add Flax

* Adapt index
==

README.md
docs/source/index.rst
==================
8d43c71a1;Suraj Patil;2021-04-27 19:36:36 +0530;fix docs for decoder_input_ids (#11466)
* fix docs for decoder_input_ids

* revert the changes for bart and mbart
==

src/transformers/models/bart/modeling_bart.py
src/transformers/models/bart/modeling_tf_bart.py
src/transformers/models/blenderbot/modeling_blenderbot.py
src/transformers/models/blenderbot/modeling_tf_blenderbot.py
src/transformers/models/blenderbot_small/modeling_blenderbot_small.py
src/transformers/models/blenderbot_small/modeling_tf_blenderbot_small.py
src/transformers/models/fsmt/modeling_fsmt.py
src/transformers/models/m2m_100/modeling_m2m_100.py
src/transformers/models/marian/modeling_marian.py
src/transformers/models/marian/modeling_tf_marian.py
src/transformers/models/mbart/modeling_mbart.py
src/transformers/models/mbart/modeling_tf_mbart.py
src/transformers/models/pegasus/modeling_pegasus.py
src/transformers/models/pegasus/modeling_tf_pegasus.py
src/transformers/models/prophetnet/modeling_prophetnet.py
src/transformers/models/speech_to_text/modeling_speech_to_text.py
src/transformers/models/t5/modeling_t5.py
==================
7ceff67e1;Hamel Husain;2021-04-27 07:04:12 -0700;Finish Making Quick Tour respect the model object (#11467)
* finish quicktour

* fix import

* fix print

* explain config default better

* Update docs/source/quicktour.rst

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

docs/source/quicktour.rst
==================
88ac60f7b;Hamel Husain;2021-04-26 19:18:37 -0700;update QuickTour docs to reflect model output object (#11462)
* update docs to reflect model output object

* run make style`
==

docs/source/main_classes/output.rst
docs/source/quicktour.rst
==================
741d48f5c;Ashwin Geet D'Sa;2021-04-27 00:28:40 +0200;Remove max length beam scorer (#11378)
* removed max_len

* removed max_length from BeamSearchScorer

* correct max length

* finish

* del vim

* finish & add test

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
==

src/transformers/generation_beam_search.py
src/transformers/generation_utils.py
src/transformers/models/marian/modeling_marian.py
src/transformers/models/rag/modeling_rag.py
tests/test_generation_beam_search.py
tests/test_generation_utils.py
==================
bc2571e61;Stas Bekman;2021-04-26 10:40:32 -0700;[Deepspeed] ZeRO-Infinity integration plus config revamp (#11418)
* adding Z-inf

* revamp config process

* up version requirement

* wip

* massive rewrite

* cleanup

* cleanup

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* consistent json commas

* act on suggestions

* leave this feature for 0.3.16

* style

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

docs/source/main_classes/trainer.rst
setup.py
src/transformers/dependency_versions_table.py
src/transformers/integrations.py
src/transformers/modeling_utils.py
src/transformers/training_args.py
tests/deepspeed/ds_config_zero2.json
tests/deepspeed/ds_config_zero3.json
tests/deepspeed/test_deepspeed.py
tests/test_trainer.py
==================
0661abc54;Jaimeen Ahn;2021-04-27 02:30:48 +0900;Variable Correction for Consistency in Distillation Example (#11444)
As the error comes from the inconsistency of variable meaning number of gpus in parser and its actual usage in the train.py script, 'gpus' and 'n_gpu' respectively,  the correction makes the example work
==

examples/research_projects/distillation/README.md
examples/research_projects/distillation/train.py
==================
1d30ec95c;Bhadresh Savani;2021-04-26 17:24:31 +0100;[Examples] Fixes inconsistency around eval vs val and predict vs test (#11380)
* added changes for uniformity

* modified files

* corrected typo

* fixed qa scripts

* fix typos

* fixed predict typo in qa no trainer

* fixed test file

* reverted trainer changes

* reverted trainer changes in custom exmaples

* updated readme

* added changes in deepspeed test

* added changes for predict and eval
==

examples/pytorch/README.md
examples/pytorch/language-modeling/run_clm.py
examples/pytorch/language-modeling/run_mlm.py
examples/pytorch/language-modeling/run_plm.py
examples/pytorch/multiple-choice/run_swag.py
examples/pytorch/question-answering/run_qa.py
examples/pytorch/question-answering/run_qa_beam_search.py
examples/pytorch/question-answering/run_qa_beam_search_no_trainer.py
examples/pytorch/question-answering/run_qa_no_trainer.py
examples/pytorch/question-answering/trainer_qa.py
examples/pytorch/summarization/run_summarization.py
examples/pytorch/text-classification/run_glue.py
examples/pytorch/text-classification/run_xnli.py
examples/pytorch/token-classification/run_ner.py
examples/pytorch/translation/run_translation.py
examples/tensorflow/text-classification/run_text_classification.py
templates/adding_a_new_example_script/{{cookiecutter.directory_name}}/run_{{cookiecutter.example_shortcut}}.py
tests/deepspeed/test_deepspeed.py
tests/extended/test_trainer_ext.py
==================
7959d8359;Sylvain Gugger;2021-04-26 11:52:23 -0400;Give each test a different repo name (#11453)

==

tests/test_configuration_common.py
tests/test_modeling_tf_common.py
tests/test_tokenization_common.py
tests/test_trainer.py
==================
b03b2a653;Sylvain Gugger;2021-04-26 11:45:04 -0400;Style

==

src/transformers/models/tapas/tokenization_tapas.py
src/transformers/tokenization_utils.py
src/transformers/tokenization_utils_base.py
==================
ce11318e7;Stas Bekman;2021-04-26 08:42:43 -0700;make sure to test against the local checkout (#11437)

==

Makefile
==================
a753cafdc;Stas Bekman;2021-04-26 08:37:32 -0700;[docs] fix invalid class name (#11438)
* fix invalid class name

* proper ref

* proper ref
==

docs/source/migration.md
src/transformers/integrations.py
==================
6715e3b6a;Kostas Stathoulopoulos;2021-04-26 16:29:36 +0100;Clarify description of the is_split_into_words argument (#11449)
* Improve documentation for is_split_into_words argument

* Change description wording
==

src/transformers/models/tapas/tokenization_tapas.py
src/transformers/tokenization_utils.py
src/transformers/tokenization_utils_base.py
==================
ab2cabb96;Sylvain Gugger;2021-04-26 10:26:52 -0400;Pass along seed to DistributedSampler (#11406)
* Pass along seed to DistributedSampler

* Add seed to DistributedLengthGroupedSampler
==

src/transformers/trainer.py
==================
b24ead87e;LSinev;2021-04-26 16:14:25 +0300;fix some typos in docs, comments, logging/errors (#11432)

==

src/transformers/commands/add_new_model.py
src/transformers/data/processors/squad.py
src/transformers/feature_extraction_sequence_utils.py
src/transformers/file_utils.py
src/transformers/generation_logits_process.py
src/transformers/generation_stopping_criteria.py
src/transformers/generation_tf_utils.py
src/transformers/generation_utils.py
src/transformers/modeling_flax_utils.py
src/transformers/modeling_outputs.py
src/transformers/modeling_tf_pytorch_utils.py
src/transformers/modeling_tf_utils.py
src/transformers/modeling_utils.py
src/transformers/models/auto/modeling_auto.py
src/transformers/models/auto/modeling_flax_auto.py
src/transformers/models/auto/modeling_tf_auto.py
src/transformers/models/bart/configuration_bart.py
src/transformers/models/bart/modeling_bart.py
src/transformers/models/bart/modeling_tf_bart.py
src/transformers/models/bert_japanese/tokenization_bert_japanese.py
src/transformers/models/bertweet/tokenization_bertweet.py
src/transformers/models/big_bird/modeling_big_bird.py
src/transformers/models/blenderbot/modeling_blenderbot.py
src/transformers/models/blenderbot_small/modeling_blenderbot_small.py
src/transformers/models/convbert/modeling_tf_convbert.py
src/transformers/models/ctrl/modeling_ctrl.py
src/transformers/models/deberta_v2/modeling_deberta_v2.py
src/transformers/models/deberta_v2/tokenization_deberta_v2.py
src/transformers/models/fsmt/convert_fsmt_original_pytorch_checkpoint_to_pytorch.py
src/transformers/models/fsmt/modeling_fsmt.py
src/transformers/models/funnel/modeling_funnel.py
src/transformers/models/funnel/modeling_tf_funnel.py
src/transformers/models/gpt2/modeling_tf_gpt2.py
src/transformers/models/gpt2/tokenization_gpt2.py
src/transformers/models/gpt_neo/modeling_gpt_neo.py
src/transformers/models/ibert/quant_modules.py
src/transformers/models/led/modeling_led.py
src/transformers/models/led/modeling_tf_led.py
src/transformers/models/longformer/modeling_longformer.py
src/transformers/models/longformer/modeling_tf_longformer.py
src/transformers/models/lxmert/configuration_lxmert.py
src/transformers/models/m2m_100/modeling_m2m_100.py
src/transformers/models/marian/modeling_marian.py
src/transformers/models/mbart/modeling_mbart.py
src/transformers/models/mbart/modeling_tf_mbart.py
src/transformers/models/mobilebert/modeling_mobilebert.py
src/transformers/models/mpnet/modeling_tf_mpnet.py
src/transformers/models/mpnet/tokenization_mpnet_fast.py
src/transformers/models/openai/modeling_openai.py
src/transformers/models/openai/modeling_tf_openai.py
src/transformers/models/pegasus/modeling_pegasus.py
src/transformers/models/pegasus/modeling_tf_pegasus.py
src/transformers/models/prophetnet/modeling_prophetnet.py
src/transformers/models/rag/modeling_tf_rag.py
src/transformers/models/rag/retrieval_rag.py
src/transformers/models/reformer/modeling_reformer.py
src/transformers/models/roberta/tokenization_roberta_fast.py
src/transformers/models/speech_to_text/modeling_speech_to_text.py
src/transformers/models/t5/modeling_t5.py
src/transformers/models/t5/modeling_tf_t5.py
src/transformers/models/tapas/modeling_tapas.py
src/transformers/models/tapas/tokenization_tapas.py
src/transformers/models/transfo_xl/tokenization_transfo_xl.py
src/transformers/models/wav2vec2/modeling_wav2vec2.py
src/transformers/models/wav2vec2/processing_wav2vec2.py
src/transformers/models/xlm/modeling_xlm.py
src/transformers/models/xlm/tokenization_xlm.py
src/transformers/models/xlnet/modeling_tf_xlnet.py
src/transformers/optimization_tf.py
src/transformers/pipelines/conversational.py
src/transformers/pipelines/text2text_generation.py
src/transformers/tokenization_utils_fast.py
src/transformers/trainer.py
src/transformers/trainer_callback.py
src/transformers/trainer_pt_utils.py
src/transformers/trainer_seq2seq.py
src/transformers/utils/logging.py
==================
e3e70f955;Amine Abdaoui;2021-04-26 15:08:43 +0200;docs(examples): fix link to TPU launcher script (#11427)

==

examples/pytorch/README.md
==================
d7633a4e4;Sylvain Gugger;2021-04-26 08:55:14 -0400;Add basic support for FP16 in SageMaker model parallelism (#11407)
* Add FP16 support for SageMaker MP

* Add print debugs

* Squeeze

* Remove debug statements

* Add defensive check

* Typo
==

src/transformers/trainer.py
src/transformers/trainer_pt_utils.py
==================
38a716cd4;Daniel Stancl;2021-04-26 14:16:21 +0200;TF BART models - Add `cross_attentions` to model output and fix cross-attention head masking (#10699)
* Add cross_attn_head_mask to BART

* Fix cross_attentions in TFBart-like models

* This commit enables returning of `cross_attentions`
for TFBart-like models

* It also fixes attention head masking in cross-attenion module

* Update TF model templates

* Fix missing , in TF model templates

* Fix typo: congig -> config
==

src/transformers/modeling_tf_outputs.py
src/transformers/models/bart/modeling_tf_bart.py
src/transformers/models/blenderbot/modeling_tf_blenderbot.py
src/transformers/models/blenderbot_small/modeling_tf_blenderbot_small.py
src/transformers/models/marian/modeling_tf_marian.py
src/transformers/models/mbart/modeling_tf_mbart.py
src/transformers/models/pegasus/modeling_tf_pegasus.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_tf_{{cookiecutter.lowercase_modelname}}.py
tests/test_modeling_tf_bart.py
tests/test_modeling_tf_blenderbot.py
tests/test_modeling_tf_blenderbot_small.py
tests/test_modeling_tf_common.py
tests/test_modeling_tf_marian.py
tests/test_modeling_tf_mbart.py
tests/test_modeling_tf_pegasus.py
==================
4bd6b54fa;Sylvain Gugger;2021-04-26 08:12:54 -0400;Pin black to 21.4b0

==

setup.py
src/transformers/dependency_versions_table.py
==================
c1625b326;Sylvain Gugger;2021-04-26 08:07:29 -0400;With style

==

src/transformers/dependency_versions_table.py
==================
4b72cfd95;Sylvain Gugger;2021-04-26 08:06:50 -0400;Pin black to 20.8.b1

==

setup.py
==================
32dbb2d95;Patrick von Platen;2021-04-26 13:50:34 +0200;make style (#11442)

==

examples/legacy/question-answering/run_squad.py
examples/legacy/run_openai_gpt.py
examples/legacy/run_swag.py
examples/legacy/seq2seq/minify_dataset.py
examples/research_projects/bert-loses-patience/run_glue_with_pabee.py
examples/research_projects/bertabs/modeling_bertabs.py
examples/research_projects/bertabs/test_utils_summarization.py
examples/research_projects/bertabs/utils_summarization.py
examples/research_projects/bertology/run_bertology.py
examples/research_projects/bertology/run_prune_gpt.py
examples/research_projects/deebert/run_glue_deebert.py
examples/research_projects/distillation/run_squad_w_distillation.py
examples/research_projects/mm-imdb/run_mmimdb.py
examples/research_projects/movement-pruning/emmental/modeling_bert_masked.py
examples/research_projects/movement-pruning/masked_run_glue.py
examples/research_projects/movement-pruning/masked_run_squad.py
src/transformers/commands/lfs.py
src/transformers/data/processors/utils.py
src/transformers/file_utils.py
src/transformers/modelcard.py
src/transformers/models/albert/modeling_albert.py
src/transformers/models/albert/modeling_tf_albert.py
src/transformers/models/albert/tokenization_albert.py
src/transformers/models/barthez/tokenization_barthez.py
src/transformers/models/bert/modeling_bert.py
src/transformers/models/bert/tokenization_bert.py
src/transformers/models/bert_generation/modeling_bert_generation.py
src/transformers/models/bert_generation/tokenization_bert_generation.py
src/transformers/models/bertweet/tokenization_bertweet.py
src/transformers/models/big_bird/modeling_big_bird.py
src/transformers/models/big_bird/tokenization_big_bird.py
src/transformers/models/blenderbot_small/tokenization_blenderbot_small.py
src/transformers/models/camembert/tokenization_camembert.py
src/transformers/models/convbert/modeling_convbert.py
src/transformers/models/ctrl/tokenization_ctrl.py
src/transformers/models/deberta_v2/tokenization_deberta_v2.py
src/transformers/models/deit/modeling_deit.py
src/transformers/models/distilbert/modeling_distilbert.py
src/transformers/models/distilbert/modeling_tf_distilbert.py
src/transformers/models/electra/modeling_electra.py
src/transformers/models/flaubert/modeling_tf_flaubert.py
src/transformers/models/fsmt/tokenization_fsmt.py
src/transformers/models/funnel/modeling_funnel.py
src/transformers/models/funnel/modeling_tf_funnel.py
src/transformers/models/gpt2/tokenization_gpt2.py
src/transformers/models/ibert/modeling_ibert.py
src/transformers/models/layoutlm/modeling_layoutlm.py
src/transformers/models/led/modeling_led.py
src/transformers/models/led/modeling_tf_led.py
src/transformers/models/longformer/modeling_longformer.py
src/transformers/models/longformer/modeling_tf_longformer.py
src/transformers/models/lxmert/modeling_lxmert.py
src/transformers/models/marian/tokenization_marian.py
src/transformers/models/mbart/tokenization_mbart50.py
src/transformers/models/megatron_bert/modeling_megatron_bert.py
src/transformers/models/mobilebert/modeling_mobilebert.py
src/transformers/models/mpnet/modeling_mpnet.py
src/transformers/models/mpnet/modeling_tf_mpnet.py
src/transformers/models/mpnet/tokenization_mpnet.py
src/transformers/models/openai/tokenization_openai.py
src/transformers/models/pegasus/tokenization_pegasus.py
src/transformers/models/phobert/tokenization_phobert.py
src/transformers/models/prophetnet/tokenization_prophetnet.py
src/transformers/models/reformer/modeling_reformer.py
src/transformers/models/reformer/tokenization_reformer.py
src/transformers/models/retribert/modeling_retribert.py
src/transformers/models/roberta/modeling_roberta.py
src/transformers/models/squeezebert/modeling_squeezebert.py
src/transformers/models/t5/modeling_t5.py
src/transformers/models/t5/modeling_tf_t5.py
src/transformers/models/t5/tokenization_t5.py
src/transformers/models/tapas/modeling_tapas.py
src/transformers/models/tapas/tokenization_tapas.py
src/transformers/models/transfo_xl/tokenization_transfo_xl.py
src/transformers/models/vit/modeling_vit.py
src/transformers/models/wav2vec2/modeling_wav2vec2.py
src/transformers/models/xlm/modeling_tf_xlm.py
src/transformers/models/xlm/modeling_xlm.py
src/transformers/models/xlm/tokenization_xlm.py
src/transformers/models/xlm_prophetnet/tokenization_xlm_prophetnet.py
src/transformers/models/xlm_roberta/tokenization_xlm_roberta.py
src/transformers/models/xlnet/tokenization_xlnet.py
src/transformers/testing_utils.py
src/transformers/tokenization_utils_base.py
src/transformers/trainer.py
src/transformers/trainer_callback.py
src/transformers/trainer_pt_utils.py
src/transformers/trainer_utils.py
src/transformers/utils/versions.py
tests/deepspeed/test_deepspeed.py
tests/test_modeling_common.py
tests/test_modeling_funnel.py
tests/test_modeling_layoutlm.py
tests/test_modeling_lxmert.py
tests/test_modeling_tapas.py
tests/test_modeling_tf_funnel.py
tests/test_tokenization_common.py
tests/test_tokenization_fsmt.py
tests/test_tokenization_layoutlm.py
tests/test_tokenization_xlm.py
utils/check_copies.py
utils/check_dummies.py
utils/check_repo.py
utils/check_table.py
utils/style_doc.py
==================
04ab2ca63;Vasudev Gupta;2021-04-26 12:35:53 +0530;add pooling layer support (#11439)

==

src/transformers/models/big_bird/modeling_big_bird.py
==================
30f065890;abiolaTresor;2021-04-26 06:58:51 +0200;updating the checkpoint for GPT2ForSequence Classification to one with classification head (#11434)

==

src/transformers/models/gpt2/modeling_gpt2.py
==================
35cd8eed8;cronoik;2021-04-25 11:45:46 +0200;EncoderDecoderConfigs should not create new objects (#11300)
* removes the creation of separate config objects and uses the existing ones instead+overwrite resize_token_embeddings from parent class because it is not working for the EncoderDecoderModel

* rollback to current version of the huggingface master branch

* reworked version that ties the encoder and decoder config of the parent encoderdecoder instance

* overwrite of resize_token_embeddings throws an error now

* review comment suggestion

Co-authored-by: Suraj Patil <surajp815@gmail.com>

* implemented warning in case encoderdecoder is created with differing configs of encoderdecoderconfig and decoderconfig or encoderconfig

* added test to avoid diverging configs of wrapper class and wrapped classes

* Update src/transformers/models/encoder_decoder/modeling_encoder_decoder.py

* make style

Co-authored-by: Suraj Patil <surajp815@gmail.com>
Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
==

src/transformers/models/encoder_decoder/modeling_encoder_decoder.py
tests/test_modeling_encoder_decoder.py
==================
f45cb66bf;Daniel Stancl;2021-04-25 11:06:16 +0200;Add head_mask, decoder_head_mask, cross_head_mask to ProphetNet (#9964)
* Add head_mask & decoder_head_mask + some corrections

* Fix head masking for N-grams

* Enable test_headmasking for encoder and decod

* Fix one typo regarding in modeling_propgetnet.py

* Enable test_headmasking for ProphetNetStandaloneDecoderModelTest
and ProphetNetStandaloneEncoderModelTest in test_modeling_prophetnet.py

* make style

* Fix cross_head_mask

* Fix attention head mask naming

* `cross_head_mask` -> `cross_attn_head_mask`

* `cross_layer_head_mask` -> `cross_attn_layer_head_mask`

* Still need to merge #10605 to master to pass the tests
==

src/transformers/models/prophetnet/modeling_prophetnet.py
tests/test_modeling_prophetnet.py
==================
52166f672;Sylvain Gugger;2021-04-23 20:40:17 -0400;Style

==

src/transformers/models/roberta/tokenization_roberta.py
==================
9cac4fab0;cronoik;2021-04-24 02:19:15 +0200;documentation linked to the parent class PreTrainedTokenizerFast but it should be the slow tokenizer (#11410)

==

src/transformers/models/roberta/tokenization_roberta.py
==================
b7fc043fc;Sylvain Gugger;2021-04-23 18:47:55 -0400;Merge branch 'master' of github.com:huggingface/transformers

==
==================
81a6c7cd3;Sylvain Gugger;2021-04-23 18:47:46 -0400;Use 3 workers for torch tests

==

.circleci/config.yml
==================
195bfd118;Philip May;2021-04-23 23:52:31 +0200;Enable option for subword regularization in `XLMRobertaTokenizer` (#11149)
* enable subword regularization.

* fix tokenizer storage

* fix docstring formatting

* Update src/transformers/models/xlm_roberta/tokenization_xlm_roberta.py

Co-authored-by: Stefan Schweter <stefan@schweter.it>

* fix docstring formatting

* add test for subword regularization tokenizer

* improve comments of test

* add sp_model_kwargs

* reformat docstring to match the style

* add some more documentation

* Update src/transformers/models/xlm_roberta/tokenization_xlm_roberta.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* improve docstring

* empty commit to trigger CI

* Update src/transformers/models/xlm_roberta/tokenization_xlm_roberta.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* fix docstring formatting for sphinx

Co-authored-by: Stefan Schweter <stefan@schweter.it>
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

src/transformers/models/xlm_roberta/tokenization_xlm_roberta.py
tests/test_tokenization_xlm_roberta.py
==================
1ef152eb4;Sylvain Gugger;2021-04-23 14:49:59 -0400;Default to accuracy metric (#11405)

==

examples/pytorch/text-classification/run_glue_no_trainer.py
==================
e3ff165aa;Daniel Stancl;2021-04-23 18:58:06 +0200;Fix cross-attention head mask for Torch encoder-decoder models (#10605)
* Fix cross-attention head mask for Torch BART models

* Fix head masking for cross-attention module for the following
models: BART, Blenderbot, Blenderbot_small, M2M_100, Marian, MBart,
Pegasus

* Enable test_headmasking for M2M_100 model

* Fix cross_head_mask for FSMT, LED and T5

* This commit fixes `head_mask` for cross-attention modules
in the following models: FSMT, LED, T5

* It also contains some smaller changes in doc so that
it is be perfectly clear the shape of `cross_head_mask`
is the same as of `decoder_head_mask`

* Update template

* Fix template for BartForCausalLM

* Fix cross_head_mask for Speech2Text models

* Fix cross_head_mask in templates

* Fix args order in BartForCausalLM template

* Fix doc in BART templates

* Make more explicit naming

* `cross_head_mask` -> `cross_attn_head_mask`

* `cross_layer_head_mask` -> `cross_attn_layer_head_mask`

* Fix doc

* make style quality

* Fix speech2text docstring
==

src/transformers/models/bart/modeling_bart.py
src/transformers/models/blenderbot/modeling_blenderbot.py
src/transformers/models/blenderbot_small/modeling_blenderbot_small.py
src/transformers/models/fsmt/modeling_fsmt.py
src/transformers/models/led/modeling_led.py
src/transformers/models/m2m_100/modeling_m2m_100.py
src/transformers/models/marian/modeling_marian.py
src/transformers/models/mbart/modeling_mbart.py
src/transformers/models/pegasus/modeling_pegasus.py
src/transformers/models/speech_to_text/modeling_speech_to_text.py
src/transformers/models/t5/modeling_t5.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_{{cookiecutter.lowercase_modelname}}.py
tests/test_modeling_bart.py
tests/test_modeling_blenderbot.py
tests/test_modeling_blenderbot_small.py
tests/test_modeling_common.py
tests/test_modeling_fsmt.py
tests/test_modeling_led.py
tests/test_modeling_m2m_100.py
tests/test_modeling_marian.py
tests/test_modeling_mbart.py
tests/test_modeling_pegasus.py
tests/test_modeling_speech_to_text.py
==================
ca6b80cad;Sylvain Gugger;2021-04-23 12:46:54 -0400;Wrong branch Sylvain...

==

.circleci/config.yml
==================
3951fc55e;Sylvain Gugger;2021-04-23 12:44:54 -0400;Try to trigger failure more

==

.circleci/config.yml
==================
bd41a0f74;Sylvain Gugger;2021-04-23 12:32:37 -0400;Style

==

src/transformers/models/t5/modeling_t5.py
==================
1811883e8;Nicola De Cao;2021-04-23 17:24:26 +0100;Fixing bug in generation (#11297)
When passing `inputs_embeds` and not `input_ids=None` the generation function fails because `input_ids` is created but the function but it should not.
==

src/transformers/generation_utils.py
==================
5c0091868;Kiran R;2021-04-23 21:44:20 +0530;added support for exporting of t5 to onnx with past_key_values (#10651)

==

src/transformers/models/t5/modeling_t5.py
==================
50f4539b8;Patrick von Platen;2021-04-23 15:36:27 +0200;push (#11400)

==

src/transformers/models/wav2vec2/convert_wav2vec2_original_pytorch_checkpoint_to_pytorch.py
==================
bf2e0cf70;Sylvain Gugger;2021-04-23 09:17:37 -0400;Trainer push to hub (#11328)
* Initial support for upload to hub

* push -> upload

* Fixes + examples

* Fix torchhub test

* Torchhub test I hate you

* push_model_to_hub -> push_to_hub

* Apply mixin to other pretrained models

* Remove ABC inheritance

* Add tests

* Typo

* Run tests

* Install git-lfs

* Change approach

* Add push_to_hub to all

* Staging test suite

* Typo

* Maybe like this?

* More deps

* Cache

* Adapt name

* Quality

* MOAR tests

* Put it in testing_utils

* Docs + torchhub last hope

* Styling

* Wrong method

* Typos

* Update src/transformers/file_utils.py

Co-authored-by: Julien Chaumond <julien@huggingface.co>

* Address review comments

* Apply suggestions from code review

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

Co-authored-by: Julien Chaumond <julien@huggingface.co>
Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
==

.circleci/config.yml
docs/source/main_classes/model.rst
docs/source/model_sharing.rst
examples/legacy/text-classification/run_tf_text_classification.py
examples/pytorch/language-modeling/run_clm.py
examples/pytorch/language-modeling/run_mlm.py
examples/pytorch/language-modeling/run_plm.py
examples/pytorch/multiple-choice/run_swag.py
examples/pytorch/question-answering/run_qa.py
examples/pytorch/question-answering/run_qa_beam_search.py
examples/pytorch/summarization/run_summarization.py
examples/pytorch/text-classification/run_glue.py
examples/pytorch/token-classification/run_ner.py
examples/pytorch/translation/run_translation.py
hubconf.py
src/transformers/configuration_utils.py
src/transformers/file_utils.py
src/transformers/modeling_flax_utils.py
src/transformers/modeling_tf_utils.py
src/transformers/modeling_utils.py
src/transformers/testing_utils.py
src/transformers/tokenization_utils_base.py
src/transformers/trainer.py
src/transformers/training_args.py
tests/conftest.py
tests/test_configuration_common.py
tests/test_hf_api.py
tests/test_modeling_common.py
tests/test_modeling_tf_common.py
tests/test_tokenization_common.py
tests/test_trainer.py
==================
7bc86bea6;Teven;2021-04-23 13:53:33 +0200;Fixed trainer total_flos relaoding in distributed mode (#11383)
* Fixed trainer total_flos relaoding in distributed mode

* logging flos at the end of training
==

src/transformers/trainer.py
==================
74e84f1fa;Patrick von Platen;2021-04-23 13:49:09 +0200;make blenderbot test slow (#11395)

==

tests/test_modeling_blenderbot_small.py
==================
c3d6f3391;Yoshitomo Matsubara;2021-04-23 04:48:42 -0700;fixed typos (#11391)

==

examples/pytorch/text-classification/README.md
==================
a90d3f186;Max Del;2021-04-23 14:37:19 +0300;Fix typo in text (#11396)

==

examples/pytorch/translation/README.md
==================
2dc2d79ac;Patrick von Platen;2021-04-23 11:59:34 +0200;correct conversion (#11394)

==

src/transformers/modeling_flax_pytorch_utils.py
==================
b48cf7124;Patrick von Platen;2021-04-23 11:34:59 +0200;correct typo (#11393)

==

examples/flax/language-modeling/run_mlm_flax.py
==================
8c9b5fcba;Patrick von Platen;2021-04-23 09:53:09 +0200;[Flax] Big FlaxBert Refactor (#11364)
* improve flax

* refactor

* typos

* Update src/transformers/modeling_flax_utils.py

* Apply suggestions from code review

* Update src/transformers/modeling_flax_utils.py

* fix typo

* improve error tolerance

* typo

* correct nasty saving bug

* fix from pretrained

* correct tree map

* add note

* correct weight tying
==

src/transformers/modeling_flax_pytorch_utils.py
src/transformers/modeling_flax_utils.py
src/transformers/modeling_utils.py
src/transformers/models/bert/modeling_flax_bert.py
src/transformers/models/roberta/modeling_flax_roberta.py
tests/test_modeling_flax_common.py
==================
3ed5e97ba;Sylvain Gugger;2021-04-22 11:16:24 -0400;Fix Trainer with remove_unused_columns=False (#11382)
* Fix Trainer with remove_unused_columns=False

* Typo
==

src/transformers/trainer.py
==================
0f3ad1507;PenutChen;2021-04-22 22:10:16 +0800;Fix typo (#11369)

==

src/transformers/generation_utils.py
==================
261739609;Matt;2021-04-22 13:49:59 +0100;Correctly cast num_train_epochs to int (#11379)

==

examples/tensorflow/text-classification/run_text_classification.py
==================
881945c0b;Takuya Makino;2021-04-22 21:18:58 +0900;Add space (#11373)

==

src/transformers/tokenization_utils_base.py
==================
5b5e4ca36;johnson7788;2021-04-22 20:17:11 +0800;[run_translation.py] fix typo  (#11372)
fix typo

Co-authored-by: johnson <johnson@github.com>
==

examples/pytorch/translation/run_translation.py
==================
58d8795d7;Patrick von Platen;2021-04-22 13:11:44 +0200;[Flax] Correct typo (#11374)
* finish

* fix copy
==

src/transformers/models/bert/modeling_flax_bert.py
src/transformers/models/roberta/modeling_flax_roberta.py
==================
880154d2e;Patrick von Platen;2021-04-22 12:23:08 +0200;[Wav2Vec2] Fix special tokens for Wav2Vec2 tokenizer (#11349)
* fix wav2vec2 tok

* up
==

src/transformers/models/wav2vec2/tokenization_wav2vec2.py
tests/test_tokenization_wav2vec2.py
==================
6f14eab50;Sylvain Gugger;2021-04-21 19:17:29 -0400;Add in torchhub

==

setup.py
==================
ff26f8ee3;Sylvain Gugger;2021-04-21 19:12:58 -0400;Add huggingface_hub dep for #11328

==

setup.py
src/transformers/dependency_versions_table.py
==================
5e04d7086;wlhgtc;2021-04-22 01:37:57 +0800;Fix token_type_ids error for big_bird model. (#11355)
* MOD: fit chinese wwm to new datasets

* MOD: move wwm to new folder

* MOD: formate code

* Styling

* MOD add param and recover trainer

* MOD: add token_type_ids method for big bird

* MOD: format code

* MOD: format code

Co-authored-by: Sylvain Gugger <sylvain.gugger@gmail.com>
==

src/transformers/models/big_bird/tokenization_big_bird.py
==================
5aaf5aac0;Stas Bekman;2021-04-21 10:10:11 -0700;[contributing doc] explain/link to good first issue (#11346)
* explain/link to good first issue

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

CONTRIBUTING.md
==================
6fe79e57d;Matt;2021-04-21 17:36:18 +0100;Move old TF text classification script to legacy (#11361)
And update README to explain the work-in-progress!
==

examples/legacy/text-classification/run_tf_text_classification.py
examples/tensorflow/README.md
==================
50595a333;Patrick von Platen;2021-04-21 18:34:38 +0200;Remove boiler plate code (#11340)
* remove boiler plate code

* adapt roberta

* correct docs

* finish refactor
==

src/transformers/file_utils.py
src/transformers/modeling_flax_utils.py
src/transformers/models/auto/auto_factory.py
src/transformers/models/bert/modeling_flax_bert.py
src/transformers/models/roberta/modeling_flax_roberta.py
==================
ac588594e;Matt;2021-04-21 17:04:55 +0100;Merge new TF example script (#11360)
First of the new and more idiomatic TF examples!
==

examples/tensorflow/README.md
examples/tensorflow/text-classification/README.md
examples/tensorflow/text-classification/requirements.txt
examples/tensorflow/text-classification/run_text_classification.py
==================
9f72e8f4e;Stas Bekman;2021-04-21 08:51:00 -0700;[testing doc] bring doc up to date (#11359)
* bring doc up to date

* fix
==

docs/source/testing.rst
==================
41f3133a3;lewtun;2021-04-21 17:12:09 +0200;Extract metric_key_prefix during NotebookProgressCallback.on_evaluate (#11347)
* Pass metric_key_prefix as kwarg to on_evaluate

* Replace eval_loss with metric_key_prefix_loss

* Default to "eval" if metric_key_prefix not in kwargs

* Add kwargs to CallbackHandler.on_evaluate signature

* Revert "Add kwargs to CallbackHandler.on_evaluate signature"

This reverts commit 8d4c85ed512f558f7579d36771e907b3379947b7.

* Revert "Pass metric_key_prefix as kwarg to on_evaluate"

This reverts commit 7766bfe2718601230ae593d37b1317bd53cfc075.

* Extract metric_key_prefix from metrics
==

src/transformers/utils/notebook.py
==================
dabeb1529;Sylvain Gugger;2021-04-21 11:11:20 -0400;Examples reorg (#11350)
* Base move

* Examples reorganization

* Update references

* Put back test data

* Move conftest

* More fixes

* Move test data to test fixtures

* Update path

* Apply suggestions from code review

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* Address review comments and clean

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
==

.circleci/config.yml
.github/workflows/self-scheduled.yml
CONTRIBUTING.md
Makefile
docker/transformers-pytorch-tpu/Dockerfile
docker/transformers-pytorch-tpu/bert-base-cased.jsonnet
docs/source/benchmarks.rst
docs/source/converting_tensorflow_models.rst
docs/source/installation.md
docs/source/main_classes/processors.rst
docs/source/main_classes/trainer.rst
docs/source/model_doc/bart.rst
docs/source/model_doc/barthez.rst
docs/source/model_doc/distilbert.rst
docs/source/model_doc/pegasus.rst
docs/source/model_doc/retribert.rst
docs/source/model_doc/xlnet.rst
docs/source/model_summary.rst
docs/source/multilingual.rst
docs/source/sagemaker.md
docs/source/task_summary.rst
examples/README.md
examples/benchmarking/requirements.txt
examples/flax/language-modeling/run_mlm_flax.py
examples/pytorch/README.md
examples/pytorch/_tests_requirements.txt
examples/pytorch/benchmarking/README.md
examples/pytorch/benchmarking/plot_csv_file.py
examples/pytorch/benchmarking/requirements.txt
examples/pytorch/benchmarking/run_benchmark.py
examples/pytorch/conftest.py
examples/pytorch/language-modeling/README.md
examples/pytorch/language-modeling/requirements.txt
examples/pytorch/language-modeling/run_clm.py
examples/pytorch/language-modeling/run_clm_no_trainer.py
examples/pytorch/language-modeling/run_mlm.py
examples/pytorch/language-modeling/run_mlm_no_trainer.py
examples/pytorch/language-modeling/run_plm.py
examples/pytorch/multiple-choice/README.md
examples/pytorch/multiple-choice/requirements.txt
examples/pytorch/multiple-choice/run_no_trainer.sh
examples/pytorch/multiple-choice/run_swag.py
examples/pytorch/multiple-choice/run_swag_no_trainer.py
examples/pytorch/question-answering/README.md
examples/pytorch/question-answering/requirements.txt
examples/pytorch/question-answering/run_qa.py
examples/pytorch/question-answering/run_qa_beam_search.py
examples/pytorch/question-answering/run_qa_beam_search_no_trainer.py
examples/pytorch/question-answering/run_qa_no_trainer.py
examples/pytorch/question-answering/trainer_qa.py
examples/pytorch/question-answering/utils_qa.py
examples/pytorch/summarization/README.md
examples/pytorch/summarization/requirements.txt
examples/pytorch/summarization/run_summarization.py
examples/pytorch/summarization/run_summarization_no_trainer.py
examples/pytorch/test_examples.py
examples/pytorch/test_xla_examples.py
examples/pytorch/text-classification/README.md
examples/pytorch/text-classification/requirements.txt
examples/pytorch/text-classification/run_glue.py
examples/pytorch/text-classification/run_glue_no_trainer.py
examples/pytorch/text-classification/run_xnli.py
examples/pytorch/text-generation/README.md
examples/pytorch/text-generation/requirements.txt
examples/pytorch/text-generation/run_generation.py
examples/pytorch/token-classification/README.md
examples/pytorch/token-classification/requirements.txt
examples/pytorch/token-classification/run.sh
examples/pytorch/token-classification/run_ner.py
examples/pytorch/token-classification/run_ner_no_trainer.py
examples/pytorch/token-classification/run_no_trainer.sh
examples/pytorch/translation/README.md
examples/pytorch/translation/requirements.txt
examples/pytorch/translation/run_translation.py
examples/pytorch/translation/run_translation_no_trainer.py
examples/pytorch/xla_spawn.py
examples/tensorflow/README.md
examples/tensorflow/benchmarking/README.md
examples/tensorflow/benchmarking/plot_csv_file.py
examples/tensorflow/benchmarking/requirements.txt
examples/tensorflow/benchmarking/run_benchmark_tf.py
examples/tensorflow/multiple-choice/README.md
examples/tensorflow/multiple-choice/requirements.txt
examples/tensorflow/multiple-choice/run_tf_multiple_choice.py
examples/tensorflow/multiple-choice/utils_multiple_choice.py
examples/tensorflow/question-answering/README.md
examples/tensorflow/question-answering/requirements.txt
examples/tensorflow/question-answering/run_tf_squad.py
examples/tensorflow/text-classification/README.md
examples/tensorflow/text-classification/requirements.txt
examples/tensorflow/text-classification/run_tf_glue.py
examples/tensorflow/text-classification/run_tf_text_classification.py
src/transformers/data/datasets/glue.py
src/transformers/data/datasets/language_modeling.py
src/transformers/data/metrics/__init__.py
src/transformers/data/processors/glue.py
tests/deepspeed/test_deepspeed.py
tests/extended/test_trainer_ext.py
tests/fixtures/tests_samples/wmt_en_ro/test.json
tests/fixtures/tests_samples/wmt_en_ro/train.json
tests/fixtures/tests_samples/wmt_en_ro/val.json
tests/sagemaker/test_multi_node_data_parallel.py
tests/sagemaker/test_multi_node_model_parallel.py
tests/sagemaker/test_single_node_gpu.py
tests/test_trainer_tpu.py
==================
ca7ff64f5;Stas Bekman;2021-04-21 07:48:15 -0700;[deepspeed] fix resume from checkpoint (#11352)
This PR fixes a bug that most likely somehow got exposed (not caused) by https://github.com/huggingface/transformers/pull/11318 - surprisingly the same test worked just fine before that other PR.

==

src/transformers/trainer.py
==================
74712e22f;Sylvain Gugger;2021-04-21 09:47:27 -0400;Honor contributors to models (#11329)
* Honor contributors to models

* Fix typo

* Address review comments

* Add more authors
==

docs/source/model_doc/albert.rst
docs/source/model_doc/bart.rst
docs/source/model_doc/barthez.rst
docs/source/model_doc/bert.rst
docs/source/model_doc/bert_japanese.rst
docs/source/model_doc/bertgeneration.rst
docs/source/model_doc/bertweet.rst
docs/source/model_doc/bigbird.rst
docs/source/model_doc/blenderbot.rst
docs/source/model_doc/blenderbot_small.rst
docs/source/model_doc/bort.rst
docs/source/model_doc/camembert.rst
docs/source/model_doc/convbert.rst
docs/source/model_doc/cpm.rst
docs/source/model_doc/ctrl.rst
docs/source/model_doc/deberta.rst
docs/source/model_doc/deberta_v2.rst
docs/source/model_doc/deit.rst
docs/source/model_doc/distilbert.rst
docs/source/model_doc/dpr.rst
docs/source/model_doc/electra.rst
docs/source/model_doc/flaubert.rst
docs/source/model_doc/fsmt.rst
docs/source/model_doc/funnel.rst
docs/source/model_doc/gpt.rst
docs/source/model_doc/gpt2.rst
docs/source/model_doc/gpt_neo.rst
docs/source/model_doc/herbert.rst
docs/source/model_doc/ibert.rst
docs/source/model_doc/layoutlm.rst
docs/source/model_doc/led.rst
docs/source/model_doc/longformer.rst
docs/source/model_doc/lxmert.rst
docs/source/model_doc/m2m_100.rst
docs/source/model_doc/marian.rst
docs/source/model_doc/mbart.rst
docs/source/model_doc/megatron_bert.rst
docs/source/model_doc/megatron_gpt2.rst
docs/source/model_doc/mobilebert.rst
docs/source/model_doc/mt5.rst
docs/source/model_doc/pegasus.rst
docs/source/model_doc/phobert.rst
docs/source/model_doc/rag.rst
docs/source/model_doc/reformer.rst
docs/source/model_doc/retribert.rst
docs/source/model_doc/roberta.rst
docs/source/model_doc/speech_to_text.rst
docs/source/model_doc/squeezebert.rst
docs/source/model_doc/t5.rst
docs/source/model_doc/tapas.rst
docs/source/model_doc/transformerxl.rst
docs/source/model_doc/vit.rst
docs/source/model_doc/wav2vec2.rst
docs/source/model_doc/xlm.rst
docs/source/model_doc/xlmroberta.rst
docs/source/model_doc/xlnet.rst
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/{{cookiecutter.lowercase_modelname}}.rst
==================
aad95c7cd;Nicolas Patry;2021-04-21 11:56:45 +0200;Removed `max_length` from being mandatory within `generate`. (#11314)
* Removed `max_length` from being mandatory within `generate`.

- Moving on to fully using `StoppingCriteria` for `greedy` and `sample`
modes.
- `max_length` still used for `beam_search` and `group_beam_search`
(Follow up PR)
- Fixes a bug with MaxLengthStoppingCriteria (we should stop as soon a
we hit the max_length, the comparison needs to be or equal, that affects
the tests).
- Added options to use `logits_processor` and `stopping_criteria`
directly within `generate` function (so some users can define their own
`logits_processor` and `stopping_criteria`).
- Modified the backward compat tests to make sure we issue a warning.

* Fix `max_length` argument in `generate`.

* Moving validate to being functional.

- Renamed `smax_length` to `stoppping_max_length`.

* Removing `logits_processor` and `stopping_criteria` from `generate`
arguments.

* Deepcopy.

* Fix global variable name.
==

src/transformers/generation_stopping_criteria.py
src/transformers/generation_utils.py
tests/test_generation_stopping_criteria.py
tests/test_generation_utils.py
==================
95dab34d5;Yusuke Mori;2021-04-21 07:23:37 +0900;Add an error message that fires when Reformer is not in training mode, but one runs .backward() (#11117)

==

src/transformers/models/reformer/modeling_reformer.py
==================
f1b938fda;Sylvain Gugger;2021-04-20 14:12:01 -0400;Update to use datasets remove_cloumns method (#11343)
* Update to use datasets remove_cloumns method

* Quality
==

examples/question-answering/requirements.txt
examples/question-answering/trainer_qa.py
src/transformers/trainer.py
==================
cfd2eaa8c;Suraj Patil;2021-04-20 18:37:44 +0530;[GPTNeo] create local attention mask ones (#11335)
* create local attention mask ones

* remove old method, address patricks comment
==

src/transformers/models/gpt_neo/modeling_gpt_neo.py
tests/test_modeling_gpt_neo.py
==================
f464f10a2;Patrick von Platen;2021-04-20 15:16:02 +0300;[Generate] Remove outdated code (#11331)
* remove update function

* update

* refactor more

* refactor
==

src/transformers/generation_utils.py
==================
bfd83c17a;rajvi-k;2021-04-20 07:18:47 -0400;Added translation example script  (#11196)
* initial changes

* modified evaluation

* updated evaluation

* updated evaluation on text translation example script

* added translation example script

* Formatted translation example script

* Reformatted translation example

* Fixed evaluation bug and added support for other tokenisers

* Fixed evaluation bug and added support for other tokenisers

* Added translation example script

* Formatted summarization example script

* Removed typos from summarization example script
==

examples/seq2seq/run_summarization_no_trainer.py
examples/seq2seq/run_translation_no_trainer.py
==================
c0328a6c2;Sylvain Gugger;2021-04-19 20:31:29 -0400;Load checkpoint without re-creating the model (#11318)

==

src/transformers/configuration_utils.py
src/transformers/trainer.py
tests/test_trainer.py
==================
95037a169;Sylvain Gugger;2021-04-19 19:04:52 -0400;[Trainer] Add a progress bar for batches skipped (#11324)

==

src/transformers/trainer.py
==================
95ffbe168;Stas Bekman;2021-04-19 11:55:33 -0700;[Trainer] fix the placement on device with fp16_full_eval (#11322)
* fix the placement on device with fp16_full_eval

* deepspeed never goes on device
==

src/transformers/trainer.py
==================
3981ce3dd;TAE YOUNGDON;2021-04-20 00:24:43 +0900;modify double considering special tokens in `language_modeling.py` (#11275)
* Update language_modeling.py

in "class TextDatasetForNextSentencePrediction(Dataset)", double considering "self.tokenizer.num_special_tokens_to_add(pair=True)" 

so, i remove self.block_size, and add parameter for "def create_examples_from_document". like "class LineByLineWithSOPTextDataset" do

* Update language_modeling.py
==

src/transformers/data/datasets/language_modeling.py
==================
5a34d8d98;e;2021-04-19 20:25:40 +0800;move device statements outside if statements (#11292)

==

src/transformers/models/ctrl/modeling_ctrl.py
src/transformers/models/gpt2/modeling_gpt2.py
src/transformers/models/gpt_neo/modeling_gpt_neo.py
==================
d9c62047a;Sylvain Gugger;2021-04-16 16:01:58 -0400;Trainer support for IterableDataset for evaluation and predict (#11286)
* Bulk of the work

* Polish and tests

* Update QA Trainer

* Avoid breaking the predict method

* Deprecation warnings

* Store real eval dataloder

* Get eval dataset reference before wrap
==

src/transformers/trainer.py
src/transformers/trainer_callback.py
src/transformers/trainer_pt_utils.py
src/transformers/trainer_utils.py
src/transformers/training_args.py
src/transformers/utils/notebook.py
tests/test_trainer.py
tests/test_trainer_utils.py
==================
e783ea730;Lysandre;2021-04-16 08:09:51 -0400;Fix failing workflows

==

.github/workflows/self-push.yml
.github/workflows/self-scheduled.yml
==================
92970c0cb;Nicolas Patry;2021-04-16 11:31:35 +0200;Enabling multilingual models for translation pipelines. (#10536)
* [WIP] Enabling multilingual models for translation pipelines.

* decoder_input_ids -> forced_bos_token_id

* Improve docstring.

* Rebase

* Fixing 2 bugs

- Type token_ids coming from `_parse_and_tokenize`
- Wrong index from tgt_lang.

* Fixing black version.

* Adding tests for _build_translation_inputs and add them for all
tokenizers.

* Mbart actually puts the lang code at the end.

* Fixing m2m100.

* Adding TF support to `deep_round`.

* Update src/transformers/pipelines/text2text_generation.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Adding one line comment.

* Fixing M2M100 `_build_translation_input_ids`, and fix the call site.

* Fixing tests + deep_round -> nested_simplify

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

src/transformers/models/m2m_100/tokenization_m2m_100.py
src/transformers/models/mbart/tokenization_mbart.py
src/transformers/models/mbart/tokenization_mbart50.py
src/transformers/models/mbart/tokenization_mbart50_fast.py
src/transformers/models/mbart/tokenization_mbart_fast.py
src/transformers/pipelines/base.py
src/transformers/pipelines/text2text_generation.py
src/transformers/testing_utils.py
tests/test_pipelines_translation.py
tests/test_tokenization_m2m_100.py
tests/test_tokenization_mbart.py
tests/test_tokenization_mbart50.py
==================
5254220e7;Lysandre Debut;2021-04-15 23:21:17 -0400;Workflow fixes (#11270)

==

.github/workflows/self-push.yml
.github/workflows/self-scheduled.yml
utils/notification_service.py
==================
dfc6dd858;Stas Bekman;2021-04-15 19:10:29 -0700;update dependency_versions_table (#11273)
missed this updating when bumped the version.
==

src/transformers/dependency_versions_table.py
==================
2550b41aa;Sylvain Gugger;2021-04-15 09:32:32 -0400;Tokenizer fast save (#11234)
* Save fast tokenizers in both formats

* Fix for HerBERT

* Proper fix

* Properly test new behavior
==

src/transformers/models/herbert/tokenization_herbert.py
src/transformers/tokenization_utils_base.py
src/transformers/tokenization_utils_fast.py
tests/test_tokenization_common.py
==================
6e1ee47b3;Sylvain Gugger;2021-04-15 07:36:32 -0400;Support for set_epoch (#11258)

==

src/transformers/trainer.py
src/transformers/trainer_pt_utils.py
==================
c3fcba321;Nicolas Patry;2021-04-15 09:51:24 +0200;Adding pipeline task aliases. (#11247)
* Adding task aliases and adding `token-classification` and
`text-classification` tasks.

* Cleaning docstring.
==

src/transformers/commands/run.py
src/transformers/commands/serving.py
src/transformers/pipelines/__init__.py
tests/test_pipelines_text_classification.py
tests/test_pipelines_token_classification.py
==================
aaaed56ff;Sylvain Gugger;2021-04-14 17:02:26 -0400;Trainer iterable dataset (#11254)
* IterableDatasetShard

* Test and integration in Trainer

* Update src/transformers/trainer_pt_utils.py

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* Style

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
==

src/transformers/trainer.py
src/transformers/trainer_pt_utils.py
tests/test_trainer.py
tests/test_trainer_utils.py
==================
83206ca6a;Stas Bekman;2021-04-14 11:06:59 -0700; [deepspeed] test on one node 2 gpus max (#11237)
* test on one node 2 gpus max

* fix the other place

* refactor

* fix

* cleanup

* more exact version
==

setup.py
tests/deepspeed/test_deepspeed.py
==================
25e1af36e;Sylvain Gugger;2021-04-14 11:47:54 -0400;Fix #10128 (#11248)

==

src/transformers/trainer_pt_utils.py
==================
63ca40238;Stas Bekman;2021-04-14 08:39:23 -0700;[troubleshooting] add 2 points of reference to the offline mode (#11236)
* add 2 points of reference to the offline mode

* link the new doc

* add error message

* Update src/transformers/modeling_utils.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* style

* rename

* Trigger CI

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

docs/source/index.rst
docs/source/troubleshooting.md
src/transformers/modeling_utils.py
==================
075e821d1;Yusuke Mori;2021-04-14 23:58:55 +0900;Add prefix to examples in model_doc rst (#11226)
* Add prefix to examples in model_doc rst

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

docs/source/model_doc/bertgeneration.rst
docs/source/model_doc/bertweet.rst
docs/source/model_doc/herbert.rst
docs/source/model_doc/phobert.rst
==================
4670b57ce;Thomas Wood;2021-04-14 07:39:37 -0700;Fix dimention misspellings. (#11238)
* Update modeling_gpt_neo.py

dimention -> dimension

* Update configuration_speech_to_text.py

dimention -> dimension
==

src/transformers/models/gpt_neo/modeling_gpt_neo.py
src/transformers/models/speech_to_text/configuration_speech_to_text.py
==================
f25444cb2;Sudharsan S T;2021-04-14 20:01:04 +0530;Close open files to suppress ResourceWarning (#11240)
Co-authored-by: Sudharsan Thirumalai <sudharsan.t@sprinklr.com>
==

examples/legacy/seq2seq/run_distributed_eval.py
examples/research_projects/seq2seq-distillation/run_eval.py
src/transformers/convert_slow_tokenizer.py
==================
7fe5aaa8b;Lysandre Debut;2021-04-14 10:24:31 -0400;Stale bot updated (#10562)
* Updated stale bot

* Specify issue number

* Remove particular handling of assignees

* Unleash the stalebot

* Remove debug branch
==

.github/workflows/stale.yml
scripts/stale.py
==================
9337c6c66;Joel Stremmel;2021-04-14 09:13:25 -0500;make embeddings plural in warning message (#11228)

==

src/transformers/tokenization_utils_base.py
==================
653076ca3;Nithin Holla;2021-04-14 13:52:06 +0200;Save the Wav2Vec2 processor before training starts (#10910)
Co-authored-by: nithin19 <nithin@amberscript.com>
==

examples/research_projects/wav2vec2/run_common_voice.py
==================
3d339ee65;Stas Bekman;2021-04-13 14:58:09 -0700;[Deepspeed] zero3 tests band aid (#11235)
* temp band-aid

* style
==

tests/deepspeed/test_deepspeed.py
==================
1ad7b0398;Lysandre Debut;2021-04-13 15:47:06 -0400;Run CI on deepspeed and fairscale (#11172)
* Run CI on deepspeed and fairscale

* Test it on this branch :)

* Rename

* Update the CI image
==

.github/workflows/self-push.yml
.github/workflows/self-scheduled.yml
==================
f38cd4373;Sylvain Gugger;2021-04-13 15:36:36 -0400;Indent code block in the documentation (#11233)
* Indent code block

* Indent code blocks version 2

* Quality
==

docs/source/add_new_model.rst
docs/source/converting_tensorflow_models.rst
docs/source/glossary.rst
docs/source/main_classes/trainer.rst
docs/source/model_doc/bert_japanese.rst
docs/source/model_doc/bertgeneration.rst
docs/source/model_doc/bertweet.rst
docs/source/model_doc/herbert.rst
docs/source/model_doc/layoutlm.rst
docs/source/model_doc/megatron_bert.rst
docs/source/model_doc/megatron_gpt2.rst
docs/source/model_doc/phobert.rst
docs/source/model_doc/reformer.rst
docs/source/model_doc/t5.rst
docs/source/testing.rst
utils/style_doc.py
==================
9d8e8a870;Sylvain Gugger;2021-04-13 15:34:00 -0400;Avoid using no_sync on SageMaker DP (#11229)

==

src/transformers/training_args.py
==================
9fa299599;Philipp Schmid;2021-04-13 18:35:18 +0200;added cache_dir=model_args.cache_dir to all example with cache_dir arg (#11220)

==

examples/language-modeling/run_clm.py
examples/language-modeling/run_mlm.py
examples/language-modeling/run_mlm_flax.py
examples/language-modeling/run_plm.py
examples/multiple-choice/run_swag.py
examples/question-answering/run_qa.py
examples/question-answering/run_qa_beam_search.py
examples/seq2seq/run_summarization.py
examples/seq2seq/run_translation.py
examples/text-classification/run_glue.py
examples/text-classification/run_xnli.py
examples/token-classification/run_ner.py
==================
3312e96bf;Sylvain Gugger;2021-04-13 12:14:25 -0400;Doc check: a bit of clean up (#11224)

==

docs/source/main_classes/data_collator.rst
utils/check_repo.py
==================
edca520d0;Suraj Patil;2021-04-13 21:15:24 +0530;Refactor GPT2 (#11225)
* refactor GPT2

* fix mlp and head pruning

* address Sylvains comments

* apply suggestion from code review

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
==

src/transformers/models/gpt2/configuration_gpt2.py
src/transformers/models/gpt2/modeling_gpt2.py
==================
893e51a53;Sylvain Gugger;2021-04-13 11:28:17 -0400;Document v4.5.1

==

.circleci/deploy.sh
docs/source/_static/js/custom.js
==================
81009b7a5;Sylvain Gugger;2021-04-13 10:33:52 -0400;Replace error by warning when loading an architecture in another (#11207)
* Replace error by warning when loading an architecture in another

* Style

* Style again

* Add a test

* Adapt old test
==

src/transformers/configuration_utils.py
tests/test_modeling_bert_generation.py
tests/test_modeling_common.py
==================
22fa0a600;Yusuke Mori;2021-04-13 22:49:15 +0900;Add documentation for BertJapanese (#11219)
* Start writing BERT-Japanese doc

* Fix typo, Update toctree

* Modify model file to use comment for document, Add examples

* Clean bert_japanese by make style

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Split a big code block into two

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Add prefix >>> to all lines in code blocks

* Clean bert_japanese by make fixup

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

docs/source/index.rst
docs/source/model_doc/bert_japanese.rst
src/transformers/models/bert_japanese/tokenization_bert_japanese.py
==================
896d7be97;Suraj Patil;2021-04-13 18:28:08 +0530;fix docstrings (#11221)

==

src/transformers/models/blenderbot/modeling_blenderbot.py
src/transformers/models/blenderbot_small/modeling_blenderbot_small.py
src/transformers/models/fsmt/modeling_fsmt.py
src/transformers/models/m2m_100/modeling_m2m_100.py
src/transformers/models/marian/modeling_marian.py
src/transformers/models/mbart/modeling_mbart.py
src/transformers/models/pegasus/modeling_pegasus.py
src/transformers/models/pegasus/modeling_tf_pegasus.py
src/transformers/models/prophetnet/modeling_prophetnet.py
src/transformers/models/speech_to_text/modeling_speech_to_text.py
src/transformers/models/t5/modeling_t5.py
src/transformers/models/t5/modeling_tf_t5.py
==================
823df9395;Lysandre Debut;2021-04-13 08:53:03 -0400;Fix GPT-2 warnings (#11213)
* Fix GPT-2 warnings

* Update src/transformers/models/gpt2/modeling_gpt2.py

Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>

Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>
==

src/transformers/modeling_tf_pytorch_utils.py
src/transformers/models/gpt2/modeling_gpt2.py
==================
0cd89d8c8;Lysandre Debut;2021-04-13 08:52:30 -0400;Add Matt as the TensorFlow reference (#11212)

==

.github/ISSUE_TEMPLATE/bug-report.md
==================
7c205bf40;Ceyda Cinarel;2021-04-13 19:24:33 +0900;wav2vec2 converter: create the proper vocab.json while converting fairseq wav2vec2 finetuned model (#11041)
* add vocab while converting wav2vec2 original finetuned model

* check save directory exists

* return_attention_mask fix

* quality
==

src/transformers/models/wav2vec2/convert_wav2vec2_original_pytorch_checkpoint_to_pytorch.py
==================
d49d3cf6d;calpt;2021-04-13 11:54:46 +0200;Use MSELoss in (M)BartForSequenceClassification (#11178)

==

src/transformers/models/bart/modeling_bart.py
src/transformers/models/mbart/modeling_mbart.py
==================
f243a5ec0;Philipp Schmid;2021-04-13 01:08:33 +0200;Sagemaker test docs update for framework upgrade (#11206)
* increased train_runtime for model parallelism

* added documentation for framework upgrade
==

tests/sagemaker/README.md
tests/sagemaker/test_multi_node_model_parallel.py
==================
74d7c24d8;Lysandre Debut;2021-04-12 18:56:17 -0400;Import torch.utils.checkpoint in ProphetNet (#11214)

==

src/transformers/models/prophetnet/modeling_prophetnet.py
==================
38a10c6b5;cronoik;2021-04-13 00:08:28 +0200;Replaced `which` with `who` (#11183)

==

.github/PULL_REQUEST_TEMPLATE.md
==================
9f1260971;NielsRogge;2021-04-13 00:07:10 +0200;Add DeiT (PyTorch) (#11056)
* First draft of deit

* More improvements

* Remove DeiTTokenizerFast from init

* Conversion script works

* Add DeiT to ViT conversion script

* Add tests, add head model, add support for deit in vit conversion script

* Update model checkpoint names

* Update image_mean and image_std, set resample to bicubic

* Improve docs

* Docs improvements

* Add DeiTForImageClassificationWithTeacher to init

* Address comments by @sgugger

* Improve feature extractors

* Make fix-copies

* Minor fixes

* Address comments by @patil-suraj

* All models uploaded

* Fix tests

* Remove labels argument from DeiTForImageClassificationWithTeacher

* Fix-copies, style and quality

* Fix tests

* Fix typo

* Multiple docs improvements

* More docs fixes
==

README.md
docs/source/index.rst
docs/source/model_doc/deit.rst
docs/source/model_doc/vit.rst
src/transformers/__init__.py
src/transformers/image_utils.py
src/transformers/models/__init__.py
src/transformers/models/auto/configuration_auto.py
src/transformers/models/auto/feature_extraction_auto.py
src/transformers/models/auto/modeling_auto.py
src/transformers/models/deit/__init__.py
src/transformers/models/deit/configuration_deit.py
src/transformers/models/deit/convert_deit_timm_to_pytorch.py
src/transformers/models/deit/feature_extraction_deit.py
src/transformers/models/deit/modeling_deit.py
src/transformers/models/vit/convert_vit_timm_to_pytorch.py
src/transformers/models/vit/feature_extraction_vit.py
src/transformers/models/vit/modeling_vit.py
src/transformers/utils/dummy_pt_objects.py
src/transformers/utils/dummy_vision_objects.py
tests/test_configuration_common.py
tests/test_feature_extraction_deit.py
tests/test_feature_extraction_vit.py
tests/test_modeling_deit.py
tests/test_modeling_vit.py
==================
cb251ba61;Takuya Makino;2021-04-13 06:35:32 +0900;Fix typo (#11188)

==

examples/token-classification/README.md
==================
0c6fcd303;fghuman;2021-04-12 17:59:46 +0200;Added documentation for data collator. (#10941)
* Added documentation for data collator.

* Update docs/source/data_collator.rst

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Added documentation for data collator.

* Added documentation for the data collator.

* Merge branch 'doc_DataCollator' of C:\Users\mahii\PycharmProjects\transformers with conflicts.

* Update documentation for the data collator.

* Update documentation for the data collator.

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
Co-authored-by: Amna <A.A.Ahmad@student.tudelft.nl>
==

docs/source/index.rst
docs/source/main_classes/data_collator.rst
==================
ef102c488;Masatoshi TSUCHIYA;2021-04-12 22:06:41 +0900;model_path should be ignored as the checkpoint path (#11157)
* model_path is refered as the path of the trainer, and should be ignored as the checkpoint path.

* Improved according to Sgugger's comment.
==

examples/text-classification/run_xnli.py
==================
623cd6aef;Sylvain Gugger;2021-04-12 08:14:29 -0400;Fix style

==

src/transformers/models/reformer/configuration_reformer.py
==================
a99f7f5c7;cronoik;2021-04-12 13:55:40 +0200;Minor typos fixed (#11182)

==

src/transformers/models/reformer/configuration_reformer.py
==================
26212c14e;Sylvain Gugger;2021-04-09 18:09:53 -0400;Reactivate Megatron tests an use less workers

==

.circleci/config.yml
tests/test_modeling_megatron_bert.py
==================
716120cbd;Lysandre;2021-04-09 17:46:52 -0400;Fix Typo

==

src/transformers/models/vit/modeling_vit.py
==================
6f90c29ea;Philipp Schmid;2021-04-09 21:18:00 +0200;added json dump and extraction of train run time (#11167)
* added json dump and extraction of train run time

* make style happy
==

tests/sagemaker/test_multi_node_data_parallel.py
tests/sagemaker/test_multi_node_model_parallel.py
tests/sagemaker/test_single_node_gpu.py
==================
07f0bb691;Stas Bekman;2021-04-09 11:39:12 -0700;[examples run_clm] fix _LazyModule hasher error (#11168)
* fix _LazyModule hasher error

* reword
==

examples/language-modeling/run_clm.py
==================
c161dd56d;Suraj Patil;2021-04-09 23:58:42 +0530;[examples/translation] support mBART-50 and M2M100 fine-tuning (#11170)
* keep a list of multilingual tokenizers

* add forced_bos_token argument
==

examples/seq2seq/run_translation.py
==================
fb41f9f50;Kevin Canwen Xu;2021-04-10 02:07:47 +0800;Add a special tokenizer for CPM model (#11068)
* Add a special tokenizer for CPM model

* make style

* fix

* Add docs

* styles

* cpm doc

* fix ci

* fix the overview

* add test

* make style

* typo

* Custom tokenizer flag

* Add REAMDE.md

Co-authored-by: Lysandre <lysandre.debut@reseau.eseo.fr>
==

.circleci/config.yml
README.md
docs/source/index.rst
docs/source/model_doc/cpm.rst
setup.py
src/transformers/__init__.py
src/transformers/dependency_versions_table.py
src/transformers/models/__init__.py
src/transformers/models/auto/tokenization_auto.py
src/transformers/models/cpm/__init__.py
src/transformers/models/cpm/tokenization_cpm.py
tests/test_tokenization_cpm.py
==================
45fc8c795;Sylvain Gugger;2021-04-09 11:57:44 -0400;Make `get_special_tokens_mask` consider all tokens (#11163)

==

docs/source/model_doc/convbert.rst
docs/source/model_doc/led.rst
src/transformers/models/albert/tokenization_albert.py
src/transformers/models/albert/tokenization_albert_fast.py
src/transformers/models/barthez/tokenization_barthez.py
src/transformers/models/barthez/tokenization_barthez_fast.py
src/transformers/models/bert/tokenization_bert.py
src/transformers/models/bertweet/tokenization_bertweet.py
src/transformers/models/big_bird/tokenization_big_bird.py
src/transformers/models/camembert/tokenization_camembert.py
src/transformers/models/camembert/tokenization_camembert_fast.py
src/transformers/models/deberta/tokenization_deberta.py
src/transformers/models/deberta_v2/tokenization_deberta_v2.py
src/transformers/models/fsmt/tokenization_fsmt.py
src/transformers/models/herbert/tokenization_herbert_fast.py
src/transformers/models/m2m_100/tokenization_m2m_100.py
src/transformers/models/mbart/tokenization_mbart.py
src/transformers/models/mbart/tokenization_mbart50.py
src/transformers/models/mbart/tokenization_mbart50_fast.py
src/transformers/models/mbart/tokenization_mbart_fast.py
src/transformers/models/mpnet/tokenization_mpnet.py
src/transformers/models/phobert/tokenization_phobert.py
src/transformers/models/prophetnet/tokenization_prophetnet.py
src/transformers/models/roberta/tokenization_roberta.py
src/transformers/models/speech_to_text/tokenization_speech_to_text.py
src/transformers/models/t5/tokenization_t5.py
src/transformers/models/tapas/tokenization_tapas.py
src/transformers/models/xlm/tokenization_xlm.py
src/transformers/models/xlm_prophetnet/tokenization_xlm_prophetnet.py
src/transformers/models/xlm_roberta/tokenization_xlm_roberta.py
src/transformers/models/xlm_roberta/tokenization_xlm_roberta_fast.py
src/transformers/models/xlnet/tokenization_xlnet.py
src/transformers/models/xlnet/tokenization_xlnet_fast.py
src/transformers/tokenization_utils.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/tokenization_{{cookiecutter.lowercase_modelname}}.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/{{cookiecutter.lowercase_modelname}}.rst
==================
606074657;Saviour Owolabi;2021-04-09 16:52:21 +0100;Update README.md (#11161)
Corrected a typo ('Downlowd' to 'Download')
==

examples/legacy/seq2seq/README.md
==================
b9b60c163;Keisuke Hirota;2021-04-09 16:09:44 +0900;Fix LogitsProcessor documentation (#11130)
* Change duplicated LogitsProcessor to LogitsWarper in LogitsProcessorList document

* Write more detailed information about LogitsProcessor's scores argument

* apply suggestion from review

* style

Co-authored-by: Suraj Patil <surajp815@gmail.com>
==

src/transformers/generation_logits_process.py
==================
8b78a32be;Niklas Muennighoff;2021-04-09 08:40:37 +0200;[Community notebooks] Add Wav2Vec notebook for creating captions for YT Clips (#11142)
* Add Wav2Vec Inference notebook

* Update docs/source/community.md

Co-authored-by: Suraj Patil <surajp815@gmail.com>
==

docs/source/community.md
==================
0311ba215;Stas Bekman;2021-04-08 19:47:31 -0700;typo (#11152)
* typo

* style
==

docs/source/main_classes/trainer.rst
==================
269c9638d;Sylvain Gugger;2021-04-08 21:14:56 -0400;Merge branch 'master' of github.com:huggingface/transformers

==
==================
d31c7b104;Sylvain Gugger;2021-04-08 21:14:43 -0400;Skip Megatron tests for now

==

tests/test_modeling_megatron_bert.py
==================
c2e0fd528;Stas Bekman;2021-04-08 15:46:54 -0700;[setup] make fairscale and deepspeed setup extras (#11151)
* make fairscale and deepspeed setup extras

* fix default

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* no reason not to ask for the good version

* update the CIs

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

.github/workflows/self-scheduled.yml
docs/source/main_classes/trainer.rst
setup.py
src/transformers/dependency_versions_check.py
src/transformers/dependency_versions_table.py
src/transformers/integrations.py
src/transformers/trainer.py
src/transformers/utils/versions.py
==================
ba8b1f475;Sylvain Gugger;2021-04-08 18:41:36 -0400;Add support for multiple models for one config in auto classes (#11150)
* Add support for multiple models for one config in auto classes

* Use get_values everywhere

* Prettier doc
==

src/transformers/modeling_flax_utils.py
src/transformers/modeling_tf_utils.py
src/transformers/models/auto/__init__.py
src/transformers/models/auto/auto_factory.py
src/transformers/models/auto/configuration_auto.py
src/transformers/models/auto/modeling_auto.py
src/transformers/models/auto/modeling_tf_auto.py
tests/test_modeling_albert.py
tests/test_modeling_auto.py
tests/test_modeling_bert.py
tests/test_modeling_big_bird.py
tests/test_modeling_common.py
tests/test_modeling_convbert.py
tests/test_modeling_electra.py
tests/test_modeling_flax_bert.py
tests/test_modeling_funnel.py
tests/test_modeling_led.py
tests/test_modeling_lxmert.py
tests/test_modeling_megatron_bert.py
tests/test_modeling_mobilebert.py
tests/test_modeling_tapas.py
tests/test_modeling_tf_albert.py
tests/test_modeling_tf_auto.py
tests/test_modeling_tf_bert.py
tests/test_modeling_tf_common.py
utils/check_repo.py
==================
97ccf67bb;Stas Bekman;2021-04-08 15:10:44 -0700;[setup] extras[docs] must include 'all' (#11148)
* extras[doc] must include 'all'

* fix

* better

* regroup
==

.circleci/config.yml
setup.py
==================
66446909b;Stas Bekman;2021-04-08 13:13:17 -0700;[tests] relocate core integration tests (#11146)
* relocate core integration tests

* add sys.path context manager

* cleanup

* try

* try2

* fix path

* doc

* style

* add dep

* add 2 more deps
==

docs/source/main_classes/trainer.rst
docs/source/testing.rst
setup.py
src/transformers/dependency_versions_table.py
src/transformers/testing_utils.py
tests/deepspeed/ds_config_zero2.json
tests/deepspeed/ds_config_zero3.json
tests/deepspeed/test_deepspeed.py
tests/extended/test_trainer_ext.py
==================
6c40e4971;Andrea Cappelli;2021-04-08 22:12:49 +0200;Run mlm pad to multiple for fp16 (#11128)
* Add mlm collator pad to multiple option (#10627)

* Use padding to 8x in run mlm (#10627)
==

examples/language-modeling/run_mlm.py
src/transformers/data/data_collator.py
tests/test_data_collator.py
==================
dfed4ec26;Sylvain Gugger;2021-04-08 16:12:36 -0400;Don't duplicate logs in TensorBoard and handle --use_env (#11141)

==

src/transformers/integrations.py
src/transformers/training_args.py
==================
9c9b8e707;Philipp Schmid;2021-04-08 22:05:53 +0200;Updates SageMaker docs for updating DLCs (#11140)

==

tests/sagemaker/README.md
==================
ba2cf5f90;Lysandre Debut;2021-04-08 14:36:45 -0400;Add fairscale and deepspeed back to the CI (#11147)
* Add fairscale and deepspeed back to the CI

* Add deepspeed to single GPU tests
==

.github/workflows/self-scheduled.yml
==================
1ed24afe9;Stas Bekman;2021-04-08 11:28:48 -0700;[trainer] solve "scheduler before optimizer step" warning (#11144)
* solve "scheduler before optimizer step" warning

* style

* correct the state evaluation test
==

src/transformers/trainer.py
==================
02ec02d6d;Julien Demouth;2021-04-08 20:09:11 +0200;Add nvidia megatron models (#10911)
* Add support for NVIDIA Megatron models

* Add support for NVIDIA Megatron GPT2 and BERT

Add the megatron_gpt2 model. That model reuses the existing GPT2 model. This
commit includes a script to convert a Megatron-GPT2 checkpoint downloaded
from NVIDIA GPU Cloud. See examples/megatron-models/README.md for details.

Add the megatron_bert model. That model is implemented as a modification of
the existing BERT model in Transformers. This commit includes a script to
convert a Megatron-BERT checkpoint downloaded from NVIDIA GPU Cloud. See
examples/megatron-models/README.md for details.

* Update src/transformers/models/megatron_bert/configuration_megatron_bert.py

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* Update src/transformers/models/megatron_bert/configuration_megatron_bert.py

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* Update src/transformers/models/megatron_bert/configuration_megatron_bert.py

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* Remove model.half in tests + add "# Copied ..."

Remove the model.half() instruction which makes tests fail on the CPU.

Add a comment "# Copied ..." before many classes in the model to enable automatic
tracking in CI between the new Megatron classes and the original Bert ones.

* Fix issues

* Fix Flax/TF tests

* Fix copyright

* Update src/transformers/models/megatron_bert/configuration_megatron_bert.py

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* Update src/transformers/models/megatron_bert/configuration_megatron_bert.py

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* Update src/transformers/models/megatron_bert/modeling_megatron_bert.py

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* Update src/transformers/models/megatron_bert/modeling_megatron_bert.py

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* Update src/transformers/models/megatron_bert/modeling_megatron_bert.py

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* Update src/transformers/models/megatron_bert/modeling_megatron_bert.py

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* Update docs/source/model_doc/megatron_bert.rst

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update docs/source/model_doc/megatron_gpt2.rst

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/models/megatron_bert/__init__.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/models/megatron_bert/modeling_megatron_bert.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/models/megatron_gpt2/convert_megatron_gpt2_checkpoint.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/models/megatron_gpt2/convert_megatron_gpt2_checkpoint.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/models/megatron_gpt2/convert_megatron_gpt2_checkpoint.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/models/megatron_bert/convert_megatron_bert_checkpoint.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/models/megatron_bert/convert_megatron_bert_checkpoint.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/models/megatron_bert/convert_megatron_bert_checkpoint.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/models/megatron_bert/modeling_megatron_bert.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/models/megatron_bert/modeling_megatron_bert.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/models/megatron_bert/modeling_megatron_bert.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/models/megatron_bert/modeling_megatron_bert.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/models/megatron_bert/modeling_megatron_bert.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/models/megatron_bert/modeling_megatron_bert.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/models/megatron_bert/modeling_megatron_bert.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/models/megatron_bert/modeling_megatron_bert.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/models/megatron_bert/modeling_megatron_bert.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/models/megatron_bert/modeling_megatron_bert.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/models/megatron_bert/modeling_megatron_bert.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Resolve most of 'sgugger' comments

* Fix conversion issue + Run make fix-copies/quality/docs

* Apply suggestions from code review

* Causal LM & merge

* Fix init

* Add CausalLM to last auto class

Co-authored-by: Julien Demouth <jdemouth@nvidia.com>
Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
Co-authored-by: Lysandre <lysandre.debut@reseau.eseo.fr>
==

README.md
docs/source/index.rst
docs/source/model_doc/megatron_bert.rst
docs/source/model_doc/megatron_gpt2.rst
src/transformers/__init__.py
src/transformers/models/__init__.py
src/transformers/models/auto/configuration_auto.py
src/transformers/models/auto/modeling_auto.py
src/transformers/models/megatron_bert/__init__.py
src/transformers/models/megatron_bert/configuration_megatron_bert.py
src/transformers/models/megatron_bert/convert_megatron_bert_checkpoint.py
src/transformers/models/megatron_bert/modeling_megatron_bert.py
src/transformers/models/megatron_gpt2/convert_megatron_gpt2_checkpoint.py
src/transformers/utils/dummy_pt_objects.py
src/transformers/utils/modeling_auto_mapping.py
tests/test_modeling_megatron_bert.py
utils/check_repo.py
==================
c6d664849;Stas Bekman;2021-04-08 09:53:01 -0700;[DeepSpeed] ZeRO Stage 3 (#10753)
* synced gpus

* fix

* fix

* need to use t5-small for quality tests

* notes

* complete merge

* fix a disappearing std stream problem

* start zero3 tests

* wip

* tune params

* sorting out the pre-trained model loading

* reworking generate loop wip

* wip

* style

* fix tests

* split the tests

* refactor tests

* wip

* parameterized

* fix

* workout the resume from non-ds checkpoint pass + test

* cleanup

* remove no longer needed code

* split getter/setter functions

* complete the docs

* suggestions

* gpus and their compute capabilities link

* Apply suggestions from code review

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* style

* remove invalid paramgd

* automatically configure zero3 params that rely on hidden size

* make _get_resized_embeddings zero3-aware

* add test exercising resize_token_embeddings()

* add docstring

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
==

docs/source/main_classes/trainer.rst
examples/tests/deepspeed/ds_config_zero2.json
examples/tests/deepspeed/ds_config_zero3.json
examples/tests/deepspeed/test_deepspeed.py
src/transformers/generation_utils.py
src/transformers/integrations.py
src/transformers/modeling_utils.py
src/transformers/trainer.py
src/transformers/trainer_seq2seq.py
tests/test_trainer.py
==================
acc851e1f;Stas Bekman;2021-04-08 09:46:28 -0700;[run_clm] clarify why we get the tokenizer warning on long input (#11145)
* clarify why we get the warning here

* Update examples/language-modeling/run_clm.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* wording

* style

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

examples/language-modeling/run_clm.py
==================
5bf5d50c8;Yusuke Mori;2021-04-08 21:22:58 +0900;Typo fix of the name of BertLMHeadModel in BERT doc (#11133)

==

docs/source/model_doc/bert.rst
==================
f8e90d6fb;Jannis Born;2021-04-08 14:22:25 +0200;Fix typing error in Trainer class (prediction_step) (#11138)
* fix: docstrings in prediction_step

* ci: Satisfy line length requirements

* ci: character length requirements
==

src/transformers/trainer.py
==================
ffe076177;Sylvain Gugger;2021-04-07 17:56:21 -0400;Fix and refactor check_repo (#11127)

==

utils/check_repo.py
==================
3fd7eee18;Philipp Schmid;2021-04-07 20:32:59 +0200;Adds use_auth_token with pipelines (#11123)
* added model_kwargs to infer_framework_from_model

* added model_kwargs to tokenizer

* added use_auth_token as named parameter

* added dynamic get for use_auth_token
==

src/transformers/pipelines/__init__.py
src/transformers/pipelines/base.py
==================
1c1512831;Stas Bekman;2021-04-07 09:09:38 -0700;[versions] handle version requirement ranges (#11110)
* handle version requirement ranges

* add mixed requirement test

* cleanup
==

src/transformers/utils/versions.py
tests/test_versions_utils.py
==================
7442801df;Vasudev Gupta;2021-04-07 19:37:26 +0530;fix tests (#11109)

==

tests/test_modeling_big_bird.py
==================
c0d97cee1;Lysandre Debut;2021-04-07 10:06:45 -0400;Adds a note to resize the token embedding matrix when adding special ‚Ä¶ (#11120)
* Adds a note to resize the token embedding matrix when adding special tokens

* Remove superfluous space
==

src/transformers/tokenization_utils_base.py
==================
02f7c2fe6;Sylvain Gugger;2021-04-07 10:00:33 -0400;Some styling of the training table in Notebooks (#11118)

==

src/transformers/utils/notebook.py
==================
11505fa13;Sylvain Gugger;2021-04-07 09:56:40 -0400;Dummies multi backend (#11100)
* Replaces requires_xxx by one generic method

* Quality and update check_dummies

* Fix inits check

* Post-merge cleanup
==

src/transformers/__init__.py
src/transformers/convert_slow_tokenizer.py
src/transformers/data/metrics/__init__.py
src/transformers/file_utils.py
src/transformers/models/rag/retrieval_rag.py
src/transformers/models/tapas/modeling_tapas.py
src/transformers/pipelines/table_question_answering.py
src/transformers/utils/dummy_flax_objects.py
src/transformers/utils/dummy_pt_objects.py
src/transformers/utils/dummy_sentencepiece_and_speech_objects.py
src/transformers/utils/dummy_sentencepiece_and_tokenizers_objects.py
src/transformers/utils/dummy_sentencepiece_objects.py
src/transformers/utils/dummy_speech_objects.py
src/transformers/utils/dummy_tf_objects.py
src/transformers/utils/dummy_tokenizers_objects.py
src/transformers/utils/dummy_vision_objects.py
utils/check_dummies.py
utils/check_inits.py
==================
424419f54;Stas Bekman;2021-04-07 06:20:58 -0700;[examples] fix white space (#11099)
these get concatenated without whitespace, so fix it
==

examples/language-modeling/run_clm.py
==================
c9035e453;Stas Bekman;2021-04-07 06:20:06 -0700;fix: The 'warn' method is deprecated (#11105)
* The 'warn' method is deprecated

* fix test
==

examples/language-modeling/run_clm.py
examples/language-modeling/run_clm_no_trainer.py
examples/language-modeling/run_mlm.py
examples/language-modeling/run_mlm_no_trainer.py
examples/language-modeling/run_plm.py
examples/legacy/question-answering/run_squad.py
examples/legacy/seq2seq/seq2seq_trainer.py
examples/multiple-choice/run_swag.py
examples/question-answering/run_qa.py
examples/question-answering/run_qa_beam_search.py
examples/question-answering/run_qa_beam_search_no_trainer.py
examples/question-answering/run_qa_no_trainer.py
examples/question-answering/run_tf_squad.py
examples/research_projects/movement-pruning/masked_run_squad.py
examples/seq2seq/run_summarization.py
examples/seq2seq/run_translation.py
examples/text-classification/run_glue.py
examples/text-classification/run_glue_no_trainer.py
src/transformers/configuration_utils.py
src/transformers/data/datasets/squad.py
src/transformers/file_utils.py
src/transformers/integrations.py
src/transformers/modeling_tf_utils.py
src/transformers/models/auto/tokenization_auto.py
src/transformers/models/bart/modeling_bart.py
src/transformers/models/bert/modeling_bert.py
src/transformers/models/bert_generation/modeling_bert_generation.py
src/transformers/models/big_bird/modeling_big_bird.py
src/transformers/models/blenderbot/modeling_blenderbot.py
src/transformers/models/blenderbot_small/modeling_blenderbot_small.py
src/transformers/models/electra/modeling_electra.py
src/transformers/models/gpt2/modeling_gpt2.py
src/transformers/models/gpt_neo/modeling_gpt_neo.py
src/transformers/models/layoutlm/modeling_layoutlm.py
src/transformers/models/led/modeling_led.py
src/transformers/models/m2m_100/modeling_m2m_100.py
src/transformers/models/marian/modeling_marian.py
src/transformers/models/mbart/modeling_mbart.py
src/transformers/models/pegasus/modeling_pegasus.py
src/transformers/models/prophetnet/modeling_prophetnet.py
src/transformers/models/roberta/modeling_roberta.py
src/transformers/models/speech_to_text/modeling_speech_to_text.py
src/transformers/models/wav2vec2/convert_wav2vec2_original_pytorch_checkpoint_to_pytorch.py
src/transformers/models/xlm/modeling_tf_xlm.py
src/transformers/models/xlm/modeling_xlm.py
src/transformers/pipelines/zero_shot_classification.py
src/transformers/trainer_callback.py
src/transformers/trainer_pt_utils.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_{{cookiecutter.lowercase_modelname}}.py
tests/sagemaker/scripts/pytorch/run_glue_model_parallelism.py
tests/test_logging.py
tests/test_trainer_callback.py
==================
247bed385;Leo Gao;2021-04-07 06:05:20 -0600;GPTNeo: handle padded wte (#11079)
* GPTNeo: handle padded wte

* Switch to config.vocab_size

* apply review suggestion

Co-authored-by: Suraj Patil <surajp815@gmail.com>
==

src/transformers/models/gpt_neo/modeling_gpt_neo.py
==================
083ad7d46;cronoik;2021-04-07 13:50:47 +0200;dead link fixed (#11103)

==

src/transformers/models/reformer/configuration_reformer.py
==================
fd338abde;Sylvain Gugger;2021-04-06 19:54:13 -0400;Style

==

examples/question-answering/run_qa_beam_search_no_trainer.py
examples/question-answering/run_qa_no_trainer.py
==================
aef4cf8c5;SHYAM SUNDER KUMAR;2021-04-07 05:05:21 +0530;accelerate question answering examples with no trainer (#11091)
* accelerate question answering examples with no trainer

* removed train and eval flags also fixed fill np array function

* Update examples/question-answering/run_qa_beam_search_no_trainer.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update examples/question-answering/run_qa_no_trainer.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

examples/question-answering/run_qa_beam_search_no_trainer.py
examples/question-answering/run_qa_no_trainer.py
examples/question-answering/utils_qa.py
==================
403d530ee;Sylvain Gugger;2021-04-06 19:20:08 -0400;Auto feature extractor (#11097)
* AutoFeatureExtractor

* Init and first tests

* Tests

* Damn you gitignore

* Quality

* Defensive test for when not all backends are here

* Use pattern for Speech2Text models
==

.gitignore
docs/source/model_doc/auto.rst
src/transformers/__init__.py
src/transformers/dependency_versions_table.py
src/transformers/feature_extraction_utils.py
src/transformers/file_utils.py
src/transformers/models/auto/__init__.py
src/transformers/models/auto/feature_extraction_auto.py
src/transformers/models/speech_to_text/__init__.py
src/transformers/models/speech_to_text/feature_extraction_speech_to_text.py
src/transformers/utils/dummy_sentencepiece_objects.py
src/transformers/utils/dummy_speech_objects.py
tests/fixtures/dummy_feature_extractor_config.json
tests/test_feature_extraction_auto.py
tests/test_feature_extraction_speech_to_text.py
tests/test_processor_speech_to_text.py
utils/check_dummies.py
utils/check_inits.py
==================
520198f56;Stas Bekman;2021-04-06 13:42:06 -0700;[doc] gpt-neo (#11098)
make the example work
==

docs/source/model_doc/gpt_neo.rst
==================
9853c5dd5;Lysandre;2021-04-06 12:53:25 -0400;Development on v4.6.0dev0

==

.circleci/deploy.sh
docs/source/_static/js/custom.js
examples/language-modeling/run_clm.py
examples/language-modeling/run_mlm.py
examples/language-modeling/run_plm.py
examples/multiple-choice/run_swag.py
examples/question-answering/run_qa.py
examples/question-answering/run_qa_beam_search.py
examples/seq2seq/run_summarization.py
examples/seq2seq/run_translation.py
examples/text-classification/run_glue.py
examples/text-classification/run_xnli.py
examples/token-classification/run_ner.py
setup.py
src/transformers/__init__.py
==================
4906a29f7;Lysandre;2021-04-06 12:37:47 -0400;Release v4.5.0

==

examples/language-modeling/run_clm.py
examples/language-modeling/run_mlm.py
examples/language-modeling/run_plm.py
examples/multiple-choice/run_swag.py
examples/question-answering/run_qa.py
examples/question-answering/run_qa_beam_search.py
examples/seq2seq/run_summarization.py
examples/seq2seq/run_translation.py
examples/text-classification/run_glue.py
examples/text-classification/run_xnli.py
examples/token-classification/run_ner.py
setup.py
src/transformers/__init__.py
==================
2a8115f08;Suraj Patil;2021-04-06 21:54:15 +0530;[WIP] GPT Neo cleanup (#10985)
* better names

* add attention mixin

* all slow tests in one class

* make helper methods static so we can test

* add local attention tests

* better names

* doc

* apply review suggestions
==

src/transformers/models/gpt_neo/modeling_gpt_neo.py
tests/test_modeling_gpt_neo.py
==================
76800fb8e;Philipp Schmid;2021-04-06 15:12:21 +0200;added new merged Trainer test (#11090)

==

tests/sagemaker/test_multi_node_model_parallel.py
==================
b219d6b5a;Philipp Schmid;2021-04-06 14:56:18 +0200;added social thumbnail for docs (#11083)

==

docs/source/conf.py
setup.py
==================
6c1bee7d8;Sylvain Gugger;2021-04-06 08:55:40 -0400;Link to new blog

==

docs/source/_static/js/custom.js
==================
f7328de46;Stas Bekman;2021-04-06 05:03:00 -0700;HF emoji unicode doesn't work in console (#11081)
It doesn't look like using ü§ó is a great idea for printing to console. See attachment.

This PR proposes to replace ü§ó with "HuggingFace" for an exception message.

@LysandreJik
==

src/transformers/utils/__init__.py
==================
6ab7d1a42;Hemil Desai;2021-04-06 06:26:12 +0530;Add Readme for language modeling scripts with accelerate (#11073)

==

examples/language-modeling/README.md
examples/language-modeling/run_mlm_no_trainer.py
==================
2199608ca;Sylvain Gugger;2021-04-05 18:02:28 -0400;Make a base init in FeatureExtractionMixin (#11074)

==

src/transformers/feature_extraction_sequence_utils.py
src/transformers/feature_extraction_utils.py
==================
04ceee7d2;Sylvain Gugger;2021-04-05 16:21:49 -0400;Fix distributed gather for tuples of tensors of varying sizes (#11071)

==

src/transformers/trainer_pt_utils.py
tests/test_trainer_utils.py
==================
f05a8a0c5;Sylvain Gugger;2021-04-05 15:29:01 -0400;Document common config attributes (#11070)

==

src/transformers/configuration_utils.py
==================
090e3e689;Sylvain Gugger;2021-04-05 15:28:51 -0400;Add center_crop to ImageFeatureExtractoMixin (#11066)

==

src/transformers/image_utils.py
tests/test_image_utils.py
==================
abb743000;konstin;2021-04-05 21:12:19 +0200;Replace pkg_resources with importlib_metadata (#11061)
* Replace pkg_resources with importlib_metadata

Fixes #10964. The other reason for this change is that pkg_resources has been [deprecated](https://github.com/pypa/setuptools/commit/8fe85c22cee7fde5e6af571b30f864bad156a010) in favor of importlib_metadata.

* Reduce to a single importlib_metadata import switch

* Trigger CI

Co-authored-by: Stas Bekman <stas@stason.org>
==

src/transformers/file_utils.py
src/transformers/utils/versions.py
tests/test_versions_utils.py
==================
b51b87c41;Hemil Desai;2021-04-05 21:57:52 +0530;Add `examples/language_modeling/run_clm_no_trainer.py` (#11026)
* Initial draft for clm no trainer

* Remove unwanted args

* Fix bug

* Update examples/language-modeling/run_clm_no_trainer.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

examples/language-modeling/run_clm_no_trainer.py
==================
e1c02e018;Amala Deshmukh;2021-04-05 12:27:23 -0400;Add example for registering callbacks with trainers (#10928)
* Add example for callback registry

Resolves: #9036

* Update callback registry documentation

* Added comments for other ways to register callback
==

docs/source/main_classes/callback.rst
==================
9f4e0c23d;Lysandre Debut;2021-04-05 10:51:16 -0400;Documentation about loading a fast tokenizer within Transformers (#11029)
* Documentation about loading a fast tokenizer within Transformers

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* style

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

docs/source/fast_tokenizers.rst
docs/source/index.rst
docs/source/main_classes/tokenizer.rst
src/transformers/tokenization_utils_fast.py
tests/test_tokenization_utils.py
==================
6c25f5228;Sylvain Gugger;2021-04-05 10:11:28 -0400;Refactor AutoModel classes and add Flax Auto classes (#11027)
* Refactor AutoModel classes and add Flax Auto classes

* Add new objects to the init

* Fix hubconf and sort models

* Fix TF tests

* Missing coma

* Update src/transformers/models/auto/auto_factory.py

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* Fix init

* Fix dummies

* Other init to fix

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
==

docs/source/model_doc/auto.rst
hubconf.py
src/transformers/__init__.py
src/transformers/models/auto/__init__.py
src/transformers/models/auto/auto_factory.py
src/transformers/models/auto/configuration_auto.py
src/transformers/models/auto/modeling_auto.py
src/transformers/models/auto/modeling_flax_auto.py
src/transformers/models/auto/modeling_tf_auto.py
src/transformers/utils/dummy_flax_objects.py
==================
eb3479e7c;Lysandre Debut;2021-04-05 09:37:49 -0400;Some models have no tokenizers (#11064)

==

tests/test_tokenization_common.py
==================
773e4c726;Lysandre Debut;2021-04-05 09:36:20 -0400;Remove unnecessary space (#11060)

==

docs/source/task_summary.rst
==================
ef62f038f;Lysandre Debut;2021-04-05 09:35:21 -0400;Pin docutils (#11062)
* Pin docutils

* Versions table
==

setup.py
src/transformers/dependency_versions_table.py
==================
6e3101411;Eren ≈ûahin;2021-04-05 16:06:07 +0300;[doc] update code-block rendering (#11053)
double : prevents code-block section to be rendered, so made it single :
==

docs/source/model_doc/gpt.rst
==================
3d39226a5;Stas Bekman;2021-04-04 18:08:42 -0700;s|Pretrained|PreTrained| (#11048)

==

examples/research_projects/rag/distributed_pytorch_retriever.py
examples/research_projects/rag/distributed_ray_retriever.py
src/transformers/generation_beam_search.py
src/transformers/generation_logits_process.py
src/transformers/models/ctrl/modeling_ctrl.py
src/transformers/models/encoder_decoder/modeling_encoder_decoder.py
src/transformers/models/gpt2/modeling_gpt2.py
src/transformers/models/transfo_xl/modeling_transfo_xl.py
src/transformers/models/xlnet/modeling_xlnet.py
src/transformers/pipelines/__init__.py
src/transformers/tokenization_utils_base.py
==================
b0d49fd53;Sylvain Gugger;2021-04-04 20:41:34 -0400;Add a script to check inits are consistent (#11024)

==

.circleci/config.yml
Makefile
src/transformers/__init__.py
src/transformers/models/gpt_neo/__init__.py
src/transformers/models/mt5/__init__.py
src/transformers/utils/dummy_pt_objects.py
utils/check_inits.py
==================
335c0ca35;versis;2021-04-02 15:22:22 +0200;fixed typo: logging instead of logger (#11025)

==

examples/research_projects/zero-shot-distillation/distill_classifier.py
==================
34e1bec64;Philipp Schmid;2021-04-01 23:13:47 +0200;added new notebook and merge of trainer (#11015)
* added new notebook and merge of trainer

* Update docs/source/sagemaker.md

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
==

docs/source/sagemaker.md
==================
e8da77d18;Julien Chaumond;2021-04-01 20:25:47 +0200;[doc] no more bucket

==

docs/source/installation.md
==================
f4ad3d8ce;Joe Davison;2021-04-01 11:58:37 -0600;minor typo fix
*negative* log-likelihood
==

docs/source/perplexity.rst
==================
57c1749ef;cronoik;2021-04-01 19:53:53 +0200;DebertaTokenizer Rework closes #10258 (#10703)
* closes #10258

* typo

* reworked deberta test

* implemented the comments from BigBird01 regarding sequence pair encoding of deberta

* Update style

* VOCAB_FILES_NAMES is now a oneliner as suggested by @sgugger

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* added #fmt: on as requested by @sgugger

* Style

Co-authored-by: Lysandre <lysandre.debut@reseau.eseo.fr>
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
==

src/transformers/models/deberta/tokenization_deberta.py
tests/test_tokenization_deberta.py
==================
30677dc74;NielsRogge;2021-04-01 17:16:05 +0200;Add Vision Transformer and ViTFeatureExtractor (#10950)
* Squash all commits into one

* Update ViTFeatureExtractor to use image_utils instead of torchvision

* Remove torchvision and add Pillow

* Small docs improvement

* Address most comments by @sgugger

* Fix tests

* Clean up conversion script

* Pooler first draft

* Fix quality

* Improve conversion script

* Make style and quality

* Make fix-copies

* Minor docs improvements

* Should use fix-copies instead of manual handling

* Revert "Should use fix-copies instead of manual handling"

This reverts commit fd4e591bce4496d41406425c82606a8fdaf8a50b.

* Place ViT in alphabetical order

Co-authored-by: Lysandre <lysandre.debut@reseau.eseo.fr>
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

.circleci/config.yml
README.md
docs/source/index.rst
docs/source/model_doc/vit.rst
setup.py
src/transformers/__init__.py
src/transformers/dependency_versions_table.py
src/transformers/file_utils.py
src/transformers/image_utils.py
src/transformers/models/__init__.py
src/transformers/models/auto/__init__.py
src/transformers/models/auto/configuration_auto.py
src/transformers/models/auto/modeling_auto.py
src/transformers/models/vit/__init__.py
src/transformers/models/vit/configuration_vit.py
src/transformers/models/vit/convert_vit_timm_to_pytorch.py
src/transformers/models/vit/feature_extraction_vit.py
src/transformers/models/vit/modeling_vit.py
src/transformers/utils/dummy_pt_objects.py
src/transformers/utils/dummy_vision_objects.py
src/transformers/utils/imagenet_classes.py
tests/test_feature_extraction_vit.py
tests/test_image_utils.py
tests/test_modeling_common.py
tests/test_modeling_vit.py
==================
af6732225;cchen-dialpad;2021-04-01 05:56:12 -0700;Improve the speed of adding tokens from added_tokens.json (#10780)
* use bisect to add one token to unique_no_split_tokens

* fix style
==

src/transformers/tokenization_utils.py
==================
c301c2637;Josh;2021-03-31 21:03:38 -0700;Fix Adafactor documentation (recommend correct settings) (#10526)
* Update optimization.py

Fix documentation to reflect optimal settings for Adafactor

* update and expand on the recommendations

* style

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* flip scale_parameter to True for the 2nd recommendatoin

Co-authored-by: Stas Bekman <stas@stason.org>
Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

src/transformers/optimization.py
==================
838f83d84;Hemil Desai;2021-04-01 04:19:45 +0530;Add `examples/language_modeling/run_mlm_no_trainer.py` (#11001)
* Add initial script for finetuning MLM models with accelerate

* Add evaluation metric calculation

* Fix bugs

* Use no_grad on evaluation

* update script docstring

* Update examples/language-modeling/run_mlm_no_trainer.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* PR feedback

* Fix CI failure

* Update examples/language-modeling/run_mlm_no_trainer.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

examples/language-modeling/run_mlm_no_trainer.py
==================
455f81711;JohnnyC08;2021-03-31 15:28:07 -0700;Update training_args.py (#11000)
In the group by length documentation length is misspelled as legnth
==

src/transformers/training_args.py
==================
01068abdb;Patrick von Platen;2021-03-31 18:36:00 +0300;add blog to docs (#10997)

==

docs/source/model_doc/bigbird.rst
==================
cd56f3fe7;Sylvain Gugger;2021-03-31 10:01:30 -0400;Merge trainers (#10975)
* Replace is_sagemaker_distributed_available

* Merge SageMakerTrainer into Trainer

* Test with shorter condition

* Put back deleted line

* Deprecate SageMakerTrainer and SageMakerTrainingArguments

* Apply suggestions from code review

Co-authored-by: Philipp Schmid <32632186+philschmid@users.noreply.github.com>

Co-authored-by: Philipp Schmid <32632186+philschmid@users.noreply.github.com>
==

src/transformers/file_utils.py
src/transformers/sagemaker/__init__.py
src/transformers/sagemaker/trainer_sm.py
src/transformers/sagemaker/training_args_sm.py
src/transformers/trainer.py
src/transformers/trainer_pt_utils.py
src/transformers/trainer_utils.py
src/transformers/training_args.py
tests/sagemaker/scripts/tensorflow/run_tf_dist.py
==================
b6dddda4d;Patrick von Platen;2021-03-31 17:00:56 +0300;add notebook (#10995)

==

docs/source/community.md
==================
acc3bd9d2;Sylvain Gugger;2021-03-31 10:00:27 -0400;Enforce string-formatting with f-strings (#10980)
* First third

* Styling and fix mistake

* Quality

* All the rest

* Treat %s and %d

* typo

* Missing )

* Apply suggestions from code review

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
==

examples/language-modeling/run_clm.py
examples/language-modeling/run_mlm.py
examples/language-modeling/run_mlm_flax.py
examples/language-modeling/run_plm.py
examples/multiple-choice/run_swag.py
examples/multiple-choice/run_tf_multiple_choice.py
examples/multiple-choice/utils_multiple_choice.py
examples/question-answering/run_qa.py
examples/question-answering/run_qa_beam_search.py
examples/question-answering/run_tf_squad.py
examples/seq2seq/run_summarization.py
examples/seq2seq/run_translation.py
examples/text-classification/run_tf_glue.py
examples/text-classification/run_tf_text_classification.py
examples/text-generation/run_generation.py
examples/token-classification/run_ner.py
src/transformers/activations.py
src/transformers/activations_tf.py
src/transformers/benchmark/benchmark.py
src/transformers/benchmark/benchmark_tf.py
src/transformers/benchmark/benchmark_utils.py
src/transformers/commands/convert.py
src/transformers/commands/env.py
src/transformers/commands/run.py
src/transformers/commands/serving.py
src/transformers/commands/train.py
src/transformers/commands/user.py
src/transformers/configuration_utils.py
src/transformers/convert_graph_to_onnx.py
src/transformers/convert_pytorch_checkpoint_to_tf2.py
src/transformers/convert_slow_tokenizer.py
src/transformers/convert_slow_tokenizers_checkpoints_to_fast.py
src/transformers/convert_tf_hub_seq_to_seq_bert_to_pytorch.py
src/transformers/data/datasets/glue.py
src/transformers/data/datasets/language_modeling.py
src/transformers/data/datasets/squad.py
src/transformers/data/metrics/squad_metrics.py
src/transformers/data/processors/glue.py
src/transformers/data/processors/squad.py
src/transformers/data/processors/utils.py
src/transformers/data/processors/xnli.py
src/transformers/file_utils.py
src/transformers/generation_beam_search.py
src/transformers/generation_logits_process.py
src/transformers/generation_tf_utils.py
src/transformers/hf_api.py
src/transformers/hf_argparser.py
src/transformers/integrations.py
src/transformers/modelcard.py
src/transformers/modeling_flax_pytorch_utils.py
src/transformers/modeling_flax_utils.py
src/transformers/modeling_tf_pytorch_utils.py
src/transformers/modeling_tf_utils.py
src/transformers/modeling_utils.py
src/transformers/models/albert/convert_albert_original_tf_checkpoint_to_pytorch.py
src/transformers/models/albert/modeling_albert.py
src/transformers/models/albert/modeling_tf_albert.py
src/transformers/models/albert/tokenization_albert.py
src/transformers/models/albert/tokenization_albert_fast.py
src/transformers/models/auto/configuration_auto.py
src/transformers/models/auto/modeling_auto.py
src/transformers/models/auto/modeling_tf_auto.py
src/transformers/models/auto/tokenization_auto.py
src/transformers/models/barthez/tokenization_barthez.py
src/transformers/models/barthez/tokenization_barthez_fast.py
src/transformers/models/bert/convert_bert_original_tf2_checkpoint_to_pytorch.py
src/transformers/models/bert/convert_bert_original_tf_checkpoint_to_pytorch.py
src/transformers/models/bert/convert_bert_pytorch_checkpoint_to_original_tf.py
src/transformers/models/bert/modeling_bert.py
src/transformers/models/bert/modeling_tf_bert.py
src/transformers/models/bert/tokenization_bert.py
src/transformers/models/bert_generation/modeling_bert_generation.py
src/transformers/models/bert_generation/tokenization_bert_generation.py
src/transformers/models/bert_japanese/tokenization_bert_japanese.py
src/transformers/models/bertweet/tokenization_bertweet.py
src/transformers/models/big_bird/convert_bigbird_original_tf_checkpoint_to_pytorch.py
src/transformers/models/big_bird/modeling_big_bird.py
src/transformers/models/big_bird/tokenization_big_bird.py
src/transformers/models/blenderbot_small/tokenization_blenderbot_small.py
src/transformers/models/camembert/tokenization_camembert.py
src/transformers/models/camembert/tokenization_camembert_fast.py
src/transformers/models/convbert/modeling_convbert.py
src/transformers/models/convbert/modeling_tf_convbert.py
src/transformers/models/ctrl/modeling_tf_ctrl.py
src/transformers/models/ctrl/tokenization_ctrl.py
src/transformers/models/deberta/modeling_deberta.py
src/transformers/models/deberta/tokenization_deberta.py
src/transformers/models/deberta_v2/modeling_deberta_v2.py
src/transformers/models/deberta_v2/tokenization_deberta_v2.py
src/transformers/models/distilbert/modeling_distilbert.py
src/transformers/models/distilbert/modeling_tf_distilbert.py
src/transformers/models/dpr/convert_dpr_original_checkpoint_to_pytorch.py
src/transformers/models/dpr/tokenization_dpr.py
src/transformers/models/dpr/tokenization_dpr_fast.py
src/transformers/models/electra/convert_electra_original_tf_checkpoint_to_pytorch.py
src/transformers/models/electra/modeling_electra.py
src/transformers/models/electra/modeling_tf_electra.py
src/transformers/models/encoder_decoder/modeling_encoder_decoder.py
src/transformers/models/flaubert/modeling_tf_flaubert.py
src/transformers/models/flaubert/tokenization_flaubert.py
src/transformers/models/fsmt/modeling_fsmt.py
src/transformers/models/fsmt/tokenization_fsmt.py
src/transformers/models/funnel/convert_funnel_original_tf_checkpoint_to_pytorch.py
src/transformers/models/funnel/modeling_funnel.py
src/transformers/models/gpt2/convert_gpt2_original_tf_checkpoint_to_pytorch.py
src/transformers/models/gpt2/modeling_gpt2.py
src/transformers/models/gpt2/modeling_tf_gpt2.py
src/transformers/models/gpt2/tokenization_gpt2.py
src/transformers/models/gpt_neo/convert_gpt_neo_mesh_tf_to_pytorch.py
src/transformers/models/gpt_neo/modeling_gpt_neo.py
src/transformers/models/ibert/modeling_ibert.py
src/transformers/models/ibert/quant_modules.py
src/transformers/models/layoutlm/modeling_layoutlm.py
src/transformers/models/layoutlm/modeling_tf_layoutlm.py
src/transformers/models/led/modeling_led.py
src/transformers/models/led/modeling_tf_led.py
src/transformers/models/longformer/convert_longformer_original_pytorch_lightning_to_pytorch.py
src/transformers/models/longformer/modeling_longformer.py
src/transformers/models/longformer/modeling_tf_longformer.py
src/transformers/models/lxmert/convert_lxmert_original_tf_checkpoint_to_pytorch.py
src/transformers/models/lxmert/modeling_lxmert.py
src/transformers/models/lxmert/modeling_tf_lxmert.py
src/transformers/models/mbart/tokenization_mbart50.py
src/transformers/models/mbart/tokenization_mbart50_fast.py
src/transformers/models/mobilebert/convert_mobilebert_original_tf_checkpoint_to_pytorch.py
src/transformers/models/mobilebert/modeling_mobilebert.py
src/transformers/models/mobilebert/modeling_tf_mobilebert.py
src/transformers/models/mpnet/modeling_mpnet.py
src/transformers/models/mpnet/modeling_tf_mpnet.py
src/transformers/models/mpnet/tokenization_mpnet.py
src/transformers/models/openai/convert_openai_original_tf_checkpoint_to_pytorch.py
src/transformers/models/openai/modeling_openai.py
src/transformers/models/openai/modeling_tf_openai.py
src/transformers/models/openai/tokenization_openai.py
src/transformers/models/pegasus/tokenization_pegasus.py
src/transformers/models/pegasus/tokenization_pegasus_fast.py
src/transformers/models/phobert/tokenization_phobert.py
src/transformers/models/prophetnet/tokenization_prophetnet.py
src/transformers/models/rag/modeling_rag.py
src/transformers/models/rag/modeling_tf_rag.py
src/transformers/models/rag/retrieval_rag.py
src/transformers/models/rag/tokenization_rag.py
src/transformers/models/reformer/convert_reformer_trax_checkpoint_to_pytorch.py
src/transformers/models/reformer/modeling_reformer.py
src/transformers/models/reformer/tokenization_reformer.py
src/transformers/models/reformer/tokenization_reformer_fast.py
src/transformers/models/roberta/modeling_roberta.py
src/transformers/models/roberta/modeling_tf_roberta.py
src/transformers/models/squeezebert/modeling_squeezebert.py
src/transformers/models/t5/convert_t5_original_tf_checkpoint_to_pytorch.py
src/transformers/models/t5/modeling_t5.py
src/transformers/models/t5/modeling_tf_t5.py
src/transformers/models/t5/tokenization_t5.py
src/transformers/models/t5/tokenization_t5_fast.py
src/transformers/models/tapas/convert_tapas_original_tf_checkpoint_to_pytorch.py
src/transformers/models/tapas/modeling_tapas.py
src/transformers/models/tapas/tokenization_tapas.py
src/transformers/models/transfo_xl/convert_transfo_xl_original_tf_checkpoint_to_pytorch.py
src/transformers/models/transfo_xl/modeling_tf_transfo_xl.py
src/transformers/models/transfo_xl/modeling_tf_transfo_xl_utilities.py
src/transformers/models/transfo_xl/modeling_transfo_xl.py
src/transformers/models/transfo_xl/tokenization_transfo_xl.py
src/transformers/models/wav2vec2/tokenization_wav2vec2.py
src/transformers/models/xlm/convert_xlm_original_pytorch_checkpoint_to_pytorch.py
src/transformers/models/xlm/modeling_tf_xlm.py
src/transformers/models/xlm/modeling_xlm.py
src/transformers/models/xlm/tokenization_xlm.py
src/transformers/models/xlm_prophetnet/tokenization_xlm_prophetnet.py
src/transformers/models/xlm_roberta/tokenization_xlm_roberta.py
src/transformers/models/xlm_roberta/tokenization_xlm_roberta_fast.py
src/transformers/models/xlnet/convert_xlnet_original_tf_checkpoint_to_pytorch.py
src/transformers/models/xlnet/modeling_tf_xlnet.py
src/transformers/models/xlnet/modeling_xlnet.py
src/transformers/models/xlnet/tokenization_xlnet.py
src/transformers/models/xlnet/tokenization_xlnet_fast.py
src/transformers/optimization.py
src/transformers/optimization_tf.py
src/transformers/pipelines/__init__.py
src/transformers/pipelines/base.py
src/transformers/pipelines/conversational.py
src/transformers/pipelines/fill_mask.py
src/transformers/pipelines/question_answering.py
src/transformers/pipelines/text2text_generation.py
src/transformers/sagemaker/trainer_sm.py
src/transformers/testing_utils.py
src/transformers/tokenization_utils.py
src/transformers/tokenization_utils_base.py
src/transformers/tokenization_utils_fast.py
src/transformers/trainer.py
src/transformers/trainer_tf.py
templates/adding_a_new_example_script/{{cookiecutter.directory_name}}/run_{{cookiecutter.example_shortcut}}.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_tf_{{cookiecutter.lowercase_modelname}}.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_{{cookiecutter.lowercase_modelname}}.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/test_modeling_tf_{{cookiecutter.lowercase_modelname}}.py
tests/sagemaker/scripts/tensorflow/run_tf.py
tests/sagemaker/scripts/tensorflow/run_tf_dist.py
tests/test_hf_api.py
tests/test_modeling_common.py
tests/test_modeling_fsmt.py
tests/test_modeling_rag.py
tests/test_modeling_tf_auto.py
tests/test_modeling_tf_bart.py
tests/test_modeling_tf_blenderbot.py
tests/test_modeling_tf_blenderbot_small.py
tests/test_modeling_tf_led.py
tests/test_modeling_tf_marian.py
tests/test_modeling_tf_mbart.py
tests/test_modeling_tf_pegasus.py
tests/test_modeling_wav2vec2.py
tests/test_tokenization_auto.py
tests/test_tokenization_bart.py
tests/test_tokenization_bert.py
tests/test_tokenization_bertweet.py
tests/test_tokenization_common.py
tests/test_tokenization_gpt2.py
tests/test_tokenization_openai.py
tests/test_tokenization_phobert.py
tests/test_tokenization_reformer.py
tests/test_tokenization_roberta.py
tests/test_tokenization_tapas.py
tests/test_trainer_distributed.py
tests/test_trainer_tpu.py
utils/download_glue_data.py
utils/link_tester.py
==================
d0b3797a3;Sylvain Gugger;2021-03-31 09:36:07 -0400;Add more metadata to the user agent (#10972)
* Add more metadata to the user agent

* Fix typo

* Use DISABLE_TELEMETRY

* Address review comments

* Use global env

* Add clean envs on circle CI
==

.circleci/config.yml
.github/workflows/self-push.yml
.github/workflows/self-scheduled.yml
src/transformers/configuration_utils.py
src/transformers/file_utils.py
src/transformers/modelcard.py
src/transformers/modeling_flax_utils.py
src/transformers/modeling_tf_utils.py
src/transformers/modeling_utils.py
src/transformers/models/auto/configuration_auto.py
src/transformers/models/auto/modeling_auto.py
src/transformers/models/auto/modeling_flax_auto.py
src/transformers/models/auto/modeling_tf_auto.py
src/transformers/models/auto/tokenization_auto.py
src/transformers/pipelines/__init__.py
src/transformers/pipelines/base.py
src/transformers/tokenization_utils_base.py
==================
a8549bdd8;Suraj Patil;2021-03-31 17:38:57 +0530;fix example in config (#10993)

==

src/transformers/models/gpt_neo/configuration_gpt_neo.py
==================
a96edb85c;Lysandre Debut;2021-03-31 08:03:20 -0400;GPT Neo configuration needs to be set to use GPT2 tokenizer (#10992)

==

src/transformers/models/auto/tokenization_auto.py
==================
bf0840acc;Lysandre Debut;2021-03-31 08:02:51 -0400;Fix the checkpoint for I-BERT (#10994)

==

src/transformers/models/ibert/modeling_ibert.py
==================
ced7284a6;Philipp Schmid;2021-03-31 13:44:22 +0200;Sagemaker test fix (#10987)
* wrong makefile command

* ddp test fix
==

tests/sagemaker/README.md
tests/sagemaker/test_multi_node_data_parallel.py
==================
645f45c46;WybeKoper;2021-03-31 13:23:15 +0200;Fixed some typos and removed legacy url (#10989)
* Fixed typos

* Removed legacy colab notebook from readme

Co-authored-by: WybeKoper <WybeKoper@users.noreply.github.com>
==

examples/multiple-choice/README.md
src/transformers/generation_utils.py
src/transformers/models/xlm_prophetnet/tokenization_xlm_prophetnet.py
src/transformers/models/xlm_roberta/tokenization_xlm_roberta_fast.py
==================
e87505f3a;Patrick von Platen;2021-03-31 09:45:58 +0300;[Flax] Add other BERT classes (#10977)
* add first code structures

* add all bert models

* add to init and docs

* correct docs

* make style
==

docs/source/model_doc/bert.rst
src/transformers/__init__.py
src/transformers/models/bert/__init__.py
src/transformers/models/bert/modeling_flax_bert.py
src/transformers/utils/dummy_flax_objects.py
tests/test_modeling_flax_bert.py
tests/test_modeling_flax_common.py
==================
e031162a6;Yih-Dar;2021-03-30 20:26:22 +0200;fix md file to avoid evaluation crash (#10962)

==

examples/research_projects/wav2vec2/FINE_TUNE_XLSR_WAV2VEC2.md
==================
3e09d813a;Philipp Schmid;2021-03-30 19:47:12 +0200;[examples/s2s] added py7zr dep (#10971)
* added py7zr

* comment out check_min for sagemaker test

* added min version again
==

examples/seq2seq/requirements.txt
==================
c32b432a6;Nicolas Patry;2021-03-30 19:26:35 +0200;Fixed a bug where the `pipeline.framework` would actually contain (#10970)
a fully qualified model.

We simply forgot to change the call for this one when this landed:
https://github.com/huggingface/transformers/pull/10888

It's odd that tests didn't catch that. Should we add some ?
(It's a pretty edgy test case, but it does run within the API).
==

src/transformers/pipelines/base.py
==================
e3c8443f0;Philipp Schmid;2021-03-30 18:00:52 +0200;improved sagemaker documentation for git_config and examples (#10966)
* improved branch usage

* fixed grammar and comma
==

docs/source/sagemaker.md
==================
83d38c9ff;Suraj Patil;2021-03-30 20:45:55 +0530;GPT Neo few fixes (#10968)
* fix checkpoint names

* auto model

* fix doc
==

docs/source/model_doc/gpt_neo.rst
docs/source/pretrained_models.rst
src/transformers/models/auto/modeling_auto.py
src/transformers/models/gpt_neo/configuration_gpt_neo.py
src/transformers/models/gpt_neo/modeling_gpt_neo.py
src/transformers/pipelines/text_generation.py
tests/test_modeling_gpt_neo.py
==================
7772ddb47;Patrick von Platen;2021-03-30 17:03:48 +0300;fix big bird gpu test (#10967)

==

tests/test_modeling_big_bird.py
==================
860264379;Suraj Patil;2021-03-30 19:12:30 +0530;GPT Neo (#10848)
* lets begin

* boom boom

* fix out proj in attn

* fix attention

* fix local attention

* add tokenizer

* fix imports

* autotokenizer

* fix checkpoint name

* cleanup

* more clean-up

* more cleanup

* output attentions

* fix attn mask creation

* fix imports

* config doc

* add tests

* add slow tests

* quality

* add conversion script

* copyright

* typo

* another bites the dust

* fix attention tests

* doc

* add embed init in convert function

* fix copies

* remove tokenizer

* enable caching

* address review comments

* improve config and create attn layer list internally

* more consistent naming

* init hf config from mesh-tf config json file

* remove neo tokenizer from doc

* handle attention_mask in local attn layer

* attn_layers => attention_layers

* add tokenizer_class in config

* fix docstring

* raise if len of attention_layers is not same as num_layers

* remove tokenizer_class from config

* more consistent naming

* fix doc

* fix checkpoint names

* fp16 compat

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

README.md
docs/source/index.rst
docs/source/model_doc/gpt_neo.rst
docs/source/pretrained_models.rst
src/transformers/__init__.py
src/transformers/models/__init__.py
src/transformers/models/auto/configuration_auto.py
src/transformers/models/auto/modeling_auto.py
src/transformers/models/gpt_neo/__init__.py
src/transformers/models/gpt_neo/configuration_gpt_neo.py
src/transformers/models/gpt_neo/convert_gpt_neo_mesh_tf_to_pytorch.py
src/transformers/models/gpt_neo/modeling_gpt_neo.py
src/transformers/utils/dummy_pt_objects.py
tests/test_modeling_gpt_neo.py
==================
a04eb8d36;Philipp Schmid;2021-03-30 14:28:58 +0200;Fix summarization notebook link (#10959)

==

notebooks/README.md
==================
8780caa38;Patrick von Platen;2021-03-30 12:13:59 +0300;[WIP][Flax] Add general conversion script (#10809)
* save intermediate

* finish first version

* delete some more

* improve import

* fix roberta

* Update src/transformers/modeling_flax_pytorch_utils.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/modeling_flax_pytorch_utils.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* small corrections

* apply all comments

* fix deterministic

* make fix-copies

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

src/transformers/modeling_flax_pytorch_utils.py
src/transformers/modeling_flax_utils.py
src/transformers/models/bert/modeling_flax_bert.py
src/transformers/models/roberta/modeling_flax_roberta.py
tests/test_modeling_flax_bert.py
tests/test_modeling_flax_common.py
tests/test_modeling_flax_roberta.py
==================
604c08508;Philipp Schmid;2021-03-30 08:28:02 +0200;Sagemaker test (#10925)
* init

* first working test

* added todo for setup.py

* working test for single node multi node ddp and smd

* added tensorflow single node test

* added directory for pytorch and tensorflow due to different requirements.txt

* added directory for pytorch and tensorflow

* added comment for run_glue until it is available

* added output_dir to it

* smaller dataset to make test running faster

* adjust HP and script

* adjusted parameter for tensorflow

* refactored test scripts

* adjusted make file

* init

* first working test

* added todo for setup.py

* working test for single node multi node ddp and smd

* added tensorflow single node test

* added directory for pytorch and tensorflow due to different requirements.txt

* added directory for pytorch and tensorflow

* added comment for run_glue until it is available

* added output_dir to it

* smaller dataset to make test running faster

* adjust HP and script

* adjusted parameter for tensorflow

* refactored test scripts

* adjusted make file

* updated dlc container

* commented in all tests

* added both ecr images

* added new master branches

* debug

* added new datasets version

* init

* strange rebase bug

* removed changes

* changed min version for tests to work

* updated DLC

* added model parallel test

* removed test files

* removed test files

* tested with ned dlc

* added correct sagemaker sdk version

* adjust DLCs for official one

* reworked tests

* quality

* removed default profile added documentation to it

* added step in release for sagemaker tests

* reverted version for example script removed duplicated script and added install from master to requirements.txt

* removed mistaken .DS_Stores from mac

* fixed tests

* added Sylvains feedback

* make style

* added lysandre's feedback
==

Makefile
setup.py
src/transformers/dependency_versions_table.py
tests/sagemaker/README.md
tests/sagemaker/__init__.py
tests/sagemaker/conftest.py
tests/sagemaker/scripts/pytorch/requirements.txt
tests/sagemaker/scripts/pytorch/run_ddp.py
tests/sagemaker/scripts/pytorch/run_glue_model_parallelism.py
tests/sagemaker/scripts/tensorflow/requirements.txt
tests/sagemaker/scripts/tensorflow/run_tf.py
tests/sagemaker/scripts/tensorflow/run_tf_dist.py
tests/sagemaker/test_multi_node_data_parallel.py
tests/sagemaker/test_multi_node_model_parallel.py
tests/sagemaker/test_single_node_gpu.py
==================
6dfd02727;Vasudev Gupta;2021-03-30 11:21:34 +0530;BigBird (#10183)
* init bigbird

* model.__init__ working, conversion script ready, config updated

* add conversion script

* BigBirdEmbeddings working :)

* slightly update conversion script

* BigBirdAttention working :) ; some bug in layer.output.dense

* add debugger-notebook

* forward() working for BigBirdModel :) ; replaced gelu with gelu_fast

* tf code adapted to torch till rand_attn in bigbird_block_sparse_attention ; till now everything working :)

* BigBirdModel working in block-sparse attention mode :)

* add BigBirdForPreTraining

* small fix

* add tokenizer for BigBirdModel

* fix config & hence modeling

* fix base prefix

* init testing

* init tokenizer test

* pos_embed must be absolute, attn_type=original_full when add_cross_attn=True , nsp loss is optional in BigBirdForPreTraining, add assert statements

* remove position_embedding_type arg

* complete normal tests

* add comments to block sparse attention

* add attn_probs for sliding & global tokens

* create fn for block sparse attn mask creation

* add special tests

* restore pos embed arg

* minor fix

* attn probs update

* make big bird fully gpu friendly

* fix tests

* remove pruning

* correct tokenzier & minor fixes

* update conversion script , remove norm_type

* tokenizer-inference test add

* remove extra comments

* add docs

* save intermediate

* finish trivia_qa conversion

* small update to forward

* correct qa and layer

* better error message

* BigBird QA ready

* fix rebased

* add triva-qa debugger notebook

* qa setup

* fixed till embeddings

* some issue in q/k/v_layer

* fix bug in conversion-script

* fixed till self-attn

* qa fixed except layer norm

* add qa end2end test

* fix gradient ckpting ; other qa test

* speed-up big bird a bit

* hub_id=google

* clean up

* make quality

* speed up einsum with bmm

* finish perf improvements for big bird

* remove wav2vec2 tok

* fix tokenizer

* include docs

* correct docs

* add helper to auto pad block size

* make style

* remove fast tokenizer for now

* fix some

* add pad test

* finish

* fix some bugs

* fix another bug

* fix buffer tokens

* fix comment and merge from master

* add comments

* make style

* commit some suggestions

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Fix typos

* fix some more suggestions

* add another patch

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* fix copies

* another path

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* update

* update nit suggestions

* make style

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
==

README.md
docs/source/index.rst
docs/source/model_doc/bigbird.rst
src/transformers/__init__.py
src/transformers/models/__init__.py
src/transformers/models/auto/configuration_auto.py
src/transformers/models/auto/modeling_auto.py
src/transformers/models/auto/tokenization_auto.py
src/transformers/models/big_bird/__init__.py
src/transformers/models/big_bird/configuration_big_bird.py
src/transformers/models/big_bird/convert_bigbird_original_tf_checkpoint_to_pytorch.py
src/transformers/models/big_bird/modeling_big_bird.py
src/transformers/models/big_bird/tokenization_big_bird.py
src/transformers/utils/dummy_pt_objects.py
src/transformers/utils/modeling_auto_mapping.py
tests/test_modeling_big_bird.py
tests/test_tokenization_big_bird.py
==================
700229f8a;Sylvain Gugger;2021-03-29 17:36:13 -0400;Fixes in the templates (#10951)
* Fixes in the templates

* Define in all cases

* Dimensionality -> Dimension

Co-authored-by: Lysandre <lysandre.debut@reseau.eseo.fr>
==

templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/configuration_{{cookiecutter.lowercase_modelname}}.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_tf_{{cookiecutter.lowercase_modelname}}.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_{{cookiecutter.lowercase_modelname}}.py
==================
05c966f24;Stas Bekman;2021-03-29 14:25:47 -0700;[vulnerability] dep fix (#10954)
Fixes https://github.com/huggingface/transformers/security/dependabot/examples/research_projects/lxmert/requirements.txt/Pygments/open

@LysandreJik
==

examples/research_projects/lxmert/requirements.txt
==================
fb7fca718;Stas Bekman;2021-03-29 13:47:02 -0700;[trainer metrics] fix cpu mem metrics; reformat runtime metric (#10937)
* fix cpu mem metrics; reformat runtime metric

* adjust dependency

* extend docs

* soft dependency

* cleanup

* fix the runtime metric issue

* restore

* move docs, cross reference from 2 places, improve

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

src/transformers/trainer_pt_utils.py
src/transformers/trainer_utils.py
==================
5057213bc;Daniel Stancl;2021-03-29 22:41:09 +0200;Add `examples/multiple-choice/run_swag_no_trainer.py` (#10934)
* Initial commit

* Another bunch of updates

* make style quliaty + delete debug arg from bash script

* Use compue_metrics func

* Do a few fixes

* Add copyright

* Fix typos
==

examples/multiple-choice/README.md
examples/multiple-choice/run_no_trainer.sh
examples/multiple-choice/run_swag_no_trainer.py
==================
ae6b6963a;pcuenca;2021-03-29 21:44:19 +0200;Allow use of pre-computed lengths when grouping by length. (#10953)
A new argument `length_column_name` has been added to
`TrainingArguments`, with default value `"length"`. If this column
exists and `group_by_length` is `True`, the train sampler will use
it for grouping rather than computing it before training starts.

This is an optimization that allows the user to prepare data for fast
processing, preventing sequential access to the dataset as described in
issue #10909.
==

src/transformers/trainer.py
src/transformers/training_args.py
==================
4002f95eb;Sylvain Gugger;2021-03-29 15:27:12 -0400;Remove duplicate code

==

examples/token-classification/run_ner_no_trainer.py
==================
d7b50ce46;Daniel Stancl;2021-03-29 21:11:23 +0200;Add `examples/run_ner_no_trainer.py` (#10902)
* Add NER example with accelerate library

* This commit contains the first (yet really unfinished)
version of a script for showing how to train HuggingFace model
with their new accelerate library.

* Fix metric calculation

* make style quality

* mv ner_no_trainer to token-classification dir

* Delete --debug flag from running script

* hf_datasets -> raw_datasets

* Make a few slight adjustments

* Add an informative comment + rewrite a help comment

* Change header

* Fix a few things

* Enforce to use fast tokenizers only

* DataCollatorWithPadding -> DataCollatorForTokenClassification

* Change bash script: python3 -> accelerate launch

* make style

* Add a few missing things (see below)

* Add a max-lenghth padding to predictions and labels to
enable accelerate gather functionality

* Add PyTorch no trainer example to the example README.md

* Remove --do-train from args as being redundant for now

* DataCollatorWithPadding -> DataCollatorForTokenClassification

* Remove some obsolete args.do_train conditions from the script

* Delete --do_train from bash running script

* Delete use_slow_tokenizer from args

* Add unintentionally removed flag --label_all_tokens

* Delete --debug flag from running script
==

examples/token-classification/README.md
examples/token-classification/run_ner_no_trainer.py
examples/token-classification/run_no_trainer.sh
==================
06a6fea78;Sylvain Gugger;2021-03-29 10:39:14 -0400;Instantiate model only once in pipeline (#10888)
* Instantiate model only once in pipeline

* Remove documentation of deprecated method

* Add FutureWarning

* Update src/transformers/pipelines/base.py

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
==

docs/source/internal/pipelines_utils.rst
src/transformers/pipelines/__init__.py
src/transformers/pipelines/base.py
==================
cc2366bbb;Masatoshi Suzuki;2021-03-29 23:26:15 +0900;Ignore not initialized NO_CONFIG_TOKENIZERs (#10936)

==

src/transformers/models/auto/tokenization_auto.py
==================
ddea8771c;WybeKoper;2021-03-29 14:47:09 +0200;Updated colab links in readme of examples (#10932)
Co-authored-by: WybeKoper <WybeKoper@users.noreply.github.com>
==

examples/README.md
==================
b3544e4cc;Guillaume Filion;2021-03-29 08:00:23 -0400;Return global attentions (see #7514) (#10906)

==

src/transformers/models/longformer/modeling_longformer.py
==================
4f21e1ddd;Bhadresh Savani;2021-03-28 22:18:12 +0530;fixed finename (#10939)

==

examples/question-answering/utils_qa.py
==================
b0595d33c;Sylvain Gugger;2021-03-26 11:23:56 -0400;Add ImageFeatureExtractionMixin (#10905)
* Add ImageFeatureExtractionMixin

* Add dummy vision objects

* Add require_vision

* Add tests

* Fix test
==

docs/source/main_classes/feature_extractor.rst
src/transformers/__init__.py
src/transformers/file_utils.py
src/transformers/image_utils.py
src/transformers/testing_utils.py
src/transformers/utils/dummy_vision_objects.py
tests/test_image_utils.py
utils/check_dummies.py
==================
3c27d246e;Stas Bekman;2021-03-26 06:06:11 -0700;[vulnerability] fix dependency (#10914)
this PR fixes https://github.com/huggingface/transformers/security/dependabot/examples/research_projects/lxmert/requirements.txt/PyYAML/open
==

examples/research_projects/lxmert/requirements.txt
==================
4b2b50aa7;Tomy Hsieh;2021-03-26 20:07:59 +0800;Rename NLP library to Datasets library (#10920)
* Rename NLP library to Datasets library

* Update github template

* Fix styling
==

.github/ISSUE_TEMPLATE/bug-report.md
.github/PULL_REQUEST_TEMPLATE.md
docs/source/custom_datasets.rst
==================
86c6f8a8b;lexhuismans;2021-03-25 19:23:56 +0100;Fix comment (#10886)

==

src/transformers/models/t5/modeling_t5.py
==================
9856c9213;Sylvain Gugger;2021-03-25 12:51:43 -0400;Reorder init imports

==

src/transformers/__init__.py
==================
e70068a71;Sylvain Gugger;2021-03-25 12:40:25 -0400;Fix typo

==

src/transformers/__init__.py
==================
f183a7a3c;Sylvain Gugger;2021-03-25 12:38:54 -0400;Sort init imports

==

src/transformers/__init__.py
src/transformers/models/layoutlm/__init__.py
==================
4684bfc75;Amir Tahmasbi;2021-03-25 09:32:38 -0700;Layout lm tf 2 (#10636)
* Added embeddings layer

* Added layoutlm layers, main model, maskedlm and token classification classes

* Added model classes to tf auto models

* Added model to PT to TF conversion script

* Added model to doc README

* Added tests

* Removed unused imports

* Added layoutlm model, test, and doc for sequence classification, and fix imports in __init__.py

* Made tests pass!

* Fixed typos in imports and docs

* Fixed a typo in embeddings layer

* Removed imports

* Fixed formatting issues, imports, tests

* Added layoutlm layers, main model, maskedlm and token classification classes

* Added model classes to tf auto models

* Added model to PT to TF conversion script

* Removed unused imports

* Added layoutlm model, test, and doc for sequence classification, and fix imports in __init__.py

* Made tests pass!

* Fixed typos in imports and docs

* Removed imports

* Fixed small formatting issues

* Removed duplicates import from main __init__.py

* Chnaged deafult arg to true for adding  pooling layer to tf layoutlm

* Fixed formatting issues

* Style

* Added copied from to classes copied from bert

* Fixed doc strings examples to work with layoutlm inputs

* Removed PyTorch reference in doc strings example

* Added integration tests

* Cleaned up initialization file

* Updated model checkpoint identifiers

* Fixed imports

Co-authored-by: Amir Tahmasbi <amir@ehsai.ca>
Co-authored-by: Lysandre <lysandre.debut@reseau.eseo.fr>
==

docs/source/index.rst
docs/source/model_doc/layoutlm.rst
src/transformers/__init__.py
src/transformers/convert_pytorch_checkpoint_to_tf2.py
src/transformers/modeling_tf_pytorch_utils.py
src/transformers/models/auto/modeling_tf_auto.py
src/transformers/models/layoutlm/__init__.py
src/transformers/models/layoutlm/modeling_tf_layoutlm.py
src/transformers/utils/dummy_tf_objects.py
tests/test_modeling_tf_layoutlm.py
==================
1a3e0c4fe;Philipp Schmid;2021-03-25 14:01:31 +0100;make local setup more clearer and added missing links (#10899)

==

docs/source/sagemaker.md
==================
5f1491d3b;Jethro Kuan;2021-03-25 20:28:17 +0800;run_glue_no_trainer: datasets -> raw_datasets (#10898)
Use the correct variable (raw_datasets) instead of the module (datasets)
where appropriate.
==

examples/text-classification/run_glue_no_trainer.py
==================
1c06240e1;Sidd Karamcheti;2021-03-24 13:44:51 -0700;Update training args ignore_skip_data -> ignore_data_skip (#10891)

==

src/transformers/training_args.py
==================
3b20e910b;Sylvain Gugger;2021-03-24 15:21:40 -0400;Remove version warning in pretrained BART models (#10890)
* Remove version warning in pretrained BART models

* Put it at the base model
==

src/transformers/models/bart/modeling_bart.py
==================
3c12e3c1c;Lysandre Debut;2021-03-24 15:13:56 -0400;Fix overflowing bad word ids (#10889)
* Removes overflowing bad word IDs

* Raise warning
==

src/transformers/generation_logits_process.py
==================
1f5ea9e04;Eliza Szczechla;2021-03-24 16:03:37 +0100;Add notebook on fine-tuning Bart (#10883)
Co-authored-by: Eliza <eliza@habanero.tiger.com.pl>
==

docs/source/community.md
==================
f81077fcf;imzhengzx;2021-03-24 23:00:14 +0800;error type of tokenizer in __init__ definition (#10879)
the orignal code in line 246 is
```
tokenizer: Optional["PreTrainedTokenizerBase"] = None,
```

it should be
```
tokenizer: Optional[PreTrainedTokenizerBase] = None,
```
==

src/transformers/trainer.py
==================
1aed2b908;Sylvain Gugger;2021-03-24 09:45:08 -0400;Add new notebook links in the docs (#10876)

==

notebooks/README.md
==================
a735f727c;Sylvain Gugger;2021-03-23 19:03:06 -0400;Fix test_trainer_distributed (#10875)

==

tests/test_trainer_distributed.py
==================
8c297cdb3;Philipp Schmid;2021-03-23 20:07:55 +0100;Sm trainer smp init fix (#10870)
* rewrote is_sagemaker_model_parallel_available

* added is_sagemaker_model_parallel_available to SageMakerTrainer

* removed unnecessary mp_parameters as TrainingArguments

* make style happy

* added mp_parameters again to parse mp-specific args.
==

src/transformers/sagemaker/trainer_sm.py
src/transformers/sagemaker/training_args_sm.py
==================
d4d4447d5;RafaelWO;2021-03-23 18:48:22 +0100;fixed prefix_allowed_tokens_fn docstring in generate() (#10862)

==

src/transformers/generation_utils.py
==================
7ef40120a;Bhadresh Savani;2021-03-23 23:07:59 +0530;[Examples] Added predict stage and Updated Example Template (#10868)
* added predict stage

* added test keyword in exception message

* removed example specific saving predictions

* fixed f-string error

* removed extra line

Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>

Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>
==

examples/text-classification/run_xnli.py
templates/adding_a_new_example_script/{{cookiecutter.directory_name}}/run_{{cookiecutter.example_shortcut}}.py
==================
fb2b89840;Stas Bekman;2021-03-23 09:41:41 -0700;[file_utils] import refactor (#10859)
* import refactor

* fix the fallback
==

src/transformers/file_utils.py
==================
3f48b2bc3;Lysandre;2021-03-23 11:01:16 -0400;Update stable docs

==

.circleci/deploy.sh
==================
77ffd5edd;Philipp Schmid;2021-03-23 15:56:44 +0100;Amazon SageMaker Documentation (#10867)
* added finished documentation

* changed version from 1.6 to 1.6.0 for distributed

* updated versions

* updated urls
==

docs/source/index.rst
docs/source/sagemaker.md
==================
bf1f43fbd;Sylvain Gugger;2021-03-23 10:02:39 -0400;Update the example template for a no Trainer option (#10865)

==

templates/adding_a_new_example_script/cookiecutter.json
templates/adding_a_new_example_script/{{cookiecutter.directory_name}}/run_{{cookiecutter.example_shortcut}}.py
==================
2eb596f08;Marta Ma≈õlankowska;2021-03-23 14:08:39 +0100;Fix p_mask cls token masking in qa pipeline (#10863)

==

src/transformers/pipelines/question_answering.py
==================
eb330e890;Bhadresh Savani;2021-03-23 17:45:28 +0530;fixed typo (#10861)

==

src/transformers/trainer.py
==================
e21f89f64;Stas Bekman;2021-03-22 19:23:24 -0700;fix nan in full-fp16 label_smoothing eval (#10815)

==

src/transformers/trainer_pt_utils.py
==================
b5b957a65;Sylvain Gugger;2021-03-22 22:16:39 -0400;Make convert_to_onnx runable as script again (#10857)

==

src/transformers/convert_graph_to_onnx.py
==================
77bf3fe78;Patrick von Platen;2021-03-23 01:00:05 +0300;[Generate] Add save mode logits processor to remove nans and infs if necessary (#10769)
* push

* finish

* finish

* make fix copies

* change name
==

docs/source/internal/generation_utils.rst
src/transformers/__init__.py
src/transformers/configuration_utils.py
src/transformers/generation_logits_process.py
src/transformers/generation_utils.py
src/transformers/models/rag/modeling_rag.py
src/transformers/utils/dummy_pt_objects.py
tests/test_generation_logits_process.py
tests/test_generation_utils.py
==================
9f8fa4e97;Eliza Szczechla;2021-03-22 20:05:39 +0100;Use DataCollatorForSeq2Seq in run_summarization in all cases (#10856)
Co-authored-by: Eliza <eliza@habanero.tiger.com.pl>
==

examples/seq2seq/run_summarization.py
==================
a8d4d6776;Ruan Chaves;2021-03-22 15:04:51 -0300;Modify the Trainer class to handle simultaneous execution of Ray Tune and Weights & Biases (#10823)
* Modify the _hp_search_setup method on the Trainer class to handle the wandb argument passed by Ray Tune to model config.

* Reformat single quotes as double quotes.
==

src/transformers/trainer.py
==================
125ccead7;Boris Dayma;2021-03-22 09:45:17 -0500;feat(wandb): logging and configuration improvements (#10826)
* feat: ensure unique artifact id

* feat: allow manual init

* fix: simplify reinit logic

* fix: no dropped value + immediate commits

* fix: wandb use in sagemaker

* docs: improve documenation and formatting

* fix: typos

* docs: improve formatting
==

examples/README.md
src/transformers/integrations.py
==================
b230181d4;Sidd Karamcheti;2021-03-22 06:15:39 -0700;Add simple one character fix so that on_step_begin and on_step_end are called at the right times (#10839)

==

src/transformers/trainer.py
==================
24ab5b08a;Stas Bekman;2021-03-22 06:14:22 -0700;[makefile] autogenerate target (#10814)
* autogenerate target

* clarify comment
==

Makefile
==================
2c6684239;Sebastian Olsson;2021-03-22 14:12:44 +0100;Correct AutoConfig call docstrings (#10822)

==

hubconf.py
src/transformers/models/auto/modeling_auto.py
src/transformers/models/auto/modeling_tf_auto.py
==================
8fb467181;Stas Bekman;2021-03-22 06:05:24 -0700;[vulnerability] in example deps fix (#10817)
Takes care of:
https://github.com/huggingface/transformers/security/dependabot/examples/research_projects/lxmert/requirements.txt/jinja2/open

@LysandreJik

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
==

examples/research_projects/lxmert/requirements.txt
==================
dbfe37951;dependabot[bot];2021-03-22 08:54:50 -0400;Bump jinja2 from 2.11.2 to 2.11.3 in /examples/research_projects/lxmert (#10818)
Bumps [jinja2](https://github.com/pallets/jinja) from 2.11.2 to 2.11.3.
- [Release notes](https://github.com/pallets/jinja/releases)
- [Changelog](https://github.com/pallets/jinja/blob/master/CHANGES.rst)
- [Commits](https://github.com/pallets/jinja/compare/2.11.2...2.11.3)

Signed-off-by: dependabot[bot] <support@github.com>

Co-authored-by: dependabot[bot] <49699333+dependabot[bot]@users.noreply.github.com>
==

examples/research_projects/lxmert/requirements.txt
==================
29904a967;Qiushi Pan;2021-03-22 20:58:59 +0900;Update FINE_TUNE_XLSR_WAV2VEC2.md (#10849)
Fix typo.
==

examples/research_projects/wav2vec2/FINE_TUNE_XLSR_WAV2VEC2.md
==================
0f226f78c;Patrick von Platen;2021-03-22 10:32:21 +0300;push (#10846)

==

examples/research_projects/wav2vec2/FINE_TUNE_XLSR_WAV2VEC2.md
==================
82b8d8c7b;Suraj Patil;2021-03-21 22:47:09 +0530;Update FINE_TUNE_XLSR_WAV2VEC2.md

==

examples/research_projects/wav2vec2/FINE_TUNE_XLSR_WAV2VEC2.md
==================
af6125ffd;Patrick von Platen;2021-03-21 12:31:33 +0300;Update FINE_TUNE_XLSR_WAV2VEC2.md

==

examples/research_projects/wav2vec2/FINE_TUNE_XLSR_WAV2VEC2.md
==================
5aaf6e146;Patrick von Platen;2021-03-21 11:41:44 +0300;small improvements for wav2vec2 info script (#10829)

==

examples/research_projects/wav2vec2/FINE_TUNE_XLSR_WAV2VEC2.md
==================
be87b8427;Eric Lam;2021-03-21 15:59:53 +0800;Add new community notebook - wav2vec2 with GPT (#10794)
* Add new community notebook - wav2vec2 with GPT

* Update:community.md, new nb add
* feat: notebook of wav2vec xlsr ctc decoding with gpt logit adjustment
* Update: Wav2vec2 CTC decoding with gpt2 adjustment

* Update docs/source/community.md

Co-authored-by: Suraj Patil <surajp815@gmail.com>
==

docs/source/community.md
==================
68b55885e;Suraj Patil;2021-03-21 13:25:34 +0530;add doc for Local machine (#10828)

==

examples/research_projects/wav2vec2/FINE_TUNE_XLSR_WAV2VEC2.md
==================
21e86f99e;Sylvain Gugger;2021-03-19 16:17:13 -0400;Sort init import (#10801)
* Initial script

* Add script to properly sort imports in init.

* Add to the CI

* Update utils/custom_init_isort.py

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* Separate scripts that change content from quality

* Move class_mapping_update to style_checks

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
==

.circleci/config.yml
Makefile
src/transformers/__init__.py
src/transformers/models/blenderbot/__init__.py
src/transformers/models/blenderbot_small/__init__.py
src/transformers/models/deberta/__init__.py
src/transformers/models/deberta_v2/__init__.py
src/transformers/models/ibert/__init__.py
src/transformers/models/marian/__init__.py
src/transformers/models/mbart/__init__.py
src/transformers/models/pegasus/__init__.py
src/transformers/models/speech_to_text/__init__.py
src/transformers/models/wav2vec2/__init__.py
src/transformers/utils/dummy_tf_objects.py
utils/custom_init_isort.py
==================
1438c487d;Julien Chaumond;2021-03-19 17:48:54 +0100;wav2vec doc tweaks (#10808)
* wording/typos tweaks

* Make model upload instructions simpler
==

examples/research_projects/wav2vec2/FINE_TUNE_XLSR_WAV2VEC2.md
==================
b9570a813;Patrick von Platen;2021-03-19 19:45:28 +0300;Update FINE_TUNE_XLSR_WAV2VEC2.md

==

examples/research_projects/wav2vec2/FINE_TUNE_XLSR_WAV2VEC2.md
==================
f2b744f69;Philipp Schmid;2021-03-19 16:26:32 +0100;Add transformers id to hub requests (#10811)
* add uuid.hext to user_agent

* add log

* changed order of it

* renamed as session id

* renamed variable

* reverted naming of the const
==

src/transformers/file_utils.py
==================
946400fb6;Sylvain Gugger;2021-03-19 10:06:08 -0400;Expand a bit the presentation of examples (#10799)
* Expand a bit the presentation of examples

* Apply suggestions from code review

Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>

* Address review comments

Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>
==

examples/README.md
examples/language-modeling/README.md
examples/multiple-choice/README.md
examples/question-answering/README.md
examples/seq2seq/README.md
examples/token-classification/README.md
==================
fd1d9f1ab;Bhadresh Savani;2021-03-19 19:12:17 +0530;[Example] Updating Question Answering examples for Predict Stage (#10792)
* added prediction stage and eval fix

* style correction

* removed extra lines
==

examples/question-answering/run_qa.py
examples/question-answering/run_qa_beam_search.py
examples/question-answering/trainer_qa.py
examples/question-answering/utils_qa.py
==================
e8968bd03;Patrick von Platen;2021-03-19 12:52:54 +0300;[XLSR-Wav2Vec2 Info doc] Add a couple of lines (#10806)
* finish

* fix

* fix

* fix

* fix
==

examples/research_projects/wav2vec2/FINE_TUNE_XLSR_WAV2VEC2.md
==================
117dba994;Th√©o Matussi√®re;2021-03-19 03:13:45 +0100;fix backend tokenizer args override: key mismatch (#10686)
* fix backend tokenizer args override: key mismatch

* no touching the docs

* fix mpnet

* add mpnet to test

* fix test

Co-authored-by: theo <theo@matussie.re>
==

src/transformers/models/bert/tokenization_bert_fast.py
src/transformers/models/mpnet/tokenization_mpnet_fast.py
tests/test_tokenization_auto.py
==================
427ea3fec;Stas Bekman;2021-03-18 19:02:10 -0700;addressing vulnerability report in research project deps (#10802)
Following up on a security alert:
https://github.com/huggingface/transformers/security/dependabot/examples/research_projects/lxmert/requirements.txt/Pillow/open
==

examples/research_projects/lxmert/requirements.txt
==================
2ae678229;Patrick von Platen;2021-03-19 00:29:20 +0300;Update FINE_TUNE_XLSR_WAV2VEC2.md

==

examples/research_projects/wav2vec2/FINE_TUNE_XLSR_WAV2VEC2.md
==================
68a321594;Patrick von Platen;2021-03-19 00:27:40 +0300;Update FINE_TUNE_XLSR_WAV2VEC2.md

==

examples/research_projects/wav2vec2/FINE_TUNE_XLSR_WAV2VEC2.md
==================
03df3fbcb;Patrick von Platen;2021-03-19 00:26:49 +0300;Update FINE_TUNE_XLSR_WAV2VEC2.md

==

examples/research_projects/wav2vec2/FINE_TUNE_XLSR_WAV2VEC2.md
==================
e84adbed4;Patrick von Platen;2021-03-19 00:22:43 +0300;Add XLSR-Wav2Vec2 Fine-Tuning README.md (#10786)
* upload

* upload fine-tuning script

* improve

* adapt

* Apply suggestions from code review

* correct

* upload

* finalize

* remove @

* correct typos
==

examples/research_projects/wav2vec2/FINE_TUNE_XLSR_WAV2VEC2.md
==================
dcebe254f;Sylvain Gugger;2021-03-18 15:19:25 -0400;Document v4.4.2

==

.circleci/deploy.sh
docs/source/_static/js/custom.js
==================
008672e6e;Sylvain Gugger;2021-03-18 13:12:04 -0400;Fix distributed evaluation (#10795)
* Fix distributed evaluation

* Use logger
==

src/transformers/trainer.py
tests/test_trainer_distributed.py
==================
9352b5151;Stas Bekman;2021-03-18 09:55:39 -0700;[examples/seq2seq/README.md] fix t5 examples (#10734)
* [examples/seq2seq] fix t5 examples

This PR:
* fixes T5 examples to include `--source_prefix` - it's **not** optional. If you give it a try you will see that you get 10x worse bleu scores w/o it. w/ `27.6849`, w/ `2.374`
* added a normal translation example w/o the peculiarities of MBart and T5
* reduces the default max samples to 50 so it's much faster to test quickly

summarization seems to be broken for t5 score-wise: https://github.com/huggingface/transformers/issues/10733

@sgugger

* specify explicitly the t5 models requiring the special handling

* one more

* update the t5 summarization example to use cnn_dailymail

* move max*samples into the top level README.md

* better wording

* better wording
==

examples/README.md
examples/seq2seq/README.md
==================
094afa515;Vimarsh Chaturvedi;2021-03-18 22:21:42 +0530; from_pretrained: check that the pretrained model is for the right model architecture (#10586)
* Added check to ensure model name passed to from_pretrained and model are the same

* Added test to check from_pretrained throws assert error when passed an incompatiable model name

* Modified assert in from_pretrained with f-strings. Modified test to ensure desired assert message is being generated

* Added check to ensure config and model has model_type

* Fix FlauBERT heads

Co-authored-by: vimarsh chaturvedi <vimarsh chaturvedi>
Co-authored-by: Stas Bekman <stas@stason.org>
Co-authored-by: Lysandre <lysandre.debut@reseau.eseo.fr>
==

src/transformers/configuration_utils.py
src/transformers/models/flaubert/modeling_tf_flaubert.py
tests/test_modeling_common.py
==================
4f3e93cfa;Julien Chaumond;2021-03-18 17:37:45 +0100;[file_utils] do not gobble certain kinds of requests.ConnectionError (#10235)
* do not gobble certain kinds of requests.ConnectionError

* Apply review comments

Co-authored-by: Lysandre <lysandre.debut@reseau.eseo.fr>
==

examples/legacy/token-classification/run_tf_ner.py
src/transformers/file_utils.py
==================
ce9724e1b;James Thomin;2021-03-18 09:25:57 -0500;Fix bug in input check for LengthGroupSampler (#10783)
This commit fixes a bug in the LengthGroupSampler where if
model_input_name is not set, the default value is None instead of
"input_ids"
==

src/transformers/trainer_pt_utils.py
==================
5f19c07a7;Suraj Patil;2021-03-18 17:21:16 +0530;add run_common_voice script (#10767)
* add initial script

* finish script

* add shell script example

* accept chars_to_ignor as cl arg

* align the script with other example scripts

* add torchaudio dep
==

examples/research_projects/wav2vec2/finetune_wav2vec2_xlsr_turkish.sh
examples/research_projects/wav2vec2/requirements.txt
examples/research_projects/wav2vec2/run_common_voice.py
==================
af8afdc88;Mohamed El-Geish;2021-03-18 00:20:26 -0700;wav2vec2: support datasets other than LibriSpeech (#10581)
* wav2vec2: support datasets other than LibriSpeech

* Formatting run_asr.py to pass code quality test

* bundled orthography options and added verbose logs

* fixing a typo in timit fine-tuning script

* update comment for clarity

* resize_lm_head and load custom vocab from file

* adding a max_duration_in_seconds filter

* do not assign `duration_filter` lambda, use a def

* log untransliterated text as well

* fix base model for arabic

* fix duration filter when target_sr is not set

* drop duration_in_seconds when unneeded

* script for wav2vec2-large-lv60-timit-asr

* fix for "tha" in arabic corpus (huggingface#10581)

* adding more options to work with common_voice

* PR feedback (huggingface#10581)

* small README change
==

examples/research_projects/wav2vec2/README.md
examples/research_projects/wav2vec2/finetune_base_timit_asr.sh
examples/research_projects/wav2vec2/finetune_large_lv60_timit_asr.sh
examples/research_projects/wav2vec2/finetune_large_xlsr_53_arabic_speech_corpus.sh
examples/research_projects/wav2vec2/requirements.txt
examples/research_projects/wav2vec2/run_asr.py
examples/research_projects/wav2vec2/vocab/buckwalter.json
src/transformers/models/wav2vec2/tokenization_wav2vec2.py
==================
0b98ca368;Patrick von Platen;2021-03-18 09:44:17 +0300;[Flax] Adapt Flax models to new structure (#9484)
* Create modeling_flax_eletra with code copied from modeling_flax_bert

* Add ElectraForMaskedLM and ElectraForPretraining

* Add modeling test for Flax electra and fix naming and arg in Flax Electra model

* Add documentation

* Fix code style

* Create modeling_flax_eletra with code copied from modeling_flax_bert

* Add ElectraForMaskedLM and ElectraForPretraining

* Add modeling test for Flax electra and fix naming and arg in Flax Electra model

* Add documentation

* Fix code style

* Fix code quality

* Adjust tol in assert_almost_equal due to very small difference between model output, ranging 0.0010 - 0.0016

* Remove redundant ElectraPooler

* save intermediate

* adapt

* correct bert flax design

* adapt roberta as well

* finish roberta flax

* finish

* apply suggestions

* apply suggestions

Co-authored-by: Chris Nguyen <anhtu2687@gmail.com>
==

src/transformers/models/bert/modeling_flax_bert.py
src/transformers/models/roberta/modeling_flax_roberta.py
tests/test_modeling_flax_common.py
==================
5c0bf3978;Funtowicz Morgan;2021-03-18 01:25:47 +0100;Add support for detecting intel-tensorflow version (#10781)
Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>
==

src/transformers/file_utils.py
==================
0282e24ee;Mansi Mane;2021-03-17 16:18:11 -0700;Smmp batch not divisible by microbatches fix (#10778)
* Added debug prints

* Added config

* Added prints

* Added prints

* Added extra samples to SequentialDistributedSampler

* Added extra samples to SequentialDistributedSampler

Updated SequentialDistributedSampler call

* Added deubg prints

* Removed extra prints

* Making predicitons and labels multiple of batchsize

* updated number of microbatches

* Removed extra prints

* Made start_remainder similar to DistributedSamplerWithLoop

* Minor spacing update

* Added debug prints

Added config

Added prints

Added prints

* Added extra samples to SequentialDistributedSampler

Updated SequentialDistributedSampler call

Added extra samples to SequentialDistributedSampler

Added deubg prints

Removed extra prints

Making predicitons and labels multiple of batchsize

updated number of microbatches

Removed extra prints

Squashing redundant commits

* Made start_remainder similar to DistributedSamplerWithLoop

Minor spacing update

Made start_remainder similar to DistributedSamplerWithLoop

* Test and styling

* Rename test

Co-authored-by: Sylvain Gugger <sylvain.gugger@gmail.com>
==

src/transformers/sagemaker/trainer_sm.py
src/transformers/trainer.py
src/transformers/trainer_pt_utils.py
tests/test_trainer_utils.py
==================
40b049c70;Sylvain Gugger;2021-03-17 18:11:20 -0400;Check copies blackify (#10775)
* Apply black before checking copies

* Fix for class methods

* Deal with lonely brackets

* Remove debug and add forward changes

* Separate copies and fix test

* Add black as a test dependency
==

setup.py
src/transformers/models/m2m_100/modeling_m2m_100.py
src/transformers/models/mobilebert/modeling_mobilebert.py
src/transformers/models/roberta/modeling_roberta.py
src/transformers/models/speech_to_text/modeling_speech_to_text.py
tests/test_utils_check_copies.py
utils/check_copies.py
==================
393739194;Stas Bekman;2021-03-17 12:48:35 -0700;[examples] document resuming  (#10776)
* document resuming in examples

* fix

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* put trainer code last, adjust notes

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

examples/README.md
==================
85a114ef4;Stas Bekman;2021-03-17 11:33:14 -0700;[Issue template] need to update/extend who to tag (#10728)
* [Issue template] need to update/extend who to tag

1. need to update who to tag for `tensorflow`
2. also requesting to add someone to tag for models hub issues - perhaps separate sub-entries for UI and code - e.g. I don't know who to tag for broken models: https://github.com/huggingface/transformers/issues/10726

Thanks.

* model hub instructions

* s/jplu/LysandreJik/
==

.github/ISSUE_TEMPLATE/bug-report.md
.github/PULL_REQUEST_TEMPLATE.md
==================
3318c246f;Stas Bekman;2021-03-17 11:16:37 -0700;make failure to find a resume checkpoint fatal + tests (#10777)

==

src/transformers/trainer.py
tests/test_trainer.py
==================
cd8c93f70;Stas Bekman;2021-03-17 10:22:58 -0700;[DeepSpeed] improve checkpoint loading code plus tests (#10760)
* deepspeed checkpoint loading code plus tests

* style

* style
==

examples/tests/deepspeed/test_deepspeed.py
src/transformers/integrations.py
src/transformers/trainer.py
tests/test_trainer.py
==================
01c7fb04b;Stas Bekman;2021-03-17 10:21:03 -0700;[DeepSpeed] simplify init (#10762)

==

src/transformers/integrations.py
==================
0486ccdd3;Patrick von Platen;2021-03-17 18:10:17 +0300;small improvements (#10773)

==

tests/test_modeling_wav2vec2.py
==================
d7e0d59bb;Sylvain Gugger;2021-03-17 11:03:43 -0400;Fix URLs

==

src/transformers/models/xlm/tokenization_xlm.py
==================
8715d20c9;Stas Bekman;2021-03-17 06:23:38 -0700;[doc] [testing] extend the pytest -k section with more examples (#10761)
* [doc] [testing] extend -k section

This PR adds more examples on using `pytest -k` - I always forget that I want to use `-k A OR B` when I want several tests - I keep trying AND and it doesn't match any.

* style
==

docs/source/testing.rst
==================
f20d75a13;Patrick von Platen;2021-03-17 16:15:14 +0300;up (#10771)

==

tests/test_modeling_prophetnet.py
==================
c83fbc5f2;Cheng Li;2021-03-16 15:51:09 -0700;[Deepspeed] Allow HF optimizer and scheduler to be passed to deepspeed  (#10464)
* pass hf optimizer and scheduler to deepspeed if not specified in ds config

* pass hf optimizer and scheduler to deepspeed if not specified in ds config

* update

* make init_deepspeed support config dict

* fix docstring formatting

* clean up trainer's comments

* add new tests

* fix type

* composit argparse doesn't work

* style

* add a new test, rename others

* document new functionality

* complete tests, add docs

* style

* correct level

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* add new methods to the doc

* must tell DS we are using a non-native optimizer

* add protection against cpu_offload + HF optimizer combo

* fix the cli overrides

* sync docs + tests

* restore AdamW

* better docs

* need new version

* no longer needed

* remove outdate information

* refactor duplicated code

Co-authored-by: Stas Bekman <stas@stason.org>
Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

docs/source/main_classes/trainer.rst
examples/tests/deepspeed/test_deepspeed.py
src/transformers/integrations.py
src/transformers/testing_utils.py
src/transformers/trainer.py
src/transformers/training_args.py
==================
c23248443;Lysandre Debut;2021-03-16 15:58:20 -0400;Patches full import failure when sentencepiece is not installed (#10752)
* Patches full import failure when sentencepiece is not installed

* Dummies :)
==

src/transformers/__init__.py
src/transformers/utils/dummy_sentencepiece_objects.py
==================
73fe40898;Lysandre;2021-03-16 15:41:49 -0400;Docs for v4.4.1

==

.circleci/deploy.sh
docs/source/_static/js/custom.js
==================
2097aa182;Lysandre Debut;2021-03-16 15:37:52 -0400;Patches the full import failure and adds a test (#10750)
* Patches the full import failure and adds a test

* Add comment
==

src/transformers/__init__.py
src/transformers/models/ibert/__init__.py
tests/test_file_utils.py
==================
1b5ce1e63;Lysandre;2021-03-16 11:41:15 -0400;Development on v4.5.0dev0

==

.circleci/deploy.sh
docs/source/_static/js/custom.js
docs/source/conf.py
examples/language-modeling/run_clm.py
examples/language-modeling/run_mlm.py
examples/language-modeling/run_plm.py
examples/multiple-choice/run_swag.py
examples/question-answering/run_qa.py
examples/question-answering/run_qa_beam_search.py
examples/seq2seq/run_summarization.py
examples/seq2seq/run_translation.py
examples/text-classification/run_glue.py
examples/text-classification/run_xnli.py
examples/token-classification/run_ner.py
setup.py
src/transformers/__init__.py
==================
c988db5af;Lysandre;2021-03-16 11:33:35 -0400;Release v4.4.0

==

README.md
docs/source/conf.py
docs/source/index.rst
examples/language-modeling/run_clm.py
examples/language-modeling/run_mlm.py
examples/language-modeling/run_plm.py
examples/multiple-choice/run_swag.py
examples/question-answering/run_qa.py
examples/question-answering/run_qa_beam_search.py
examples/seq2seq/run_summarization.py
examples/seq2seq/run_translation.py
examples/text-classification/run_glue.py
examples/text-classification/run_xnli.py
examples/token-classification/run_ner.py
setup.py
src/transformers/__init__.py
==================
5c02b97ca;Sylvain Gugger;2021-03-16 11:31:29 -0400;Fix URLs from #10744 (#10748)

==

src/transformers/models/herbert/tokenization_herbert.py
src/transformers/models/herbert/tokenization_herbert_fast.py
src/transformers/models/reformer/configuration_reformer.py
src/transformers/models/xlm_prophetnet/tokenization_xlm_prophetnet.py
==================
a0a027c2e;Sylvain Gugger;2021-03-16 11:22:39 -0400;Add DistributedSamplerWithLoop (#10746)
* Add DistributedSamplerWithLoop

* Fix typo

* Test and small fix
==

src/transformers/sagemaker/trainer_sm.py
src/transformers/trainer.py
src/transformers/trainer_pt_utils.py
src/transformers/training_args.py
tests/test_trainer_utils.py
==================
144922221;Lysandre Debut;2021-03-16 11:18:20 -0400;Fix DeBERTa + Conversational pipeline slow tests (#10743)
* Fix DeBERTa-v2 variable assignment

* Fix conversational pipeline test
==

src/transformers/models/deberta_v2/modeling_deberta_v2.py
src/transformers/pipelines/conversational.py
==================
d3d388b93;Suraj Patil;2021-03-16 20:20:00 +0530;fix M2M100  example (#10745)

==

docs/source/model_doc/m2m_100.rst
==================
b5492582d;Sylvain Gugger;2021-03-16 10:48:53 -0400;Remove old links to CDN (#10744)

==

src/transformers/models/herbert/tokenization_herbert.py
src/transformers/models/herbert/tokenization_herbert_fast.py
src/transformers/models/reformer/configuration_reformer.py
src/transformers/models/xlm_prophetnet/tokenization_xlm_prophetnet.py
==================
5dcc08f1d;Lysandre Debut;2021-03-16 08:55:07 -0400;Fix S2T example (#10741)

==

docs/source/model_doc/speech_to_text.rst
==================
813d730c4;Sylvain Gugger;2021-03-16 08:41:47 -0400;Release utils (#10735)
* Examples version update

* Refactor a bit

* All version updates

* Fixes

* README cleanup

* Post-release/patch

* Fixes

* More fixes

* Tests

* More fixes

* Moar fixes

* Make commands and update setup

* Replace spaces with weird tabs

* Fix test

* Style
==

Makefile
setup.py
utils/release.py
==================
9f8619c6a;Patrick von Platen;2021-03-16 08:05:37 +0300;Flax testing should not run the full torch test suite (#10725)
* make flax tests pytorch independent

* fix typo

* finish

* improve circle ci

* fix return tensors

* correct flax test

* re-add sentencepiece

* last tokenizer fixes

* finish maybe now
==

.circleci/config.yml
setup.py
src/transformers/dependency_versions_table.py
src/transformers/testing_utils.py
tests/conftest.py
tests/test_modeling_flax_common.py
tests/test_tokenization_common.py
tests/test_tokenization_marian.py
tests/test_tokenization_t5.py
==================
87d685b8a;Russell Klopfer;2021-03-15 19:35:26 -0400;independent training / eval with local files (#10710)
* independent training / eval with local files

* remove redundant assert
==

examples/question-answering/run_qa.py
==================
4c379daf6;Sylvain Gugger;2021-03-15 19:29:54 -0400;Add minimum version check in examples (#10724)
* Add minimum version check in examples

* Style

* No need for new line maybe?

* Add helpful comment
==

examples/README.md
examples/language-modeling/run_clm.py
examples/language-modeling/run_mlm.py
examples/language-modeling/run_plm.py
examples/multiple-choice/run_swag.py
examples/question-answering/run_qa.py
examples/question-answering/run_qa_beam_search.py
examples/seq2seq/run_summarization.py
examples/seq2seq/run_translation.py
examples/text-classification/run_glue.py
examples/text-classification/run_xnli.py
examples/token-classification/run_ner.py
src/transformers/utils/__init__.py
==================
966ba081c;Joe Davison;2021-03-15 16:02:46 -0600;zero-shot pipeline multi_class -> multi_label (#10727)

==

examples/research_projects/zero-shot-distillation/distill_classifier.py
src/transformers/pipelines/zero_shot_classification.py
tests/test_pipelines_zero_shot.py
==================
58f672e65;Lysandre Debut;2021-03-15 17:28:01 -0400;Tests run on Docker (#10681)
* Tests run on Docker

Co-authored-by: Morgan <funtowiczmo@gmail.com>

* Comments from code review

* Reply to itself

* Dependencies

Co-authored-by: Morgan <funtowiczmo@gmail.com>
==

.github/workflows/self-push.yml
.github/workflows/self-scheduled.yml
setup.py
src/transformers/dependency_versions_table.py
src/transformers/testing_utils.py
tests/test_modeling_tf_common.py
utils/notification_service.py
==================
d41dd5359;MikeG112;2021-03-15 13:11:17 -0400;[Wav2Vec2] Fix documentation inaccuracy (#10694)
* Update super class reference

* Update default value reference

* Update src/transformers/models/wav2vec2/feature_extraction_wav2vec2.py

* Fix format style

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
==

src/transformers/models/wav2vec2/feature_extraction_wav2vec2.py
==================
f5c097fc4;Sylvain Gugger;2021-03-15 10:20:38 -0400;Fix backward compatibility with EvaluationStrategy (#10718)

==

src/transformers/training_args.py
==================
d9e693e1d;Patrick von Platen;2021-03-15 16:50:05 +0300;make wav2vec2 test deterministic (#10714)

==

tests/test_modeling_wav2vec2.py
==================
6bef76450;Sylvain Gugger;2021-03-15 09:28:15 -0400;Multiple fixes in SageMakerTrainer (#10687)
* Handle save differently

* Missing imports

* Fix typo

* Adapt to recent changes in save_pretrained

* Forgotten brackets

* Optimizer load

* Fix world size

* Deal wth None

* Remove needless self
==

src/transformers/sagemaker/trainer_sm.py
src/transformers/sagemaker/training_args_sm.py
src/transformers/trainer.py
src/transformers/training_args.py
==================
3f1714f8a;Adam Pocock;2021-03-15 09:27:55 -0400;Adding required flags to non-default arguments in hf_argparser (#10688)
* Adding required flags to non-default arguments.

Signed-off-by: Adam Pocock <adam.pocock@oracle.com>

* make style fix.

* Update src/transformers/hf_argparser.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

src/transformers/hf_argparser.py
tests/test_hf_argparser.py
==================
6f840990a;Th√©o Matussi√®re;2021-03-15 14:11:42 +0100;split seq2seq script into summarization & translation (#10611)
* split seq2seq script, update docs

* needless diff

* fix readme

* remove test diff

* s/summarization/translation

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* cr

* fix arguments & better mbart/t5 refs

* copyright

Co-authored-by: Suraj Patil <surajp815@gmail.com>

* reword readme

Co-authored-by: Suraj Patil <surajp815@gmail.com>

* s/summarization/translation

* short script names

* fix tests

* fix isort, include mbart doc

* delete old script, update tests

* automate source prefix

* automate source prefix for translation

* s/translation/trans

Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>

* fix script name (short version)

* typos

Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>

* exact parameter

Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>

* remove superfluous source_prefix calls in docs

* rename scripts & warn for source prefix

* black

* flake8

Co-authored-by: theo <theo@matussie.re>
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
Co-authored-by: Suraj Patil <surajp815@gmail.com>
Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>
==

docs/source/installation.md
docs/source/main_classes/trainer.rst
docs/source/task_summary.rst
examples/seq2seq/README.md
examples/seq2seq/run_summarization.py
examples/seq2seq/run_translation.py
examples/test_examples.py
examples/tests/deepspeed/test_deepspeed.py
examples/tests/trainer/test_trainer_ext.py
==================
505494a86;Igor Shalyminov;2021-03-15 13:10:44 +0000;GPT2DoubleHeadsModel made parallelizable (#10658)
* GPT2DoubleHeadsModel made parallelizeable

* GPT2DoubleHeadsModel added as parallelizeable onto the GPT2 test suite
==

src/transformers/models/gpt2/modeling_gpt2.py
tests/test_modeling_gpt2.py
==================
e12d6f513;Sylvain Gugger;2021-03-15 08:28:15 -0400;Distributed barrier before loading model (#10685)

==

src/transformers/trainer.py
==================
339fc51ac;Sylvain Gugger;2021-03-15 07:59:35 -0400;fix styling

==

src/transformers/models/pegasus/tokenization_pegasus_fast.py
==================
4c41c6622;cronoik;2021-03-15 12:39:10 +0100;Wrong link to super class (#10709)
Documentation was referring to slow tokenizer class while it should be the fast tokenizer.
==

src/transformers/models/pegasus/tokenization_pegasus_fast.py
==================
fcf10214e;Suraj Patil;2021-03-15 16:20:37 +0530;enable loading Mbart50Tokenizer with AutoTokenizer  (#10690)
* enable auto tokenizer for mbart50 tokenizers

* fix imports
==

src/transformers/models/auto/tokenization_auto.py
==================
bd8f6cafd;Patrick von Platen;2021-03-15 10:07:12 +0300;make rag tests smaller (#10679)

==

tests/test_modeling_rag.py
tests/test_modeling_tf_rag.py
==================
4c32f9f26;Stas Bekman;2021-03-12 13:40:07 -0800;AdamW is now supported by default (#9624)

==

docs/source/main_classes/trainer.rst
examples/tests/deepspeed/ds_config.json
src/transformers/integrations.py
==================
fa35cda91;ymfa;2021-03-12 16:13:11 +0000;Pass encoder outputs into GenerationMixin (#10599)
* Pass encoder_outputs into generate()

* Remove an if-statement

* Reformat

* Minimize changes to generate()

* Comment on input_ids
==

src/transformers/generation_utils.py
==================
00cad2e5c;PaulLerner;2021-03-12 15:18:19 +0100;fix: #10628 expanduser path in TrainingArguments (#10660)
* fix: #10628 expanduser path in TrainingArguments

* docs: explain why we expand paths in TrainingArguments

* Style

Co-authored-by: Sylvain Gugger <sylvain.gugger@gmail.com>
==

src/transformers/training_args.py
==================
e8246f78f;Sylvain Gugger;2021-03-12 07:50:20 -0500;Add auto_wrap option in fairscale integration (#10673)
* Add auto_wrap option in fairscale integration

* Style
==

docs/source/main_classes/trainer.rst
src/transformers/trainer.py
src/transformers/trainer_utils.py
src/transformers/training_args.py
==================
184ef8ecd;Lysandre Debut;2021-03-12 06:16:40 -0500;TensorFlow tests: having from_pt set to True requires torch to be installed. (#10664)
* TF model exists for Blenderbot 400M

* Marian

* RAG
==

tests/test_modeling_tf_blenderbot.py
tests/test_modeling_tf_marian.py
tests/test_modeling_tf_rag.py
==================
543d0549f;Nicolas Patry;2021-03-12 10:11:50 +0100;Adding new parameter to `generate`:  `max_time`. (#9846)
* [WIP] Adding new parameter to `generate`:  `max_time`.

Generation by tokens number is sometimes a bit clunky because we don't
know how many tokens are good enough or even how many tokens are in
the payload (for pipelines users for instance). This leads to hard
to understand behavior.

This PR proposes a new argument `max_time` which is a float of seconds
for the allowed time for `generate` to run on.
Ideally combinations of `max_tokens=None`, `max_time=2` could be used to
generate as many tokens as possible within time budget.

NB: Another possible approach consists of passing a callback to `generate`
  putting the caller in charge of the actual decision of when to stop
  generating tokens. It opens the door to 'which args should we pass'
  to this callback. It's hard to imagine other use-cases for this
  early stopping behavior than time (that are not already covered by
  parameters of generate)

* Revamp with StoppingCriteria

* Removing deprecated mentions.

* Forgot arguments to stopping criteria.

* Readding max_length it's not just used as a stopping criteria.

* Default value for `stopping_criteria`.

* Address @patrickvonplaten comments.

- More docstrings
- Actual doc
- Include in global namespace
- Remove TF work.

* Put back `max_length` (deprecation different PR).

* Doc quality.

* Fixing old behavior without `stopping_criteria` but with `max_length`.

Making sure we don't break that in the future.

* Adding more tests for possible inconsistencies between

`max_length` and `stopping_criteria`.

* Fixing the torch imports.
==

docs/source/internal/generation_utils.rst
src/transformers/__init__.py
src/transformers/generation_stopping_criteria.py
src/transformers/generation_utils.py
tests/test_generation_stopping_criteria.py
tests/test_generation_utils.py
tests/test_modeling_gpt2.py
==================
ea46e3fa9;Lysandre Debut;2021-03-12 01:09:46 -0500;Adjust loss difference (#10669)

==

tests/test_modeling_tf_mt5.py
==================
c526bde31;Benjamin Fineran;2021-03-11 17:42:54 -0500;fix typing error for HfArgumentParser for Optional[bool] (#10672)
* fix typing error for TrainingArguments Optional[bool]

* updating equality check for Optional[bool]
==

src/transformers/hf_argparser.py
==================
fa1a8d102;Sylvain Gugger;2021-03-11 14:44:29 -0500;Tentative fix for HFArgumentParser in Python 3.8

==

src/transformers/hf_argparser.py
==================
2f8485199;WybeKoper;2021-03-11 20:29:02 +0100;Fix broken link (#10656)
* Fixed broken link

* fixed max length violation

Co-authored-by: WybeKoper <WybeKoper@users.noreply.github.com>
==

docs/source/model_doc/pegasus.rst
==================
a01ea31b5;jeswan;2021-03-11 13:56:47 -0500;Add DeBERTa to MODEL_FOR_PRETRAINING_MAPPING (#10668)
* add deberta to pretraining mapping

* add deberta_v2 to PRETRAINING_MAPPING
==

src/transformers/models/auto/modeling_auto.py
==================
9fbb4cdc8;Lysandre Debut;2021-03-11 13:45:06 -0500;Specify minimum version for sacrebleu (#10662)

==

examples/_tests_requirements.txt
examples/seq2seq/requirements.txt
==================
fda703a55;Sylvain Gugger;2021-03-11 13:43:53 -0500;Fix integration slow tests (#10670)
* PoC

* Fix slow tests for the PT1.8 Embedding problem
==

tests/test_modeling_albert.py
tests/test_modeling_bert.py
tests/test_modeling_convbert.py
tests/test_modeling_deberta.py
tests/test_modeling_deberta_v2.py
tests/test_modeling_distilbert.py
tests/test_modeling_electra.py
tests/test_modeling_mbart.py
tests/test_modeling_squeezebert.py
==================
3ab682037;Funtowicz Morgan;2021-03-11 19:38:29 +0100;Onnx fix test (#10663)
* Allow to pass kwargs to model's from_pretrained when using pipeline.

* Disable the use of past_keys_values for GPT2 when exporting to ONNX.

* style

* Remove comment.

* Appease the documentation gods

* Fix style

Co-authored-by: Lysandre <lysandre.debut@reseau.eseo.fr>
==

src/transformers/convert_graph_to_onnx.py
src/transformers/pipelines/__init__.py
tests/test_onnx.py
==================
a637ae00c;Lysandre Debut;2021-03-11 13:35:50 -0500;Fixes Pegasus tokenization tests (#10671)

==

tests/test_modeling_tf_pegasus.py
==================
7e4428749;Lysandre Debut;2021-03-11 12:58:15 -0500;Conversion to tensors requires padding (#10661)

==

tests/test_modeling_marian.py
tests/test_modeling_tf_marian.py
==================
2adc8c926;Lysandre Debut;2021-03-11 12:56:12 -0500;W2v2 test require torch (#10665)
* Adds a @require_torch to a test that requires it

* Tokenizer too

* Style
==

tests/test_feature_extraction_wav2vec2.py
tests/test_tokenization_wav2vec2.py
==================
055ed78f5;Suraj Patil;2021-03-11 22:43:37 +0530;[S2T] fix example in docs  (#10667)

==

docs/source/model_doc/speech_to_text.rst
==================
89693e170;Sylvain Gugger;2021-03-11 11:11:56 -0500;Remove special treatment for custom vocab files (#10637)
* Remove special path for custom vocab files

* Update src/transformers/tokenization_utils_base.py

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* Expand error message

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
==

src/transformers/tokenization_utils_base.py
==================
6d9e11a19;Lysandre Debut;2021-03-11 09:53:36 -0500;S2S + M2M100 should be available in tokenization_auto (#10657)
* S2S + M2M100 should be available in tokenization_auto

* Requires sentencepiece

* SentencePiece for S2T as well :)
==

src/transformers/models/auto/tokenization_auto.py
==================
602d63f05;Patrick von Platen;2021-03-11 17:44:18 +0300;[XLSR-Wav2Vec2] Add multi-lingual Wav2Vec2 models (#10648)
* add conversion script

* add wav2vec2 xslr models

* finish

* Update docs/source/model_doc/xlsr_wav2vec2.rst

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

README.md
docs/source/index.rst
docs/source/model_doc/xlsr_wav2vec2.rst
src/transformers/models/wav2vec2/convert_wav2vec2_original_pytorch_checkpoint_to_pytorch.py
==================
63c295ac0;Sylvain Gugger;2021-03-11 09:00:23 -0500;Ensure metric results are JSON-serializable (#10632)

==

src/transformers/trainer.py
src/transformers/trainer_utils.py
==================
27d9e05ce;ArvidYin;2021-03-11 21:58:04 +0800;Update README.md (#10647)
correct spell error: 'nether'
==

examples/seq2seq/README.md
==================
053f0197b;Lysandre Debut;2021-03-11 08:34:08 -0500;merge_file -> merges_file (#10653)

==

src/transformers/models/longformer/tokenization_longformer.py
src/transformers/models/longformer/tokenization_longformer_fast.py
==================
26a33cfd8;Sylvain Gugger;2021-03-10 14:58:22 -0500;Document Trainer limitation on custom models (#10635)

==

docs/source/main_classes/trainer.rst
==================
49c61a4ae;Philipp Schmid;2021-03-10 20:53:49 +0100;Extend trainer logging for sm (#10633)
* renamed logging to hf_logging

* changed logging from hf_logging to logging and loggin to native_logging

* removed everything trying to fix import Trainer error

* adding imports again

* added custom add_handler function to logging.py

* make style

* added remove_handler

* added another conditional to assert
==

src/transformers/trainer.py
src/transformers/utils/logging.py
==================
1aa9c13f7;Sylvain Gugger;2021-03-10 12:51:06 -0500;Fix GPU tests with speech

==

.github/workflows/self-push.yml
==================
2295d783d;Sylvain Gugger;2021-03-10 11:26:23 -0500;Copy tokenizer files in each of their repo (#10624)
* Move tokenizer files in each repo

* Fix mBART50 tests

* Fix mBART tests

* Fix Marian tests

* Update templates
==

src/transformers/models/bart/tokenization_bart.py
src/transformers/models/bart/tokenization_bart_fast.py
src/transformers/models/bert_generation/tokenization_bert_generation.py
src/transformers/models/blenderbot/tokenization_blenderbot.py
src/transformers/models/blenderbot_small/tokenization_blenderbot_small.py
src/transformers/models/blenderbot_small/tokenization_blenderbot_small_fast.py
src/transformers/models/camembert/tokenization_camembert.py
src/transformers/models/camembert/tokenization_camembert_fast.py
src/transformers/models/distilbert/tokenization_distilbert.py
src/transformers/models/distilbert/tokenization_distilbert_fast.py
src/transformers/models/dpr/tokenization_dpr.py
src/transformers/models/dpr/tokenization_dpr_fast.py
src/transformers/models/fsmt/tokenization_fsmt.py
src/transformers/models/herbert/tokenization_herbert_fast.py
src/transformers/models/layoutlm/tokenization_layoutlm.py
src/transformers/models/layoutlm/tokenization_layoutlm_fast.py
src/transformers/models/longformer/tokenization_longformer.py
src/transformers/models/longformer/tokenization_longformer_fast.py
src/transformers/models/lxmert/tokenization_lxmert.py
src/transformers/models/lxmert/tokenization_lxmert_fast.py
src/transformers/models/m2m_100/tokenization_m2m_100.py
src/transformers/models/marian/tokenization_marian.py
src/transformers/models/mbart/tokenization_mbart.py
src/transformers/models/mbart/tokenization_mbart50.py
src/transformers/models/mbart/tokenization_mbart50_fast.py
src/transformers/models/mbart/tokenization_mbart_fast.py
src/transformers/models/reformer/tokenization_reformer.py
src/transformers/models/reformer/tokenization_reformer_fast.py
src/transformers/models/retribert/tokenization_retribert.py
src/transformers/models/retribert/tokenization_retribert_fast.py
src/transformers/models/roberta/tokenization_roberta.py
src/transformers/models/roberta/tokenization_roberta_fast.py
src/transformers/models/t5/tokenization_t5.py
src/transformers/models/t5/tokenization_t5_fast.py
src/transformers/models/wav2vec2/tokenization_wav2vec2.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/tokenization_fast_{{cookiecutter.lowercase_modelname}}.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/tokenization_{{cookiecutter.lowercase_modelname}}.py
tests/test_tokenization_marian.py
==================
d26b37e74;Suraj Patil;2021-03-10 21:42:04 +0530;Speech2TextTransformer (#10175)
* s2t

* fix config

* conversion script

* fix import

* add tokenizer

* fix tok init

* fix tokenizer

* first version working

* fix embeds

* fix lm head

* remove extra heads

* fix convert script

* handle encoder attn mask

* style

* better enc attn mask

* override _prepare_attention_mask_for_generation

* handle attn_maks in encoder and decoder

* input_ids => input_features

* enable use_cache

* remove old code

* expand embeddings if needed

* remove logits bias

* masked_lm_loss => loss

* hack tokenizer to support feature processing

* fix model_input_names

* style

* fix error message

* doc

* remove inputs_embeds

* remove input_embeds

* remove unnecessary docstring

* quality

* SpeechToText => Speech2Text

* style

* remove shared_embeds

* subsample => conv

* remove Speech2TextTransformerDecoderWrapper

* update output_lengths formula

* fix table

* remove max_position_embeddings

* update conversion scripts

* add possibility to do upper case for now

* add FeatureExtractor and Processor

* add tests for extractor

* require_torch_audio => require_torchaudio

* add processor test

* update import

* remove classification head

* attention mask is now 1D

* update docstrings

* attention mask should be of type long

* handle attention mask from generate

* alwyas return attention_mask

* fix test

* style

* doc

* Speech2TextTransformer => Speech2Text

* Speech2TextTransformerConfig => Speech2TextConfig

* remove dummy_inputs

* nit

* style

* multilinguial tok

* fix tokenizer

* add tgt_lang setter

* save lang_codes

* fix tokenizer

* add forced_bos_token_id to tokenizer

* apply review suggestions

* add torchaudio to extra deps

* add speech deps to CI

* fix dep

* add libsndfile to ci

* libsndfile1

* add speech to extras all

* libsndfile1 -> libsndfile1

* libsndfile

* libsndfile1-dev

* apt update

* add sudo to install

* update deps table

* install libsndfile1-dev on CI

* tuple to list

* init conv layer

* add model tests

* quality

* add integration tests

* skip_special_tokens

* add speech_to_text_transformer in toctree

* fix tokenizer

* fix fp16 tests

* add tokenizer tests

* fix copyright

* input_values => input_features

* doc

* add model in readme

* doc

* change checkpoint names

* fix copyright

* fix code example

* add max_model_input_sizes in tokenizer

* fix integration tests

* add do_lower_case to tokenizer

* remove clamp trick

* fix "Add modeling imports here"

* fix copyrights

* fix tests

* SpeechToTextTransformer => SpeechToText

* fix naming

* fix table formatting

* fix typo

* style

* fix typos

* remove speech dep from extras[testing]

* fix copies

* rename doc file,

* put imports under is_torch_available

* run feat extract tests when torch is available

* dummy objects for processor and extractor

* fix imports in tests

* fix import in modeling test

* fxi imports

* fix torch import

* fix imports again

* fix positional embeddings

* fix typo in import

* adapt new extractor refactor

* style

* fix torchscript test

* doc

* doc

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* fix docs, copied from, style

* fix docstring

* handle imports

* remove speech from all extra deps

* remove s2t from seq2seq lm mapping

* better names

* skip training tests

* add install instructions

* List => Tuple

* doc

* fix conversion script

* fix urls

* add instruction for libsndfile

* fix fp16 test

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

.circleci/config.yml
README.md
docs/source/index.rst
docs/source/model_doc/speech_to_text.rst
setup.cfg
setup.py
src/transformers/__init__.py
src/transformers/dependency_versions_table.py
src/transformers/file_utils.py
src/transformers/generation_utils.py
src/transformers/models/__init__.py
src/transformers/models/auto/configuration_auto.py
src/transformers/models/auto/modeling_auto.py
src/transformers/models/speech_to_text/__init__.py
src/transformers/models/speech_to_text/configuration_speech_to_text.py
src/transformers/models/speech_to_text/convert_s2t_fairseq_to_tfms.py
src/transformers/models/speech_to_text/feature_extraction_speech_to_text.py
src/transformers/models/speech_to_text/modeling_speech_to_text.py
src/transformers/models/speech_to_text/processing_speech_to_text.py
src/transformers/models/speech_to_text/tokenization_speech_to_text.py
src/transformers/testing_utils.py
src/transformers/utils/dummy_pt_objects.py
src/transformers/utils/dummy_sentencepiece_objects.py
tests/test_feature_extraction_speech_to_text.py
tests/test_generation_utils.py
tests/test_modeling_speech_to_text.py
tests/test_processor_speech_to_text.py
tests/test_sequence_feature_extraction_common.py
tests/test_tokenization_speech_to_text.py
utils/check_repo.py
==================
efb5c0a45;Sylvain Gugger;2021-03-10 09:29:19 -0500;Add new GLUE example with no Trainer. (#10555)
* Add new GLUE example with no Trainer.

* Style

* Address review comments
==

examples/text-classification/README.md
examples/text-classification/requirements.txt
examples/text-classification/run_glue_no_trainer.py
==================
44f64132a;Suraj Patil;2021-03-10 09:52:31 +0530;remove final_logits_bias (#10606)

==

src/transformers/models/m2m_100/modeling_m2m_100.py
==================
6f52fce67;Allen Wang;2021-03-09 19:13:45 -0800;Fixes an issue in `text-classification` where MNLI eval/test datasets are not being preprocessed. (#10621)
* Fix MNLI tests

* Linter fix
==

examples/text-classification/run_glue.py
==================
72d9e039f;Sylvain Gugger;2021-03-09 16:25:32 -0500;Fix tests of TrainerCallback (#10615)
* Fix tests of TrainerCallback

* Update tests/test_trainer_callback.py

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
==

tests/test_trainer_callback.py
==================
0d909f6bd;Sylvain Gugger;2021-03-09 14:42:07 -0500;Fairscale FSDP fix model save (#10596)
* Hotfix fairscale FSDP

* Evaluation works

* Save on process zero
==

examples/tests/trainer/test_trainer_ext.py
src/transformers/trainer.py
==================
ac17f7115;Bhadresh Savani;2021-03-09 22:36:56 +0530;added max_sample args and metrics changes (#10602)

==

templates/adding_a_new_example_script/{{cookiecutter.directory_name}}/run_{{cookiecutter.example_shortcut}}.py
==================
c19c811a2;Philipp Schmid;2021-03-09 17:31:45 +0100;Trigger add sm information (#10610)
* added sm to ua

* update id

* removed id

* removed comments

* added env variable

* changed variable name

* make quality happy

* added sguggers feedback

* make styling happy and remove brackets

* added sm to ua

* update id

* removed id

* removed comments

* added env variable

* changed variable name

* make quality happy

* added sguggers feedback

* make styling happy and remove brackets
==

src/transformers/file_utils.py
==================
20c10258a;Suraj Patil;2021-03-09 20:05:07 +0530;layerdrop 0 (#10604)

==

tests/test_modeling_m2m_100.py
==================
95ab06778;Lysandre;2021-03-09 07:10:58 -0500;Update cache version for github actions

==

.github/workflows/self-push.yml
.github/workflows/self-scheduled.yml
==================
9a06b6b11;Patrick von Platen;2021-03-09 12:16:59 +0300;[FeatureExtractorSavingUtils] Refactor PretrainedFeatureExtractor (#10594)
* save first version

* finish refactor

* finish refactor

* correct naming

* correct naming

* shorter names

* Update src/transformers/feature_extraction_common_utils.py

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* change name

* finish

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
==

docs/source/main_classes/feature_extractor.rst
src/transformers/__init__.py
src/transformers/feature_extraction_sequence_utils.py
src/transformers/feature_extraction_utils.py
src/transformers/models/wav2vec2/feature_extraction_wav2vec2.py
src/transformers/models/wav2vec2/processing_wav2vec2.py
src/transformers/tokenization_utils_base.py
tests/test_feature_extraction_common.py
tests/test_feature_extraction_wav2vec2.py
tests/test_sequence_feature_extraction_common.py
==================
b6a28e9ac;Stas Bekman;2021-03-08 20:16:33 -0800;[docs] How to solve "Title level inconsistent" sphinx error (#10600)
* How to solve: Title level inconsistent

* list chars
==

docs/README.md
==================
546cbe7e9;Lysandre Debut;2021-03-08 21:44:07 -0500;Speedup tf tests (#10601)
* Pipeline tests should be slow

* Temporarily mark some tests as slow

* Temporarily mark Barthez tests as slow
==

tests/test_modeling_tf_common.py
tests/test_tokenization_barthez.py
==================
696e8a436;Ratthachat (Jung);2021-03-09 04:49:51 +0700;Add TFRag (#9002)
* Create modeling_tf_dpr.py

* Add TFDPR

* Add back TFPegasus, TFMarian, TFMBart, TFBlenderBot

last commit accidentally deleted these 4 lines, so I recover them back

* Add TFDPR

* Add TFDPR

* clean up some comments, add TF input-style doc string

* Add TFDPR

* Make return_dict=False as default

* Fix return_dict bug (in .from_pretrained)

* Add get_input_embeddings()

* Create test_modeling_tf_dpr.py

The current version is already passed all 27 tests!
Please see the test run at : 
https://colab.research.google.com/drive/1czS_m9zy5k-iSJbzA_DP1k1xAAC_sdkf?usp=sharing

* fix quality

* delete init weights

* run fix copies

* fix repo consis

* del config_class, load_tf_weights

They shoud be 'pytorch only'

* add config_class back

after removing it, test failed ... so totally only removing "use_tf_weights = None" on Lysandre suggestion

* newline after .. note::

* import tf, np (Necessary for ModelIntegrationTest)

* slow_test from_pretrained with from_pt=True

At the moment we don't have TF weights (since we don't have official official TF model)
Previously, I did not run slow test, so I missed this bug

* Add simple TFDPRModelIntegrationTest

Note that this is just a test that TF and Pytorch gives approx. the same output.
However, I could not test with the official DPR repo's output yet

* upload correct tf model

* remove position_ids as missing keys

* create modeling_tf_rag

* add tests for tf

* add tf tests

* revert wrong pt commit

* further refactor

* further refactor

* refactor

* Update modeling_tf_rag.py

- input_processing
- fix prepare_input_for_generation (mostly fix generate bug)
- bring back from_pretrained hack in order to test generate

* delete colab pieces of code

* Show case of greedy "generate"

Temporarily change from beam_search test to greedy_search test to show case that TF and PT do get equivalent output.

* cosmetic update

* correct typos

* update

* push some progress

* make easy check

* fix rag save from pretrained

* Update src/transformers/modeling_tf_utils.py

* remove commented out lines

* delete unnecessary lines

* add simple test case for nq_checkpoint

Add nq_checkpoint test to show that current version without hack still fails

* temporarily put ugly hack back again

* Add TFRagSequenceForGeneration!!

* __init__.py , import TFRagSequenceForGeneration

* Add TFRagSequence tests!

* rag init.py - add TFRagSequenceForGeneration

* fix from_pretrained

* fix prepare_inputs_for_generation

* Beam search for RagToken!

* minor clean up

* add tf.cast in TFRagModel

* More tf.cast

* Add all remaining tests (still have issues)

* delete all T5 related

* make style

* fix load weight prefix

* fix bart

* fix return_dict for tf_rag

make all tests pass .. Hooray

* fix some tests

* fix code quality

* fix qualtiy check

* finish tests tf rag

* add tf rag to docs

* remove TFT5 from docstring

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* remove TFT5 from docstring

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* Delete outdated comments

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* improve doc strings

* add generative model classes

* fix adjust token logic

* refactor generate for TFRag

* using shape_list, not _get_shape

Co-authored-by: Julien Plu <plu.julien@gmail.com>

* axis=[1]->axis=1

* delete NEED_HELP comment

* improve readability

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* improve readability

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* improve readability

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Indicating model is in a developing state in docstrings

As suggested by Julien

* small last changes

* apply sylvains suggestions

* finish tf rag

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
Co-authored-by: patrickvonplaten <patrick@huggingface.co>
Co-authored-by: Julien Plu <plu.julien@gmail.com>
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

docs/source/index.rst
docs/source/model_doc/rag.rst
src/transformers/__init__.py
src/transformers/generation_tf_utils.py
src/transformers/modeling_tf_utils.py
src/transformers/models/auto/modeling_tf_auto.py
src/transformers/models/bart/modeling_tf_bart.py
src/transformers/models/rag/__init__.py
src/transformers/models/rag/modeling_tf_rag.py
src/transformers/utils/dummy_tf_objects.py
tests/test_modeling_tf_rag.py
utils/check_repo.py
==================
3ced9b3eb;Sylvain Gugger;2021-03-08 16:40:11 -0500;Check layer types for Optimizer construction (#10598)
* Check layer types for Optimizer construction

* Duplicate class
==

src/transformers/trainer.py
src/transformers/trainer_pt_utils.py
tests/test_trainer.py
tests/test_trainer_utils.py
==================
821d518e0;Sylvain Gugger;2021-03-08 16:04:30 -0500;Revert "Tests"
This reverts commit b35e7b68caade1df761454501bbd7248c64b6bc9.

==

src/transformers/trainer.py
src/transformers/trainer_pt_utils.py
tests/test_trainer.py
tests/test_trainer_utils.py
==================
4196bfeda;Sylvain Gugger;2021-03-08 16:04:46 -0500;Revert "Style"
This reverts commit a8ec52efc217474ff164461bebcfec060cff6837.

==

tests/test_trainer.py
tests/test_trainer_utils.py
==================
a8ec52efc;Sylvain Gugger;2021-03-08 16:04:46 -0500;Style

==

tests/test_trainer.py
tests/test_trainer_utils.py
==================
b35e7b68c;Sylvain Gugger;2021-03-08 16:04:30 -0500;Tests

==

src/transformers/trainer.py
src/transformers/trainer_pt_utils.py
tests/test_trainer.py
tests/test_trainer_utils.py
==================
f284089ec;Stas Bekman;2021-03-08 11:11:40 -0800;[examples tests on multigpu] resolving require_torch_non_multi_gpu_but_fix_me (#10561)
* batch 1

* this is tpu

* deebert attempt

* the rest
==

examples/legacy/seq2seq/old_test_datasets.py
examples/legacy/seq2seq/old_test_tatoeba_conversion.py
examples/research_projects/bert-loses-patience/test_run_glue_with_pabee.py
examples/research_projects/deebert/test_glue_deebert.py
examples/research_projects/rag/test_distributed_retriever.py
examples/research_projects/seq2seq-distillation/_test_make_student.py
examples/test_examples.py
examples/test_xla_examples.py
src/transformers/testing_utils.py
==================
dfd16af83;Bhadresh Savani;2021-03-09 00:27:10 +0530;Added max_sample_ arguments (#10551)
* reverted changes of logging and saving metrics

* added max_sample arguments

* fixed code

* white space diff

* reformetting code

* reformatted code
==

examples/language-modeling/run_clm.py
examples/language-modeling/run_mlm.py
examples/language-modeling/run_plm.py
examples/multiple-choice/run_swag.py
examples/multiple-choice/run_tf_multiple_choice.py
examples/question-answering/run_qa.py
examples/question-answering/run_qa_beam_search.py
examples/seq2seq/run_seq2seq.py
examples/test_examples.py
examples/text-classification/run_glue.py
examples/text-classification/run_tf_glue.py
examples/text-classification/run_tf_text_classification.py
examples/text-classification/run_xnli.py
examples/token-classification/run_ner.py
==================
917f10450;Stas Bekman;2021-03-08 10:28:44 -0800;[examples tests] various fixes (#10584)
* fix sharded ddp enum

* test fixes

* stronger validation + apex breaks other tests
==

examples/tests/trainer/test_trainer_ext.py
src/transformers/trainer_utils.py
==================
6f84531e6;Stas Bekman;2021-03-08 08:52:20 -0800;offline mode for firewalled envs (part 2) (#10569)
* more readable test

* add all the missing places

* one more nltk

* better exception check

* revert
==

src/transformers/feature_extraction_utils.py
src/transformers/file_utils.py
src/transformers/modeling_flax_utils.py
src/transformers/modeling_tf_utils.py
tests/test_offline.py
==================
546936948;Sylvain Gugger;2021-03-08 10:19:22 -0500;Fix version control with anchors (#10595)
* Fix version control with anchors

* Simplify
==

docs/source/_static/js/custom.js
==================
f88296600;Stas Bekman;2021-03-08 07:15:55 -0800;fix double wrapping + test (#10583)

==

src/transformers/trainer.py
tests/test_trainer.py
==================
b88050844;Mehrad Moradshahi;2021-03-08 05:14:31 -0800;tokenization_marian.py: use current_spm for decoding (#10357)
* Fix Marian decoding

Tokenizer's decode and batch_decode now accepts a new argument (use_source_tokenizer) which indicates whether the source spm should be used to decode ids. This is useful for Marian models specificallly when decoding source input ids.

* Adapt docstrings

Co-authored-by: Sylvain Gugger <sylvain.gugger@gmail.com>
==

src/transformers/models/marian/tokenization_marian.py
src/transformers/models/wav2vec2/tokenization_wav2vec2.py
src/transformers/tokenization_utils.py
src/transformers/tokenization_utils_fast.py
==================
8fd7eb34e;Lysandre;2021-03-08 07:13:49 -0500;Correct YAML

==

.github/workflows/self-push.yml
.github/workflows/self-scheduled.yml
==================
89b8d4f56;Lysandre Debut;2021-03-08 07:11:43 -0500;Enable torch 1.8.0 on GPU CI (#10593)
* Enable torch 1.8.0 in GPU CI

* Disable torch-scatter
==

.github/workflows/self-push.yml
.github/workflows/self-scheduled.yml
==================
2a737bffe;Suraj Patil;2021-03-08 16:06:19 +0530;[M2M100] fix positional embeddings (#10590)
* fix tests

* emb should be a parameter

* fix positional embeddings

* fix make_weights

* don't save pos embeds

* add comment to describe the clamping
==

src/transformers/models/m2m_100/modeling_m2m_100.py
tests/test_modeling_m2m_100.py
==================
d59464db6;Oren Amsalem;2021-03-08 12:15:06 +0200;fix BART Summarization example in doc (#10582)

==

src/transformers/models/bart/modeling_bart.py
==================
3b583d02d;Eunhyuk Shin;2021-03-08 19:10:03 +0900;Fix typo in docstring for pipeline (#10591)

==

src/transformers/pipelines/__init__.py
==================
e6ce636e0;Stas Bekman;2021-03-07 22:09:58 -0800;fix nltk lookup (#10585)

==

examples/seq2seq/run_seq2seq.py
==================
9dd054fba;Yu;2021-03-08 11:31:50 +0800;fix tf doc bug (#10570)

==

src/transformers/models/bert/modeling_tf_bert.py
==================
f6e74a63c;Suraj Patil;2021-03-06 22:14:16 +0530;Add m2m100 (#10236)
* m2m_100

* no layernorm_embedding

* sinusoidal positional embeddings

* update pos embeddings

* add default config values

* tokenizer

* add conversion script

* fix config

* fix pos embed

* remove _float_tensor

* update tokenizer

* update lang codes

* handle lang codes

* fix pos embeds

* fix spm key

* put embedding weights on device

* remove qa and seq classification heads

* fix convert script

* lang codes pn one line

* fix embeds

* fix tokenizer

* fix tokenizer

* add fast tokenizer

* style

* M2M100MT => M2M100

* fix copyright, style

* tokenizer converter

* vocab file

* remove fast tokenizer

* fix embeds

* fix tokenizer

* fix tests

* add tokenizer tests

* add integration test

* quality

* fix model name

* fix test

* doc

* doc

* fix doc

* add copied from statements

* fix tokenizer tests

* apply review suggestions

* fix urls

* fix shift_tokens_right

* apply review suggestions

* fix

* fix doc

* add lang code to id

* remove unused function

* update checkpoint names

* fix copy

* fix tokenizer

* fix checkpoint names

* fix merge issue

* style
==

README.md
docs/source/index.rst
docs/source/model_doc/m2m_100.rst
docs/source/pretrained_models.rst
src/transformers/__init__.py
src/transformers/models/__init__.py
src/transformers/models/auto/configuration_auto.py
src/transformers/models/auto/modeling_auto.py
src/transformers/models/m2m_100/__init__.py
src/transformers/models/m2m_100/configuration_m2m_100.py
src/transformers/models/m2m_100/convert_m2m100_original_checkpoint_to_pytorch.py
src/transformers/models/m2m_100/modeling_m2m_100.py
src/transformers/models/m2m_100/tokenization_m2m_100.py
src/transformers/utils/dummy_pt_objects.py
tests/test_modeling_m2m_100.py
tests/test_tokenization_m2m_100.py
utils/check_repo.py
==================
fd0110443;Lysandre;2021-03-06 00:21:50 -0500;Temporarily disable stale bot

==

scripts/stale.py
==================
88a951e3c;Stas Bekman;2021-03-05 17:27:48 -0800;offline mode for firewalled envs (#10407)
* offline mode start

* add specific values

* fix fallback

* add test

* better values check and range

* test that actually works

* document the offline mode

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* more strict check

* cleaner test

* pt-only test

* style

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

docs/source/installation.md
examples/seq2seq/run_seq2seq.py
src/transformers/configuration_utils.py
src/transformers/file_utils.py
src/transformers/modeling_utils.py
src/transformers/tokenization_utils_base.py
tests/test_offline.py
==================
90ecc2965;Daniel Hug;2021-03-05 18:06:55 -0500;Refactoring checkpoint names for multiple models (#10527)
* Refactor checkpoint name in ALBERT and ALBERT_tf

* Refactor checkpoint name in BART and BART_tf

* Refactor checkpoint name in BERT generation

* Refactor checkpoint name in Blenderbot_tf

* Refactor checkpoint name in Blenderbot_small_tf

* Refactor checkpoint name in ConvBERT AND CONVBERT_TF

* Refactor checkpoint name in CTRL AND CTRL_TF

* Refactor checkpoint name in DistilBERT AND DistilBERT_TF

* Refactor checkpoint name in DistilBERT redo

* Refactor checkpoint name in Electra and Electra_tf

* Refactor checkpoint name in FlauBERT and FlauBERT_tf

* Refactor checkpoint name in FSMT

* Refactor checkpoint name in GPT2 and GPT2_tf

* Refactor checkpoint name in IBERT

* Refactor checkpoint name in LED and LED_tf

* Refactor checkpoint name in Longformer and Longformer_tf

* Refactor checkpoint name in Lxmert and Lxmert_tf

* Refactor checkpoint name in Marian_tf

* Refactor checkpoint name in MBART and MBART_tf

* Refactor checkpoint name in MobileBERT and MobileBERT_tf

* Refactor checkpoint name in mpnet and mpnet_tf

* Refactor checkpoint name in openai and openai_tf

* Refactor checkpoint name in pegasus_tf

* Refactor checkpoint name in reformer

* Refactor checkpoint name in Roberta and Roberta_tf

* Refactor checkpoint name in SqueezeBert

* Refactor checkpoint name in Transformer_xl and Transformer_xl_tf

* Refactor checkpoint name in XLM and XLM_tf

* Refactor checkpoint name in XLNET and XLNET_tf

* Refactor checkpoint name in BERT_tf

* run make tests, style, quality, fixup
==

src/transformers/models/albert/modeling_albert.py
src/transformers/models/albert/modeling_tf_albert.py
src/transformers/models/bart/modeling_bart.py
src/transformers/models/bart/modeling_tf_bart.py
src/transformers/models/bert/modeling_tf_bert.py
src/transformers/models/bert_generation/modeling_bert_generation.py
src/transformers/models/blenderbot/modeling_tf_blenderbot.py
src/transformers/models/blenderbot_small/modeling_tf_blenderbot_small.py
src/transformers/models/convbert/modeling_convbert.py
src/transformers/models/convbert/modeling_tf_convbert.py
src/transformers/models/ctrl/modeling_ctrl.py
src/transformers/models/ctrl/modeling_tf_ctrl.py
src/transformers/models/distilbert/modeling_distilbert.py
src/transformers/models/distilbert/modeling_tf_distilbert.py
src/transformers/models/electra/modeling_electra.py
src/transformers/models/electra/modeling_tf_electra.py
src/transformers/models/flaubert/modeling_flaubert.py
src/transformers/models/flaubert/modeling_tf_flaubert.py
src/transformers/models/fsmt/modeling_fsmt.py
src/transformers/models/gpt2/modeling_gpt2.py
src/transformers/models/gpt2/modeling_tf_gpt2.py
src/transformers/models/ibert/modeling_ibert.py
src/transformers/models/led/modeling_led.py
src/transformers/models/led/modeling_tf_led.py
src/transformers/models/longformer/modeling_longformer.py
src/transformers/models/longformer/modeling_tf_longformer.py
src/transformers/models/lxmert/modeling_lxmert.py
src/transformers/models/lxmert/modeling_tf_lxmert.py
src/transformers/models/marian/modeling_tf_marian.py
src/transformers/models/mbart/modeling_mbart.py
src/transformers/models/mbart/modeling_tf_mbart.py
src/transformers/models/mobilebert/modeling_tf_mobilebert.py
src/transformers/models/mpnet/modeling_mpnet.py
src/transformers/models/mpnet/modeling_tf_mpnet.py
src/transformers/models/openai/modeling_openai.py
src/transformers/models/openai/modeling_tf_openai.py
src/transformers/models/pegasus/modeling_tf_pegasus.py
src/transformers/models/reformer/modeling_reformer.py
src/transformers/models/roberta/modeling_roberta.py
src/transformers/models/roberta/modeling_tf_roberta.py
src/transformers/models/squeezebert/modeling_squeezebert.py
src/transformers/models/transfo_xl/modeling_tf_transfo_xl.py
src/transformers/models/transfo_xl/modeling_transfo_xl.py
src/transformers/models/xlm/modeling_tf_xlm.py
src/transformers/models/xlm/modeling_xlm.py
src/transformers/models/xlnet/modeling_tf_xlnet.py
src/transformers/models/xlnet/modeling_xlnet.py
==================
defe9e20f;Lysandre Debut;2021-03-05 22:41:50 +0100;Stale Bot (#10509)
* Add stale bot to Github Actions

* Update message

* Message for assignee

* Update scripts/stale.py

* Uncomment & stop testing
==

.github/stale.yml
.github/workflows/stale.yml
scripts/stale.py
==================
7da995c00;Sylvain Gugger;2021-03-05 16:18:48 -0500;Fix embeddings for PyTorch 1.8 (#10549)
* Fix embeddings for PyTorch 1.8

* Try with PyTorch 1.8.0

* Fix embeddings init

* Fix copies

* Typo

* More typos
==

.circleci/config.yml
src/transformers/models/albert/modeling_albert.py
src/transformers/models/bert/modeling_bert.py
src/transformers/models/bert_generation/modeling_bert_generation.py
src/transformers/models/convbert/modeling_convbert.py
src/transformers/models/ctrl/modeling_ctrl.py
src/transformers/models/deberta/modeling_deberta.py
src/transformers/models/deberta_v2/modeling_deberta_v2.py
src/transformers/models/distilbert/modeling_distilbert.py
src/transformers/models/electra/modeling_electra.py
src/transformers/models/funnel/modeling_funnel.py
src/transformers/models/gpt2/modeling_gpt2.py
src/transformers/models/ibert/modeling_ibert.py
src/transformers/models/layoutlm/modeling_layoutlm.py
src/transformers/models/longformer/modeling_longformer.py
src/transformers/models/lxmert/modeling_lxmert.py
src/transformers/models/mobilebert/modeling_mobilebert.py
src/transformers/models/mpnet/modeling_mpnet.py
src/transformers/models/openai/modeling_openai.py
src/transformers/models/reformer/modeling_reformer.py
src/transformers/models/retribert/modeling_retribert.py
src/transformers/models/roberta/modeling_roberta.py
src/transformers/models/squeezebert/modeling_squeezebert.py
src/transformers/models/tapas/modeling_tapas.py
src/transformers/models/xlm/modeling_xlm.py
src/transformers/models/xlnet/modeling_xlnet.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_{{cookiecutter.lowercase_modelname}}.py
==================
3e056c100;Chen Liang;2021-03-05 12:27:09 -0800;Typo correction. (#10531)
DEBERTA_PRETRAINED_MODEL_ARCHIVE_LIST => DEBERTA_V2_PRETRAINED_MODEL_ARCHIVE_LIST in line 31.
==

src/transformers/models/deberta_v2/__init__.py
==================
9f8bc87cb;Joakim Warholm;2021-03-05 20:56:37 +0100;fixed dead link in trainer doc (#10554)

==

src/transformers/trainer.py
==================
6b58e1550;Lysandre Debut;2021-03-05 18:10:19 +0100;Fix torch 1.8.0 segmentation fault (#10546)
* Only run one test

* Patch segfault

* Fix summarization pipeline

* Ready for merge
==

tests/test_modeling_fsmt.py
tests/test_modeling_t5.py
tests/test_pipelines_summarization.py
==================
395ffcd75;Patrick von Platen;2021-03-05 18:17:12 +0300;fix run seq2seq (#10547)

==

examples/research_projects/wav2vec2/run_asr.py
==================
54e55b52d;Nicolas Patry;2021-03-05 15:24:14 +0100;Fixing conversation test for torch 1.8 (#10545)

==

tests/test_pipelines_conversational.py
==================
dc9aaa384;Lysandre;2021-03-05 07:56:04 -0500;Pin torch to 1.7.1 in tests while we resolve issues

==

.circleci/config.yml
.github/workflows/self-push.yml
.github/workflows/self-scheduled.yml
==================
12b66215c;lewtun;2021-03-05 13:44:53 +0100;Fix example of custom Trainer to reflect signature of compute_loss (#10537)

==

docs/source/main_classes/trainer.rst
==================
093b88f4e;Lysandre;2021-03-05 07:31:51 -0500;Update scatter to use torch 1.8.0

==

.circleci/config.yml
.github/workflows/self-push.yml
.github/workflows/self-scheduled.yml
==================
c503a1c15;Patrick von Platen;2021-03-04 23:27:12 +0300;[ProphetNet] Bart-like Refactor (#10501)
* first step to refactor

* make all fast tests pass

* make all slow tests pass

* save intermediate

* correct cache

* finish PR

* make fp16 work
==

src/transformers/models/prophetnet/configuration_prophetnet.py
src/transformers/models/prophetnet/modeling_prophetnet.py
tests/test_modeling_prophetnet.py
==================
6290169eb;Sylvain Gugger;2021-03-04 11:46:11 -0500;Rework TPU checkpointing in Trainer (#10504)
* Rework TPU checkpointing in Trainer

* Wraps the barrier in a dist test

* Address review comments

* Remove line
==

src/transformers/configuration_utils.py
src/transformers/modeling_utils.py
src/transformers/trainer.py
tests/test_trainer.py
==================
805c5200d;Philipp Schmid;2021-03-04 17:12:37 +0100;Removes overwrites for output_dir  (#10521)
* removed overwrites

* remove default value for output_dir

* adjusted typing
==

src/transformers/trainer.py
src/transformers/training_args.py
==================
a5bd40b75;Sylvain Gugger;2021-03-04 11:11:39 -0500;Not always consider a local model a checkpoint in run_glue (#10517)

==

examples/text-classification/run_glue.py
==================
745ea78dc;Sylvain Gugger;2021-03-04 09:44:02 -0500;Revert "Not always consider a local model a checkpoint in run_glue"
This reverts commit f3660613bc14188e04e8eb4e27ae97f57b6b92d6.

==

examples/text-classification/run_glue.py
==================
f3660613b;Sylvain Gugger;2021-03-04 09:44:02 -0500;Not always consider a local model a checkpoint in run_glue

==

examples/text-classification/run_glue.py
==================
948b730f9;Sylvain Gugger;2021-03-03 14:55:18 -0500;Remove unsupported methods from ModelOutput doc (#10505)

==

docs/source/main_classes/output.rst
==================
b70f441b7;Sylvain Gugger;2021-03-03 12:13:29 -0500;Smp grad accum (#10488)
* Fix gradient accumulation for SM Model Parallelism

* Style and divide loss by grad accum steps
==

src/transformers/sagemaker/trainer_sm.py
src/transformers/sagemaker/training_args_sm.py
src/transformers/trainer.py
src/transformers/training_args.py
==================
d064fb564;felixgwu;2021-03-03 12:05:21 -0500;Fix the bug in constructing the all_hidden_states of DeBERTa v2 (#10466)
* fix all_hidden_states

* use output_states instead of next_kv
==

src/transformers/models/deberta_v2/modeling_deberta_v2.py
==================
188574ac5;Stas Bekman;2021-03-03 08:54:00 -0800;remap MODEL_FOR_QUESTION_ANSWERING_MAPPING classes to names auto-generated file (#10487)
* remap classes to strings

* missing new util

* style

* doc

* move the autogenerated file

* Trigger CI
==

Makefile
src/transformers/trainer.py
src/transformers/utils/modeling_auto_mapping.py
utils/class_mapping_update.py
==================
801ff969c;Sylvain Gugger;2021-03-03 11:21:17 -0500;Refactor checkpoint name in BERT and MobileBERT (#10424)
* Refactor checkpoint name in BERT and MobileBERT

* Add option to check copies

* Add QuestionAnswering

* Add last models

* Make black happy
==

src/transformers/models/bert/modeling_bert.py
src/transformers/models/mobilebert/modeling_mobilebert.py
utils/check_copies.py
==================
39f70a405;Jeff Yang;2021-03-03 22:47:12 +0630;feat(docs): navigate with left/right arrow keys (#10481)
* feat(docs): navigate with left/right arrow keys

* fix: add missing comma
==

docs/source/conf.py
==================
2d2ed2cc1;Patrick von Platen;2021-03-03 12:42:41 +0300;[T5] Fix speed degradation bug t5 (#10496)
* fix speed degradation bug t5

* fix for all models

* fix code quality
==

src/transformers/models/bart/modeling_bart.py
src/transformers/models/blenderbot/modeling_blenderbot.py
src/transformers/models/blenderbot_small/modeling_blenderbot_small.py
src/transformers/models/led/modeling_led.py
src/transformers/models/marian/modeling_marian.py
src/transformers/models/mbart/modeling_mbart.py
src/transformers/models/pegasus/modeling_pegasus.py
src/transformers/models/t5/modeling_t5.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_{{cookiecutter.lowercase_modelname}}.py
==================
5dc303e28;WybeKoper;2021-03-03 09:47:25 +0100;Fixed minor spelling mistakes (#10489)
Co-authored-by: WybeKoper <WybeKoper@users.noreply.github.com>
==

src/transformers/data/processors/squad.py
src/transformers/models/ibert/quant_modules.py
src/transformers/trainer_pt_utils.py
==================
1750e6290;Mehrad Moradshahi;2021-03-03 00:27:02 -0800;Generate can return cross-attention weights too (#10493)

==

src/transformers/generation_utils.py
tests/test_generation_utils.py
==================
b01384224;Martin Schmitt;2021-03-02 08:41:54 +0100;Changed `num_beams` to `num_beams // num_beam_groups` when initialising `PrefixConstrainedLogitsProcessor` in `_get_logits_processor` to fix compatibility issue when constrained decoding is used together with grouped beam search (#10475)

==

src/transformers/generation_utils.py
==================
0c2325198;Lysandre Debut;2021-03-01 18:12:31 +0100;Add I-BERT to README (#10462)

==

README.md
docs/source/index.rst
==================
9248e2703;Lysandre Debut;2021-03-01 10:23:40 -0500;Remove Anthony from the bug reports in Transformers

==

.github/ISSUE_TEMPLATE/bug-report.md
==================
a106bde5a;Suraj Patil;2021-03-01 20:19:52 +0530;[Wav2Vec2FeatureExtractor] smal fixes (#10455)
* smal fixes

* don't check for None
==

src/transformers/models/wav2vec2/feature_extraction_wav2vec2.py
==================
11655fafd;Patrick von Platen;2021-03-01 12:30:12 +0300;remove feature extraction config (#10457)

==

src/transformers/models/wav2vec2/configuration_wav2vec2.py
==================
0234de841;Patrick von Platen;2021-03-01 12:13:17 +0300;Add Fine-Tuning for Wav2Vec2 (#10145)
* add encode labels function to tokenizer

* start adding finetuning

* init dropout

* upload

* correct convert script

* apply changes

* fix second typo

* make first dummy training run

* adapt convert script

* push confg for comparison

* remove conf

* finish training

* adapt data collator

* add research folder

* update according to fairseq feedback

* some minor corrections

* refactor masking indices a bit

* some minor changes

* clean tokenizer

* finish clean-up

* remove previous logic

* update run script

* correct training

* finish changes

* finish model

* correct bug

* fix training a bit more

* add some tests

* finish gradient checkpointing

* finish example

* correct gradient checkpointing

* improve tokenization method

* revert changes in tokenizer

* revert general change

* adapt fine-tuning

* update

* save intermediate test

* Update README.md

* finish finetuning

* delete conversion script

* Update src/transformers/models/wav2vec2/configuration_wav2vec2.py

* Update src/transformers/models/wav2vec2/processing_wav2vec2.py

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* finish wav2vec2 script

* finish wav2vec2 fine-tuning

* finalize test

* correct test

* adapt tests

* finish

* remove test file

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
==

docs/source/model_doc/wav2vec2.rst
examples/research_projects/wav2vec2/README.md
examples/research_projects/wav2vec2/finetune_base_100.sh
examples/research_projects/wav2vec2/finetune_large_lv60_100.sh
examples/research_projects/wav2vec2/requirements.txt
examples/research_projects/wav2vec2/run_asr.py
src/transformers/models/wav2vec2/configuration_wav2vec2.py
src/transformers/models/wav2vec2/convert_wav2vec2_original_pytorch_checkpoint_to_pytorch.py
src/transformers/models/wav2vec2/modeling_wav2vec2.py
src/transformers/models/wav2vec2/processing_wav2vec2.py
src/transformers/trainer.py
src/transformers/trainer_pt_utils.py
tests/test_modeling_common.py
tests/test_modeling_wav2vec2.py
==================
3c733f320;Patrick von Platen;2021-02-28 19:03:49 +0300;Update ibert.rst (#10445)

==

docs/source/model_doc/ibert.rst
==================
aeba4f95b;Darigov Research;2021-02-28 13:27:54 +0000;Adds terms to Glossary (#10443)
* feat: Adds three definitions to glossary from @cronoik

Needed a definition for transformer which in turn needed 2 more definitions

To do with issue https://github.com/huggingface/transformers/issues/9078

* fix: Adjusts definition of neural network to make it easier to read
==

docs/source/glossary.rst
==================
256482ac9;Tanmay Garg;2021-02-28 06:04:22 +0530;Introduce save_strategy training argument (#10286)
* Introduce save_strategy training argument

* deprecate EvaluationStrategy

* collapse EvaluationStrategy and LoggingStrategy into a single
  IntervalStrategy enum

* modify tests to use modified enum
==

docs/source/internal/trainer_utils.rst
src/transformers/__init__.py
src/transformers/integrations.py
src/transformers/trainer_callback.py
src/transformers/trainer_tf.py
src/transformers/trainer_utils.py
src/transformers/training_args.py
src/transformers/training_args_tf.py
src/transformers/utils/notebook.py
tests/test_trainer.py
tests/test_trainer_callback.py
==================
aca6288ff;Bhadresh Savani;2021-02-27 23:23:44 +0530;updated logging and saving metrics (#10436)
* updated logging and saving metrics

* space removal
==

examples/language-modeling/run_clm.py
examples/language-modeling/run_mlm.py
examples/language-modeling/run_plm.py
examples/multiple-choice/run_swag.py
examples/multiple-choice/run_tf_multiple_choice.py
examples/question-answering/run_qa.py
examples/question-answering/run_qa_beam_search.py
examples/text-classification/run_glue.py
examples/text-classification/run_tf_glue.py
examples/text-classification/run_tf_text_classification.py
examples/text-classification/run_xnli.py
examples/token-classification/run_ner.py
==================
f52a15897;Stas Bekman;2021-02-27 08:21:50 -0800;[run_seq2seq.py] restore functionality: saving to test_generations.txt (#10428)
This PR restores the original functionality that for some reason was modified.

Fixes: https://github.com/huggingface/transformers/issues/10381

@sgugger
==

examples/seq2seq/run_seq2seq.py
==================
311b7048c;Lysandre Debut;2021-02-27 02:20:30 +0100;Fix conda-build (#10431)

==

.github/conda/meta.yaml
==================
ee04b6982;Stas Bekman;2021-02-26 17:01:01 -0800;[examples] better model example (#10427)
* refactors

* typo
==

examples/seq2seq/run_seq2seq.py
src/transformers/trainer.py
src/transformers/trainer_pt_utils.py
==================
a85eb616f;Amog Kamsetty;2021-02-26 16:06:08 -0800;Ray Tune Integration Bug Fixes (#10406)
* fixes

* update resources

* formatting

* remove import

* add log statement

* use fstring

* add period

* Update src/transformers/integrations.py
==

src/transformers/integrations.py
src/transformers/trainer.py
==================
98569d4ba;Kai Fricke;2021-02-26 16:18:33 +0100;Add Ray Tune hyperparameter search integration test (#10414)

==

tests/test_trainer.py
==================
d03695f3a;Patrick von Platen;2021-02-26 17:53:28 +0300;[LED] Correct Docs (#10419)
* correct docs

* correct tf model docs as well
==

src/transformers/models/led/modeling_led.py
src/transformers/models/led/modeling_tf_led.py
==================
7fc686efb;Mansi Mane;2021-02-26 05:04:55 -0800;Sagemaker Model Parallel tensoboard writing fix (#10403)
* Added tb fix

* Removed local rank condition

* Updated reference to args
==

src/transformers/sagemaker/trainer_sm.py
==================
83d2d55c9;Julien Chaumond;2021-02-26 10:35:36 +0100;[ci, flax] non-existing models are unlikely to pass tests (#10409)
üòÇ
==

tests/test_flax_auto.py
==================
17b6e0d47;Sylvain Gugger;2021-02-25 15:30:38 -0500;Fix run_glue evaluation when model has a label correspondence (#10401)

==

examples/text-classification/run_glue.py
src/transformers/configuration_utils.py
==================
26f8b2cb1;Sylvain Gugger;2021-02-25 11:42:25 -0500;Make Barthez tokenizer tests a bit faster (#10399)
* Make Barthez tokenizer tests a bit faster

* Quality
==

tests/test_tokenization_barthez.py
tests/test_tokenization_common.py
tests/test_tokenization_mbart.py
tests/test_tokenization_mbart50.py
==================
b040e6efc;Andrea Bacciu;2021-02-25 17:18:33 +0100;Fix None in add_token_positions - issue #10210 (#10374)
* Fix None in add_token_positions - issue #10210

Fix None in add_token_positions related to the issue #10210

* add_token_positions fix None values in end_positions vector

add_token_positions fix None in end_positions vector as proposed by @joeddav
==

docs/source/custom_datasets.rst
==================
9d14be5c2;Sylvain Gugger;2021-02-25 11:07:53 -0500;Add support for ZeRO-2/3 and ZeRO-offload in fairscale (#10354)
* Ass support for ZeRO-2/3 and ZeRO-offload in fairscale

* Quality

* Rework from review comments

* Add doc

* Apply suggestions from code review

Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>

* Address review comments

Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>
==

docs/source/main_classes/trainer.rst
examples/tests/trainer/test_trainer_ext.py
src/transformers/trainer.py
src/transformers/trainer_utils.py
src/transformers/training_args.py
==================
88cc26dcd;Lysandre Debut;2021-02-25 16:42:27 +0100;Ignore unexpected weights from PT conversion (#10397)

==

src/transformers/models/bert/modeling_tf_bert.py
==================
63645b3b1;Sehoon Kim;2021-02-26 00:06:42 +0900;I-BERT model support (#10153)
* IBertConfig, IBertTokentizer added

* IBert Model names moified

* tokenizer bugfix

* embedding -> QuantEmbedding

* quant utils added

* quant_mode added to configuration

* QuantAct added, Embedding layer + QuantAct addition

* QuantAct added

* unused path removed, QKV quantized

* self attention layer all quantized, except softmax

* temporarl commit

* all liner layers quantized

* quant_utils bugfix

* bugfix: requantization missing

* IntGELU added

* IntSoftmax added

* LayerNorm implemented

* LayerNorm implemented all

* names changed: roberta->ibert

* config not inherit from ROberta

* No support for CausalLM

* static quantization added, quantize_model.py removed

* import modules uncommented

* copyrights fixed

* minor bugfix

* quant_modules, quant_utils merged as one file

* import * fixed

* unused runfile removed

* make style run

* configutration.py docstring fixed

* refactoring: comments removed, function name fixed

* unused dependency removed

* typo fixed

* comments(Copied from), assertion string added

* refactoring: super(..) -> super(), etc.

* refactoring

* refarctoring

* make style

* refactoring

* cuda -> to(x.device)

* weight initialization removed

* QuantLinear set_param removed

* QuantEmbedding set_param removed

* IntLayerNorm set_param removed

* assert string added

* assertion error message fixed

* is_decoder removed

* enc-dec arguments/functions removed

* Converter removed

* quant_modules docstring fixed

* conver_slow_tokenizer rolled back

* quant_utils docstring fixed

* unused aruments e.g. use_cache removed from config

* weight initialization condition fixed

* x_min, x_max initialized with small values to avoid div-zero exceptions

* testing code for ibert

* test emb, linear, gelu, softmax added

* test ln and act added

* style reformatted

* force_dequant added

* error tests overrided

* make style

* Style + Docs

* force dequant tests added

* Fix fast tokenizer in init

* Fix doc

* Remove space

* docstring, IBertConfig, chunk_size

* test_modeling_ibert refactoring

* quant_modules.py refactoring

* e2e integration test added

* tokenizers removed

* IBertConfig added to tokenizer_auto.py

* bugfix

* fix docs & test

* fix style num 2

* final fixes

Co-authored-by: Sehoon Kim <sehoonkim@berkeley.edu>
Co-authored-by: Lysandre <lysandre.debut@reseau.eseo.fr>
Co-authored-by: Sylvain Gugger <sylvain.gugger@gmail.com>
Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
==

docs/source/index.rst
docs/source/model_doc/ibert.rst
src/transformers/__init__.py
src/transformers/models/auto/configuration_auto.py
src/transformers/models/auto/modeling_auto.py
src/transformers/models/auto/tokenization_auto.py
src/transformers/models/ibert/__init__.py
src/transformers/models/ibert/configuration_ibert.py
src/transformers/models/ibert/modeling_ibert.py
src/transformers/models/ibert/quant_modules.py
src/transformers/utils/dummy_pt_objects.py
tests/test_modeling_ibert.py
==================
cb38ffcc5;Patrick von Platen;2021-02-25 17:42:46 +0300;[PretrainedFeatureExtractor] + Wav2Vec2FeatureExtractor, Wav2Vec2Processor, Wav2Vec2Tokenizer (#10324)
* push to show

* small improvement

* small improvement

* Update src/transformers/feature_extraction_utils.py

* Update src/transformers/feature_extraction_utils.py

* implement base

* add common tests

* make all tests pass for wav2vec2

* make padding work & add more tests

* finalize feature extractor utils

* add call method to feature extraction

* finalize feature processor

* finish tokenizer

* finish general processor design

* finish tests

* typo

* remove bogus file

* finish docstring

* add docs

* finish docs

* small fix

* correct docs

* save intermediate

* load changes

* apply changes

* apply changes to doc

* change tests

* apply surajs recommend

* final changes

* Apply suggestions from code review

* fix typo

* fix import

* correct docstring
==

docs/source/index.rst
docs/source/internal/file_utils.rst
docs/source/internal/tokenization_utils.rst
docs/source/main_classes/feature_extractor.rst
docs/source/model_doc/wav2vec2.rst
examples/multiple-choice/run_swag.py
src/transformers/__init__.py
src/transformers/data/data_collator.py
src/transformers/feature_extraction_utils.py
src/transformers/file_utils.py
src/transformers/models/auto/tokenization_auto.py
src/transformers/models/dpr/tokenization_dpr.py
src/transformers/models/dpr/tokenization_dpr_fast.py
src/transformers/models/rag/retrieval_rag.py
src/transformers/models/tapas/tokenization_tapas.py
src/transformers/models/wav2vec2/__init__.py
src/transformers/models/wav2vec2/feature_extraction_wav2vec2.py
src/transformers/models/wav2vec2/modeling_wav2vec2.py
src/transformers/models/wav2vec2/processing_wav2vec2.py
src/transformers/models/wav2vec2/tokenization_wav2vec2.py
src/transformers/pipelines/question_answering.py
src/transformers/pipelines/table_question_answering.py
src/transformers/tokenization_utils.py
src/transformers/tokenization_utils_base.py
src/transformers/tokenization_utils_fast.py
src/transformers/trainer_utils.py
tests/test_feature_extraction_common.py
tests/test_feature_extraction_wav2vec2.py
tests/test_modeling_wav2vec2.py
tests/test_pipelines_common.py
tests/test_processor_wav2vec2.py
tests/test_tokenization_wav2vec2.py
utils/check_repo.py
==================
9dc782574;abhishek thakur;2021-02-25 15:18:47 +0100;Remove unused variable in example for Q&A (#10392)

==

docs/source/task_summary.rst
==================
894db6701;mingruimingrui;2021-02-25 19:33:13 +0800;Bugfix: Removal of padding_idx in BartLearnedPositionalEmbedding (#10200)
* Assumption of padding_idx <2 might not stand

* Use offset instead of 2

* Fix with black

* Change behavior to warning instead for backward compatibility.

* Fix with black

* Remove warning

* Make padding_idx non-required

* padding_idx fix for blenderbot

* padding_idx fix for blenderbot_small

* padding_idx fix for led

* padding_idx fix for mbart

* Remove extra whitespaces

* padding_idx fix for template

* Fix padding_idx passed to nn.Embedding mistake

* Fixed padding_idx passed to positional embedding in template

* Remove padding_idx from pytorch learned positional embeddings

* Remove accidentally added quotes

* Remove padding_idx from tf learned positional embeddings

* Remove zeroing of weights in __init__

Co-authored-by: Wang Ming Rui <mingrui.wang@C02CJTUYMD6M.local>
==

src/transformers/models/bart/modeling_bart.py
src/transformers/models/bart/modeling_tf_bart.py
src/transformers/models/blenderbot/modeling_blenderbot.py
src/transformers/models/blenderbot/modeling_tf_blenderbot.py
src/transformers/models/blenderbot_small/modeling_blenderbot_small.py
src/transformers/models/blenderbot_small/modeling_tf_blenderbot_small.py
src/transformers/models/led/modeling_led.py
src/transformers/models/led/modeling_tf_led.py
src/transformers/models/mbart/modeling_mbart.py
src/transformers/models/mbart/modeling_tf_mbart.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_tf_{{cookiecutter.lowercase_modelname}}.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_{{cookiecutter.lowercase_modelname}}.py
==================
55fe80d08;Lysandre Debut;2021-02-25 01:48:00 +0100;Only run model templates tests once (#10388)

==

.github/workflows/model-templates.yml
==================
22bd047e9;Lysandre Debut;2021-02-25 01:23:39 +0100;Run GA on every push even on forks (#10383)

==

.github/workflows/model-templates.yml
==================
359184430;Lysandre;2021-02-24 15:19:01 -0500;v4.3.3 docs

==

.circleci/deploy.sh
docs/source/_static/js/custom.js
==================
bdbb2c756;Stas Bekman;2021-02-24 08:32:52 -0800;[trainer] move secondary methods into a separate file (#10363)
* move secondary methods into a separate file

* cleanup

* style
==

src/transformers/trainer.py
src/transformers/trainer_pt_utils.py
==================
5f2a3d721;Poedator;2021-02-24 17:01:28 +0300;fix deprecated ref to `tokenizer.max_len` (#10220)
This is to fix deprecated reference to `tokenizer.max_len` with `tokenizer.model_max_length` - similar to [issue 8739](https://github.com/huggingface/transformers/issues/8739) and [PR 8604](https://github.com/huggingface/transformers/pull/8604). 
Example [here](https://colab.research.google.com/gist/poedator/f8776349e5c625ce287fc6fcd312fa1e/tokenizer-max_len-error-in-transformers_glue.ipynb). The error happens when `glue_convert_examples_to_features` is called without `max_length` parameter specified. In that case line 119 with wrong reference gets called. This simple fix should  do it.
==

src/transformers/data/processors/glue.py
==================
cdcdd5f03;Julien Plu;2021-02-24 14:38:29 +0100;Rework casts (#10274)

==

src/transformers/models/xlnet/modeling_tf_xlnet.py
==================
2d458b2c7;abhishek thakur;2021-02-24 12:55:34 +0100;ConvBERT fix torch <> tf weights conversion (#10314)
* convbert conversion test

* fin

* fin

* fin

* clean up tf<->pt conversion

* remove from_pt

Co-authored-by: patrickvonplaten <patrick.v.platen@gmail.com>
==

src/transformers/modeling_tf_pytorch_utils.py
src/transformers/models/convbert/convert_convbert_original_tf1_checkpoint_to_pytorch_and_tf2.py
src/transformers/models/convbert/modeling_tf_convbert.py
tests/test_modeling_tf_convbert.py
==================
3437d1213;Stas Bekman;2021-02-23 17:42:25 -0800;[Trainer/Deepspeed] handle get_last_lr() before first step() (#10362)
* handle get_last_lr() before first step()

* abstract away the lr getting logic

* cleanup

* add test

* move to utils
==

examples/tests/deepspeed/test_deepspeed.py
src/transformers/trainer.py
src/transformers/trainer_pt_utils.py
==================
4a1ab7cb6;Julien Chaumond;2021-02-23 18:30:47 +0100;[bert-base-german-cased] cp to hardcoded urls (#10353)

==

src/transformers/models/bert/tokenization_bert.py
src/transformers/models/bert/tokenization_bert_fast.py
==================
23e87c27b;Akmal;2021-02-23 22:49:25 +0700;Fix broken examples/seq2seq/README.md markdown (#10344)

==

examples/seq2seq/README.md
==================
83f890ddd;Lysandre;2021-02-23 08:53:55 -0500;Easier self-scheduled debugging

==

.github/workflows/self-scheduled.yml
==================
461e8cacf;Sylvain Gugger;2021-02-22 16:39:02 -0500;Fix evaluation with label smoothing in Trainer (#10338)

==

src/transformers/trainer.py
==================
622a8c599;Stas Bekman;2021-02-22 13:02:53 -0800;[trainer] add Trainer methods for metrics logging and saving  (#10266)
* make logging and saving trainer built-in

* Update src/transformers/trainer.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

examples/seq2seq/run_seq2seq.py
src/transformers/trainer.py
==================
94d8767ba;Tanmay Garg;2021-02-23 02:03:00 +0530;Loading from last checkpoint functionality in Trainer.train (#10334)
Enhance resume_from_checkpoint argument of Trainer.train to accept
bool type. If True given, last saved checkpoint in self.args.output_dir
will be loaded. (#10280)
==

src/transformers/trainer.py
==================
eab0afc19;Stas Bekman;2021-02-22 11:15:59 -0800;[Trainer] implement gradient_accumulation_steps support in DeepSpeed integration (#10310)
* implement gradient_accumulation_steps support in DeepSpeed integration

* typo

* cleanup

* cleanup
==

docs/source/main_classes/trainer.rst
examples/tests/deepspeed/ds_config.json
examples/tests/deepspeed/test_deepspeed.py
src/transformers/testing_utils.py
src/transformers/trainer.py
==================
f991daed1;Stas Bekman;2021-02-22 10:58:50 -0800;defensive programming + expand/correct README (#10295)

==

examples/seq2seq/README.md
examples/seq2seq/run_seq2seq.py
==================
9e147d31f;Sylvain Gugger;2021-02-22 12:36:16 -0500;Deprecate prepare_seq2seq_batch (#10287)
* Deprecate prepare_seq2seq_batch

* Fix last tests

* Apply suggestions from code review

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
Co-authored-by: Suraj Patil <surajp815@gmail.com>

* More review comments

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
Co-authored-by: Suraj Patil <surajp815@gmail.com>
==

docs/source/model_doc/fsmt.rst
docs/source/model_doc/marian.rst
docs/source/model_doc/mbart.rst
docs/source/model_doc/pegasus.rst
docs/source/model_doc/rag.rst
docs/source/model_doc/t5.rst
scripts/fsmt/fsmt-make-super-tiny-model.py
scripts/fsmt/fsmt-make-tiny-model.py
src/transformers/models/marian/modeling_marian.py
src/transformers/models/marian/modeling_tf_marian.py
src/transformers/models/marian/tokenization_marian.py
src/transformers/models/mbart/tokenization_mbart.py
src/transformers/models/mbart/tokenization_mbart_fast.py
src/transformers/models/mt5/modeling_mt5.py
src/transformers/models/mt5/modeling_tf_mt5.py
src/transformers/models/rag/modeling_rag.py
src/transformers/models/rag/tokenization_rag.py
src/transformers/tokenization_utils_base.py
tests/test_modeling_marian.py
tests/test_modeling_mbart.py
tests/test_modeling_tf_marian.py
tests/test_modeling_tf_mbart.py
tests/test_modeling_tf_pegasus.py
tests/test_tokenization_bart.py
tests/test_tokenization_barthez.py
tests/test_tokenization_marian.py
tests/test_tokenization_mbart.py
tests/test_tokenization_mbart50.py
tests/test_tokenization_pegasus.py
tests/test_tokenization_prophetnet.py
tests/test_tokenization_t5.py
==================
e73a3e189;Lysandre Debut;2021-02-22 15:48:20 +0100;Add note to resize token embeddings matrix when adding new tokens to voc (#10331)

==

src/transformers/tokenization_utils_base.py
==================
19e737b93;Julien Plu;2021-02-22 15:41:56 +0100;Making TF Longformer-like models compliant with AMP (#10233)
* AMP

* Add LED

* Apply style

* Fix longformer
==

src/transformers/models/led/modeling_tf_led.py
src/transformers/models/longformer/modeling_tf_longformer.py
tests/test_modeling_tf_led.py
tests/test_modeling_tf_longformer.py
==================
cd8c4c3fc;Lysandre Debut;2021-02-22 13:45:18 +0100;DeBERTa-v2 fixes (#10328)
Co-authored-by: Pengcheng He <penhe@microsoft.com>

Co-authored-by: Pengcheng He <penhe@microsoft.com>
==

README.md
docs/source/index.rst
==================
88605f37a;tagucci;2021-02-22 00:54:27 +0900;fix typo in conversion script (#10316)
* fix typo in conversion script

* style

Co-authored-by: Stas Bekman <stas@stason.org>
==

src/transformers/models/fsmt/convert_fsmt_original_pytorch_checkpoint_to_pytorch.py
==================
cdd31b4de;Stas Bekman;2021-02-20 13:28:43 -0800;don't fail when there are no zombies (#10308)

==

.github/workflows/self-push.yml
.github/workflows/self-scheduled.yml
==================
a2e379743;Sylvain Gugger;2021-02-20 15:46:54 -0500;Fix style

==

src/transformers/tokenization_utils_base.py
==================
a0dfc2d30;cronoik;2021-02-20 21:21:33 +0100;fixes #10303 (#10304)

==

src/transformers/tokenization_utils_base.py
==================
9a7e63729;Pengcheng He;2021-02-19 15:34:44 -0800;Integrate DeBERTa v2(the 1.5B model surpassed human performance on Su‚Ä¶ (#10018)
* Integrate DeBERTa v2(the 1.5B model surpassed human performance on SuperGLUE); Add DeBERTa v2 900M,1.5B models;

* DeBERTa-v2

* Fix v2 model loading issue (#10129)

* Doc members

* Update src/transformers/models/deberta/modeling_deberta.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Address Sylvain's comments

* Address Patrick's comments

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* Style

Co-authored-by: Lysandre <lysandre.debut@reseau.eseo.fr>
Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
==

README.md
docs/source/index.rst
docs/source/model_doc/deberta.rst
docs/source/model_doc/deberta_v2.rst
docs/source/pretrained_models.rst
src/transformers/__init__.py
src/transformers/models/auto/configuration_auto.py
src/transformers/models/auto/modeling_auto.py
src/transformers/models/auto/tokenization_auto.py
src/transformers/models/deberta/configuration_deberta.py
src/transformers/models/deberta/modeling_deberta.py
src/transformers/models/deberta/tokenization_deberta.py
src/transformers/models/deberta_v2/__init__.py
src/transformers/models/deberta_v2/configuration_deberta_v2.py
src/transformers/models/deberta_v2/modeling_deberta_v2.py
src/transformers/models/deberta_v2/tokenization_deberta_v2.py
src/transformers/utils/dummy_pt_objects.py
tests/test_modeling_deberta_v2.py
tests/test_tokenization_deberta_v2.py
==================
f6e53e3c2;Sylvain Gugger;2021-02-19 18:04:15 -0500;Fix example links in the task summary (#10291)

==

docs/source/task_summary.rst
==================
536aee99b;Julien Plu;2021-02-19 22:06:13 +0100;Move the TF NER example (#10276)

==

examples/legacy/token-classification/README.md
examples/legacy/token-classification/run_tf_ner.py
examples/token-classification/README.md
==================
cbadb5243;Joe Davison;2021-02-19 14:06:57 -0500;Zero shot distillation script cuda patch (#10284)

==

examples/research_projects/zero-shot-distillation/distill_classifier.py
==================
f1299f503;Stas Bekman;2021-02-19 10:36:37 -0800;Kill any run-away pytest processes (#10281)

==

.github/workflows/self-push.yml
.github/workflows/self-scheduled.yml
==================
709c86b5a;Tanmay Garg;2021-02-19 22:19:22 +0530;Introduce logging_strategy training argument (#10267) (#10267)
Introduce logging_strategy training argument
in TrainingArguments and TFTrainingArguments. (#9838)
==

src/transformers/trainer_callback.py
src/transformers/trainer_utils.py
src/transformers/training_args.py
src/transformers/training_args_tf.py
==================
34df26ec3;Julien Plu;2021-02-19 15:33:25 +0100;Making TF OpenAI GPT model compliant with AMP and XLA (#10261)
* Fix AMP and XLA

* Remove useless var
==

src/transformers/models/openai/modeling_tf_openai.py
tests/test_modeling_tf_openai.py
==================
3e116ed33;Julien Plu;2021-02-19 12:58:07 +0100;Making TF TransfoXL model compliant with AMP (#10264)
* Fix AMP

* Apply style

* Remove unused import
==

src/transformers/models/transfo_xl/modeling_tf_transfo_xl.py
src/transformers/models/transfo_xl/modeling_tf_transfo_xl_utilities.py
tests/test_modeling_tf_transfo_xl.py
==================
86caeb763;Julien Plu;2021-02-19 12:57:16 +0100;Fix XLA and AMP (#10262)

==

src/transformers/models/t5/modeling_tf_t5.py
tests/test_modeling_tf_t5.py
==================
3d72d47f0;Julien Plu;2021-02-19 12:56:41 +0100;Making TF MPNet model compliant with XLA (#10260)
* Fix XLA

* Rework cast

* Apply style
==

src/transformers/models/mpnet/modeling_tf_mpnet.py
tests/test_modeling_tf_mpnet.py
==================
fb56bf258;Julien Plu;2021-02-19 12:55:25 +0100;Making TF MobileBert model compliant with AMP  (#10259)
* Fix AMP

* Trigger CI

* Rework cast
==

src/transformers/models/mobilebert/modeling_tf_mobilebert.py
tests/test_modeling_tf_mobilebert.py
==================
2fc6284f0;Julien Plu;2021-02-19 12:54:14 +0100;Making TF Lxmert model compliant with AMP (#10257)
* Fix AMP

* Rework cast

* Apply style
==

src/transformers/models/lxmert/modeling_tf_lxmert.py
tests/test_modeling_tf_lxmert.py
==================
d27b28d95;Stas Bekman;2021-02-18 17:15:51 -0800;[ISSUES.md] propose using google colab to reproduce problems (#10270)
* propose using google colab to reproduce problems

* Update ISSUES.md

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

ISSUES.md
==================
4eddc459a;Stas Bekman;2021-02-18 17:02:35 -0800;[trainer] implement support for full fp16 in evaluation/predict (#10268)
* implement --fp16_full_eval

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* style

* add test

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

src/transformers/trainer.py
src/transformers/training_args.py
tests/test_trainer.py
==================
d9a81fc0c;Stas Bekman;2021-02-18 16:44:42 -0800;fix func signature (#10271)

==

tests/test_trainer.py
==================
c6fe17557;Joe Davison;2021-02-18 17:08:45 -0500;Script for distilling zero-shot classifier to more efficient student (#10244)
* add zero-shot distillation script

* readme wordsmithing

* clean up code

* add multi-gpu teacher inference
plus tidying up more code

* add use_fast_tokenizer arg

* update results in readme

* more readme wordsmithing

* style

* Add handle to readme

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* fix code block

* add error+docs about distributed & tpu

* add @sgugger format requests

* xla -> tpu

* support fp16 for teacher preds

* no checkpoint by default

* add demo colab link

* add model sharing prompt + model link

* correct resulting acc of example

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
==

examples/research_projects/zero-shot-distillation/README.md
examples/research_projects/zero-shot-distillation/distill_classifier.py
==================
97e688bc2;Stas Bekman;2021-02-18 09:27:32 -0800;[Trainer] memory tracker metrics (#10225)
* memory tracker metrics

* go back to eval for somewhat consistency

* handle no-gpu case

* deal with stackable eval calls

* restore callback order

* style

* simplify the API

* add test

* docs

* consistently use eval_ prefix

* improve docs

* Update src/transformers/trainer_utils.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* rename method

* style

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

examples/seq2seq/run_seq2seq.py
examples/tests/deepspeed/test_deepspeed.py
src/transformers/file_utils.py
src/transformers/trainer.py
src/transformers/trainer_utils.py
src/transformers/training_args.py
tests/test_trainer.py
==================
d7f38c5d1;Tanmay Garg;2021-02-18 22:53:33 +0530;Introduce warmup_ratio training argument (#10229)
Introduce warmup_ratio training argument in both
TrainingArguments and TFTrainingArguments classes (#6673)
==

src/transformers/trainer.py
src/transformers/trainer_tf.py
src/transformers/training_args.py
src/transformers/training_args_tf.py
==================
2acae50a0;Julien Plu;2021-02-18 15:52:57 +0100;Reduce the time spent for the TF slow tests (#10152)
* rework savedmodel slow test

* Improve savedmodel tests

* Remove useless content
==

src/transformers/modeling_tf_utils.py
tests/test_modeling_tf_common.py
tests/test_modeling_tf_convbert.py
tests/test_modeling_tf_led.py
tests/test_modeling_tf_longformer.py
tests/test_modeling_tf_lxmert.py
tests/test_modeling_tf_t5.py
==================
14ed3b978;Julien Plu;2021-02-18 12:29:43 +0100;Fix AMP (#10216)

==

src/transformers/models/funnel/modeling_tf_funnel.py
tests/test_modeling_tf_funnel.py
==================
bdf1669e3;Julien Plu;2021-02-18 09:36:01 +0100;Making TF GPT2 compliant with XLA and AMP (#10230)
* Fix XLA and AMP

* Fix AMP and XLA

* Apply style

* Apply Patrick's comment
==

src/transformers/modeling_tf_utils.py
src/transformers/models/gpt2/modeling_tf_gpt2.py
tests/test_modeling_tf_gpt2.py
==================
5da7c78ed;Stas Bekman;2021-02-17 15:58:08 -0800;update to new script; notebook notes (#10241)

==

docs/source/main_classes/trainer.rst
==================
dee876cef;Stas Bekman;2021-02-17 15:52:36 -0800;[trainer] refactor place_model_on_device logic, add deepspeed (#10243)
* refactor place_model_on_device logic, add deepspeed

* doc

* style
==

src/transformers/trainer.py
==================
d1eb88f42;Stas Bekman;2021-02-17 14:12:39 -0800;[CI] 2 fixes (#10248)
* fix invalid port

* missing requirements
==

.github/workflows/self-scheduled.yml
examples/tests/deepspeed/test_deepspeed.py
==================
7246785a6;Julien Plu;2021-02-17 18:54:15 +0100;Make TF CTRL compliant with XLA and AMP (#10209)
* Fix XLA and AMP

* Apply style

* Remove useless cast
==

src/transformers/models/ctrl/modeling_tf_ctrl.py
tests/test_modeling_tf_ctrl.py
==================
fdb2351eb;Julien Plu;2021-02-17 18:02:48 +0100;Making TF XLM-like models XLA and AMP compliant (#10211)
* Fix Flaubert and XLM

* Remove useless cast

* Tiny fix

* Tiny fix
==

src/transformers/models/flaubert/modeling_tf_flaubert.py
src/transformers/models/xlm/modeling_tf_xlm.py
tests/test_modeling_tf_flaubert.py
tests/test_modeling_tf_xlm.py
==================
83d803ba0;Julien Plu;2021-02-17 17:48:56 +0100;Making TF BART-like models XLA and AMP compliant (#10191)
* Update BART

* Update Blenderbot

* Update BlenderbotSmall

* Update Marian

* Update MBart

* Update MBart

* Update Pegasus

* Update template

* Fix Marian and Pegasus

* Apply style

* Default initializer

* Default initializer

* Default initializer

* Remove int32 casts

* Fix template

* Remove more cast
==

src/transformers/models/bart/modeling_tf_bart.py
src/transformers/models/blenderbot/modeling_tf_blenderbot.py
src/transformers/models/blenderbot_small/modeling_tf_blenderbot_small.py
src/transformers/models/marian/modeling_tf_marian.py
src/transformers/models/mbart/modeling_tf_mbart.py
src/transformers/models/pegasus/modeling_tf_pegasus.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_tf_{{cookiecutter.lowercase_modelname}}.py
tests/test_modeling_tf_bart.py
tests/test_modeling_tf_blenderbot.py
tests/test_modeling_tf_blenderbot_small.py
tests/test_modeling_tf_marian.py
tests/test_modeling_tf_mbart.py
tests/test_modeling_tf_pegasus.py
==================
8d79e5ca4;Daniel Stancl;2021-02-17 17:00:09 +0100;Fix head masking for TFT5 (#9877)
* Fix head_mask and decoder_head_mask in TFT5 models

* Enable test_headmasking both fot TFT5 tester
and TFT5EncoderOnly tester

Co-authored-by: patrickvonplaten <patrick.v.platen@gmail.com>
==

src/transformers/models/t5/modeling_tf_t5.py
tests/test_modeling_tf_t5.py
==================
4b9196573;Lysandre Debut;2021-02-17 15:53:43 +0100;Factor out methods (#10215)

==

src/transformers/modeling_utils.py
==================
e94d63f6c;Stas Bekman;2021-02-16 13:35:39 -0800;[trainer] fix ignored columns logger (#10219)
* [trainer] fix ignored columns logger

This PR fixes a confusing log entry that says:
```
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: .
```
when everything is in order.

* Update src/transformers/trainer.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

src/transformers/trainer.py
==================
4210cd96f;Joe Davison;2021-02-16 14:00:05 -0500;fix add_token_positions fn (#10217)

==

docs/source/custom_datasets.rst
==================
7169d1ea7;Sylvain Gugger;2021-02-16 11:15:15 -0500;Store FLOS as floats to avoid overflow. (#10213)

==

src/transformers/trainer.py
src/transformers/trainer_callback.py
tests/test_trainer.py
==================
df1b0fb54;Zhang Cheng;2021-02-16 23:39:37 +0900;set tgt_lang of MBart Tokenizer for summarization (#10205)

==

examples/seq2seq/run_seq2seq.py
==================
5c2d66a2f;Julien Plu;2021-02-16 13:59:41 +0100;Unlock XLA test for convbert (#10207)

==

tests/test_modeling_tf_convbert.py
==================
1c8c2d9ab;Suraj Patil;2021-02-16 00:18:02 +0530;[WIP][examples/seq2seq] move old s2s scripts to legacy (#10136)
* move old s2s scripts to legacy

* add the tests back

* proper rename

* restore

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

Co-authored-by: Stas Bekman <stas@stason.org>
Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

examples/legacy/seq2seq/README.md
examples/legacy/seq2seq/__init__.py
examples/legacy/seq2seq/convert_model_to_fp16.py
examples/legacy/seq2seq/download_wmt.py
examples/legacy/seq2seq/finetune.sh
examples/legacy/seq2seq/finetune_tpu.sh
examples/legacy/seq2seq/finetune_trainer.py
examples/legacy/seq2seq/minify_dataset.py
examples/legacy/seq2seq/old_test_calculate_rouge.py
examples/legacy/seq2seq/old_test_datasets.py
examples/legacy/seq2seq/old_test_fsmt_bleu_score.py
examples/legacy/seq2seq/old_test_seq2seq_examples.py
examples/legacy/seq2seq/old_test_seq2seq_examples_multi_gpu.py
examples/legacy/seq2seq/old_test_tatoeba_conversion.py
examples/legacy/seq2seq/pack_dataset.py
examples/legacy/seq2seq/requirements.txt
examples/legacy/seq2seq/romanian_postprocessing.md
examples/legacy/seq2seq/rouge_cli.py
examples/legacy/seq2seq/run_distributed_eval.py
examples/legacy/seq2seq/run_eval.py
examples/legacy/seq2seq/run_eval_search.py
examples/legacy/seq2seq/save_len_file.py
examples/legacy/seq2seq/save_randomly_initialized_model.py
examples/legacy/seq2seq/sentence_splitter.py
examples/legacy/seq2seq/seq2seq_trainer.py
examples/legacy/seq2seq/seq2seq_training_args.py
examples/legacy/seq2seq/test_data/fsmt/build-eval-data.py
examples/legacy/seq2seq/test_data/fsmt/fsmt_val_data.json
examples/legacy/seq2seq/test_data/test_data
examples/legacy/seq2seq/test_data/wmt_en_ro/test.source
examples/legacy/seq2seq/test_data/wmt_en_ro/test.target
examples/legacy/seq2seq/test_data/wmt_en_ro/train.len
examples/legacy/seq2seq/test_data/wmt_en_ro/train.source
examples/legacy/seq2seq/test_data/wmt_en_ro/train.target
examples/legacy/seq2seq/test_data/wmt_en_ro/val.len
examples/legacy/seq2seq/test_data/wmt_en_ro/val.source
examples/legacy/seq2seq/test_data/wmt_en_ro/val.target
examples/legacy/seq2seq/train_distil_marian_enro.sh
examples/legacy/seq2seq/train_distil_marian_enro_tpu.sh
examples/legacy/seq2seq/train_distilbart_cnn.sh
examples/legacy/seq2seq/train_mbart_cc25_enro.sh
examples/legacy/seq2seq/utils.py
examples/legacy/seq2seq/xla_spawn.py
examples/seq2seq/README.md
examples/seq2seq/requirements.txt
==================
96897a353;Stas Bekman;2021-02-15 10:01:35 -0800;make the sub-group of tests run always (#10196)

==

.github/workflows/self-scheduled.yml
==================
8cbd0bd13;Lysandre Debut;2021-02-15 18:57:17 +0100;Specify dataset dtype (#10195)
Co-authored-by: Quentin Lhoest <lhoest.q@gmail.com>

Co-authored-by: Quentin Lhoest <lhoest.q@gmail.com>
==

tests/test_trainer.py
==================
0b1f552a2;Stas Bekman;2021-02-15 09:12:17 -0800;fix run_seq2seq.py; porting trainer tests to it (#10162)
* fix run_seq2seq.py; porting DeepSpeed tests to it

* unrefactor

* defensive programming

* defensive programming 2

* port the rest of the trainer tests

* style

* a cleaner scripts dir finder

* cleanup
==

examples/seq2seq/run_seq2seq.py
examples/test_data/wmt_en_ro/test.json
examples/test_data/wmt_en_ro/train.json
examples/test_data/wmt_en_ro/val.json
examples/tests/deepspeed/test_deepspeed.py
examples/tests/trainer/test_trainer_ext.py
==================
31b0560ab;Julien Plu;2021-02-15 17:18:33 +0100;Add AMP for Albert (#10141)

==

src/transformers/models/albert/modeling_tf_albert.py
src/transformers/models/bert/modeling_tf_bert.py
src/transformers/models/convbert/modeling_tf_convbert.py
src/transformers/models/electra/modeling_tf_electra.py
src/transformers/models/longformer/modeling_tf_longformer.py
src/transformers/models/roberta/modeling_tf_roberta.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_tf_{{cookiecutter.lowercase_modelname}}.py
tests/test_modeling_tf_albert.py
==================
6fc940ed0;Suraj Patil;2021-02-15 20:58:54 +0530;Add mBART-50 (#10154)
* add tokenizer for mBART-50

* update tokenizers

* make src_lang and tgt_lang optional

* update tokenizer test

* add setter

* update docs

* update conversion script

* update docs

* update conversion script

* update tokenizer

* update test

* update docs

* doc

* address Sylvain's suggestions

* fix test

* fix formatting

* nits
==

README.md
docs/source/index.rst
docs/source/model_doc/mbart.rst
docs/source/pretrained_models.rst
src/transformers/__init__.py
src/transformers/convert_slow_tokenizer.py
src/transformers/models/mbart/__init__.py
src/transformers/models/mbart/convert_mbart_original_checkpoint_to_pytorch.py
src/transformers/models/mbart/tokenization_mbart50.py
src/transformers/models/mbart/tokenization_mbart50_fast.py
src/transformers/utils/dummy_sentencepiece_objects.py
src/transformers/utils/dummy_tokenizers_objects.py
tests/test_tokenization_mbart50.py
==================
570218878;Julien Plu;2021-02-15 15:21:57 +0100;Fix TF template (#10189)
* Fix template

* Update Seq2Seq tests
==

templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/test_modeling_tf_{{cookiecutter.lowercase_modelname}}.py
==================
2a5c99003;Suraj Patil;2021-02-15 19:48:12 +0530;fix RagTokenizer (#10167)

==

src/transformers/models/rag/tokenization_rag.py
==================
c8d3fa0df;Julien Plu;2021-02-15 13:55:10 +0100;Check TF ops for ONNX compliance (#10025)
* Add check-ops script

* Finish to implement check_tf_ops and start the test

* Make the test mandatory only for BERT

* Update tf_ops folder

* Remove useless classes

* Add the ONNX test for GPT2 and BART

* Add a onnxruntime slow test + better opset flexibility

* Fix test + apply style

* fix tests

* Switch min opset from 12 to 10

* Update src/transformers/file_utils.py

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* Fix GPT2

* Remove extra shape_list usage

* Fix GPT2

* Address Morgan's comments

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
==

src/transformers/file_utils.py
src/transformers/models/gpt2/modeling_tf_gpt2.py
src/transformers/testing_utils.py
tests/test_modeling_tf_albert.py
tests/test_modeling_tf_bart.py
tests/test_modeling_tf_bert.py
tests/test_modeling_tf_blenderbot.py
tests/test_modeling_tf_blenderbot_small.py
tests/test_modeling_tf_common.py
tests/test_modeling_tf_convbert.py
tests/test_modeling_tf_ctrl.py
tests/test_modeling_tf_distilbert.py
tests/test_modeling_tf_dpr.py
tests/test_modeling_tf_electra.py
tests/test_modeling_tf_flaubert.py
tests/test_modeling_tf_funnel.py
tests/test_modeling_tf_gpt2.py
tests/test_modeling_tf_led.py
tests/test_modeling_tf_longformer.py
tests/test_modeling_tf_lxmert.py
tests/test_modeling_tf_marian.py
tests/test_modeling_tf_mbart.py
tests/test_modeling_tf_mobilebert.py
tests/test_modeling_tf_mpnet.py
tests/test_modeling_tf_openai.py
tests/test_modeling_tf_pegasus.py
tests/test_modeling_tf_roberta.py
tests/test_modeling_tf_t5.py
tests/test_modeling_tf_transfo_xl.py
tests/test_modeling_tf_xlm.py
tests/test_modeling_tf_xlnet.py
utils/check_tf_ops.py
utils/tf_ops/onnx.json
==================
93bd2f709;Lysandre Debut;2021-02-15 12:31:29 +0100;Add new model to labels that should not stale (#10187)

==

.github/stale.yml
==================
900daec24;Nicolas Patry;2021-02-15 12:22:45 +0100;Fixing NER pipeline for list inputs. (#10184)
Fixes #10168
==

src/transformers/pipelines/token_classification.py
tests/test_pipelines_ner.py
==================
587197dcd;Sylvain Gugger;2021-02-15 05:49:07 -0500;Fix datasets set_format (#10178)

==

src/transformers/trainer.py
==================
8fae93ca1;Stas Bekman;2021-02-13 06:10:22 -0800;[t5 tokenizer] add info logs (#9897)
* save fast tokenizer + add info logs

* fix tests

* remove the saving of fast tokenizer
==

src/transformers/configuration_utils.py
src/transformers/models/t5/tokenization_t5.py
src/transformers/models/t5/tokenization_t5_fast.py
src/transformers/tokenization_utils_base.py
==================
803498318;Sylvain Gugger;2021-02-13 08:52:30 -0500;[Doc] Fix version control in internal pages (#10124)

==

docs/source/_static/js/custom.js
==================
698c9e2db;Manuel Romero;2021-02-13 14:26:25 +0100;Fix typo in comment (#10156)

==

src/transformers/generation_tf_utils.py
==================
c96936687;Manuel Romero;2021-02-13 14:26:01 +0100;Fix typo in comments (#10157)

==

src/transformers/generation_utils.py
==================
c9837a0d2;Nicolas Patry;2021-02-13 14:24:53 +0100;Conversion from slow to fast for BPE spm vocabs contained an error. (#10120)
* Conversion from slow to fast for BPE spm vocabs contained an error.

- There is only 1 test currently (tokenizers + slow) that used the modified path
and it's reformer, which does not contain any ids modification so the
bug was silent for now.
- The real issue is that vocab variable was overloaded by
SentencePieceExtractor, leading to Slow specific vocab oddities to be
completely ignored
- The bug was reported here https://github.com/huggingface/transformers/issues/9518
- Ran the complete tokenization test suite with slow without error
(`RUN_SLOW=1 pytest -sv tests/test_tokenization_*`)

* Remove rebase error.

* Adding the fixture.
==

src/transformers/convert_slow_tokenizer.py
tests/fixtures/test_sentencepiece_bpe.model
tests/test_tokenization_camembert.py
==================
dd3a7f964;Lysandre Debut;2021-02-13 14:19:56 +0100;Revert propagation (#10171)

==

src/transformers/utils/logging.py
==================
641f418e1;Julien Chaumond;2021-02-12 21:46:17 +0100;[hf_api] delete deprecated methods and tests (2)

==

tests/test_hf_api.py
==================
eed31db94;Julien Chaumond;2021-02-12 21:35:06 +0100;[hf_api] delete deprecated methods and tests (#10159)
* [hf_api] delete deprecated methods and tests

cc @lhoestq

* Update test_hf_api.py
==

src/transformers/hf_api.py
tests/test_hf_api.py
==================
1321356bd;Mohamed Al Salti;2021-02-12 19:18:39 +0200;Fix typo in GPT2DoubleHeadsModel docs (#10148)
* Fix typo

* apply suggestion

Co-authored-by: Suraj Patil <surajp815@gmail.com>
==

src/transformers/models/gpt2/modeling_gpt2.py
==================
f51188cbe;Suraj Patil;2021-02-12 17:18:21 +0530;[examples/run_s2s] remove task_specific_params and update rouge computation (#10133)
* fix rouge metrics and task specific params

* fix typo

* round metrics

* typo

* remove task_specific_params
==

examples/seq2seq/run_seq2seq.py
==================
31245775e;Sylvain Gugger;2021-02-11 18:44:18 -0500;Add SageMakerTrainer for model paralellism (#10122)
* Refactor things out of main train

* Store signature

* Add SageMakerTrainer

* Init + Copyright

* Address review comments
==

src/transformers/sagemaker/__init__.py
src/transformers/sagemaker/trainer_sm.py
src/transformers/sagemaker/training_args_sm.py
src/transformers/trainer.py
src/transformers/training_args.py
==================
b54cb0bd8;Stas Bekman;2021-02-11 14:02:05 -0800;[DeepSpeed in notebooks] Jupyter + Colab (#10130)
* init devices/setup explicitly

* docs + test

* simplify

* cleanup

* cleanup

* cleanup

* correct the required dist setup

* derive local_rank from env LOCAL_RANK
==

docs/source/main_classes/trainer.rst
examples/tests/deepspeed/test_deepspeed.py
src/transformers/trainer.py
src/transformers/training_args.py
==================
6710d1d5e;Sylvain Gugger;2021-02-11 15:12:35 -0500;Typo fix

==

README.md
==================
8e13b7359;Patrick von Platen;2021-02-11 18:35:27 +0300;Update README.md

==

templates/adding_a_new_model/open_model_proposals/README.md
==================
d6b4f48ec;Patrick von Platen;2021-02-11 18:34:17 +0300;Update ADD_BIG_BIRD.md

==

templates/adding_a_new_model/open_model_proposals/ADD_BIG_BIRD.md
==================
495c157d6;Patrick von Platen;2021-02-11 15:40:54 +0300;[Wav2Vec2] Improve Tokenizer & Model for batched inference (#10117)
* save intermediate

* finish batch the same as fairseq

* add normalization

* fix batched input

* add better comment

* Update src/transformers/models/wav2vec2/modeling_wav2vec2.py

* add nice docstring

* add tokenizer tests

* make all slow tests pass

* finish PR

* correct import
==

src/transformers/models/wav2vec2/modeling_wav2vec2.py
src/transformers/models/wav2vec2/tokenization_wav2vec2.py
tests/test_modeling_wav2vec2.py
tests/test_tokenization_wav2vec2.py
==================
2f3b5f4dc;Tanmay Thakur;2021-02-11 15:23:40 +0530;Add new community notebook - Blenderbot (#10126)
* Update:community.md, new nb add

* feat: updated grammar on  nb description

* Update: Train summarizer for BlenderBotSmall
==

docs/source/community.md
==================
8dcfaea08;Qbiwan;2021-02-11 12:57:23 +0800;Update run_xnli.py to use Datasets library (#9829)
* remove xnli_compute_metrics, add load_dataset, load_metric, set_seed,metric.compute,load_metric

* fix

* fix

* fix

* push

* fix

* everything works

* fix init

* fix

* special treatment for sepconv1d

* style

* üôèüèΩ

* add doc and cleanup


* fix doc

* fix doc again

* fix doc again

* Apply suggestions from code review

* make style

* Proposal that should work

* Remove needless code

* Fix test

* Apply suggestions from code review

* remove xnli_compute_metrics, add load_dataset, load_metric, set_seed,metric.compute,load_metric

* amend README

* removed data_args.task_name and replaced with task_name = "xnli"; use split function to load train and validation dataset separately; remove __post_init__; remove flag --task_name from README.

* removed dict task_to_keys, use str "xnli" instead of variable task_name, change preprocess_function to use examples["premise"], examples["hypothesis"] directly, remove sentence1_key and sentence2_key, change compute_metrics function to cater only to accuracy metric, add condition for train_langauge is None when using dataset.load_dataset()

* removed `torch.distributed.barrier()` and `import torch` as `from_pretrained` is able to do the work; amend README
==

examples/text-classification/README.md
examples/text-classification/run_xnli.py
==================
77b862847;Stas Bekman;2021-02-10 09:09:48 -0800;[DeepSpeed] restore memory for evaluation (#10114)
* free up memory at the end of train

* rework tests

* consistent formatting

* correction
==

examples/tests/deepspeed/ds_config.json
examples/tests/deepspeed/test_deepspeed.py
src/transformers/trainer.py
==================
c130e67dc;Suraj Patil;2021-02-10 22:39:09 +0530;remove adjust_logits_during_generation method (#10087)
* add forced logits processors

* delete adjust_logits method

* add forced_eos_token_id argument in config

* add tests for forced logits processors

* update gen utils tests

* add forced option to tf generate

* remove adjust_logits method from tf models

* update adjust_logits for marian

* delete _force_token_id_to_be_generated method

* style

* import warnings

* pass max_length to _get_logits_processor

* set forced_eos_token_id to None

* set forced attributes in conf utils

* typo

* fix rag generate

* add forced_eos_token_id in rag config

* remove force_bos_token_to_be_generated from BartConfig

* remove _force_token_ids_generation from FSMT

* nit

* fix negative constant

* apply suggestions from code review
==

src/transformers/configuration_utils.py
src/transformers/generation_logits_process.py
src/transformers/generation_tf_utils.py
src/transformers/generation_utils.py
src/transformers/models/bart/configuration_bart.py
src/transformers/models/bart/modeling_bart.py
src/transformers/models/bart/modeling_tf_bart.py
src/transformers/models/blenderbot/configuration_blenderbot.py
src/transformers/models/blenderbot/modeling_blenderbot.py
src/transformers/models/blenderbot/modeling_tf_blenderbot.py
src/transformers/models/blenderbot_small/configuration_blenderbot_small.py
src/transformers/models/blenderbot_small/modeling_blenderbot_small.py
src/transformers/models/blenderbot_small/modeling_tf_blenderbot_small.py
src/transformers/models/fsmt/configuration_fsmt.py
src/transformers/models/fsmt/modeling_fsmt.py
src/transformers/models/marian/configuration_marian.py
src/transformers/models/marian/modeling_marian.py
src/transformers/models/marian/modeling_tf_marian.py
src/transformers/models/mbart/configuration_mbart.py
src/transformers/models/mbart/modeling_mbart.py
src/transformers/models/mbart/modeling_tf_mbart.py
src/transformers/models/pegasus/configuration_pegasus.py
src/transformers/models/pegasus/modeling_pegasus.py
src/transformers/models/pegasus/modeling_tf_pegasus.py
src/transformers/models/rag/configuration_rag.py
src/transformers/models/rag/modeling_rag.py
tests/test_generation_logits_process.py
tests/test_generation_utils.py
tests/test_pipelines_summarization.py
==================
22a32cf48;Julien Plu;2021-02-10 16:58:37 +0100;Fix TF LED/Longformer attentions computation (#10007)
* Fix test

* Remove commented test

* Fix name

* Apply style

* Fix check copies

* Remove prints

* Restore boolean

* Fix reshape
==

src/transformers/models/led/modeling_tf_led.py
src/transformers/models/longformer/modeling_tf_longformer.py
tests/test_modeling_tf_led.py
tests/test_modeling_tf_longformer.py
==================
0d8e554d4;Lysandre Debut;2021-02-10 16:50:00 +0100;Line endings should be LF across repo and not CRLF (#10119)

==

.gitattributes
examples/research_projects/bertology/run_prune_gpt.py
tests/test_modeling_deberta.py
==================
937f67074;Stas Bekman;2021-02-10 00:12:27 -0800;add deepspeed fairscale (#10116)

==

.github/workflows/self-scheduled.yml
==================
d478257d9;Stas Bekman;2021-02-10 00:02:39 -0800;[CI] build docs faster (#10115)
I assume the CI machine should have at least 4 cores, so let's build docs faster
==

.circleci/config.yml
==================
7c07a47df;Stas Bekman;2021-02-09 22:16:20 -0800;[DeepSpeed docs] new information (#9610)
* how to specify a specific gpu

* new paper

* expand on buffer sizes

* style

* where to find config examples

* specific example

* small updates
==

docs/source/main_classes/trainer.rst
==================
1fbaa3c11;Anthony MOI;2021-02-09 21:48:22 -0500;Fix tokenizers training in notebook (#10110)

==

notebooks/01-training-tokenizers.ipynb
==================
85395e490;Shiva Zamani;2021-02-09 17:03:02 -0700;Remove speed metrics from default compute objective (#10107)

==

src/transformers/trainer_utils.py
==================
7c7962ba8;Boris Dayma;2021-02-09 13:47:52 -0600;doc: update W&B related doc (#10086)
* doc: update W&B related doc

* doc(wandb): mention report_to

* doc(wandb): commit suggestion

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* doc(wandb): fix typo

* doc(wandb): remove WANDB_DISABLED

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

docs/source/community.md
examples/README.md
src/transformers/integrations.py
==================
480a9d6ba;abhishek thakur;2021-02-09 20:22:54 +0100;Fix TFConvBertModelIntegrationTest::test_inference_masked_lm Test (#10104)

==

tests/test_modeling_tf_convbert.py
==================
0c3d23dff;Sylvain Gugger;2021-02-09 14:17:09 -0500;Add patch releases to the doc

==

.circleci/deploy.sh
docs/source/_static/js/custom.js
==================
3e0c62b61;Suraj Patil;2021-02-10 00:27:38 +0530;[RAG] fix generate (#10094)
* fix rag generate and tests

* put back adjust_logits_during_generation

* tests are okay

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
==

src/transformers/models/rag/modeling_rag.py
==================
226973a9c;Patrick von Platen;2021-02-09 21:43:41 +0300;fix import (#10103)

==

src/transformers/file_utils.py
==================
4cda2d73e;Patrick von Platen;2021-02-09 19:58:35 +0300;Update ADD_BIG_BIRD.md

==

templates/adding_a_new_model/open_model_proposals/ADD_BIG_BIRD.md
==================
b82fe7d25;Julien Plu;2021-02-09 17:48:28 +0100;Replace strided slice with tf.expand_dims (#10078)
* Replace tf.newaxis -> tf.expand_dims

* Fix tests

* Fix tests

* Use reshape when a tensors needs a double expand

* Fix GPT2

* Fix GPT2
==

src/transformers/modeling_tf_utils.py
src/transformers/models/albert/modeling_tf_albert.py
src/transformers/models/bert/modeling_tf_bert.py
src/transformers/models/convbert/modeling_tf_convbert.py
src/transformers/models/ctrl/modeling_tf_ctrl.py
src/transformers/models/distilbert/modeling_tf_distilbert.py
src/transformers/models/electra/modeling_tf_electra.py
src/transformers/models/flaubert/modeling_tf_flaubert.py
src/transformers/models/gpt2/modeling_tf_gpt2.py
src/transformers/models/longformer/modeling_tf_longformer.py
src/transformers/models/lxmert/modeling_tf_lxmert.py
src/transformers/models/mobilebert/modeling_tf_mobilebert.py
src/transformers/models/mpnet/modeling_tf_mpnet.py
src/transformers/models/openai/modeling_tf_openai.py
src/transformers/models/roberta/modeling_tf_roberta.py
src/transformers/models/xlm/modeling_tf_xlm.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_tf_{{cookiecutter.lowercase_modelname}}.py
==================
e7381c459;Daniel Stancl;2021-02-09 17:45:18 +0100;Add head_mask and decoder_head_mask to TF LED (#9988)
* Add head masking to TF LED

* Add head_mask to Longformer + one doc piece to LED

* Fix integration tests
==

src/transformers/models/led/modeling_tf_led.py
src/transformers/models/longformer/modeling_tf_longformer.py
tests/test_modeling_tf_led.py
tests/test_modeling_tf_longformer.py
==================
77c0ce8c0;Sylvain Gugger;2021-02-09 10:38:12 -0500;Fix some edge cases in report_to and add deprecation warnings (#10100)

==

src/transformers/integrations.py
src/transformers/training_args.py
==================
78f4a0e7e;Lysandre Debut;2021-02-09 16:27:49 +0100;Logging propagation (#10092)
* Enable propagation by default

* Document enable/disable default handler
==

docs/source/main_classes/logging.rst
src/transformers/utils/logging.py
==================
63fddcf69;Suraj Patil;2021-02-09 20:41:41 +0530;[examples/s2s] add test set predictions (#10085)
* add do_predict, pass eval_beams durig eval

* update help

* apply suggestions from code review
==

examples/seq2seq/run_seq2seq.py
==================
c6d5e5659;Julien Plu;2021-02-09 12:10:31 +0100;Fix naming (#10095)

==

src/transformers/models/mobilebert/modeling_tf_mobilebert.py
==================
4ed763779;abhishek thakur;2021-02-09 12:07:56 +0100;Fix example in Wav2Vec2 documentation (#10096)
* Fix example in Wav2Vec2 documentation

* fix style
==

src/transformers/models/wav2vec2/modeling_wav2vec2.py
==================
bf1a06a43;Lysandre;2021-02-09 10:02:50 +0100;Docs for v4.3.1 release

==

.circleci/deploy.sh
==================
b972125ce;Patrick von Platen;2021-02-09 11:49:02 +0300;Deprecate Wav2Vec2ForMaskedLM and add Wav2Vec2ForCTC (#10089)
* add wav2vec2CTC and deprecate for maskedlm

* remove from docs
==

docs/source/model_doc/wav2vec2.rst
src/transformers/__init__.py
src/transformers/models/wav2vec2/__init__.py
src/transformers/models/wav2vec2/convert_wav2vec2_original_pytorch_checkpoint_to_pytorch.py
src/transformers/models/wav2vec2/modeling_wav2vec2.py
src/transformers/utils/dummy_pt_objects.py
tests/test_modeling_wav2vec2.py
utils/check_repo.py
==================
ba542ffb4;Lysandre;2021-02-09 08:43:00 +0100;Fix deployment script

==

.circleci/deploy.sh
==================
263fac71a;sandip;2021-02-09 02:12:25 +0530;Integration test for electra model (#10073)

==

tests/test_modeling_electra.py
==================
781220aca;Stas Bekman;2021-02-08 12:41:52 -0800;transition to new tests dir (#10080)

==

examples/tests/deepspeed/ds_config.json
examples/tests/deepspeed/test_deepspeed.py
==================
84acf0c7b;demSd;2021-02-08 19:05:32 +0100;remove token_type_ids from TokenizerBertGeneration output (#10070)

==

src/transformers/models/bert_generation/tokenization_bert_generation.py
==================
e4bf9910d;Juan Cruz-Benito;2021-02-08 19:04:21 +0100;Removing run_pl_glue.py from text classification docs, include run_xnli.py & run_tf_text_classification.py (#10066)
* Removing run_pl_glue.py from seq classification docs

* Adding run_tf_text_classification.py

* Using :prefix_link: to refer local files

* Applying "make style" to the branch

* Update docs/source/task_summary.rst

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Removing last underscores

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

docs/source/task_summary.rst
==================
0dd579c9c;Lysandre;2021-02-08 18:53:24 +0100;Docs for v4.3.0

==

.circleci/deploy.sh
docs/source/_static/js/custom.js
==================
322037e84;Stas Bekman;2021-02-08 09:44:02 -0800;[trainer] deepspeed bug fixes and tests (#10039)
* deepspeed bug fixes and tests

* manual wrap?
==

examples/seq2seq/test_deepspeed.py
examples/seq2seq/test_finetune_trainer.py
src/transformers/trainer.py
==================
f285e4c3a;Anthony MOI;2021-02-08 12:27:26 -0500;Update tokenizers requirement (#10077)

==

setup.py
src/transformers/dependency_versions_table.py
==================
ddaafd78f;noise-field;2021-02-08 19:58:02 +0300;Fix mlflow param overflow clean (#10071)
* Unify logging with f-strings

* Get limits from MLflow rather than hardcode

* Add a check for parameter length overflow

Also constants are marked as internal

* Don't stop run in on_train_end

This causes bad behaviour when there is a seprarte validation step:
validation gets recorded as separate run.

* Fix style
==

src/transformers/integrations.py
==================
ece6c5145;Olivier;2021-02-08 10:08:16 -0500;[s2s examples] Replace -100 token ids with the tokenizer pad_id for compute_metrics (#10046)
* replace -100 token ids with the tokenizer pad_id for compute_metrics

* fixed typo for label_ids
==

examples/seq2seq/utils.py
==================
c9df1b1d5;Lysandre Debut;2021-02-08 15:07:02 +0100;Model templates (#10072)

==

templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_tf_{{cookiecutter.lowercase_modelname}}.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_{{cookiecutter.lowercase_modelname}}.py
==================
3b7e612a5;demSd;2021-02-08 14:22:19 +0100;Implementing the test integration of BertGeneration (#9990)
* claiming this issue

* Integration test for BertGeneration(Encoder and Decoder)

* fix code quality
==

tests/test_modeling_bert_generation.py
==================
cdd865923;Julien Plu;2021-02-08 14:10:50 +0100;Fix TF template (#10069)
* Fix template

* Fix template
==

templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_tf_{{cookiecutter.lowercase_modelname}}.py
==================
9e795eac8;Patrick von Platen;2021-02-08 16:04:28 +0300;fix bert2bert test (#10063)

==

tests/test_trainer_seq2seq.py
==================
31563e056;Julien Plu;2021-02-08 12:36:30 +0100;Restore TF embeddings and attention layers to their previous version (#9890)
* Refacto BERT

* Restore all the concerned models

* Remove print

* Update template

* Apply Sylvain's and Morgan's comments

* Fix cast

* Put the cast inside call

* Remove cond in ebds

* Fix funnel

* Restore previous dot product (attention_scores) computation

* Add ConvBERT and BART

* Make all the S2S models ONNX compliant

* Fix test

* Fix check copies
==

src/transformers/models/albert/modeling_tf_albert.py
src/transformers/models/bart/modeling_tf_bart.py
src/transformers/models/bert/modeling_tf_bert.py
src/transformers/models/blenderbot/modeling_tf_blenderbot.py
src/transformers/models/blenderbot_small/modeling_tf_blenderbot_small.py
src/transformers/models/convbert/modeling_tf_convbert.py
src/transformers/models/distilbert/modeling_tf_distilbert.py
src/transformers/models/electra/modeling_tf_electra.py
src/transformers/models/funnel/modeling_tf_funnel.py
src/transformers/models/led/modeling_tf_led.py
src/transformers/models/longformer/modeling_tf_longformer.py
src/transformers/models/lxmert/modeling_tf_lxmert.py
src/transformers/models/marian/modeling_tf_marian.py
src/transformers/models/mbart/modeling_tf_mbart.py
src/transformers/models/mobilebert/modeling_tf_mobilebert.py
src/transformers/models/mpnet/modeling_tf_mpnet.py
src/transformers/models/pegasus/modeling_tf_pegasus.py
src/transformers/models/roberta/modeling_tf_roberta.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_tf_{{cookiecutter.lowercase_modelname}}.py
tests/test_modeling_tf_common.py
==================
8bb52bd24;Julien Plu;2021-02-08 12:32:31 +0100;Disable temporarily too slow tests (Longformer/LED) (#10062)
* Disable temporarily too slow tests

* Fix style

* Fix template
==

tests/test_modeling_tf_led.py
tests/test_modeling_tf_longformer.py
==================
b1aa4982c;Nicolas Patry;2021-02-08 12:29:07 +0100;Cleaning up `ConversationalPipeline` to support more than DialoGPT. (#10002)
* Cleaning up `ConversationalPipeline` to support more than DialoGPT.

Currently ConversationalPipeline was heavily biased towards DialoGPT
,which is the default model for this pipeline.

This PR proposes changes to put back the modifications specific to
DialoGPT into tokenizer-specific behavior wherever possible, by
creating `_build_conversation_input_ids` function that takes
conversation as input, and returns a list of ints corresponding
to the tokens. It feels natural to put here because all models
have probably different strategies to build input_ids from the
full conversation and it's the tokenizer's job to transform strings
into tokens (and vice-versa)

If `_build_conversation_input_ids` is missing, previous behavior is
used so we don't break anything so far (except for blenderbot where it's a fix).

This PR also contains a fix for too long inputs. There used
to be dead code for trying to limit the size of incoming input.
The introduced fixed is that we limit
within `_build_conversation_input_ids` to `tokenizer.model_max_length`.
It corresponds to the intent of the removed dead code and is actually
better because it corresponds to `model_max_length` which is different
from `max_length` (which is a default parameter for `generate`).

- Removed `history` logic from the Conversation as it's not relevant
anymore because tokenization logic has been moved to tokenizer.
And tokenizer cannot save any cache, and conversation cannot know
what is relevant or not.
Also it's not usable from `blenderbot` because the input_ids are
not append only (EOS tokens is always at the end).

- Added `iter_texts` method on `Conversation` because all
the code was literred with some form of this iteration of
past/generated_responses.

* Removing torch mention in types.

* Adding type checking to `_build_conversation_input_ids`.

* Fixing import in strings.
==

src/transformers/models/blenderbot/tokenization_blenderbot.py
src/transformers/models/gpt2/tokenization_gpt2.py
src/transformers/models/gpt2/tokenization_gpt2_fast.py
src/transformers/pipelines/conversational.py
tests/test_pipelines_conversational.py
==================
ae37ceacb;Lysandre Debut;2021-02-08 12:02:05 +0100;Fix typo (#10064)

==

templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_tf_{{cookiecutter.lowercase_modelname}}.py
==================
9a0399e18;Patrick von Platen;2021-02-08 13:25:09 +0300;fix bart tests (#10060)

==

tests/test_modeling_bart.py
==================
b01483faa;Sylvain Gugger;2021-02-08 05:03:55 -0500;Truncate max length if needed in all examples (#10034)

==

examples/language-modeling/run_mlm.py
examples/language-modeling/run_plm.py
examples/multiple-choice/run_swag.py
examples/question-answering/run_qa.py
examples/question-answering/run_qa_beam_search.py
examples/text-classification/run_glue.py
==================
45aaf5f7a;Sylvain Gugger;2021-02-08 05:02:01 -0500;A few fixes in the documentation (#10033)

==

.circleci/deploy.sh
docs/source/_static/js/custom.js
docs/source/main_classes/tokenizer.rst
src/transformers/tokenization_utils.py
src/transformers/tokenization_utils_base.py
src/transformers/tokenization_utils_fast.py
==================
04fd783cc;Sylvain Gugger;2021-02-08 04:58:25 -0500;Check copies match full class/function names (#10030)

==

utils/check_copies.py
==================
d51302cca;Lysandre Debut;2021-02-08 10:43:25 +0100;Fix slow dpr test (#10059)
* Correct cast to device

* Comment back the slow test
==

tests/test_modeling_dpr.py
==================
12e44af5d;sandip;2021-02-08 15:06:50 +0530;Integration test for FlauBert (#10022)

==

tests/test_modeling_flaubert.py
==================
24db8cc32;Stas Bekman;2021-02-07 17:54:20 -0800;Can't mix --fp16 and --device cpu (#10041)

==

examples/seq2seq/run_eval.py
==================
769948fad;Stas Bekman;2021-02-07 17:51:34 -0800;json to jsonlines, and doc, and typo (#10043)

==

examples/seq2seq/README.md
==================
8ea412a86;Stas Bekman;2021-02-05 15:51:18 -0800;[examples] make run scripts executable (#10037)
* make executable

* make executable

* same for the template

* cleanup
==

examples/benchmarking/run_benchmark.py
examples/benchmarking/run_benchmark_tf.py
examples/language-modeling/run_clm.py
examples/language-modeling/run_mlm.py
examples/language-modeling/run_mlm_flax.py
examples/language-modeling/run_plm.py
examples/legacy/run_camembert.py
examples/legacy/run_chinese_ref.py
examples/legacy/run_language_modeling.py
examples/legacy/run_openai_gpt.py
examples/legacy/run_swag.py
examples/legacy/run_transfo_xl.py
examples/multiple-choice/run_swag.py
examples/multiple-choice/run_tf_multiple_choice.py
examples/question-answering/run_qa.py
examples/question-answering/run_qa_beam_search.py
examples/question-answering/run_tf_squad.py
examples/seq2seq/run_seq2seq.py
examples/text-classification/run_glue.py
examples/text-classification/run_tf_glue.py
examples/text-classification/run_tf_text_classification.py
examples/text-classification/run_xnli.py
examples/text-generation/run_generation.py
examples/token-classification/run_ner.py
examples/token-classification/run_tf_ner.py
templates/adding_a_new_example_script/{{cookiecutter.directory_name}}/run_{{cookiecutter.example_shortcut}}.py
==================
1cd16512d;Suraj Patil;2021-02-05 23:21:57 +0530;[examples/seq2seq] support label smoothing (#9844)
* add prepare_decoder_input_ids_from_labels in s2s models

* support lbl smoothing and enc/emb freezing

* fix freezing

* use pad_token_id from config

* remove embed freezing and add warning

* prepare decoder_input_ids inside DataCollatorForSeq2Seq
==

examples/seq2seq/run_seq2seq.py
src/transformers/data/data_collator.py
src/transformers/models/bart/modeling_bart.py
src/transformers/models/fsmt/modeling_fsmt.py
src/transformers/models/led/modeling_led.py
src/transformers/models/marian/modeling_marian.py
src/transformers/models/mbart/modeling_mbart.py
src/transformers/models/pegasus/modeling_pegasus.py
src/transformers/models/prophetnet/modeling_prophetnet.py
src/transformers/models/t5/modeling_t5.py
==================
b9720dd6f;Patrick von Platen;2021-02-05 16:20:26 +0300;Bump minimum Jax requirement to 2.8.0 (#10027)
* Bump minimum Jax requirement to 2.8.0

* update table
==

setup.py
src/transformers/dependency_versions_table.py
==================
89be094e2;Patrick von Platen;2021-02-05 15:47:54 +0300;[Templates] Add template "call-for-model" markdown and "call-for-big-bird" markdown (#9921)
* add big bird

* change teacher to mentor

* add proposal template

* adapt template

* delete old template

* correct some links

* finish template

* create big bird from template

* add big bird

* improve boxes

* finish boxes

* add pointers for BigBird

* finish big bird

* up

* up

* up

* up

* apply lysandres and sylvains suggestions

* delete bogus file

* correct markdown

* try different style

* try different style

* finalize
==

docs/source/imgs/transformers_overview.png
templates/adding_a_new_model/ADD_NEW_MODEL_PROPOSAL_TEMPLATE.md
templates/adding_a_new_model/open_model_proposals/ADD_BIG_BIRD.md
templates/adding_a_new_model/open_model_proposals/README.md
==================
4bbad604e;Lysandre Debut;2021-02-05 11:40:30 +0100;Clarify QA pipeline output based on character (#10021)
* Clarify QA pipeline output based on character

* Style
==

src/transformers/pipelines/question_answering.py
==================
ad2c43109;Lysandre;2021-02-05 11:18:59 +0100;Update doc deployment script path

==

.circleci/deploy.sh
==================
95a5f271e;Lysandre;2021-02-05 11:10:29 +0100;Update doc deployment script

==

.circleci/deploy.sh
.github/conda/meta.yaml
==================
3be965c5d;Sylvain Gugger;2021-02-04 16:52:27 -0500;Update doc for pre-release (#10014)
* Update doc for pre-release

* Use stable as default

* Use the right commit :facepalms:
==

.circleci/deploy.sh
docs/source/_static/js/custom.js
==================
ba607db18;Sylvain Gugger;2021-02-04 16:23:05 -0500;Bump version

==

setup.py
src/transformers/__init__.py
==================
4cd22512d;Sylvain Gugger;2021-02-04 15:41:19 -0500;Release: 4.3.0.rc1

==

setup.py
src/transformers/__init__.py
==================
4739ce177;Sylvain Gugger;2021-02-04 15:06:58 -0500;Fix test for sagemaker and TPU integrations

==

src/transformers/trainer.py
==================
21b3922e3;Sylvain Gugger;2021-02-04 14:18:33 -0500;Authorize last version of tokenizer (#9799)
* Authorize last version of tokenizer

* Update version table

* Fix conversion of spm tokenizers and fix some hub links

* Bump tokenizers version to 0.10.1rc1

* Add script to check tokenizers conversion with XNLI

* Add some more mask_token lstrip support

* Must modify mask_token in slow tokenizers too

* Keep using the old method for Pegasus

* add missing import

Co-authored-by: Anthony MOI <m.anthony.moi@gmail.com>
==

scripts/check_tokenizers.py
setup.py
src/transformers/convert_slow_tokenizer.py
src/transformers/dependency_versions_table.py
src/transformers/models/albert/tokenization_albert.py
src/transformers/models/albert/tokenization_albert_fast.py
src/transformers/models/barthez/tokenization_barthez.py
src/transformers/models/barthez/tokenization_barthez_fast.py
src/transformers/models/camembert/tokenization_camembert.py
src/transformers/models/camembert/tokenization_camembert_fast.py
src/transformers/models/pegasus/tokenization_pegasus.py
src/transformers/models/pegasus/tokenization_pegasus_fast.py
src/transformers/models/reformer/tokenization_reformer.py
src/transformers/models/reformer/tokenization_reformer_fast.py
src/transformers/models/xlm_roberta/tokenization_xlm_roberta.py
src/transformers/models/xlm_roberta/tokenization_xlm_roberta_fast.py
src/transformers/models/xlnet/tokenization_xlnet.py
src/transformers/models/xlnet/tokenization_xlnet_fast.py
==================
d5888ef0a;Nicolas Patry;2021-02-04 17:41:34 +0100;Hotfixing tests (blenderbot decoderonly tests, also need to remove (#10003)
`encoder_no_repeat_ngram_size` from their config.
==

tests/test_modeling_blenderbot.py
==================
8c3b1fcb6;Stas Bekman;2021-02-04 07:44:56 -0800;[trainer] a few fixes (#9993)
* trainer fixes

* don't switch the model  just for deepspeed and mp

* correct the fix
==

src/transformers/trainer.py
==================
714855bd8;Daniel Stancl;2021-02-04 16:24:47 +0100;Remove "double" assignment  in TF-BART like models (#9997)
* Replace `attn_weights = attn_wegihts = tf.reshape(...)`
with `attn_weights = tf.reshape(...)` and thus remove
unintentionally used "double" assignment.
==

src/transformers/models/bart/modeling_tf_bart.py
src/transformers/models/blenderbot/modeling_tf_blenderbot.py
src/transformers/models/blenderbot_small/modeling_tf_blenderbot_small.py
src/transformers/models/marian/modeling_tf_marian.py
src/transformers/models/mbart/modeling_tf_mbart.py
src/transformers/models/pegasus/modeling_tf_pegasus.py
==================
b72f16b3e;Sylvain Gugger;2021-02-04 10:14:46 -0500;Fix doc for TFConverBertModel

==

docs/source/model_doc/convbert.rst
==================
aeb18b922;Nicolas Patry;2021-02-04 15:00:18 +0100;Adding new `encoder_no_repeat_ngram_size` to `generate`. (#9984)
Adding new `encoder_no_repeat_ngram_size` to `generate`.

Blenderbot results seemed off compared to original ParlAI script:
`https://parl.ai/projects/recipes/`. Notably the model seems
to repeat a lot what was said during the conversation.

The actual problem was that `no_repeat_ngram_size` actually applies
to the `encoder_input_ids` but HF's `no_repeat_ngram_size` applies
to the previously generated ids (within the decoder). The history
conversation of blenderbot is within the `encoder` part so that
explains why HF's implementation had the repetitions.

This fix was focused on blenderbot *not* small and added tests
for those because they are quite different in configuration.

This change includes:

- Adding a new EncoderNoRepeatLogitProcessor.
- Adding 1 new arg to `generate` (`encoder_no_repeat_ngram_size`)
- Adding 1 new config parameter `encoder_no_repeat_ngram_size`.
- Adding 2 tests, one for the pipeline (high level, inputs exhibited
repeat behavior, one low level for EncoderNoRepeatLogitProcessor)
- Factored NoRepeatLogitProcessor so that logic could be reused.

Further work:

- Blenderbot conversational pipeline still does not behave correctly
 as they way input is prepared within the pipeline is still incorrect
(follow up PR)
- Blenderbot allows the bot to have personas, which is done by
prepending "your personna: XXXX" to the input, this could be explored
too in a follow up PR.

@patrickvonplaten
@LysandreJik

* Update src/transformers/generation_logits_process.py

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* Update src/transformers/generation_utils.py

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* Update src/transformers/generation_utils.py

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* Update src/transformers/configuration_utils.py

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* Doc quality.

* Fixing test.

* Last fixes.

* Fixing to account for batch_size.

* Update src/transformers/configuration_utils.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/generation_utils.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

src/transformers/configuration_utils.py
src/transformers/generation_logits_process.py
src/transformers/generation_utils.py
src/transformers/models/blenderbot/configuration_blenderbot.py
tests/test_generation_logits_process.py
tests/test_pipelines_conversational.py
==================
e89c959af;Lysandre Debut;2021-02-04 13:47:26 +0100;Fix model templates (#9999)

==

src/transformers/models/bart/modeling_bart.py
src/transformers/models/blenderbot/modeling_blenderbot.py
src/transformers/models/blenderbot_small/modeling_blenderbot_small.py
src/transformers/models/marian/modeling_marian.py
src/transformers/models/mbart/modeling_mbart.py
src/transformers/models/pegasus/modeling_pegasus.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/__init__.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_{{cookiecutter.lowercase_modelname}}.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/test_modeling_{{cookiecutter.lowercase_modelname}}.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/to_replace_{{cookiecutter.lowercase_modelname}}.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/{{cookiecutter.lowercase_modelname}}.rst
==================
804cd185d;Daniel Hug;2021-02-04 04:24:59 -0500;Added Integration testing for DistilBert model from issue #9948' (#9995)

==

tests/test_modeling_distilbert.py
==================
00031785a;demSd;2021-02-04 09:56:12 +0100;BartForCausalLM analogs to `ProphetNetForCausalLM` (#9128)
* initiliaze bart4causalLM

* create BartDecoderWrapper, setters/getters

* delete spaces

* forward and additional methods

* update cache function, loss function, remove ngram* params in data class.

* add bartcausallm, bartdecoder testing

* correct bart for causal lm

* remove at

* add mbart as well

* up

* fix typo

* up

* correct

* add pegasusforcausallm

* add blenderbotforcausallm

* add blenderbotsmallforcausallm

* add marianforcausallm

* add test for MarianForCausalLM

* add Pegasus test

* add BlenderbotSmall test

* add blenderbot test

* fix a fail

* fix an import fail

* a fix

* fix

* Update modeling_pegasus.py

* fix models

* fix inputs_embeds setting getter

* adapt tests

* correct repo utils check

* finish test improvement

* fix tf models as well

* make style

* make fix-copies

* fix copies

* run all tests

* last changes

* fix all tests

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
==

docs/source/model_doc/bart.rst
docs/source/model_doc/blenderbot.rst
docs/source/model_doc/blenderbot_small.rst
docs/source/model_doc/marian.rst
docs/source/model_doc/mbart.rst
docs/source/model_doc/pegasus.rst
src/transformers/__init__.py
src/transformers/models/auto/modeling_auto.py
src/transformers/models/bart/__init__.py
src/transformers/models/bart/modeling_bart.py
src/transformers/models/bart/modeling_tf_bart.py
src/transformers/models/blenderbot/__init__.py
src/transformers/models/blenderbot/modeling_blenderbot.py
src/transformers/models/blenderbot/modeling_tf_blenderbot.py
src/transformers/models/blenderbot_small/__init__.py
src/transformers/models/blenderbot_small/modeling_blenderbot_small.py
src/transformers/models/blenderbot_small/modeling_tf_blenderbot_small.py
src/transformers/models/marian/__init__.py
src/transformers/models/marian/modeling_marian.py
src/transformers/models/marian/modeling_tf_marian.py
src/transformers/models/mbart/__init__.py
src/transformers/models/mbart/modeling_mbart.py
src/transformers/models/mbart/modeling_tf_mbart.py
src/transformers/models/pegasus/__init__.py
src/transformers/models/pegasus/modeling_pegasus.py
src/transformers/models/pegasus/modeling_tf_pegasus.py
src/transformers/models/prophetnet/modeling_prophetnet.py
src/transformers/utils/dummy_pt_objects.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_{{cookiecutter.lowercase_modelname}}.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/test_modeling_{{cookiecutter.lowercase_modelname}}.py
tests/test_modeling_bart.py
tests/test_modeling_blenderbot.py
tests/test_modeling_blenderbot_small.py
tests/test_modeling_encoder_decoder.py
tests/test_modeling_marian.py
tests/test_modeling_mbart.py
tests/test_modeling_pegasus.py
utils/check_repo.py
==================
7898fc03b;Sylvain Gugger;2021-02-04 03:34:23 -0500;Add `from_slow` in fast tokenizers build and fixes some bugs (#9987)

==

src/transformers/models/albert/tokenization_albert.py
src/transformers/models/albert/tokenization_albert_fast.py
src/transformers/models/auto/tokenization_auto.py
src/transformers/models/bart/tokenization_bart.py
src/transformers/models/bart/tokenization_bart_fast.py
src/transformers/models/barthez/tokenization_barthez.py
src/transformers/models/barthez/tokenization_barthez_fast.py
src/transformers/models/blenderbot/tokenization_blenderbot.py
src/transformers/models/camembert/tokenization_camembert.py
src/transformers/models/camembert/tokenization_camembert_fast.py
src/transformers/models/t5/tokenization_t5.py
src/transformers/models/xlm_prophetnet/tokenization_xlm_prophetnet.py
src/transformers/models/xlm_roberta/tokenization_xlm_roberta.py
src/transformers/models/xlm_roberta/tokenization_xlm_roberta_fast.py
src/transformers/tokenization_utils_base.py
src/transformers/tokenization_utils_fast.py
==================
6244727e0;Stefan Schweter;2021-02-03 17:42:16 +0100;distilbert: fix creation of sinusoidal embeddings when using PyTorch 1.8+ (#9917)

==

src/transformers/models/distilbert/modeling_distilbert.py
==================
2f06f2bcd;sandip;2021-02-03 22:11:10 +0530;Alber model integration testing added (#9980)

==

tests/test_modeling_albert.py
==================
75fd00fb2;sandip;2021-02-03 22:09:40 +0530;Integration test added for TF MPnet (#9979)

==

tests/test_modeling_tf_mpnet.py
==================
ce08043f7;sandip;2021-02-03 22:06:45 +0530;Integration test for mobilebert (#9978)

==

tests/test_modeling_tf_mobilebert.py
==================
1486205d2;sandip;2021-02-03 20:21:00 +0530;TF DistilBERT integration tests  (#9975)
* TF DistilBERT integration test

* Update test_modeling_tf_distilbert.py
==

tests/test_modeling_tf_distilbert.py
==================
f2d5c04e1;sandip;2021-02-03 20:19:18 +0530;Added integration tests for TensorFlow implementation of the ALBERT model (#9976)
* TF Albert integration test

* TF Alber integration test added
==

tests/test_modeling_tf_albert.py
==================
bca0dd5ee;Suraj Patil;2021-02-03 20:14:42 +0530;[run_clm.py] fix getting extention

==

examples/language-modeling/run_clm.py
==================
5442a11f5;yylun;2021-02-03 22:30:37 +0800;fix steps_in_epoch variable in trainer when using max_steps (#9969)
* fix steps_in_epoch variable when using max_steps

* redundant sentence

* Revert "redundant sentence"

This reverts commit ad5c0e9b6e66d65732dee2239cdc9c76dfa0dc5a.

* remove redundant sentence

Co-authored-by: wujindou <wujindou@sogou-inc.com>
==

README.md
docs/source/index.rst
src/transformers/trainer.py
==================
3f77c26d7;Julien Plu;2021-02-03 12:26:32 +0100;Fix Longformer and LED (#9942)
* Fix Longformer and LED

* Add a test for graph execution with inputs_embeds

* Apply style
==

src/transformers/models/led/modeling_tf_led.py
src/transformers/models/longformer/modeling_tf_longformer.py
tests/test_modeling_tf_common.py
==================
d55e10bea;Stas Bekman;2021-02-03 02:24:40 -0800;[research proj] [lxmert] rm bleach dependency (#9970)
Looks like a vulnerability and it's not really used anywhere in the code, so just as well remove it completely from deps.
https://github.com/huggingface/transformers/security/dependabot/examples/research_projects/lxmert/requirements.txt/bleach/open
==

examples/research_projects/lxmert/requirements.txt
==================
a1a67a3ce;abhishek thakur;2021-02-03 10:49:07 +0100;Fix GroupedLinearLayer in TF ConvBERT (#9972)

==

src/transformers/models/convbert/modeling_tf_convbert.py
==================
71bdc076d;Daniel Stancl;2021-02-02 20:06:52 +0100;Add head_mask and decoder_head_mask to PyTorch LED (#9856)
* Add {decoder_,}head_mask to LED

* Fix create_custom_forward signatue in encoder

* Add head_mask to longformer

* Add head_mask to longformer to fix dependencies
of LED on Longformer.

* Not working yet

* Add mising one input in longofrmer_modeling.py

* make fix-copies
==

src/transformers/models/led/modeling_led.py
src/transformers/models/longformer/modeling_longformer.py
tests/test_modeling_common.py
tests/test_modeling_led.py
tests/test_modeling_longformer.py
==================
d6217fb30;Patrick von Platen;2021-02-02 15:52:10 +0300;Wav2Vec2 (#9659)
* add raw scaffold

* implement feat extract layers

* make style

* remove +

* correctly convert weights

* make feat extractor work

* make feature extraction proj work

* run forward pass

* finish forward pass

* Succesful decoding example

* remove unused files

* more changes

* add wav2vec tokenizer

* add new structure

* fix run forward

* add other layer norm architecture

* finish 2nd structure

* add model tests

* finish tests for tok and model

* clean-up

* make style

* finish docstring for model and config

* make style

* correct docstring

* correct tests

* change checkpoints to fairseq

* fix examples

* finish wav2vec2

* make style

* apply sylvains suggestions

* apply lysandres suggestions

* change print to log.info

* re-add assert statement

* add input_values as required input name

* finish wav2vec2 tokenizer

* Update tests/test_tokenization_wav2vec2.py

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* apply sylvains suggestions

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
==

README.md
docs/source/index.rst
docs/source/model_doc/wav2vec2.rst
setup.py
src/transformers/__init__.py
src/transformers/dependency_versions_table.py
src/transformers/file_utils.py
src/transformers/models/__init__.py
src/transformers/models/auto/configuration_auto.py
src/transformers/models/auto/modeling_auto.py
src/transformers/models/auto/tokenization_auto.py
src/transformers/models/wav2vec2/__init__.py
src/transformers/models/wav2vec2/configuration_wav2vec2.py
src/transformers/models/wav2vec2/convert_wav2vec2_original_pytorch_checkpoint_to_pytorch.py
src/transformers/models/wav2vec2/modeling_wav2vec2.py
src/transformers/models/wav2vec2/tokenization_wav2vec2.py
src/transformers/testing_utils.py
src/transformers/utils/dummy_pt_objects.py
tests/test_modeling_wav2vec2.py
tests/test_tokenization_wav2vec2.py
==================
d996024af;Sylvain Gugger;2021-02-02 07:00:17 -0500;Use compute_loss in prediction_step (#9935)

==

src/transformers/trainer.py
==================
aa438a426;Stefan Schweter;2021-02-02 12:09:24 +0100;convbert: minor fixes for conversion script (#9937)

==

src/transformers/models/convbert/convert_convbert_original_tf1_checkpoint_to_pytorch.py
==================
62024453c;Sylvain Gugger;2021-02-02 05:46:33 -0500;Bump numpy (#9934)

==

setup.py
src/transformers/dependency_versions_table.py
==================
de38a6e4d;Sylvain Gugger;2021-02-02 05:22:20 -0500;Fix 9918 (#9932)
* Initial work

* Fix doc styler and other models
==

docs/source/main_classes/tokenizer.rst
src/transformers/models/dpr/modeling_dpr.py
src/transformers/models/dpr/modeling_tf_dpr.py
src/transformers/models/rag/modeling_rag.py
src/transformers/models/t5/modeling_t5.py
src/transformers/models/t5/modeling_tf_t5.py
utils/style_doc.py
==================
1809de516;Lysandre Debut;2021-02-02 10:39:33 +0100;ALBERT Tokenizer integration test (#9943)
* ALBERT Tokenizer integration test

* Batching

* Style
==

tests/test_tokenization_albert.py
==================
0f4dc5d86;Patrick von Platen;2021-02-02 12:22:42 +0300;fix typo in naming (#9944)

==

src/transformers/models/bart/modeling_tf_bart.py
src/transformers/models/blenderbot/modeling_tf_blenderbot.py
src/transformers/models/blenderbot_small/modeling_tf_blenderbot_small.py
src/transformers/models/mbart/modeling_tf_mbart.py
src/transformers/models/pegasus/modeling_tf_pegasus.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_tf_{{cookiecutter.lowercase_modelname}}.py
==================
538b3b460;Patrick von Platen;2021-02-02 10:35:27 +0300;[Tokenizer Utils Base] Make pad function more flexible (#9928)
* change tokenizer requirement

* split line

* Correct typo from list to str

* improve style

* make other function pretty as well

* add comment

* correct typo

* add new test

* pass tests for tok without padding token

* Apply suggestions from code review
==

examples/text-classification/run_tf_text_classification.py
src/transformers/data/processors/glue.py
src/transformers/models/barthez/tokenization_barthez.py
src/transformers/models/barthez/tokenization_barthez_fast.py
src/transformers/models/blenderbot_small/tokenization_blenderbot_small.py
src/transformers/models/camembert/tokenization_camembert.py
src/transformers/models/camembert/tokenization_camembert_fast.py
src/transformers/models/distilbert/tokenization_distilbert.py
src/transformers/models/distilbert/tokenization_distilbert_fast.py
src/transformers/models/dpr/tokenization_dpr.py
src/transformers/models/dpr/tokenization_dpr_fast.py
src/transformers/models/fsmt/tokenization_fsmt.py
src/transformers/models/gpt2/tokenization_gpt2.py
src/transformers/models/gpt2/tokenization_gpt2_fast.py
src/transformers/models/marian/tokenization_marian.py
src/transformers/models/mpnet/tokenization_mpnet.py
src/transformers/models/mpnet/tokenization_mpnet_fast.py
src/transformers/models/openai/tokenization_openai.py
src/transformers/models/openai/tokenization_openai_fast.py
src/transformers/models/pegasus/tokenization_pegasus.py
src/transformers/models/pegasus/tokenization_pegasus_fast.py
src/transformers/models/reformer/tokenization_reformer.py
src/transformers/models/reformer/tokenization_reformer_fast.py
src/transformers/models/retribert/tokenization_retribert.py
src/transformers/models/retribert/tokenization_retribert_fast.py
src/transformers/models/roberta/tokenization_roberta.py
src/transformers/models/roberta/tokenization_roberta_fast.py
src/transformers/models/t5/tokenization_t5.py
src/transformers/models/t5/tokenization_t5_fast.py
src/transformers/models/transfo_xl/tokenization_transfo_xl.py
src/transformers/models/xlm_prophetnet/tokenization_xlm_prophetnet.py
src/transformers/models/xlm_roberta/tokenization_xlm_roberta.py
src/transformers/models/xlm_roberta/tokenization_xlm_roberta_fast.py
src/transformers/pipelines/question_answering.py
src/transformers/tokenization_utils_base.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/tokenization_{{cookiecutter.lowercase_modelname}}.py
tests/test_tokenization_common.py
tests/test_tokenization_gpt2.py
tests/test_tokenization_openai.py
tests/test_tokenization_reformer.py
==================
d1b14c9b5;Jan Jitse Venselaar;2021-02-01 17:17:50 +0100;Tensorflow doc changes on loss output size (#9922)
* Change documentation to correctly specify loss tensor size

* Change documentation to correct input format for labels

* Corrected output size of loss tensor for sequence classifier, multiple choice model and question answering
==

src/transformers/modeling_tf_outputs.py
src/transformers/models/blenderbot/modeling_tf_blenderbot.py
src/transformers/models/blenderbot_small/modeling_tf_blenderbot_small.py
src/transformers/models/marian/modeling_tf_marian.py
src/transformers/models/pegasus/modeling_tf_pegasus.py
==================
343057e14;Suraj Patil;2021-02-01 21:47:14 +0530;Fix bart conversion script (#9923)
* fix conversion script

* typo

* import nn
==

src/transformers/models/bart/convert_bart_original_pytorch_checkpoint_to_pytorch.py
==================
0e3be1ac8;Patrick von Platen;2021-02-01 17:55:10 +0300;Add new model docs (#9667)
* add new model logic

* fix docs

* change structure

* improve add_new_model

* push new changes

* up

* up

* correct spelling

* improve docstring

* correct line length

* update readme

* correct links

* correct typos

* only add rst file for now

* Apply suggestions from code review 1

Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>
Co-authored-by: Bram Vanroy <Bram.Vanroy@UGent.be>

* Apply suggestions from code review

Co-authored-by: Bram Vanroy <Bram.Vanroy@UGent.be>
Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>

* Apply suggestions from code review

Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>

* Apply suggestions from code review

Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>
Co-authored-by: Stefan Schweter <stefan@schweter.it>
Co-authored-by: Bram Vanroy <Bram.Vanroy@UGent.be>

* Apply suggestions from code review

Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>
Co-authored-by: Pierric Cistac <Pierrci@users.noreply.github.com>

* finish adding all suggestions

* make style

* apply Niels feedback

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* apply sylvains suggestions

Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>
Co-authored-by: Bram Vanroy <Bram.Vanroy@UGent.be>
Co-authored-by: Stefan Schweter <stefan@schweter.it>
Co-authored-by: Pierric Cistac <Pierrci@users.noreply.github.com>
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

docs/source/add_new_model.rst
docs/source/imgs/transformers_overview.png
docs/source/index.rst
templates/adding_a_new_model/open_model_proposals/README.md
==================
0842c33ed;Suraj Patil;2021-02-01 18:47:45 +0530;fix typos (#9924)

==

src/transformers/models/auto/modeling_auto.py
src/transformers/models/auto/modeling_tf_auto.py
==================
8672bcda1;CeShine Lee;2021-02-01 21:07:33 +0800;Adafactor: avoid updating group["lr"] attributes (#9751)
This affects Adafactor with relative_step=False and scale_parameter=True.
Updating group["lr"] makes the result of ._get_lr() depends on the previous call,
i.e., on the scale of other parameters. This isn't supposed to happen.
==

src/transformers/optimization.py
==================
115d97dd2;Sylvain Gugger;2021-02-01 08:06:32 -0500;Remove subclass for sortish sampler (#9907)
* Remove subclass for sortish sampler

* Use old Seq2SeqTrainer in script

* Styling
==

examples/seq2seq/finetune_trainer.py
src/transformers/trainer_seq2seq.py
==================
1682804eb;wlhgtc;2021-02-01 16:37:59 +0800;Fit chinese wwm to new datasets (#9887)
* MOD: fit chinese wwm to new datasets

* MOD: move wwm to new folder

* MOD: formate code

* Styling

* MOD add param and recover trainer

Co-authored-by: Sylvain Gugger <sylvain.gugger@gmail.com>
==

examples/language-modeling/README.md
examples/research_projects/mlm_wwm/README.md
examples/research_projects/mlm_wwm/requirements.txt
examples/research_projects/mlm_wwm/run_chinese_ref.py
examples/research_projects/mlm_wwm/run_mlm_wwm.py
src/transformers/data/data_collator.py
==================
24881008a;Stas Bekman;2021-02-01 00:14:06 -0800;[wandb] restore WANDB_DISABLED=true to disable wandb  (#9896)
* [t5 doc] typos

a few run away backticks

@sgugger

* style

* [trainer] put fp16 args together

this PR proposes a purely cosmetic change that puts all the fp16 args together - so they are easier to manager/read

@sgugger

* style

* [wandb] make WANDB_DISABLED disable wandb with any value

This PR solves part of https://github.com/huggingface/transformers/issues/9623

It tries to actually do what https://github.com/huggingface/transformers/issues/9699 requested/discussed and that is any value of `WANDB_DISABLED` should disable wandb.

The current behavior is that it has to be one of `ENV_VARS_TRUE_VALUES = {"1", "ON", "YES"}`

I have been using `WANDB_DISABLED=true` everywhere in scripts as it was originally advertised. I have no idea why this was changed to a sub-set of possible values. And it's not documented anywhere.

@sgugger

* WANDB_DISABLED=true to disable; make tf trainer consistent

* style
==

src/transformers/file_utils.py
src/transformers/integrations.py
src/transformers/trainer_tf.py
==================
6bab83683;Stas Bekman;2021-02-01 00:08:12 -0800;fix logger format for non-main process (#9911)

==

examples/seq2seq/finetune_trainer.py
==================
d85691ac7;Sylvain Gugger;2021-02-01 03:05:31 -0500;Doc title in the template (#9910)

==

templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/{{cookiecutter.lowercase_modelname}}.rst
==================
0c6c0afc0;Daniel Stancl;2021-02-01 07:30:21 +0100;Add head_mask and decoder_head_mask to FSMT (#9819)
* Add {decoder_,}head_mask to fsmt_modeling.py

* Enable test_headmasking and some changes to docs

* Remove test_head_masking flag from fsmt test file

Remove test_head_masking flag from test_modeling_fsmt.py
since test_head_masking is set to be True by default (thus it is redundant to store).

* Merge master and remove test_head_masking = True

* Rebase necessary due to an update of jaxlib

* Remove test_head_masking=True in tests/test_modeling_fsmt.py
as it is redundant.
==

src/transformers/models/fsmt/modeling_fsmt.py
tests/test_modeling_fsmt.py
==================
74f16b827;Kiyoung Kim;2021-02-01 07:31:29 +0900;TFBart lables consider both pad token and -100 (#9847)
* TFBart lables consider both pad token and -100

* make style

* fix for all other models

Co-authored-by: kykim <kykim>
Co-authored-by: patrickvonplaten <patrick.v.platen@gmail.com>
==

src/transformers/models/bart/modeling_tf_bart.py
src/transformers/models/blenderbot/modeling_tf_blenderbot.py
src/transformers/models/blenderbot_small/modeling_tf_blenderbot_small.py
src/transformers/models/marian/modeling_tf_marian.py
src/transformers/models/mbart/modeling_tf_mbart.py
src/transformers/models/pegasus/modeling_tf_pegasus.py
==================
22121e813;lewtun;2021-01-31 17:09:31 +0100;Clarify definition of seed argument in TrainingArguments (#9903)
* Clarify definition of seed argument in Trainer

* Update src/transformers/training_args.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/training_args_tf.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Fix style

* Update src/transformers/training_args.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

src/transformers/training_args.py
src/transformers/training_args_tf.py
==================
40cfc355f;Stas Bekman;2021-01-30 06:59:19 -0800;[doc] nested markup is invalid in rst (#9898)
Apparently nested markup in RST is invalid: https://docutils.sourceforge.io/FAQ.html#is-nested-inline-markup-possible

So currently this line doesn't get rendered properly, leaving inner markdown unrendered, resulting in:
```
https://docutils.sourceforge.io/FAQ.html#is-nested-inline-markup-possible
```

This PR removes the bold which fixes the link.
==

docs/source/model_sharing.rst
==================
1420b5ff6;Stas Bekman;2021-01-29 08:18:04 -0800;refactor deepspeed setup devices (#9880)

==

src/transformers/training_args.py
==================
6bf94bc0b;Stas Bekman;2021-01-29 08:11:22 -0800;correctly handle mt5 (#9879)

==

examples/seq2seq/utils.py
==================
7eadfe166;Sylvain Gugger;2021-01-29 09:52:26 -0500;When on sagemaker use their env variables for saves (#9876)
* When on sagemaker use their env variables for saves

* Address review comments

* Quality
==

src/transformers/trainer.py
src/transformers/training_args.py
==================
fdcde144d;Julien Plu;2021-01-29 11:25:03 +0100;Add XLA test (#9848)

==

tests/test_modeling_tf_bart.py
tests/test_modeling_tf_blenderbot.py
tests/test_modeling_tf_blenderbot_small.py
tests/test_modeling_tf_common.py
tests/test_modeling_tf_convbert.py
tests/test_modeling_tf_ctrl.py
tests/test_modeling_tf_flaubert.py
tests/test_modeling_tf_gpt2.py
tests/test_modeling_tf_led.py
tests/test_modeling_tf_longformer.py
tests/test_modeling_tf_marian.py
tests/test_modeling_tf_mbart.py
tests/test_modeling_tf_mpnet.py
tests/test_modeling_tf_openai.py
tests/test_modeling_tf_pegasus.py
tests/test_modeling_tf_t5.py
tests/test_modeling_tf_transfo_xl.py
tests/test_modeling_tf_xlm.py
==================
99b9affa0;Ethan Chau;2021-01-29 02:11:53 -0800;Clarify use of unk_token in tokenizer docstrings (#9875)

==

src/transformers/tokenization_utils.py
src/transformers/tokenization_utils_base.py
==================
c2d0ffec8;Nicolas Patry;2021-01-29 10:27:32 +0100;Adding a new `return_full_text` parameter to TextGenerationPipeline. (#9852)
* Adding a new `return_full_text` parameter to TextGenerationPipeline.

For text-generation, it's sometimes used as prompting text.
In that context, prefixing `generated_text` with the actual input
forces the caller to take an extra step to remove it.

The proposed change adds a new parameter (for backward compatibility).
`return_full_text` that enables the caller to prevent adding the prefix.

* Doc quality.
==

src/transformers/pipelines/text_generation.py
tests/test_pipelines_text_generation.py
==================
bc109ae5b;abhishek thakur;2021-01-28 21:10:46 +0100;pin_memory -> dataloader_pin_memory (#9874)

==

src/transformers/trainer.py
src/transformers/training_args.py
==================
80e4184fb;abhishek thakur;2021-01-28 19:11:04 +0100;on_log event should occur *after* the current log is written (#9872)

==

src/transformers/trainer.py
==================
15e4ce353;Stas Bekman;2021-01-28 09:36:46 -0800;[docs] expand install instructions (#9817)
* expand install instructions

* fix

* white space

* rewrite as discussed in the PR

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* change the wording to encourage issue report

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

README.md
docs/source/installation.md
==================
4c3ae89ad;Daniel Stancl;2021-01-28 16:09:13 +0100;Remove redundant `test_head_masking  = True` flags in test files (#9858)
* Remove redundant test_head_masking = True flags

* Remove all redundant test_head_masking flags in PyTorch test_modeling_* files

* Make test_head_masking = True as a default choice in test_modeling_tf_commong.py

* Remove all redundant test_head_masking flags in TensorFlow
test_modeling_tf_* files

* Put back test_head_masking=False fot TFT5 models
==

tests/test_modeling_bart.py
tests/test_modeling_blenderbot.py
tests/test_modeling_blenderbot_small.py
tests/test_modeling_distilbert.py
tests/test_modeling_lxmert.py
tests/test_modeling_marian.py
tests/test_modeling_mbart.py
tests/test_modeling_pegasus.py
tests/test_modeling_tf_bart.py
tests/test_modeling_tf_blenderbot.py
tests/test_modeling_tf_blenderbot_small.py
tests/test_modeling_tf_common.py
tests/test_modeling_tf_marian.py
tests/test_modeling_tf_mbart.py
tests/test_modeling_tf_pegasus.py
==================
caddf9126;Joe Davison;2021-01-28 09:21:58 -0500;tutorial typo

==

docs/source/custom_datasets.rst
==================
b4e559cfa;Sylvain Gugger;2021-01-28 08:32:46 -0500;Deprecate model_path in Trainer.train (#9854)

==

examples/language-modeling/run_clm.py
examples/language-modeling/run_mlm.py
examples/language-modeling/run_mlm_wwm.py
examples/language-modeling/run_plm.py
examples/multiple-choice/run_swag.py
examples/question-answering/run_qa.py
examples/question-answering/run_qa_beam_search.py
examples/seq2seq/run_seq2seq.py
examples/text-classification/run_glue.py
examples/token-classification/run_ner.py
src/transformers/integrations.py
src/transformers/trainer.py
templates/adding_a_new_example_script/{{cookiecutter.directory_name}}/run_{{cookiecutter.example_shortcut}}.py
tests/test_trainer.py
==================
2ee9f9b69;Funtowicz Morgan;2021-01-28 12:11:52 +0100;Fix computation of attention_probs when head_mask is provided. (#9853)
* Fix computation of attention_probs when head_mask is provided.

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* Apply changes to the template

Co-authored-by: Lysandre <lysandre.debut@reseau.eseo.fr>
==

src/transformers/models/bert/modeling_tf_bert.py
src/transformers/models/electra/modeling_tf_electra.py
src/transformers/models/roberta/modeling_tf_roberta.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_tf_{{cookiecutter.lowercase_modelname}}.py
==================
b936582f7;Nicolas Patry;2021-01-28 10:19:55 +0100;Fixing flaky conversational test + flag it as a pipeline test. (#9837)

==

tests/test_pipelines_conversational.py
==================
58fbef9eb;Lysandre Debut;2021-01-28 10:03:53 +0100;Remove submodule (#9868)

==

datasets
==================
6cb0a6f01;Lysandre Debut;2021-01-28 09:29:12 +0100;Partial local tokenizer load (#9807)
* Allow partial loading of a cached tokenizer

* Warning > Info

* Update src/transformers/tokenization_utils_base.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Raise error if not local_files_only

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

src/transformers/file_utils.py
src/transformers/tokenization_utils_base.py
==================
25fcb5c17;abhishek thakur;2021-01-28 08:50:46 +0100;Pin memory in Trainer by default (#9857)
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>
==

src/transformers/trainer.py
src/transformers/training_args.py
==================
5ed5a5468;Stefan Schweter;2021-01-27 19:25:11 +0100;ADD BORT (#9813)
* tests: add integration tests for new Bort model

* bort: add conversion script from Gluonnlp to Transformers üöÄ

* bort: minor cleanup (BORT -> Bort)

* add docs

* make fix-copies

* clean doc a bit

* correct docs

* Update docs/source/model_doc/bort.rst

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update docs/source/model_doc/bort.rst

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* correct dialogpt doc

* correct link

* Update docs/source/model_doc/bort.rst

* Update docs/source/model_doc/dialogpt.rst

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* make style

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

README.md
docs/source/index.rst
docs/source/model_doc/bort.rst
docs/source/model_doc/dialogpt.rst
src/transformers/models/bort/convert_bort_original_gluonnlp_checkpoint_to_pytorch.py
tests/test_modeling_bort.py
tests/test_modeling_tf_bort.py
==================
7c6d63298;Stas Bekman;2021-01-27 07:12:15 -0800;[traner] fix --lr_scheduler_type choices (#9800)
* fix --lr_scheduler_type choices

* rewrite to fix for all enum-based cl args

* cleanup

* adjust test

* style

* Proposal that should work

* Remove needless code

* Fix test

Co-authored-by: Sylvain Gugger <sylvain.gugger@gmail.com>
==

src/transformers/hf_argparser.py
tests/test_hf_argparser.py
==================
893120fac;Sylvain Gugger;2021-01-27 09:31:42 -0500;Allow --arg Value for booleans in HfArgumentParser (#9823)
* Allow --arg Value for booleans in HfArgumentParser

* Update last test

* Better error message
==

src/transformers/hf_argparser.py
tests/test_hf_argparser.py
==================
35d55b7b8;Sylvain Gugger;2021-01-27 09:31:18 -0500;When resuming training from checkpoint, Trainer loads model (#9818)
* Whenresuming training from checkpoint, Trainer loads model

* Finish cleaning tests

* Address review comment

* Use global_step from state
==

src/transformers/trainer.py
tests/test_trainer.py
==================
6b6c2b487;Lysandre Debut;2021-01-27 15:11:53 +0100;Test (#9851)

==

.github/workflows/model-templates.yml
==================
56c3f07a1;Lysandre Debut;2021-01-27 14:45:54 +0100;Labeled pull requests (#9849)

==

.github/workflows/model-templates.yml
==================
20932e552;Kiyoung Kim;2021-01-27 22:45:09 +0900;Add tpu_zone and gcp_project in training_args_tf.py (#9825)
* add tpu_zone and gcp_project in training_args_tf.py

* make style

Co-authored-by: kykim <kykim>
==

datasets
src/transformers/training_args_tf.py
==================
763ece2fe;Lysandre Debut;2021-01-27 14:20:58 +0100;Fix model templates (#9842)

==

templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_tf_{{cookiecutter.lowercase_modelname}}.py
==================
bd701ab1a;Julien Plu;2021-01-27 13:40:30 +0100;Fix template (#9840)

==

src/transformers/models/bert/modeling_tf_bert.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_tf_{{cookiecutter.lowercase_modelname}}.py
==================
c7b7bd996;Sylvain Gugger;2021-01-27 06:18:06 -0500;Add a flag for find_unused_parameters (#9820)
* Add a flag for find_unused_parameters

* Apply suggestions from code review

Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>

* Remove negation

Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>
==

src/transformers/trainer.py
src/transformers/training_args.py
==================
4adbdce5e;Julien Plu;2021-01-27 11:28:11 +0100;Clean TF Bert (#9788)
* Start cleaning BERT

* Clean BERT and all those depends of it

* Fix attribute name

* Apply style

* Apply Sylvain's comments

* Apply Lysandre's comments

* remove unused import
==

src/transformers/modeling_tf_utils.py
src/transformers/models/albert/modeling_tf_albert.py
src/transformers/models/bert/modeling_tf_bert.py
src/transformers/models/ctrl/modeling_tf_ctrl.py
src/transformers/models/distilbert/modeling_tf_distilbert.py
src/transformers/models/electra/modeling_tf_electra.py
src/transformers/models/funnel/modeling_tf_funnel.py
src/transformers/models/longformer/modeling_tf_longformer.py
src/transformers/models/lxmert/modeling_tf_lxmert.py
src/transformers/models/mobilebert/modeling_tf_mobilebert.py
src/transformers/models/mpnet/modeling_tf_mpnet.py
src/transformers/models/openai/modeling_tf_openai.py
src/transformers/models/roberta/modeling_tf_roberta.py
src/transformers/models/xlm/modeling_tf_xlm.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_tf_{{cookiecutter.lowercase_modelname}}.py
==================
f0329ea51;tomohideshibata;2021-01-27 19:15:23 +0900;Delete a needless duplicate condition (#9826)
Co-authored-by: Tomohide Shibata <tomshiba@yahoo-corp.jp>
==

src/transformers/generation_logits_process.py
==================
a1720694a;Julien Plu;2021-01-27 10:45:42 +0100;Remove a TF usage warning and rework the documentation (#9756)
* Rework documentation

* Update the template

* Trigger CI

* Restore the warning but with the TF logger

* Update convbert doc
==

src/transformers/modeling_tf_utils.py
src/transformers/models/albert/modeling_tf_albert.py
src/transformers/models/bart/modeling_tf_bart.py
src/transformers/models/bert/modeling_tf_bert.py
src/transformers/models/blenderbot/modeling_tf_blenderbot.py
src/transformers/models/blenderbot_small/modeling_tf_blenderbot_small.py
src/transformers/models/convbert/modeling_tf_convbert.py
src/transformers/models/ctrl/modeling_tf_ctrl.py
src/transformers/models/distilbert/modeling_tf_distilbert.py
src/transformers/models/dpr/modeling_tf_dpr.py
src/transformers/models/electra/modeling_tf_electra.py
src/transformers/models/flaubert/modeling_tf_flaubert.py
src/transformers/models/funnel/modeling_tf_funnel.py
src/transformers/models/gpt2/modeling_tf_gpt2.py
src/transformers/models/led/modeling_tf_led.py
src/transformers/models/longformer/modeling_tf_longformer.py
src/transformers/models/lxmert/modeling_tf_lxmert.py
src/transformers/models/marian/modeling_tf_marian.py
src/transformers/models/mbart/modeling_tf_mbart.py
src/transformers/models/mobilebert/modeling_tf_mobilebert.py
src/transformers/models/mpnet/modeling_tf_mpnet.py
src/transformers/models/openai/modeling_tf_openai.py
src/transformers/models/pegasus/modeling_tf_pegasus.py
src/transformers/models/roberta/modeling_tf_roberta.py
src/transformers/models/t5/modeling_tf_t5.py
src/transformers/models/transfo_xl/modeling_tf_transfo_xl.py
src/transformers/models/xlm/modeling_tf_xlm.py
src/transformers/models/xlnet/modeling_tf_xlnet.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_tf_{{cookiecutter.lowercase_modelname}}.py
==================
285c6262a;Nicolas Patry;2021-01-27 10:10:53 +0100;Adding a test to prevent late failure in the Table question answering (#9808)
pipeline.

- If table is empty then the line that contain `answer[0]` will fail.
- This PR add a check to prevent `answer[0]`.
- Also adds an early check for presence of `table` and `query` to
prevent late failure and give better error message.
- Adds a few tests to make sure these errors are correctly raised.
==

src/transformers/pipelines/table_question_answering.py
tests/test_pipelines_table_question_answering.py
==================
a46050d0f;Patrick von Platen;2021-01-27 10:09:56 +0100;fix typo with mt5 init (#9830)

==

src/transformers/models/mt5/__init__.py
==================
f4bf0dea4;jncasey;2021-01-27 03:48:18 -0500;Fix auto-resume training from checkpoint (#9822)
* Fix auto-resume training from checkpoint

* style fixes
==

src/transformers/trainer_utils.py
==================
f2fabedba;Sylvain Gugger;2021-01-27 03:39:11 -0500;Setup logging with a stdout handler (#9816)

==

examples/language-modeling/run_clm.py
examples/language-modeling/run_mlm.py
examples/language-modeling/run_mlm_wwm.py
examples/language-modeling/run_plm.py
examples/multiple-choice/run_swag.py
examples/question-answering/run_qa.py
examples/question-answering/run_qa_beam_search.py
examples/seq2seq/run_seq2seq.py
examples/text-classification/run_glue.py
examples/token-classification/run_ner.py
templates/adding_a_new_example_script/{{cookiecutter.directory_name}}/run_{{cookiecutter.example_shortcut}}.py
==================
2c891c156;Julien Plu;2021-01-27 09:36:49 +0100;Add a test for mixed precision (#9806)

==

tests/test_modeling_tf_albert.py
tests/test_modeling_tf_bart.py
tests/test_modeling_tf_blenderbot.py
tests/test_modeling_tf_blenderbot_small.py
tests/test_modeling_tf_common.py
tests/test_modeling_tf_ctrl.py
tests/test_modeling_tf_flaubert.py
tests/test_modeling_tf_funnel.py
tests/test_modeling_tf_gpt2.py
tests/test_modeling_tf_led.py
tests/test_modeling_tf_longformer.py
tests/test_modeling_tf_lxmert.py
tests/test_modeling_tf_marian.py
tests/test_modeling_tf_mbart.py
tests/test_modeling_tf_mobilebert.py
tests/test_modeling_tf_openai.py
tests/test_modeling_tf_pegasus.py
tests/test_modeling_tf_t5.py
tests/test_modeling_tf_transfo_xl.py
tests/test_modeling_tf_xlm.py
==================
d5b40d669;Patrick von Platen;2021-01-27 09:34:21 +0100;[Setup.py] update jaxlib (#9831)
* update jaxlib

* Update setup.py

* update table
==

setup.py
src/transformers/dependency_versions_table.py
==================
f617490e7;abhishek thakur;2021-01-27 09:20:09 +0100;ConvBERT Model (#9717)
* finalize convbert

* finalize convbert

* fix

* fix

* fix

* push

* fix

* tf image patches

* fix torch model

* tf tests

* conversion

* everything aligned

* remove print

* tf tests

* fix tf

* make tf tests pass

* everything works

* fix init

* fix

* special treatment for sepconv1d

* style

* üôèüèΩ

* add doc and cleanup

* add electra test again

* fix doc

* fix doc again

* fix doc again

* Update src/transformers/modeling_tf_pytorch_utils.py

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* Update src/transformers/models/conv_bert/configuration_conv_bert.py

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* Update docs/source/model_doc/conv_bert.rst

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/models/auto/configuration_auto.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/models/conv_bert/configuration_conv_bert.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* conv_bert -> convbert

* more fixes from review

* add conversion script

* dont use pretrained embed

* unused config

* suggestions from julien

* some more fixes

* p -> param

* fix copyright

* fix doc

* Update src/transformers/models/convbert/configuration_convbert.py

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* comments from reviews

* fix-copies

* fix style

* revert shape_list

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
==

README.md
docs/source/index.rst
docs/source/model_doc/convbert.rst
docs/source/model_summary.rst
src/transformers/__init__.py
src/transformers/convert_slow_tokenizer.py
src/transformers/modeling_tf_pytorch_utils.py
src/transformers/models/__init__.py
src/transformers/models/auto/configuration_auto.py
src/transformers/models/auto/modeling_auto.py
src/transformers/models/auto/modeling_tf_auto.py
src/transformers/models/auto/tokenization_auto.py
src/transformers/models/convbert/__init__.py
src/transformers/models/convbert/configuration_convbert.py
src/transformers/models/convbert/convert_convbert_original_tf1_checkpoint_to_pytorch.py
src/transformers/models/convbert/modeling_convbert.py
src/transformers/models/convbert/modeling_tf_convbert.py
src/transformers/models/convbert/tokenization_convbert.py
src/transformers/models/convbert/tokenization_convbert_fast.py
src/transformers/utils/dummy_pt_objects.py
src/transformers/utils/dummy_tf_objects.py
src/transformers/utils/dummy_tokenizers_objects.py
tests/test_modeling_convbert.py
tests/test_modeling_tf_convbert.py
utils/check_repo.py
==================
e575e0628;Patrick von Platen;2021-01-27 08:43:14 +0100;fix led not defined (#9828)

==

src/transformers/models/auto/tokenization_auto.py
==================
059bb2581;Yusuke Mori;2021-01-27 04:32:19 +0900;Fix a bug in run_glue.py (#9812) (#9815)

==

examples/text-classification/run_glue.py
==================
eba418ac5;Tristan Deleu;2021-01-26 19:21:26 +0100;Commit the last step on world_process_zero in WandbCallback (#9805)
* Commit the last step on world_process_zero in WandbCallback

* Use the environment variable WANDB_LOG_MODEL as a default value in WandbCallback
==

src/transformers/integrations.py
==================
8edc98bb7;Derrick Blakely;2021-01-26 12:32:46 -0500;Allow RAG to output decoder cross-attentions (#9789)
* get cross attns

* add cross-attns doc strings

* fix typo

* line length

* Apply suggestions from code review

Co-authored-by: Quentin Lhoest <42851186+lhoestq@users.noreply.github.com>

Co-authored-by: Quentin Lhoest <42851186+lhoestq@users.noreply.github.com>
==

src/transformers/models/rag/modeling_rag.py
==================
8f6c12d30;Magdalena Biesialska;2021-01-26 17:30:31 +0100;Fix fine-tuning translation scripts (#9809)

==

examples/seq2seq/README.md
==================
c37dcff76;Michael Glass;2021-01-26 10:44:02 -0500;Fixed parameter name for logits_processor (#9790)

==

src/transformers/models/rag/modeling_rag.py
==================
0d0efd3a0;Sylvain Gugger;2021-01-26 10:28:21 -0500;Smdistributed trainer (#9798)
* Add a debug print

* Adapt Trainer to use smdistributed if available

* Forgotten parenthesis

* Real check for sagemaker

* Donforget to define device...

* Woopsie, local)rank is defined differently

* Update since local_rank has the proper value

* Remove debug statement

* More robust check for smdistributed

* Quality

* Deal with key not present error
==

src/transformers/file_utils.py
src/transformers/trainer.py
src/transformers/trainer_pt_utils.py
src/transformers/trainer_utils.py
src/transformers/training_args.py
==================
897a24c86;Lysandre;2021-01-26 11:02:48 +0100;Fix head_mask for model templates

==

templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/test_modeling_tf_{{cookiecutter.lowercase_modelname}}.py
==================
10e5f2821;Andrea Cappelli;2021-01-26 10:47:07 +0100;Improve pytorch examples for fp16 (#9796)
* Pad to 8x for fp16 multiple choice example (#9752)

* Pad to 8x for fp16 squad trainer example (#9752)

* Pad to 8x for fp16 ner example (#9752)

* Pad to 8x for fp16 swag example (#9752)

* Pad to 8x for fp16 qa beam search example (#9752)

* Pad to 8x for fp16 qa example (#9752)

* Pad to 8x for fp16 seq2seq example (#9752)

* Pad to 8x for fp16 glue example (#9752)

* Pad to 8x for fp16 new ner example (#9752)

* update script template #9752

* Update examples/multiple-choice/run_swag.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update examples/question-answering/run_qa.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update examples/question-answering/run_qa_beam_search.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* improve code quality #9752

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

examples/legacy/multiple_choice/run_multiple_choice.py
examples/legacy/question-answering/run_squad_trainer.py
examples/legacy/token-classification/run_ner.py
examples/multiple-choice/run_swag.py
examples/question-answering/run_qa.py
examples/question-answering/run_qa_beam_search.py
examples/seq2seq/run_seq2seq.py
examples/text-classification/run_glue.py
examples/token-classification/run_ner.py
templates/adding_a_new_example_script/{{cookiecutter.directory_name}}/run_{{cookiecutter.example_shortcut}}.py
==================
781e4b138;Nicolas Patry;2021-01-26 10:06:28 +0100;Adding `skip_special_tokens=True` to FillMaskPipeline (#9783)
* We most likely don't want special tokens in this output.

* Adding `skip_special_tokens=True` to FillMaskPipeline

- It's backward incompatible.
- It makes for sense for pipelines to remove references to
special_tokens (all of the other pipelines do that).
- Keeping special tokens makes it hard for users to actually remove them
  because all models have different tokens (<s>, <cls>, [CLS], ....)

* Fixing `token_str` in the same vein, and actually fix the tests too !
==

src/transformers/pipelines/fill_mask.py
tests/test_pipelines_fill_mask.py
==================
1867d9a8d;Daniel Stancl;2021-01-26 09:50:00 +0100;Add head_mask/decoder_head_mask for TF BART models (#9639)
* Add head_mask/decoder_head_mask for TF BART models

* Add head_mask and decoder_head_mask input arguments for TF BART-based
models as a TF counterpart to the PR #9569

* Add test_headmasking functionality to tests/test_modeling_tf_common.py

* TODO: Add a test to verify that we can get a gradient back for
importance score computation

* Remove redundant #TODO note

Remove redundant #TODO note from tests/test_modeling_tf_common.py

* Fix assertions

* Make style

* Fix ...Model input args and adjust one new test

* Add back head_mask and decoder_head_mask to BART-based ...Model
after the last commit

* Remove head_mask ande decoder_head_mask from input_dict
in TF test_train_pipeline_custom_model as these two have different
shape than other input args (Necessary for passing this test)

* Revert adding global_rng in test_modeling_tf_common.py
==

src/transformers/models/bart/modeling_tf_bart.py
src/transformers/models/blenderbot/modeling_tf_blenderbot.py
src/transformers/models/blenderbot_small/modeling_tf_blenderbot_small.py
src/transformers/models/marian/modeling_tf_marian.py
src/transformers/models/mbart/modeling_tf_mbart.py
src/transformers/models/pegasus/modeling_tf_pegasus.py
tests/test_modeling_tf_albert.py
tests/test_modeling_tf_bart.py
tests/test_modeling_tf_bert.py
tests/test_modeling_tf_blenderbot.py
tests/test_modeling_tf_blenderbot_small.py
tests/test_modeling_tf_common.py
tests/test_modeling_tf_ctrl.py
tests/test_modeling_tf_distilbert.py
tests/test_modeling_tf_electra.py
tests/test_modeling_tf_flaubert.py
tests/test_modeling_tf_funnel.py
tests/test_modeling_tf_gpt2.py
tests/test_modeling_tf_led.py
tests/test_modeling_tf_longformer.py
tests/test_modeling_tf_lxmert.py
tests/test_modeling_tf_marian.py
tests/test_modeling_tf_mbart.py
tests/test_modeling_tf_mobilebert.py
tests/test_modeling_tf_mpnet.py
tests/test_modeling_tf_openai.py
tests/test_modeling_tf_pegasus.py
tests/test_modeling_tf_roberta.py
tests/test_modeling_tf_t5.py
tests/test_modeling_tf_transfo_xl.py
tests/test_modeling_tf_xlm.py
tests/test_modeling_tf_xlnet.py
==================
cb73ab5a3;Yusuke Mori;2021-01-26 17:37:57 +0900;Fix broken links in the converting tf ckpt document (#9791)
* Fix broken links in the converting tf ckpt document

* Update docs/source/converting_tensorflow_models.rst

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Reflect the review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

docs/source/converting_tensorflow_models.rst
==================
d94cc2f90;Patrick von Platen;2021-01-26 09:21:44 +0100;[Flaky Generation Tests] Make sure that no early stopping is happening for beam search (#9794)
* fix ci

* fix ci

* renaming

* fix dup line
==

tests/test_generation_utils.py
==================
0fdbf0850;Stas Bekman;2021-01-25 21:09:01 -0800;[PR/Issue templates] normalize, group, sort + add myself for deepspeed (#9706)
* normalize, group, sort + add myself for deepspeed

* new structure

* add ray

* typo

* more suggestions

* more suggestions

* white space

* Update .github/ISSUE_TEMPLATE/bug-report.md

Co-authored-by: Suraj Patil <surajp815@gmail.com>

* add bullets

* sync

* Apply suggestions from code review

Co-authored-by: Suraj Patil <surajp815@gmail.com>

* Apply suggestions from code review

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* sync

Co-authored-by: Suraj Patil <surajp815@gmail.com>
Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
==

.github/ISSUE_TEMPLATE/bug-report.md
.github/PULL_REQUEST_TEMPLATE.md
==================
af41da509;Sylvain Gugger;2021-01-25 12:40:58 -0500;Fix style

==

src/transformers/trainer.py
==================
caf4abf76;Sylvain Gugger;2021-01-25 12:03:51 -0500;Auto-resume training from checkpoint (#9776)
* Auto-resume training from checkpoint

* Update examples/text-classification/run_glue.py

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* Roll out to other examples

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
==

examples/language-modeling/run_clm.py
examples/language-modeling/run_mlm.py
examples/language-modeling/run_mlm_wwm.py
examples/language-modeling/run_plm.py
examples/multiple-choice/run_swag.py
examples/question-answering/run_qa.py
examples/question-answering/run_qa_beam_search.py
examples/seq2seq/run_seq2seq.py
examples/text-classification/run_glue.py
examples/token-classification/run_ner.py
src/transformers/trainer_utils.py
templates/adding_a_new_example_script/{{cookiecutter.directory_name}}/run_{{cookiecutter.example_shortcut}}.py
==================
0f443436f;Lysandre Debut;2021-01-25 17:12:07 +0100;Actual fix (#9787)

==

src/transformers/models/gpt2/modeling_gpt2.py
src/transformers/models/t5/modeling_t5.py
==================
fac7cfb16;Stas Bekman;2021-01-25 05:57:37 -0800;[fsmt] onnx triu workaround (#9738)
* onnx triu workaround

* style

* working this time

* add test

* more efficient version
==

src/transformers/models/fsmt/modeling_fsmt.py
tests/test_modeling_fsmt.py
==================
626116b7d;Sorami Hisamoto;2021-01-25 20:40:03 +0900;Fix a typo in Trainer.hyperparameter_search docstring (#9762)
`compute_objectie` => `compute_objective`
==

src/transformers/trainer.py
==================
d63ab6152;Kai Fricke;2021-01-25 11:01:55 +0100;Use object store to pass trainer object to Ray Tune (#9749)

==

src/transformers/integrations.py
==================
6312fed47;Maria Janina Sarol;2021-01-25 03:27:12 -0600;Fix TFTrainer prediction output (#9662)
* Fix TFTrainer prediction output

* Update trainer_tf.py

* Fix TFTrainer prediction output

* Fix evaluation_loss update in TFTrainer

* Fix TFTrainer prediction output
==

src/transformers/trainer_tf.py
==================
9152f1602;Wilfried L. Bounsi;2021-01-23 10:41:46 +0100;Fix broken [Open in Colab] links (#9761)

==

examples/README.md
==================
b7b7e5d04;Stas Bekman;2021-01-22 20:38:53 -0800;token_type_ids isn't used (#9736)

==

src/transformers/models/fsmt/tokenization_fsmt.py
==================
a449ffcbd;Julien Plu;2021-01-22 17:40:16 +0100;Fix test (#9755)

==

tests/test_modeling_tf_common.py
==================
82d46febe;Sylvain Gugger;2021-01-22 10:34:34 -0500;Add `report_to` training arguments to control the reporting integrations used (#9735)

==

src/transformers/integrations.py
src/transformers/trainer.py
src/transformers/training_args.py
==================
411c58210;Sylvain Gugger;2021-01-22 10:03:57 -0500;Fixes to run_seq2seq and instructions (#9734)
* Fixes to run_seq2seq and instructions

* Add more defaults for summarization
==

examples/seq2seq/README.md
examples/seq2seq/run_seq2seq.py
==================
d7c31abf3;Julien Plu;2021-01-22 14:50:46 +0100;Fix some TF slow tests (#9728)
* Fix saved model tests + fix a graph issue in longformer

* Apply style
==

src/transformers/models/longformer/modeling_tf_longformer.py
tests/test_modeling_tf_common.py
==================
08b22722c;Stefan Schweter;2021-01-22 13:43:52 +0100;examples: fix XNLI url (#9741)

==

examples/text-classification/README.md
==================
5f80c15ef;Sylvain Gugger;2021-01-21 12:05:46 -0500;Fix memory regression in Seq2Seq example (#9713)
* Fix memory regression in Seq2Seq example

* Fix test and properly deal with -100

* Easier condition with device safety

* Patch for MBartTokenzierFast
==

examples/seq2seq/finetune_trainer.py
examples/seq2seq/utils.py
src/transformers/trainer.py
src/transformers/trainer_pt_utils.py
tests/test_trainer_utils.py
==================
a7dabfb3d;Julien Plu;2021-01-21 17:03:29 +0100;Fix TF s2s models (#9478)
* Fix Seq2Seq models for serving

* Apply style

* Fix lonfgormer

* Fix mBart/Pegasus/Blenderbot

* Apply style

* Add a main intermediate layer

* Apply style

* Remove import

* Apply tf.function to Longformer

* Fix utils check_copy

* Update S2S template

* Fix BART + Blenderbot

* Fix BlenderbotSmall

* Fix BlenderbotSmall

* Fix BlenderbotSmall

* Fix MBart

* Fix Marian

* Fix Pegasus + template

* Apply style

* Fix common attributes test

* Forgot to fix the LED test

* Apply Patrick's comment on LED Decoder
==

src/transformers/modeling_tf_utils.py
src/transformers/models/bart/modeling_tf_bart.py
src/transformers/models/blenderbot/modeling_tf_blenderbot.py
src/transformers/models/blenderbot_small/__init__.py
src/transformers/models/blenderbot_small/modeling_tf_blenderbot_small.py
src/transformers/models/led/modeling_tf_led.py
src/transformers/models/longformer/modeling_tf_longformer.py
src/transformers/models/marian/modeling_tf_marian.py
src/transformers/models/mbart/modeling_tf_mbart.py
src/transformers/models/pegasus/modeling_tf_pegasus.py
src/transformers/models/t5/modeling_tf_t5.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_tf_{{cookiecutter.lowercase_modelname}}.py
tests/test_modeling_tf_bart.py
tests/test_modeling_tf_blenderbot.py
tests/test_modeling_tf_blenderbot_small.py
tests/test_modeling_tf_common.py
tests/test_modeling_tf_led.py
tests/test_modeling_tf_marian.py
tests/test_modeling_tf_mbart.py
tests/test_modeling_tf_pegasus.py
==================
23e5a36ee;Nicolas Patry;2021-01-21 14:31:51 +0100;Changing model default for TableQuestionAnsweringPipeline. (#9729)
* Changing model default for TableQuestionAnsweringPipeline.

- Discussion: https://discuss.huggingface.co/t/table-question-answering-is-not-an-available-task-under-pipeline/3284/6

* Updating slow tests that were out of sync.
==

src/transformers/pipelines/__init__.py
tests/test_pipelines_table_question_answering.py
==================
3f290e6c8;Julien Plu;2021-01-21 13:00:11 +0100;Fix mixed precision in TF models (#9163)
* Fix Gelu precision

* Fix gelu_fast

* Naming

* Fix usage and apply style

* add TF gelu approximate version

* add TF gelu approximate version

* add TF gelu approximate version

* Apply style

* Fix albert

* Remove the usage of the Activation layer
==

src/transformers/activations_tf.py
src/transformers/models/albert/modeling_tf_albert.py
src/transformers/models/bert/modeling_tf_bert.py
src/transformers/models/electra/modeling_tf_electra.py
src/transformers/models/longformer/modeling_tf_longformer.py
src/transformers/models/mpnet/modeling_tf_mpnet.py
src/transformers/models/roberta/modeling_tf_roberta.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_tf_{{cookiecutter.lowercase_modelname}}.py
==================
248fa1ae7;Suraj Patil;2021-01-21 16:46:14 +0530;fix T5 head mask in model_parallel (#9726)
* fix head mask in model_parallel

* pass correct head mask
==

src/transformers/models/t5/modeling_t5.py
==================
ca422e3d7;Patrick von Platen;2021-01-21 11:17:13 +0100;finish (#9721)

==

src/transformers/models/t5/modeling_t5.py
==================
c8ea582ed;Patrick von Platen;2021-01-21 11:16:15 +0100;reduce led memory (#9723)

==

tests/test_modeling_led.py
==================
fb36c273a;guillaume-be;2021-01-21 11:13:38 +0100;Allow text generation for ProphetNetForCausalLM  (#9707)
* Moved ProphetNetForCausalLM's parent initialization after config update

* Added unit tests for generation for ProphetNetForCausalLM
==

src/transformers/models/prophetnet/modeling_prophetnet.py
tests/test_modeling_prophetnet.py
==================
910aa8967;Lysandre Debut;2021-01-21 10:17:39 +0100;Temporarily deactivate TPU tests while we work on fixing them (#9720)

==

.circleci/config.yml
==================
6a346f035;Muennighoff;2021-01-21 09:21:01 +0100;fix typo (#9708)
* fix typo

Co-authored-by: Suraj Patil <surajp815@gmail.com>
==

src/transformers/modeling_tf_utils.py
==================
4a20b7c45;Stas Bekman;2021-01-20 16:50:21 -0800;[trainer] no --deepspeed and --sharded_ddp together (#9712)
* no --deepspeed and --sharded_ddp together

* Update src/transformers/trainer.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* style

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

src/transformers/trainer.py
==================
7acfa95af;Sylvain Gugger;2021-01-20 14:13:16 -0500;Add missing new line

==

docs/source/community.md
==================
5a307ece8;Darigov Research;2021-01-20 18:28:40 +0000;Adds flashcards to Glossary & makes small corrections (#8949)
* fix: Makes small typo corrections & standardises glossary

* feat: Adds introduction & links to transformer flashcards

* feat: Adds attribution & adjustments requested in #8949

* feat: Adds flashcards to community.md

* refactor: Removes flashcards from glossary
==

docs/source/community.md
docs/source/glossary.rst
==================
3cd91e816;Sylvain Gugger;2021-01-20 12:30:24 -0500;Fix WAND_DISABLED test (#9703)
* Fix WAND_DISABLED test

* Remove duplicate import

* Make a test that actually works...

* Fix style
==

src/transformers/integrations.py
==================
2a703773a;Sylvain Gugger;2021-01-20 12:17:40 -0500;Fix style

==

src/transformers/training_args.py
==================
cd5565bed;Stas Bekman;2021-01-20 09:07:07 -0800;fix the backward for deepspeed (#9705)

==

src/transformers/trainer.py
==================
538245b0c;Gunjan Chhablani;2021-01-20 22:29:31 +0530;Fix Trainer and Args to mention AdamW, not Adam. (#9685)
* Fix Trainer and Args to mention AdamW, not Adam.

* Update the docs for Training Arguments.

* Change arguments adamw_* to adam_*

* Fixed links to AdamW in TrainerArguments docs

* Fix line length in Training Args docs.
==

src/transformers/training_args.py
==================
88583d495;NielsRogge;2021-01-20 16:19:26 +0100;Add notebook (#9696)

==

docs/source/community.md
==================
d1370d29b;NielsRogge;2021-01-20 16:18:50 +0100;Add DeBERTa head models (#9691)
* Add DebertaForMaskedLM, DebertaForTokenClassification, DebertaForQuestionAnswering

* Add docs and fix quality

* Fix Deberta not having pooler
==

docs/source/model_doc/deberta.rst
src/transformers/__init__.py
src/transformers/models/auto/modeling_auto.py
src/transformers/models/deberta/__init__.py
src/transformers/models/deberta/modeling_deberta.py
src/transformers/utils/dummy_pt_objects.py
tests/test_modeling_deberta.py
==================
a7b62fece;Sylvain Gugger;2021-01-20 09:50:20 -0500;Fix Funnel Transformer conversion script (#9683)

==

src/transformers/models/funnel/convert_funnel_original_tf_checkpoint_to_pytorch.py
==================
8940c7662;acul3;2021-01-20 21:34:27 +0700;Add t5 convert to transformers-cli (#9654)
* Update run_mlm.py

* add t5 model to transformers-cli convert

* update rum_mlm.py same as master

* update converting model docs

* update converting model docs

* Update convert.py

* Trigger notification

* update import sorted

* fix typo t5
==

docs/source/converting_tensorflow_models.rst
src/transformers/commands/convert.py
==================
7251a4736;Julien Plu;2021-01-20 15:04:53 +0100;Fix template (#9697)

==

src/transformers/models/roberta/modeling_tf_roberta.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_tf_{{cookiecutter.lowercase_modelname}}.py
==================
14042d560;Julien Plu;2021-01-20 12:08:12 +0100;New TF embeddings (cleaner and faster) (#9418)
* Create new embeddings + add to BERT

* Add Albert

* Add DistilBert

* Add Albert + Electra + Funnel

* Add Longformer + Lxmert

* Add last models

* Apply style

* Update the template

* Remove unused imports

* Rename attribute

* Import embeddings in their own model file

* Replace word_embeddings per weight

* fix naming

* Fix Albert

* Fix Albert

* Fix Longformer

* Fix Lxmert Mobilebert and MPNet

* Fix copy

* Fix template

* Update the get weights function

* Update src/transformers/modeling_tf_utils.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/models/electra/modeling_tf_electra.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* address Sylvain's comments

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

src/transformers/modeling_tf_utils.py
src/transformers/models/albert/modeling_tf_albert.py
src/transformers/models/bert/modeling_tf_bert.py
src/transformers/models/distilbert/modeling_tf_distilbert.py
src/transformers/models/electra/modeling_tf_electra.py
src/transformers/models/funnel/modeling_tf_funnel.py
src/transformers/models/longformer/modeling_tf_longformer.py
src/transformers/models/lxmert/modeling_tf_lxmert.py
src/transformers/models/mobilebert/modeling_tf_mobilebert.py
src/transformers/models/mpnet/modeling_tf_mpnet.py
src/transformers/models/roberta/modeling_tf_roberta.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_tf_{{cookiecutter.lowercase_modelname}}.py
tests/test_modeling_tf_common.py
==================
12f0d7e8e;Julien Plu;2021-01-20 12:08:00 +0100;Fix label datatype in TF Trainer (#9616)
* Fix label datatype

* Apply style
==

src/transformers/trainer_tf.py
==================
76f36e183;Sylvain Gugger;2021-01-20 04:54:36 -0500;Add a community page to the docs (#9682)

==

docs/source/community.md
docs/source/index.rst
notebooks/README.md
==================
582f516ad;Sylvain Gugger;2021-01-20 04:52:13 -0500;Use datasets squad_v2 metric in run_qa (#9677)

==

examples/question-answering/requirements.txt
examples/question-answering/run_qa.py
examples/question-answering/run_qa_beam_search.py
examples/question-answering/squad_v2_local/evaluate.py
examples/question-answering/squad_v2_local/squad_v2_local.py
==================
a98173cc4;LSinev;2021-01-20 12:23:01 +0300;make RepetitionPenaltyLogitsProcessor faster (#9600)

==

src/transformers/generation_logits_process.py
==================
a1ad16a44;Sylvain Gugger;2021-01-20 04:17:39 -0500;Restrain tokenizer.model_max_length default (#9681)
* Restrain tokenizer.model_max_length default

* Fix indent
==

examples/language-modeling/run_mlm.py
==================
7e662e6a3;Sylvain Gugger;2021-01-19 17:11:22 -0500;Fix model templates and use less than 119 chars (#9684)
* Fix model templates and use less than 119 chars

* Missing new line
==

src/transformers/models/bart/modeling_bart.py
src/transformers/models/bert/modeling_bert.py
src/transformers/models/blenderbot/modeling_blenderbot.py
src/transformers/models/blenderbot_small/modeling_blenderbot_small.py
src/transformers/models/electra/modeling_electra.py
src/transformers/models/gpt2/modeling_gpt2.py
src/transformers/models/layoutlm/modeling_layoutlm.py
src/transformers/models/led/modeling_led.py
src/transformers/models/marian/modeling_marian.py
src/transformers/models/mbart/modeling_mbart.py
src/transformers/models/pegasus/modeling_pegasus.py
src/transformers/models/roberta/modeling_roberta.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_{{cookiecutter.lowercase_modelname}}.py
==================
2ebbbf558;Daniel Stancl;2021-01-19 22:50:25 +0100;Add separated decoder_head_mask for T5 Models (#9634)
* Add decoder_head_mask for PyTorch T5 model

* Add decoder_head_mask args into T5Model and T5ForConditionalGeneration

* Slightly change the order of input args to be in accordance
with the convention from BART-based models introduced within the PR #9569.

* Make style for modeling_t5.py

* Add decoder_head_mask for TF T5 models

* Separate head_mask and decoder_head_mask args in TF T5 models

* Slightly change the order of input args to follow convention
of BART-based models updated in PR #9569

* Update test_forward_signature tests/test_modeling_tf_common.py
w.r.t. the changed order of input args

* Add FutureWarnings for T5 and TFT5 models

* Add FutureWarnings for T5 and TFT5 models warning a user that
input argument `head_mask` was split into two arguments -
`head_mask` and `decoder_head_mask`

* Add default behaviour - `decoder_head_mask` is set to copy
`head_mask`

* Fix T5 modeling and FutureWarning

* Make proper usage of head_mask and decoder_head_mask
in cross_attention

* Fix conditions for raising FutureWarning

* Reformat FutureWarning in T5 modeling

* Refactor the warning message
==

src/transformers/models/t5/modeling_t5.py
src/transformers/models/t5/modeling_tf_t5.py
tests/test_modeling_tf_common.py
==================
e4c06ed66;Sylvain Gugger;2021-01-19 15:22:17 -0500;New run_seq2seq script (#9605)
* New run_seq2seq script

* Add tests

* Mark as slow

* Update examples/seq2seq/run_seq2seq.py

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* Update src/transformers/data/data_collator.py

Co-authored-by: Suraj Patil <surajp815@gmail.com>

* Update src/transformers/data/data_collator.py

Co-authored-by: Suraj Patil <surajp815@gmail.com>

* Address review comments

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
Co-authored-by: Suraj Patil <surajp815@gmail.com>
==

examples/seq2seq/run_seq2seq.py
examples/test_examples.py
src/transformers/__init__.py
src/transformers/data/data_collator.py
src/transformers/utils/dummy_pt_objects.py
tests/fixtures/tests_samples/wmt16/sample.json
tests/fixtures/tests_samples/xsum/sample.json
==================
fa876aee2;Julien Plu;2021-01-19 18:02:57 +0100;Fix TF Flaubert and XLM (#9661)
* Fix Flaubert and XLM

* Fix Flaubert and XLM

* Apply style
==

src/transformers/models/flaubert/modeling_tf_flaubert.py
src/transformers/models/xlm/modeling_tf_xlm.py
==================
11ec74905;max yue;2021-01-20 00:39:49 +0800;Update integrations.py (#9652)
File "/share/apps/anaconda3/envs/my_env/lib/python3.7/site-packages/transformers/integrations.py", line 419, in __init__
    self._SummaryWriter = SummaryWriter
UnboundLocalError: local variable 'SummaryWriter' referenced before assignment
==

src/transformers/integrations.py
==================
b020a736c;Yusuke Mori;2021-01-20 00:00:15 +0900;Update `past_key_values` in GPT-2 (#9596)
* Update past_key_values in gpt2 (#9391)

* Update generation_utils, and rename some items

* Update modeling_gpt2 to avoid an error in gradient_checkpointing

* Remove 'reorder_cache' from util and add variations to XLNet, TransfoXL, GPT-2

* Change the location of '_reorder_cache' in modeling files

* Add '_reorder_cache' in modeling_ctrl

* Fix a bug of my last commit in CTRL

* Add '_reorder_cache' to GPT2DoubleHeadsModel

* Manage 'use_cache' in config of test_modeling_gpt2

* Clean up the doc string

* Update src/transformers/models/gpt2/modeling_gpt2.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Fix the doc string (GPT-2, CTRL)

* improve gradient_checkpointing_behavior

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
==

src/transformers/generation_utils.py
src/transformers/models/bart/modeling_bart.py
src/transformers/models/bert/modeling_bert.py
src/transformers/models/blenderbot/modeling_blenderbot.py
src/transformers/models/blenderbot_small/modeling_blenderbot_small.py
src/transformers/models/ctrl/modeling_ctrl.py
src/transformers/models/electra/modeling_electra.py
src/transformers/models/gpt2/modeling_gpt2.py
src/transformers/models/layoutlm/modeling_layoutlm.py
src/transformers/models/led/modeling_led.py
src/transformers/models/longformer/modeling_longformer.py
src/transformers/models/marian/modeling_marian.py
src/transformers/models/mbart/modeling_mbart.py
src/transformers/models/pegasus/modeling_pegasus.py
src/transformers/models/roberta/modeling_roberta.py
src/transformers/models/transfo_xl/modeling_transfo_xl.py
src/transformers/models/xlnet/modeling_xlnet.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_{{cookiecutter.lowercase_modelname}}.py
tests/test_modeling_gpt2.py
==================
97b787fb4;Sylvain Gugger;2021-01-19 09:56:25 -0500;Fix old Seq2SeqTrainer (#9675)

==

examples/seq2seq/seq2seq_trainer.py
==================
d302d88b4;Sylvain Gugger;2021-01-19 09:55:37 -0500;Fix GPT conversion script (#9676)

==

src/transformers/models/openai/convert_openai_original_tf_checkpoint_to_pytorch.py
==================
053efc5d2;Sylvain Gugger;2021-01-19 09:40:15 -0500;Fix imports in conversion scripts (#9674)

==

src/transformers/models/albert/convert_albert_original_tf_checkpoint_to_pytorch.py
src/transformers/models/bart/convert_bart_original_pytorch_checkpoint_to_pytorch.py
src/transformers/models/bert/convert_bert_original_tf2_checkpoint_to_pytorch.py
src/transformers/models/bert/convert_bert_original_tf_checkpoint_to_pytorch.py
src/transformers/models/bert/convert_bert_pytorch_checkpoint_to_original_tf.py
src/transformers/models/blenderbot/convert_blenderbot_original_pytorch_checkpoint_to_pytorch.py
src/transformers/models/dialogpt/convert_dialogpt_original_pytorch_checkpoint_to_pytorch.py
src/transformers/models/dpr/convert_dpr_original_checkpoint_to_pytorch.py
src/transformers/models/electra/convert_electra_original_tf_checkpoint_to_pytorch.py
src/transformers/models/fsmt/convert_fsmt_original_pytorch_checkpoint_to_pytorch.py
src/transformers/models/funnel/__init__.py
src/transformers/models/funnel/convert_funnel_original_tf_checkpoint_to_pytorch.py
src/transformers/models/gpt2/convert_gpt2_original_tf_checkpoint_to_pytorch.py
src/transformers/models/longformer/convert_longformer_original_pytorch_lightning_to_pytorch.py
src/transformers/models/lxmert/convert_lxmert_original_tf_checkpoint_to_pytorch.py
src/transformers/models/marian/convert_marian_tatoeba_to_pytorch.py
src/transformers/models/marian/convert_marian_to_pytorch.py
src/transformers/models/mbart/convert_mbart_original_checkpoint_to_pytorch.py
src/transformers/models/mobilebert/convert_mobilebert_original_tf_checkpoint_to_pytorch.py
src/transformers/models/pegasus/convert_pegasus_tf_to_pytorch.py
src/transformers/models/prophetnet/convert_prophetnet_original_pytorch_checkpoint_to_pytorch.py
src/transformers/models/reformer/convert_reformer_trax_checkpoint_to_pytorch.py
src/transformers/models/roberta/convert_roberta_original_pytorch_checkpoint_to_pytorch.py
src/transformers/models/t5/convert_t5_original_tf_checkpoint_to_pytorch.py
src/transformers/models/tapas/convert_tapas_original_tf_checkpoint_to_pytorch.py
src/transformers/models/transfo_xl/convert_transfo_xl_original_tf_checkpoint_to_pytorch.py
src/transformers/models/xlm/convert_xlm_original_pytorch_checkpoint_to_pytorch.py
src/transformers/models/xlnet/convert_xlnet_original_tf_checkpoint_to_pytorch.py
==================
2390c16fd;Patrick von Platen;2021-01-19 15:19:11 +0100;add mbart to automodel for masked lm (#9673)

==

src/transformers/models/auto/modeling_auto.py
==================
b39bd763e;Patrick von Platen;2021-01-19 12:25:51 +0100;Update README.md

==

notebooks/README.md
==================
917dbb15e;Sergey Mkrtchyan;2021-01-19 05:43:11 -0500;Fix DPRReaderTokenizer's attention_mask (#9663)
* Fix the attention_mask in DPRReaderTokenizer

* Add an integration test for DPRReader inference

* Run make style
==

src/transformers/models/dpr/tokenization_dpr.py
src/transformers/models/dpr/tokenization_dpr_fast.py
tests/test_modeling_dpr.py
==================
12c1b5b8f;Patrick von Platen;2021-01-19 09:06:24 +0100;fix test (#9669)

==

tests/test_modeling_bart.py
tests/test_modeling_blenderbot.py
tests/test_modeling_blenderbot_small.py
tests/test_modeling_common.py
tests/test_modeling_marian.py
tests/test_modeling_mbart.py
tests/test_modeling_pegasus.py
==================
357fb1c5d;Daniel Stancl;2021-01-18 13:35:22 +0100;Add head_mask/decoder_head_mask for BART (#9569)
* Add head_mask/decoder_head_mask for BART

This branch implement head_mask and decoder_head_mask
for BART-based models. Full list below:
- BART
- MBart
- Blenderbot
- BlenderbotSmall
- Marian
- Pegasus

Everything is accompanied with updated testing.

* Fix test_headmasking for BART models

* Fix text_headmasking for BART-like models
which has only 2 layers in each modules.
The condition
```
self.assertNotEqual(attentions[1][..., 0, :, :].flatten().sum().item(), 0.0)
```
is, therefore, invalid for encoder-decoder models considering
the `head_mask`
```
head_mask = torch.ones(
    self.model_tester.num_hidden_layers,
    self.model_tester.num_attention_heads,
    device=torch_device,
)
head_mask[0, 0] = 0
head_mask[-1, :-1] = 0
```
specified in the `test_headmasking` test/function.

* Adjust test_modeling_common.py to reflect T5 input args

* Update tests/test_modeling_common.py

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* make style

* make fix-copies

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

src/transformers/models/bart/modeling_bart.py
src/transformers/models/blenderbot/modeling_blenderbot.py
src/transformers/models/blenderbot_small/modeling_blenderbot_small.py
src/transformers/models/marian/modeling_marian.py
src/transformers/models/mbart/modeling_mbart.py
src/transformers/models/pegasus/modeling_pegasus.py
tests/test_modeling_bart.py
tests/test_modeling_blenderbot.py
tests/test_modeling_blenderbot_small.py
tests/test_modeling_common.py
tests/test_modeling_marian.py
tests/test_modeling_mbart.py
tests/test_modeling_pegasus.py
==================
65eb5d9ac;Devrim;2021-01-18 12:33:39 +0300;Fix: torch.utils.checkpoint import error. (#9626)

==

src/transformers/models/bart/modeling_bart.py
src/transformers/models/blenderbot/modeling_blenderbot.py
src/transformers/models/blenderbot_small/modeling_blenderbot_small.py
src/transformers/models/electra/modeling_electra.py
src/transformers/models/gpt2/modeling_gpt2.py
src/transformers/models/layoutlm/modeling_layoutlm.py
src/transformers/models/led/modeling_led.py
src/transformers/models/longformer/modeling_longformer.py
src/transformers/models/marian/modeling_marian.py
src/transformers/models/mbart/modeling_mbart.py
src/transformers/models/pegasus/modeling_pegasus.py
src/transformers/models/roberta/modeling_roberta.py
src/transformers/models/tapas/modeling_tapas.py
==================
72fc9abf1;Anthony MOI;2021-01-18 04:24:21 -0500;Remove duplicated extra["retrieval"] (#9621)

==

setup.py
==================
c60e0e1ee;Stas Bekman;2021-01-15 10:12:26 -0800;deepspeed + grad acumm (#9622)

==

examples/seq2seq/test_finetune_trainer.py
src/transformers/trainer.py
==================
6d3b688b0;Lysandre Debut;2021-01-15 15:40:21 +0100;Ignore lm_head decoder bias warning (#9615)
* Ignore lm_head decoder bias warning

* Revert "Ignore lm_head decoder bias warning"

This reverts commit f25177a9da6ca898e351f46c8b1515971de5c670.

* predictions -> lm_head
==

src/transformers/models/roberta/modeling_roberta.py
==================
8eba1f8ca;Julien Plu;2021-01-15 14:06:29 +0100;Remove unused token_type_ids in MPNet (#9564)
* Add warning

* Remove unused import

* Fix missing call

* Fix missing call

* Completely remove token_type_ids

* Apply style

* Remove unused import

* Update src/transformers/models/mpnet/modeling_tf_mpnet.py

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
==

src/transformers/models/mpnet/modeling_mpnet.py
src/transformers/models/mpnet/modeling_tf_mpnet.py
src/transformers/models/mpnet/tokenization_mpnet.py
src/transformers/models/mpnet/tokenization_mpnet_fast.py
==================
90ca8d36e;Patrick von Platen;2021-01-15 12:40:27 +0100;[TF Led] Fix wrong decoder attention mask behavior (#9601)
* fix tf led

* remove loop file
==

src/transformers/models/led/modeling_tf_led.py
==================
85788bae5;Kiyoung Kim;2021-01-15 00:16:39 +0900;Revert "Gradient accumulation for TFTrainer (#9585)"
This reverts commit 3f40070c88de07169fe18b0b4c4003ef2858a284.

==

src/transformers/trainer_tf.py
==================
82498cbc3;Stas Bekman;2021-01-14 11:05:04 -0800;[deepspeed doc] install issues + 1-gpu deployment (#9582)
* [doc] install + 1-gpu deployment

* Apply suggestions from code review

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* improvements

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

docs/source/main_classes/trainer.rst
==================
329fe2746;Sylvain Gugger;2021-01-14 10:38:14 -0500;Upstream (and rename) sortish sampler (#9574)
* Upstream (and rename) sortish sampler

* Use proper sampler

* Update src/transformers/trainer_pt_utils.py

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
==

examples/seq2seq/test_finetune_trainer.py
src/transformers/trainer.py
src/transformers/trainer_pt_utils.py
src/transformers/training_args.py
tests/test_trainer_utils.py
==================
3f40070c8;Kiyoung Kim;2021-01-15 00:16:39 +0900;Gradient accumulation for TFTrainer (#9585)
* gradient accumulation for tftrainer

* label naming

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* label naming

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

src/transformers/trainer_tf.py
==================
e43f3b619;Lysandre;2021-01-14 14:25:30 +0100;v4.2.1 in docs

==

docs/source/_static/js/custom.js
==================
280db79ac;Lysandre Debut;2021-01-14 07:57:58 -0500;BatchEncoding.to with device with tests (#9584)

==

src/transformers/tokenization_utils_base.py
tests/test_tokenization_common.py
==================
8bf27075a;Lysandre Debut;2021-01-14 05:51:52 -0500;Fix conda build (#9589)
* conda build -> conda-build

* Syntax error

* conda build -> conda-build + 4.2.0

* Prepare to merge in `master`
==

.github/workflows/release-conda.yml
==================
c99751dd9;Stas Bekman;2021-01-14 02:04:08 -0800;[setup.py] note on how to get to transformers exact dependencies from shell (#9553)
* note on how to get to deps from shell

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* fix text

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

setup.py
==================
a26536f0c;Julien Plu;2021-01-14 10:56:53 +0100;Make logs tf compliant (#9565)

==

src/transformers/modeling_tf_utils.py
==================
14d677ca4;Julien Plu;2021-01-14 10:35:35 +0100;Compliancy with tf-nightly (#9570)
* Compliancy with tf-nightly

* Add more version + restore min version check
==

src/transformers/file_utils.py
==================
46ed56cfd;Sylvain Gugger;2021-01-14 03:37:07 -0500;Switch metrics in run_ner to datasets (#9567)
* Switch metrics in run_ner to datasets

* Add flag to return all metrics

* Upstream (and rename) sortish_sampler

* Revert "Upstream (and rename) sortish_sampler"

This reverts commit e07d0dcf650c2bae36da011dd76c77a8bb4feb0d.
==

examples/test_examples.py
examples/token-classification/run_ner.py
==================
5e1bea4f1;Sylvain Gugger;2021-01-14 03:23:41 -0500;Fix Trainer with a parallel model (#9578)
* Fix Trainer with a parallel model

* More clean up
==

src/transformers/training_args.py
tests/test_trainer.py
==================
126fd281b;Patrick von Platen;2021-01-13 16:55:59 +0100;Update README.md

==

notebooks/README.md
==================
e63cad793;Lysandre;2021-01-13 16:16:54 +0100;v4.3.0.dev0

==

setup.py
src/transformers/__init__.py
==================
33a8497db;Lysandre;2021-01-13 16:15:40 +0100;v4.2.0 documentation

==

.circleci/deploy.sh
docs/source/_static/js/custom.js
==================
7d9a9d0c7;Lysandre;2021-01-13 16:01:47 +0100;Release: v4.2.0

==

README.md
docs/source/conf.py
docs/source/index.rst
setup.py
src/transformers/__init__.py
==================
c94951669;Lysandre Debut;2021-01-13 09:55:48 -0500;Fix slow tests v4.2.0 (#9561)
* Fix conversational pipeline test

* LayoutLM

* ProphetNet

* BART

* Blenderbot & small

* Marian

* mBART

* Pegasus

* Tapas tokenizer

* BERT2BERT test

* Style

* Example requirements

* TF BERT2BERT test
==

.github/workflows/self-scheduled.yml
tests/test_modeling_encoder_decoder.py
tests/test_modeling_layoutlm.py
tests/test_modeling_tf_bart.py
tests/test_modeling_tf_blenderbot.py
tests/test_modeling_tf_blenderbot_small.py
tests/test_modeling_tf_marian.py
tests/test_modeling_tf_mbart.py
tests/test_modeling_tf_pegasus.py
tests/test_pipelines_conversational.py
tests/test_tokenization_tapas.py
tests/test_trainer_seq2seq.py
==================
04dc65e5c;Sylvain Gugger;2021-01-13 09:54:41 -0500;Fix data parallelism in Trainer (#9566)
* Fix data parallelism in Trainer

* Update src/transformers/training_args.py

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
==

src/transformers/training_args.py
tests/test_trainer.py
==================
b2dfcc567;Stas Bekman;2021-01-13 05:02:53 -0800;use correct deps for torchhub (#9552)

==

.github/workflows/github-torch-hub.yml
setup.py
==================
eabad8fd9;Yusuke Mori;2021-01-13 21:48:35 +0900;Update run_glue for do_predict with local test data (#9442) (#9486)
* Update run_glue for do_predict with local test data (#9442)

* Update run_glue (#9442): fix comments ('files' to 'a file')

* Update run_glue (#9442): reflect the code review

* Update run_glue (#9442): auto format

* Update run_glue (#9442): reflect the code review
==

examples/text-classification/run_glue.py
==================
0c9f01a8e;LSinev;2021-01-13 15:47:47 +0300;Speed up TopKLogitsWarper and TopPLogitsWarper (pytorch) (#9557)
* make TopKLogitsWarper faster

* make TopPLogitsWarper faster
==

src/transformers/generation_logits_process.py
==================
27d0e01d7;Pavel Tarashkevich;2021-01-13 15:46:48 +0300;Fix classification script: enable dynamic padding with truncation (#9554)
Co-authored-by: Pavel Tarashkevich <Pavel.Tarashkievich@orange.com>
==

examples/text-classification/run_glue.py
==================
245cdb469;Lysandre Debut;2021-01-13 06:24:10 -0500;Fix barthez tokenizer (#9562)

==

src/transformers/models/auto/tokenization_auto.py
==================
247a7b202;Julien Chaumond;2021-01-13 11:58:05 +0100;Doc: Update pretrained_models wording (#9545)
* Update pretrained_models.rst

To clarify things cf. this tweet for instance https://twitter.com/RTomMcCoy/status/1349094111505211395

* format
==

docs/source/pretrained_models.rst
==================
69ed36063;Suraj Patil;2021-01-13 10:53:43 +0530;fix BlenderbotSmallTokenizer (#9538)
* add model_input_names

* fix test
==

src/transformers/models/blenderbot_small/tokenization_blenderbot_small.py
tests/test_modeling_blenderbot_small.py
==================
2df34f4ab;Stas Bekman;2021-01-12 19:05:18 -0800;[trainer] deepspeed integration (#9211)
* deepspeed integration

* style

* add test

* ds wants to do its own backward

* fp16 assert

* Update src/transformers/training_args.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* style

* for clarity extract what args are being passed to deepspeed

* introduce the concept of self.wrapped_model

* s/self.wrapped_model/self.model_wrapped/

* complete transition to self.wrapped_model / self.model

* fix

* doc

* give ds its own init

* add custom overrides, handle bs correctly

* fix test

* clean up model_init logic, fix small bug

* complete fix

* collapse --deepspeed_config into --deepspeed

* style

* start adding doc notes

* style

* implement hf2ds optimizer and scheduler configuration remapping

* oops

* call get_num_training_steps absolutely when needed

* workaround broken auto-formatter

* deepspeed_config arg is no longer needed - fixed in deepspeed master

* use hf's fp16 args in config

* clean

* start on the docs

* rebase cleanup

* finish up --fp16

* clarify the supported stages

* big refactor thanks to discovering deepspeed.init_distributed

* cleanup

* revert fp16 part

* add checkpoint-support

* more init ds into integrations

* extend docs

* cleanup

* unfix docs

* clean up old code

* imports

* move docs

* fix logic

* make it clear which file it's referring to

* document nodes/gpus

* style

* wrong format

* style

* deepspeed handles gradient clipping

* easier to read

* major doc rewrite

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* docs

* switch to AdamW optimizer

* style

* Apply suggestions from code review

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* clarify doc

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
==

docs/source/main_classes/trainer.rst
docs/source/training.rst
examples/seq2seq/ds_config.json
examples/seq2seq/test_finetune_trainer.py
src/transformers/integrations.py
src/transformers/trainer.py
src/transformers/training_args.py
==================
5f6721032;Sylvain Gugger;2021-01-12 18:55:45 -0500;Use the right version of tokenizers (#9550)
* Use the right version of tokenizers

* Try another way

* Try another way

* Deps are installed from there...

* Deps are installed from there...

* Revert last

* remove needless comment
==

.github/workflows/github-torch-hub.yml
==================
063d8d27f;Sylvain Gugger;2021-01-12 18:19:38 -0500;Refactor `prepare_seq2seq_batch` (#9524)
* Add target contextmanager and rework prepare_seq2seq_batch

* Fix tests, treat BART and Barthez

* Add last tokenizers

* Fix test

* Set src token before calling the superclass

* Remove special behavior for T5

* Remove needless imports

* Remove needless asserts
==

src/transformers/models/bart/tokenization_bart.py
src/transformers/models/bart/tokenization_bart_fast.py
src/transformers/models/barthez/tokenization_barthez.py
src/transformers/models/barthez/tokenization_barthez_fast.py
src/transformers/models/fsmt/tokenization_fsmt.py
src/transformers/models/marian/tokenization_marian.py
src/transformers/models/mbart/tokenization_mbart.py
src/transformers/models/mbart/tokenization_mbart_fast.py
src/transformers/models/pegasus/tokenization_pegasus.py
src/transformers/models/pegasus/tokenization_pegasus_fast.py
src/transformers/models/prophetnet/tokenization_prophetnet.py
src/transformers/models/rag/tokenization_rag.py
src/transformers/models/t5/tokenization_t5.py
src/transformers/models/t5/tokenization_t5_fast.py
src/transformers/tokenization_utils.py
src/transformers/tokenization_utils_base.py
tests/test_modeling_marian.py
tests/test_tokenization_common.py
tests/test_tokenization_ctrl.py
tests/test_tokenization_gpt2.py
tests/test_tokenization_openai.py
tests/test_tokenization_reformer.py
tests/test_tokenization_tapas.py
tests/test_tokenization_transfo_xl.py
==================
e6ecef711;Sylvain Gugger;2021-01-12 18:00:22 -0500;Revert, it was not the issue.

==

setup.py
==================
250f27f20;Sylvain Gugger;2021-01-12 17:50:27 -0500;Fix tokenizers install for now

==

setup.py
==================
dfbf0f559;Lysandre Debut;2021-01-12 16:21:29 -0500;topk -> top_k (#9541)

==

tests/test_pipelines_fill_mask.py
==================
a1100fac6;Lysandre Debut;2021-01-12 10:03:50 -0500;LayoutLM Config (#9539)

==

src/transformers/models/auto/modeling_auto.py
tests/test_modeling_auto.py
==================
e45eba3b1;NielsRogge;2021-01-12 15:26:32 +0100;Improve LayoutLM (#9476)
* Add LayoutLMForSequenceClassification and integration tests

Improve docs

Add LayoutLM notebook to list of community notebooks

* Make style & quality

* Address comments by @sgugger, @patrickvonplaten and @LysandreJik

* Fix rebase with master

* Reformat in one line

* Improve code examples as requested by @patrickvonplaten

Co-authored-by: Lysandre <lysandre.debut@reseau.eseo.fr>
Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
==

docs/source/model_doc/layoutlm.rst
notebooks/README.md
src/transformers/__init__.py
src/transformers/models/auto/modeling_auto.py
src/transformers/models/layoutlm/__init__.py
src/transformers/models/layoutlm/modeling_layoutlm.py
src/transformers/utils/dummy_pt_objects.py
tests/test_modeling_layoutlm.py
==================
ccd1923f4;Suraj Patil;2021-01-12 17:12:33 +0530;[T5] enable T5 fp16 (#9487)
* fix t5 fp16
==

src/transformers/models/t5/modeling_t5.py
==================
2aa9c2f20;Patrick von Platen;2021-01-12 11:53:32 +0100;fix blenderbot tok (#9532)

==

src/transformers/models/blenderbot/tokenization_blenderbot.py
src/transformers/models/blenderbot_small/tokenization_blenderbot_small.py
==================
406cbf58b;Lysandre Debut;2021-01-12 04:49:15 -0500;Shouldn't stale issues/PRs with feature request label (#9511)

==

.github/stale.yml
==================
3b67c5abb;Simon Brandeis;2021-01-12 10:15:16 +0100;Update 'Develop on Windows' guidelines (#9519)

==

CONTRIBUTING.md
==================
a051d8928;Patrick von Platen;2021-01-12 10:10:05 +0100;[ProphetNet] Fix naming and wrong config (#9514)
* fix naming issues

* better names
==

src/transformers/models/prophetnet/modeling_prophetnet.py
==================
7f2861321;Patrick von Platen;2021-01-12 02:06:32 +0100;[TFBart] Split TF-Bart (#9497)
* make templates ready

* make add_new_model_command_ready

* finish tf bart

* prepare tf mbart

* finish tf bart

* add tf mbart

* add marian

* prep pegasus

* add tf pegasus

* push blenderbot tf

* add blenderbot

* add blenderbot small

* clean-up

* make fix copy

* define blend bot tok

* fix

* up

* make style

* add to docs

* add copy statements

* overwrite changes

* improve

* fix docs

* finish

* fix last slow test

* fix missing git conflict line

* fix blenderbot

* up

* fix blenderbot small

* load changes

* finish copied from

* upload fix
==

docs/source/index.rst
docs/source/model_doc/blenderbot.rst
docs/source/model_doc/blenderbot_small.rst
docs/source/model_doc/marian.rst
docs/source/model_doc/mbart.rst
docs/source/model_doc/pegasus.rst
src/transformers/__init__.py
src/transformers/models/auto/modeling_tf_auto.py
src/transformers/models/auto/tokenization_auto.py
src/transformers/models/bart/configuration_bart.py
src/transformers/models/bart/modeling_tf_bart.py
src/transformers/models/blenderbot/__init__.py
src/transformers/models/blenderbot/configuration_blenderbot.py
src/transformers/models/blenderbot/modeling_tf_blenderbot.py
src/transformers/models/blenderbot_small/__init__.py
src/transformers/models/blenderbot_small/modeling_blenderbot_small.py
src/transformers/models/blenderbot_small/modeling_tf_blenderbot_small.py
src/transformers/models/blenderbot_small/tokenization_blenderbot_small.py
src/transformers/models/led/modeling_tf_led.py
src/transformers/models/marian/__init__.py
src/transformers/models/marian/configuration_marian.py
src/transformers/models/marian/modeling_tf_marian.py
src/transformers/models/mbart/__init__.py
src/transformers/models/mbart/configuration_mbart.py
src/transformers/models/mbart/modeling_tf_mbart.py
src/transformers/models/pegasus/__init__.py
src/transformers/models/pegasus/configuration_pegasus.py
src/transformers/models/pegasus/modeling_tf_pegasus.py
src/transformers/utils/dummy_tf_objects.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_tf_{{cookiecutter.lowercase_modelname}}.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_{{cookiecutter.lowercase_modelname}}.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/test_modeling_tf_{{cookiecutter.lowercase_modelname}}.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/test_modeling_{{cookiecutter.lowercase_modelname}}.py
tests/test_modeling_tf_bart.py
tests/test_modeling_tf_blenderbot.py
tests/test_modeling_tf_blenderbot_small.py
tests/test_modeling_tf_marian.py
tests/test_modeling_tf_mbart.py
tests/test_modeling_tf_pegasus.py
==================
0ecbb6980;Stas Bekman;2021-01-11 13:00:08 -0800;[make docs] parallel build (#9522)
After experimenting with different number of workers https://github.com/huggingface/transformers/issues/9496#issuecomment-758145868 4-5 workers seems to be the most optimal - let's go with 4 as surely we wouldn't find a cpu with less cores these days.

Fixes part of https://github.com/huggingface/transformers/issues/9496

@sgugger
==

Makefile
==================
e6f211cad;Stas Bekman;2021-01-11 10:17:49 -0800;[trainer] round numbers in trainer state (#9491)
* round numbers

* style

* round only on logging
==

src/transformers/trainer.py
==================
01a168407;Sylvain Gugger;2021-01-11 10:25:24 -0500;Make doc styler behave properly on Windows (#9516)

==

utils/style_doc.py
==================
6009668c6;Sylvain Gugger;2021-01-11 10:00:59 -0500;Add link to forums thread

==

ISSUES.md
==================
ba702966b;Julien Plu;2021-01-11 15:42:19 +0100;Fix cardinality (#9505)

==

src/transformers/trainer_tf.py
==================
33b742283;Stas Bekman;2021-01-11 06:39:28 -0800;[trainer] remove `--model_parallel` (#9451)
* fix bad merge - dropped code

* remove --model_parallel

* Deal with TrainingArguments

* Use a private attr and fix batch sizes

* fix _n_gpu

* add is_parallel helper wrapper

* fix attribute

* introduce a new attribute is_model_parallel

* docs

* docs

* Put back init False and rearrange doc

* Ignore non-init args in HFArgumentParser

Co-authored-by: Sylvain Gugger <sylvain.gugger@gmail.com>
==

src/transformers/hf_argparser.py
src/transformers/trainer.py
src/transformers/training_args.py
==================
6f6350138;Stas Bekman;2021-01-11 06:23:51 -0800;[doc] How To Request Support document stab (#9288)
* How To Request Support document stab

* integrate suggestions

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* small corrections

* expand on how to search for issues with examples

* address issues

* Update ISSUES.md

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* patrick's suggestion

* patrick's suggestion

* small fix

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
==

ISSUES.md
==================
d20e9c729;Nicolas Patry;2021-01-11 15:23:28 +0100;Enable TruncationStrategy override for pipelines (#9432)
* Enable TruncationStrategy override for pipelines

* Update isort.

* Fixing test

* Fixing text_generation pipeline.

* Using same DummyTok as other PR  for easier merge later.

* Some more import guards.

* Remove bogus file.

* Do not pass `generate_kwargs` to `_parse_and_tokenize`.
@patrickvonplaten

* Removed DummyTok.

* Doc quality.
==

src/transformers/pipelines/base.py
src/transformers/pipelines/conversational.py
src/transformers/pipelines/text2text_generation.py
src/transformers/pipelines/text_generation.py
src/transformers/pipelines/zero_shot_classification.py
tests/test_pipelines_summarization.py
==================
8d25df2c7;Sylvain Gugger;2021-01-11 08:53:41 -0500;Make doc styler detect lists on rst (#9488)

==

docs/source/benchmarks.rst
utils/style_doc.py
==================
5a442a8db;Aakash Tripathi;2021-01-11 19:04:39 +0530;New Updated DistilGPT-2 Finetuning and Generation (#9494)
https://github.com/huggingface/transformers/pull/3177
==

notebooks/README.md
==================
6c8ec2a93;Patrick von Platen;2021-01-11 14:14:48 +0100;fix tf led pt test (#9513)

==

tests/test_modeling_tf_led.py
==================
1e3c36223;Julien Plu;2021-01-11 14:03:28 +0100;Fix template (#9512)

==

templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_tf_{{cookiecutter.lowercase_modelname}}.py
==================
d415882b4;Lysandre Debut;2021-01-11 08:02:41 -0500;Remove tolerance + drop_rows_to_fit by default (#9507)
* Remove tolerance + drop_rows_to_fit by default

* remove drop_rows_to_fit
==

src/transformers/models/tapas/tokenization_tapas.py
tests/test_modeling_tapas.py
tests/test_tokenization_tapas.py
==================
1243ee7d0;Julien Plu;2021-01-11 12:27:28 +0100;Full rework of the TF input/output embeddings and bias resizing (#9193)
* Start rework resizing

* Rework bias/decoder resizing

* Full resizing rework

* Full resizing rework

* Start to update the models with the new approach

* Finish to update the models

* Update all the tests

* Update the template

* Fix tests

* Fix tests

* Test a new approach

* Refactoring

* Refactoring

* Refactoring

* New rework

* Rework BART

* Rework bert+blenderbot

* Rework CTRL

* Rework Distilbert

* Rework DPR

* Rework Electra

* Rework Flaubert

* Rework Funnel

* Rework GPT2

* Rework Longformer

* Rework Lxmert

* Rework marian+mbart

* Rework mobilebert

* Rework mpnet

* Rework openai

* Rework pegasus

* Rework Roberta

* Rework T5

* Rework xlm+xlnet

* Rework template

* Fix TFT5EncoderOnly + DPRs

* Restore previous methods

* Fix Funnel

* Fix CTRL and TransforXL

* Apply style

* Apply Sylvain's comments

* Restore a test in DPR

* Address the comments

* Fix bug

* Apply style

* remove unused import

* Fix test

* Forgot a method

* missing test

* Trigger CI

* naming update

* Rebase

* Trigger CI
==

src/transformers/modeling_tf_utils.py
src/transformers/models/albert/modeling_tf_albert.py
src/transformers/models/bart/modeling_tf_bart.py
src/transformers/models/bert/modeling_tf_bert.py
src/transformers/models/ctrl/modeling_tf_ctrl.py
src/transformers/models/distilbert/modeling_tf_distilbert.py
src/transformers/models/dpr/modeling_tf_dpr.py
src/transformers/models/electra/modeling_tf_electra.py
src/transformers/models/flaubert/modeling_tf_flaubert.py
src/transformers/models/funnel/modeling_tf_funnel.py
src/transformers/models/gpt2/modeling_tf_gpt2.py
src/transformers/models/led/modeling_tf_led.py
src/transformers/models/longformer/modeling_tf_longformer.py
src/transformers/models/lxmert/modeling_tf_lxmert.py
src/transformers/models/mobilebert/modeling_tf_mobilebert.py
src/transformers/models/mpnet/modeling_tf_mpnet.py
src/transformers/models/openai/modeling_tf_openai.py
src/transformers/models/roberta/modeling_tf_roberta.py
src/transformers/models/t5/modeling_tf_t5.py
src/transformers/models/transfo_xl/modeling_tf_transfo_xl.py
src/transformers/models/xlm/modeling_tf_xlm.py
src/transformers/models/xlnet/modeling_tf_xlnet.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_tf_{{cookiecutter.lowercase_modelname}}.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/test_modeling_tf_{{cookiecutter.lowercase_modelname}}.py
tests/test_modeling_tf_albert.py
tests/test_modeling_tf_bart.py
tests/test_modeling_tf_bert.py
tests/test_modeling_tf_blenderbot.py
tests/test_modeling_tf_common.py
tests/test_modeling_tf_ctrl.py
tests/test_modeling_tf_gpt2.py
tests/test_modeling_tf_led.py
tests/test_modeling_tf_lxmert.py
tests/test_modeling_tf_marian.py
tests/test_modeling_tf_mbart.py
tests/test_modeling_tf_mobilebert.py
tests/test_modeling_tf_openai.py
tests/test_modeling_tf_pegasus.py
tests/test_modeling_tf_t5.py
tests/test_modeling_tf_transfo_xl.py
==================
cf416764f;Julien Plu;2021-01-11 11:21:25 +0100;Fix template (#9504)

==

templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_tf_{{cookiecutter.lowercase_modelname}}.py
==================
09926c8e8;Richard Liaw;2021-01-10 17:34:17 -0800;fix-template (#9499)
Signed-off-by: Richard Liaw <rliaw@berkeley.edu>
==

.github/ISSUE_TEMPLATE/bug-report.md
==================
4f7022d68;Julien Plu;2021-01-10 15:10:15 +0100;Reformat (#9482)

==

src/transformers/models/albert/modeling_tf_albert.py
src/transformers/models/bert/modeling_tf_bert.py
src/transformers/models/ctrl/modeling_tf_ctrl.py
src/transformers/models/distilbert/modeling_tf_distilbert.py
src/transformers/models/dpr/modeling_tf_dpr.py
src/transformers/models/electra/modeling_tf_electra.py
src/transformers/models/flaubert/modeling_tf_flaubert.py
src/transformers/models/funnel/modeling_tf_funnel.py
src/transformers/models/gpt2/modeling_tf_gpt2.py
src/transformers/models/longformer/modeling_tf_longformer.py
src/transformers/models/mobilebert/modeling_tf_mobilebert.py
src/transformers/models/mpnet/modeling_tf_mpnet.py
src/transformers/models/openai/modeling_tf_openai.py
src/transformers/models/roberta/modeling_tf_roberta.py
src/transformers/models/t5/modeling_tf_t5.py
src/transformers/models/transfo_xl/modeling_tf_transfo_xl.py
src/transformers/models/xlm/modeling_tf_xlm.py
src/transformers/models/xlnet/modeling_tf_xlnet.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_tf_{{cookiecutter.lowercase_modelname}}.py
==================
96f1f74aa;Nicolas Patry;2021-01-10 15:08:20 +0100;Fixing tests. It seems master changed something in the warnings. (#9483)
Trying to keep warning tests for now. Should be discarded if it becomes
too hard to maintain.
==

tests/test_pipelines_conversational.py
==================
1c19b423b;Boris Dayma;2021-01-08 20:32:02 +0100;fix(wandb): fix config (#9489)

==

src/transformers/integrations.py
==================
02e05fb0a;Nicolas Patry;2021-01-08 14:33:25 +0100;Making Conversation possible to create directly a full conversation (#9434)
* Cleaning up conversation tests.

* Adding tests that don't require downloading models + conversation can be

fully created from static state.

* Making tests non flaky (by fixing generation length)

* Bumping isort version.

* Doc cleanup.

* Remove unused test in this PR.

* Torch import guard for TF.

* Missing torch guard.

* Small mistake in doc.

* Actual uses `_history` and `_index` cache.

+ remove dead enumerate
+ improve warning message.

* Update src/transformers/pipelines/conversational.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/pipelines/conversational.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/pipelines/conversational.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Adding comments and cleaner code to address history copy.

* Improving pipeline name in tests.

* Change tokenizer to a real one (still created at runtime with no

external dependency)

* Simplify DummyTok, reverse changes on tokenization.

* Removing DummyTok.

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

src/transformers/pipelines/conversational.py
tests/test_pipelines_conversational.py
==================
4fbcf8ea4;Julien Plu;2021-01-08 14:23:29 +0100;Fix TF input for np.ndarray (#9294)
* Fix input for np.ndarray"

* add a test

* add a test

* Add a test

* Apply style

* Fix test
==

src/transformers/modeling_tf_utils.py
tests/test_modeling_tf_common.py
==================
e34e45536;Thomas Tanon;2021-01-08 14:10:44 +0100;Makes HfArgumentParser compatible with Python 3.9 (#9479)
Python 3.9 changed the format of the string serialization of `typing.Optional`.
For example, `str(typing.Optional[str])` is
`typing.Union[str, NoneType]` in python 3.8 and
`typing.Optional[str]` in Python 3.9.
==

src/transformers/hf_argparser.py
==================
1bdf42409;Sylvain Gugger;2021-01-08 07:40:59 -0500;Fast imports part 3 (#9474)
* New intermediate inits

* Update template

* Avoid importing torch/tf/flax in tokenization unless necessary

* Styling

* Shutup flake8

* Better python version check
==

src/transformers/file_utils.py
src/transformers/models/albert/__init__.py
src/transformers/models/auto/__init__.py
src/transformers/models/bart/__init__.py
src/transformers/models/barthez/__init__.py
src/transformers/models/bert/__init__.py
src/transformers/models/bert_generation/__init__.py
src/transformers/models/bert_japanese/__init__.py
src/transformers/models/bertweet/__init__.py
src/transformers/models/blenderbot/__init__.py
src/transformers/models/blenderbot_small/__init__.py
src/transformers/models/camembert/__init__.py
src/transformers/models/ctrl/__init__.py
src/transformers/models/deberta/__init__.py
src/transformers/models/distilbert/__init__.py
src/transformers/models/dpr/__init__.py
src/transformers/models/electra/__init__.py
src/transformers/models/encoder_decoder/__init__.py
src/transformers/models/flaubert/__init__.py
src/transformers/models/fsmt/__init__.py
src/transformers/models/funnel/__init__.py
src/transformers/models/gpt2/__init__.py
src/transformers/models/herbert/__init__.py
src/transformers/models/layoutlm/__init__.py
src/transformers/models/led/__init__.py
src/transformers/models/longformer/__init__.py
src/transformers/models/lxmert/__init__.py
src/transformers/models/marian/__init__.py
src/transformers/models/mbart/__init__.py
src/transformers/models/mmbt/__init__.py
src/transformers/models/mobilebert/__init__.py
src/transformers/models/mpnet/__init__.py
src/transformers/models/mt5/__init__.py
src/transformers/models/openai/__init__.py
src/transformers/models/pegasus/__init__.py
src/transformers/models/phobert/__init__.py
src/transformers/models/prophetnet/__init__.py
src/transformers/models/rag/__init__.py
src/transformers/models/reformer/__init__.py
src/transformers/models/retribert/__init__.py
src/transformers/models/roberta/__init__.py
src/transformers/models/squeezebert/__init__.py
src/transformers/models/t5/__init__.py
src/transformers/models/tapas/__init__.py
src/transformers/models/transfo_xl/__init__.py
src/transformers/models/xlm/__init__.py
src/transformers/models/xlm_roberta/__init__.py
src/transformers/models/xlnet/__init__.py
src/transformers/tokenization_utils_base.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/__init__.py
==================
79bbcc526;Patrick von Platen;2021-01-08 11:50:39 +0100;[Generation] Fix bug for manual decoder_input_ids + warning message (#9472)
* up

* improve style
==

src/transformers/generation_utils.py
==================
9e1ea846b;Patrick von Platen;2021-01-08 11:49:43 +0100;[README] Add new models (#9465)
* add new models

* make fix-copies
==

README.md
docs/source/index.rst
==================
bf9056442;Nicolas Patry;2021-01-07 23:10:16 +0100;Removing duplicated code for Translation,Summarization and Text2TextGeneration pipelines (#9433)
* Merging all duplicated codes for Text2TextPipeline while preserving
backward compat.

* Fixing TranslationPipeline Hierarchy + return_name

* torch import guard.

* Update isort version.

* Remove code from other PR disentanglement.

* Removed named example to something more agnostic.
==

src/transformers/pipelines/text2text_generation.py
==================
f33a6f344;Patrick von Platen;2021-01-07 16:12:08 +0100;[TFGPT2] - Fix flaky past_key_values test (#9460)
* fix tf flakey

* remove test files
==

tests/test_modeling_tf_gpt2.py
==================
758ed3332;Sylvain Gugger;2021-01-07 09:36:14 -0500;Transformers fast import part 2 (#9446)
* Main init work

* Add version

* Change from absolute to relative imports

* Fix imports

* One more typo

* More typos

* Styling

* Make quality script pass

* Add necessary replace in template

* Fix typos

* Spaces are ignored in replace for some reason

* Forgot one models.

* Fixes for import

Co-authored-by: LysandreJik <lysandre.debut@reseau.eseo.fr>

* Add documentation

* Styling

Co-authored-by: LysandreJik <lysandre.debut@reseau.eseo.fr>
==

src/transformers/__init__.py
src/transformers/benchmark/benchmark_utils.py
src/transformers/commands/add_new_model.py
src/transformers/commands/convert.py
src/transformers/commands/download.py
src/transformers/commands/env.py
src/transformers/commands/lfs.py
src/transformers/commands/run.py
src/transformers/commands/serving.py
src/transformers/commands/train.py
src/transformers/commands/transformers_cli.py
src/transformers/commands/user.py
src/transformers/convert_graph_to_onnx.py
src/transformers/convert_pytorch_checkpoint_to_tf2.py
src/transformers/convert_slow_tokenizers_checkpoints_to_fast.py
src/transformers/convert_tf_hub_seq_to_seq_bert_to_pytorch.py
src/transformers/data/metrics/squad_metrics.py
src/transformers/data/test_generation_utils.py
src/transformers/file_utils.py
src/transformers/integrations.py
src/transformers/models/__init__.py
src/transformers/models/albert/convert_albert_original_tf_checkpoint_to_pytorch.py
src/transformers/models/bart/convert_bart_original_pytorch_checkpoint_to_pytorch.py
src/transformers/models/bart/tokenization_bart.py
src/transformers/models/bart/tokenization_bart_fast.py
src/transformers/models/bert/convert_bert_original_tf2_checkpoint_to_pytorch.py
src/transformers/models/bert/convert_bert_original_tf_checkpoint_to_pytorch.py
src/transformers/models/bert/convert_bert_pytorch_checkpoint_to_original_tf.py
src/transformers/models/blenderbot/convert_blenderbot_original_pytorch_checkpoint_to_pytorch.py
src/transformers/models/dialogpt/convert_dialogpt_original_pytorch_checkpoint_to_pytorch.py
src/transformers/models/dpr/convert_dpr_original_checkpoint_to_pytorch.py
src/transformers/models/electra/convert_electra_original_tf_checkpoint_to_pytorch.py
src/transformers/models/fsmt/convert_fsmt_original_pytorch_checkpoint_to_pytorch.py
src/transformers/models/funnel/convert_funnel_original_tf_checkpoint_to_pytorch.py
src/transformers/models/gpt2/convert_gpt2_original_tf_checkpoint_to_pytorch.py
src/transformers/models/longformer/convert_longformer_original_pytorch_lightning_to_pytorch.py
src/transformers/models/lxmert/convert_lxmert_original_tf_checkpoint_to_pytorch.py
src/transformers/models/marian/convert_marian_tatoeba_to_pytorch.py
src/transformers/models/marian/convert_marian_to_pytorch.py
src/transformers/models/mbart/convert_mbart_original_checkpoint_to_pytorch.py
src/transformers/models/mobilebert/convert_mobilebert_original_tf_checkpoint_to_pytorch.py
src/transformers/models/openai/convert_openai_original_tf_checkpoint_to_pytorch.py
src/transformers/models/pegasus/convert_pegasus_tf_to_pytorch.py
src/transformers/models/prophetnet/convert_prophetnet_original_pytorch_checkpoint_to_pytorch.py
src/transformers/models/reformer/convert_reformer_trax_checkpoint_to_pytorch.py
src/transformers/models/roberta/convert_roberta_original_pytorch_checkpoint_to_pytorch.py
src/transformers/models/t5/convert_t5_original_tf_checkpoint_to_pytorch.py
src/transformers/models/t5/modeling_tf_t5.py
src/transformers/models/tapas/convert_tapas_original_tf_checkpoint_to_pytorch.py
src/transformers/models/tapas/tokenization_tapas.py
src/transformers/models/transfo_xl/convert_transfo_xl_original_tf_checkpoint_to_pytorch.py
src/transformers/models/xlm/convert_xlm_original_pytorch_checkpoint_to_pytorch.py
src/transformers/models/xlnet/convert_xlnet_original_tf_checkpoint_to_pytorch.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/to_replace_{{cookiecutter.lowercase_modelname}}.py
utils/check_dummies.py
utils/check_repo.py
==================
a400fe893;Patrick von Platen;2021-01-07 12:29:03 +0100;[LED Test] fix common inputs pt for flaky pt-tf led test (#9459)
* fix common inputs pt flakey led

* fix other tests correspondingly
==

tests/test_modeling_led.py
==================
ae5a32bb0;Patrick von Platen;2021-01-07 11:51:02 +0100;up (#9454)

==

docs/source/model_sharing.rst
==================
812045adc;Julien Plu;2021-01-07 11:48:49 +0100;New serving (#9419)
* Add a serving method

* Add albert

* Add serving for BERT and BART

* Add more models

* Finish the serving addition

* Temp fix

* Restore DPR

* Fix funnel attribute

* Fix attributes GPT2

* Fix OpenAIGPT attribute

* Fix T5 attributes

* Fix Bart attributes

* Fix TransfoXL attributes

* Add versioning

* better test

* Update template

* Fix Flaubert

* Fix T5

* Apply style

* Remove unused imports

* Deactivate extra parameters

* Remove too long test + saved_model default to False

* Ignore the saved model test for some models

* Fix some inputs

* Fix mpnet serving

* Trigger CI

* Address all comments
==

src/transformers/modeling_tf_utils.py
src/transformers/models/albert/modeling_tf_albert.py
src/transformers/models/bart/modeling_tf_bart.py
src/transformers/models/bert/modeling_tf_bert.py
src/transformers/models/ctrl/modeling_tf_ctrl.py
src/transformers/models/distilbert/modeling_tf_distilbert.py
src/transformers/models/dpr/modeling_tf_dpr.py
src/transformers/models/electra/modeling_tf_electra.py
src/transformers/models/flaubert/modeling_tf_flaubert.py
src/transformers/models/funnel/modeling_tf_funnel.py
src/transformers/models/gpt2/modeling_tf_gpt2.py
src/transformers/models/led/modeling_tf_led.py
src/transformers/models/longformer/modeling_tf_longformer.py
src/transformers/models/lxmert/modeling_tf_lxmert.py
src/transformers/models/mobilebert/modeling_tf_mobilebert.py
src/transformers/models/mpnet/modeling_tf_mpnet.py
src/transformers/models/openai/modeling_tf_openai.py
src/transformers/models/roberta/modeling_tf_roberta.py
src/transformers/models/t5/modeling_tf_t5.py
src/transformers/models/transfo_xl/modeling_tf_transfo_xl.py
src/transformers/models/xlm/modeling_tf_xlm.py
src/transformers/models/xlnet/modeling_tf_xlnet.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_tf_{{cookiecutter.lowercase_modelname}}.py
tests/test_modeling_tf_bart.py
tests/test_modeling_tf_blenderbot.py
tests/test_modeling_tf_common.py
tests/test_modeling_tf_dpr.py
tests/test_modeling_tf_funnel.py
tests/test_modeling_tf_led.py
tests/test_modeling_tf_longformer.py
tests/test_modeling_tf_lxmert.py
tests/test_modeling_tf_marian.py
tests/test_modeling_tf_mbart.py
tests/test_modeling_tf_mobilebert.py
tests/test_modeling_tf_pegasus.py
tests/test_modeling_tf_t5.py
==================
390cf16bc;guillaume-be;2021-01-07 11:41:58 +0100;Prophetnet optimization (#9453)
* Vectorized `ngram_attention_bias` calculation

* updated formatting with black

* Further optimization

* one (last) optimization
==

src/transformers/models/prophetnet/modeling_prophetnet.py
==================
28d74872c;Stas Bekman;2021-01-07 01:47:50 -0800;a more reliable version of branching point discovery (#9449)

==

utils/get_modified_files.py
==================
3ec40299c;Sylvain Gugger;2021-01-07 04:10:41 -0500;Remove nested lxmert (#9440)

==

examples/research_projects/movement-pruning/lxmert/README.md
examples/research_projects/movement-pruning/lxmert/demo.ipynb
examples/research_projects/movement-pruning/lxmert/extracting_data.py
examples/research_projects/movement-pruning/lxmert/modeling_frcnn.py
examples/research_projects/movement-pruning/lxmert/processing_image.py
examples/research_projects/movement-pruning/lxmert/requirements.txt
examples/research_projects/movement-pruning/lxmert/utils.py
examples/research_projects/movement-pruning/lxmert/visualizing_image.py
==================
b8462b5b2;Patrick von Platen;2021-01-06 19:37:02 +0100;[GenerationOutputs] Fix GenerationOutputs Tests  (#9443)
* fix generation models

* fix led

* fix docs

* add is_decoder

* fix last docstrings

* make style

* fix t5 cross attentions

* correct t5
==

src/transformers/generation_utils.py
src/transformers/models/bert/modeling_bert.py
src/transformers/models/bert_generation/modeling_bert_generation.py
src/transformers/models/encoder_decoder/modeling_encoder_decoder.py
src/transformers/models/roberta/modeling_roberta.py
src/transformers/models/t5/modeling_t5.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_{{cookiecutter.lowercase_modelname}}.py
tests/test_generation_utils.py
tests/test_modeling_led.py
==================
0c96262f7;Sylvain Gugger;2021-01-06 12:17:24 -0500;Fast transformers import part 1 (#9441)
* Don't import libs to check they are available

* Don't import integrations at init

* Add importlib_metdata to deps

* Remove old vars references

* Avoid syntax error

* Adapt testing utils

* Try to appease torchhub

* Add dependency

* Remove more private variables

* Fix typo

* Another typo

* Refine the tf availability test
==

.github/workflows/github-torch-hub.yml
hubconf.py
setup.py
src/transformers/dependency_versions_check.py
src/transformers/dependency_versions_table.py
src/transformers/file_utils.py
src/transformers/integrations.py
src/transformers/testing_utils.py
src/transformers/trainer.py
tests/test_tokenization_camembert.py
tests/test_tokenization_marian.py
tests/test_tokenization_mbart.py
tests/test_tokenization_t5.py
==================
c89f1bc92;Simon Brandeis;2021-01-06 17:11:42 +0100;Add flags to return scores, hidden states and / or attention weights in GenerationMixin (#9150)
* Define new output dataclasses for greedy generation

* Add output_[...] flags in greedy generation methods

Added output_attentions, output_hidden_states, output_scores flags in
generate and greedy_search methods in GenerationMixin.

* [WIP] Implement logic and tests for output flags in generation

* Update GreedySearchOutput classes & docstring

* Implement greedy search output accumulation logic

Update greedy_search unittests

Fix generate method return value docstring

Properly init flags with the default config

* Update configuration to add output_scores flag

* Fix test_generation_utils

Sort imports and fix isinstance tests for GreedySearchOutputs

* Fix typo in generation_utils

* Add return_dict_in_generate for backwards compatibility

* Add return_dict_in_generate flag in config

* Fix tyPo in configuration

* Fix handling of attentions and hidden_states flags

* Make style & quality

* first attempt attentions

* some corrections

* improve tests

* special models requires special test

* disable xlm test for now

* clean tests

* fix for tf

* isort

* Add output dataclasses for other generation methods

* Add logic to return dict in sample generation

* Complete test for sample generation

- Pass output_attentions and output_hidden_states flags to encoder in
encoder-decoder models
- Fix import satements order in test_generation_utils file

* Add logic to return dict in sample generation

- Refactor tests to avoid using self.assertTrue, which provides
scarce information when the test fails
- Add tests for the three beam_search methods: vanilla, sample and
grouped

* Style doc

* Fix copy-paste error in generation tests

* Rename logits to scores and refactor

* Refactor group_beam_search for consistency

* make style

* add sequences_scores

* fix all tests

* add docs

* fix beam search finalize test

* correct docstring

* clean some files

* Made suggested changes to the documentation

* Style doc ?

* Style doc using the Python util

* Update src/transformers/generation_utils.py

* fix empty lines

* fix all test

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
==

docs/source/internal/generation_utils.rst
src/transformers/configuration_utils.py
src/transformers/generation_beam_search.py
src/transformers/generation_utils.py
src/transformers/models/openai/configuration_openai.py
tests/test_generation_beam_search.py
tests/test_generation_utils.py
tests/test_modeling_reformer.py
tests/test_modeling_transfo_xl.py
tests/test_modeling_xlm.py
tests/test_modeling_xlnet.py
==================
7a9f1b5c9;Kevin Canwen Xu;2021-01-06 23:34:48 +0800;Store transformers version info when saving the model (#9421)
* Store transformers version info when saving the model

* Store transformers version info when saving the model

* fix format

* fix format

* fix format

* Update src/transformers/configuration_utils.py

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* Update configuration_utils.py

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
==

src/transformers/configuration_utils.py
==================
ecfcac223;Qbiwan;2021-01-06 23:04:32 +0800;Improve documentation coverage for Phobert  (#9427)
* first commit

* change phobert to phoBERT as per author in overview

* v3 and v4 both runs on same code hence there is no need to differentiate them

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

docs/source/index.rst
docs/source/model_doc/phobert.rst
utils/check_repo.py
==================
be898998b;Qbiwan;2021-01-06 22:13:43 +0800;Improve documentation coverage for Herbert (#9428)
* first commit

* changed XLMTokenizer to HerbertTokenizer in code example
==

docs/source/index.rst
docs/source/model_doc/herbert.rst
utils/check_repo.py
==================
b972c1bfb;Patrick von Platen;2021-01-06 14:36:55 +0100;finalize (#9431)

==

docs/source/model_sharing.rst
==================
bcb55d33c;Sylvain Gugger;2021-01-06 07:46:17 -0500;Upgrade styler to better handle lists (#9423)
* Add missing lines before a new list.

* Update doc styler and restyle some files.

* Fix docstrings of LED and Longformer
==

docs/source/model_doc/led.rst
src/transformers/models/dpr/modeling_tf_dpr.py
src/transformers/models/layoutlm/modeling_layoutlm.py
src/transformers/models/led/modeling_led.py
src/transformers/models/led/modeling_tf_led.py
src/transformers/models/longformer/modeling_longformer.py
src/transformers/models/longformer/modeling_tf_longformer.py
src/transformers/testing_utils.py
src/transformers/trainer_utils.py
utils/style_doc.py
==================
b7e548976;NielsRogge;2021-01-06 13:20:41 +0100;Fix URLs to TAPAS notebooks (#9435)

==

docs/source/model_doc/tapas.rst
notebooks/README.md
==================
9f675b05d;Stas Bekman;2021-01-06 03:50:11 -0800;[trainer] self.model_wrapped + _model_unwrap (#9390)
* model wrapped + model_unwrap

* cleanup

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* style

* deprecation warning

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

src/transformers/trainer.py
tests/test_trainer.py
==================
453a70d4c;Sylvain Gugger;2021-01-06 06:49:23 -0500;Allow example to use a revision and work with private models (#9407)
* Allow example to use a revision and work with private models

* Copy to other examples and template

* Styling
==

examples/language-modeling/run_clm.py
examples/language-modeling/run_mlm.py
examples/language-modeling/run_mlm_wwm.py
examples/language-modeling/run_plm.py
examples/multiple-choice/run_swag.py
examples/question-answering/run_qa.py
examples/question-answering/run_qa_beam_search.py
examples/text-classification/run_glue.py
examples/token-classification/run_ner.py
templates/adding_a_new_example_script/{{cookiecutter.directory_name}}/run_{{cookiecutter.example_shortcut}}.py
==================
7988edc03;Manuel Romero;2021-01-06 09:44:52 +0100;Fix link to Notebook to fine-tune TAPAS (#9413)
Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
==

notebooks/README.md
==================
c9553c035;Manuel Romero;2021-01-06 09:42:50 +0100;Fix link to Evaluate TAPAS Notebook (#9414)

==

notebooks/README.md
==================
090d28e32;Nicolas Patry;2021-01-06 09:33:50 +0100;[Refactor] Splitting pipelines.py into its own module. (#9279)
* Splitting pipelines into its own module.

* Moving everything into base.py

* Moving FeatureExtractionPipeline into its own file.

* TextGenerationPipeline.

* TextClassifictionPipeline

* ZeroShot + get_framework import.

* FillMaskPipeline

* NerPipeline + TokenClassificationPipeline

* QuestionAnsweringPipeline

* TableQuestionAnsweringPipeline

* ConversationnalPipeline

* Text2TextGenerationPipeline, TranslationPipeline, SummarizationPipeline

* Typo import fix.

* Relative imports.
==

src/transformers/pipelines.py
src/transformers/pipelines/__init__.py
src/transformers/pipelines/base.py
src/transformers/pipelines/conversational.py
src/transformers/pipelines/feature_extraction.py
src/transformers/pipelines/fill_mask.py
src/transformers/pipelines/question_answering.py
src/transformers/pipelines/table_question_answering.py
src/transformers/pipelines/text2text_generation.py
src/transformers/pipelines/text_classification.py
src/transformers/pipelines/text_generation.py
src/transformers/pipelines/token_classification.py
src/transformers/pipelines/zero_shot_classification.py
==================
d64372fdf;Stas Bekman;2021-01-05 17:34:15 -0800;[docs] outline sharded ddp doc (#9208)
* outline sharded dpp doc

* fix link

* add example

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* narrow the command and remove non-essentials

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

docs/source/training.rst
==================
eef66035a;Patrick von Platen;2021-01-05 22:00:05 +0100;[PyTorch Bart] Split Bart into different models (#9343)
* first try

* remove old template

* finish bart

* finish mbart

* delete unnecessary line

* init pegasus

* save intermediate

* correct pegasus

* finish pegasus

* remove cookie cutter leftover

* add marian

* finish blenderbot

* replace in file

* correctly split blenderbot

* delete "old" folder

* correct "add statement"

* adapt config for tf comp

* correct configs for tf

* remove ipdb

* fix more stuff

* fix mbart

* push pegasus fix

* fix mbart

* more fixes

* fix research projects code

* finish docs for bart, mbart, and marian

* delete unnecessary file

* correct attn typo

* correct configs

* remove pegasus for seq class

* correct peg docs

* correct peg docs

* finish configs

* further improve docs

* add copied from statements to mbart

* fix copied from in mbart

* add copy statements to marian

* add copied from to marian

* add pegasus copied from

* finish pegasus

* finish copied from

* Apply suggestions from code review

* make style

* backward comp blenderbot

* apply lysandres and sylvains suggestions

* apply suggestions

* push last fixes

* fix docs

* fix tok tests

* fix imports code style

* fix doc
==

docs/source/index.rst
docs/source/model_doc/bart.rst
docs/source/model_doc/blenderbot.rst
docs/source/model_doc/blenderbot_small.rst
docs/source/model_doc/marian.rst
docs/source/model_doc/mbart.rst
docs/source/model_doc/pegasus.rst
examples/research_projects/seq2seq-distillation/utils copy.py
examples/seq2seq/test_datasets.py
examples/seq2seq/utils.py
src/transformers/__init__.py
src/transformers/models/auto/configuration_auto.py
src/transformers/models/auto/modeling_auto.py
src/transformers/models/auto/tokenization_auto.py
src/transformers/models/bart/__init__.py
src/transformers/models/bart/configuration_bart.py
src/transformers/models/bart/modeling_bart.py
src/transformers/models/bart/modeling_tf_bart.py
src/transformers/models/blenderbot/__init__.py
src/transformers/models/blenderbot/configuration_blenderbot.py
src/transformers/models/blenderbot/modeling_blenderbot.py
src/transformers/models/blenderbot/tokenization_blenderbot.py
src/transformers/models/blenderbot_small/__init__.py
src/transformers/models/blenderbot_small/configuration_blenderbot_small.py
src/transformers/models/blenderbot_small/modeling_blenderbot_small.py
src/transformers/models/blenderbot_small/tokenization_blenderbot_small.py
src/transformers/models/blenderbot_small/tokenization_blenderbot_small_fast.py
src/transformers/models/encoder_decoder/modeling_encoder_decoder.py
src/transformers/models/fsmt/modeling_fsmt.py
src/transformers/models/marian/__init__.py
src/transformers/models/marian/configuration_marian.py
src/transformers/models/marian/modeling_marian.py
src/transformers/models/marian/tokenization_marian.py
src/transformers/models/mbart/__init__.py
src/transformers/models/mbart/configuration_mbart.py
src/transformers/models/mbart/modeling_mbart.py
src/transformers/models/pegasus/__init__.py
src/transformers/models/pegasus/configuration_pegasus.py
src/transformers/models/pegasus/modeling_pegasus.py
src/transformers/models/prophetnet/modeling_prophetnet.py
src/transformers/models/t5/modeling_t5.py
src/transformers/models/t5/modeling_tf_t5.py
src/transformers/utils/dummy_pt_objects.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/configuration_{{cookiecutter.lowercase_modelname}}.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_tf_{{cookiecutter.lowercase_modelname}}.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_{{cookiecutter.lowercase_modelname}}.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/test_modeling_{{cookiecutter.lowercase_modelname}}.py
tests/test_modeling_bart.py
tests/test_modeling_blenderbot.py
tests/test_modeling_blenderbot_small.py
tests/test_modeling_marian.py
tests/test_modeling_mbart.py
tests/test_modeling_pegasus.py
tests/test_modeling_tf_bart.py
tests/test_modeling_tf_pegasus.py
tests/test_tokenization_blenderbot.py
tests/test_tokenization_mbart.py
tests/test_tokenization_small_blenderbot.py
utils/check_repo.py
==================
4eec5d0cf;Clement;2021-01-05 15:02:46 -0500;improve readme text to private models/versioning/api (#9424)

==

README.md
==================
d9e848c1d;Stas Bekman;2021-01-05 07:05:32 -0800;add experimental warning (#9412)

==

src/transformers/models/gpt2/modeling_gpt2.py
src/transformers/models/t5/modeling_t5.py
==================
29acabd88;Stas Bekman;2021-01-05 06:39:38 -0800;[trainer] group fp16 args together (#9409)
* [t5 doc] typos

a few run away backticks

@sgugger

* style

* [trainer] put fp16 args together

this PR proposes a purely cosmetic change that puts all the fp16 args together - so they are easier to manager/read

@sgugger

* style
==

src/transformers/training_args.py
==================
57a662692;Yusuke Mori;2021-01-05 22:15:06 +0900;[examples/text-classification] Fix a bug for using one's own dataset of a regression task (#9411)

==

examples/text-classification/run_glue.py
==================
189387e9b;Patrick von Platen;2021-01-05 13:14:30 +0100;LED (#9278)
* create model

* add integration

* save current state

* make integration tests pass

* add one more test

* add explanation to tests

* remove from bart

* add padding

* remove unnecessary test

* make all tests pass

* re-add cookie cutter tests

* finish PyTorch

* fix attention test

* Update tests/test_modeling_common.py

* revert change

* remove unused file

* add string to doc

* save intermediate

* make tf integration tests pass

* finish tf

* fix doc

* fix docs again

* add led to doctree

* add to auto tokenizer

* added tips for led

* make style

* apply jplus statements

* correct tf longformer

* apply lysandres suggestions

* apply sylvains suggestions

* Apply suggestions from code review
==

docs/source/index.rst
docs/source/model_doc/led.rst
src/transformers/__init__.py
src/transformers/convert_slow_tokenizer.py
src/transformers/models/auto/configuration_auto.py
src/transformers/models/auto/modeling_auto.py
src/transformers/models/auto/modeling_tf_auto.py
src/transformers/models/auto/tokenization_auto.py
src/transformers/models/led/__init__.py
src/transformers/models/led/configuration_led.py
src/transformers/models/led/modeling_led.py
src/transformers/models/led/modeling_tf_led.py
src/transformers/models/led/tokenization_led.py
src/transformers/models/led/tokenization_led_fast.py
src/transformers/models/longformer/modeling_longformer.py
src/transformers/models/longformer/modeling_tf_longformer.py
src/transformers/utils/dummy_pt_objects.py
src/transformers/utils/dummy_tf_objects.py
src/transformers/utils/dummy_tokenizers_objects.py
tests/test_modeling_led.py
tests/test_modeling_longformer.py
tests/test_modeling_tf_led.py
tests/test_modeling_tf_longformer.py
utils/check_repo.py
==================
314cca284;Sugeeth;2021-01-05 16:48:48 +0530;Fix documentation links always pointing to master. (#9217)
* Use extlinks to point hyperlink with the version of code

* Point to version on release and master until then

* Apply style

* Correct links

* Add missing backtick

* Simple missing backtick after all.

Co-authored-by: Raghavendra Sugeeth P S <raghav-5305@raghav-5305.csez.zohocorpin.com>
Co-authored-by: Lysandre <lysandre.debut@reseau.eseo.fr>
==

docs/source/benchmarks.rst
docs/source/bertology.rst
docs/source/conf.py
docs/source/converting_tensorflow_models.rst
docs/source/main_classes/processors.rst
docs/source/model_doc/bart.rst
docs/source/model_doc/barthez.rst
docs/source/model_doc/marian.rst
docs/source/model_doc/mbart.rst
docs/source/model_doc/pegasus.rst
docs/source/multilingual.rst
docs/source/task_summary.rst
docs/source/testing.rst
==================
52d62e686;Julien Plu;2021-01-05 11:54:50 +0100;Fix  TF Funnel (#9300)
* Fix Funnel

* Apply Patrick's comment

* Remove comment

* Fix dummy value

* Apply style
==

src/transformers/models/funnel/modeling_tf_funnel.py
==================
748006c0b;Stas Bekman;2021-01-05 01:01:30 -0800;[trainer] --model_parallel hasn't been implemented for most models (#9347)
* --model_parallel hasn't been implemented for most models

* make the help clear as well

* implement is_parallelizable; use it

* oops

* remove property
==

src/transformers/modeling_utils.py
src/transformers/models/gpt2/modeling_gpt2.py
src/transformers/models/t5/modeling_t5.py
src/transformers/trainer.py
src/transformers/training_args.py
==================
4225740a7;Julien Plu;2021-01-05 09:58:26 +0100;Use stable functions (#9369)

==

src/transformers/benchmark/benchmark_args_tf.py
src/transformers/trainer_tf.py
src/transformers/training_args_tf.py
tests/test_modeling_tf_common.py
tests/test_optimization_tf.py
==================
4aa8f6ad9;Stas Bekman;2021-01-05 00:57:57 -0800;[logging] autoflush (#9385)
This PR proposes to:

* auto-flush `transformers` logging 

When using logging for tracing signals from different parts of the code and which could be mixed with print debug this aids to get all the logging events synchronized. 

I don't think this change will introduce any performance impacts.

If it helps someone here is the code I used to sync `transformers` logging with various other debug prints.

I was porting bart to MP and I needed to trace that the device switching happens correctly and I added a bunch of logger.info calls inside `modeling_bart.py` and also had some other helpers `print` debug messages which weren't logger based:

```

# auto flush std streams
from sys import stdout, stderr
def stdout_write_flush(args, w=stderr.write): w(args); stderr.flush()
def stderr_write_flush(args, w=stderr.write): w(args); stderr.flush()
stdout.write = stdout_write_flush
stderr.write = stderr_write_flush

from transformers import BartTokenizer, BartForConditionalGeneration, BartConfig

import logging
import transformers.utils.logging
import transformers.models.bart.modeling_bart

# I wanted a shorter simpler format
handlers = transformers.utils.logging._get_library_root_logger().handlers
for handler in handlers:
    formatter = logging.Formatter("[%(funcName)s] %(message)s")
    handler.setFormatter(formatter)

transformers.models.bart.modeling_bart.logger.setLevel(transformers.logging.INFO)
```

@LysandreJik, @sgugger, @patrickvonplaten
==

src/transformers/utils/logging.py
==================
83eec97ec;Julien Plu;2021-01-05 09:49:54 +0100;Fix TF Longformer (#9348)
* Fix longformer

* Apply style

* Remove serving content

* Forgot a condition

* Apply style

* Address Patrick's comments

* Fix dtype
==

src/transformers/models/longformer/modeling_tf_longformer.py
==================
30fa0b780;Boris Dayma;2021-01-05 02:30:46 -0600;feat(wandb): save model as artifact (#8119)
* feat(wandb): log artifacts

* fix: typo

* feat(wandb): ensure name is allowed

* feat(wandb): log artifact

* feat(wandb): saving logic

* style: improve formatting

* fix: unrelated typo

* feat:¬†use a fake trainer

* fix:¬†simplify

* feat(wandb): log model files as artifact

* style: fix style

* docs(wandb): correct description

* feat: unpack model + allow env Truethy values

* feat: TrainerCallback can access tokenizer

* style:¬†fix style

* feat(wandb): log more interesting metadata

* feat: unpack tokenizer

* feat(wandb): metadata with load_best_model_at_end

* feat(wandb): more robust metadata

* style(wandb): fix formatting
==

src/transformers/integrations.py
src/transformers/trainer.py
src/transformers/trainer_callback.py
==================
143289dcf;Stas Bekman;2021-01-04 12:09:12 -0800;[test_model_parallelization] multiple fixes (#9354)

==

tests/test_modeling_common.py
==================
086718ac6;Qbiwan;2021-01-05 02:12:59 +0800;Improve documentation coverage for Bertweet (#9379)
* bertweet docs coverage

* style doc max len 119

* maxlen style rst

* run main() from style_doc

* changed according to  comments
==

docs/source/index.rst
docs/source/model_doc/bertweet.rst
utils/check_repo.py
==================
47ca0eaaa;Stas Bekman;2021-01-04 10:00:08 -0800;replace apex.normalization.FusedLayerNorm with torch.nn.LayerNorm (#9386)

==

src/transformers/models/bart/modeling_bart.py
src/transformers/models/fsmt/modeling_fsmt.py
src/transformers/models/prophetnet/modeling_prophetnet.py
==================
75ff53055;Patrick von Platen;2021-01-04 17:27:29 +0100;correct docs (#9378)

==

docs/source/custom_datasets.rst
==================
ec54d70e1;Julien Plu;2021-01-04 17:26:56 +0100;Fix TF DPR (#9283)
* Fix DPR

* Keep usual models

* Apply style

* Address Sylvain's comments
==

src/transformers/models/dpr/modeling_tf_dpr.py
==================
de29ff9bd;Julien Plu;2021-01-04 16:22:15 +0100;Fix open (#9368)

==

utils/check_repo.py
==================
d018afced;Stas Bekman;2021-01-04 07:14:32 -0800;[trainer] parametrize default output_dir (#9352)
This PR:

* fixes trainer to have the logger agree with the actual default `output_dir`, but setting it one place and passing it as an argument to both places

@sgugger
==

src/transformers/trainer.py
==================
d735b074d;Julien Plu;2021-01-04 16:06:28 +0100;Fix Flaubert (#9292)

==

src/transformers/models/flaubert/modeling_tf_flaubert.py
==================
5dd389d1c;dependabot[bot];2021-01-04 10:02:07 -0500;Bump notebook from 6.1.4 to 6.1.5 in /examples/research_projects/lxmert (#9402)
Bumps [notebook](https://github.com/jupyter/jupyterhub) from 6.1.4 to 6.1.5.
- [Release notes](https://github.com/jupyter/jupyterhub/releases)
- [Changelog](https://github.com/jupyterhub/jupyterhub/blob/master/CHECKLIST-Release.md)
- [Commits](https://github.com/jupyter/jupyterhub/commits)

Signed-off-by: dependabot[bot] <support@github.com>

Co-authored-by: dependabot[bot] <49699333+dependabot[bot]@users.noreply.github.com>
==

examples/research_projects/lxmert/requirements.txt
==================
23a71449c;Sylvain Gugger;2021-01-04 09:59:07 -0500;Put back LXMert example (#9401)

==

examples/research_projects/lxmert/README.md
examples/research_projects/lxmert/demo.ipynb
examples/research_projects/lxmert/extracting_data.py
examples/research_projects/lxmert/modeling_frcnn.py
examples/research_projects/lxmert/processing_image.py
examples/research_projects/lxmert/requirements.txt
examples/research_projects/lxmert/utils.py
examples/research_projects/lxmert/visualizing_image.py
==================
6c03d4ac7;Julien Plu;2021-01-04 15:56:51 +0100;Fix CTRL (#9291)

==

src/transformers/models/ctrl/modeling_tf_ctrl.py
==================
c581d8af7;Charles;2021-01-04 14:53:54 +0000;Add utility function for retrieving locally cached models  (#8836)
* add get_cached_models function

* add List type to import

* fix code quality

* Update src/transformers/file_utils.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/file_utils.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/file_utils.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/file_utils.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/file_utils.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Fix style

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

src/transformers/file_utils.py
==================
8eb7f26d5;Sam Shleifer;2021-01-04 00:51:24 -0500;simplify marian distillation script (#9394)

==

examples/research_projects/seq2seq-distillation/distil_marian_no_teacher.sh
==================
d944966b1;Yoshitomo Matsubara;2021-01-03 07:00:30 -0800;Fix typos in README and bugs in RAG example code for end-to-end evaluation and finetuning (#9355)
* fix a bug in eval_batch_retrieval

* should return parser as well as other staticmethod

* remove duplicate argument

* these kwargs are no longer accepted (cause TypeError in self.generator.generate of modeling_rag.py)

* fixed file paths in README

* moved an arg to add_ray_specific_args
==

examples/research_projects/rag/README.md
examples/research_projects/rag/eval_rag.py
examples/research_projects/rag/finetune_rag.py
==================
c4fd609af;Chris Kennedy;2021-01-02 11:58:16 -0500;file_utils.py: TF examples outputs.last_hidden_states -> state (#9382)

==

src/transformers/file_utils.py
==================
b01f451ca;Patrick von Platen;2021-01-02 15:55:07 +0100;[Docs] `past_key_values` return a tuple of tuple as a default (#9381)
* push

* make style
==

src/transformers/modeling_outputs.py
==================
5f7a07c0c;Derrick Blakely;2021-01-02 04:39:14 -0700;use return dict for rag encoder (#9363)

==

src/transformers/models/rag/modeling_rag.py
==================
ae333d04b;Stas Bekman;2020-12-30 01:09:51 -0800;torch.cuda.is_available() is redundant as apex handles that internally (#9350)

==

src/transformers/models/bart/modeling_bart.py
src/transformers/models/fsmt/modeling_fsmt.py
src/transformers/models/prophetnet/modeling_prophetnet.py
==================
8217d4e37;Stas Bekman;2020-12-29 13:32:07 -0800;[prophetnet] wrong import (#9349)
```
python -c "from apex.normalization import FusedProphetNetLayerNorm"
Traceback (most recent call last):
  File "<string>", line 1, in <module>
ImportError: cannot import name 'FusedProphetNetLayerNorm' from 'apex.normalization' (/home/stas/anaconda3/envs/main-38/lib/python3.8/site-packages/apex/normalization/__init__.py)
```
It looks like this code has never been tested, so it silently fails inside try/except.

Discovered this by accident in https://github.com/huggingface/transformers/issues/9338#issuecomment-752217708
==

src/transformers/models/prophetnet/modeling_prophetnet.py
==================
912f6881d;Patrick von Platen;2020-12-29 19:35:06 +0100;add import math (#9346)

==

templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_tf_{{cookiecutter.lowercase_modelname}}.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_{{cookiecutter.lowercase_modelname}}.py
==================
785e52cd3;Patrick von Platen;2020-12-29 16:48:44 +0100;improve templates (#9342)

==

templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/configuration_{{cookiecutter.lowercase_modelname}}.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_tf_{{cookiecutter.lowercase_modelname}}.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_{{cookiecutter.lowercase_modelname}}.py
==================
64103fb6b;Julien Plu;2020-12-28 20:52:18 +0100;Fix TransfoXL (#9302)

==

src/transformers/models/transfo_xl/modeling_tf_transfo_xl.py
==================
d97d06d05;Julien Plu;2020-12-28 20:51:40 +0100;Fix TF T5 (#9301)
* Fix T5

* Fix test

* Fix test
==

src/transformers/models/t5/modeling_tf_t5.py
==================
83fdd252f;Patrick von Platen;2020-12-28 17:51:04 +0100;[Seq2Seq Templates] Correct some TF-serving errors and add gradient checkpointing to PT by default. (#9334)
* correct tests

* correct shape and get_tf_activation

* more correction tf

* add gradient checkpointing to templates

* correct typo
==

templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/configuration_{{cookiecutter.lowercase_modelname}}.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_tf_{{cookiecutter.lowercase_modelname}}.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_{{cookiecutter.lowercase_modelname}}.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/test_modeling_tf_{{cookiecutter.lowercase_modelname}}.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/test_modeling_{{cookiecutter.lowercase_modelname}}.py
==================
8e74eca7f;Patrick von Platen;2020-12-27 21:57:50 +0100;push (#9320)

==

src/transformers/trainer_seq2seq.py
==================
61443cd7d;Patrick von Platen;2020-12-25 23:28:12 +0100;[GPT2] Correct gradient checkpointing (#9308)
* correct gpt2

* fix gpt2

* fix use_cache ordering

* correct past tolerance

* fix for all cases

* style
==

src/transformers/models/gpt2/modeling_gpt2.py
tests/test_modeling_common.py
tests/test_modeling_tf_gpt2.py
==================
21fc67664;Vasudev Gupta;2020-12-25 14:47:49 +0530;add translation example (#9303)
* Created using Colaboratory

* mbart-training examples add

* link add

* Update description

Co-authored-by: Suraj Patil <surajp815@gmail.com>
==

notebooks/README.md
==================
52b3a05e8;Patrick von Platen;2020-12-24 14:47:53 +0100;[Bart doc] Fix outdated statement (#9299)
* fix bart doc

* fix docs
==

docs/source/model_doc/bart.rst
==================
7777db159;Bram Vanroy;2020-12-24 14:43:14 +0100;Update tokenization_utils_base.py (#9293)
Missing "s" typo
==

src/transformers/tokenization_utils_base.py
==================
71963a663;Daniele Sartiano;2020-12-24 14:38:08 +0100;fix typo in modeling_encoder_decoder.py (#9297)
* Update modeling_encoder_decoder.py

Fixed typo.

* typo

Co-authored-by: Suraj Patil <surajp815@gmail.com>
==

src/transformers/models/encoder_decoder/modeling_encoder_decoder.py
==================
f3a3b91d6;Ratthachat (Jung);2020-12-24 19:38:00 +0700;Proposed Fix : [RagSequenceForGeneration] generate "without" input_ids  (#9220)
* Create modeling_tf_dpr.py

* Add TFDPR

* Add back TFPegasus, TFMarian, TFMBart, TFBlenderBot

last commit accidentally deleted these 4 lines, so I recover them back

* Add TFDPR

* Add TFDPR

* clean up some comments, add TF input-style doc string

* Add TFDPR

* Make return_dict=False as default

* Fix return_dict bug (in .from_pretrained)

* Add get_input_embeddings()

* Create test_modeling_tf_dpr.py

The current version is already passed all 27 tests!
Please see the test run at : 
https://colab.research.google.com/drive/1czS_m9zy5k-iSJbzA_DP1k1xAAC_sdkf?usp=sharing

* fix quality

* delete init weights

* run fix copies

* fix repo consis

* del config_class, load_tf_weights

They shoud be 'pytorch only'

* add config_class back

after removing it, test failed ... so totally only removing "use_tf_weights = None" on Lysandre suggestion

* newline after .. note::

* import tf, np (Necessary for ModelIntegrationTest)

* slow_test from_pretrained with from_pt=True

At the moment we don't have TF weights (since we don't have official official TF model)
Previously, I did not run slow test, so I missed this bug

* Add simple TFDPRModelIntegrationTest

Note that this is just a test that TF and Pytorch gives approx. the same output.
However, I could not test with the official DPR repo's output yet

* upload correct tf model

* remove position_ids as missing keys

* fix RagSeq generate with context_input_ids

fix RagSeq generate with context_input_ids

* apply style

* delete unused lines

* Add test_rag_sequence_generate_batch_from_context_input_ids

* Readability improved

* stylying

* Stylize

* typos

* add check_model_generate_from_context_input_ids

* make style

* Apply suggestions from code review

* make style2

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
Co-authored-by: patrickvonplaten <patrick@huggingface.co>
==

src/transformers/models/rag/modeling_rag.py
tests/test_modeling_rag.py
==================
2a18b7099;Suraj Patil;2020-12-24 17:47:36 +0530;enable cache by default (#9296)

==

src/transformers/models/bert_generation/configuration_bert_generation.py
src/transformers/models/bert_generation/modeling_bert_generation.py
==================
6189ae996;Jungwhan;2020-12-24 17:18:33 +0900;Fix typo in file_utils.py (#9289)

==

src/transformers/file_utils.py
==================
222dbdb20;Jethro Kuan;2020-12-24 16:01:56 +0800;allow integer device for BatchEncoding (#9271)
Fixes #9244

Co-authored-by: Jethro Kuan <jethro.kuan@bytedance.com>
==

src/transformers/tokenization_utils_base.py
==================
6c091abef;Patrick von Platen;2020-12-24 01:44:33 +0100;[Templates] Adapt Bert (#9284)
* adapt templates

* adapt config

* add test as well

* fix output type

* fix cache false naming

* finish tests

* last fix
==

templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/configuration_{{cookiecutter.lowercase_modelname}}.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_{{cookiecutter.lowercase_modelname}}.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/test_modeling_{{cookiecutter.lowercase_modelname}}.py
==================
88ef8893c;Suraj Patil;2020-12-23 23:01:32 +0530;Add caching mechanism to BERT, RoBERTa (#9183)
* add past_key_values

* add use_cache option

* make mask before cutting ids

* adjust position_ids according to past_key_values

* flatten past_key_values

* fix positional embeds

* fix _reorder_cache

* set use_cache to false when not decoder, fix attention mask init

* add test for caching

* add past_key_values for Roberta

* fix position embeds

* add caching test for roberta

* add doc

* make style

* doc, fix attention mask, test

* small fixes

* adress patrick's comments

* input_ids shouldn't start with pad token

* use_cache only when decoder

* make consistent with bert

* make copies consistent

* add use_cache to encoder

* add past_key_values to tapas attention

* apply suggestions from code review

* make coppies consistent

* add attn mask in tests

* remove copied from longformer

* apply suggestions from code review

* fix bart test

* nit

* simplify model outputs

* fix doc

* fix output ordering
==

docs/source/main_classes/output.rst
src/transformers/modeling_outputs.py
src/transformers/models/albert/modeling_albert.py
src/transformers/models/bert/configuration_bert.py
src/transformers/models/bert/modeling_bert.py
src/transformers/models/bert_generation/modeling_bert_generation.py
src/transformers/models/electra/modeling_electra.py
src/transformers/models/encoder_decoder/modeling_encoder_decoder.py
src/transformers/models/gpt2/modeling_gpt2.py
src/transformers/models/layoutlm/modeling_layoutlm.py
src/transformers/models/longformer/modeling_longformer.py
src/transformers/models/roberta/modeling_roberta.py
src/transformers/models/tapas/modeling_tapas.py
tests/test_modeling_bart.py
tests/test_modeling_bert.py
tests/test_modeling_bert_generation.py
tests/test_modeling_roberta.py
==================
a1cb6e986;Sylvain Gugger;2020-12-23 11:05:21 -0500;Adapt to new name of `label_smoothing_factor` training arg (#9282)

==

examples/seq2seq/test_finetune_trainer.py
examples/seq2seq/train_distil_marian_enro.sh
examples/seq2seq/train_distil_marian_enro_tpu.sh
==================
bcc87c639;Connor Brinton;2020-12-23 10:15:49 -0500;Minor documentation revisions from copyediting (#9266)
* typo: Revise "checkout" to "check out"

* typo: Change "seemlessly" to "seamlessly"

* typo: Close parentheses in "Using the tokenizer"

* typo: Add closing parenthesis to supported models aside

* docs: Treat ``position_ids`` as plural

Alternatively, the word "argument" could be added to make the subject singular.

* docs: Remove comma, making subordinate clause

* docs: Remove comma separating verb and direct object

* docs: Fix typo ("next" -> "text")

* docs: Reverse phrase order to simplify sentence

* docs: "quicktour" -> "quick tour"

* docs: "to throw" -> "from throwing"

* docs: Remove disruptive newline in padding/truncation section

* docs: "show exemplary" -> "show examples of"

* docs: "much harder as" -> "much harder than"

* docs: Fix typo "seach" -> "search"

* docs: Fix subject-verb disagreement in WordPiece description

* docs: Fix style in preprocessing.rst
==

docs/source/glossary.rst
docs/source/model_summary.rst
docs/source/philosophy.rst
docs/source/preprocessing.rst
docs/source/quicktour.rst
docs/source/task_summary.rst
docs/source/tokenizer_summary.rst
docs/source/training.rst
==================
d5db6c37d;Patrick von Platen;2020-12-23 11:40:20 +0100;[Seq2Seq Templates] Fix check_repo.py templates file (#9277)
* add enc dec pt model to check repo

* fix indent
==

templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/to_replace_{{cookiecutter.lowercase_modelname}}.py
utils/check_repo.py
==================
4bafc43b0;Xu Song;2020-12-23 18:34:57 +0800;Fix param error (#9273)
TypeError: forward() got an unexpected keyword argument 'token_type_ids'
==

src/transformers/models/bert_generation/modeling_bert_generation.py
==================
58e8a7611;Xu Song;2020-12-23 18:34:15 +0800;Fix gpt2 document (#9272)

==

src/transformers/models/gpt2/modeling_gpt2.py
==================
cbe63949d;Patrick von Platen;2020-12-22 23:41:20 +0100;Model Templates for Seq2Seq (#9251)
* adapt cookie cutter

* fix copy past statement

* delete copy statements for now

* remove unused import from template

* make doc rst

* correct config docstring

* correct training

* correct inputs processing tf enc dec

* make style

* adapt templates

* clean tabs

* correct tensor -> Tensor naming

* correct indent

* correct templates

* fix the test

* break lines to avoid > 119

* Apply suggestions from code review
==

.github/workflows/model-templates.yml
src/transformers/models/bart/modeling_tf_bart.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/__init__.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/configuration.json
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/configuration_{{cookiecutter.lowercase_modelname}}.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_tf_{{cookiecutter.lowercase_modelname}}.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_{{cookiecutter.lowercase_modelname}}.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/test_modeling_tf_{{cookiecutter.lowercase_modelname}}.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/test_modeling_{{cookiecutter.lowercase_modelname}}.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/to_replace_{{cookiecutter.lowercase_modelname}}.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/tokenization_fast_{{cookiecutter.lowercase_modelname}}.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/tokenization_{{cookiecutter.lowercase_modelname}}.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/{{cookiecutter.lowercase_modelname}}.rst
templates/adding_a_new_model/cookiecutter.json
templates/adding_a_new_model/tests/encoder-bert-tokenizer.json
templates/adding_a_new_model/tests/pt-encoder-bert-tokenizer.json
templates/adding_a_new_model/tests/pt-seq-2-seq-bart-tokenizer.json
templates/adding_a_new_model/tests/standalone.json
templates/adding_a_new_model/tests/tf-encoder-bert-tokenizer.json
templates/adding_a_new_model/tests/tf-seq-2-seq-bart-tokenizer.json
tests/test_modeling_tf_bart.py
==================
e6c1f1cad;Sylvain Gugger;2020-12-22 15:42:34 -0500;Revert renaming in finetune_trainer (#9262)

==

examples/seq2seq/finetune_trainer.py
examples/seq2seq/test_finetune_trainer.py
examples/seq2seq/train_distil_marian_enro.sh
examples/seq2seq/train_distil_marian_enro_tpu.sh
examples/seq2seq/train_distilbart_cnn.sh
examples/seq2seq/train_mbart_cc25_enro.sh
examples/seq2seq/utils.py
==================
ab1775887;Sylvain Gugger;2020-12-22 14:02:26 -0500;Add speed metrics to all example scripts + template (#9260)

==

examples/language-modeling/run_clm.py
examples/language-modeling/run_mlm.py
examples/language-modeling/run_mlm_wwm.py
examples/language-modeling/run_plm.py
examples/multiple-choice/run_swag.py
examples/question-answering/run_qa.py
examples/question-answering/run_qa_beam_search.py
examples/token-classification/run_ner.py
templates/adding_a_new_example_script/{{cookiecutter.directory_name}}/run_{{cookiecutter.example_shortcut}}.py
==================
5b5f7dd09;Julien Chaumond;2020-12-22 19:52:47 +0100;[hf_api] Fix incorrect typing

==

src/transformers/hf_api.py
==================
1558d191e;Julien Plu;2020-12-22 18:07:04 +0100;Fix TF BART for saved model creation (#9252)
* Fix TF BART for saved model creation

* Apply style

* Update src/transformers/models/bart/modeling_tf_bart.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/models/bart/modeling_tf_bart.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Rework the fix

* Fix condition

* Apply style

* Fix condition

* Fix shape_list

* Apply Patrick's solution

* Apply Patrick's solution

* Rebase

* make tests pass

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
Co-authored-by: patrickvonplaten <patrick.v.platen@gmail.com>
==

src/transformers/modeling_tf_utils.py
src/transformers/models/bart/modeling_tf_bart.py
==================
37d6fb5d0;Manuel Romero;2020-12-22 17:41:23 +0100;Fix link to bertabs/README.md (#9255)

==

examples/seq2seq/README.md
==================
189c1b91a;Manuel Romero;2020-12-22 17:40:47 +0100;Fix link to old language modeling script (#9254)

==

examples/language-modeling/README.md
==================
490b39e61;Sylvain Gugger;2020-12-22 11:33:44 -0500;Seq2seq trainer (#9241)
* Add label smoothing in Trainer

* Add options for scheduler and Adafactor in Trainer

* Put Seq2SeqTrainer in the main lib

* Apply suggestions from code review

Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>
Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* Address review comments and adapt scripts

* Documentation

* Move test not using script to tests folder

Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>
Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
==

docs/source/main_classes/optimizer_schedules.rst
docs/source/main_classes/trainer.rst
examples/seq2seq/finetune_trainer.py
examples/seq2seq/test_finetune_trainer.py
examples/seq2seq/train_distil_marian_enro.sh
examples/seq2seq/train_distil_marian_enro_tpu.sh
examples/seq2seq/train_distilbart_cnn.sh
examples/seq2seq/train_mbart_cc25_enro.sh
examples/seq2seq/utils.py
src/transformers/__init__.py
src/transformers/optimization.py
src/transformers/trainer.py
src/transformers/trainer_pt_utils.py
src/transformers/trainer_seq2seq.py
src/transformers/trainer_utils.py
src/transformers/training_args.py
src/transformers/training_args_seq2seq.py
src/transformers/utils/dummy_pt_objects.py
tests/test_trainer_seq2seq.py
tests/test_trainer_utils.py
==================
1fc711918;Sylvain Gugger;2020-12-22 11:12:58 -0500;Fix script that check objects are documented (#9259)

==

docs/source/model_doc/transformerxl.rst
utils/check_repo.py
==================
e9d77ccd5;Patrick von Platen;2020-12-22 17:00:04 +0100;[EncoderDecoder] Make tests more aggressive (#9256)
* add tests

* make style and fix bart bug

* fix bart past key value edge case

* correct tf bart test

* fix gpt2 tf

* fix t5 test
==

src/transformers/models/bart/modeling_bart.py
src/transformers/models/bart/modeling_tf_bart.py
tests/test_modeling_bart.py
tests/test_modeling_encoder_decoder.py
tests/test_modeling_gpt2.py
tests/test_modeling_t5.py
tests/test_modeling_tf_bart.py
tests/test_modeling_tf_gpt2.py
tests/test_modeling_tf_t5.py
==================
ec07da65e;Sylvain Gugger;2020-12-21 15:23:40 -0500;Update the README of the text classification example (#9237)
* Update the README of the text classification example

* Update examples/README.md

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* Adapt comment from review

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
==

examples/README.md
examples/text-classification/README.md
==================
4eef5889a;Teven;2020-12-21 21:19:41 +0100;Adding performer fine-tuning research exampke (#9239)
* added run_mlm_performer.py research example

* make styke

* make styke

* Added a README !
==

examples/research_projects/performer/README.md
examples/research_projects/performer/full_script.sh
examples/research_projects/performer/modeling_flax_performer.py
examples/research_projects/performer/modeling_flax_performer_utils.py
examples/research_projects/performer/run_mlm_performer.py
examples/research_projects/performer/sanity_script.sh
==================
9a12b9696;Patrick von Platen;2020-12-21 15:41:34 +0100;[MPNet] Add slow to fast tokenizer converter (#9233)
* add converter

* delet unnecessary comments
==

src/transformers/convert_slow_tokenizer.py
tests/test_tokenization_mpnet.py
==================
f4432b7e0;Suraj Patil;2020-12-21 19:56:46 +0530;add base model classes to  bart subclassed models (#9230)
* add base model classes to  bart subclassed models

* add doc
==

docs/source/model_doc/blenderbot.rst
docs/source/model_doc/mbart.rst
docs/source/model_doc/pegasus.rst
src/transformers/__init__.py
src/transformers/models/auto/modeling_auto.py
src/transformers/models/blenderbot/__init__.py
src/transformers/models/blenderbot/modeling_blenderbot.py
src/transformers/models/mbart/__init__.py
src/transformers/models/mbart/modeling_mbart.py
src/transformers/models/pegasus/__init__.py
src/transformers/models/pegasus/modeling_pegasus.py
src/transformers/utils/dummy_pt_objects.py
tests/test_modeling_blenderbot.py
tests/test_modeling_mbart.py
tests/test_modeling_pegasus.py
==================
08abdabda;TobiasNorlund;2020-12-21 14:05:23 +0100;Fixed beam search generation for GPT2 and T5 (#9219)

==

src/transformers/generation_utils.py
tests/test_modeling_common.py
==================
161a6461d;Julien Plu;2020-12-21 13:52:16 +0100;Fix TF template (#9234)

==

templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_tf_{{cookiecutter.lowercase_modelname}}.py
==================
5a8a4eb18;Julien Plu;2020-12-21 13:10:15 +0100;Improve BERT-like models performance with better self attention (#9124)
* Improve BERT-like models attention layers

* Apply style

* Put back error raising instead of assert

* Update template

* Fix copies

* Apply raising valueerror in MPNet

* Restore the copy check for the Intermediate layer in Longformer

* Update longformer
==

setup.py
src/transformers/dependency_versions_table.py
src/transformers/models/bert/modeling_tf_bert.py
src/transformers/models/electra/modeling_tf_electra.py
src/transformers/models/longformer/modeling_tf_longformer.py
src/transformers/models/mpnet/modeling_tf_mpnet.py
src/transformers/models/roberta/modeling_tf_roberta.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_tf_{{cookiecutter.lowercase_modelname}}.py
==================
6b034309c;Patrick von Platen;2020-12-21 10:41:34 +0100;fix warning (#9231)

==

src/transformers/models/t5/modeling_t5.py
==================
a4b21cdd2;Amog Kamsetty;2020-12-21 01:39:30 -0800;[RAG] Add Ray implementation for distributed retrieval (#9197)
* wip

* wip

* wip

* wip

* wip

* wip

* wip

* wip

* uncomment

* uncomment

* wip

* updates

* add docstring

* updates

* fix arg

* fixes

* add unit tests

* update readme

* update readme

* update finetune script

* update test

* add test

* add ray to test dependencies

* separate ray and ray tune

* formatting

* shutdown ray at end of test

* fix tests

* formatting

* formatting

* even more formatting

* address comments

* formatting

* add files

* Update examples/research_projects/rag/test_distributed_retriever.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* address comments

* addressing comments

Co-authored-by: Ubuntu <ubuntu@ip-172-31-21-208.us-west-2.compute.internal>
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

examples/legacy/pytorch-lightning/requirements.txt
examples/research_projects/rag/README.md
examples/research_projects/rag/_test_finetune_rag.py
examples/research_projects/rag/distributed_pytorch_retriever.py
examples/research_projects/rag/distributed_ray_retriever.py
examples/research_projects/rag/finetune_rag.py
examples/research_projects/rag/finetune_rag.sh
examples/research_projects/rag/finetune_rag_ray.sh
examples/research_projects/rag/test_distributed_retriever.py
src/transformers/__init__.py
src/transformers/integrations.py
src/transformers/models/rag/retrieval_rag.py
src/transformers/trainer.py
src/transformers/trainer_utils.py
==================
f38c4ad30;Stas Bekman;2020-12-20 10:28:28 -0800;better logging and help (#9203)

==

examples/seq2seq/finetune_trainer.py
examples/seq2seq/utils.py
==================
e0e255be1;sandip;2020-12-19 19:14:04 +0530;Added TF TransfoXL Sequence Classification (#9169)
* TF Transfoxl seq classification

* Update test_modeling_tf_transfo_xl.py

Added num_labels to config level

* TF Transfoxl seq classification

* Update test_modeling_tf_transfo_xl.py

Added num_labels to config level

* code refactor

* code refactor

* code refator
==

src/transformers/__init__.py
src/transformers/models/auto/modeling_tf_auto.py
src/transformers/models/transfo_xl/__init__.py
src/transformers/models/transfo_xl/modeling_tf_transfo_xl.py
src/transformers/utils/dummy_tf_objects.py
tests/test_modeling_tf_transfo_xl.py
==================
6b850b671;Stas Bekman;2020-12-18 17:09:30 -0800;[run_glue] add speed metrics (#9198)
* add speed metrics

* suggestions
==

examples/text-classification/run_glue.py
==================
3ff5e8955;Stas Bekman;2020-12-18 16:03:26 -0800;[t5 doc] typos (#9199)
* [t5 doc] typos

a few run away backticks

@sgugger

* style
==

docs/source/model_doc/t5.rst
==================
291974c65;Aleksey Tikhonov;2020-12-18 22:32:10 +0100;GPT-model attention heads pruning example (#9189)
* Pruning for GPT attn heads

* The code formatted according to the transformers requirements

* Update run_prune_gpt.py

* Update run_prune_gpt.py
==

examples/research_projects/bertology/run_prune_gpt.py
==================
1198ba8fb;Sylvain Gugger;2020-12-18 15:10:39 -0500;Add timing inside Trainer (#9196)
* Add timing inside Trainer

* Fix tests

* Add n_objs for train

* Sort logs
==

examples/seq2seq/finetune_trainer.py
examples/test_examples.py
src/transformers/trainer.py
src/transformers/trainer_utils.py
src/transformers/training_args.py
tests/test_trainer.py
==================
9a25c5bd3;Sylvain Gugger;2020-12-18 14:19:24 -0500;Add new run_swag example (#9175)
* Add new run_swag example

* Add check

* Add sample

* Apply suggestions from code review

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* Very important change to make Lysandre happy

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
==

examples/README.md
examples/legacy/multiple_choice/run_multiple_choice.py
examples/legacy/multiple_choice/utils_multiple_choice.py
examples/multiple-choice/README.md
examples/multiple-choice/run_swag.py
examples/test_examples.py
tests/fixtures/tests_samples/swag/sample.json
==================
3e56e2ce0;Sylvain Gugger;2020-12-18 10:11:07 -0500;Fix typo

==

docs/source/preprocessing.rst
==================
077a5dce3;Manuel Romero;2020-12-18 15:12:10 +0100;Fix link to old SQUAD fine-tuning script (#9181)

==

examples/question-answering/README.md
==================
84d5879ea;Stas Bekman;2020-12-18 05:55:55 -0800;[setup] correct transformers version format (#9176)
setuptools has a pretty fixed expectation of version numbers.

This PR fixes the dev version number and adds a comment with correct formats for the future editors

This fix removes this warning on `make fixup|style|etc` or any other time `setup.py` is being run.
```
setuptools/dist.py:452: UserWarning: Normalizing '4.2.0dev0' to '4.2.0.dev0'
  warnings.warn(tmpl.format(**locals()))
```
and the alternative:
```
/setuptools/dist.py:452: UserWarning: Normalizing '4.0.0-rc-1' to '4.0.0rc1
```

Fixes: #8749

@LysandreJik, @sgugger
==

setup.py
==================
fd7b6a527;Wissam Antoun;2020-12-18 14:53:23 +0200;fixed JSON error in run_qa with fp16 (#9186)

==

examples/question-answering/utils_qa.py
==================
66a14a2f6;Manuel Romero;2020-12-18 01:50:01 +0100;Fix link to old NER fine-tuning script (#9182)

==

examples/token-classification/README.md
==================
f06d0fadc;Stas Bekman;2020-12-17 16:49:11 -0800;[trainer] apex fixes and tests (#9180)

==

examples/seq2seq/test_finetune_trainer.py
src/transformers/trainer.py
==================
467e9158b;sandip;2020-12-18 04:40:57 +0530;Added TF CTRL Sequence Classification (#9151)
* Added TF CTRL Sequence Classification

* code refactor
==

docs/source/model_doc/ctrl.rst
src/transformers/__init__.py
src/transformers/models/auto/modeling_tf_auto.py
src/transformers/models/ctrl/__init__.py
src/transformers/models/ctrl/modeling_tf_ctrl.py
src/transformers/utils/dummy_tf_objects.py
tests/test_modeling_tf_ctrl.py
==================
63841c559;Stas Bekman;2020-12-17 14:24:03 -0800;add tests for the new sharded ddp fairscale integration (#9177)

==

examples/seq2seq/test_finetune_trainer.py
==================
bf713cdec;Lysandre;2020-12-17 11:29:31 -0500;setup.py development version

==

setup.py
src/transformers/__init__.py
==================
bd40345d3;Lysandre;2020-12-17 11:28:38 -0500;v4.1.1 docs

==

.circleci/deploy.sh
docs/source/_static/js/custom.js
==================
bfa4ccf77;Lysandre;2020-12-17 11:25:49 -0500;Release: v4.1.1

==

docs/source/conf.py
setup.py
src/transformers/__init__.py
==================
e0790cca7;Lysandre;2020-12-17 11:24:23 -0500;Fix TAPAS doc

==

docs/source/model_doc/tapas.rst
==================
6d2e864db;Sylvain Gugger;2020-12-17 11:23:21 -0500;Put all models in the constants (#9170)
* Put all models in the constants

* Add Google AI mention in the main README
==

README.md
docs/source/index.rst
src/transformers/models/tapas/tokenization_tapas.py
==================
f83d9c8da;Lysandre;2020-12-17 10:16:07 -0500;v4.1.0 docs

==

.circleci/deploy.sh
docs/source/_static/js/custom.js
==================
f5438ab8a;Lysandre;2020-12-17 10:04:55 -0500;Release: v4.1.0

==

docs/source/conf.py
setup.py
src/transformers/__init__.py
==================
ac2c7e398;Lysandre;2020-12-17 09:47:19 -0500;Remove erroneous character

==

docs/source/model_doc/tapas.rst
==================
77d6941e6;Sylvain Gugger;2020-12-17 09:44:24 -0500;Fix gradient clipping for Sharded DDP (#9168)
* Fix gradient clipping for Sharded DDP

* Fix typos in comments
==

src/transformers/trainer.py
==================
1aca3d6af;Lysandre Debut;2020-12-17 09:34:06 -0500;Add disclaimer to TAPAS rst file (#9167)
Co-authored-by: sgugger <sylvain.gugger@gmail.com>

Co-authored-by: sgugger <sylvain.gugger@gmail.com>
==

docs/source/model_doc/tapas.rst
==================
dc9f24544;Lysandre;2020-12-16 13:48:52 -0500;Torch scatter with torch 1.7.0

==

.github/workflows/self-push.yml
==================
9a6718534;Sylvain Gugger;2020-12-16 13:47:48 -0500;Experimental support for fairscale ShardedDDP (#9139)
* Experimental stupport for fairscale ShardedDDP

* Add import error if fairscale not available

* Address review comments

* Fix seq2seq trainer
==

examples/seq2seq/seq2seq_trainer.py
src/transformers/integrations.py
src/transformers/trainer.py
src/transformers/training_args.py
==================
1c1a2ffbf;Lysandre Debut;2020-12-16 12:31:50 -0500;TableQuestionAnsweringPipeline (#9145)
* AutoModelForTableQuestionAnswering

* TableQuestionAnsweringPipeline

* Apply suggestions from Patrick's code review

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* Sylvain and Patrick comments

* Better PyTorch/TF error message

* Add integration tests

* Argument Handler naming

Co-authored-by: patrickvonplaten <patrick.v.platen@gmail.com>

* Fix docs to appease the documentation gods

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
==

docs/source/main_classes/pipelines.rst
src/transformers/__init__.py
src/transformers/file_utils.py
src/transformers/models/tapas/configuration_tapas.py
src/transformers/models/tapas/tokenization_tapas.py
src/transformers/pipelines.py
src/transformers/testing_utils.py
tests/test_pipelines_table_question_answering.py
==================
07384baf7;Lysandre Debut;2020-12-16 12:14:33 -0500;AutoModelForTableQuestionAnswering (#9154)
* AutoModelForTableQuestionAnswering

* Update src/transformers/models/auto/modeling_auto.py

* Style
==

docs/source/model_doc/auto.rst
src/transformers/__init__.py
src/transformers/dependency_versions_table.py
src/transformers/models/auto/__init__.py
src/transformers/models/auto/modeling_auto.py
src/transformers/utils/dummy_pt_objects.py
tests/test_modeling_auto.py
tests/test_modeling_tapas.py
==================
34334662d;Hayden Housen;2020-12-16 11:06:14 -0500;Add message to documentation that longformer doesn't support token_type_ids (#9152)
* Add message to documentation that longformer doesn't support token_type_ids

* Format changes
==

docs/source/model_doc/longformer.rst
==================
2f918defa;Lysandre;2020-12-16 10:26:13 -0500;hotfix torch scatter version

==

.github/workflows/self-push.yml
==================
4d4897352;Sylvain Gugger;2020-12-16 10:24:31 -0500;Update notebook table and transformers intro notebook (#9136)

==

examples/README.md
notebooks/02-transformers.ipynb
notebooks/README.md
==================
fb650df85;Julien Chaumond;2020-12-16 16:09:57 +0100;Support for private models from huggingface.co (#9141)
* minor wording tweaks

* Create private model repo + exist_ok flag

* file_utils: `use_auth_token`

* Update src/transformers/file_utils.py

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* Propagate doc from @sgugger

Co-Authored-By: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

src/transformers/configuration_utils.py
src/transformers/file_utils.py
src/transformers/hf_api.py
src/transformers/modeling_flax_utils.py
src/transformers/modeling_tf_utils.py
src/transformers/modeling_utils.py
src/transformers/pipelines.py
src/transformers/tokenization_utils_base.py
==================
c69d19faa;AndreaSottana;2020-12-16 14:21:42 +0000;DistilBertForSequenceClassification (#9148)
fix small shape error in comments
==

src/transformers/models/distilbert/modeling_distilbert.py
==================
640e6fe19;Patrick von Platen;2020-12-16 13:03:32 +0100;[Flax] Align FlaxBertForMaskedLM with BertForMaskedLM, implement from_pretrained, init (#9054)
* save intermediate

* save intermediate

* save intermediate

* correct flax bert model file

* new module / model naming

* make style

* almost finish BERT

* finish roberta

* make fix-copies

* delete keys file

* last refactor

* fixes in run_mlm_flax.py

* remove pooled from run_mlm_flax.py`

* fix gelu | gelu_new

* remove Module from inits

* splits

* dirty print

* preventing warmup_steps == 0

* smaller splits

* make fix-copies

* dirty print

* dirty print

* initial_evaluation argument

* declaration order fix

* proper model initialization/loading

* proper initialization

* run_mlm_flax improvements: improper model inputs bugfix + automatic dataset splitting + tokenizers parallelism warning + avoiding warmup_steps=0 bug

* removed tokenizers warning hack, fixed model re-initialization

* reverted training_args.py changes

* fix flax from pretrained

* improve test in flax

* apply sylvains tips

* update init

* make 0.3.0 compatible

* revert tevens changes

* revert tevens changes 2

* finalize revert

* fix bug

* add docs

* add pretrained to init

* Update src/transformers/modeling_flax_utils.py

* fix copies

* final improvements

Co-authored-by: TevenLeScao <teven.lescao@gmail.com>
==

docs/source/main_classes/model.rst
examples/language-modeling/run_mlm_flax.py
setup.py
src/transformers/__init__.py
src/transformers/dependency_versions_table.py
src/transformers/file_utils.py
src/transformers/modeling_flax_utils.py
src/transformers/models/auto/modeling_flax_auto.py
src/transformers/models/bert/modeling_flax_bert.py
src/transformers/models/roberta/modeling_flax_roberta.py
src/transformers/utils/dummy_flax_objects.py
tests/test_modeling_flax_bert.py
tests/test_modeling_flax_common.py
tests/test_modeling_flax_roberta.py
==================
51adb97cd;Sylvain Gugger;2020-12-15 17:14:37 -0500;Fix fp16_backend field

==

src/transformers/training_args.py
==================
1551e2dc6;NielsRogge;2020-12-15 23:08:49 +0100;[WIP] Tapas v4 (tres) (#9117)
* First commit: adding all files from tapas_v3

* Fix multiple bugs including soft dependency and new structure of the library

* Improve testing by adding torch_device to inputs and adding dependency on scatter

* Use Python 3 inheritance rather than Python 2

* First draft model cards of base sized models

* Remove model cards as they are already on the hub

* Fix multiple bugs with integration tests

* All model integration tests pass

* Remove print statement

* Add test for convert_logits_to_predictions method of TapasTokenizer

* Incorporate suggestions by Google authors

* Fix remaining tests

* Change position embeddings sizes to 512 instead of 1024

* Comment out positional embedding sizes

* Update PRETRAINED_VOCAB_FILES_MAP and PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES

* Added more model names

* Fix truncation when no max length is specified

* Disable torchscript test

* Make style & make quality

* Quality

* Address CI needs

* Test the Masked LM model

* Fix the masked LM model

* Truncate when overflowing

* More much needed docs improvements

* Fix some URLs

* Some more docs improvements

* Test PyTorch scatter

* Set to slow + minify

* Calm flake8 down

* First commit: adding all files from tapas_v3

* Fix multiple bugs including soft dependency and new structure of the library

* Improve testing by adding torch_device to inputs and adding dependency on scatter

* Use Python 3 inheritance rather than Python 2

* First draft model cards of base sized models

* Remove model cards as they are already on the hub

* Fix multiple bugs with integration tests

* All model integration tests pass

* Remove print statement

* Add test for convert_logits_to_predictions method of TapasTokenizer

* Incorporate suggestions by Google authors

* Fix remaining tests

* Change position embeddings sizes to 512 instead of 1024

* Comment out positional embedding sizes

* Update PRETRAINED_VOCAB_FILES_MAP and PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES

* Added more model names

* Fix truncation when no max length is specified

* Disable torchscript test

* Make style & make quality

* Quality

* Address CI needs

* Test the Masked LM model

* Fix the masked LM model

* Truncate when overflowing

* More much needed docs improvements

* Fix some URLs

* Some more docs improvements

* Add add_pooling_layer argument to TapasModel

Fix comments by @sgugger and @patrickvonplaten

* Fix issue in docs + fix style and quality

* Clean up conversion script and add task parameter to TapasConfig

* Revert the task parameter of TapasConfig

Some minor fixes

* Improve conversion script and add test for absolute position embeddings

* Improve conversion script and add test for absolute position embeddings

* Fix bug with reset_position_index_per_cell arg of the conversion cli

* Add notebooks to the examples directory and fix style and quality

* Apply suggestions from code review

* Move from `nielsr/` to `google/` namespace

* Apply Sylvain's comments

Co-authored-by: sgugger <sylvain.gugger@gmail.com>

Co-authored-by: Rogge Niels <niels.rogge@howest.be>
Co-authored-by: LysandreJik <lysandre.debut@reseau.eseo.fr>
Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
Co-authored-by: sgugger <sylvain.gugger@gmail.com>
==

.circleci/config.yml
.github/workflows/self-push.yml
README.md
docs/source/index.rst
docs/source/model_doc/tapas.rst
model_cards/google/tapas-base/README.md
notebooks/README.md
src/transformers/__init__.py
src/transformers/file_utils.py
src/transformers/models/auto/configuration_auto.py
src/transformers/models/auto/modeling_auto.py
src/transformers/models/auto/tokenization_auto.py
src/transformers/models/tapas/__init__.py
src/transformers/models/tapas/configuration_tapas.py
src/transformers/models/tapas/convert_tapas_original_tf_checkpoint_to_pytorch.py
src/transformers/models/tapas/modeling_tapas.py
src/transformers/models/tapas/tokenization_tapas.py
src/transformers/testing_utils.py
src/transformers/utils/dummy_pt_objects.py
tests/test_modeling_tapas.py
tests/test_tokenization_common.py
tests/test_tokenization_tapas.py
==================
ad895af98;Sylvain Gugger;2020-12-15 16:38:10 -0500;Add possibility to switch between APEX and AMP in Trainer (#9137)
* Add possibility to switch between APEX and AMP in Trainer

* Update src/transformers/training_args.py

Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>

* Address review comments

* Update src/transformers/training_args.py

Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>

Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>
==

src/transformers/trainer.py
src/transformers/training_args.py
tests/test_trainer.py
==================
0b2f46fa9;Lysandre Debut;2020-12-15 16:03:59 -0500;Add large model config (#9140)

==

tests/test_modeling_t5.py
==================
2a7e8e160;Teven;2020-12-15 22:02:43 +0100;[Examples] Add automatic dataset splitting in language-modeling examples (#9133)
* replaced jnp.split + removing textual model inputs + ensuring warmup_steps > 0

* Add automatic dataset splitting in language-modeling examples
==

examples/language-modeling/run_clm.py
examples/language-modeling/run_mlm.py
examples/language-modeling/run_mlm_flax.py
examples/language-modeling/run_mlm_wwm.py
examples/language-modeling/run_plm.py
==================
e77174977;Julien Plu;2020-12-15 21:16:56 +0100;Fix add order (#9129)

==

src/transformers/models/transfo_xl/modeling_tf_transfo_xl.py
==================
18ecd36f6;Patrick von Platen;2020-12-15 19:04:31 +0100;Fix Bart Shift (#9135)
* correct mistake in order

* fix tensor copy

* clone tensor correctly
==

src/transformers/models/bart/modeling_bart.py
==================
d018622d8;Patrick von Platen;2020-12-15 18:38:31 +0100;correct mistake in order (#9134)

==

src/transformers/models/bart/modeling_bart.py
==================
80bdb9c31;Patrick von Platen;2020-12-15 18:17:17 +0100;fix bart loss masking (#9131)

==

src/transformers/models/bart/modeling_bart.py
==================
3caba8d35;Manbish;2020-12-15 12:12:28 -0500;Fix typo in trainer_tf.py (#9132)

==

src/transformers/trainer_tf.py
==================
abc573f51;Patrick von Platen;2020-12-15 17:31:28 +0100;[TF Bart] Refactor TFBart (#9029)
* reorder file

* delete unnecesarry function

* make style

* save intermediate

* fix attention masks

* correct tf bart past key values

* solve merge conflict bug

* correct tensor dims

* save intermediate tf

* change attn layer

* fix typo re-order past

* inputs_embeds

* make fix copies

* finish tests

* fix graph mode

* appyl lysandres suggestions
==

src/transformers/__init__.py
src/transformers/models/bart/__init__.py
src/transformers/models/bart/modeling_bart.py
src/transformers/models/bart/modeling_tf_bart.py
src/transformers/models/t5/modeling_tf_t5.py
src/transformers/utils/dummy_tf_objects.py
tests/test_modeling_bart.py
tests/test_modeling_tf_bart.py
==================
389aba34b;sandip;2020-12-15 21:57:08 +0530;Added TF OpenAi GPT1 Sequence Classification (#9105)
* TF OpenAI GPT Sequence Classification

* Update src/transformers/models/openai/modeling_tf_openai.py

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
==

docs/source/model_doc/gpt.rst
src/transformers/__init__.py
src/transformers/models/auto/modeling_tf_auto.py
src/transformers/models/openai/__init__.py
src/transformers/models/openai/modeling_tf_openai.py
src/transformers/utils/dummy_tf_objects.py
tests/test_modeling_tf_openai.py
==================
ef2d4cd44;Julien Plu;2020-12-15 16:10:46 +0100;Fix tf2.4 (#9120)
* Fix tests for TF 2.4

* Remove <2.4 limitation

* Add version condition

* Update tests/test_optimization_tf.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update tests/test_optimization_tf.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update tests/test_optimization_tf.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

setup.py
tests/test_modeling_tf_common.py
tests/test_optimization_tf.py
==================
6ccea0486;Lysandre Debut;2020-12-15 09:51:12 -0500;Fix T5 model parallel tes (#9107)
k
==

tests/test_modeling_t5.py
==================
59da3f270;Lysandre Debut;2020-12-15 09:15:49 -0500;Fix stack overflow (#9114)

==

src/transformers/tokenization_utils_base.py
==================
14c79c3e3;Stas Bekman;2020-12-15 06:10:41 -0800;native amp leak fix landed in 1.7.1 (#9115)
update README with good news that the leak fix has been applied to pytorch-1.7.1.
==

examples/seq2seq/README.md
==================
ed1845ef4;lewtun;2020-12-15 15:00:19 +0100;Clarify use of TrainingArguments.disable_tqdm in Jupyter Notebooks   (#9076)
* Clarify impact of disable_tqdm on Jupyter Notebooks

* Add weblink to argparse

* Replace "dev set" with more common "validation set" in do_eval

* Tweak prediction_loss_only

* Tweak description of Adam hyperparameters

* Add weblink to TensorBoard

* Capitalise apex

* Tweak local_rank description

* Add weblink for wandb

* Replace nlp with datasets

* Tweak grammar in model_parallel

* Capitalise apex

* Update TensorFlow training args to match PyTorch ones

* Fix style

* Fix underscore in weblink

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Fix underscore in weblink

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Fix underscore in weblink

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Fix underscore in weblink

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Add obj to datasets.Dataset

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

src/transformers/training_args.py
src/transformers/training_args_tf.py
==================
44c340f45;Yoshitomo Matsubara;2020-12-15 05:46:55 -0800;fix a bug in eval_batch_retrieval (#9089)

==

examples/research_projects/rag/eval_rag.py
==================
c19d04623;Stas Bekman;2020-12-14 17:45:33 -0800;[finetune_trainer] enhancements and fixes (#9042)
* trainer and finetune_trainer enhancements and fixes

* add fallback default

* move the fixing of incorrect keys back into finetune trainer

* s/eval/val/ to match the split

* trainer can now use a different prefix than eval_ for metrics

* document new arg

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* use 'eval' as the default for metric_key_prefix

* complete adjust var names + disambiguate

* fix logger

* add clarifying comment

* add clarifying comment

* style

* Apply suggestions from code review

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* Update src/transformers/trainer.py

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* complete removal of optional for metric_key_prefix

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
==

examples/seq2seq/finetune_trainer.py
examples/seq2seq/utils.py
src/transformers/trainer.py
==================
251eb70c9;Sylvain Gugger;2020-12-14 16:17:04 -0500;Also pin TF CPU

==

setup.py
==================
e4ef57a9b;Sylvain Gugger;2020-12-14 16:06:30 -0500;Pin TF to < 2.4

==

setup.py
==================
df3f4d2ae;Julien Plu;2020-12-14 18:47:00 +0100;Fix T5 and BART for TF (#9063)
* Fix T5 for graphe compilation+execution

* Fix BART

* Fix import

* Fix naming

* fix attribute name

* Oops

* fix import

* fix tests

* fix tests

* Update test

* Add mising import

* Address Patrick's comments

* Style

* Address Patrick's comment
==

docs/source/internal/modeling_utils.rst
src/transformers/modeling_tf_pytorch_utils.py
src/transformers/modeling_tf_utils.py
src/transformers/models/bart/modeling_tf_bart.py
src/transformers/models/t5/modeling_tf_t5.py
tests/test_modeling_tf_bart.py
tests/test_modeling_tf_common.py
tests/test_modeling_tf_t5.py
==================
a9c8bff72;Ahmed Elnaggar;2020-12-14 18:00:45 +0100;Add parallelization support for T5EncoderModel (#9082)
* add model parallelism to T5EncoderModel

add model parallelism to T5EncoderModel

* remove decoder from T5EncoderModel parallelize

* uodate T5EncoderModel docs

* Extend T5ModelTest for T5EncoderModel

* fix T5Stask using range for get_device_map

* fix style

Co-authored-by: Ahmed Elnaggar <elnaggar@rostlab.informatik.tu-muenchen.de>
==

docs/source/model_doc/t5.rst
src/transformers/models/t5/modeling_t5.py
tests/test_modeling_t5.py
==================
b00eb4fb0;Stas Bekman;2020-12-14 07:34:59 -0800;Testing Experimental CI Features (#9070)

==

docs/source/testing.rst
==================
74daf1f95;Simon Brandeis;2020-12-14 15:12:27 +0100;Fixed a broken link in documentation (#9101)

==

docs/source/bertology.rst
==================
d6af344c9;Navjot;2020-12-14 06:02:54 -0800;correct var name in TrainingArguments docstring (#9096)

==

src/transformers/training_args.py
==================
fa1ddced9;Patrick von Platen;2020-12-14 12:32:26 +0100;[RAG, Bart] Align RAG, Bart cache with T5 and other models of transformers (#9098)
* fix rag

* fix slow test

* fix past in bart
==

src/transformers/models/bart/modeling_bart.py
src/transformers/models/rag/modeling_rag.py
tests/test_modeling_rag.py
==================
6587cf9f8;Lysandre Debut;2020-12-14 00:39:55 -0500;Patch *ForCausalLM model (#9092)

==

templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_tf_{{cookiecutter.lowercase_modelname}}.py
==================
51d9c569f;Julien Plu;2020-12-14 05:05:24 +0100;Fix embeddings resizing in TF models (#8657)
* Resize the biases in same time than the embeddings

* Trigger CI

* Biases are not reset anymore

* Remove get_output_embeddings + better LM model detection in generation utils

* Apply style

* First test on BERT

* Update docstring + new name

* Apply the new resizing logic to all the models

* fix tests

* Apply style

* Update the template

* Fix naming

* Fix naming

* Apply style

* Apply style

* Remove unused import

* Revert get_output_embeddings

* Trigger CI

* Update num parameters

* Restore get_output_embeddings in TFPretrainedModel and add comments

* Style

* Add decoder resizing

* Style

* Fix tests

* Separate bias and decoder resize

* Fix tests

* Fix tests

* Apply style

* Add bias resizing in MPNet

* Trigger CI

* Apply style
==

src/transformers/modeling_tf_utils.py
src/transformers/models/albert/modeling_tf_albert.py
src/transformers/models/bart/modeling_tf_bart.py
src/transformers/models/bert/modeling_tf_bert.py
src/transformers/models/ctrl/modeling_tf_ctrl.py
src/transformers/models/distilbert/modeling_tf_distilbert.py
src/transformers/models/electra/modeling_tf_electra.py
src/transformers/models/flaubert/modeling_tf_flaubert.py
src/transformers/models/funnel/modeling_tf_funnel.py
src/transformers/models/longformer/modeling_tf_longformer.py
src/transformers/models/lxmert/modeling_tf_lxmert.py
src/transformers/models/mobilebert/modeling_tf_mobilebert.py
src/transformers/models/mpnet/modeling_tf_mpnet.py
src/transformers/models/roberta/modeling_tf_roberta.py
src/transformers/models/xlm/modeling_tf_xlm.py
src/transformers/models/xlnet/modeling_tf_xlnet.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_tf_{{cookiecutter.lowercase_modelname}}.py
tests/test_modeling_tf_albert.py
tests/test_modeling_tf_bart.py
tests/test_modeling_tf_bert.py
tests/test_modeling_tf_blenderbot.py
tests/test_modeling_tf_common.py
tests/test_modeling_tf_gpt2.py
tests/test_modeling_tf_lxmert.py
tests/test_modeling_tf_marian.py
tests/test_modeling_tf_mbart.py
tests/test_modeling_tf_mobilebert.py
tests/test_modeling_tf_openai.py
tests/test_modeling_tf_pegasus.py
tests/test_modeling_tf_t5.py
tests/test_modeling_tf_transfo_xl.py
==================
3552d0e0d;Julien Chaumond;2020-12-12 00:24:42 +0100;[model_cards] Migrate cards from this repo to model repos on huggingface.co (#9013)
* rm all model cards

* Update the .rst

@sgugger it is still not super crystal clear/streamlined so let me know if any ideas to make it simpler

* Add a rootlevel README.md with simple instructions/context

* Update docs/source/model_sharing.rst

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* make style

* rm all model cards

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
==

docs/source/model_sharing.rst
model_cards/Cinnamon/electra-small-japanese-discriminator/README.md
model_cards/Cinnamon/electra-small-japanese-generator/README.md
model_cards/DJSammy/bert-base-danish-uncased_BotXO,ai/README.md
model_cards/DeepPavlov/bert-base-bg-cs-pl-ru-cased/README.md
model_cards/DeepPavlov/bert-base-cased-conversational/README.md
model_cards/DeepPavlov/bert-base-multilingual-cased-sentence/README.md
model_cards/DeepPavlov/rubert-base-cased-conversational/README.md
model_cards/DeepPavlov/rubert-base-cased-sentence/README.md
model_cards/DeepPavlov/rubert-base-cased/README.md
model_cards/Geotrend/bert-base-15lang-cased/README.md
model_cards/Geotrend/bert-base-ar-cased/README.md
model_cards/Geotrend/bert-base-bg-cased/README.md
model_cards/Geotrend/bert-base-de-cased/README.md
model_cards/Geotrend/bert-base-el-cased/README.md
model_cards/Geotrend/bert-base-en-ar-cased/README.md
model_cards/Geotrend/bert-base-en-bg-cased/README.md
model_cards/Geotrend/bert-base-en-cased/README.md
model_cards/Geotrend/bert-base-en-de-cased/README.md
model_cards/Geotrend/bert-base-en-el-cased/README.md
model_cards/Geotrend/bert-base-en-es-cased/README.md
model_cards/Geotrend/bert-base-en-fr-cased/README.md
model_cards/Geotrend/bert-base-en-hi-cased/README.md
model_cards/Geotrend/bert-base-en-ru-cased/README.md
model_cards/Geotrend/bert-base-en-sw-cased/README.md
model_cards/Geotrend/bert-base-en-th-cased/README.md
model_cards/Geotrend/bert-base-en-tr-cased/README.md
model_cards/Geotrend/bert-base-en-ur-cased/README.md
model_cards/Geotrend/bert-base-en-vi-cased/README.md
model_cards/Geotrend/bert-base-en-zh-cased/README.md
model_cards/Geotrend/bert-base-es-cased/README.md
model_cards/Geotrend/bert-base-fr-cased/README.md
model_cards/Geotrend/bert-base-hi-cased/README.md
model_cards/Geotrend/bert-base-ru-cased/README.md
model_cards/Geotrend/bert-base-sw-cased/README.md
model_cards/Geotrend/bert-base-th-cased/README.md
model_cards/Geotrend/bert-base-tr-cased/README.md
model_cards/Geotrend/bert-base-ur-cased/README.md
model_cards/Geotrend/bert-base-vi-cased/README.md
model_cards/Geotrend/bert-base-zh-cased/README.md
model_cards/Hate-speech-CNERG/dehatebert-mono-arabic/README.md
model_cards/Hate-speech-CNERG/dehatebert-mono-english/README.md
model_cards/Hate-speech-CNERG/dehatebert-mono-french/README.md
model_cards/Hate-speech-CNERG/dehatebert-mono-german/README.md
model_cards/Hate-speech-CNERG/dehatebert-mono-indonesian/README.md
model_cards/Hate-speech-CNERG/dehatebert-mono-italian/README.md
model_cards/Hate-speech-CNERG/dehatebert-mono-polish/README.md
model_cards/Hate-speech-CNERG/dehatebert-mono-portugese/README.md
model_cards/Hate-speech-CNERG/dehatebert-mono-spanish/README.md
model_cards/HooshvareLab/bert-base-parsbert-armanner-uncased/README.md
model_cards/HooshvareLab/bert-base-parsbert-ner-uncased/README.md
model_cards/HooshvareLab/bert-base-parsbert-peymaner-uncased/README.md
model_cards/HooshvareLab/bert-base-parsbert-uncased/README.md
model_cards/HooshvareLab/bert-fa-base-uncased/README.md
model_cards/KB/albert-base-swedish-cased-alpha/README.md
model_cards/KB/bert-base-swedish-cased-ner/README.md
model_cards/KB/bert-base-swedish-cased/README.md
model_cards/LorenzoDeMattei/GePpeTto/README.md
model_cards/Michau/t5-base-en-generate-headline/README.md
model_cards/MoseliMotsoehli/TswanaBert/README.md
model_cards/MoseliMotsoehli/zuBERTa/README.md
model_cards/Musixmatch/umberto-commoncrawl-cased-v1/README.md
model_cards/Musixmatch/umberto-wikipedia-uncased-v1/README.md
model_cards/NLP4H/ms_bert/README.md
model_cards/Naveen-k/KanBERTo/README.md
model_cards/NeuML/bert-small-cord19-squad2/README.md
model_cards/NeuML/bert-small-cord19/README.md
model_cards/NeuML/bert-small-cord19qa/README.md
model_cards/NlpHUST/vibert4news-base-cased/README.md
model_cards/Norod78/hewiki-articles-distilGPT2py-il/README.md
model_cards/Ogayo/Hel-ach-en/README.md
model_cards/Primer/bart-squad2/README.md
model_cards/README.md
model_cards/Rostlab/prot_bert/README.md
model_cards/Rostlab/prot_bert_bfd/README.md
model_cards/Rostlab/prot_t5_xl_bfd/README.md
model_cards/SZTAKI-HLT/hubert-base-cc/README.md
model_cards/SparkBeyond/roberta-large-sts-b/README.md
model_cards/T-Systems-onsite/bert-german-dbmdz-uncased-sentence-stsb/README.md
model_cards/T-Systems-onsite/cross-en-de-roberta-sentence-transformer/README.md
model_cards/T-Systems-onsite/german-roberta-sentence-transformer-v2/README.md
model_cards/Tereveni-AI/gpt2-124M-uk-fiction/README.md
model_cards/TurkuNLP/bert-base-finnish-cased-v1/README.md
model_cards/TurkuNLP/bert-base-finnish-uncased-v1/README.md
model_cards/TypicaAI/magbert-ner/README.md
model_cards/Vamsi/T5_Paraphrase_Paws/README.md
model_cards/VictorSanh/roberta-base-finetuned-yelp-polarity/README.md
model_cards/ViktorAlm/electra-base-norwegian-uncased-discriminator/README.md
model_cards/a-ware/bart-squadv2/README.md
model_cards/a-ware/roberta-large-squad-classification/README.md
model_cards/a-ware/xlmroberta-squadv2/README.md
model_cards/abhilash1910/financial_roberta/README.md
model_cards/abhilash1910/french-roberta/README.md
model_cards/activebus/BERT-DK_laptop/README.md
model_cards/activebus/BERT-DK_rest/README.md
model_cards/activebus/BERT-PT_laptop/README.md
model_cards/activebus/BERT-PT_rest/README.md
model_cards/activebus/BERT-XD_Review/README.md
model_cards/activebus/BERT_Review/README.md
model_cards/adalbertojunior/PTT5-SMALL-SUM/README.md
model_cards/ahotrod/albert_xxlargev1_squad2_512/README.md
model_cards/ahotrod/electra_large_discriminator_squad2_512/README.md
model_cards/ahotrod/roberta_large_squad2/README.md
model_cards/ahotrod/xlnet_large_squad2_512/README.md
model_cards/ai4bharat/indic-bert/README.md
model_cards/akhooli/gpt2-small-arabic-poetry/README.md
model_cards/akhooli/gpt2-small-arabic/README.md
model_cards/akhooli/mbart-large-cc25-ar-en/README.md
model_cards/akhooli/mbart-large-cc25-en-ar/README.md
model_cards/akhooli/personachat-arabic/README.md
model_cards/akhooli/xlm-r-large-arabic-sent/README.md
model_cards/akhooli/xlm-r-large-arabic-toxic/README.md
model_cards/albert-base-v1-README.md
model_cards/albert-xxlarge-v2-README.md
model_cards/aliosm/ComVE-distilgpt2/README.md
model_cards/aliosm/ComVE-gpt2-large/README.md
model_cards/aliosm/ComVE-gpt2-medium/README.md
model_cards/aliosm/ComVE-gpt2/README.md
model_cards/aliosm/ai-soco-cpp-roberta-small-clas/README.md
model_cards/aliosm/ai-soco-cpp-roberta-small/README.md
model_cards/aliosm/ai-soco-cpp-roberta-tiny-96-clas/README.md
model_cards/aliosm/ai-soco-cpp-roberta-tiny-96/README.md
model_cards/aliosm/ai-soco-cpp-roberta-tiny-clas/README.md
model_cards/aliosm/ai-soco-cpp-roberta-tiny/README.md
model_cards/allegro/herbert-base-cased/README.md
model_cards/allegro/herbert-klej-cased-tokenizer-v1/README.md
model_cards/allegro/herbert-klej-cased-v1/README.md
model_cards/allegro/herbert-large-cased/README.md
model_cards/allenai/biomed_roberta_base/README.md
model_cards/allenai/longformer-base-4096-extra.pos.embd.only/README.md
model_cards/allenai/longformer-base-4096/README.md
model_cards/allenai/scibert_scivocab_cased/README.md
model_cards/allenai/scibert_scivocab_uncased/README.md
model_cards/allenai/wmt16-en-de-12-1/README.md
model_cards/allenai/wmt16-en-de-dist-12-1/README.md
model_cards/allenai/wmt16-en-de-dist-6-1/README.md
model_cards/allenai/wmt19-de-en-6-6-base/README.md
model_cards/allenai/wmt19-de-en-6-6-big/README.md
model_cards/allenyummy/chinese-bert-wwm-ehr-ner-sl/README.md
model_cards/amberoad/bert-multilingual-passage-reranking-msmarco/README.md
model_cards/amine/bert-base-5lang-cased/README.md
model_cards/antoiloui/belgpt2/README.md
model_cards/aodiniz/bert_uncased_L-10_H-512_A-8_cord19-200616/README.md
model_cards/aodiniz/bert_uncased_L-10_H-512_A-8_cord19-200616_squad2/README.md
model_cards/aodiniz/bert_uncased_L-2_H-512_A-8_cord19-200616/README.md
model_cards/aodiniz/bert_uncased_L-4_H-256_A-4_cord19-200616/README.md
model_cards/asafaya/bert-base-arabic/README.md
model_cards/asafaya/bert-large-arabic/README.md
model_cards/asafaya/bert-medium-arabic/README.md
model_cards/asafaya/bert-mini-arabic/README.md
model_cards/ashwani-tanwar/Gujarati-XLM-R-Base/README.md
model_cards/aubmindlab/bert-base-arabert/README.md
model_cards/aubmindlab/bert-base-arabertv01/README.md
model_cards/bart-large-cnn/README.md
model_cards/bart-large-xsum/README.md
model_cards/bashar-talafha/multi-dialect-bert-base-arabic/README.md
model_cards/bayartsogt/albert-mongolian/README.md
model_cards/bayartsogt/bert-base-mongolian-cased/README.md
model_cards/bayartsogt/bert-base-mongolian-uncased/README.md
model_cards/bert-base-cased-README.md
model_cards/bert-base-chinese-README.md
model_cards/bert-base-german-cased-README.md
model_cards/bert-base-german-dbmdz-cased-README.md
model_cards/bert-base-german-dbmdz-uncased-README.md
model_cards/bert-base-multilingual-cased-README.md
model_cards/bert-base-multilingual-uncased-README.md
model_cards/bert-base-uncased-README.md
model_cards/bert-large-cased-README.md
model_cards/binwang/xlnet-base-cased/README.md
model_cards/bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-12/README.md
model_cards/bionlp/bluebert_pubmed_mimic_uncased_L-24_H-1024_A-16/README.md
model_cards/bionlp/bluebert_pubmed_uncased_L-12_H-768_A-12/README.md
model_cards/bionlp/bluebert_pubmed_uncased_L-24_H-1024_A-16/README.md
model_cards/blinoff/roberta-base-russian-v0/README.md
model_cards/cahya/bert-base-indonesian-522M/README.md
model_cards/cahya/gpt2-small-indonesian-522M/README.md
model_cards/cahya/roberta-base-indonesian-522M/README.md
model_cards/cambridgeltl/BioRedditBERT-uncased/README.md
model_cards/camembert-base-README.md
model_cards/camembert/camembert-base-ccnet-4gb/README.md
model_cards/camembert/camembert-base-ccnet/README.md
model_cards/camembert/camembert-base-oscar-4gb/README.md
model_cards/camembert/camembert-base-wikipedia-4gb/README.md
model_cards/camembert/camembert-large/README.md
model_cards/canwenxu/BERT-of-Theseus-MNLI/README.md
model_cards/cedpsam/chatbot_fr/README.md
model_cards/ceostroff/harry-potter-gpt2-fanfiction/README.md
model_cards/chrisliu298/arxiv_ai_gpt2/README.md
model_cards/cimm-kzn/endr-bert/README.md
model_cards/cimm-kzn/enrudr-bert/README.md
model_cards/cimm-kzn/rudr-bert/README.md
model_cards/clue/albert_chinese_small/README.md
model_cards/clue/albert_chinese_tiny/README.md
model_cards/clue/roberta_chinese_3L312_clue_tiny/README.md
model_cards/clue/roberta_chinese_base/README.md
model_cards/clue/roberta_chinese_large/README.md
model_cards/clue/xlnet_chinese_large/README.md
model_cards/codegram/calbert-base-uncased/README.md
model_cards/codegram/calbert-tiny-uncased/README.md
model_cards/cooelf/limitbert/README.md
model_cards/csarron/bert-base-uncased-squad-v1/README.md
model_cards/csarron/mobilebert-uncased-squad-v1/README.md
model_cards/csarron/mobilebert-uncased-squad-v2/README.md
model_cards/csarron/roberta-base-squad-v1/README.md
model_cards/daigo/bert-base-japanese-sentiment/README.md
model_cards/dbmdz/bert-base-german-cased/README.md
model_cards/dbmdz/bert-base-german-europeana-cased/README.md
model_cards/dbmdz/bert-base-german-europeana-uncased/README.md
model_cards/dbmdz/bert-base-german-uncased/README.md
model_cards/dbmdz/bert-base-italian-cased/README.md
model_cards/dbmdz/bert-base-italian-uncased/README.md
model_cards/dbmdz/bert-base-italian-xxl-cased/README.md
model_cards/dbmdz/bert-base-italian-xxl-uncased/README.md
model_cards/dbmdz/bert-base-turkish-128k-cased/README.md
model_cards/dbmdz/bert-base-turkish-128k-uncased/README.md
model_cards/dbmdz/bert-base-turkish-cased/README.md
model_cards/dbmdz/bert-base-turkish-uncased/README.md
model_cards/dbmdz/distilbert-base-turkish-cased/README.md
model_cards/dbmdz/electra-base-italian-xxl-cased-discriminator/README.md
model_cards/dbmdz/electra-base-italian-xxl-cased-generator/README.md
model_cards/dbmdz/electra-base-turkish-cased-discriminator/README.md
model_cards/dbmdz/electra-small-turkish-cased-discriminator/README.md
model_cards/dccuchile/bert-base-spanish-wwm-cased/README.md
model_cards/dccuchile/bert-base-spanish-wwm-uncased/README.md
model_cards/deepset/bert-base-german-cased-oldvocab/README.md
model_cards/deepset/electra-base-squad2/README.md
model_cards/deepset/gbert-base/README.md
model_cards/deepset/gbert-large/README.md
model_cards/deepset/gelectra-base-generator/README.md
model_cards/deepset/gelectra-base/README.md
model_cards/deepset/gelectra-large-generator/README.md
model_cards/deepset/gelectra-large/README.md
model_cards/deepset/minilm-uncased-squad2/README.md
model_cards/deepset/quora_dedup_bert_base/README.md
model_cards/deepset/roberta-base-squad2-covid/README.md
model_cards/deepset/roberta-base-squad2-v2/README.md
model_cards/deepset/roberta-base-squad2/README.md
model_cards/deepset/sentence_bert/README.md
model_cards/deepset/xlm-roberta-large-squad2/README.md
model_cards/digitalepidemiologylab/covid-twitter-bert/README.md
model_cards/distilbert-base-cased-README.md
model_cards/distilbert-base-cased-distilled-squad-README.md
model_cards/distilbert-base-german-cased-README.md
model_cards/distilbert-base-multilingual-cased-README.md
model_cards/distilbert-base-uncased-README.md
model_cards/distilbert-base-uncased-distilled-squad-README.md
model_cards/distilbert-base-uncased-finetuned-sst-2-english-README.md
model_cards/distilgpt2-README.md
model_cards/distilroberta-base-README.md
model_cards/djstrong/bg_cs_pl_ru_cased_L-12_H-768_A-12/README.md
model_cards/dkleczek/bert-base-polish-cased-v1/README.md
model_cards/dkleczek/bert-base-polish-uncased-v1/README.md
model_cards/dslim/bert-base-NER/README.md
model_cards/dumitrescustefan/bert-base-romanian-cased-v1/README.md
model_cards/dumitrescustefan/bert-base-romanian-uncased-v1/README.md
model_cards/e-tony/gpt2-rnm/README.md
model_cards/elgeish/cs224n-squad2.0-albert-base-v2/README.md
model_cards/elgeish/cs224n-squad2.0-albert-large-v2/README.md
model_cards/elgeish/cs224n-squad2.0-albert-xxlarge-v1/README.md
model_cards/elgeish/cs224n-squad2.0-distilbert-base-uncased/README.md
model_cards/elgeish/cs224n-squad2.0-roberta-base/README.md
model_cards/emilyalsentzer/Bio_ClinicalBERT/README.md
model_cards/emilyalsentzer/Bio_Discharge_Summary_BERT/README.md
model_cards/etalab-ia/camembert-base-squadFR-fquad-piaf/README.md
model_cards/ethanyt/guwenbert-base/README.md
model_cards/ethanyt/guwenbert-large/README.md
model_cards/facebook/bart-large-cnn/README.md
model_cards/facebook/bart-large-mnli/README.md
model_cards/facebook/bart-large/README.md
model_cards/facebook/rag-sequence-base/README.md
model_cards/facebook/rag-sequence-nq/README.md
model_cards/facebook/rag-token-base/README.md
model_cards/facebook/rag-token-nq/README.md
model_cards/facebook/rag-token-nq_new/README.md
model_cards/facebook/wmt19-de-en/README.md
model_cards/facebook/wmt19-en-de/README.md
model_cards/facebook/wmt19-en-ru/README.md
model_cards/facebook/wmt19-ru-en/README.md
model_cards/flexudy/t5-base-multi-sentence-doctor/README.md
model_cards/flexudy/t5-base-multi-sentence-doctor/sent-banner.png
model_cards/fmikaelian/camembert-base-fquad/README.md
model_cards/fmikaelian/camembert-base-squad/README.md
model_cards/fmikaelian/flaubert-base-uncased-squad/README.md
model_cards/fran-martinez/scibert_scivocab_cased_ner_jnlpba/README.md
model_cards/funnel-transformer/intermediate-base/README.md
model_cards/funnel-transformer/intermediate/README.md
model_cards/funnel-transformer/large-base/README.md
model_cards/funnel-transformer/large/README.md
model_cards/funnel-transformer/medium-base/README.md
model_cards/funnel-transformer/medium/README.md
model_cards/funnel-transformer/small-base/README.md
model_cards/funnel-transformer/small/README.md
model_cards/funnel-transformer/xlarge-base/README.md
model_cards/funnel-transformer/xlarge/README.md
model_cards/ganeshkharad/gk-hinglish-sentiment/README.md
model_cards/gaochangkuan/model_dir/README.md
model_cards/german-nlp-group/electra-base-german-uncased/README.md
model_cards/giganticode/StackOBERTflow-comments-small-v1/README.md
model_cards/gilf/french-camembert-postag-model/README.md
model_cards/gilf/french-postag-model/README.md
model_cards/google/bert2bert_L-24_wmt_de_en/README.md
model_cards/google/bert2bert_L-24_wmt_en_de/README.md
model_cards/google/bert_uncased_L-10_H-128_A-2/README.md
model_cards/google/bert_uncased_L-10_H-256_A-4/README.md
model_cards/google/bert_uncased_L-10_H-512_A-8/README.md
model_cards/google/bert_uncased_L-10_H-768_A-12/README.md
model_cards/google/bert_uncased_L-12_H-128_A-2/README.md
model_cards/google/bert_uncased_L-12_H-256_A-4/README.md
model_cards/google/bert_uncased_L-12_H-512_A-8/README.md
model_cards/google/bert_uncased_L-12_H-768_A-12/README.md
model_cards/google/bert_uncased_L-2_H-128_A-2/README.md
model_cards/google/bert_uncased_L-2_H-256_A-4/README.md
model_cards/google/bert_uncased_L-2_H-512_A-8/README.md
model_cards/google/bert_uncased_L-2_H-768_A-12/README.md
model_cards/google/bert_uncased_L-4_H-128_A-2/README.md
model_cards/google/bert_uncased_L-4_H-256_A-4/README.md
model_cards/google/bert_uncased_L-4_H-512_A-8/README.md
model_cards/google/bert_uncased_L-4_H-768_A-12/README.md
model_cards/google/bert_uncased_L-6_H-128_A-2/README.md
model_cards/google/bert_uncased_L-6_H-256_A-4/README.md
model_cards/google/bert_uncased_L-6_H-512_A-8/README.md
model_cards/google/bert_uncased_L-6_H-768_A-12/README.md
model_cards/google/bert_uncased_L-8_H-128_A-2/README.md
model_cards/google/bert_uncased_L-8_H-256_A-4/README.md
model_cards/google/bert_uncased_L-8_H-512_A-8/README.md
model_cards/google/bert_uncased_L-8_H-768_A-12/README.md
model_cards/google/electra-base-discriminator/README.md
model_cards/google/electra-base-generator/README.md
model_cards/google/electra-large-discriminator/README.md
model_cards/google/electra-large-generator/README.md
model_cards/google/electra-small-discriminator/README.md
model_cards/google/electra-small-generator/README.md
model_cards/google/mobilebert-uncased/README.md
model_cards/google/reformer-crime-and-punishment/README.md
model_cards/google/reformer-enwik8/README.md
model_cards/google/roberta2roberta_L-24_bbc/README.md
model_cards/google/roberta2roberta_L-24_cnn_daily_mail/README.md
model_cards/google/roberta2roberta_L-24_discofuse/README.md
model_cards/google/roberta2roberta_L-24_gigaword/README.md
model_cards/google/roberta2roberta_L-24_wikisplit/README.md
model_cards/gpt2-README.md
model_cards/gpt2-large-README.md
model_cards/gpt2-medium-README.md
model_cards/gpt2-xl-README.md
model_cards/gsarti/biobert-nli/README.md
model_cards/gsarti/covidbert-nli/README.md
model_cards/gsarti/scibert-nli/README.md
model_cards/gurkan08/bert-turkish-text-classification/README.md
model_cards/hatmimoha/arabic-ner/README.md
model_cards/healx/gpt-2-pubmed-large/README.md
model_cards/healx/gpt-2-pubmed-medium/README.md
model_cards/henryk/bert-base-multilingual-cased-finetuned-dutch-squad2/README.md
model_cards/henryk/bert-base-multilingual-cased-finetuned-polish-squad1/README.md
model_cards/henryk/bert-base-multilingual-cased-finetuned-polish-squad2/README.md
model_cards/huawei-noah/DynaBERT_MNLI/README.md
model_cards/huawei-noah/DynaBERT_SST-2/README.md
model_cards/huawei-noah/TinyBERT_General_4L_312D/README.md
model_cards/huggingface/CodeBERTa-language-id/README.md
model_cards/huggingface/CodeBERTa-small-v1/README.md
model_cards/huseinzol05/albert-base-bahasa-cased/README.md
model_cards/huseinzol05/albert-tiny-bahasa-cased/README.md
model_cards/huseinzol05/bert-base-bahasa-cased/README.md
model_cards/huseinzol05/electra-base-discriminator-bahasa-cased/README.md
model_cards/huseinzol05/electra-base-generator-bahasa-cased/README.md
model_cards/huseinzol05/electra-small-discriminator-bahasa-cased/README.md
model_cards/huseinzol05/electra-small-generator-bahasa-cased/README.md
model_cards/huseinzol05/gpt2-117M-bahasa-cased/README.md
model_cards/huseinzol05/gpt2-345M-bahasa-cased/README.md
model_cards/huseinzol05/t5-base-bahasa-cased/README.md
model_cards/huseinzol05/t5-base-bahasa-summarization-cased/README.md
model_cards/huseinzol05/t5-small-bahasa-cased/README.md
model_cards/huseinzol05/t5-small-bahasa-summarization-cased/README.md
model_cards/huseinzol05/tiny-bert-bahasa-cased/README.md
model_cards/huseinzol05/xlnet-base-bahasa-cased/README.md
model_cards/iarfmoose/bert-base-cased-qa-evaluator/README.md
model_cards/iarfmoose/roberta-base-bulgarian-pos/README.md
model_cards/iarfmoose/roberta-base-bulgarian/README.md
model_cards/iarfmoose/roberta-small-bulgarian-pos/README.md
model_cards/iarfmoose/roberta-small-bulgarian/README.md
model_cards/iarfmoose/t5-base-question-generator/README.md
model_cards/illuin/camembert-base-fquad/README.md
model_cards/illuin/camembert-large-fquad/README.md
model_cards/illuin/lepetit/README.md
model_cards/indobenchmark/indobert-base-p1/README.md
model_cards/indobenchmark/indobert-base-p2/README.md
model_cards/indobenchmark/indobert-large-p1/README.md
model_cards/indobenchmark/indobert-large-p2/README.md
model_cards/indobenchmark/indobert-lite-base-p1/README.md
model_cards/indobenchmark/indobert-lite-base-p2/README.md
model_cards/indobenchmark/indobert-lite-large-p1/README.md
model_cards/indobenchmark/indobert-lite-large-p2/README.md
model_cards/indolem/indobert-base-uncased/README.md
model_cards/ipuneetrathore/bert-base-cased-finetuned-finBERT/README.md
model_cards/iuliaturc/bert_uncased_L-2_H-128_A-2/README.md
model_cards/ixa-ehu/berteus-base-cased/README.md
model_cards/ixa-ehu/ixambert-base-cased/README.md
model_cards/jannesg/bertsson/README.md
model_cards/jannesg/takalane_afr_roberta/README.md
model_cards/jannesg/takalane_nbl_roberta/README.md
model_cards/jannesg/takalane_nso_roberta/README.md
model_cards/jannesg/takalane_sot_roberta/README.md
model_cards/jannesg/takalane_ssw_roberta/README.md
model_cards/jannesg/takalane_tsn_roberta/README.md
model_cards/jannesg/takalane_tso_roberta/README.md
model_cards/jannesg/takalane_ven_roberta/README.md
model_cards/jannesg/takalane_xho_roberta/README.md
model_cards/jannesg/takalane_zul_roberta/README.md
model_cards/jcblaise/bert-tagalog-base-cased-WWM/README.md
model_cards/jcblaise/bert-tagalog-base-cased/README.md
model_cards/jcblaise/bert-tagalog-base-uncased-WWM/README.md
model_cards/jcblaise/bert-tagalog-base-uncased/README.md
model_cards/jcblaise/distilbert-tagalog-base-cased/README.md
model_cards/jcblaise/electra-tagalog-base-cased-discriminator/README.md
model_cards/jcblaise/electra-tagalog-base-cased-generator/README.md
model_cards/jcblaise/electra-tagalog-base-uncased-discriminator/README.md
model_cards/jcblaise/electra-tagalog-base-uncased-generator/README.md
model_cards/jcblaise/electra-tagalog-small-cased-discriminator/README.md
model_cards/jcblaise/electra-tagalog-small-cased-generator/README.md
model_cards/jcblaise/electra-tagalog-small-uncased-discriminator/README.md
model_cards/jcblaise/electra-tagalog-small-uncased-generator/README.md
model_cards/jimregan/BERTreach/README.md
model_cards/jme-p/shrugging-grace-tweet-classifier/README.md
model_cards/joeddav/bart-large-mnli-yahoo-answers/README.md
model_cards/joeddav/xlm-roberta-large-xnli/README.md
model_cards/jordimas/julibert/README.md
model_cards/jplu/tf-camembert-base/README.md
model_cards/jplu/tf-xlm-r-ner-40-lang/README.md
model_cards/jplu/tf-xlm-roberta-base/README.md
model_cards/jplu/tf-xlm-roberta-large/README.md
model_cards/julien-c/EsperBERTo-small-pos/README.md
model_cards/julien-c/EsperBERTo-small/README.md
model_cards/julien-c/bert-xsmall-dummy/README.md
model_cards/julien-c/dummy-unknown/README.md
model_cards/keshan/SinhalaBERTo/README.md
model_cards/kiri-ai/distiluse-base-multilingual-cased-et/README.md
model_cards/krevas/finance-koelectra-base-discriminator/README.md
model_cards/krevas/finance-koelectra-base-generator/README.md
model_cards/krevas/finance-koelectra-small-discriminator/README.md
model_cards/krevas/finance-koelectra-small-generator/README.md
model_cards/ktrapeznikov/albert-xlarge-v2-squad-v2/README.md
model_cards/ktrapeznikov/biobert_v1.1_pubmed_squad_v2/README.md
model_cards/ktrapeznikov/gpt2-medium-topic-news/README.md
model_cards/ktrapeznikov/scibert_scivocab_uncased_squad_v2/README.md
model_cards/kuisailab/albert-base-arabic/README.md
model_cards/kuisailab/albert-large-arabic/README.md
model_cards/kuisailab/albert-xlarge-arabic/README.md
model_cards/kuppuluri/telugu_bertu/README.md
model_cards/kuppuluri/telugu_bertu_ner/README.md
model_cards/kuppuluri/telugu_bertu_pos/README.md
model_cards/kuppuluri/telugu_bertu_tydiqa/README.md
model_cards/lanwuwei/GigaBERT-v3-Arabic-and-English/README.md
model_cards/loodos/albert-base-turkish-uncased/README.md
model_cards/loodos/bert-base-turkish-uncased/README.md
model_cards/loodos/electra-base-turkish-64k-uncased-discriminator/README.md
model_cards/loodos/electra-base-turkish-uncased-discriminator/README.md
model_cards/loodos/electra-small-turkish-cased-discriminator/README.md
model_cards/loodos/electra-small-turkish-uncased-discriminator/README.md
model_cards/lordtt13/COVID-SciBERT/README.md
model_cards/lordtt13/emo-mobilebert/README.md
model_cards/lserinol/bert-turkish-question-answering/README.md
model_cards/lvwerra/bert-imdb/README.md
model_cards/lvwerra/gpt2-imdb-ctrl/README.md
model_cards/lvwerra/gpt2-imdb-pos/README.md
model_cards/lvwerra/gpt2-imdb/README.md
model_cards/lvwerra/gpt2-medium-taboo/README.md
model_cards/lysandre/arxiv-nlp/README.md
model_cards/lysandre/arxiv/README.md
model_cards/m3hrdadfi/albert-fa-base-v2/README.md
model_cards/m3hrdadfi/bert2bert-fa-news-headline/README.md
model_cards/m3hrdadfi/bert2bert-fa-wiki-summary/README.md
model_cards/microsoft/DeBERTa-base/README.md
model_cards/microsoft/DeBERTa-large/README.md
model_cards/microsoft/DialoGPT-large/README.md
model_cards/microsoft/DialoGPT-medium/README.md
model_cards/microsoft/DialoGPT-small/README.md
model_cards/microsoft/MiniLM-L12-H384-uncased/README.md
model_cards/microsoft/Multilingual-MiniLM-L12-H384/README.md
model_cards/microsoft/codebert-base-mlm/README.md
model_cards/microsoft/codebert-base/README.md
model_cards/microsoft/layoutlm-base-uncased/README.md
model_cards/microsoft/layoutlm-large-uncased/README.md
model_cards/microsoft/prophetnet-large-uncased-cnndm/README.md
model_cards/microsoft/prophetnet-large-uncased-squad-qg/README.md
model_cards/microsoft/prophetnet-large-uncased/README.md
model_cards/microsoft/xprophetnet-large-wiki100-cased-xglue-ntg/README.md
model_cards/microsoft/xprophetnet-large-wiki100-cased-xglue-qg/README.md
model_cards/microsoft/xprophetnet-large-wiki100-cased/README.md
model_cards/monilouise/ner_pt_br/README.md
model_cards/monologg/koelectra-base-discriminator/README.md
model_cards/monologg/koelectra-base-generator/README.md
model_cards/monologg/koelectra-small-discriminator/README.md
model_cards/monologg/koelectra-small-generator/README.md
model_cards/monsoon-nlp/dv-wave/README.md
model_cards/moumeneb1/flaubert-base-cased-ecology_crisis/README.md
model_cards/mrm8488/CodeBERTaPy/README.md
model_cards/mrm8488/GPT-2-finetuned-CORD19/README.md
model_cards/mrm8488/GPT-2-finetuned-common_gen/README.md
model_cards/mrm8488/GPT-2-finetuned-covid-bio-medrxiv/README.md
model_cards/mrm8488/GuaPeTe-2-tiny/README.md
model_cards/mrm8488/RoBERTinha/README.md
model_cards/mrm8488/RoBasquERTa/README.md
model_cards/mrm8488/RuPERTa-base-finetuned-ner/README.md
model_cards/mrm8488/RuPERTa-base-finetuned-pawsx-es/README.md
model_cards/mrm8488/RuPERTa-base-finetuned-pos/README.md
model_cards/mrm8488/RuPERTa-base-finetuned-squadv1/README.md
model_cards/mrm8488/RuPERTa-base-finetuned-squadv2/README.md
model_cards/mrm8488/RuPERTa-base/README.md
model_cards/mrm8488/TinyBERT-spanish-uncased-finetuned-ner/README.md
model_cards/mrm8488/bert-base-german-dbmdz-cased-finetuned-pawsx-de/README.md
model_cards/mrm8488/bert-base-german-finetuned-ler/README.md
model_cards/mrm8488/bert-base-spanish-wwm-cased-finetuned-spa-squad2-es/README.md
model_cards/mrm8488/bert-italian-finedtuned-squadv1-it-alfa/README.md
model_cards/mrm8488/bert-medium-finetuned-squadv2/README.md
model_cards/mrm8488/bert-mini-finetuned-squadv2/README.md
model_cards/mrm8488/bert-mini2bert-mini-finetuned-cnn_daily_mail-summarization/README.md
model_cards/mrm8488/bert-multi-cased-finedtuned-xquad-tydiqa-goldp/README.md
model_cards/mrm8488/bert-multi-cased-finetuned-xquadv1/README.md
model_cards/mrm8488/bert-multi-uncased-finetuned-xquadv1/README.md
model_cards/mrm8488/bert-small-finetuned-squadv2/README.md
model_cards/mrm8488/bert-small-finetuned-typo-detection/README.md
model_cards/mrm8488/bert-small2bert-small-finetuned-cnn_daily_mail-summarization/README.md
model_cards/mrm8488/bert-spanish-cased-finetuned-ner/README.md
model_cards/mrm8488/bert-spanish-cased-finetuned-pos-syntax/README.md
model_cards/mrm8488/bert-spanish-cased-finetuned-pos/README.md
model_cards/mrm8488/bert-tiny-finetuned-squadv2/README.md
model_cards/mrm8488/bert-uncased-finetuned-qnli/README.md
model_cards/mrm8488/camembert-base-finetuned-pawsx-fr/README.md
model_cards/mrm8488/chEMBL_smiles_v1/README.md
model_cards/mrm8488/codeBERTaJS/README.md
model_cards/mrm8488/codebert-base-finetuned-detect-insecure-code/README.md
model_cards/mrm8488/distilbert-base-multi-cased-finetuned-typo-detection/README.md
model_cards/mrm8488/distilbert-multi-finetuned-for-xqua-on-tydiqa/README.md
model_cards/mrm8488/distill-bert-base-spanish-wwm-cased-finetuned-spa-squad2-es/README.md
model_cards/mrm8488/distilroberta-base-finetuned-sentiment/README.md
model_cards/mrm8488/electra-base-finetuned-squadv1/README.md
model_cards/mrm8488/electra-small-finetuned-squadv1/README.md
model_cards/mrm8488/electra-small-finetuned-squadv2/README.md
model_cards/mrm8488/electricidad-base-discriminator/README.md
model_cards/mrm8488/electricidad-base-finetuned-pawsx-es/README.md
model_cards/mrm8488/electricidad-base-generator/README.md
model_cards/mrm8488/electricidad-small-discriminator/README.md
model_cards/mrm8488/electricidad-small-finetuned-squadv1-es/README.md
model_cards/mrm8488/gpt2-finetuned-recipes-cooking/README.md
model_cards/mrm8488/gpt2-finetuned-recipes-cooking_v2/README.md
model_cards/mrm8488/gpt2-imdb-neg/README.md
model_cards/mrm8488/gpt2-imdb-neutral/README.md
model_cards/mrm8488/longformer-base-4096-finetuned-squadv2/README.md
model_cards/mrm8488/mT5-small-finetuned-tydiqa-for-xqa/README.md
model_cards/mrm8488/mobilebert-uncased-finetuned-squadv1/README.md
model_cards/mrm8488/mobilebert-uncased-finetuned-squadv2/README.md
model_cards/mrm8488/roberta-base-1B-1-finetuned-squadv1/README.md
model_cards/mrm8488/roberta-base-1B-1-finetuned-squadv2/README.md
model_cards/mrm8488/roberta-large-finetuned-wsc/README.md
model_cards/mrm8488/spanbert-base-finetuned-squadv1/README.md
model_cards/mrm8488/spanbert-base-finetuned-squadv2/README.md
model_cards/mrm8488/spanbert-base-finetuned-tacred/README.md
model_cards/mrm8488/spanbert-finetuned-squadv1/README.md
model_cards/mrm8488/spanbert-finetuned-squadv2/README.md
model_cards/mrm8488/spanbert-large-finetuned-squadv1/README.md
model_cards/mrm8488/spanbert-large-finetuned-squadv2/README.md
model_cards/mrm8488/spanbert-large-finetuned-tacred/README.md
model_cards/mrm8488/squeezebert-finetuned-squadv1/README.md
model_cards/mrm8488/squeezebert-finetuned-squadv2/README.md
model_cards/mrm8488/t5-base-finetuned-break_data-question-retrieval/README.md
model_cards/mrm8488/t5-base-finetuned-break_data/README.md
model_cards/mrm8488/t5-base-finetuned-common_gen/README.md
model_cards/mrm8488/t5-base-finetuned-e2m-intent/README.md
model_cards/mrm8488/t5-base-finetuned-emotion/README.md
model_cards/mrm8488/t5-base-finetuned-imdb-sentiment/README.md
model_cards/mrm8488/t5-base-finetuned-qasc/README.md
model_cards/mrm8488/t5-base-finetuned-quarel/README.md
model_cards/mrm8488/t5-base-finetuned-quartz/README.md
model_cards/mrm8488/t5-base-finetuned-question-generation-ap/README.md
model_cards/mrm8488/t5-base-finetuned-sarcasm-twitter/README.md
model_cards/mrm8488/t5-base-finetuned-span-sentiment-extraction/README.md
model_cards/mrm8488/t5-base-finetuned-squadv2/README.md
model_cards/mrm8488/t5-base-finetuned-summarize-news/README.md
model_cards/mrm8488/t5-base-finetuned-wikiSQL-sql-to-en/README.md
model_cards/mrm8488/t5-base-finetuned-wikiSQL/README.md
model_cards/mrm8488/t5-small-finetuned-emotion/README.md
model_cards/mrm8488/t5-small-finetuned-imdb-sentiment/README.md
model_cards/mrm8488/t5-small-finetuned-quora-for-paraphrasing/README.md
model_cards/mrm8488/t5-small-finetuned-squadv1/README.md
model_cards/mrm8488/t5-small-finetuned-squadv2/README.md
model_cards/mrm8488/t5-small-finetuned-wikiSQL/README.md
model_cards/mrm8488/umberto-wikipedia-uncased-v1-finetuned-squadv1-it/README.md
model_cards/mrm8488/xlm-multi-finetuned-xquadv1/README.md
model_cards/mymusise/gpt2-medium-chinese/README.md
model_cards/mys/electra-base-turkish-cased-ner/README.md
model_cards/ncoop57/bart-base-code-summarizer-java-v0/README.md
model_cards/neuralmind/bert-base-portuguese-cased/README.md
model_cards/neuralmind/bert-large-portuguese-cased/README.md
model_cards/neuralspace-reverie/indic-transformers-bn-bert/README.md
model_cards/neuralspace-reverie/indic-transformers-bn-distilbert/README.md
model_cards/neuralspace-reverie/indic-transformers-bn-roberta/README.md
model_cards/neuralspace-reverie/indic-transformers-bn-xlmroberta/README.md
model_cards/neuralspace-reverie/indic-transformers-hi-bert/README.md
model_cards/neuralspace-reverie/indic-transformers-hi-distilbert/README.md
model_cards/neuralspace-reverie/indic-transformers-hi-roberta/README.md
model_cards/neuralspace-reverie/indic-transformers-hi-xlmroberta/README.md
model_cards/neuralspace-reverie/indic-transformers-te-bert/README.md
model_cards/neuralspace-reverie/indic-transformers-te-distilbert/README.md
model_cards/neuralspace-reverie/indic-transformers-te-roberta/README.md
model_cards/neuralspace-reverie/indic-transformers-te-xlmroberta/README.md
model_cards/neuraly/bert-base-italian-cased-sentiment/README.md
model_cards/neurocode/IsRoBERTa/README.md
model_cards/nghuyong/ernie-1.0/README.md
model_cards/nghuyong/ernie-2.0-en/README.md
model_cards/nghuyong/ernie-2.0-large-en/README.md
model_cards/nghuyong/ernie-tiny/README.md
model_cards/nikokons/gpt2-greek/README.md
model_cards/nlpaueb/bert-base-greek-uncased-v1/README.md
model_cards/nlpaueb/legal-bert-base-uncased/README.md
model_cards/nlptown/bert-base-multilingual-uncased-sentiment/README.md
model_cards/nyu-mll/roberta-base-100M-1/README.md
model_cards/nyu-mll/roberta-base-100M-2/README.md
model_cards/nyu-mll/roberta-base-100M-3/README.md
model_cards/nyu-mll/roberta-base-10M-1/README.md
model_cards/nyu-mll/roberta-base-10M-2/README.md
model_cards/nyu-mll/roberta-base-10M-3/README.md
model_cards/nyu-mll/roberta-base-1B-1/README.md
model_cards/nyu-mll/roberta-base-1B-2/README.md
model_cards/nyu-mll/roberta-base-1B-3/README.md
model_cards/nyu-mll/roberta-med-small-1M-1/README.md
model_cards/nyu-mll/roberta-med-small-1M-2/README.md
model_cards/nyu-mll/roberta-med-small-1M-3/README.md
model_cards/nyu-mll/roberta_1M_to_1B/README.md
model_cards/oliverguhr/german-sentiment-bert/README.md
model_cards/panggi/t5-base-indonesian-summarization-cased/README.md
model_cards/panggi/t5-small-indonesian-summarization-cased/README.md
model_cards/patrickvonplaten/bert2bert-cnn_dailymail-fp16/README.md
model_cards/patrickvonplaten/bert2bert_cnn_daily_mail/README.md
model_cards/patrickvonplaten/bert2gpt2-cnn_dailymail-fp16/README.md
model_cards/patrickvonplaten/longformer2roberta-cnn_dailymail-fp16/README.md
model_cards/patrickvonplaten/roberta2roberta-cnn_dailymail-fp16/README.md
model_cards/patrickvonplaten/roberta2roberta-share-cnn_dailymail-fp16/README.md
model_cards/patrickvonplaten/roberta_shared_bbc_xsum/README.md
model_cards/pdelobelle/robbert-v2-dutch-base/README.md
model_cards/pedropei/question-intimacy/README.md
model_cards/pierreguillou/gpt2-small-portuguese/README.md
model_cards/pradhyra/AWSBlogBert/README.md
model_cards/pranavpsv/gpt2-genre-story-generator/README.md
model_cards/pvl/labse_bert/README.md
model_cards/qarib/bert-base-qarib60_1790k/README.md
model_cards/qarib/bert-base-qarib60_1970k/README.md
model_cards/qarib/bert-base-qarib60_860k/README.md
model_cards/ramsrigouthamg/t5_paraphraser/README.md
model_cards/rdenadai/BR_BERTo/README.md
model_cards/redewiedergabe/bert-base-historical-german-rw-cased/README.md
model_cards/rjbownes/Magic-The-Generating/README.md
model_cards/roberta-base-README.md
model_cards/roberta-large-README.md
model_cards/roberta-large-mnli-README.md
model_cards/rohanrajpal/bert-base-codemixed-uncased-sentiment/README.md
model_cards/rohanrajpal/bert-base-en-es-codemix-cased/README.md
model_cards/rohanrajpal/bert-base-en-hi-codemix-cased/README.md
model_cards/rohanrajpal/bert-base-multilingual-codemixed-cased-sentiment/README.md
model_cards/sachaarbonel/bert-italian-cased-finetuned-pos/README.md
model_cards/sagorsarker/bangla-bert-base/README.md
model_cards/sagorsarker/bangla-bert-sentiment/README.md
model_cards/sagorsarker/codeswitch-hineng-lid-lince/README.md
model_cards/sagorsarker/codeswitch-hineng-ner-lince/README.md
model_cards/sagorsarker/codeswitch-hineng-pos-lince/README.md
model_cards/sagorsarker/codeswitch-nepeng-lid-lince/README.md
model_cards/sagorsarker/codeswitch-spaeng-lid-lince/README.md
model_cards/sagorsarker/codeswitch-spaeng-ner-lince/README.md
model_cards/sagorsarker/codeswitch-spaeng-pos-lince/README.md
model_cards/sagorsarker/codeswitch-spaeng-sentiment-analysis-lince/README.md
model_cards/sarahlintang/IndoBERT/README.md
model_cards/sarnikowski/electra-small-discriminator-da-256-cased/README.md
model_cards/savasy/bert-base-turkish-ner-cased/README.md
model_cards/savasy/bert-base-turkish-sentiment-cased/README.md
model_cards/savasy/bert-base-turkish-squad/README.md
model_cards/savasy/bert-turkish-text-classification/README.md
model_cards/schmidek/electra-small-cased/README.md
model_cards/seiya/oubiobert-base-uncased/README.md
model_cards/sentence-transformers/LaBSE/README.md
model_cards/sentence-transformers/bert-base-nli-cls-token/README.md
model_cards/sentence-transformers/bert-base-nli-max-tokens/README.md
model_cards/sentence-transformers/bert-base-nli-mean-tokens/README.md
model_cards/severinsimmler/literary-german-bert/README.md
model_cards/severinsimmler/literary-german-bert/kfold.png
model_cards/severinsimmler/literary-german-bert/prosa-jahre.png
model_cards/seyonec/ChemBERTa-zinc-base-v1/README.md
model_cards/shoarora/alectra-small-owt/README.md
model_cards/shoarora/electra-small-owt/README.md
model_cards/shrugging-grace/tweetclassifier/README.md
model_cards/smanjil/German-MedBERT/README.md
model_cards/spentaur/yelp/README.md
model_cards/squeezebert/squeezebert-mnli-headless/README.md
model_cards/squeezebert/squeezebert-mnli/README.md
model_cards/squeezebert/squeezebert-uncased/README.md
model_cards/stas/tiny-wmt19-en-de/README.md
model_cards/stevhliu/astroGPT/README.md
model_cards/surajp/RoBERTa-hindi-guj-san/README.md
model_cards/surajp/SanBERTa/README.md
model_cards/surajp/albert-base-sanskrit/README.md
model_cards/t5-11b-README.md
model_cards/t5-3b-README.md
model_cards/t5-base-README.md
model_cards/t5-large-README.md
model_cards/t5-small-README.md
model_cards/tartuNLP/EstBERT/README.md
model_cards/tblard/tf-allocine/README.md
model_cards/tuner007/pegasus_paraphrase/README.md
model_cards/tuner007/pegasus_qa/README.md
model_cards/tuner007/t5_abs_qa/README.md
model_cards/twmkn9/albert-base-v2-squad2/README.md
model_cards/twmkn9/bert-base-uncased-squad2/README.md
model_cards/twmkn9/distilbert-base-uncased-squad2/README.md
model_cards/twmkn9/distilroberta-base-squad2/README.md
model_cards/uer/chinese_roberta_L-2_H-128/README.md
model_cards/uer/gpt2-chinese-couplet/README.md
model_cards/uer/gpt2-chinese-poem/README.md
model_cards/uncnlp/lxmert-base-uncased/LICENSE
model_cards/uncnlp/lxmert-base-uncased/README.md
model_cards/uncnlp/lxmert-base-uncased/lxmert_model-1.jpg
model_cards/unideeplearning/polibert_sa/README.md
model_cards/urduhack/roberta-urdu-small/README.md
model_cards/valhalla/bart-large-finetuned-squadv1/README.md
model_cards/valhalla/distilbart-mnli-12-1/README.md
model_cards/valhalla/distilbart-mnli-12-3/README.md
model_cards/valhalla/distilbart-mnli-12-6/README.md
model_cards/valhalla/distilbart-mnli-12-9/README.md
model_cards/valhalla/electra-base-discriminator-finetuned_squadv1/README.md
model_cards/valhalla/longformer-base-4096-finetuned-squadv1/README.md
model_cards/valhalla/t5-base-e2e-qg/README.md
model_cards/valhalla/t5-base-qa-qg-hl/README.md
model_cards/valhalla/t5-base-qg-hl/README.md
model_cards/valhalla/t5-base-squad/README.md
model_cards/valhalla/t5-samll-qg-prepend/README.md
model_cards/valhalla/t5-small-e2e-qg/README.md
model_cards/valhalla/t5-small-qa-qg-hl/README.md
model_cards/valhalla/t5-small-qg-hl/README.md
model_cards/vinai/bertweet-base/README.md
model_cards/vinai/bertweet-covid19-base-cased/README.md
model_cards/vinai/bertweet-covid19-base-uncased/README.md
model_cards/vinai/phobert-base/README.md
model_cards/vinai/phobert-large/README.md
model_cards/voidful/albert_chinese_base/README.md
model_cards/voidful/albert_chinese_large/README.md
model_cards/voidful/albert_chinese_small/README.md
model_cards/voidful/albert_chinese_tiny/README.md
model_cards/voidful/albert_chinese_xlarge/README.md
model_cards/voidful/albert_chinese_xxlarge/README.md
model_cards/wietsedv/bert-base-dutch-cased/README.md
model_cards/wptoux/albert-chinese-large-qa/README.md
model_cards/xlm-mlm-en-2048-README.md
model_cards/xlm-roberta-base-README.md
model_cards/xlm-roberta-large-finetuned-conll03-german-README.md
model_cards/yjernite/bart_eli5/README.md
model_cards/ynie/roberta-large-snli_mnli_fever_anli_R1_R2_R3-nli/README.md
model_cards/youscan/ukr-roberta-base/README.md
model_cards/yuvraj/summarizer-cnndm/README.md
model_cards/yuvraj/xSumm/README.md
model_cards/zanelim/singbert-large-sg/README.md
model_cards/zanelim/singbert-lite-sg/README.md
model_cards/zanelim/singbert/README.md
==================
29e459795;Sylvain Gugger;2020-12-11 16:26:05 -0500;Fix min_null_pred in the run_qa script (#9067)

==

examples/question-answering/utils_qa.py
==================
9cc9f4122;Patrick von Platen;2020-12-11 16:59:54 +0100;Make ProphetNetModel really compatible with EncoderDecoder (#9033)
* improve

* finish

* upload model

* fix lm head

* fix test
==

src/transformers/models/prophetnet/modeling_prophetnet.py
src/transformers/models/xlm_prophetnet/modeling_xlm_prophetnet.py
tests/test_modeling_encoder_decoder.py
tests/test_modeling_prophetnet.py
utils/check_repo.py
==================
24f6cdeab;dependabot[bot];2020-12-11 10:32:43 -0500;Bump notebook in /examples/research_projects/movement-pruning/lxmert (#9062)
Bumps [notebook](https://github.com/jupyter/jupyterhub) from 6.1.4 to 6.1.5.
- [Release notes](https://github.com/jupyter/jupyterhub/releases)
- [Changelog](https://github.com/jupyterhub/jupyterhub/blob/master/CHECKLIST-Release.md)
- [Commits](https://github.com/jupyter/jupyterhub/commits)

Signed-off-by: dependabot[bot] <support@github.com>

Co-authored-by: dependabot[bot] <49699333+dependabot[bot]@users.noreply.github.com>
==

examples/research_projects/movement-pruning/lxmert/requirements.txt
==================
91fa70721;Lysandre Debut;2020-12-11 10:27:31 -0500;Remove docs only check (#9065)

==

.circleci/config.yml
==================
70527ba69;Sylvain Gugger;2020-12-11 10:25:00 -0500;Fix PreTrainedTokenizer.pad when first inputs are empty (#9018)
* Fix PreTrainedTokenizer.pad when first inputs are empty

* Handle empty inputs case
==

src/transformers/tokenization_utils_base.py
==================
783d7d262;Sylvain Gugger;2020-12-11 10:07:02 -0500;Reorganize examples (#9010)
* Reorganize example folder

* Continue reorganization

* Change requirements for tests

* Final cleanup

* Finish regroup with tests all passing

* Copyright

* Requirements and readme

* Make a full link for the documentation

* Address review comments

* Apply suggestions from code review

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* Add symlink

* Reorg again

* Apply suggestions from code review

Co-authored-by: Thomas Wolf <thomwolf@users.noreply.github.com>

* Adapt title

* Update to new strucutre

* Remove test

* Update READMEs

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
Co-authored-by: Thomas Wolf <thomwolf@users.noreply.github.com>
==

.circleci/config.yml
examples/README.md
examples/_tests_requirements.txt
examples/benchmarking/README.md
examples/benchmarking/plot_csv_file.py
examples/benchmarking/requirements.txt
examples/conftest.py
examples/contrib/README.md
examples/language-modeling/README.md
examples/language-modeling/requirements.txt
examples/legacy/README.md
examples/legacy/pytorch-lightning/lightning_base.py
examples/legacy/pytorch-lightning/requirements.txt
examples/legacy/pytorch-lightning/run_glue.py
examples/legacy/pytorch-lightning/run_glue.sh
examples/legacy/pytorch-lightning/run_ner.py
examples/legacy/pytorch-lightning/run_ner.sh
examples/legacy/pytorch-lightning/run_pos.sh
examples/legacy/question-answering/run_squad.py
examples/legacy/question-answering/run_squad_trainer.py
examples/legacy/run_camembert.py
examples/legacy/run_chinese_ref.py
examples/legacy/run_language_modeling.py
examples/legacy/run_openai_gpt.py
examples/legacy/run_swag.py
examples/legacy/run_transfo_xl.py
examples/legacy/token-classification/README.md
examples/legacy/token-classification/run.sh
examples/legacy/token-classification/run_chunk.sh
examples/legacy/token-classification/run_ner.py
examples/legacy/token-classification/run_pos.sh
examples/legacy/token-classification/scripts/preprocess.py
examples/legacy/token-classification/tasks.py
examples/legacy/token-classification/utils_ner.py
examples/multiple-choice/README.md
examples/multiple-choice/requirements.txt
examples/question-answering/README.md
examples/question-answering/requirements.txt
examples/research_projects/README.md
examples/research_projects/adversarial/README.md
examples/research_projects/adversarial/requirements.txt
examples/research_projects/adversarial/run_hans.py
examples/research_projects/adversarial/utils_hans.py
examples/research_projects/bert-loses-patience/README.md
examples/research_projects/bert-loses-patience/pabee/__init__.py
examples/research_projects/bert-loses-patience/pabee/modeling_pabee_albert.py
examples/research_projects/bert-loses-patience/pabee/modeling_pabee_bert.py
examples/research_projects/bert-loses-patience/requirements.txt
examples/research_projects/bert-loses-patience/run_glue_with_pabee.py
examples/research_projects/bert-loses-patience/test_run_glue_with_pabee.py
examples/research_projects/bertabs/README.md
examples/research_projects/bertabs/__init__.py
examples/research_projects/bertabs/configuration_bertabs.py
examples/research_projects/bertabs/convert_bertabs_original_pytorch_checkpoint.py
examples/research_projects/bertabs/modeling_bertabs.py
examples/research_projects/bertabs/requirements.txt
examples/research_projects/bertabs/run_summarization.py
examples/research_projects/bertabs/test_utils_summarization.py
examples/research_projects/bertabs/utils_summarization.py
examples/research_projects/bertology/requirements.txt
examples/research_projects/bertology/run_bertology.py
examples/research_projects/deebert/README.md
examples/research_projects/deebert/entropy_eval.sh
examples/research_projects/deebert/eval_deebert.sh
examples/research_projects/deebert/requirements.txt
examples/research_projects/deebert/run_glue_deebert.py
examples/research_projects/deebert/src/__init__.py
examples/research_projects/deebert/src/modeling_highway_bert.py
examples/research_projects/deebert/src/modeling_highway_roberta.py
examples/research_projects/deebert/test_glue_deebert.py
examples/research_projects/deebert/train_deebert.sh
examples/research_projects/distillation/README.md
examples/research_projects/distillation/distiller.py
examples/research_projects/distillation/grouped_batch_sampler.py
examples/research_projects/distillation/lm_seqs_dataset.py
examples/research_projects/distillation/requirements.txt
examples/research_projects/distillation/run_squad_w_distillation.py
examples/research_projects/distillation/scripts/binarized_data.py
examples/research_projects/distillation/scripts/extract.py
examples/research_projects/distillation/scripts/extract_distilbert.py
examples/research_projects/distillation/scripts/token_counts.py
examples/research_projects/distillation/train.py
examples/research_projects/distillation/training_configs/distilbert-base-cased.json
examples/research_projects/distillation/training_configs/distilbert-base-multilingual-cased.json
examples/research_projects/distillation/training_configs/distilbert-base-uncased.json
examples/research_projects/distillation/training_configs/distilgpt2.json
examples/research_projects/distillation/training_configs/distilroberta-base.json
examples/research_projects/distillation/utils.py
examples/research_projects/longform-qa/README.md
examples/research_projects/longform-qa/eli5_app.py
examples/research_projects/longform-qa/eli5_utils.py
examples/research_projects/longform-qa/requirements.txt
examples/research_projects/mm-imdb/README.md
examples/research_projects/mm-imdb/run_mmimdb.py
examples/research_projects/mm-imdb/utils_mmimdb.py
examples/research_projects/movement-pruning/README.md
examples/research_projects/movement-pruning/Saving_PruneBERT.ipynb
examples/research_projects/movement-pruning/bertarize.py
examples/research_projects/movement-pruning/counts_parameters.py
examples/research_projects/movement-pruning/emmental/__init__.py
examples/research_projects/movement-pruning/emmental/configuration_bert_masked.py
examples/research_projects/movement-pruning/emmental/modeling_bert_masked.py
examples/research_projects/movement-pruning/emmental/modules/__init__.py
examples/research_projects/movement-pruning/emmental/modules/binarizer.py
examples/research_projects/movement-pruning/emmental/modules/masked_nn.py
examples/research_projects/movement-pruning/lxmert/README.md
examples/research_projects/movement-pruning/lxmert/demo.ipynb
examples/research_projects/movement-pruning/lxmert/extracting_data.py
examples/research_projects/movement-pruning/lxmert/modeling_frcnn.py
examples/research_projects/movement-pruning/lxmert/processing_image.py
examples/research_projects/movement-pruning/lxmert/requirements.txt
examples/research_projects/movement-pruning/lxmert/utils.py
examples/research_projects/movement-pruning/lxmert/visualizing_image.py
examples/research_projects/movement-pruning/masked_run_glue.py
examples/research_projects/movement-pruning/masked_run_squad.py
examples/research_projects/movement-pruning/requirements.txt
examples/research_projects/pplm/README.md
examples/research_projects/pplm/imgs/headfigure.png
examples/research_projects/pplm/imgs/wooly.png
examples/research_projects/pplm/pplm_classification_head.py
examples/research_projects/pplm/requirements.txt
examples/research_projects/pplm/run_pplm.py
examples/research_projects/pplm/run_pplm_discrim_train.py
examples/research_projects/rag/README.md
examples/research_projects/rag/__init__.py
examples/research_projects/rag/_test_finetune_rag.py
examples/research_projects/rag/callbacks_rag.py
examples/research_projects/rag/consolidate_rag_checkpoint.py
examples/research_projects/rag/distributed_retriever.py
examples/research_projects/rag/eval_rag.py
examples/research_projects/rag/finetune_rag.py
examples/research_projects/rag/finetune_rag.sh
examples/research_projects/rag/lightning_base.py
examples/research_projects/rag/parse_dpr_relevance_data.py
examples/research_projects/rag/requirements.txt
examples/research_projects/rag/test_data/my_knowledge_dataset.csv
examples/research_projects/rag/test_distributed_retriever.py
examples/research_projects/rag/use_own_knowledge_dataset.py
examples/research_projects/rag/utils_rag.py
examples/research_projects/seq2seq-distillation/README.md
examples/research_projects/seq2seq-distillation/_test_bash_script.py
examples/research_projects/seq2seq-distillation/_test_make_student.py
examples/research_projects/seq2seq-distillation/_test_seq2seq_examples.py
examples/research_projects/seq2seq-distillation/_test_seq2seq_examples_multi_gpu.py
examples/research_projects/seq2seq-distillation/callbacks.py
examples/research_projects/seq2seq-distillation/convert_pl_checkpoint_to_hf.py
examples/research_projects/seq2seq-distillation/distil_marian_enro_teacher.sh
examples/research_projects/seq2seq-distillation/distil_marian_no_teacher.sh
examples/research_projects/seq2seq-distillation/distillation.py
examples/research_projects/seq2seq-distillation/dynamic_bs_example.sh
examples/research_projects/seq2seq-distillation/finetune.py
examples/research_projects/seq2seq-distillation/finetune.sh
examples/research_projects/seq2seq-distillation/finetune_bart_tiny.sh
examples/research_projects/seq2seq-distillation/finetune_pegasus_xsum.sh
examples/research_projects/seq2seq-distillation/finetune_t5.sh
examples/research_projects/seq2seq-distillation/lightning_base.py
examples/research_projects/seq2seq-distillation/make_student.py
examples/research_projects/seq2seq-distillation/precomputed_pseudo_labels.md
examples/research_projects/seq2seq-distillation/requirements.txt
examples/research_projects/seq2seq-distillation/run_eval.py
examples/research_projects/seq2seq-distillation/sentence_splitter.py
examples/research_projects/seq2seq-distillation/train_distilbart_cnn.sh
examples/research_projects/seq2seq-distillation/train_distilbart_xsum.sh
examples/research_projects/seq2seq-distillation/train_mbart_cc25_enro.sh
examples/research_projects/seq2seq-distillation/utils copy.py
examples/research_projects/seq2seq-distillation/utils.py
examples/seq2seq/README.md
examples/seq2seq/builtin_trainer/finetune.sh
examples/seq2seq/builtin_trainer/finetune_tpu.sh
examples/seq2seq/builtin_trainer/train_distilbart_cnn.sh
examples/seq2seq/builtin_trainer/train_mbart_cc25_enro.sh
examples/seq2seq/convert_model_to_fp16.py
examples/seq2seq/download_wmt.py
examples/seq2seq/finetune.sh
examples/seq2seq/finetune_tpu.sh
examples/seq2seq/finetune_trainer.py
examples/seq2seq/minify_dataset.py
examples/seq2seq/pack_dataset.py
examples/seq2seq/requirements.txt
examples/seq2seq/rouge_cli.py
examples/seq2seq/run_distributed_eval.py
examples/seq2seq/run_eval.py
examples/seq2seq/run_eval_search.py
examples/seq2seq/save_len_file.py
examples/seq2seq/save_randomly_initialized_model.py
examples/seq2seq/sentence_splitter.py
examples/seq2seq/seq2seq_trainer.py
examples/seq2seq/seq2seq_training_args.py
examples/seq2seq/test_calculate_rouge.py
examples/seq2seq/test_data/test_data
examples/seq2seq/test_data/wmt_en_ro/train.len
examples/seq2seq/test_data/wmt_en_ro/val.len
examples/seq2seq/test_datasets.py
examples/seq2seq/test_finetune_trainer.py
examples/seq2seq/test_seq2seq_examples.py
examples/seq2seq/test_seq2seq_examples_multi_gpu.py
examples/seq2seq/test_tatoeba_conversion.py
examples/seq2seq/train_distil_marian_enro.sh
examples/seq2seq/train_distil_marian_enro_tpu.sh
examples/seq2seq/train_distilbart_cnn.sh
examples/seq2seq/train_mbart_cc25_enro.sh
examples/seq2seq/utils.py
examples/seq2seq/xla_spawn.py
examples/test_examples.py
examples/text-classification/README.md
examples/text-classification/requirements.txt
examples/text-classification/run_tf_glue.py
examples/text-classification/run_tf_text_classification.py
examples/text-generation/README.md
examples/text-generation/requirements.txt
examples/token-classification/README.md
examples/token-classification/requirements.txt
examples/token-classification/run.sh
examples/token-classification/test_ner_examples.py
examples/xla_spawn.py
==================
86896de06;Suraj Patil;2020-12-11 20:29:15 +0530;update tatoeba workflow (#9051)

==

scripts/tatoeba/README.md
scripts/tatoeba/upload_models.sh
==================
7c8f5f648;Ganesh Kharad;2020-12-11 20:15:12 +0530;Create README.md (#8096)
* Create README.md

* Fix model card

Co-authored-by: Julien Chaumond <julien@huggingface.co>
==

model_cards/ganeshkharad/gk-hinglish-sentiment/README.md
==================
5527f7872;RamonMamon;2020-12-11 22:41:29 +0800;Create README.md (#8281)
* Create README.md

* Update model_cards/kiri-ai/distiluse-base-multilingual-cased-et/README.md

Co-authored-by: Julien Chaumond <chaumond@gmail.com>
==

model_cards/kiri-ai/distiluse-base-multilingual-cased-et/README.md
==================
c615df742;joangines;2020-12-11 23:40:14 +0900;Create README.md (#8751)
* Create README.md

* Update model_cards/Cinnamon/electra-small-japanese-generator/README.md

Co-authored-by: Julien Chaumond <chaumond@gmail.com>
==

model_cards/Cinnamon/electra-small-japanese-generator/README.md
==================
76df55938;Ahmed Abdelali;2020-12-11 17:38:38 +0300;QARiB Arabic and dialects models (#8796)
* Add QARiB models

* fix README.md

* Fix README.md

* Fix README.md

* Fix README.md

* Fix QARiB files

* add models card for QARiB models 860k, 1790k, and 1970k

* try to fix PR

* re-add files

* links aren't allowed here :)

Co-authored-by: Ahmed Abdelali <aabdelali@hbku.edu.qa>
Co-authored-by: Julien Chaumond <julien@huggingface.co>
==

model_cards/qarib/bert-base-qarib60_1790k/README.md
model_cards/qarib/bert-base-qarib60_1970k/README.md
model_cards/qarib/bert-base-qarib60_860k/README.md
==================
b161f1ae5;moniquebm;2020-12-11 11:24:21 -0300;Update README.md (#8820)

==

model_cards/monilouise/ner_pt_br/README.md
==================
649d389da;Panggi Libersa Jasri Akadol;2020-12-11 21:18:16 +0700;Initial README for `t5-base-indonesian-summarization-cased` model (#9028)
* Create README.md

Initial README for `t5-base-indonesian-summarization-cased` model

* Update README for t5-base-indonesian-summarization-cased

Typo in README, change from `small` to `base`
==

model_cards/panggi/t5-base-indonesian-summarization-cased/README.md
==================
5e794b662;Panggi Libersa Jasri Akadol;2020-12-11 21:17:29 +0700;Create README.md (#9030)
Initial README for `t5-small-indonesian-summarization-cased` model
==

model_cards/panggi/t5-small-indonesian-summarization-cased/README.md
==================
935e34695;Cola;2020-12-11 18:40:25 +0900;:art: Change nn.dropout to layer.Dropout (#9047)

==

src/transformers/models/bart/modeling_tf_bart.py
==================
b01ddc957;Julien Plu;2020-12-10 23:17:19 +0100;Remove value error (#8985)
* Remove value error

* Try a fix for parameter ordering

* Restore previous behavior

* Add documentation

* Review the comment
==

src/transformers/modeling_tf_utils.py
==================
91ab02af2;NatLun137;2020-12-10 22:41:00 +0100;Fix typo #9012 (#1) (#9038)
There is a tiny typo in the code "transformers/examples/language-modeling/run_mlm_wwm.py" at line 284. [Details.](https://github.com/huggingface/transformers/issues/9012)
==

examples/language-modeling/run_mlm_wwm.py
==================
8d4bb0205;Sylvain Gugger;2020-12-10 15:57:39 -0500;Refactor FLAX tests (#9034)

==

src/transformers/tokenization_utils_base.py
tests/test_modeling_flax_bert.py
tests/test_modeling_flax_common.py
tests/test_modeling_flax_roberta.py
==================
1310e1a75;Sylvain Gugger;2020-12-10 11:57:12 -0500;Enforce all objects in the main init are documented (#9014)

==

docs/source/internal/generation_utils.rst
docs/source/internal/trainer_utils.rst
docs/source/main_classes/optimizer_schedules.rst
docs/source/main_classes/pipelines.rst
docs/source/model_doc/albert.rst
docs/source/model_doc/auto.rst
docs/source/model_doc/bart.rst
docs/source/model_doc/barthez.rst
docs/source/model_doc/bert.rst
docs/source/model_doc/camembert.rst
docs/source/model_doc/layoutlm.rst
docs/source/model_doc/mbart.rst
docs/source/model_doc/mt5.rst
docs/source/model_doc/pegasus.rst
docs/source/model_doc/reformer.rst
docs/source/model_doc/t5.rst
docs/source/model_doc/transformerxl.rst
docs/source/model_doc/xlmroberta.rst
docs/source/model_doc/xlnet.rst
utils/check_repo.py
==================
51e81e589;Sylvain Gugger;2020-12-10 09:29:38 -0500;MPNet copyright files (#9015)

==

docs/source/model_doc/mpnet.rst
src/transformers/models/mpnet/__init__.py
==================
35bffd70e;Sylvain Gugger;2020-12-10 09:28:49 -0500;Fix documention of book in LayoutLM (#9017)

==

src/transformers/models/layoutlm/modeling_layoutlm.py
==================
c95de29e3;Cola;2020-12-10 16:22:52 +0900;:pencil2: Fix typo (#9020)

==

src/transformers/models/bart/modeling_tf_bart.py
==================
5e637e6c6;Stas Bekman;2020-12-09 12:36:36 -0800;[wip] [ci] doc-job-skip take #4 dry-run (#8980)
* ci-doc-job-skip-take-4

* wip

* wip

* wip

* wip

* skip yaml

* wip

* wip

* wip

* wip

* wip

* wip

* wip

* wip

* wip

* wip

* wip

* wip

* ready to test

* yet another way

* trying with HEAD

* trying with head.sha

* trying with head.sha fix

* trying with head.sha fix wip

* undo

* try to switch to sha

* current branch

* current branch

* PR number check

* joy ride

* joy ride

* joy ride

* joy ride

* joy ride

* joy ride

* joy ride

* joy ride

* joy ride

* joy ride

* joy ride

* joy ride
==

.circleci/config.yml
==================
06971ac4f;Patrick von Platen;2020-12-09 20:55:24 +0100;[Bart] Refactor - fix issues, consistency with the library, naming (#8900)
* remove make on the fly linear embedding

* start refactor

* big first refactor

* save intermediate

* save intermediat

* correct mask issue

* save tests

* refactor padding masks

* make all tests pass

* further refactor

* make pegasus test pass

* fix bool if

* fix leftover tests

* continue

* bart renaming

* delete torchscript test hack

* fix imports in tests

* correct shift

* fix docs and repo cons

* re-add fix for FSTM

* typo in test

* fix typo

* fix another typo

* continue

* hot fix 2 for tf

* small fixes

* refactor types linting

* continue

* finish refactor

* fix import in tests

* better bart names

* further refactor and add test

* delete hack

* apply sylvains and lysandres commens

* small perf improv

* further perf improv

* improv perf

* fix typo

* make style

* small perf improv
==

docs/source/model_doc/bart.rst
src/transformers/__init__.py
src/transformers/models/bart/__init__.py
src/transformers/models/bart/modeling_bart.py
src/transformers/models/bart/modeling_tf_bart.py
src/transformers/models/t5/modeling_t5.py
src/transformers/utils/dummy_pt_objects.py
tests/test_modeling_bart.py
tests/test_modeling_blenderbot.py
tests/test_modeling_common.py
tests/test_modeling_rag.py
tests/test_modeling_tf_common.py
utils/check_repo.py
==================
75627148e;Funtowicz Morgan;2020-12-09 17:13:56 +0100;Flax Masked Language Modeling training example (#8728)
* Remove "Model" suffix from Flax models to look more :hugs:

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Initial working (forward + backward) for Flax MLM training example.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Simply code

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Addressing comments, using module and moving to LM task.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Restore parameter name "module" wrongly renamed model.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Restore correct output ordering...

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Actually commit the example :sweat_smile:

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Add FlaxBertModelForMaskedLM after rebasing.

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* Make it possible to initialize the training from scratch

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* Reuse flax linen example of cross entropy loss

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* Added specific data collator for flax

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* Remove todo for data collator

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* Added evaluation step

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* Added ability to provide dtype to support bfloat16 on TPU

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* Enable flax tensorboard output

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* Enable jax.pmap support.

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* Ensure batches are correctly sized to be dispatched with jax.pmap

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* Enable bfloat16 with --fp16 cmdline args

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* Correctly export metrics to tensorboard

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* Added dropout and ability to use it.

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* Effectively enable & disable during training and evaluation steps.

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* Oops.

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* Enable specifying kernel initializer scale

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* Style.

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* Added warmup step to the learning rate scheduler.

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* Fix typo.

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* Print training loss

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* Make style

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* fix linter issue (flake8)

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* Fix model matching

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* Fix dummies

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* Fix non default dtype on Flax models

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* Use the same create_position_ids_from_input_ids for FlaxRoberta

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* Make Roberta attention as Bert

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* fix copy

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* Wording.

Co-authored-by: Marc van Zee <marcvanzee@gmail.com>

Co-authored-by: Marc van Zee <marcvanzee@gmail.com>
==

examples/language-modeling/run_mlm_flax.py
src/transformers/__init__.py
src/transformers/modeling_flax_utils.py
src/transformers/models/bert/__init__.py
src/transformers/models/bert/modeling_flax_bert.py
src/transformers/models/roberta/modeling_flax_roberta.py
src/transformers/utils/dummy_flax_objects.py
tests/test_modeling_flax_roberta.py
==================
df2af6d8b;StillKeepTry;2020-12-09 10:25:31 -0500;Add MP Net 2 (#9004)

==

README.md
docs/source/index.rst
docs/source/model_doc/mpnet.rst
src/transformers/__init__.py
src/transformers/data/processors/squad.py
src/transformers/models/auto/configuration_auto.py
src/transformers/models/auto/modeling_auto.py
src/transformers/models/auto/modeling_tf_auto.py
src/transformers/models/auto/tokenization_auto.py
src/transformers/models/mpnet/__init__.py
src/transformers/models/mpnet/configuration_mpnet.py
src/transformers/models/mpnet/modeling_mpnet.py
src/transformers/models/mpnet/modeling_tf_mpnet.py
src/transformers/models/mpnet/tokenization_mpnet.py
src/transformers/models/mpnet/tokenization_mpnet_fast.py
src/transformers/utils/dummy_pt_objects.py
src/transformers/utils/dummy_tf_objects.py
src/transformers/utils/dummy_tokenizers_objects.py
tests/test_modeling_mpnet.py
tests/test_modeling_tf_mpnet.py
tests/test_tokenization_mpnet.py
==================
872910985;cronoik;2020-12-09 16:21:41 +0100;fixes #8968 (#9009)

==

notebooks/02-transformers.ipynb
==================
e977ed214;Simon Brandeis;2020-12-09 15:43:19 +0100;Add the code_search_net dataset tag to CodeBERTa model cards (#9005)

==

model_cards/huggingface/CodeBERTa-language-id/README.md
model_cards/huggingface/CodeBERTa-small-v1/README.md
==================
da37a21c8;Patrick von Platen;2020-12-09 15:14:33 +0100;push (#9008)

==

docs/source/internal/generation_utils.rst
src/transformers/generation_logits_process.py
==================
61abd50b9;Sylvain Gugger;2020-12-09 09:13:41 -0500;Remove use of deprected method in Trainer HP search (#8996)

==

src/transformers/trainer.py
==================
7e1d709e2;Sylvain Gugger;2020-12-09 09:11:39 -0500;Fix link to stable version in the doc navbar (#9007)

==

docs/source/_static/js/custom.js
==================
02d0e0355;Patrick von Platen;2020-12-09 15:00:37 +0100;Diverse beam search 2 (#9006)
* diverse beam search

* bug fixes

* bug fixes

* bug fix

* separate out diverse_beam_search function

* separate out diverse_beam_search function

* bug fix

* improve code quality

* bug fix

* bug fix

* separate out diverse beam search scorer

* code format

* code format

* code format

* code format

* add test

* code format

* documentation changes

* code quality

* add slow integration tests

* more general name

* refactor into logits processor

* add test

* avoid too much copy paste

* refactor

* add to docs

* fix-copies

* bug fix

* Revert "bug fix"

This reverts commit c99eb5a8dc57a7b0d33a8ac06d8c6a32a7812ad4.

* improve comment

* implement sylvains feedback

Co-authored-by: Ayush Jain <a.jain@sprinklr.com>
Co-authored-by: ayushtiku5 <40797286+ayushtiku5@users.noreply.github.com>
==

docs/source/internal/generation_utils.rst
src/transformers/__init__.py
src/transformers/configuration_utils.py
src/transformers/generation_beam_search.py
src/transformers/generation_logits_process.py
src/transformers/generation_utils.py
src/transformers/models/rag/modeling_rag.py
src/transformers/utils/dummy_pt_objects.py
tests/test_generation_logits_process.py
tests/test_generation_utils.py
==================
67ff1c314;Lysandre Debut;2020-12-08 18:00:07 -0500;Templates overhaul 1 (#8993)

==

.github/workflows/model-templates.yml
src/transformers/commands/add_new_model.py
src/transformers/file_utils.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/__init__.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_tf_{{cookiecutter.lowercase_modelname}}.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_{{cookiecutter.lowercase_modelname}}.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/test_modeling_tf_{{cookiecutter.lowercase_modelname}}.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/test_modeling_{{cookiecutter.lowercase_modelname}}.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/to_replace_{{cookiecutter.lowercase_modelname}}.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/tokenization_fast_{{cookiecutter.lowercase_modelname}}.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/tokenization_{{cookiecutter.lowercase_modelname}}.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/{{cookiecutter.lowercase_modelname}}.rst
templates/adding_a_new_model/tests/encoder-bert-tokenizer.json
templates/adding_a_new_model/tests/pt-encoder-bert-tokenizer.json
templates/adding_a_new_model/tests/standalone.json
templates/adding_a_new_model/tests/tf-encoder-bert-tokenizer.json
==================
447808c85;Sylvain Gugger;2020-12-08 14:39:29 -0500;New squad example (#8992)
* Add new SQUAD example

* Same with a task-specific Trainer

* Address review comment.

* Small fixes

* Initial work for XLNet

* Apply suggestions from code review

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* Final clean up and working XLNet script

* Test and debug

* Final working version

* Add new SQUAD example

* Same with a task-specific Trainer

* Address review comment.

* Small fixes

* Initial work for XLNet

* Apply suggestions from code review

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* Final clean up and working XLNet script

* Test and debug

* Final working version

* Add tick

* Update README

* Address review comments

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
==

.gitignore
examples/README.md
examples/question-answering/README.md
examples/question-answering/run_qa.py
examples/question-answering/run_qa_beam_search.py
examples/question-answering/squad_v2_local/evaluate.py
examples/question-answering/squad_v2_local/squad_v2_local.py
examples/question-answering/trainer_qa.py
examples/question-answering/utils_qa.py
examples/test_examples.py
tests/fixtures/tests_samples/SQUAD/dev-v2.0.json
tests/fixtures/tests_samples/SQUAD/sample.json
tests/fixtures/tests_samples/SQUAD/train-v2.0.json
==================
7809eb82a;guillaume-be;2020-12-08 18:04:34 +0100;Removed unused `encoder_hidden_states` and `encoder_attention_mask`  (#8972)
* Removed unused `encoder_hidden_states` and `encoder_attention_mask` from MobileBert

* Removed decoder tests for MobileBert

* Removed now unnecessary import
==

src/transformers/models/mobilebert/modeling_mobilebert.py
tests/test_modeling_mobilebert.py
==================
b7cdd00f1;Lysandre Debut;2020-12-08 12:04:01 -0500;Fix interaction of return_token_type_ids and add_special_tokens (#8854)

==

src/transformers/tokenization_utils_base.py
==================
04c446f76;Sylvain Gugger;2020-12-08 11:59:40 -0500;Make `ModelOutput` pickle-able (#8989)

==

src/transformers/modeling_outputs.py
==================
0d9e6ca9e;Julien Chaumond;2020-12-08 09:58:45 -0500;[model_card] remove bogus testing changes

==

model_cards/huggingface/CodeBERTa-language-id/README.md
==================
bf7f79cd5;Julien Plu;2020-12-08 15:14:09 +0100;Optional layers (#8961)
* Apply on BERT and ALBERT

* Update TF Bart

* Add input processing to TF BART

* Add input processing for TF CTRL

* Add input processing to TF Distilbert

* Add input processing to TF DPR

* Add input processing to TF Electra

* Add deprecated arguments

* Add input processing to TF XLM

* remove unused imports

* Add input processing to TF Funnel

* Add input processing to TF GPT2

* Add input processing to TF Longformer

* Add input processing to TF Lxmert

* Apply style

* Add input processing to TF Mobilebert

* Add input processing to TF GPT

* Add input processing to TF Roberta

* Add input processing to TF T5

* Add input processing to TF TransfoXL

* Apply style

* Rebase on master

* Fix wrong model name

* Fix BART

* Apply style

* Put the deprecated warnings in the input processing function

* Remove the unused imports

* Raise an error when len(kwargs)>0

* test ModelOutput instead of TFBaseModelOutput

* Address Patrick's comments

* Address Patrick's comments

* Add boolean processing for the inputs

* Take into account the optional layers

* Add missing/unexpected weights in the other models

* Apply style

* rename parameters

* Apply style

* Remove useless

* Remove useless

* Remove useless

* Update num parameters

* Fix tests

* Address Patrick's comment

* Remove useless attribute
==

src/transformers/models/albert/modeling_tf_albert.py
src/transformers/models/bart/modeling_tf_bart.py
src/transformers/models/bert/modeling_tf_bert.py
src/transformers/models/electra/modeling_tf_electra.py
src/transformers/models/funnel/modeling_tf_funnel.py
src/transformers/models/gpt2/modeling_tf_gpt2.py
src/transformers/models/longformer/modeling_tf_longformer.py
src/transformers/models/lxmert/modeling_tf_lxmert.py
src/transformers/models/mobilebert/modeling_tf_mobilebert.py
src/transformers/models/roberta/modeling_tf_roberta.py
src/transformers/models/t5/modeling_tf_t5.py
src/transformers/models/transfo_xl/modeling_tf_transfo_xl.py
src/transformers/models/xlm/modeling_tf_xlm.py
src/transformers/models/xlnet/modeling_tf_xlnet.py
tests/test_modeling_tf_auto.py
tests/test_modeling_tf_bert.py
tests/test_modeling_tf_pytorch.py
==================
9d7d0005b;Stas Bekman;2020-12-07 21:59:55 -0800;[training] SAVE_STATE_WARNING was removed in pytorch (#8979)
* [training] SAVE_STATE_WARNING was removed in pytorch

FYI `SAVE_STATE_WARNING` has been removed 3 days ago: pytorch/pytorch#46813

Fixes: #8232

@sgugger

* style, but add () to prevent autoformatters from botching it

* switch to try/except

* cleanup
==

src/transformers/trainer_pt_utils.py
==================
2ae7388ee;Lysandre Debut;2020-12-07 19:55:12 -0500;Check table as independent script (#8976)

==

.circleci/config.yml
Makefile
utils/check_copies.py
utils/check_table.py
==================
00aa9dbca;Sylvain Gugger;2020-12-07 18:36:34 -0500;Copyright (#8970)
* Add copyright everywhere missing

* Style
==

CONTRIBUTING.md
LICENSE
README.md
docs/README.md
docs/source/benchmarks.rst
docs/source/bertology.rst
docs/source/conf.py
docs/source/converting_tensorflow_models.rst
docs/source/custom_datasets.rst
docs/source/glossary.rst
docs/source/index.rst
docs/source/installation.md
docs/source/internal/generation_utils.rst
docs/source/internal/modeling_utils.rst
docs/source/internal/pipelines_utils.rst
docs/source/internal/tokenization_utils.rst
docs/source/internal/trainer_utils.rst
docs/source/main_classes/callback.rst
docs/source/main_classes/configuration.rst
docs/source/main_classes/logging.rst
docs/source/main_classes/model.rst
docs/source/main_classes/optimizer_schedules.rst
docs/source/main_classes/output.rst
docs/source/main_classes/pipelines.rst
docs/source/main_classes/processors.rst
docs/source/main_classes/tokenizer.rst
docs/source/main_classes/trainer.rst
docs/source/migration.md
docs/source/model_doc/albert.rst
docs/source/model_doc/auto.rst
docs/source/model_doc/bart.rst
docs/source/model_doc/barthez.rst
docs/source/model_doc/bert.rst
docs/source/model_doc/bertgeneration.rst
docs/source/model_doc/blenderbot.rst
docs/source/model_doc/camembert.rst
docs/source/model_doc/ctrl.rst
docs/source/model_doc/deberta.rst
docs/source/model_doc/dialogpt.rst
docs/source/model_doc/distilbert.rst
docs/source/model_doc/dpr.rst
docs/source/model_doc/electra.rst
docs/source/model_doc/encoderdecoder.rst
docs/source/model_doc/flaubert.rst
docs/source/model_doc/fsmt.rst
docs/source/model_doc/funnel.rst
docs/source/model_doc/gpt.rst
docs/source/model_doc/gpt2.rst
docs/source/model_doc/layoutlm.rst
docs/source/model_doc/longformer.rst
docs/source/model_doc/lxmert.rst
docs/source/model_doc/marian.rst
docs/source/model_doc/mbart.rst
docs/source/model_doc/mobilebert.rst
docs/source/model_doc/mt5.rst
docs/source/model_doc/pegasus.rst
docs/source/model_doc/prophetnet.rst
docs/source/model_doc/rag.rst
docs/source/model_doc/reformer.rst
docs/source/model_doc/retribert.rst
docs/source/model_doc/roberta.rst
docs/source/model_doc/squeezebert.rst
docs/source/model_doc/t5.rst
docs/source/model_doc/transformerxl.rst
docs/source/model_doc/xlm.rst
docs/source/model_doc/xlmprophetnet.rst
docs/source/model_doc/xlmroberta.rst
docs/source/model_doc/xlnet.rst
docs/source/model_sharing.rst
docs/source/model_summary.rst
docs/source/multilingual.rst
docs/source/perplexity.rst
docs/source/philosophy.rst
docs/source/preprocessing.rst
docs/source/pretrained_models.rst
docs/source/quicktour.rst
docs/source/serialization.rst
docs/source/task_summary.rst
docs/source/testing.rst
docs/source/tokenizer_summary.rst
docs/source/training.rst
examples/README.md
hubconf.py
notebooks/README.md
scripts/fsmt/convert-allenai-wmt16.sh
scripts/fsmt/convert-allenai-wmt19.sh
scripts/fsmt/convert-facebook-wmt19.sh
scripts/fsmt/eval-allenai-wmt16.sh
scripts/fsmt/eval-allenai-wmt19.sh
scripts/fsmt/eval-facebook-wmt19.sh
scripts/fsmt/fsmt-make-super-tiny-model.py
scripts/fsmt/fsmt-make-tiny-model.py
scripts/fsmt/gen-card-allenai-wmt16.py
scripts/fsmt/gen-card-allenai-wmt19.py
scripts/fsmt/gen-card-facebook-wmt19.py
scripts/fsmt/s3-move.sh
scripts/fsmt/tests-to-run.sh
scripts/pegasus/build_test_sample_spm_no_bos.py
scripts/tatoeba/README.md
setup.py
src/transformers/__init__.py
src/transformers/activations.py
src/transformers/activations_tf.py
src/transformers/benchmark/benchmark_utils.py
src/transformers/commands/__init__.py
src/transformers/commands/add_new_model.py
src/transformers/commands/convert.py
src/transformers/commands/download.py
src/transformers/commands/env.py
src/transformers/commands/run.py
src/transformers/commands/serving.py
src/transformers/commands/train.py
src/transformers/commands/transformers_cli.py
src/transformers/commands/user.py
src/transformers/convert_graph_to_onnx.py
src/transformers/data/__init__.py
src/transformers/data/data_collator.py
src/transformers/data/datasets/__init__.py
src/transformers/data/datasets/glue.py
src/transformers/data/datasets/language_modeling.py
src/transformers/data/datasets/squad.py
src/transformers/data/metrics/squad_metrics.py
src/transformers/data/processors/__init__.py
src/transformers/data/processors/squad.py
src/transformers/data/test_generation_utils.py
src/transformers/dependency_versions_check.py
src/transformers/file_utils.py
src/transformers/hf_argparser.py
src/transformers/integrations.py
src/transformers/modeling_outputs.py
src/transformers/modeling_tf_outputs.py
src/transformers/models/albert/__init__.py
src/transformers/models/auto/__init__.py
src/transformers/models/bart/__init__.py
src/transformers/models/barthez/__init__.py
src/transformers/models/bert/__init__.py
src/transformers/models/bert/convert_bert_original_tf2_checkpoint_to_pytorch.py
src/transformers/models/bert_generation/__init__.py
src/transformers/models/bert_japanese/__init__.py
src/transformers/models/bertweet/__init__.py
src/transformers/models/blenderbot/__init__.py
src/transformers/models/camembert/__init__.py
src/transformers/models/ctrl/__init__.py
src/transformers/models/deberta/__init__.py
src/transformers/models/dialogpt/convert_dialogpt_original_pytorch_checkpoint_to_pytorch.py
src/transformers/models/distilbert/__init__.py
src/transformers/models/dpr/__init__.py
src/transformers/models/dpr/configuration_dpr.py
src/transformers/models/dpr/convert_dpr_original_checkpoint_to_pytorch.py
src/transformers/models/dpr/modeling_dpr.py
src/transformers/models/dpr/modeling_tf_dpr.py
src/transformers/models/dpr/tokenization_dpr.py
src/transformers/models/dpr/tokenization_dpr_fast.py
src/transformers/models/electra/__init__.py
src/transformers/models/encoder_decoder/__init__.py
src/transformers/models/flaubert/__init__.py
src/transformers/models/fsmt/__init__.py
src/transformers/models/funnel/__init__.py
src/transformers/models/gpt2/__init__.py
src/transformers/models/herbert/__init__.py
src/transformers/models/layoutlm/__init__.py
src/transformers/models/longformer/__init__.py
src/transformers/models/lxmert/__init__.py
src/transformers/models/marian/__init__.py
src/transformers/models/marian/convert_marian_tatoeba_to_pytorch.py
src/transformers/models/marian/convert_marian_to_pytorch.py
src/transformers/models/marian/tokenization_marian.py
src/transformers/models/mbart/__init__.py
src/transformers/models/mbart/convert_mbart_original_checkpoint_to_pytorch.py
src/transformers/models/mbart/modeling_mbart.py
src/transformers/models/mmbt/__init__.py
src/transformers/models/mobilebert/__init__.py
src/transformers/models/mobilebert/configuration_mobilebert.py
src/transformers/models/mobilebert/convert_mobilebert_original_tf_checkpoint_to_pytorch.py
src/transformers/models/mobilebert/tokenization_mobilebert.py
src/transformers/models/mobilebert/tokenization_mobilebert_fast.py
src/transformers/models/mt5/__init__.py
src/transformers/models/openai/__init__.py
src/transformers/models/pegasus/__init__.py
src/transformers/models/phobert/__init__.py
src/transformers/models/prophetnet/__init__.py
src/transformers/models/rag/__init__.py
src/transformers/models/reformer/__init__.py
src/transformers/models/retribert/__init__.py
src/transformers/models/roberta/__init__.py
src/transformers/models/squeezebert/__init__.py
src/transformers/models/t5/__init__.py
src/transformers/models/transfo_xl/__init__.py
src/transformers/models/xlm/__init__.py
src/transformers/models/xlm_prophetnet/__init__.py
src/transformers/models/xlm_roberta/__init__.py
src/transformers/models/xlnet/__init__.py
src/transformers/optimization_tf.py
src/transformers/testing_utils.py
src/transformers/trainer_tf.py
src/transformers/training_args.py
src/transformers/training_args_tf.py
src/transformers/utils/hp_naming.py
src/transformers/utils/model_parallel_utils.py
src/transformers/utils/sentencepiece_model_pb2.py
src/transformers/utils/versions.py
templates/adding_a_new_example_script/README.md
templates/adding_a_new_example_script/{{cookiecutter.directory_name}}/run_{{cookiecutter.example_shortcut}}.py
templates/adding_a_new_model/README.md
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/__init__.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/configuration_{{cookiecutter.lowercase_modelname}}.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_tf_{{cookiecutter.lowercase_modelname}}.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_{{cookiecutter.lowercase_modelname}}.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/test_modeling_tf_{{cookiecutter.lowercase_modelname}}.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/test_modeling_{{cookiecutter.lowercase_modelname}}.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/to_replace_{{cookiecutter.lowercase_modelname}}.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/tokenization_{{cookiecutter.lowercase_modelname}}.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/{{cookiecutter.lowercase_modelname}}.rst
tests/conftest.py
tests/test_activations.py
tests/test_activations_tf.py
tests/test_benchmark.py
tests/test_benchmark_tf.py
tests/test_cli.py
tests/test_data_collator.py
tests/test_file_utils.py
tests/test_flax_auto.py
tests/test_hf_argparser.py
tests/test_logging.py
tests/test_modeling_albert.py
tests/test_modeling_auto.py
tests/test_modeling_bart.py
tests/test_modeling_bert.py
tests/test_modeling_bert_generation.py
tests/test_modeling_blenderbot.py
tests/test_modeling_camembert.py
tests/test_modeling_distilbert.py
tests/test_modeling_electra.py
tests/test_modeling_flaubert.py
tests/test_modeling_flax_bert.py
tests/test_modeling_flax_roberta.py
tests/test_modeling_gpt2.py
tests/test_modeling_layoutlm.py
tests/test_modeling_longformer.py
tests/test_modeling_lxmert.py
tests/test_modeling_mbart.py
tests/test_modeling_mobilebert.py
tests/test_modeling_mt5.py
tests/test_modeling_openai.py
tests/test_modeling_pegasus.py
tests/test_modeling_roberta.py
tests/test_modeling_tf_albert.py
tests/test_modeling_tf_auto.py
tests/test_modeling_tf_bert.py
tests/test_modeling_tf_blenderbot.py
tests/test_modeling_tf_camembert.py
tests/test_modeling_tf_ctrl.py
tests/test_modeling_tf_distilbert.py
tests/test_modeling_tf_dpr.py
tests/test_modeling_tf_electra.py
tests/test_modeling_tf_gpt2.py
tests/test_modeling_tf_longformer.py
tests/test_modeling_tf_lxmert.py
tests/test_modeling_tf_mobilebert.py
tests/test_modeling_tf_mt5.py
tests/test_modeling_tf_openai.py
tests/test_modeling_tf_pegasus.py
tests/test_modeling_tf_pytorch.py
tests/test_modeling_tf_roberta.py
tests/test_modeling_tf_t5.py
tests/test_modeling_tf_transfo_xl.py
tests/test_modeling_tf_xlm.py
tests/test_modeling_tf_xlm_roberta.py
tests/test_modeling_tf_xlnet.py
tests/test_modeling_transfo_xl.py
tests/test_modeling_xlm.py
tests/test_modeling_xlm_roberta.py
tests/test_modeling_xlnet.py
tests/test_onnx.py
tests/test_optimization.py
tests/test_optimization_tf.py
tests/test_pipelines_common.py
tests/test_pipelines_conversational.py
tests/test_pipelines_feature_extraction.py
tests/test_pipelines_fill_mask.py
tests/test_pipelines_ner.py
tests/test_pipelines_question_answering.py
tests/test_pipelines_sentiment_analysis.py
tests/test_pipelines_summarization.py
tests/test_pipelines_text2text_generation.py
tests/test_pipelines_text_generation.py
tests/test_pipelines_translation.py
tests/test_pipelines_zero_shot.py
tests/test_retrieval_rag.py
tests/test_tokenization_auto.py
tests/test_tokenization_bart.py
tests/test_tokenization_bert.py
tests/test_tokenization_bert_generation.py
tests/test_tokenization_bert_japanese.py
tests/test_tokenization_blenderbot.py
tests/test_tokenization_camembert.py
tests/test_tokenization_deberta.py
tests/test_tokenization_distilbert.py
tests/test_tokenization_fsmt.py
tests/test_tokenization_gpt2.py
tests/test_tokenization_layoutlm.py
tests/test_tokenization_lxmert.py
tests/test_tokenization_mbart.py
tests/test_tokenization_pegasus.py
tests/test_tokenization_rag.py
tests/test_tokenization_reformer.py
tests/test_tokenization_roberta.py
tests/test_tokenization_transfo_xl.py
tests/test_tokenization_utils.py
tests/test_tokenization_xlm.py
tests/test_tokenization_xlm_roberta.py
tests/test_tokenization_xlnet.py
tests/test_trainer_callback.py
tests/test_trainer_distributed.py
tests/test_trainer_tpu.py
tests/test_utils_check_copies.py
tests/test_versions_utils.py
utils/link_tester.py
==================
c108d0b5a;Navjot;2020-12-07 15:35:39 -0800;add max_length to showcase the use of truncation (#8975)

==

docs/source/quicktour.rst
==================
62d30e058;Sylvain Gugger;2020-12-07 17:32:09 -0500;Small fix to the run clm script (#8973)

==

examples/language-modeling/run_clm.py
==================
28fa014a1;Julien Chaumond;2020-12-07 22:38:39 +0100;transformers-cli: LFS multipart uploads (> 5GB) (#8663)
* initial commit

* [cli] lfs commands

* Fix FileSlice

* Tweak to FileSlice

* [hf_api] Backport filetype arg from `datasets`

cc @lhoestq

* Silm down the CI while i'm working

* Ok let's try this in CI

* Update config.yml

* Do not try this at home

* one more try

* Update lfs.py

* Revert "Tweak to FileSlice"

This reverts commit d7e32c4b3500400486411e85a2b74e57fb6b52f5.

* Update test_hf_api.py

* Update test_hf_api.py

* Update test_hf_api.py

* CI still green?

* make CI green again?

* Update test_hf_api.py

* make CI red again?

* Update test_hf_api.py

* add CI style back

* Fix CI?

* oh my

* doc + switch back to real staging endpoint

* Apply suggestions from code review

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
Co-authored-by: Pierric Cistac <Pierrci@users.noreply.github.com>

* Fix docblock + f-strings

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
Co-authored-by: Pierric Cistac <Pierrci@users.noreply.github.com>
==

.circleci/config.yml
src/transformers/commands/lfs.py
src/transformers/commands/transformers_cli.py
src/transformers/hf_api.py
src/transformers/testing_utils.py
tests/test_hf_api.py
==================
0bce7c550;Wietse de Vries;2020-12-07 22:04:14 +0100;Create README.md (#8964)

==

model_cards/wietsedv/bert-base-dutch-cased/README.md
==================
7ccd973ea;Nguyen Van Nha;2020-12-08 04:01:49 +0700;Update README.txt (#8957)

==

model_cards/NlpHUST/vibert4news-base-cased/README.md
==================
37f4c24f1;Stas Bekman;2020-12-07 12:18:05 -0800;> 30 files leads to hanging on --More--
cancel debug printing for now. As it can be seen lead to a failing test here:
https://app.circleci.com/pipelines/github/huggingface/transformers/16894/workflows/cc86f7a9-4020-45af-8ab3-c22f79b427cf/jobs/131924
==

.circleci/config.yml
==================
7f9ccffc5;Sylvain Gugger;2020-12-07 14:26:36 -0500;Use word_ids to get labels in run_ner (#8962)
* Use word_ids to get labels in run_ner

* Add sanity check
==

examples/token-classification/run_ner.py
==================
de6befd41;Clement;2020-12-07 11:15:29 -0500;Remove sourcerer (#8965)

==

README.md
==================
483e13273;sandip;2020-12-07 21:28:37 +0530;Add TFGPT2ForSequenceClassification based on DialogRPT (#8714)
* Add TFGPT2ForSequenceClassification based on DialogRPT

* Add TFGPT2ForSequenceClassification based on DialogRPT

* TFGPT2ForSequenceClassification based on DialogRPT-refactored code, implemented review comments and added input processing

* Add TFGPT2ForSequenceClassification based on DialogRPT

* TFGPT2ForSequenceClassification based on DialogRPT-refactored code, implemented review comments and added input processing

* code refactor for latest other TF PR

* code refactor

* code refactor

* Update modeling_tf_gpt2.py
==

docs/source/model_doc/gpt2.rst
src/transformers/__init__.py
src/transformers/modeling_tf_outputs.py
src/transformers/models/auto/modeling_tf_auto.py
src/transformers/models/gpt2/__init__.py
src/transformers/models/gpt2/modeling_tf_gpt2.py
src/transformers/utils/dummy_tf_objects.py
tests/test_modeling_tf_gpt2.py
==================
28c77ddf3;Sylvain Gugger;2020-12-07 09:50:32 -0500;Fix QA pipeline on Windows (#8947)

==

src/transformers/pipelines.py
==================
72d6c9c68;Philip Tamimi-Sarnikowski;2020-12-06 17:16:32 +0100;Add model card (#8948)
* add model card

* lowercase identifier

Co-authored-by: Julien Chaumond <chaumond@gmail.com>
==

model_cards/sarnikowski/electra-small-discriminator-da-256-cased/README.md
==================
ef93a2542;Machel Reid;2020-12-05 23:57:37 +0900;Fix typo for `modeling_bert` import resulting in ImportError (#8931)
Self-explanatory ;) - Hope it helps!
==

src/transformers/models/roberta/convert_roberta_original_pytorch_checkpoint_to_pytorch.py
==================
8dfc8c722;Ethan Perez;2020-12-05 07:52:16 -0700;Don't pass in token_type_ids to BART for GLUE (#8929)
Without this fix, training a `BARTForSequenceClassification` model with `run_pl_glue.py` gives `TypeError: forward() got an unexpected keyword argument 'token_type_ids'`, because BART does not have token_type_ids. I've solved this issue in the same way as it's solved for the "distilbert" model, and I can train BART models on SNLI without errors now.
==

examples/text-classification/run_pl_glue.py
==================
df311a5cc;Stas Bekman;2020-12-04 15:43:35 -0800;[seq2seq] document the caveat of leaky native amp (#8930)
* document the caveat of leaky native amp

* Update examples/seq2seq/README.md

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

examples/seq2seq/README.md
==================
73c51f7fc;Stas Bekman;2020-12-04 10:13:42 -0800;[ci] skip doc jobs - circleCI is not reliable - disable skip for now (#8926)
* disable skipping, but leave logging for the future
==

.circleci/config.yml
==================
71688a888;Lysandre Debut;2020-12-04 12:28:47 -0500;Fix TF T5 only encoder model with booleans (#8925)

==

src/transformers/models/t5/modeling_tf_t5.py
==================
dcd3046f9;Julien Plu;2020-12-04 15:08:29 +0100;Better booleans handling in the TF models (#8777)
* Apply on BERT and ALBERT

* Update TF Bart

* Add input processing to TF BART

* Add input processing for TF CTRL

* Add input processing to TF Distilbert

* Add input processing to TF DPR

* Add input processing to TF Electra

* Add deprecated arguments

* Add input processing to TF XLM

* Add input processing to TF Funnel

* Add input processing to TF GPT2

* Add input processing to TF Longformer

* Add input processing to TF Lxmert

* Apply style

* Add input processing to TF Mobilebert

* Add input processing to TF GPT

* Add input processing to TF Roberta

* Add input processing to TF T5

* Add input processing to TF TransfoXL

* Apply style

* Rebase on master

* Bug fix

* Retry to bugfix

* Retry bug fix

* Fix wrong model name

* Try another fix

* Fix BART

* Fix input precessing

* Apply style

* Put the deprecated warnings in the input processing function

* Remove the unused imports

* Raise an error when len(kwargs)>0

* test ModelOutput instead of TFBaseModelOutput

* Bug fix

* Address Patrick's comments

* Address Patrick's comments

* Address Sylvain's comments

* Add boolean processing for the inputs

* Apply style

* Missing optional

* Fix missing some input proc

* Update the template

* Fix missing inputs

* Missing input

* Fix args parameter

* Trigger CI

* Trigger CI

* Trigger CI

* Address Patrick's and Sylvain's comments

* Replace warn by warning

* Trigger CI

* Fix XLNET

* Fix detection
==

src/transformers/modeling_tf_utils.py
src/transformers/models/albert/modeling_tf_albert.py
src/transformers/models/bart/modeling_tf_bart.py
src/transformers/models/bert/modeling_tf_bert.py
src/transformers/models/ctrl/modeling_tf_ctrl.py
src/transformers/models/distilbert/modeling_tf_distilbert.py
src/transformers/models/dpr/modeling_tf_dpr.py
src/transformers/models/electra/modeling_tf_electra.py
src/transformers/models/flaubert/modeling_tf_flaubert.py
src/transformers/models/funnel/modeling_tf_funnel.py
src/transformers/models/gpt2/modeling_tf_gpt2.py
src/transformers/models/longformer/modeling_tf_longformer.py
src/transformers/models/lxmert/modeling_tf_lxmert.py
src/transformers/models/mobilebert/modeling_tf_mobilebert.py
src/transformers/models/openai/modeling_tf_openai.py
src/transformers/models/roberta/modeling_tf_roberta.py
src/transformers/models/t5/modeling_tf_t5.py
src/transformers/models/transfo_xl/modeling_tf_transfo_xl.py
src/transformers/models/xlm/modeling_tf_xlm.py
src/transformers/models/xlnet/modeling_tf_xlnet.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_tf_{{cookiecutter.lowercase_modelname}}.py
==================
4c3d98ddd;Stas Bekman;2020-12-03 16:05:55 -0800;[s2s finetune_trainer] add instructions for distributed training (#8884)

==

examples/seq2seq/README.md
==================
aa60b230e;Lysandre Debut;2020-12-03 17:15:47 -0500;Patch model parallel test (#8920)
* Patch model parallel test

* Remove line

* Remove `ci_*` from scheduled branches
==

.github/workflows/self-push.yml
.github/workflows/self-scheduled.yml
tests/test_modeling_common.py
==================
0c5615af6;Lysandre Debut;2020-12-03 14:28:49 -0500;Put Transformers on Conda (#8918)
* conda

* Guide

* correct tag

* Update README.md

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update docs/source/installation.md

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Sylvain's comments

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

.github/conda/build.sh
.github/conda/meta.yaml
.github/workflows/release-conda.yml
README.md
docs/source/installation.md
==================
9ad619431;Julien Chaumond;2020-12-03 16:56:55 +0100;Tweak wording + Add badge w/ number of models on the hub (#8914)
* Add badge w/ number of models on the hub

* try to apease @sgugger üòá

* not sure what this `c` was about [ci skip]

* Fix script and move stuff around

* Fix doc styling error

Co-authored-by: Sylvain Gugger <sylvain.gugger@gmail.com>
==

README.md
docs/source/index.rst
utils/check_copies.py
utils/style_doc.py
==================
6ed7e32f7;Sylvain Gugger;2020-12-03 10:50:13 -0500;Fix move when the two cache folders exist (#8917)

==

src/transformers/file_utils.py
==================
8453201cf;Sylvain Gugger;2020-12-03 10:45:07 -0500;Avoid erasing the attention mask when double padding (#8915)

==

src/transformers/tokenization_utils_base.py
==================
0deece9c5;Skye Wanderman-Milne;2020-12-03 07:33:12 -0800;Don't warn that models aren't available if Flax is available. (#8841)

==

src/transformers/__init__.py
==================
2b7fc9a0f;Julien Chaumond;2020-12-03 15:05:01 +0100;[model_cards] lm-head was deprecated
(and wasn't needed here anyways as it was added automatically)

==

model_cards/DJSammy/bert-base-danish-uncased_BotXO,ai/README.md
model_cards/codegram/calbert-base-uncased/README.md
model_cards/codegram/calbert-tiny-uncased/README.md
model_cards/jannesg/takalane_afr_roberta/README.md
model_cards/jannesg/takalane_nbl_roberta/README.md
model_cards/jannesg/takalane_nso_roberta/README.md
model_cards/jannesg/takalane_sot_roberta/README.md
model_cards/jannesg/takalane_ssw_roberta/README.md
model_cards/jannesg/takalane_tsn_roberta/README.md
model_cards/jannesg/takalane_tso_roberta/README.md
model_cards/jannesg/takalane_ven_roberta/README.md
model_cards/jannesg/takalane_xho_roberta/README.md
model_cards/jannesg/takalane_zul_roberta/README.md
model_cards/kuisailab/albert-base-arabic/README.md
model_cards/kuisailab/albert-large-arabic/README.md
model_cards/kuisailab/albert-xlarge-arabic/README.md
==================
443f67e88;Patrick von Platen;2020-12-02 19:19:50 +0100;[PyTorch] Refactor Resize Token Embeddings (#8880)
* fix resize tokens

* correct mobile_bert

* move embedding fix into modeling_utils.py

* refactor

* fix lm head resize

* refactor

* break lines to make sylvain happy

* add news tests

* fix typo

* improve test

* skip bart-like for now

* check if base_model = get(...) is necessary

* clean files

* improve test

* fix tests

* revert style templates

* Update templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_{{cookiecutter.lowercase_modelname}}.py
==

src/transformers/modeling_utils.py
src/transformers/models/albert/modeling_albert.py
src/transformers/models/bert/modeling_bert.py
src/transformers/models/bert_generation/modeling_bert_generation.py
src/transformers/models/ctrl/modeling_ctrl.py
src/transformers/models/distilbert/modeling_distilbert.py
src/transformers/models/electra/modeling_electra.py
src/transformers/models/encoder_decoder/modeling_encoder_decoder.py
src/transformers/models/funnel/modeling_funnel.py
src/transformers/models/gpt2/modeling_gpt2.py
src/transformers/models/layoutlm/modeling_layoutlm.py
src/transformers/models/longformer/modeling_longformer.py
src/transformers/models/mobilebert/modeling_mobilebert.py
src/transformers/models/openai/modeling_openai.py
src/transformers/models/prophetnet/modeling_prophetnet.py
src/transformers/models/rag/modeling_rag.py
src/transformers/models/reformer/modeling_reformer.py
src/transformers/models/roberta/modeling_roberta.py
src/transformers/models/squeezebert/modeling_squeezebert.py
src/transformers/models/t5/modeling_t5.py
src/transformers/models/xlm/modeling_xlm.py
src/transformers/models/xlnet/modeling_xlnet.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_{{cookiecutter.lowercase_modelname}}.py
tests/test_modeling_bart.py
tests/test_modeling_blenderbot.py
tests/test_modeling_common.py
tests/test_modeling_fsmt.py
tests/test_modeling_reformer.py
tests/test_modeling_t5.py
tests/test_modeling_transfo_xl.py
==================
e52f9c0ad;Devangi Purkayastha;2020-12-02 22:58:44 +0530;Update README.md (#8906)

==

README.md
==================
801b2cb36;ryota-mo;2020-12-03 02:08:31 +0900;Fix typo in docstring (#8905)

==

src/transformers/models/bert_japanese/tokenization_bert_japanese.py
==================
7e1cb00c3;Stas Bekman;2020-12-02 09:07:42 -0800;[trainer] improve code readability (#8903)
* [trainer] improve code

This PR:
- removes redundant code 
```
self.model = model if model is not None else None
```
and
```
self.model = model
```
are the same.

* separate attribute assignment from code logic - which simplifies things further.

* whitespace
==

src/transformers/trainer.py
==================
a8c3f9aa7;Nicolas Patry;2020-12-02 16:18:28 +0100;Warning about too long input for fast tokenizers too (#8799)
* Warning about too long input for fast tokenizers too

If truncation is not set in tokenizers, but the tokenization is too long
for the model (`model_max_length`), we used to trigger a warning that

The input would probably fail (which it most likely will).

This PR re-enables the warning for fast tokenizers too and uses common
code for the trigger to make sure it's consistent across.

* Checking for pair of inputs too.

* Making the function private and adding it's doc.

* Remove formatting ?? in odd place.

* Missed uppercase.
==

src/transformers/tokenization_utils_base.py
src/transformers/tokenization_utils_fast.py
tests/test_tokenization_common.py
==================
f6b44e619;sandip;2020-12-02 20:38:32 +0530;Transfoxl seq classification (#8868)
* Transfoxl sequence classification

* Transfoxl sequence classification
==

docs/source/model_doc/transformerxl.rst
src/transformers/__init__.py
src/transformers/models/auto/modeling_auto.py
src/transformers/models/transfo_xl/__init__.py
src/transformers/models/transfo_xl/modeling_transfo_xl.py
src/transformers/utils/dummy_pt_objects.py
tests/test_modeling_transfo_xl.py
==================
24f0c2fe3;Stas Bekman;2020-12-02 07:06:45 -0800;[ci] skip doc jobs take #3 (#8885)
* check that we get any match first

* docs only

* 2 docs only

* add code

* restore
==

.circleci/config.yml
==================
693ac3594;Stas Bekman;2020-12-01 12:03:29 -0800;disable job skip - need more work
reference: https://github.com/huggingface/transformers/pull/8853#issuecomment-736779863
==

.circleci/config.yml
==================
379005c9d;Stas Bekman;2020-12-01 11:40:36 -0800;start using training_args.parallel_mode (#8882)

==

examples/seq2seq/finetune_trainer.py
examples/seq2seq/seq2seq_trainer.py
==================
b08843cf4;Sylvain Gugger;2020-12-01 13:46:09 -0500;Add a `parallel_mode` property to TrainingArguments (#8877)
* Add a `distributed_env` property to TrainingArguments

* Change name

* Address comment
==

src/transformers/training_args.py
==================
7c10dd22a;Sylvain Gugger;2020-12-01 13:45:21 -0500;Better support for resuming training (#8878)

==

src/transformers/trainer.py
src/transformers/training_args.py
tests/test_trainer.py
==================
21db560df;Stas Bekman;2020-12-01 10:15:25 -0800;[CI] skip docs-only jobs take #2 (#8853)
* restore skip

* Revert "Remove deprecated `evalutate_during_training` (#8852)"

This reverts commit 553029909620455e040a49032a9c45f6a5f0cd52.

* check that pipeline.git.base_revision is defined before proceeding

* Revert "Revert "Remove deprecated `evalutate_during_training` (#8852)""

This reverts commit dfec84db3fdce1079f01f1bc8dfaf21db2ccaba1.

* check that pipeline.git.base_revision is defined before proceeding

* doc only

* doc + code

* restore

* restore

* typo
==

.circleci/config.yml
==================
a947386ce;Lysandre Debut;2020-12-01 13:13:11 -0500;Better warning when loading a tokenizer with AutoTokenizer w/o SnetencePiece (#8881)

==

src/transformers/models/auto/tokenization_auto.py
==================
9c18f1568;Adam Pocock;2020-12-01 13:01:52 -0500;Prevent BatchEncoding from blindly passing casts down to the tensors it contains. Fixes #6582. (#8860)
Update src/transformers/tokenization_utils_base.py with review fix

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
==

src/transformers/tokenization_utils_base.py
==================
c0df963ee;Sylvain Gugger;2020-12-01 11:45:57 -0500;Make the big table creation/check platform independent (#8856)

==

src/transformers/__init__.py
src/transformers/models/auto/tokenization_auto.py
src/transformers/models/mt5/__init__.py
src/transformers/utils/dummy_sentencepiece_objects.py
src/transformers/utils/dummy_tokenizers_objects.py
utils/check_copies.py
==================
d366228df;Ratthachat (Jung);2020-12-01 22:16:48 +0700;2 typos in modeling_rag.py (#8676)
* 2 typos - from_question_encoder_generator_configs

fix 2 typos
from_encoder_generator_configs --> from_question_encoder_generator_configs

* apply make style
==

src/transformers/models/rag/modeling_rag.py
==================
814b9550d;Rodolfo Quispe;2020-12-01 01:44:37 -0800;Fix doc for language code (#8848)

==

src/transformers/models/mbart/tokenization_mbart.py
src/transformers/models/mbart/tokenization_mbart_fast.py
==================
4a9e502a3;elk-cloner;2020-12-01 12:19:27 +0330;Ctrl for sequence classification (#8812)
* add CTRLForSequenceClassification

* pass local test

* merge with master

* fix modeling test for sequence classification

* fix deco

* fix assert
==

docs/source/model_doc/ctrl.rst
src/transformers/__init__.py
src/transformers/models/auto/modeling_auto.py
src/transformers/models/ctrl/__init__.py
src/transformers/models/ctrl/modeling_ctrl.py
src/transformers/utils/dummy_pt_objects.py
tests/test_modeling_ctrl.py
==================
7f34d7578;Stas Bekman;2020-11-30 12:55:56 -0800;[s2s trainer] fix DP mode (#8823)
* fix DP case on multi-gpu

* make executable

* test all 3 modes

* use the correct check for distributed

* dp doesn't need a special case

* restore original name

* cleanup
==

examples/seq2seq/finetune_trainer.py
examples/seq2seq/seq2seq_trainer.py
examples/seq2seq/test_finetune_trainer.py
==================
d8fc26e91;Nicolas Patry;2020-11-30 20:05:08 +0100;NerPipeline (TokenClassification) now outputs offsets of words (#8781)
* NerPipeline (TokenClassification) now outputs offsets of words

- It happens that the offsets are missing, it forces the user to pattern
match the "word" from his input, which is not always feasible.
For instance if a sentence contains the same word twice, then there
is no way to know which is which.
- This PR proposes to fix that by outputting 2 new keys for this
pipelines outputs, "start" and "end", which correspond to the string
offsets of the word. That means that we should always have the
invariant:

```python
input[entity["start"]: entity["end"]] == entity["entity_group"]
                                    # or entity["entity"] if not grouped
```

* Fixing doc style
==

src/transformers/pipelines.py
tests/test_pipelines_ner.py
==================
5fd3d81ec;LysandreJik;2020-11-30 13:54:52 -0500;fix pypi complaint on version naming

==

setup.py
src/transformers/__init__.py
==================
51b071313;Funtowicz Morgan;2020-11-30 19:43:17 +0100;Attempt to fix Flax CI error(s) (#8829)
* Slightly increase tolerance between pytorch and flax output

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* test_multiple_sentences doesn't require torch

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* Simplify parameterization on "jit" to use boolean rather than str

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* Use `require_torch` on `test_multiple_sentences` because we pull the weight from the hub.

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* Rename "jit" parameter to "use_jit" for (hopefully) making it self-documenting.

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* Remove pytest.mark.parametrize which seems to fail in some circumstances

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* Fix unused imports.

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* Fix style.

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* Give default parameters values for traced model.

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* Review comment: Change sentences to sequences

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

tests/test_modeling_flax_bert.py
tests/test_modeling_flax_roberta.py
==================
9995a341c;LysandreJik;2020-11-30 12:01:45 -0500;Update docs

==

.circleci/deploy.sh
docs/source/_static/js/custom.js
==================
22b0ff757;LysandreJik;2020-11-30 12:07:43 -0500;Release: v4.0.0

==

docs/source/conf.py
setup.py
src/transformers/__init__.py
src/transformers/models/longformer/modeling_longformer.py
==================
553029909;Sylvain Gugger;2020-11-30 11:12:15 -0500;Remove deprecated `evalutate_during_training` (#8852)
* Remove deprecated `evalutate_during_training`

* Update src/transformers/training_args_tf.py

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
==

examples/seq2seq/builtin_trainer/finetune.sh
examples/seq2seq/builtin_trainer/finetune_tpu.sh
examples/seq2seq/builtin_trainer/train_distil_marian_enro.sh
examples/seq2seq/builtin_trainer/train_distil_marian_enro_tpu.sh
examples/seq2seq/builtin_trainer/train_distilbart_cnn.sh
examples/seq2seq/builtin_trainer/train_mbart_cc25_enro.sh
src/transformers/integrations.py
src/transformers/trainer_tf.py
src/transformers/training_args_tf.py
==================
773849415;Shai Erera;2020-11-30 18:11:10 +0200;Use model.from_pretrained for DataParallel also (#8795)
* Use model.from_pretrained for DataParallel also

When training on multiple GPUs, the code wraps a model with torch.nn.DataParallel. However if the model has custom from_pretrained logic, it does not get applied during load_best_model_at_end.

This commit uses the underlying model during load_best_model_at_end, and re-wraps the loaded model with DataParallel.

If you choose to reject this change, then could you please move the this logic to a function, e.g. def load_best_model_checkpoint(best_model_checkpoint) or something, so that it can be overridden?

* Fix silly bug

* Address review comments

Thanks for the feedback. I made the change that you proposed, but I also think we should update L811 to check if `self.mode` is an instance of `PreTrained`, otherwise we would still not get into that `if` section, right?
==

src/transformers/trainer.py
==================
4062c75e4;Sylvain Gugger;2020-11-30 10:51:35 -0500;Merge remote-tracking branch 'origin/master'

==
==================
08e707633;Sylvain Gugger;2020-11-30 10:51:25 -0500;Comment the skip job on doc line

==

.circleci/config.yml
==================
75f8100fc;Sylvain Gugger;2020-11-30 10:29:23 -0500;Add a direct link to the big table (#8850)

==

README.md
docs/source/index.rst
utils/check_copies.py
==================
cc983cd9c;Fraser Greenlee;2020-11-30 14:33:30 +0000;Correct docstring. (#8845)
Related issue: https://github.com/huggingface/transformers/issues/8837
==

src/transformers/tokenization_utils_base.py
==================
19fa01ce2;Stefan Schweter;2020-11-30 15:21:56 +0100;token-classification: use is_world_process_zero instead of deprecated is_world_master() (#8828)

==

examples/token-classification/run_ner.py
examples/token-classification/run_ner_old.py
==================
40ecaf0c2;Ahmed Elnaggar;2020-11-30 08:34:40 +0100;Add T5 Encoder for Feature Extraction (#8717)
* Add T5 Encoder class for feature extraction

* fix T5 encoder add_start_docstrings indent

* update init with T5 encoder

* update init with TFT5ModelEncoder

* remove TFT5ModelEncoder

* change T5ModelEncoder order in init

* add T5ModelEncoder to transformers init

* clean T5ModelEncoder

* update init with TFT5ModelEncoder

* add TFModelEncoder for Tensorflow

* update init with TFT5ModelEncoder

* Update src/transformers/models/t5/modeling_t5.py

change output from Seq2SeqModelOutput to BaseModelOutput

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* remove encoder_outputs

1. remove encoder_outputs from the function call.
2. remove the encoder_outputs If statement.
3. remove isinstance from return_dict.

* Authorize missing decoder keys

* remove unnecessary input parameters

remove pask_key_values and use_cache

* remove use_cache

remove use_cache from the forward method

* add doctoring for T5 encoder

add doctoring for T5 encoder with T5_ENCODER_INPUTS_DOCSTRING

* change return_dict to dot access

* add T5_ENCODER_INPUTS_DOCSTRING for TF T5

* change TFT5Encoder output type to BaseModelOutput

* remove unnecessary parameters for TFT5Encoder

* remove unnecessary if statement

* add import BaseModelOutput

* fix BaseModelOutput typo to TFBaseModelOutput

* update T5 doc with T5ModelEncoder

* add T5ModelEncoder to tests

* finish pytorch

* finish docs and mt5

* add mtf to init

* fix init

* remove n_positions

* finish PR

* Update src/transformers/models/mt5/modeling_mt5.py

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* Update src/transformers/models/t5/modeling_t5.py

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* Update src/transformers/models/t5/modeling_tf_t5.py

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* Update src/transformers/models/mt5/modeling_tf_mt5.py

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* make style

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
==

docs/source/model_doc/mt5.rst
docs/source/model_doc/t5.rst
src/transformers/__init__.py
src/transformers/models/mt5/__init__.py
src/transformers/models/mt5/modeling_mt5.py
src/transformers/models/mt5/modeling_tf_mt5.py
src/transformers/models/t5/__init__.py
src/transformers/models/t5/modeling_t5.py
src/transformers/models/t5/modeling_tf_t5.py
src/transformers/utils/dummy_pt_objects.py
src/transformers/utils/dummy_tf_objects.py
tests/test_modeling_rag.py
tests/test_modeling_t5.py
tests/test_modeling_tf_t5.py
utils/check_repo.py
==================
610cb106a;Lysandre Debut;2020-11-29 20:13:07 -0500;Migration guide from v3.x to v4.x (#8763)
* Migration guide from v3.x to v4.x

* Better wording

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Sylvain's comments

* Better wording.

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

docs/source/migration.md
==================
c239dcda8;Stas Bekman;2020-11-29 08:31:30 -0800;[CI] implement job skipping for doc-only PRs (#8826)
* implement job skipping for doc-only PRs

* silent grep is crucial

* wip

* wip

* wip

* wip

* wip

* wip

* wip

* wip

* let's add doc

* let's add code

* revert test commits

* restore

* Better name

* Better name

* Better name

* some more testing

* some more testing

* some more testing

* finish testing
==

.circleci/config.yml
==================
3a08cc1ce;Guy Rosin;2020-11-29 18:27:00 +0200;Minor docs typo fixes (#8797)
* Fix minor typos

* Additional typos

* Style fix

Co-authored-by: guyrosin <guyrosin@assist-561.cs.technion.ac.il>
==

CONTRIBUTING.md
docs/source/preprocessing.rst
docs/source/quicktour.rst
docs/source/serialization.rst
src/transformers/data/data_collator.py
==================
5ced23dc8;Patrick von Platen;2020-11-29 16:57:43 +0100;[Pegasus] Refactor Tokenizer (#8731)
* refactor

* further refactor

* fix the rest tomorrow

* save intermediate

* finish slow tokenizer

* make more tests pass

* finish refactor

* fix comment

* clean further

* fix name

* fix naming

* Update src/transformers/models/reformer/tokenization_reformer.py

* Apply suggestions from code review

* Apply suggestions from code review

* refactor

* fix init tokenizers

* refactor

* improve convert

* refactor

* correct convert slow tokenizer

* final fix for Pegasus Tok

* remove ipdb

* improve links
==

src/transformers/convert_slow_tokenizer.py
src/transformers/models/albert/tokenization_albert_fast.py
src/transformers/models/camembert/tokenization_camembert_fast.py
src/transformers/models/mbart/tokenization_mbart_fast.py
src/transformers/models/pegasus/tokenization_pegasus.py
src/transformers/models/pegasus/tokenization_pegasus_fast.py
src/transformers/models/reformer/tokenization_reformer_fast.py
src/transformers/models/t5/tokenization_t5_fast.py
src/transformers/models/xlm_roberta/tokenization_xlm_roberta_fast.py
src/transformers/models/xlnet/tokenization_xlnet_fast.py
tests/test_tokenization_pegasus.py
==================
36b60ce9e;Patrick von Platen;2020-11-28 19:50:49 +0100;fix mt5 config (#8832)

==

src/transformers/models/mt5/configuration_mt5.py
==================
18c32eeb2;Lysandre Debut;2020-11-27 16:41:29 -0500;Model parallel tests should return, not pass in non model parallel settings. (#8825)

==

tests/test_modeling_common.py
==================
edbff1fd0;LysandreJik;2020-11-27 16:15:00 -0500;Temporarily deactivate model generation

==

.github/workflows/self-push.yml
==================
00ea45659;Stas Bekman;2020-11-27 13:04:54 -0800;suggest a numerical limit of 50MB for determining @slow (#8824)

==

docs/source/testing.rst
==================
0a921b645;Max Del;2020-11-27 19:35:34 +0200;BART & FSMT: fix decoder not returning hidden states from the last layer (#8597)
* Fix decoder not returning hidden states from the last layer

* Resolve conflict

* Change the way to gather hidden states

* Add decoder hidden states test

* Make pytest and black happy

* Remove redundant line

* remove new line

Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>
==

src/transformers/models/bart/modeling_bart.py
src/transformers/models/fsmt/modeling_fsmt.py
tests/test_modeling_common.py
==================
81fe0bf08;Moussa Kamal Eddine;2020-11-27 18:31:42 +0100;Add barthez model (#8393)
* Add init barthez

* Add barthez model, tokenizer and docs

BARThez is a pre-trained french seq2seq model that uses BART objective.

* Apply suggestions from code review docs typos

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Add license

* Change URLs scheme

* Remove barthez model keep tokenizer

* Fix style

* Fix quality

* Update tokenizer

* Add fast tokenizer

* Add fast tokenizer test

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

README.md
docs/source/index.rst
docs/source/model_doc/barthez.rst
docs/source/pretrained_models.rst
src/transformers/__init__.py
src/transformers/convert_slow_tokenizer.py
src/transformers/models/auto/tokenization_auto.py
src/transformers/models/barthez/__init__.py
src/transformers/models/barthez/tokenization_barthez.py
src/transformers/models/barthez/tokenization_barthez_fast.py
src/transformers/utils/dummy_sentencepiece_objects.py
src/transformers/utils/dummy_tokenizers_objects.py
tests/test_tokenization_barthez.py
==================
b0f2dbc59;Julien Plu;2020-11-27 18:25:20 +0100;Fix setup.py (#8798)
enforce unix newline encoding regardless of OS creating the file
==

setup.py
==================
03bddc375;Manuel Romero;2020-11-27 18:19:15 +0100;Create README.md (#8729)
* Create README.md

* Fix model path
==

model_cards/mrm8488/mT5-small-finetuned-tydiqa-for-xqa/README.md
==================
f9a2a9e32;Giovanni Compagnoni;2020-11-27 16:52:58 +0100;Extend typing to path-like objects in `PretrainedConfig` and `PreTrainedModel` (#8770)
* update configuration_utils.py typing to allow pathlike objects when sensible

* update modeling_utils.py typing to allow pathlike objects when sensible

* black

* update tokenization_utils_base.py typing to allow pathlike objects when sensible

* update tokenization_utils_fast.py typing to allow pathlike objects when sensible

* update configuration_auto.py typing to allow pathlike objects when sensible

* update configuration_auto.py docstring to allow pathlike objects when sensible

* update tokenization_auto.py docstring to allow pathlike objects when sensible

* black
==

src/transformers/configuration_utils.py
src/transformers/modeling_utils.py
src/transformers/models/auto/configuration_auto.py
src/transformers/models/auto/modeling_auto.py
src/transformers/models/auto/tokenization_auto.py
src/transformers/tokenization_utils_base.py
src/transformers/tokenization_utils_fast.py
==================
a7d46a060;Patrick von Platen;2020-11-27 16:26:45 +0100;Fix dpr<>bart config for RAG (#8808)
* correct dpr test and bert pos fault

* fix dpr bert config problem

* fix layoutlm

* add config to dpr as well
==

src/transformers/models/albert/modeling_albert.py
src/transformers/models/bert/modeling_bert.py
src/transformers/models/dpr/configuration_dpr.py
src/transformers/models/electra/modeling_electra.py
src/transformers/models/layoutlm/modeling_layoutlm.py
src/transformers/models/roberta/modeling_roberta.py
tests/test_modeling_dpr.py
==================
a2cf37595;Patrick von Platen;2020-11-27 14:40:42 +0100;[Flax test] Add require pytorch to flix flax test (#8816)
* try flax fix

* same for roberta
==

tests/test_modeling_flax_bert.py
tests/test_modeling_flax_roberta.py
==================
e3ef62bce;mdermentzi;2020-11-27 14:34:57 +0100;Update README.md (#8815)
The tokenizer called at the input_ids of example 2 is currently encoding text_1. I think this should be changed to text_2.
==

model_cards/nlpaueb/bert-base-greek-uncased-v1/README.md
==================
f8eda599b;Kristian Holsheimer;2020-11-27 23:21:19 +1100;[FlaxBert] Fix non-broadcastable attention mask for batched forward-passes (#8791)
* [FlaxBert] Fix non-broadcastable attention mask for batched forward-passes

* [FlaxRoberta] Fix non-broadcastable attention mask

* Use jax.numpy instead of ordinary numpy (otherwise not jit-able)

* Partially revert "Use jax.numpy ..."

* Add tests for batched forward passes

* Avoid unnecessary OOMs due to preallocation of GPU memory by XLA

* Auto-fix style

* Re-enable GPU memory preallocation but with mem fraction < 1/paralleism
==

src/transformers/models/bert/modeling_flax_bert.py
src/transformers/models/roberta/modeling_flax_roberta.py
tests/test_modeling_flax_bert.py
tests/test_modeling_flax_roberta.py
==================
cb7602b38;Stas Bekman;2020-11-26 14:47:36 -0800;typo (#8810)

==

.github/PULL_REQUEST_TEMPLATE.md
==================
ddf3c6465;Stas Bekman;2020-11-26 14:06:27 -0800;potpurri of small fixes (#8807)

==

examples/seq2seq/README.md
examples/seq2seq/builtin_trainer/train_mbart_cc25_enro.sh
==================
52708d263;chutaklee;2020-11-27 05:23:36 +0800;Fix PPLM (#8779)
* Fix pplm

* fix style

* make style

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
==

examples/text-generation/pplm/run_pplm.py
examples/text-generation/pplm/run_pplm_discrim_train.py
==================
8f07f5c44;Patrick von Platen;2020-11-26 20:12:01 +0100;Revert "finetune.py: specifying generation min_length (#8478)" (#8805)
This reverts commit 5aa361f3e56de0f65720f291bb3975bfc98f2837.
==

examples/seq2seq/finetune.py
==================
66e9608ba;Manuel Romero;2020-11-26 18:43:43 +0100;Create README.md (#8760)

==

model_cards/mrm8488/bert-mini2bert-mini-finetuned-cnn_daily_mail-summarization/README.md
==================
5aa361f3e;Daniel Khashabi;2020-11-25 23:03:02 -0800;finetune.py: specifying generation min_length (#8478)

==

examples/seq2seq/finetune.py
==================
30e7f7e5d;joangines;2020-11-26 07:38:21 +0900;Create README.md (#8752)

==

model_cards/Cinnamon/electra-small-japanese-discriminator/README.md
==================
2a6fbe6a4;Patrick von Platen;2020-11-25 22:54:59 +0100;[XLNet] Fix mems behavior (#8567)
* fix mems in xlnet

* fix use_mems

* fix use_mem_len

* fix use mems

* clean docs

* fix tf typo

* make xlnet tf for generation work

* fix tf test

* refactor use cache

* add use cache for missing models

* correct use_cache in generate

* correct use cache in tf generate

* fix tf

* correct getattr typo

* make sylvain happy

* change in docs as well

* do not apply to cookie cutter statements

* fix tf test

* make pytorch model fully backward compatible
==

docs/source/installation.md
docs/source/model_doc/bertgeneration.rst
docs/source/model_doc/deberta.rst
docs/source/model_doc/distilbert.rst
docs/source/model_doc/electra.rst
docs/source/model_doc/flaubert.rst
docs/source/model_doc/gpt.rst
docs/source/model_doc/layoutlm.rst
docs/source/model_doc/lxmert.rst
docs/source/model_doc/mbart.rst
docs/source/model_doc/prophetnet.rst
docs/source/model_doc/t5.rst
docs/source/model_doc/xlmprophetnet.rst
docs/source/model_summary.rst
docs/source/task_summary.rst
src/transformers/configuration_utils.py
src/transformers/data/datasets/language_modeling.py
src/transformers/generation_tf_utils.py
src/transformers/generation_utils.py
src/transformers/models/albert/modeling_albert.py
src/transformers/models/albert/modeling_tf_albert.py
src/transformers/models/bart/configuration_bart.py
src/transformers/models/bert/modeling_bert.py
src/transformers/models/bert/modeling_tf_bert.py
src/transformers/models/bertweet/tokenization_bertweet.py
src/transformers/models/ctrl/configuration_ctrl.py
src/transformers/models/deberta/modeling_deberta.py
src/transformers/models/electra/modeling_electra.py
src/transformers/models/electra/modeling_tf_electra.py
src/transformers/models/fsmt/configuration_fsmt.py
src/transformers/models/funnel/modeling_tf_funnel.py
src/transformers/models/gpt2/configuration_gpt2.py
src/transformers/models/lxmert/modeling_lxmert.py
src/transformers/models/lxmert/modeling_tf_lxmert.py
src/transformers/models/mobilebert/modeling_mobilebert.py
src/transformers/models/mobilebert/modeling_tf_mobilebert.py
src/transformers/models/openai/configuration_openai.py
src/transformers/models/prophetnet/configuration_prophetnet.py
src/transformers/models/rag/configuration_rag.py
src/transformers/models/reformer/configuration_reformer.py
src/transformers/models/t5/configuration_t5.py
src/transformers/models/t5/modeling_tf_t5.py
src/transformers/models/xlnet/configuration_xlnet.py
src/transformers/models/xlnet/modeling_tf_xlnet.py
src/transformers/models/xlnet/modeling_xlnet.py
tests/test_modeling_tf_xlnet.py
tests/test_modeling_xlnet.py
==================
369f1d77b;Joe Davison;2020-11-25 16:06:04 -0500;Return correct Bart hidden state tensors (#8747)
* bart output hidden states upstream

* same w/ decoder

* add tests

* fix prophetnet

* fix gpt2 and ctrl

* fix fstm and skip test for reformer and longformer

* fix all models

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
==

src/transformers/models/bart/modeling_bart.py
src/transformers/models/ctrl/modeling_ctrl.py
src/transformers/models/fsmt/modeling_fsmt.py
src/transformers/models/gpt2/modeling_gpt2.py
src/transformers/models/openai/modeling_openai.py
src/transformers/models/prophetnet/modeling_prophetnet.py
src/transformers/models/squeezebert/modeling_squeezebert.py
tests/test_modeling_common.py
tests/test_modeling_longformer.py
tests/test_modeling_lxmert.py
tests/test_modeling_prophetnet.py
tests/test_modeling_reformer.py
tests/test_modeling_transfo_xl.py
tests/test_modeling_xlnet.py
==================
138f45c18;Lysandre Debut;2020-11-25 14:02:15 -0500;Fix QA argument handler (#8765)
* Fix QA argument handler

* Attempt to get a better fix for QA (#8768)

Co-authored-by: Nicolas Patry <patry.nicolas@protonmail.com>
==

src/transformers/pipelines.py
tests/test_pipelines_question_answering.py
==================
4821ea5ae;Sylvain Gugger;2020-11-25 12:02:15 -0500;Big model table (#8774)
* First draft

* Styling

* With all changes staged

* Update docs/source/index.rst

Co-authored-by: Julien Chaumond <chaumond@gmail.com>

* Styling

Co-authored-by: Julien Chaumond <chaumond@gmail.com>
==

docs/source/_static/css/huggingface.css
docs/source/index.rst
src/transformers/__init__.py
src/transformers/models/auto/__init__.py
src/transformers/models/auto/modeling_flax_auto.py
src/transformers/utils/dummy_flax_objects.py
utils/check_copies.py
utils/check_repo.py
==================
90d5ab3bf;Manuel Romero;2020-11-24 23:51:24 +0100;Create README.md (#8761)

==

model_cards/mrm8488/bert-small2bert-small-finetuned-cnn_daily_mail-summarization/README.md
==================
29d499245;Julien Plu;2020-11-24 19:55:00 +0100;New TF model inputs (#8602)
* Apply on BERT and ALBERT

* Update TF Bart

* Add input processing to TF BART

* Add input processing for TF CTRL

* Add input processing to TF Distilbert

* Add input processing to TF DPR

* Add input processing to TF Electra

* Add input processing for TF Flaubert

* Add deprecated arguments

* Add input processing to TF XLM

* remove unused imports

* Add input processing to TF Funnel

* Add input processing to TF GPT2

* Add input processing to TF Longformer

* Add input processing to TF Lxmert

* Apply style

* Add input processing to TF Mobilebert

* Add input processing to TF GPT

* Add input processing to TF Roberta

* Add input processing to TF T5

* Add input processing to TF TransfoXL

* Apply style

* Rebase on master

* Bug fix

* Retry to bugfix

* Retry bug fix

* Fix wrong model name

* Try another fix

* Fix BART

* Fix input precessing

* Apply style

* Put the deprecated warnings in the input processing function

* Remove the unused imports

* Raise an error when len(kwargs)>0

* test ModelOutput instead of TFBaseModelOutput

* Bug fix

* Address Patrick's comments

* Address Patrick's comments

* Address Sylvain's comments

* Add the new inputs in new Longformer models

* Update the template with the new input processing

* Remove useless assert

* Apply style

* Trigger CI
==

src/transformers/generation_tf_utils.py
src/transformers/modeling_tf_utils.py
src/transformers/models/albert/modeling_tf_albert.py
src/transformers/models/bart/modeling_tf_bart.py
src/transformers/models/bert/modeling_tf_bert.py
src/transformers/models/blenderbot/modeling_tf_blenderbot.py
src/transformers/models/ctrl/modeling_tf_ctrl.py
src/transformers/models/distilbert/modeling_tf_distilbert.py
src/transformers/models/dpr/modeling_tf_dpr.py
src/transformers/models/electra/modeling_tf_electra.py
src/transformers/models/flaubert/modeling_tf_flaubert.py
src/transformers/models/funnel/modeling_tf_funnel.py
src/transformers/models/gpt2/modeling_tf_gpt2.py
src/transformers/models/longformer/modeling_tf_longformer.py
src/transformers/models/lxmert/modeling_tf_lxmert.py
src/transformers/models/mobilebert/modeling_tf_mobilebert.py
src/transformers/models/openai/modeling_tf_openai.py
src/transformers/models/roberta/modeling_tf_roberta.py
src/transformers/models/t5/modeling_tf_t5.py
src/transformers/models/transfo_xl/modeling_tf_transfo_xl.py
src/transformers/models/xlm/modeling_tf_xlm.py
src/transformers/models/xlnet/modeling_tf_xlnet.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_tf_{{cookiecutter.lowercase_modelname}}.py
tests/test_modeling_tf_bart.py
tests/test_modeling_tf_blenderbot.py
tests/test_modeling_tf_common.py
==================
82d443a7f;Stas Bekman;2020-11-24 10:22:25 -0800;[core] implement support for run-time dependency version checking (#8645)
* implement support for run-time dependency version checking

* try not escaping !

* use findall that works on py36

* small tweaks

* autoformatter worship

* simplify

* shorter names

* add support for non-versioned checks

* add deps

* revert

* tokenizers not required, check version only if installed

* make a proper distutils cmd and add make target

* tqdm must be checked before tokenizers

* workaround the DistributionNotFound peculiar setup

* handle the rest of packages in setup.py

* fully sync setup.py's install_requires - to check them all

* nit

* make install_requires more readable

* typo

* Update setup.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* restyle

* add types

* simplify

* simplify2

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

Makefile
examples/lightning_base.py
setup.py
src/transformers/__init__.py
src/transformers/dependency_versions_check.py
src/transformers/dependency_versions_table.py
src/transformers/utils/versions.py
tests/test_versions_utils.py
==================
a7d73cfdd;Quentin Lhoest;2020-11-24 17:04:47 +0100;fix rag index names in eval_rag.py example (#8730)

==

examples/rag/eval_rag.py
==================
8d4ed7e95;Binoy Dalal;2020-11-24 10:11:46 -0500;added instructions for syncing upstream master with forked master via PR (#8745)
* added instructions for syncing upstream master with forked master via PR

* expand to add a note to why this is requested

Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>
==

CONTRIBUTING.md
==================
e09e54fd9;Lysandre Debut;2020-11-24 09:50:25 -0500;MT5 should have an autotokenizer (#8743)
* MT5 should have an autotokenizer

* Different configurations should be able to point to same tokenizers
==

src/transformers/models/auto/tokenization_auto.py
tests/test_tokenization_auto.py
==================
6fdd0bb23;Lysandre Debut;2020-11-24 09:35:12 -0500;Fix slow tests v2 (#8746)
* Fix BART test

* Fix MBART tests

* Remove erroneous line from yaml

* Update tests/test_modeling_bart.py

* Quality
==

.github/workflows/self-scheduled.yml
tests/test_modeling_bart.py
tests/test_modeling_mbart.py
==================
2c83b3c38;zhiheng-huang;2020-11-24 05:40:53 -0800;Support various BERT relative position embeddings (2nd) (#8276)
* Support BERT relative position embeddings

* Fix typo in README.md

* Address review comment

* Fix failing tests

* [tiny] Fix style_doc.py check by adding an empty line to configuration_bert.py

* make fix copies

* fix configs of electra and albert and fix longformer

* remove copy statement from longformer

* fix albert

* fix electra

* Add bert variants forward tests for various position embeddings

* [tiny] Fix style for test_modeling_bert.py

* improve docstring

* [tiny] improve docstring and remove unnecessary dependency

* [tiny] Remove unused import

* re-add to ALBERT

* make embeddings work for ALBERT

* add test for albert

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
==

examples/question-answering/README.md
src/transformers/models/albert/configuration_albert.py
src/transformers/models/albert/modeling_albert.py
src/transformers/models/bert/configuration_bert.py
src/transformers/models/bert/modeling_bert.py
src/transformers/models/bert_generation/configuration_bert_generation.py
src/transformers/models/electra/configuration_electra.py
src/transformers/models/electra/modeling_electra.py
src/transformers/models/layoutlm/modeling_layoutlm.py
src/transformers/models/longformer/modeling_longformer.py
src/transformers/models/roberta/modeling_roberta.py
tests/test_modeling_albert.py
tests/test_modeling_bert.py
tests/test_modeling_electra.py
tests/test_modeling_layoutlm.py
tests/test_modeling_roberta.py
==================
9e71aa2f8;Julien Chaumond;2020-11-24 14:15:30 +0100;[EsperBERTo] Fix URLs to assets

==

model_cards/julien-c/EsperBERTo-small-pos/README.md
model_cards/julien-c/EsperBERTo-small/README.md
==================
02f48b9bf;Lysandre Debut;2020-11-23 20:14:48 -0500;Model parallel documentation (#8741)
* Add parallelize methods to the .rst files

* Correct format
==

docs/source/model_doc/gpt2.rst
docs/source/model_doc/t5.rst
src/transformers/models/gpt2/modeling_gpt2.py
src/transformers/models/t5/modeling_t5.py
==================
7f2c00913;LysandreJik;2020-11-23 18:19:54 -0500;TF BERT test update

==

tests/test_modeling_tf_bert.py
==================
e1b7e10d5;LysandreJik;2020-11-23 18:19:12 -0500;Update TF BERT test

==

tests/test_modeling_tf_bert.py
==================
8ffc01a76;Colin Brochtrup;2020-11-23 17:25:35 -0500;Add early stopping callback to pytorch trainer (#8581)
* Add early stopping patience and minimum threshold metric must improve to prevent early stopping to pytorch trainer

* Add early stopping test

* Set patience counter to 0 if best metric not defined yet

* Make early stopping a callback. Add callback event for updating the best metric for early stopping callback to trigger on.

* Run make style

* make funciton name sensible

* Improve new argument docstring wording and hope that flakey CI test passes.

* Use on_evaluation callback instead of custom. Remove some debug printing

* Move early stopping arguments and state into early stopping callback

* Run make style

* Remove old code

* Fix docs formatting. make style went rogue on me.

* Remove copied attributes and fix variable

* Add assertions on training arguments instead of mutating them. Move comment out of public docs.

* Make separate test for early stopping callback. Add test of invalid arguments.

* Run make style... I remembered before CI this time!

* appease flake8

* Add EarlyStoppingCallback to callback docs

* Make docstring EarlyStoppingCallabck match other callbacks.

* Fix typo in docs
==

docs/source/main_classes/callback.rst
src/transformers/__init__.py
src/transformers/trainer_callback.py
tests/test_trainer.py
==================
367f497de;Sylvain Gugger;2020-11-23 16:02:31 -0500;Fix max length in run_plm script (#8738)

==

examples/language-modeling/run_plm.py
==================
e84786aaa;Stas Bekman;2020-11-23 12:33:13 -0800;consistent ignore keys + make private (#8737)
* consistent ignore keys + make private

* style

* - authorized_missing_keys    => _keys_to_ignore_on_load_missing
  - authorized_unexpected_keys => _keys_to_ignore_on_load_unexpected

* move public doc of private attributes to private comment
==

src/transformers/modeling_tf_pytorch_utils.py
src/transformers/modeling_tf_utils.py
src/transformers/modeling_utils.py
src/transformers/models/albert/modeling_albert.py
src/transformers/models/albert/modeling_tf_albert.py
src/transformers/models/bart/modeling_bart.py
src/transformers/models/bart/modeling_tf_bart.py
src/transformers/models/bert/modeling_bert.py
src/transformers/models/bert/modeling_tf_bert.py
src/transformers/models/bert_generation/modeling_bert_generation.py
src/transformers/models/deberta/modeling_deberta.py
src/transformers/models/dpr/modeling_dpr.py
src/transformers/models/electra/modeling_electra.py
src/transformers/models/fsmt/modeling_fsmt.py
src/transformers/models/gpt2/modeling_gpt2.py
src/transformers/models/layoutlm/modeling_layoutlm.py
src/transformers/models/longformer/modeling_longformer.py
src/transformers/models/longformer/modeling_tf_longformer.py
src/transformers/models/marian/modeling_marian.py
src/transformers/models/marian/modeling_tf_marian.py
src/transformers/models/mbart/modeling_mbart.py
src/transformers/models/mobilebert/modeling_mobilebert.py
src/transformers/models/mobilebert/modeling_tf_mobilebert.py
src/transformers/models/mt5/modeling_mt5.py
src/transformers/models/openai/modeling_openai.py
src/transformers/models/pegasus/modeling_pegasus.py
src/transformers/models/pegasus/modeling_tf_pegasus.py
src/transformers/models/rag/modeling_rag.py
src/transformers/models/roberta/modeling_roberta.py
src/transformers/models/roberta/modeling_tf_roberta.py
src/transformers/models/squeezebert/modeling_squeezebert.py
src/transformers/models/t5/modeling_t5.py
src/transformers/models/xlm/modeling_xlm.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_{{cookiecutter.lowercase_modelname}}.py
tests/test_modeling_common.py
tests/test_modeling_marian.py
tests/test_modeling_mbart.py
tests/test_modeling_pegasus.py
==================
49759c0cd;Sylvain Gugger;2020-11-23 15:02:59 -0500;Document new training argument

==

src/transformers/training_args.py
==================
1cd9be2ae;alexorona;2020-11-23 11:41:23 -0800;gpt2 and t5 parallel modeling (#8696)
* gpt2 and t5 parallel modeling

* model_parallel utils update

* adding missing model_parallel_utils

Adds missing model_parallel_utils and reverses the changes to code in modeling_gpt2 and modeling_t5

* training_args reformat

Reformatted training_args

* style formatting

Style formatting doc string length on training_args and model_parallel_utils

* style changes

make style && make quality for training_args and model_parallel_utils.

* adding tests

* minor change in trainer

reverts loss calculation

* Update training_args.py

* Update training_args.py

added back docstring language for adam_beta1 and adam_beta2

* Update trainer.py

* Update src/transformers/trainer.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Fix style & rebase

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
Co-authored-by: LysandreJik <lysandre.debut@reseau.eseo.fr>
==

src/transformers/models/gpt2/modeling_gpt2.py
src/transformers/models/t5/modeling_t5.py
src/transformers/trainer.py
src/transformers/training_args.py
src/transformers/utils/model_parallel_utils.py
tests/test_modeling_common.py
tests/test_modeling_gpt2.py
tests/test_modeling_t5.py
==================
1e45bef0a;Stas Bekman;2020-11-23 10:57:27 -0800;[trainer] make generate work with multigpu (#8716)
* make generate work with multigpu

* better fix - thanks @sgugger
==

examples/seq2seq/seq2seq_trainer.py
examples/seq2seq/test_finetune_trainer.py
==================
900024273;Sylvain Gugger;2020-11-23 13:56:45 -0500;Change default cache path (#8734)
* Change default cache path

* Document changes

* Apply suggestions from code review

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
==

docs/source/installation.md
src/transformers/file_utils.py
==================
0cc5ab133;Julien Chaumond;2020-11-23 17:15:02 +0100;Improve bert-japanese tokenizer handling (#8659)
* Make ci fail

* Try to make tests actually run?

* CI finally failing?

* Fix CI

* Revert "Fix CI"

This reverts commit ca7923be7334d4e571b023478ebdd6b33dfd0ebb.

* Ooops wrong one

* one more try

* Ok ok let's move this elsewhere

* Alternative to globals() (#8667)

* Alternative to globals()

* Error is raised later so return None

* Sentencepiece not installed make some tokenizers None

* Apply Lysandre wisdom

* Slightly clearer comment?

cc @sgugger

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

.circleci/config.yml
src/transformers/models/auto/tokenization_auto.py
tests/test_tokenization_bert_japanese.py
==================
eec76615f;Amine Abdaoui;2020-11-23 17:09:50 +0100;[model_cards]: control input examples of Geotrend models (#8727)
* [model_cards]: control arabic model examples

* [model_cards]: control input examples of Geotrend models

* [model_cards]: add link to generatation script
==

model_cards/Geotrend/bert-base-15lang-cased/README.md
model_cards/Geotrend/bert-base-ar-cased/README.md
model_cards/Geotrend/bert-base-bg-cased/README.md
model_cards/Geotrend/bert-base-de-cased/README.md
model_cards/Geotrend/bert-base-el-cased/README.md
model_cards/Geotrend/bert-base-en-ar-cased/README.md
model_cards/Geotrend/bert-base-en-bg-cased/README.md
model_cards/Geotrend/bert-base-en-cased/README.md
model_cards/Geotrend/bert-base-en-de-cased/README.md
model_cards/Geotrend/bert-base-en-el-cased/README.md
model_cards/Geotrend/bert-base-en-es-cased/README.md
model_cards/Geotrend/bert-base-en-fr-cased/README.md
model_cards/Geotrend/bert-base-en-hi-cased/README.md
model_cards/Geotrend/bert-base-en-ru-cased/README.md
model_cards/Geotrend/bert-base-en-sw-cased/README.md
model_cards/Geotrend/bert-base-en-th-cased/README.md
model_cards/Geotrend/bert-base-en-tr-cased/README.md
model_cards/Geotrend/bert-base-en-ur-cased/README.md
model_cards/Geotrend/bert-base-en-vi-cased/README.md
model_cards/Geotrend/bert-base-en-zh-cased/README.md
model_cards/Geotrend/bert-base-es-cased/README.md
model_cards/Geotrend/bert-base-fr-cased/README.md
model_cards/Geotrend/bert-base-hi-cased/README.md
model_cards/Geotrend/bert-base-ru-cased/README.md
model_cards/Geotrend/bert-base-sw-cased/README.md
model_cards/Geotrend/bert-base-th-cased/README.md
model_cards/Geotrend/bert-base-tr-cased/README.md
model_cards/Geotrend/bert-base-ur-cased/README.md
model_cards/Geotrend/bert-base-vi-cased/README.md
model_cards/Geotrend/bert-base-zh-cased/README.md
==================
143b564e5;Jessica Yung;2020-11-23 22:58:52 +0800;Add pip install update to resolve import error in transformers notebook (#8616)
* Add pip install update to resolve import error

Add pip install upgrade tensorflow-gpu to remove error below:
```
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-2-094fadb93f3f> in <module>()
      1 import torch
----> 2 from transformers import AutoModel, AutoTokenizer, BertTokenizer
      3 
      4 torch.set_grad_enabled(False)

4 frames
/usr/local/lib/python3.6/dist-packages/transformers/__init__.py in <module>()
    133 
    134 # Pipelines
--> 135 from .pipelines import (
    136     Conversation,
    137     ConversationalPipeline,

/usr/local/lib/python3.6/dist-packages/transformers/pipelines.py in <module>()
     46     import tensorflow as tf
     47 
---> 48     from .modeling_tf_auto import (
     49         TF_MODEL_FOR_QUESTION_ANSWERING_MAPPING,
     50         TF_MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING,

/usr/local/lib/python3.6/dist-packages/transformers/modeling_tf_auto.py in <module>()
     49 from .configuration_utils import PretrainedConfig
     50 from .file_utils import add_start_docstrings
---> 51 from .modeling_tf_albert import (
     52     TFAlbertForMaskedLM,
     53     TFAlbertForMultipleChoice,

/usr/local/lib/python3.6/dist-packages/transformers/modeling_tf_albert.py in <module>()
     22 import tensorflow as tf
     23 
---> 24 from .activations_tf import get_tf_activation
     25 from .configuration_albert import AlbertConfig
     26 from .file_utils import (

/usr/local/lib/python3.6/dist-packages/transformers/activations_tf.py in <module>()
     52     "gelu": tf.keras.layers.Activation(gelu),
     53     "relu": tf.keras.activations.relu,
---> 54     "swish": tf.keras.activations.swish,
     55     "silu": tf.keras.activations.swish,
     56     "gelu_new": tf.keras.layers.Activation(gelu_new),

AttributeError: module 'tensorflow_core.python.keras.api._v2.keras.activations' has no attribute 'swish'
```
I have tried running the colab after this change and it seems to work fine (all the cells run with no errors).

* Update notebooks/02-transformers.ipynb

only need to upgrade tensorflow, not tensorflow-gpu.

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
==

notebooks/02-transformers.ipynb
==================
18c8cf000;Yossi Synett;2020-11-23 12:28:29 +0000;Fix bug in x-attentions output for roberta and harden test to catch it (#8660)

==

src/transformers/models/roberta/modeling_roberta.py
tests/test_modeling_encoder_decoder.py
==================
48cc22470;Tony;2020-11-23 11:52:29 +0100;[model_cards] Add card for gpt2-rnm (#8673)

==

model_cards/e-tony/gpt2-rnm/README.md
==================
52585e40a;Nguyen Van Nha;2020-11-23 17:51:54 +0700;create README.md (#8682)
* create README.md

* Apply suggestions from code review

Co-authored-by: Julien Chaumond <chaumond@gmail.com>
==

model_cards/NlpHUST/vibert4news-base-cased/README.md
==================
b5187e317;Sagor Sarker;2020-11-23 16:51:16 +0600;added bangla-bert-sentiment model card (#8687)

==

model_cards/sagorsarker/bangla-bert-sentiment/README.md
==================
b6d864e2f;moniquebm;2020-11-23 06:48:10 -0300;Create README.md (#8630)
* Create README.md

* correct metrics id

cc @lhoestq

Co-authored-by: Julien Chaumond <chaumond@gmail.com>
==

model_cards/monilouise/ner_pt_br/README.md
==================
e1f3156b2;Santiago Castro;2020-11-22 00:58:10 -0300;Fix many typos (#8708)

==

docs/source/index.rst
docs/source/model_doc/dpr.rst
docs/source/model_summary.rst
docs/source/multilingual.rst
docs/source/perplexity.rst
examples/benchmarking/plot_csv_file.py
examples/distillation/README.md
examples/distillation/distiller.py
examples/distillation/scripts/extract_distilbert.py
examples/movement-pruning/README.md
examples/movement-pruning/emmental/modules/binarizer.py
model_cards/KB/albert-base-swedish-cased-alpha/README.md
model_cards/KB/bert-base-swedish-cased-ner/README.md
model_cards/elgeish/cs224n-squad2.0-albert-base-v2/README.md
model_cards/elgeish/cs224n-squad2.0-albert-xxlarge-v1/README.md
model_cards/elgeish/cs224n-squad2.0-distilbert-base-uncased/README.md
model_cards/elgeish/cs224n-squad2.0-roberta-base/README.md
model_cards/mrm8488/t5-base-finetuned-e2m-intent/README.md
model_cards/mrm8488/t5-base-finetuned-question-generation-ap/README.md
model_cards/mrm8488/t5-base-finetuned-squadv2/README.md
model_cards/mrm8488/t5-base-finetuned-wikiSQL-sql-to-en/README.md
model_cards/mrm8488/t5-base-finetuned-wikiSQL/README.md
model_cards/mrm8488/t5-small-finetuned-quora-for-paraphrasing/README.md
model_cards/mrm8488/t5-small-finetuned-squadv1/README.md
model_cards/mrm8488/t5-small-finetuned-squadv2/README.md
model_cards/mrm8488/t5-small-finetuned-wikiSQL/README.md
src/transformers/modeling_tf_pytorch_utils.py
src/transformers/models/fsmt/modeling_fsmt.py
src/transformers/models/t5/modeling_tf_t5.py
src/transformers/models/transfo_xl/modeling_transfo_xl.py
src/transformers/models/transfo_xl/modeling_transfo_xl_utilities.py
src/transformers/models/xlm/modeling_tf_xlm.py
src/transformers/models/xlm_roberta/tokenization_xlm_roberta.py
src/transformers/models/xlnet/modeling_xlnet.py
src/transformers/optimization_tf.py
==================
9c0afdaf7;Patrick von Platen;2020-11-20 22:07:21 +0100;fix flaky ci (#8694)

==

tests/test_generation_utils.py
==================
29bdb8836;Binoy Dalal;2020-11-20 13:59:06 -0500;Vectorize RepetitionPenaltyLogitsProcessor to improve performance (#8598)
* refactored exisiting nested loops to vectorized implementation

* replaced explicit indexing with torch.where

* modifying score for previous input_ids only
==

src/transformers/generation_logits_process.py
==================
2594bd8b7;Roman Kalyakin;2020-11-20 19:33:54 +0100;moved temperature wrapper before topP/topK (#8686)

==

src/transformers/generation_utils.py
==================
8062fa63c;Quentin Lhoest;2020-11-20 19:05:03 +0100;Fix rag finetuning + add finetuning test (#8585)
* replace init_ddp_connection for index init

* style

* add finetune test

* add test data

* move generate tensors to device

* add test on EM metric

* style

* allow multi process test

* keep gloo process group for retrieval

* add multi-gpu test

* use custom accelerator

* clean test finetune

* minor

* style

* style

* typo

* use python call instead of imported main fumction

* return_dict fix in modeling_rag

* use float32 in retrieval

* store as float32 as well in the custom knowledge dataset example

* style

* rename to finetune_rag

* style

* update readme

* rename utils and callbacks to utils_rag and callbacks_rag

* fix test

* patrick's comments

* generate dummy data in the finetue test script

* remove dummy data files

* style
==

examples/lightning_base.py
examples/rag/README.md
examples/rag/callbacks_rag.py
examples/rag/distributed_retriever.py
examples/rag/finetune_rag.py
examples/rag/finetune_rag.sh
examples/rag/test_finetune_rag.py
examples/rag/use_own_knowledge_dataset.py
examples/rag/utils_rag.py
src/transformers/models/rag/modeling_rag.py
src/transformers/models/rag/retrieval_rag.py
==================
63e91f5fd;Sylvain Gugger;2020-11-20 09:27:25 -0500;Document adam betas TrainingArguments (#8688)

==

src/transformers/training_args.py
==================
94caaa93c;Kevin Canwen Xu;2020-11-20 13:26:33 +0800;Update the bibtex with EMNLP demo (#8678)
* Update the bibtex with EMNLP demo

* Update README.md

* Update README.md
==

README.md
==================
6494910f2;Sylvain Gugger;2020-11-19 16:44:20 -0500;Add sentencepiece to the CI and fix tests (#8672)
* Fix the CI and tests

* Fix quality

* Remove that m form nowhere
==

.circleci/config.yml
src/transformers/models/mbart/tokenization_mbart.py
src/transformers/models/mbart/tokenization_mbart_fast.py
src/transformers/models/t5/tokenization_t5.py
src/transformers/models/t5/tokenization_t5_fast.py
tests/test_tokenization_mbart.py
==================
0ad45e108;Stas Bekman;2020-11-19 12:46:04 -0800;[examples/seq2seq] fix PL deprecation warning (#8577)
* fix deprecation warning

* fix
==

examples/seq2seq/callbacks.py
==================
0e19a4c2d;Arindum Roy;2020-11-19 15:45:06 -0500;Update bert-base-multilingual-cased-README.md (#8668)
The heading was originally uncased, which did not reflect the contents of this README. Changed it to cased.
==

model_cards/bert-base-multilingual-cased-README.md
==================
06518404c;Stas Bekman;2020-11-19 12:12:46 -0800;revert

==

README.md
==================
297a29382;Stas Bekman;2020-11-19 12:11:35 -0800;Please fix your software not to ping master
You may be unaware but you're running some software that meddles with every commit on https://github.com/huggingface/transformers/

Something is wrong with the software you're using. It adds a reference to almost every PR in the master tree. Which is very wrong. Please check your software and please don't do it again.

Example:
see the bottom of this PR and most other PRs:
https://github.com/huggingface/transformers/pull/8639
==

README.md
==================
42111f1d5;Stas Bekman;2020-11-19 12:06:01 -0800;[tokenizers] convert_to_tensors: don't reconvert when the type is already right (#8283)
* don't reconvert when the type is already right

* better name

* adjust logic as suggested

* merge
==

src/transformers/tokenization_utils_base.py
tests/test_tokenization_utils.py
==================
20b658607;Sylvain Gugger;2020-11-19 13:59:30 -0500;Fix run_ner script (#8664)
* Fix run_ner script

* Pin datasets
==

examples/requirements.txt
examples/token-classification/run_ner.py
==================
ca0109bd6;Zhylko Dima;2020-11-19 19:18:07 +0100;`disable_ngram_loss` fix for prophetnet (#8554)
* `disable_ngram_loss` fix for prophetnet

* add changes documentation

* fix _compute_loss to use mean reduction and -100 to masked tokens & remove unnecessary arguments

* mean label smoothing loss

* small refactor

* fix test

Co-authored-by: patrickvonplaten <patrick.v.platen@gmail.com>
==

src/transformers/models/prophetnet/modeling_prophetnet.py
tests/test_modeling_prophetnet.py
==================
0603564e9;Sylvain Gugger;2020-11-19 12:18:57 -0500;Merge remote-tracking branch 'origin/master'

==
==================
1e08af383;Sylvain Gugger;2020-11-19 12:18:50 -0500;Forgot to save...

==

scripts/fsmt/fsmt-make-super-tiny-model.py
==================
d86b5ffc6;LysandreJik;2020-11-19 12:00:03 -0500;Release: v4.0.0-rc-1

==

setup.py
src/transformers/__init__.py
==================
cb3e5c33f;Sylvain Gugger;2020-11-19 11:56:42 -0500;Fix a few last paths for the new repo org (#8666)

==

docs/source/testing.rst
examples/contrib/run_chinese_ref.py
examples/longform-qa/eli5_app.py
model_cards/julien-c/dummy-unknown/README.md
src/transformers/testing_utils.py
==================
a79a96dda;Matthias;2020-11-19 08:24:11 -0800;fix small typo (#8644)
Fixed a small typo on the XLNet and permutation language modelling section
==

examples/language-modeling/README.md
==================
4208f496e;Sylvain Gugger;2020-11-19 10:43:15 -0500;Better filtering of the model outputs in Trainer (#8633)
* Better filtering of the model outputs in Trainer

* Fix examples tests

* Add test for Lysandre
==

examples/seq2seq/seq2seq_trainer.py
src/transformers/configuration_utils.py
src/transformers/models/bart/configuration_bart.py
src/transformers/models/ctrl/configuration_ctrl.py
src/transformers/models/gpt2/configuration_gpt2.py
src/transformers/models/marian/configuration_marian.py
src/transformers/models/mbart/configuration_mbart.py
src/transformers/models/mt5/configuration_mt5.py
src/transformers/models/pegasus/configuration_pegasus.py
src/transformers/models/prophetnet/configuration_prophetnet.py
src/transformers/models/reformer/configuration_reformer.py
src/transformers/models/t5/configuration_t5.py
src/transformers/models/transfo_xl/configuration_transfo_xl.py
src/transformers/models/xlnet/configuration_xlnet.py
src/transformers/trainer.py
tests/test_trainer.py
==================
f2e07e727;Lysandre Debut;2020-11-19 10:41:41 -0500;Fix a bunch of slow tests (#8634)
* CI should install `sentencepiece`

* Requiring TF

* Fixing some TFDPR bugs

* remove return_dict=False/True hack

Co-authored-by: patrickvonplaten <patrick.v.platen@gmail.com>
==

.github/workflows/self-push.yml
.github/workflows/self-scheduled.yml
src/transformers/models/dpr/modeling_tf_dpr.py
tests/test_modeling_tf_bert.py
tests/test_modeling_tf_dpr.py
tests/test_modeling_tf_electra.py
==================
5362bb8a6;elk-cloner;2020-11-19 19:07:27 +0330;Tf longformer for sequence classification (#8231)
* working on LongformerForSequenceClassification

* add TFLongformerForMultipleChoice

* add TFLongformerForTokenClassification

* use add_start_docstrings_to_model_forward

* test TFLongformerForSequenceClassification

* test TFLongformerForMultipleChoice

* test TFLongformerForTokenClassification

* remove test from repo

* add test and doc for TFLongformerForSequenceClassification, TFLongformerForTokenClassification, TFLongformerForMultipleChoice

* add requested classes to modeling_tf_auto.py
update dummy_tf_objects
fix tests
fix bugs in requested classes

* pass all tests except test_inputs_embeds

* sync with master

* pass all tests except test_inputs_embeds

* pass all tests

* pass all tests

* work on test_inputs_embeds

* fix style and quality

* make multi choice work

* fix TFLongformerForTokenClassification signature

* fix TFLongformerForMultipleChoice, TFLongformerForSequenceClassification signature

* fix mult choice

* fix mc hint

* fix input embeds

* fix input embeds

* refactor input embeds

* fix copy issue

* apply sylvains changes and clean more

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
==

docs/source/model_doc/longformer.rst
src/transformers/__init__.py
src/transformers/models/auto/modeling_tf_auto.py
src/transformers/models/longformer/__init__.py
src/transformers/models/longformer/modeling_longformer.py
src/transformers/models/longformer/modeling_tf_longformer.py
src/transformers/models/roberta/modeling_tf_roberta.py
src/transformers/utils/dummy_tf_objects.py
tests/test_modeling_longformer.py
tests/test_modeling_tf_longformer.py
==================
62cd9ce9f;Quentin Lhoest;2020-11-19 15:17:18 +0100;fix missing return dict (#8653)

==

examples/rag/use_own_knowledge_dataset.py
==================
0c2677f52;Amine Abdaoui;2020-11-19 11:41:02 +0100;[model card] : fix bert-base-15lang-cased (#8655)
the table was badly formatted because of a single line break
==

model_cards/Geotrend/bert-base-15lang-cased/README.md
==================
0a80959bd;Amine Abdaoui;2020-11-19 10:47:24 +0100;Add cards for all Geotrend models (#8617)
* docs(bert-base-15lang-cased): add model card

* add cards for all Geotrend models

* [model cards] fix language tag for all Geotrend models
==

model_cards/Geotrend/bert-base-15lang-cased/README.md
model_cards/Geotrend/bert-base-ar-cased/README.md
model_cards/Geotrend/bert-base-bg-cased/README.md
model_cards/Geotrend/bert-base-de-cased/README.md
model_cards/Geotrend/bert-base-el-cased/README.md
model_cards/Geotrend/bert-base-en-ar-cased/README.md
model_cards/Geotrend/bert-base-en-bg-cased/README.md
model_cards/Geotrend/bert-base-en-cased/README.md
model_cards/Geotrend/bert-base-en-de-cased/README.md
model_cards/Geotrend/bert-base-en-el-cased/README.md
model_cards/Geotrend/bert-base-en-es-cased/README.md
model_cards/Geotrend/bert-base-en-fr-cased/README.md
model_cards/Geotrend/bert-base-en-hi-cased/README.md
model_cards/Geotrend/bert-base-en-ru-cased/README.md
model_cards/Geotrend/bert-base-en-sw-cased/README.md
model_cards/Geotrend/bert-base-en-th-cased/README.md
model_cards/Geotrend/bert-base-en-tr-cased/README.md
model_cards/Geotrend/bert-base-en-ur-cased/README.md
model_cards/Geotrend/bert-base-en-vi-cased/README.md
model_cards/Geotrend/bert-base-en-zh-cased/README.md
model_cards/Geotrend/bert-base-es-cased/README.md
model_cards/Geotrend/bert-base-fr-cased/README.md
model_cards/Geotrend/bert-base-hi-cased/README.md
model_cards/Geotrend/bert-base-ru-cased/README.md
model_cards/Geotrend/bert-base-sw-cased/README.md
model_cards/Geotrend/bert-base-th-cased/README.md
model_cards/Geotrend/bert-base-tr-cased/README.md
model_cards/Geotrend/bert-base-ur-cased/README.md
model_cards/Geotrend/bert-base-vi-cased/README.md
model_cards/Geotrend/bert-base-zh-cased/README.md
==================
dcc9c6429;cronoik;2020-11-19 00:56:47 +0100;Updated the Extractive Question Answering code snippets (#8636)
* Updated the Extractive Question Answering code snippets

The Extractive Question Answering code snippets do not work anymore since the models return task-specific output objects. This commit fixes the pytorch and tensorflow examples but adding `.values()` to the model call.

* Update task_summary.rst
==

docs/source/task_summary.rst
==================
28d16e7ac;Tim Isbister;2020-11-19 00:35:23 +0100;Update README.md (#8635)

==

examples/language-modeling/README.md
==================
b290195ac;cronoik;2020-11-19 00:04:25 +0100;grammar (#8639)

==

.github/PULL_REQUEST_TEMPLATE.md
==================
d86d57faa;Stas Bekman;2020-11-18 12:51:29 -0800;[s2s] distillation apex breaks return_dict obj (#8631)
* apex breaks return_dict obj

* style
==

examples/seq2seq/distillation.py
==================
bf3611b2a;Perez Ogayo;2020-11-18 21:42:13 +0200;Created ModelCard for Hel-ach-en MT model (#8496)
* Updated ModelCard

* Apply suggestions from code review

Co-authored-by: Julien Chaumond <chaumond@gmail.com>
==

model_cards/Ogayo/Hel-ach-en/README.md
==================
c95b26a71;Yifan Peng;2020-11-18 13:37:14 -0500;Create README.md (#8362)

==

model_cards/bionlp/bluebert_pubmed_mimic_uncased_L-24_H-1024_A-16/README.md
==================
fdbbb6c17;Manuel Romero;2020-11-18 19:34:27 +0100;Model card: T5-base fine-tuned on QuaRTz (#8369)
* Model card: T5-base fine-tuned on QuaRTz

* Update model_cards/mrm8488/t5-base-finetuned-quartz/README.md

Co-authored-by: Julien Chaumond <chaumond@gmail.com>
==

model_cards/mrm8488/t5-base-finetuned-quartz/README.md
==================
6e6d24c5d;Yifan Peng;2020-11-18 13:33:04 -0500;Create README.md (#8363)

==

model_cards/bionlp/bluebert_pubmed_uncased_L-24_H-1024_A-16/README.md
==================
35fd3d64e;Divyanshu Kakwani;2020-11-18 23:58:49 +0530;Add model card for ai4bharat/indic-bert (#8464)

==

model_cards/ai4bharat/indic-bert/README.md
==================
38f01dfe0;dartrevan;2020-11-18 21:23:08 +0300;Update README.md (#8405)
* Update README.md

* Update README.md
==

model_cards/cimm-kzn/endr-bert/README.md
==================
2d8fbf012;Abhilash Majumder;2020-11-18 23:52:28 +0530;Model Card for abhilash1910/financial_roberta (#8625)
* Model Card for abhilash1910/financial_roberta

* Update model_cards/abhilash1910/financial_roberta/README.md

Co-authored-by: Julien Chaumond <chaumond@gmail.com>
==

model_cards/abhilash1910/financial_roberta/README.md
==================
26dc6593f;Vishal Singh;2020-11-18 23:49:32 +0530;Update README.md (#8544)
Modified Model in Action section. The class `AutoModelWithLMHead` is deprecated so changed it to `AutoModelForSeq2SeqLM` for encoder-decoder models. Removed duplicate eos token.
==

model_cards/mrm8488/t5-base-finetuned-squadv2/README.md
==================
6c8fad4f0;smanjil;2020-11-18 19:17:46 +0100;replace performance table with markdown (#8565)
* replace performance table with markdown

* Update model_cards/smanjil/German-MedBERT/README.md

Co-authored-by: Julien Chaumond <chaumond@gmail.com>
==

model_cards/smanjil/German-MedBERT/README.md
==================
e7f77fc52;hhou435;2020-11-19 02:06:30 +0800;model_cards for Chinese Couplet and Poem GPT2 models (#8620)

==

model_cards/uer/gpt2-chinese-couplet/README.md
model_cards/uer/gpt2-chinese-poem/README.md
==================
a0c62d249;Sylvain Gugger;2020-11-18 12:15:26 -0500;Fix training from scratch in new scripts (#8623)

==

examples/language-modeling/run_clm.py
examples/language-modeling/run_mlm.py
examples/language-modeling/run_mlm_wwm.py
examples/language-modeling/run_plm.py
templates/adding_a_new_example_script/{{cookiecutter.directory_name}}/run_{{cookiecutter.example_shortcut}}.py
==================
1e62e999e;Sylvain Gugger;2020-11-18 12:00:11 -0500;Fixes the training resuming with gradient accumulation (#8624)

==

src/transformers/trainer.py
tests/test_trainer.py
==================
cdfa56afe;Patrick von Platen;2020-11-18 17:14:15 +0100;[Tokenizer Doc] Improve tokenizer summary (#8622)
* improve summary

* small fixes

* cleaned line length

* correct "" formatting

* apply sylvains suggestions
==

docs/source/tokenizer_summary.rst
==================
2f9d49b38;Nicola De Cao;2020-11-18 16:06:25 +0000;Adding PrefixConstrainedLogitsProcessor (#8529)
* Adding PrefixConstrainedLogitsProcessor

* fixing RAG and style_doc

* fixing black (v20 instead of v19)

* Improving doc in generation_logits_process.py

* Improving docs and typing in generation_utils.py

* docs improvement

* adding test and fixing doc typo

* fixing doc_len

* isort on test

* fixed test

* improve docstring a bit

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
==

src/transformers/generation_logits_process.py
src/transformers/generation_utils.py
src/transformers/models/rag/modeling_rag.py
tests/test_generation_logits_process.py
==================
3bc154007;Julien Plu;2020-11-18 16:48:31 +0100;New TF loading weights (#8490)
* New TF loading weights

* apply style

* Better naming

* Largely comment the loading method

* Apply style

* Address Patrick's comments

* Remove useless line of code

* Update Docstring

* Address Sylvain's and Lysandre's comments

* Simplify the names computation

* Typos
==

src/transformers/modeling_tf_utils.py
==================
0df91ee4f;Ratthachat (Jung);2020-11-18 22:30:29 +0700;self.self.activation_dropout -> self.activation_dropout (#8611)
(one line typo)
==

src/transformers/models/bart/modeling_tf_bart.py
==================
cdf1b7ae8;Stas Bekman;2020-11-18 07:25:00 -0800;fix to adjust for #8530 changes (#8612)

==

examples/seq2seq/finetune.py
==================
2819da02f;Stas Bekman;2020-11-18 07:15:53 -0800;[s2s] broken test (#8613)

==

examples/seq2seq/test_finetune_trainer.py
==================
9fa3ed1a7;Micha≈Ç Pogoda;2020-11-18 16:09:26 +0100;Fix missing space in multiline warning (#8593)
Multiline string informing about missing PyTorch/TensorFlow had missing space.
==

src/transformers/__init__.py
==================
8fcb6935a;Sylvain Gugger;2020-11-18 10:02:50 -0500;Fix DataCollatorForLanguageModeling (#8621)

==

src/transformers/data/data_collator.py
==================
f6fe41c96;Benjamin Minixhofer;2020-11-18 15:58:08 +0100;Reset loss to zero on logging in Trainer to avoid bfloat16 issues (#8561)
* make tr_loss regular float

* Revert "make tr_loss regular float"

This reverts commit c9d7ccfaf0c4387187b0841694f01ec0ffd5f4ba.

* reset loss at each logging step

* keep track of total loss with _total_loss_scalar

* add remaining tr_loss at the end
==

src/transformers/trainer.py
==================
b592728ef;cronoik;2020-11-18 01:00:44 +0100;Fixed link to the wrong paper. (#8607)

==

src/transformers/models/reformer/modeling_reformer.py
==================
0512444ee;Sylvain Gugger;2020-11-17 17:34:25 -0500;Remove old doc

==

src/transformers/models/mobilebert/modeling_mobilebert.py
==================
5cf9c7966;Caitlin Ostroff;2020-11-17 21:50:58 +0000;Add Harry Potter Model Card (#8605)
* Add Harry Potter Model

* Update model_cards/ceostroff/harry-potter-gpt2-fanfiction/README.md

* Update model_cards/ceostroff/harry-potter-gpt2-fanfiction/README.md

* Update model_cards/ceostroff/harry-potter-gpt2-fanfiction/README.md

Co-authored-by: Julien Chaumond <chaumond@gmail.com>
==

model_cards/ceostroff/harry-potter-gpt2-fanfiction/README.md
==================
dd52804f5;Sylvain Gugger;2020-11-17 15:11:29 -0500;Remove deprecated (#8604)
* Remove old deprecated arguments

Co-authored-by: LysandreJik <lysandre.debut@reseau.eseo.fr>

* Remove needless imports

* Fix tests

Co-authored-by: LysandreJik <lysandre.debut@reseau.eseo.fr>
==

examples/seq2seq/test_finetune_trainer.py
examples/token-classification/run_ner_old.py
src/transformers/data/processors/squad.py
src/transformers/models/albert/modeling_albert.py
src/transformers/models/bart/modeling_bart.py
src/transformers/models/bert/modeling_bert.py
src/transformers/models/ctrl/modeling_ctrl.py
src/transformers/models/distilbert/modeling_distilbert.py
src/transformers/models/electra/modeling_electra.py
src/transformers/models/fsmt/modeling_fsmt.py
src/transformers/models/gpt2/modeling_gpt2.py
src/transformers/models/gpt2/tokenization_gpt2.py
src/transformers/models/gpt2/tokenization_gpt2_fast.py
src/transformers/models/longformer/modeling_longformer.py
src/transformers/models/mobilebert/modeling_mobilebert.py
src/transformers/models/openai/modeling_openai.py
src/transformers/models/prophetnet/tokenization_prophetnet.py
src/transformers/models/roberta/modeling_roberta.py
src/transformers/models/roberta/tokenization_roberta.py
src/transformers/models/t5/modeling_t5.py
src/transformers/models/t5/modeling_tf_t5.py
src/transformers/models/transfo_xl/configuration_transfo_xl.py
src/transformers/models/transfo_xl/modeling_tf_transfo_xl.py
src/transformers/models/transfo_xl/modeling_transfo_xl.py
src/transformers/models/xlm/modeling_tf_xlm.py
src/transformers/models/xlm/modeling_xlm.py
src/transformers/pipelines.py
src/transformers/tokenization_utils.py
src/transformers/tokenization_utils_base.py
src/transformers/tokenization_utils_fast.py
src/transformers/trainer.py
src/transformers/trainer_tf.py
src/transformers/training_args.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/tokenization_{{cookiecutter.lowercase_modelname}}.py
tests/test_modeling_gpt2.py
tests/test_pipelines_fill_mask.py
tests/test_tokenization_auto.py
==================
3095ee9da;Lysandre Debut;2020-11-17 14:03:03 -0500;Tokenizers should be framework agnostic (#8599)
* Tokenizers should be framework agnostic

* Run the slow tests

* Not testing

* Fix documentation

* Apply suggestions from code review

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
==

.github/workflows/self-push.yml
.github/workflows/self-scheduled.yml
docs/source/model_doc/marian.rst
docs/source/model_doc/mbart.rst
docs/source/model_doc/pegasus.rst
model_cards/tuner007/pegasus_paraphrase/README.md
model_cards/tuner007/pegasus_qa/README.md
scripts/fsmt/fsmt-make-super-tiny-model.py
scripts/fsmt/fsmt-make-tiny-model.py
src/transformers/models/bart/tokenization_bart.py
src/transformers/models/bart/tokenization_bart_fast.py
src/transformers/models/fsmt/tokenization_fsmt.py
src/transformers/models/marian/modeling_marian.py
src/transformers/models/marian/tokenization_marian.py
src/transformers/models/mbart/modeling_mbart.py
src/transformers/models/mbart/tokenization_mbart.py
src/transformers/models/mbart/tokenization_mbart_fast.py
src/transformers/models/pegasus/modeling_pegasus.py
src/transformers/models/pegasus/tokenization_pegasus.py
src/transformers/models/pegasus/tokenization_pegasus_fast.py
src/transformers/models/rag/tokenization_rag.py
src/transformers/tokenization_utils.py
src/transformers/tokenization_utils_base.py
tests/test_modeling_marian.py
tests/test_modeling_mbart.py
tests/test_tokenization_common.py
tests/test_tokenization_mbart.py
tests/test_tokenization_pegasus.py
==================
7f3b41a30;Sylvain Gugger;2020-11-17 14:01:46 -0500;Fix check repo utils (#8600)

==

utils/check_repo.py
==================
f0435f5a6;Stas Bekman;2020-11-17 11:00:41 -0800;these should run fine on multi-gpu (#8582)

==

examples/seq2seq/test_bash_script.py
examples/seq2seq/test_fsmt_bleu_score.py
examples/seq2seq/test_seq2seq_examples.py
==================
36a19915e;Sylvain Gugger;2020-11-17 10:35:38 -0500;Fix model templates (#8595)
* First fixes

* Fix imports and add init

* Fix typo

* Move init to final dest

* Fix tokenization import

* More fixes

* Styling
==

src/transformers/commands/add_new_model.py
src/transformers/models/auto/modeling_auto.py
src/transformers/models/auto/modeling_tf_auto.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/__init__.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/configuration_{{cookiecutter.lowercase_modelname}}.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_tf_{{cookiecutter.lowercase_modelname}}.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_{{cookiecutter.lowercase_modelname}}.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/to_replace_{{cookiecutter.lowercase_modelname}}.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/tokenization_{{cookiecutter.lowercase_modelname}}.py
==================
042a6aa77;Julien Chaumond;2020-11-17 14:58:45 +0100;Tokenizers: ability to load from model subfolder (#8586)
* <small>tiny typo</small>

* Tokenizers: ability to load from model subfolder

* use subfolder for local files as well

* Uniformize model shortcut name => model id

* from s3 => from huggingface.co

Co-authored-by: Quentin Lhoest <lhoest.q@gmail.com>
==

docs/source/pretrained_models.rst
examples/adversarial/run_hans.py
examples/bert-loses-patience/run_glue_with_pabee.py
examples/bertology/run_bertology.py
examples/contrib/legacy/run_language_modeling.py
examples/contrib/mm-imdb/run_mmimdb.py
examples/deebert/run_glue_deebert.py
examples/distillation/run_squad_w_distillation.py
examples/language-modeling/run_clm.py
examples/language-modeling/run_mlm.py
examples/language-modeling/run_mlm_wwm.py
examples/language-modeling/run_plm.py
examples/lightning_base.py
examples/movement-pruning/masked_run_glue.py
examples/movement-pruning/masked_run_squad.py
examples/multiple-choice/run_multiple_choice.py
examples/multiple-choice/run_tf_multiple_choice.py
examples/question-answering/run_squad.py
examples/question-answering/run_squad_trainer.py
examples/question-answering/run_tf_squad.py
examples/rag/finetune.sh
examples/seq2seq/finetune_trainer.py
examples/text-classification/run_glue.py
examples/text-classification/run_tf_glue.py
examples/text-classification/run_tf_text_classification.py
examples/text-classification/run_xnli.py
examples/token-classification/run_ner.py
examples/token-classification/run_ner_old.py
examples/token-classification/run_tf_ner.py
hubconf.py
src/transformers/commands/user.py
src/transformers/configuration_utils.py
src/transformers/file_utils.py
src/transformers/generation_tf_utils.py
src/transformers/modelcard.py
src/transformers/modeling_tf_utils.py
src/transformers/modeling_utils.py
src/transformers/models/auto/configuration_auto.py
src/transformers/models/auto/modeling_auto.py
src/transformers/models/auto/modeling_flax_auto.py
src/transformers/models/auto/modeling_tf_auto.py
src/transformers/models/auto/tokenization_auto.py
src/transformers/models/encoder_decoder/modeling_encoder_decoder.py
src/transformers/models/lxmert/tokenization_lxmert.py
src/transformers/models/lxmert/tokenization_lxmert_fast.py
src/transformers/models/rag/modeling_rag.py
src/transformers/models/rag/tokenization_rag.py
src/transformers/models/reformer/tokenization_reformer.py
src/transformers/models/reformer/tokenization_reformer_fast.py
src/transformers/models/t5/modeling_t5.py
src/transformers/models/t5/tokenization_t5.py
src/transformers/models/t5/tokenization_t5_fast.py
src/transformers/tokenization_utils_base.py
templates/adding_a_new_example_script/{{cookiecutter.directory_name}}/run_{{cookiecutter.example_shortcut}}.py
==================
48395d6b8;Sylvain Gugger;2020-11-17 08:52:13 -0500;Fix init for MT5 (#8591)

==

src/transformers/__init__.py
src/transformers/utils/dummy_pt_objects.py
==================
a6cf9ca00;sgugger;2020-11-17 07:39:37 -0500;Add __init__ to the models folder

==

src/transformers/models/__init__.py
==================
510422355;Patrick von Platen;2020-11-17 12:47:57 +0100;[MT5] More docs (#8589)
* add docs

* make style
==

README.md
docs/source/index.rst
docs/source/model_summary.rst
==================
86822a358;Patrick von Platen;2020-11-17 12:23:09 +0100;T5 & mT5 (#8552)
* add mt5 and t5v1_1 model

* fix tests

* correct some imports

* add tf model

* finish tf t5

* improve examples

* fix copies

* clean doc
==

docs/source/index.rst
docs/source/model_doc/mt5.rst
src/transformers/__init__.py
src/transformers/models/auto/configuration_auto.py
src/transformers/models/auto/modeling_auto.py
src/transformers/models/auto/modeling_tf_auto.py
src/transformers/models/mt5/__init__.py
src/transformers/models/mt5/configuration_mt5.py
src/transformers/models/mt5/modeling_mt5.py
src/transformers/models/mt5/modeling_tf_mt5.py
src/transformers/models/t5/configuration_t5.py
src/transformers/models/t5/convert_t5_original_tf_checkpoint_to_pytorch.py
src/transformers/models/t5/modeling_t5.py
src/transformers/models/t5/modeling_tf_t5.py
src/transformers/utils/dummy_pt_objects.py
src/transformers/utils/dummy_tf_objects.py
tests/test_modeling_mt5.py
tests/test_modeling_t5.py
tests/test_modeling_tf_mt5.py
tests/test_modeling_tf_t5.py
utils/check_repo.py
==================
9e01f988d;fajri91;2020-11-17 19:36:50 +1100;model_card for indolem/indobert-base-uncased (#8579)

==

model_cards/indolem/indobert-base-uncased/README.md
==================
c89bdfbe7;Sylvain Gugger;2020-11-16 21:43:42 -0500;Reorganize repo (#8580)
* Put models in subfolders

* Styling

* Fix imports in tests

* More fixes in test imports

* Sneaky hidden imports

* Fix imports in doc files

* More sneaky imports

* Finish fixing tests

* Fix examples

* Fix path for copies

* More fixes for examples

* Fix dummy files

* More fixes for example

* More model import fixes

* Is this why you're unhappy GitHub?

* Fix imports in conver command
==

.github/workflows/github-torch-hub.yml
.gitignore
docs/source/model_doc/albert.rst
docs/source/model_doc/bart.rst
docs/source/model_doc/bert.rst
docs/source/model_doc/dpr.rst
docs/source/model_doc/electra.rst
docs/source/model_doc/funnel.rst
docs/source/model_doc/gpt.rst
docs/source/model_doc/gpt2.rst
docs/source/model_doc/longformer.rst
docs/source/model_doc/lxmert.rst
docs/source/model_doc/mobilebert.rst
docs/source/model_doc/prophetnet.rst
docs/source/model_doc/rag.rst
docs/source/model_doc/transformerxl.rst
docs/source/model_doc/xlm.rst
docs/source/model_doc/xlnet.rst
examples/bert-loses-patience/pabee/modeling_pabee_albert.py
examples/bert-loses-patience/pabee/modeling_pabee_bert.py
examples/contrib/run_camembert.py
examples/contrib/run_swag.py
examples/deebert/src/modeling_highway_bert.py
examples/deebert/src/modeling_highway_roberta.py
examples/movement-pruning/emmental/modeling_bert_masked.py
examples/rag/distributed_retriever.py
examples/rag/test_distributed_retriever.py
examples/seq2seq/distillation.py
examples/seq2seq/finetune.py
examples/seq2seq/seq2seq_trainer.py
examples/seq2seq/test_datasets.py
examples/seq2seq/test_tatoeba_conversion.py
examples/seq2seq/utils.py
examples/text-generation/pplm/run_pplm.py
model_cards/allenai/wmt16-en-de-12-1/README.md
model_cards/allenai/wmt16-en-de-dist-12-1/README.md
model_cards/allenai/wmt16-en-de-dist-6-1/README.md
model_cards/allenai/wmt19-de-en-6-6-base/README.md
model_cards/allenai/wmt19-de-en-6-6-big/README.md
model_cards/deepset/electra-base-squad2/README.md
model_cards/deepset/minilm-uncased-squad2/README.md
model_cards/deepset/roberta-base-squad2-covid/README.md
model_cards/deepset/roberta-base-squad2-v2/README.md
model_cards/deepset/roberta-base-squad2/README.md
model_cards/deepset/xlm-roberta-large-squad2/README.md
model_cards/facebook/wmt19-de-en/README.md
model_cards/facebook/wmt19-en-de/README.md
model_cards/facebook/wmt19-en-ru/README.md
model_cards/facebook/wmt19-ru-en/README.md
model_cards/julien-c/bert-xsmall-dummy/README.md
scripts/fsmt/gen-card-allenai-wmt16.py
scripts/fsmt/gen-card-allenai-wmt19.py
scripts/fsmt/gen-card-facebook-wmt19.py
src/transformers/__init__.py
src/transformers/benchmark/benchmark.py
src/transformers/benchmark/benchmark_tf.py
src/transformers/benchmark/benchmark_utils.py
src/transformers/commands/convert.py
src/transformers/data/datasets/squad.py
src/transformers/data/metrics/squad_metrics.py
src/transformers/data/processors/squad.py
src/transformers/modelcard.py
src/transformers/models/albert/__init__.py
src/transformers/models/albert/configuration_albert.py
src/transformers/models/albert/convert_albert_original_tf_checkpoint_to_pytorch.py
src/transformers/models/albert/modeling_albert.py
src/transformers/models/albert/modeling_tf_albert.py
src/transformers/models/albert/tokenization_albert.py
src/transformers/models/albert/tokenization_albert_fast.py
src/transformers/models/auto/__init__.py
src/transformers/models/auto/configuration_auto.py
src/transformers/models/auto/modeling_auto.py
src/transformers/models/auto/modeling_flax_auto.py
src/transformers/models/auto/modeling_tf_auto.py
src/transformers/models/auto/tokenization_auto.py
src/transformers/models/bart/__init__.py
src/transformers/models/bart/configuration_bart.py
src/transformers/models/bart/convert_bart_original_pytorch_checkpoint_to_pytorch.py
src/transformers/models/bart/modeling_bart.py
src/transformers/models/bart/modeling_tf_bart.py
src/transformers/models/bart/tokenization_bart.py
src/transformers/models/bart/tokenization_bart_fast.py
src/transformers/models/bert/__init__.py
src/transformers/models/bert/configuration_bert.py
src/transformers/models/bert/convert_bert_original_tf2_checkpoint_to_pytorch.py
src/transformers/models/bert/convert_bert_original_tf_checkpoint_to_pytorch.py
src/transformers/models/bert/convert_bert_pytorch_checkpoint_to_original_tf.py
src/transformers/models/bert/modeling_bert.py
src/transformers/models/bert/modeling_flax_bert.py
src/transformers/models/bert/modeling_tf_bert.py
src/transformers/models/bert/tokenization_bert.py
src/transformers/models/bert/tokenization_bert_fast.py
src/transformers/models/bert_generation/__init__.py
src/transformers/models/bert_generation/configuration_bert_generation.py
src/transformers/models/bert_generation/modeling_bert_generation.py
src/transformers/models/bert_generation/tokenization_bert_generation.py
src/transformers/models/bert_japanese/__init__.py
src/transformers/models/bert_japanese/tokenization_bert_japanese.py
src/transformers/models/bertweet/__init__.py
src/transformers/models/bertweet/tokenization_bertweet.py
src/transformers/models/blenderbot/__init__.py
src/transformers/models/blenderbot/configuration_blenderbot.py
src/transformers/models/blenderbot/convert_blenderbot_original_pytorch_checkpoint_to_pytorch.py
src/transformers/models/blenderbot/modeling_blenderbot.py
src/transformers/models/blenderbot/modeling_tf_blenderbot.py
src/transformers/models/blenderbot/tokenization_blenderbot.py
src/transformers/models/camembert/__init__.py
src/transformers/models/camembert/configuration_camembert.py
src/transformers/models/camembert/modeling_camembert.py
src/transformers/models/camembert/modeling_tf_camembert.py
src/transformers/models/camembert/tokenization_camembert.py
src/transformers/models/camembert/tokenization_camembert_fast.py
src/transformers/models/ctrl/__init__.py
src/transformers/models/ctrl/configuration_ctrl.py
src/transformers/models/ctrl/modeling_ctrl.py
src/transformers/models/ctrl/modeling_tf_ctrl.py
src/transformers/models/ctrl/tokenization_ctrl.py
src/transformers/models/deberta/__init__.py
src/transformers/models/deberta/configuration_deberta.py
src/transformers/models/deberta/modeling_deberta.py
src/transformers/models/deberta/tokenization_deberta.py
src/transformers/models/dialogpt/__init__.py
src/transformers/models/dialogpt/convert_dialogpt_original_pytorch_checkpoint_to_pytorch.py
src/transformers/models/distilbert/__init__.py
src/transformers/models/distilbert/configuration_distilbert.py
src/transformers/models/distilbert/modeling_distilbert.py
src/transformers/models/distilbert/modeling_tf_distilbert.py
src/transformers/models/distilbert/tokenization_distilbert.py
src/transformers/models/distilbert/tokenization_distilbert_fast.py
src/transformers/models/dpr/__init__.py
src/transformers/models/dpr/configuration_dpr.py
src/transformers/models/dpr/convert_dpr_original_checkpoint_to_pytorch.py
src/transformers/models/dpr/modeling_dpr.py
src/transformers/models/dpr/modeling_tf_dpr.py
src/transformers/models/dpr/tokenization_dpr.py
src/transformers/models/dpr/tokenization_dpr_fast.py
src/transformers/models/electra/__init__.py
src/transformers/models/electra/configuration_electra.py
src/transformers/models/electra/convert_electra_original_tf_checkpoint_to_pytorch.py
src/transformers/models/electra/modeling_electra.py
src/transformers/models/electra/modeling_tf_electra.py
src/transformers/models/electra/tokenization_electra.py
src/transformers/models/electra/tokenization_electra_fast.py
src/transformers/models/encoder_decoder/__init__.py
src/transformers/models/encoder_decoder/configuration_encoder_decoder.py
src/transformers/models/encoder_decoder/modeling_encoder_decoder.py
src/transformers/models/flaubert/__init__.py
src/transformers/models/flaubert/configuration_flaubert.py
src/transformers/models/flaubert/modeling_flaubert.py
src/transformers/models/flaubert/modeling_tf_flaubert.py
src/transformers/models/flaubert/tokenization_flaubert.py
src/transformers/models/fsmt/__init__.py
src/transformers/models/fsmt/configuration_fsmt.py
src/transformers/models/fsmt/convert_fsmt_original_pytorch_checkpoint_to_pytorch.py
src/transformers/models/fsmt/modeling_fsmt.py
src/transformers/models/fsmt/tokenization_fsmt.py
src/transformers/models/funnel/__init__.py
src/transformers/models/funnel/configuration_funnel.py
src/transformers/models/funnel/convert_funnel_original_tf_checkpoint_to_pytorch.py
src/transformers/models/funnel/modeling_funnel.py
src/transformers/models/funnel/modeling_tf_funnel.py
src/transformers/models/funnel/tokenization_funnel.py
src/transformers/models/funnel/tokenization_funnel_fast.py
src/transformers/models/gpt2/__init__.py
src/transformers/models/gpt2/configuration_gpt2.py
src/transformers/models/gpt2/convert_gpt2_original_tf_checkpoint_to_pytorch.py
src/transformers/models/gpt2/modeling_gpt2.py
src/transformers/models/gpt2/modeling_tf_gpt2.py
src/transformers/models/gpt2/tokenization_gpt2.py
src/transformers/models/gpt2/tokenization_gpt2_fast.py
src/transformers/models/herbert/__init__.py
src/transformers/models/herbert/tokenization_herbert.py
src/transformers/models/herbert/tokenization_herbert_fast.py
src/transformers/models/layoutlm/__init__.py
src/transformers/models/layoutlm/configuration_layoutlm.py
src/transformers/models/layoutlm/modeling_layoutlm.py
src/transformers/models/layoutlm/tokenization_layoutlm.py
src/transformers/models/layoutlm/tokenization_layoutlm_fast.py
src/transformers/models/longformer/__init__.py
src/transformers/models/longformer/configuration_longformer.py
src/transformers/models/longformer/convert_longformer_original_pytorch_lightning_to_pytorch.py
src/transformers/models/longformer/modeling_longformer.py
src/transformers/models/longformer/modeling_tf_longformer.py
src/transformers/models/longformer/tokenization_longformer.py
src/transformers/models/longformer/tokenization_longformer_fast.py
src/transformers/models/lxmert/__init__.py
src/transformers/models/lxmert/configuration_lxmert.py
src/transformers/models/lxmert/convert_lxmert_original_tf_checkpoint_to_pytorch.py
src/transformers/models/lxmert/modeling_lxmert.py
src/transformers/models/lxmert/modeling_tf_lxmert.py
src/transformers/models/lxmert/tokenization_lxmert.py
src/transformers/models/lxmert/tokenization_lxmert_fast.py
src/transformers/models/marian/__init__.py
src/transformers/models/marian/configuration_marian.py
src/transformers/models/marian/convert_marian_tatoeba_to_pytorch.py
src/transformers/models/marian/convert_marian_to_pytorch.py
src/transformers/models/marian/modeling_marian.py
src/transformers/models/marian/modeling_tf_marian.py
src/transformers/models/marian/tokenization_marian.py
src/transformers/models/mbart/__init__.py
src/transformers/models/mbart/configuration_mbart.py
src/transformers/models/mbart/convert_mbart_original_checkpoint_to_pytorch.py
src/transformers/models/mbart/modeling_mbart.py
src/transformers/models/mbart/modeling_tf_mbart.py
src/transformers/models/mbart/tokenization_mbart.py
src/transformers/models/mbart/tokenization_mbart_fast.py
src/transformers/models/mmbt/__init__.py
src/transformers/models/mmbt/configuration_mmbt.py
src/transformers/models/mmbt/modeling_mmbt.py
src/transformers/models/mobilebert/__init__.py
src/transformers/models/mobilebert/configuration_mobilebert.py
src/transformers/models/mobilebert/convert_mobilebert_original_tf_checkpoint_to_pytorch.py
src/transformers/models/mobilebert/modeling_mobilebert.py
src/transformers/models/mobilebert/modeling_tf_mobilebert.py
src/transformers/models/mobilebert/tokenization_mobilebert.py
src/transformers/models/mobilebert/tokenization_mobilebert_fast.py
src/transformers/models/openai/__init__.py
src/transformers/models/openai/configuration_openai.py
src/transformers/models/openai/convert_openai_original_tf_checkpoint_to_pytorch.py
src/transformers/models/openai/modeling_openai.py
src/transformers/models/openai/modeling_tf_openai.py
src/transformers/models/openai/tokenization_openai.py
src/transformers/models/openai/tokenization_openai_fast.py
src/transformers/models/pegasus/__init__.py
src/transformers/models/pegasus/configuration_pegasus.py
src/transformers/models/pegasus/convert_pegasus_tf_to_pytorch.py
src/transformers/models/pegasus/modeling_pegasus.py
src/transformers/models/pegasus/modeling_tf_pegasus.py
src/transformers/models/pegasus/tokenization_pegasus.py
src/transformers/models/pegasus/tokenization_pegasus_fast.py
src/transformers/models/phobert/__init__.py
src/transformers/models/phobert/tokenization_phobert.py
src/transformers/models/prophetnet/__init__.py
src/transformers/models/prophetnet/configuration_prophetnet.py
src/transformers/models/prophetnet/convert_prophetnet_original_pytorch_checkpoint_to_pytorch.py
src/transformers/models/prophetnet/modeling_prophetnet.py
src/transformers/models/prophetnet/tokenization_prophetnet.py
src/transformers/models/rag/__init__.py
src/transformers/models/rag/configuration_rag.py
src/transformers/models/rag/modeling_rag.py
src/transformers/models/rag/retrieval_rag.py
src/transformers/models/rag/tokenization_rag.py
src/transformers/models/reformer/__init__.py
src/transformers/models/reformer/configuration_reformer.py
src/transformers/models/reformer/convert_reformer_trax_checkpoint_to_pytorch.py
src/transformers/models/reformer/modeling_reformer.py
src/transformers/models/reformer/tokenization_reformer.py
src/transformers/models/reformer/tokenization_reformer_fast.py
src/transformers/models/retribert/__init__.py
src/transformers/models/retribert/configuration_retribert.py
src/transformers/models/retribert/modeling_retribert.py
src/transformers/models/retribert/tokenization_retribert.py
src/transformers/models/retribert/tokenization_retribert_fast.py
src/transformers/models/roberta/__init__.py
src/transformers/models/roberta/configuration_roberta.py
src/transformers/models/roberta/convert_roberta_original_pytorch_checkpoint_to_pytorch.py
src/transformers/models/roberta/modeling_flax_roberta.py
src/transformers/models/roberta/modeling_roberta.py
src/transformers/models/roberta/modeling_tf_roberta.py
src/transformers/models/roberta/tokenization_roberta.py
src/transformers/models/roberta/tokenization_roberta_fast.py
src/transformers/models/squeezebert/__init__.py
src/transformers/models/squeezebert/configuration_squeezebert.py
src/transformers/models/squeezebert/modeling_squeezebert.py
src/transformers/models/squeezebert/tokenization_squeezebert.py
src/transformers/models/squeezebert/tokenization_squeezebert_fast.py
src/transformers/models/t5/__init__.py
src/transformers/models/t5/configuration_t5.py
src/transformers/models/t5/convert_t5_original_tf_checkpoint_to_pytorch.py
src/transformers/models/t5/modeling_t5.py
src/transformers/models/t5/modeling_tf_t5.py
src/transformers/models/t5/tokenization_t5.py
src/transformers/models/t5/tokenization_t5_fast.py
src/transformers/models/transfo_xl/__init__.py
src/transformers/models/transfo_xl/configuration_transfo_xl.py
src/transformers/models/transfo_xl/convert_transfo_xl_original_tf_checkpoint_to_pytorch.py
src/transformers/models/transfo_xl/modeling_tf_transfo_xl.py
src/transformers/models/transfo_xl/modeling_tf_transfo_xl_utilities.py
src/transformers/models/transfo_xl/modeling_transfo_xl.py
src/transformers/models/transfo_xl/modeling_transfo_xl_utilities.py
src/transformers/models/transfo_xl/tokenization_transfo_xl.py
src/transformers/models/xlm/__init__.py
src/transformers/models/xlm/configuration_xlm.py
src/transformers/models/xlm/convert_xlm_original_pytorch_checkpoint_to_pytorch.py
src/transformers/models/xlm/modeling_tf_xlm.py
src/transformers/models/xlm/modeling_xlm.py
src/transformers/models/xlm/tokenization_xlm.py
src/transformers/models/xlm_prophetnet/__init__.py
src/transformers/models/xlm_prophetnet/configuration_xlm_prophetnet.py
src/transformers/models/xlm_prophetnet/modeling_xlm_prophetnet.py
src/transformers/models/xlm_prophetnet/tokenization_xlm_prophetnet.py
src/transformers/models/xlm_roberta/__init__.py
src/transformers/models/xlm_roberta/configuration_xlm_roberta.py
src/transformers/models/xlm_roberta/modeling_tf_xlm_roberta.py
src/transformers/models/xlm_roberta/modeling_xlm_roberta.py
src/transformers/models/xlm_roberta/tokenization_xlm_roberta.py
src/transformers/models/xlm_roberta/tokenization_xlm_roberta_fast.py
src/transformers/models/xlnet/__init__.py
src/transformers/models/xlnet/configuration_xlnet.py
src/transformers/models/xlnet/convert_xlnet_original_tf_checkpoint_to_pytorch.py
src/transformers/models/xlnet/modeling_tf_xlnet.py
src/transformers/models/xlnet/modeling_xlnet.py
src/transformers/models/xlnet/tokenization_xlnet.py
src/transformers/models/xlnet/tokenization_xlnet_fast.py
src/transformers/pipelines.py
src/transformers/trainer.py
src/transformers/utils/dummy_pt_objects.py
src/transformers/utils/dummy_tf_objects.py
src/transformers/utils/dummy_tokenizers_objects.py
templates/adding_a_new_model/README.md
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_tf_{{cookiecutter.lowercase_modelname}}.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_{{cookiecutter.lowercase_modelname}}.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/test_modeling_tf_{{cookiecutter.lowercase_modelname}}.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/test_modeling_{{cookiecutter.lowercase_modelname}}.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/to_replace_{{cookiecutter.lowercase_modelname}}.py
tests/test_configuration_auto.py
tests/test_flax_auto.py
tests/test_logging.py
tests/test_modeling_albert.py
tests/test_modeling_auto.py
tests/test_modeling_bart.py
tests/test_modeling_bert.py
tests/test_modeling_deberta.py
tests/test_modeling_dpr.py
tests/test_modeling_electra.py
tests/test_modeling_flaubert.py
tests/test_modeling_flax_bert.py
tests/test_modeling_flax_roberta.py
tests/test_modeling_fsmt.py
tests/test_modeling_lxmert.py
tests/test_modeling_marian.py
tests/test_modeling_pegasus.py
tests/test_modeling_rag.py
tests/test_modeling_roberta.py
tests/test_modeling_t5.py
tests/test_modeling_tf_albert.py
tests/test_modeling_tf_auto.py
tests/test_modeling_tf_bart.py
tests/test_modeling_tf_bert.py
tests/test_modeling_tf_ctrl.py
tests/test_modeling_tf_distilbert.py
tests/test_modeling_tf_electra.py
tests/test_modeling_tf_funnel.py
tests/test_modeling_tf_gpt2.py
tests/test_modeling_tf_lxmert.py
tests/test_modeling_tf_mobilebert.py
tests/test_modeling_tf_openai.py
tests/test_modeling_tf_pytorch.py
tests/test_modeling_tf_roberta.py
tests/test_modeling_tf_xlnet.py
tests/test_modeling_transfo_xl.py
tests/test_modeling_xlm.py
tests/test_modeling_xlnet.py
tests/test_retrieval_rag.py
tests/test_tokenization_auto.py
tests/test_tokenization_bart.py
tests/test_tokenization_bert.py
tests/test_tokenization_bert_japanese.py
tests/test_tokenization_bertweet.py
tests/test_tokenization_blenderbot.py
tests/test_tokenization_ctrl.py
tests/test_tokenization_deberta.py
tests/test_tokenization_fsmt.py
tests/test_tokenization_funnel.py
tests/test_tokenization_gpt2.py
tests/test_tokenization_herbert.py
tests/test_tokenization_layoutlm.py
tests/test_tokenization_lxmert.py
tests/test_tokenization_marian.py
tests/test_tokenization_mbart.py
tests/test_tokenization_openai.py
tests/test_tokenization_phobert.py
tests/test_tokenization_prophetnet.py
tests/test_tokenization_rag.py
tests/test_tokenization_roberta.py
tests/test_tokenization_transfo_xl.py
tests/test_tokenization_utils.py
tests/test_tokenization_xlm.py
tests/test_tokenization_xlm_prophetnet.py
tests/test_utils_check_copies.py
utils/check_repo.py
==================
901507335;Julien Plu;2020-11-16 20:44:19 +0100;Fix mixed precision issue for GPT2 (#8572)
* Fix mixed precision issue for GPT2

* Forgot one cast

* oops

* Forgotten casts
==

src/transformers/modeling_tf_gpt2.py
==================
1073a2bde;Sylvain Gugger;2020-11-16 11:43:00 -0500;Switch `return_dict` to `True` by default. (#8530)
* Use the CI to identify failing tests

* Remove from all examples and tests

* More default switch

* Fixes

* More test fixes

* More fixes

* Last fixes hopefully

* Use the CI to identify failing tests

* Remove from all examples and tests

* More default switch

* Fixes

* More test fixes

* More fixes

* Last fixes hopefully

* Run on the real suite

* Fix slow tests
==

docs/source/model_doc/bertgeneration.rst
docs/source/model_doc/t5.rst
docs/source/task_summary.rst
docs/source/training.rst
examples/lxmert/demo.ipynb
examples/question-answering/run_squad.py
examples/rag/eval_rag.py
examples/rag/finetune.py
examples/rag/use_own_knowledge_dataset.py
examples/seq2seq/distillation.py
examples/seq2seq/test_seq2seq_examples.py
model_cards/microsoft/prophetnet-large-uncased/README.md
model_cards/microsoft/xprophetnet-large-wiki100-cased/README.md
model_cards/mrm8488/codebert-base-finetuned-detect-insecure-code/README.md
model_cards/sentence-transformers/LaBSE/README.md
scripts/fsmt/fsmt-make-super-tiny-model.py
scripts/fsmt/fsmt-make-tiny-model.py
src/transformers/configuration_utils.py
src/transformers/file_utils.py
src/transformers/modeling_albert.py
src/transformers/modeling_bart.py
src/transformers/modeling_bert.py
src/transformers/modeling_bert_generation.py
src/transformers/modeling_deberta.py
src/transformers/modeling_distilbert.py
src/transformers/modeling_dpr.py
src/transformers/modeling_electra.py
src/transformers/modeling_encoder_decoder.py
src/transformers/modeling_fsmt.py
src/transformers/modeling_funnel.py
src/transformers/modeling_gpt2.py
src/transformers/modeling_layoutlm.py
src/transformers/modeling_longformer.py
src/transformers/modeling_mobilebert.py
src/transformers/modeling_openai.py
src/transformers/modeling_prophetnet.py
src/transformers/modeling_rag.py
src/transformers/modeling_roberta.py
src/transformers/modeling_squeezebert.py
src/transformers/modeling_t5.py
src/transformers/modeling_tf_bart.py
src/transformers/modeling_tf_dpr.py
src/transformers/modeling_tf_funnel.py
src/transformers/modeling_tf_lxmert.py
src/transformers/modeling_tf_t5.py
src/transformers/modeling_xlm.py
src/transformers/modeling_xlm_prophetnet.py
src/transformers/modeling_xlnet.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/test_modeling_{{cookiecutter.lowercase_modelname}}.py
tests/test_generation_utils.py
tests/test_modeling_albert.py
tests/test_modeling_bart.py
tests/test_modeling_bert.py
tests/test_modeling_bert_generation.py
tests/test_modeling_camembert.py
tests/test_modeling_common.py
tests/test_modeling_ctrl.py
tests/test_modeling_deberta.py
tests/test_modeling_distilbert.py
tests/test_modeling_dpr.py
tests/test_modeling_electra.py
tests/test_modeling_encoder_decoder.py
tests/test_modeling_flaubert.py
tests/test_modeling_flax_bert.py
tests/test_modeling_flax_roberta.py
tests/test_modeling_fsmt.py
tests/test_modeling_funnel.py
tests/test_modeling_gpt2.py
tests/test_modeling_layoutlm.py
tests/test_modeling_longformer.py
tests/test_modeling_lxmert.py
tests/test_modeling_marian.py
tests/test_modeling_mbart.py
tests/test_modeling_mobilebert.py
tests/test_modeling_openai.py
tests/test_modeling_pegasus.py
tests/test_modeling_prophetnet.py
tests/test_modeling_reformer.py
tests/test_modeling_roberta.py
tests/test_modeling_squeezebert.py
tests/test_modeling_t5.py
tests/test_modeling_tf_albert.py
tests/test_modeling_tf_bart.py
tests/test_modeling_tf_bert.py
tests/test_modeling_tf_camembert.py
tests/test_modeling_tf_common.py
tests/test_modeling_tf_ctrl.py
tests/test_modeling_tf_distilbert.py
tests/test_modeling_tf_electra.py
tests/test_modeling_tf_flaubert.py
tests/test_modeling_tf_funnel.py
tests/test_modeling_tf_gpt2.py
tests/test_modeling_tf_longformer.py
tests/test_modeling_tf_lxmert.py
tests/test_modeling_tf_mobilebert.py
tests/test_modeling_tf_openai.py
tests/test_modeling_tf_roberta.py
tests/test_modeling_tf_t5.py
tests/test_modeling_tf_transfo_xl.py
tests/test_modeling_tf_xlm.py
tests/test_modeling_tf_xlm_roberta.py
tests/test_modeling_tf_xlnet.py
tests/test_modeling_transfo_xl.py
tests/test_modeling_xlm.py
tests/test_modeling_xlm_roberta.py
tests/test_modeling_xlnet.py
==================
0d0a0785f;Sylvain Gugger;2020-11-16 10:21:19 -0500;Update version to v4.0.0-dev (#8568)

==

setup.py
src/transformers/__init__.py
==================
afb50c663;LSinev;2020-11-16 16:35:44 +0300;Fix GPT2DoubleHeadsModel to work with model.generate() (#6601)
* Fix passing token_type_ids during GPT2DoubleHeadsModel.generate() if used

and for GPT2LMHeadModel too

* Update tests to check token_type_ids usage in GPT2 models
==

src/transformers/generation_utils.py
src/transformers/modeling_gpt2.py
tests/test_modeling_gpt2.py
==================
04d8136bd;Yusuke Mori;2020-11-16 22:18:25 +0900;Adding the prepare_seq2seq_batch function to ProphetNet (#8515)
* Simply insert T5Tokenizer's prepare_seq2seq_batch

* Update/Add some 'import'

* fix RunTimeError caused by '.view'

* Moves .view related error avoidance from seq2seq_trainer to inside prophetnet

* Update test_tokenization_prophetnet.py

* Format the test code with black

* Re-format the test code

* Update test_tokenization_prophetnet.py

* Add importing require_torch in the test code

* Add importing BatchEncoding in the test code

* Re-format the test code on Colab
==

src/transformers/modeling_prophetnet.py
src/transformers/tokenization_prophetnet.py
tests/test_tokenization_prophetnet.py
==================
931b10978;Stas Bekman;2020-11-16 05:05:30 -0800;[doc] typo fix (#8535)
* [doc] typo fix

@sgugger

* Update src/transformers/modeling_utils.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

src/transformers/modeling_utils.py
==================
6db21a06a;Branden Chan;2020-11-16 12:59:10 +0100;Clearer Model Versioning Example (#8562)

==

model_cards/deepset/roberta-base-squad2/README.md
==================
daaa68451;Mehrdad Farahani;2020-11-16 13:34:46 +0330;Readme for Wiki Summary [Persian] bert2bert (#8558)

==

model_cards/m3hrdadfi/bert2bert-fa-wiki-summary/README.md
==================
06d468d3f;Mehrdad Farahani;2020-11-16 13:34:38 +0330;Readme for News Headline Generation (bert2bert) (#8557)

==

model_cards/m3hrdadfi/bert2bert-fa-news-headline/README.md
==================
9b7fb8a36;zhezhaoa;2020-11-16 04:01:28 -0600;Create README.md for Chinese RoBERTa Miniatures (#8550)
* Create README.md

* Update model_cards/uer/chinese_roberta_L-2_H-128/README.md

Co-authored-by: Julien Chaumond <chaumond@gmail.com>
==

model_cards/uer/chinese_roberta_L-2_H-128/README.md
==================
f4e04cd2c;Thomas Wolf;2020-11-15 22:50:59 +0100;[breaking|pipelines|tokenizers] Adding slow-fast tokenizers equivalence tests pipelines - Removing sentencepiece as a required dependency (#8073)
* Fixing roberta for slow-fast tests

* WIP getting equivalence on pipelines

* slow-to-fast equivalence - working on question-answering pipeline

* optional FAISS tests

* Pipeline Q&A

* Move pipeline tests to their own test job again

* update tokenizer to add sequence id methods

* update to tokenizers 0.9.4

* set sentencepiecce as optional

* clean up squad

* clean up pipelines to use sequence_ids

* style/quality

* wording

* Switch to use_fast = True by default

* update tests for use_fast at True by default

* fix rag tokenizer test

* removing protobuf from required dependencies

* fix NER test for use_fast = True by default

* fixing example tests (Q&A examples use slow tokenizers for now)

* protobuf in main deps extras["sentencepiece"] and example deps

* fix protobug install test

* try to fix seq2seq by switching to slow tokenizers for now

* Update src/transformers/tokenization_utils_base.py

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* Update src/transformers/tokenization_utils_base.py

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
==

examples/question-answering/run_squad.py
examples/question-answering/run_squad_trainer.py
examples/requirements.txt
examples/seq2seq/test_datasets.py
setup.py
src/transformers/convert_slow_tokenizer.py
src/transformers/data/processors/squad.py
src/transformers/file_utils.py
src/transformers/pipelines.py
src/transformers/tokenization_auto.py
src/transformers/tokenization_roberta_fast.py
src/transformers/tokenization_utils_base.py
src/transformers/tokenization_utils_fast.py
tests/test_pipelines_common.py
tests/test_pipelines_dialog.py
tests/test_pipelines_ner.py
tests/test_pipelines_question_answering.py
tests/test_pipelines_zero_shot.py
tests/test_retrieval_rag.py
tests/test_tokenization_auto.py
tests/test_tokenization_common.py
tests/test_tokenization_rag.py
tests/test_tokenization_xlm_prophetnet.py
==================
24184e73c;Julien Plu;2020-11-13 23:07:17 +0100;Rework some TF tests (#8492)
* Update some tests

* Small update

* Apply style

* Use max_position_embeddings

* Create a fake attribute

* Create a fake attribute

* Update wrong name

* Wrong TransfoXL model file

* Keep the common tests agnostic
==

tests/test_modeling_tf_common.py
tests/test_modeling_tf_longformer.py
==================
f6cdafdec;Patrick von Platen;2020-11-13 20:31:40 +0100;fix load weights (#8528)
* fix load weights

* delete line
==

src/transformers/modeling_t5.py
==================
f6f4da8dd;Joe Davison;2020-11-13 14:07:25 -0500;Add bart-large-mnli model card (#8527)

==

model_cards/facebook/bart-large-mnli/README.md
==================
725269746;Julien Chaumond;2020-11-13 18:10:26 +0100;Model sharing doc: more tweaks (#8520)
* More doc tweaks

* Update model_sharing.rst

* make style

* missing newline

* Add email tip

Co-authored-by: Pierric Cistac <pierric@huggingface.co>
==

docs/source/model_sharing.rst
==================
9d519dabb;LysandreJik;2020-11-13 12:04:17 -0500;Fix paths in github YAML

==

.github/workflows/self-push.yml
==================
826f04576;Lysandre Debut;2020-11-13 11:59:30 -0500;Model templates encoder only (#8509)
* Model templates

* TensorFlow

* Remove pooler

* CI

* Tokenizer + Refactoring

* Encoder-Decoder

* Let's go testing

* Encoder-Decoder in TF

* Let's go testing in TF

* Documentation

* README

* Fixes

* Better names

* Style

* Update docs

* Choose to skip either TF or PT

* Code quality fixes

* Add to testing suite

* Update file path

* Cookiecutter path

* Update `transformers` path

* Handle rebasing

* Remove seq2seq from model templates

* Remove s2s config

* Apply Sylvain and Patrick comments

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Last fixes from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

.github/workflows/self-push.yml
setup.py
src/transformers/commands/add_new_model.py
src/transformers/commands/transformers_cli.py
src/transformers/configuration_auto.py
src/transformers/modeling_auto.py
src/transformers/modeling_tf_auto.py
templates/adding_a_new_model/README.md
templates/adding_a_new_model/configuration_xxx.py
templates/adding_a_new_model/convert_xxx_original_tf_checkpoint_to_pytorch.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/configuration.json
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/configuration_{{cookiecutter.lowercase_modelname}}.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_tf_{{cookiecutter.lowercase_modelname}}.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/modeling_{{cookiecutter.lowercase_modelname}}.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/test_modeling_tf_{{cookiecutter.lowercase_modelname}}.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/test_modeling_{{cookiecutter.lowercase_modelname}}.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/to_replace_{{cookiecutter.lowercase_modelname}}.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/tokenization_{{cookiecutter.lowercase_modelname}}.py
templates/adding_a_new_model/cookiecutter-template-{{cookiecutter.modelname}}/{{cookiecutter.lowercase_modelname}}.rst
templates/adding_a_new_model/cookiecutter.json
templates/adding_a_new_model/modeling_xxx.py
templates/adding_a_new_model/tests/encoder-bert-tokenizer.json
templates/adding_a_new_model/tests/pt-encoder-bert-tokenizer.json
templates/adding_a_new_model/tests/standalone.json
templates/adding_a_new_model/tests/test_modeling_tf_xxx.py
templates/adding_a_new_model/tests/test_tokenization_xxx.py
templates/adding_a_new_model/tests/tf-encoder-bert-tokenizer.json
templates/adding_a_new_model/tokenization_xxx.py
utils/check_repo.py
==================
42e2d02e4;Patrick von Platen;2020-11-13 16:57:31 +0100;[T5] Bug correction & Refactor (#8518)
* fix bug

* T5 refactor

* refactor tf

* apply sylvains suggestions
==

src/transformers/configuration_t5.py
src/transformers/convert_t5_original_tf_checkpoint_to_pytorch.py
src/transformers/modeling_t5.py
src/transformers/modeling_tf_t5.py
tests/test_modeling_common.py
tests/test_modeling_t5.py
tests/test_modeling_tf_t5.py
==================
42f63e387;Sylvain Gugger;2020-11-13 10:30:04 -0500;Merge remote-tracking branch 'origin/master'

==
==================
bb03a14ed;Sylvain Gugger;2020-11-13 10:29:58 -0500;Update doc for v3.5.1

==

.circleci/deploy.sh
docs/source/_static/js/custom.js
==================
4df6b5931;Branden Chan;2020-11-13 15:58:27 +0100;Update deepset/roberta-base-squad2 model card (#8522)
* Update README.md

* Update README.md
==

model_cards/deepset/roberta-base-squad2/README.md
==================
0c9bae093;Sylvain Gugger;2020-11-12 22:39:57 -0500;Remove typo

==

src/transformers/data/metrics/__init__.py
==================
5d8053948;Julien Plu;2020-11-12 20:08:26 +0100;Add pretraining loss computation for TF Bert pretraining (#8470)
* Add pretraining loss computation for TF Bert pretraining

* Fix labels creation

* Fix T5 model

* restore T5 kwargs

* try a generic fix for pretraining models

* Apply style

* Overide the prepare method for the BERT tests
==

src/transformers/modeling_tf_bert.py
tests/test_modeling_tf_bert.py
tests/test_modeling_tf_common.py
==================
91a67b750;Julien Plu;2020-11-12 19:52:40 +0100;Use LF instead of os.linesep (#8491)

==

utils/check_copies.py
utils/check_dummies.py
utils/check_repo.py
utils/style_doc.py
==================
27b3ff316;Julien Plu;2020-11-12 19:43:00 +0100;Try to understand and apply Sylvain's comments (#8458)

==

examples/adversarial/run_hans.py
examples/bert-loses-patience/run_glue_with_pabee.py
examples/bertology/run_bertology.py
examples/contrib/legacy/run_language_modeling.py
examples/contrib/mm-imdb/run_mmimdb.py
examples/contrib/run_swag.py
examples/deebert/run_glue_deebert.py
examples/distillation/run_squad_w_distillation.py
examples/language-modeling/run_clm.py
examples/language-modeling/run_mlm.py
examples/language-modeling/run_mlm_wwm.py
examples/language-modeling/run_plm.py
examples/multiple-choice/run_multiple_choice.py
examples/multiple-choice/run_tf_multiple_choice.py
examples/question-answering/run_squad.py
examples/question-answering/run_squad_trainer.py
examples/question-answering/run_tf_squad.py
examples/seq2seq/finetune_trainer.py
examples/text-classification/run_glue.py
examples/text-classification/run_tf_glue.py
examples/text-classification/run_tf_text_classification.py
examples/text-classification/run_xnli.py
examples/token-classification/run_ner.py
examples/token-classification/run_ner_old.py
examples/token-classification/run_tf_ner.py
==================
0fa034988;Forrest Iandola;2020-11-12 09:19:37 -0800;fix SqueezeBertForMaskedLM (#8479)

==

model_cards/squeezebert/squeezebert-uncased/README.md
src/transformers/modeling_squeezebert.py
==================
793305463;Sylvain Gugger;2020-11-12 11:53:23 -0500;Model sharing doc (#8498)
* Model sharing doc

* Style
==

docs/source/model_doc/marian.rst
docs/source/model_sharing.rst
==================
d65e0bfea;Chengxi Guo;2020-11-13 00:47:23 +0800;Fix doc bug (#8500)
* fix doc bug

Signed-off-by: mymusise <mymusise1@gmail.com>

* fix example bug

Signed-off-by: mymusise <mymusise1@gmail.com>
==

docs/source/main_classes/trainer.rst
==================
924c624a4;zeyuyun1;2020-11-12 06:47:08 -0800;quick fix on concatenating text to support more datasets (#8474)

==

examples/language-modeling/run_clm.py
examples/language-modeling/run_mlm.py
examples/language-modeling/run_plm.py
==================
17b1fd804;Antonio Lanza;2020-11-12 11:29:37 +0100;Fix typo in roberta-base-squad2-v2 model card (#8489)

==

model_cards/deepset/roberta-base-squad2-v2/README.md
==================
c6c08ebf6;Julien Chaumond;2020-11-12 10:45:29 +0100;[model_cards] other chars than [\w\-_] not allowed anymore in model names
cc @Pierrci

==

model_cards/aliosm/ai-soco-cpp-roberta-small-clas/README.md
model_cards/aliosm/ai-soco-cpp-roberta-small/README.md
model_cards/aliosm/ai-soco-cpp-roberta-tiny-96-clas/README.md
model_cards/aliosm/ai-soco-cpp-roberta-tiny-96/README.md
model_cards/aliosm/ai-soco-cpp-roberta-tiny-clas/README.md
model_cards/aliosm/ai-soco-cpp-roberta-tiny/README.md
==================
121c24efa;Funtowicz Morgan;2020-11-12 00:31:41 +0100;Update deploy-docs dependencies on CI to enable Flax (#8475)
* Update deploy-docs dependencies on CI to enable Flax

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Added pair of ""

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>
==

.circleci/config.yml
==================
81ebd7067;Sumithra Bhakthavatsalam;2020-11-11 14:58:45 -0800;[s2s] distill t5-large -> t5-small (#8376)
Co-authored-by: Sam Shleifer <sshleifer@gmail.com>
==

examples/seq2seq/README.md
examples/seq2seq/distillation.py
examples/seq2seq/test_bash_script.py
examples/seq2seq/test_seq2seq_examples.py
==================
a5b682329;Funtowicz Morgan;2020-11-11 20:53:36 +0100;Flax/Jax documentation (#8331)
* First addition of Flax/Jax documentation

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* make style

* Ensure input order match between Bert & Roberta

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Install dependencies "all" when building doc

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* wraps build_doc deps with ""

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Addressing @sgugger comments.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Use list to highlight JAX features.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Make style.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Let's not look to much into the future for now.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Style

Co-authored-by: Lysandre <lysandre.debut@reseau.eseo.fr>
==

.circleci/config.yml
docs/source/model_doc/bert.rst
docs/source/model_doc/roberta.rst
src/transformers/modeling_flax_bert.py
src/transformers/modeling_flax_roberta.py
==================
c7b6bbec5;Lysandre;2020-11-11 12:58:59 -0500;Skip test until investigation

==

tests/test_hf_api.py
==================
aa2a2c657;Beomsoo Kim;2020-11-12 02:29:57 +0900;Replaced some iadd operations on lists with proper list methods. (#8433)

==

src/transformers/tokenization_utils.py
==================
026a2ff22;Ratthachat (Jung);2020-11-12 00:28:09 +0700;Add TFDPR (#8203)
* Create modeling_tf_dpr.py

* Add TFDPR

* Add back TFPegasus, TFMarian, TFMBart, TFBlenderBot

last commit accidentally deleted these 4 lines, so I recover them back

* Add TFDPR

* Add TFDPR

* clean up some comments, add TF input-style doc string

* Add TFDPR

* Make return_dict=False as default

* Fix return_dict bug (in .from_pretrained)

* Add get_input_embeddings()

* Create test_modeling_tf_dpr.py

The current version is already passed all 27 tests!
Please see the test run at : 
https://colab.research.google.com/drive/1czS_m9zy5k-iSJbzA_DP1k1xAAC_sdkf?usp=sharing

* fix quality

* delete init weights

* run fix copies

* fix repo consis

* del config_class, load_tf_weights

They shoud be 'pytorch only'

* add config_class back

after removing it, test failed ... so totally only removing "use_tf_weights = None" on Lysandre suggestion

* newline after .. note::

* import tf, np (Necessary for ModelIntegrationTest)

* slow_test from_pretrained with from_pt=True

At the moment we don't have TF weights (since we don't have official official TF model)
Previously, I did not run slow test, so I missed this bug

* Add simple TFDPRModelIntegrationTest

Note that this is just a test that TF and Pytorch gives approx. the same output.
However, I could not test with the official DPR repo's output yet

* upload correct tf model

* remove position_ids as missing keys

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
Co-authored-by: patrickvonplaten <patrick@huggingface.co>
==

docs/source/model_doc/dpr.rst
src/transformers/__init__.py
src/transformers/convert_pytorch_checkpoint_to_tf2.py
src/transformers/modeling_tf_auto.py
src/transformers/modeling_tf_dpr.py
src/transformers/utils/dummy_pt_objects.py
src/transformers/utils/dummy_tf_objects.py
tests/test_modeling_dpr.py
tests/test_modeling_tf_dpr.py
utils/check_repo.py
==================
a38d1c7c3;sarnoult;2020-11-11 16:28:23 +0100;Example NER script predicts on tokenized dataset (#8468)
The new run_ner.py script tries to run prediction on the input
test set `datasets["test"]`, but it should be the tokenized set
`tokenized_datasets["test"]`
==

examples/token-classification/run_ner.py
==================
069b63844;Julien Plu;2020-11-11 15:41:39 +0100;Fix next sentence output (#8466)

==

src/transformers/modeling_tf_outputs.py
==================
da842e4e7;Julien Plu;2020-11-11 15:02:06 +0100;Add next sentence prediction loss computation (#8462)
* Add next sentence prediction loss computation

* Apply style

* Fix tests

* Add forgotten import

* Add forgotten import

* Use a new parameter

* Remove kwargs and use positional arguments
==

src/transformers/modeling_tf_bert.py
src/transformers/modeling_tf_mobilebert.py
src/transformers/modeling_tf_outputs.py
src/transformers/modeling_tf_utils.py
tests/test_modeling_tf_common.py
==================
23290836c;Julien Plu;2020-11-11 12:54:15 +0100;Fix TF Longformer (#8460)

==

src/transformers/modeling_tf_longformer.py
==================
8dda9167d;Julien Chaumond;2020-11-11 12:42:50 +0100;[model_cards] harmonization

==

model_cards/deepset/gbert-large/README.md
model_cards/deepset/gelectra-large-generator/README.md
model_cards/deepset/gelectra-large/README.md
model_cards/digitalepidemiologylab/covid-twitter-bert/README.md
model_cards/pdelobelle/robbert-v2-dutch-base/README.md
model_cards/sagorsarker/codeswitch-hineng-lid-lince/README.md
model_cards/sagorsarker/codeswitch-hineng-ner-lince/README.md
model_cards/sagorsarker/codeswitch-hineng-pos-lince/README.md
model_cards/sagorsarker/codeswitch-nepeng-lid-lince/README.md
model_cards/sagorsarker/codeswitch-spaeng-lid-lince/README.md
model_cards/sagorsarker/codeswitch-spaeng-ner-lince/README.md
model_cards/sagorsarker/codeswitch-spaeng-pos-lince/README.md
model_cards/sagorsarker/codeswitch-spaeng-sentiment-analysis-lince/README.md
model_cards/valhalla/t5-base-e2e-qg/README.md
model_cards/valhalla/t5-base-qa-qg-hl/README.md
model_cards/valhalla/t5-base-qg-hl/README.md
model_cards/valhalla/t5-samll-qg-prepend/README.md
model_cards/valhalla/t5-small-e2e-qg/README.md
model_cards/valhalla/t5-small-qa-qg-hl/README.md
model_cards/valhalla/t5-small-qg-hl/README.md
==================
eb3bd73ce;Pedro;2020-11-10 15:33:11 -0500;Bug fix for modeling utilities function: apply_chunking_to_forward, chunking should be in the chunking dimension, an exception was raised if the complete shape of the inputs was not the same rather than only the chunking dimension (#8391)
Co-authored-by: pedro <pe25171@mit.edu>
==

src/transformers/modeling_utils.py
==================
70708cca1;Patrick von Platen;2020-11-10 20:21:54 +0100;fix t5 token type ids (#8437)

==

src/transformers/tokenization_t5.py
src/transformers/tokenization_t5_fast.py
tests/test_tokenization_t5.py
==================
9fd1f5623;Lysandre Debut;2020-11-10 14:02:33 -0500;[No merge] TF integration testing (#7621)
* stash

* TF Integration testing for ELECTRA, BERT, Longformer

* Trigger slow tests

* Apply suggestions from code review
==

src/transformers/modeling_electra.py
src/transformers/modeling_tf_electra.py
tests/test_modeling_tf_bert.py
tests/test_modeling_tf_electra.py
tests/test_modeling_tf_longformer.py
==================
8fe6629bb;Santiago Castro;2020-11-10 13:44:25 -0500;Add missing tasks to `pipeline` docstring (#8428)

==

docs/source/task_summary.rst
model_cards/joeddav/bart-large-mnli-yahoo-answers/README.md
model_cards/joeddav/xlm-roberta-large-xnli/README.md
src/transformers/modeling_tf_utils.py
src/transformers/modeling_utils.py
src/transformers/pipelines.py
==================
02bdfc025;Stas Bekman;2020-11-10 10:23:58 -0800;using multi_gpu consistently (#8446)
* s|multiple_gpu|multi_gpu|g; s|multigpu|multi_gpu|g'

* doc
==

.github/workflows/self-push.yml
.github/workflows/self-scheduled.yml
docs/source/testing.rst
examples/bert-loses-patience/test_run_glue_with_pabee.py
examples/deebert/test_glue_deebert.py
examples/rag/test_distributed_retriever.py
examples/seq2seq/test_bash_script.py
examples/seq2seq/test_datasets.py
examples/seq2seq/test_fsmt_bleu_score.py
examples/seq2seq/test_make_student.py
examples/seq2seq/test_seq2seq_examples.py
examples/seq2seq/test_seq2seq_examples_multi_gpu.py
examples/seq2seq/test_tatoeba_conversion.py
examples/test_examples.py
examples/test_xla_examples.py
examples/token-classification/test_ner_examples.py
src/transformers/testing_utils.py
tests/test_modeling_common.py
tests/test_modeling_rag.py
tests/test_modeling_reformer.py
tests/test_modeling_transfo_xl.py
tests/test_trainer_distributed.py
==================
b93569457;Patrick von Platen;2020-11-10 18:54:17 +0100;fix t5 special tokens (#8435)

==

src/transformers/tokenization_t5.py
tests/test_tokenization_t5.py
==================
cace39af9;Julien Plu;2020-11-10 18:01:32 +0100;Add missing import (#8444)
* Add missing import

* Fix dummy objects
==

src/transformers/__init__.py
src/transformers/utils/dummy_tf_objects.py
==================
e21340da7;Stas Bekman;2020-11-10 08:57:21 -0800;[testing utils] get_auto_remove_tmp_dir more intuitive behavior (#8401)
* [testing utils] get_auto_remove_tmp_dir default change

Now that I have been using `get_auto_remove_tmp_dir default change` for a while, I realized that the defaults aren't most optimal.

99% of the time we want the tmp dir to be empty at the beginning of the test - so changing the default to `before=True` - this shouldn't impact any tests since this feature is used only during debug.

* simplify things

* update docs

* fix doc layout

* style

* Update src/transformers/testing_utils.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* better 3-state doc

* style

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* s/tmp/temporary/ + style

* correct the statement

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

docs/source/testing.rst
src/transformers/testing_utils.py
==================
e7e154989;Julien Plu;2020-11-10 17:19:16 +0100;Windows dev section in the contributing file (#8436)
* Add a Windows dev section in the contributing file.

* Forgotten link

* Trigger CI

* Rework description

* Trigger CI
==

CONTRIBUTING.md
==================
8551a9923;Julien Plu;2020-11-10 17:11:48 +0100;Add auto next sentence prediction (#8432)
* Add auto next sentence prediction

* Fix style

* Add mobilebert next sentence prediction
==

src/transformers/modeling_tf_auto.py
utils/check_repo.py
==================
c314b1fd3;Sam Shleifer;2020-11-10 10:18:34 -0500;[docs] improve bart/marian/mBART/pegasus docs (#8421)

==

docs/source/model_doc/bart.rst
docs/source/model_doc/marian.rst
docs/source/model_doc/mbart.rst
docs/source/model_doc/pegasus.rst
tests/test_modeling_bart.py
==================
3213d3bfa;Sylvain Gugger;2020-11-10 10:07:56 -0500;Question template (#8440)
* Remove SO from question template

* Styling
==

.github/ISSUE_TEMPLATE/question-help.md
==================
5d4972e60;Stas Bekman;2020-11-10 06:33:23 -0800;[examples] better PL version check (#8429)

==

examples/lightning_base.py
==================
ae1cb4ec2;Shichao Sun;2020-11-10 22:32:01 +0800;[s2s/distill] hparams.tokenizer_name = hparams.teacher (#8382)

==

examples/seq2seq/distillation.py
==================
aec51e569;Lysandre;2020-11-10 08:58:47 -0500;v3.5.0 documentation

==

.circleci/deploy.sh
docs/source/_static/js/custom.js
==================
818878dc8;Lysandre;2020-11-10 08:50:24 -0500;Release: v3.5.0

==

docs/source/conf.py
setup.py
src/transformers/__init__.py
==================
9cebee38a;Lysandre Debut;2020-11-10 08:35:11 -0500;Model sharing rst (#8439)
* Update RST

* Finer details

* Re-organize

* Style
==

docs/source/model_sharing.rst
==================
ad2303a40;Julien Chaumond;2020-11-10 14:28:30 +0100;Fix style

==

src/transformers/configuration_openai.py
src/transformers/tokenization_bert_generation.py
src/transformers/tokenization_mobilebert.py
src/transformers/tokenization_mobilebert_fast.py
==================
55e8d0cea;Julien Chaumond;2020-11-10 14:03:29 +0100;Update links from s3 to huggingface.co

==

examples/seq2seq/bertabs/configuration_bertabs.py
src/transformers/configuration_albert.py
src/transformers/configuration_bart.py
src/transformers/configuration_bert.py
src/transformers/configuration_camembert.py
src/transformers/configuration_ctrl.py
src/transformers/configuration_deberta.py
src/transformers/configuration_distilbert.py
src/transformers/configuration_dpr.py
src/transformers/configuration_electra.py
src/transformers/configuration_flaubert.py
src/transformers/configuration_funnel.py
src/transformers/configuration_gpt2.py
src/transformers/configuration_layoutlm.py
src/transformers/configuration_longformer.py
src/transformers/configuration_marian.py
src/transformers/configuration_mbart.py
src/transformers/configuration_mobilebert.py
src/transformers/configuration_openai.py
src/transformers/configuration_prophetnet.py
src/transformers/configuration_retribert.py
src/transformers/configuration_roberta.py
src/transformers/configuration_squeezebert.py
src/transformers/configuration_t5.py
src/transformers/configuration_transfo_xl.py
src/transformers/configuration_xlm.py
src/transformers/configuration_xlm_prophetnet.py
src/transformers/configuration_xlm_roberta.py
src/transformers/configuration_xlnet.py
src/transformers/tokenization_albert.py
src/transformers/tokenization_albert_fast.py
src/transformers/tokenization_bart.py
src/transformers/tokenization_bart_fast.py
src/transformers/tokenization_bert.py
src/transformers/tokenization_bert_fast.py
src/transformers/tokenization_bert_generation.py
src/transformers/tokenization_bert_japanese.py
src/transformers/tokenization_bertweet.py
src/transformers/tokenization_camembert.py
src/transformers/tokenization_camembert_fast.py
src/transformers/tokenization_deberta.py
src/transformers/tokenization_distilbert.py
src/transformers/tokenization_distilbert_fast.py
src/transformers/tokenization_dpr.py
src/transformers/tokenization_dpr_fast.py
src/transformers/tokenization_electra.py
src/transformers/tokenization_electra_fast.py
src/transformers/tokenization_flaubert.py
src/transformers/tokenization_funnel.py
src/transformers/tokenization_funnel_fast.py
src/transformers/tokenization_gpt2.py
src/transformers/tokenization_gpt2_fast.py
src/transformers/tokenization_layoutlm.py
src/transformers/tokenization_layoutlm_fast.py
src/transformers/tokenization_longformer.py
src/transformers/tokenization_longformer_fast.py
src/transformers/tokenization_lxmert.py
src/transformers/tokenization_lxmert_fast.py
src/transformers/tokenization_marian.py
src/transformers/tokenization_mbart.py
src/transformers/tokenization_mbart_fast.py
src/transformers/tokenization_mobilebert.py
src/transformers/tokenization_mobilebert_fast.py
src/transformers/tokenization_openai.py
src/transformers/tokenization_openai_fast.py
src/transformers/tokenization_phobert.py
src/transformers/tokenization_prophetnet.py
src/transformers/tokenization_retribert.py
src/transformers/tokenization_retribert_fast.py
src/transformers/tokenization_roberta.py
src/transformers/tokenization_roberta_fast.py
src/transformers/tokenization_squeezebert.py
src/transformers/tokenization_squeezebert_fast.py
src/transformers/tokenization_t5.py
src/transformers/tokenization_t5_fast.py
src/transformers/tokenization_transfo_xl.py
src/transformers/tokenization_xlm.py
src/transformers/tokenization_xlm_roberta.py
src/transformers/tokenization_xlm_roberta_fast.py
src/transformers/tokenization_xlnet.py
src/transformers/tokenization_xlnet_fast.py
==================
850afb422;Lysandre Debut;2020-11-10 07:29:34 -0500;Patch token classification pipeline (#8364)
* Patch token classification pipeline

* Some added tests for TokenClassificationArgumentHandler (#8366)

Co-authored-by: Nicolas Patry <patry.nicolas@protonmail.com>
==

src/transformers/pipelines.py
tests/test_pipelines_ner.py
==================
70f622fab;Julien Chaumond;2020-11-10 13:11:02 +0100;Model versioning (#8324)
* fix typo

* rm use_cdn & references, and implement new hf_bucket_url

* I'm pretty sure we don't need to `read` this file

* same here

* [BIG] file_utils.networking: do not gobble up errors anymore

* Fix CI üòá

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Tiny doc tweak

* Add doc + pass kwarg everywhere

* Add more tests and explain

cc @sshleifer let me know if better

Co-Authored-By: Sam Shleifer <sshleifer@gmail.com>

* Also implement revision in pipelines

In the case where we're passing a task name or a string model identifier

* Fix CI üòá

* Fix CI

* [hf_api] new methods + command line implem

* make style

* Final endpoints post-migration

* Fix post-migration

* Py3.6 compat

cc @stefan-it

Thank you @stas00

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
Co-authored-by: Sam Shleifer <sshleifer@gmail.com>
==

model_cards/t5-11b-README.md
scripts/fsmt/convert-allenai-wmt16.sh
scripts/fsmt/convert-allenai-wmt19.sh
scripts/fsmt/convert-facebook-wmt19.sh
src/transformers/commands/user.py
src/transformers/configuration_auto.py
src/transformers/configuration_utils.py
src/transformers/file_utils.py
src/transformers/hf_api.py
src/transformers/modelcard.py
src/transformers/modeling_auto.py
src/transformers/modeling_flax_utils.py
src/transformers/modeling_tf_auto.py
src/transformers/modeling_tf_utils.py
src/transformers/modeling_utils.py
src/transformers/pipelines.py
src/transformers/retrieval_rag.py
src/transformers/tokenization_auto.py
src/transformers/tokenization_utils_base.py
tests/test_file_utils.py
tests/test_hf_api.py
tests/test_modeling_tf_bert.py
utils/check_dummies.py
==================
4185b115d;Teven;2020-11-10 02:49:51 +0100;Changing XLNet default from not using memories to 512 context size following paper (#8417)
* Move XLNet memory length FutureWarning

* isort

* style

* Changed default XLNet memory length
==

src/transformers/configuration_xlnet.py
src/transformers/modeling_xlnet.py
==================
190df5856;Stas Bekman;2020-11-09 12:47:38 -0800;[github CI] add a multi-gpu job for all example tests (#8341)
* add a multi-gpu job for all example tests

* run only ported tests

* rename

* explain why env is re-activated on each step

* mark all unported/checked tests with @require_torch_non_multigpu_but_fix_me

* style

* Apply suggestions from code review

Co-authored-by: Sam Shleifer <sshleifer@gmail.com>

Co-authored-by: Sam Shleifer <sshleifer@gmail.com>
==

.github/workflows/self-scheduled.yml
examples/bert-loses-patience/test_run_glue_with_pabee.py
examples/deebert/test_glue_deebert.py
examples/rag/test_distributed_retriever.py
examples/seq2seq/test_bash_script.py
examples/seq2seq/test_datasets.py
examples/seq2seq/test_fsmt_bleu_score.py
examples/seq2seq/test_make_student.py
examples/seq2seq/test_seq2seq_examples.py
examples/seq2seq/test_tatoeba_conversion.py
examples/test_examples.py
examples/test_xla_examples.py
examples/token-classification/test_ner_examples.py
src/transformers/testing_utils.py
==================
a39218b75;Sylvain Gugger;2020-11-09 15:44:54 -0500;Check all models are in an auto class (#8425)

==

src/transformers/modeling_tf_auto.py
utils/check_repo.py
==================
ef032ddd1;Stas Bekman;2020-11-09 11:27:42 -0800;[docs] [testing] gpu decorators table (#8422)
* gpu decorators table

* whitespace

* Update docs/source/testing.rst

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* whitespace

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
==

docs/source/testing.rst
==================
a8339b9ec;Sam Shleifer;2020-11-09 13:25:33 -0500;Fix bart shape comment (#8423)

==

src/transformers/modeling_bart.py
src/transformers/modeling_tf_bart.py
==================
46509d1c1;Sam Shleifer;2020-11-09 12:51:38 -0500;[docs] remove sshleifer from issue-template :( (#8418)

==

.github/ISSUE_TEMPLATE/bug-report.md
.github/PULL_REQUEST_TEMPLATE.md
docs/source/model_doc/bart.rst
docs/source/model_doc/marian.rst
docs/source/model_doc/mbart.rst
docs/source/model_doc/pegasus.rst
==================
9c83b96e6;Patrick von Platen;2020-11-09 18:24:41 +0100;[Tests] Add Common Test for Training + Fix a couple of bugs (#8415)
* add training tests

* correct longformer

* fix docs

* fix some tests

* fix some more train tests

* remove ipdb

* fix multiple edge case model training

* fix funnel and prophetnet

* clean gpt models

* undo renaming of albert
==

docs/source/model_doc/auto.rst
examples/lxmert/modeling_frcnn.py
model_cards/neuralmind/bert-base-portuguese-cased/README.md
src/transformers/__init__.py
src/transformers/modeling_auto.py
src/transformers/modeling_bert.py
src/transformers/modeling_gpt2.py
src/transformers/modeling_longformer.py
src/transformers/modeling_lxmert.py
src/transformers/modeling_mobilebert.py
src/transformers/modeling_openai.py
src/transformers/modeling_tf_pytorch_utils.py
src/transformers/modeling_tf_utils.py
src/transformers/modeling_utils.py
src/transformers/utils/dummy_pt_objects.py
tests/test_modeling_albert.py
tests/test_modeling_bert.py
tests/test_modeling_common.py
tests/test_modeling_dpr.py
tests/test_modeling_electra.py
tests/test_modeling_flaubert.py
tests/test_modeling_funnel.py
tests/test_modeling_gpt2.py
tests/test_modeling_lxmert.py
tests/test_modeling_mobilebert.py
tests/test_modeling_openai.py
tests/test_modeling_prophetnet.py
tests/test_modeling_transfo_xl.py
tests/test_modeling_xlm.py
tests/test_modeling_xlnet.py
==================
52040517b;Sylvain Gugger;2020-11-09 12:10:09 -0500;Deprecate old data/metrics functions (#8420)

==

src/transformers/data/datasets/glue.py
src/transformers/data/datasets/language_modeling.py
src/transformers/data/metrics/__init__.py
src/transformers/data/processors/glue.py
==================
d4d1fbfc5;Stas Bekman;2020-11-09 08:57:42 -0800;[fsmt convert script] fairseq broke chkpt data - fixing that (#8377)
* fairseq broke chkpt data - fixing that

* style

* support older bpecodes filenames - specifically "code" in iwslt14
==

src/transformers/convert_fsmt_original_pytorch_checkpoint_to_pytorch.py
==================
5c766ecb5;Sylvain Gugger;2020-11-09 11:50:51 -0500;Fix typo

==

examples/token-classification/run_pos.sh
==================
908a28894;Sylvain Gugger;2020-11-09 11:39:55 -0500;Add new token classification example (#8340)
* Add new token classification example

* Remove txt file

* Add test

* With actual testing done

* Less warmup is better

* Update examples/token-classification/run_ner_new.py

Co-authored-by: Thomas Wolf <thomwolf@users.noreply.github.com>

* Address review comments

* Fix test

* Make Lysandre happy

* Last touches and rename

* Rename in tests

* Address review comments

* More run_ner -> run_ner_old

Co-authored-by: Thomas Wolf <thomwolf@users.noreply.github.com>
==

examples/README.md
examples/test_examples.py
examples/token-classification/README.md
examples/token-classification/run.sh
examples/token-classification/run_chunk.sh
examples/token-classification/run_ner.py
examples/token-classification/run_ner_old.py
examples/token-classification/run_old.sh
examples/token-classification/run_pos.sh
examples/token-classification/test_ner_examples.py
model_cards/mrm8488/RuPERTa-base-finetuned-ner/README.md
model_cards/mrm8488/RuPERTa-base-finetuned-pos/README.md
model_cards/mrm8488/TinyBERT-spanish-uncased-finetuned-ner/README.md
model_cards/mrm8488/bert-base-german-finetuned-ler/README.md
model_cards/mrm8488/bert-small-finetuned-typo-detection/README.md
model_cards/mrm8488/bert-spanish-cased-finetuned-ner/README.md
model_cards/mrm8488/bert-spanish-cased-finetuned-pos-syntax/README.md
model_cards/mrm8488/bert-spanish-cased-finetuned-pos/README.md
model_cards/mrm8488/distilbert-base-multi-cased-finetuned-typo-detection/README.md
model_cards/savasy/bert-base-turkish-ner-cased/README.md
tests/fixtures/tests_samples/conll/sample.json
==================
c7cb1aa26;Sylvain Gugger;2020-11-09 11:32:10 -0500;Bump tokenizers (#8419)

==

setup.py
==================
78d706f3a;Stas Bekman;2020-11-09 07:41:39 -0800;[fsmt tokenizer] support lowercase tokenizer (#8389)
* support lowercase tokenizer

* fix arg pos
==

src/transformers/convert_fsmt_original_pytorch_checkpoint_to_pytorch.py
src/transformers/tokenization_fsmt.py
tests/test_tokenization_fsmt.py
==================
1e2acd0dc;Shashank Gupta;2020-11-09 20:53:26 +0530;Bug fix for permutation language modelling (#8409)

==

src/transformers/data/data_collator.py
==================
bf8625e70;Philip May;2020-11-09 15:00:59 +0100;add evaluate doc - trainer.evaluate returns 'epoch' from training (#8273)
* add evaluate doc

* fix style with utils/style.doc

* Update src/transformers/trainer.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

src/transformers/trainer.py
==================
ebde57aca;Sam Shleifer;2020-11-09 08:55:22 -0500;examples/docs: caveat that PL examples don't work on TPU (#8309)

==

examples/README.md
==================
76e7a44de;Julien Plu;2020-11-09 13:50:38 +0100;Fix some tooling for windows (#8359)
* Fix some tooling for windows

* Fix conflict

* Trigger CI
==

utils/check_repo.py
==================
507dfb40c;dartrevan;2020-11-09 11:44:43 +0300;Update README.md (#8406)

==

model_cards/cimm-kzn/enrudr-bert/README.md
==================
7247d0b4e;smanjil;2020-11-09 09:43:55 +0100;updating tag for exbert viz (#8408)

==

model_cards/smanjil/German-MedBERT/README.md
==================
4ab5617b0;Stas Bekman;2020-11-09 00:36:06 -0800;comet_ml temporary fix(#8410)

==

src/transformers/integrations.py
==================
e6d9cdaaf;Sam Shleifer;2020-11-08 16:57:43 -0500;[s2s/distill] remove run_distiller.sh, fix xsum script (#8412)

==

examples/seq2seq/run_distiller.sh
examples/seq2seq/train_distilbart_xsum.sh
==================
66582492d;Stas Bekman;2020-11-08 13:45:40 -0800;[s2s test_finetune_trainer] failing multigpu test (#8400)

==

examples/seq2seq/test_finetune_trainer.py
==================
f62755a60;Stas Bekman;2020-11-08 13:44:18 -0800;[s2s examples test] fix data path (#8398)

==

examples/seq2seq/test_seq2seq_examples_multi_gpu.py
==================
4a53e8e9e;Jonathan Chang;2020-11-08 06:53:01 -0800;Fix DataCollatorForWholeWordMask again (#8397)

==

src/transformers/data/data_collator.py
==================
610730998;Manav Rathod;2020-11-08 09:08:14 -0500;fixed default labels for QA model (#8399)

==

src/transformers/trainer.py
==================
0b02489b2;Chengxi Guo;2020-11-08 18:00:19 +0800;Add gpt2-medium-chinese model card (#8402)
* Create README.md

* Update model_cards/mymusise/gpt2-medium-chinese/README.md

Co-authored-by: Julien Chaumond <chaumond@gmail.com>
==

model_cards/mymusise/gpt2-medium-chinese/README.md
==================
187554366;Stas Bekman;2020-11-08 01:25:14 -0800;fix md table (#8395)

==

model_cards/allenai/wmt19-de-en-6-6-base/README.md
model_cards/allenai/wmt19-de-en-6-6-big/README.md
scripts/fsmt/gen-card-allenai-wmt19.py
==================
77a257fc2;Jonathan Chang;2020-11-07 09:51:56 -0800;Fix DataCollatorForWholeWordMask (#8379)
* Fix DataCollatorForWholeWordMask

* Replace all tensorize_batch in data_collator.py
==

src/transformers/data/data_collator.py
==================
517eaf460;Stas Bekman;2020-11-07 09:45:16 -0800;[make] rewrite modified_py_files in python to be cross-platform (#8371)
* rewrite modified_py_files in python to be cross-platform

* try a different way to test for variable not being ""

* improve comment
==

Makefile
utils/get_modified_files.py
==================
07708793f;Patrick von Platen;2020-11-06 21:03:25 +0100;fix encoder outputs (#8368)

==

src/transformers/generation_tf_utils.py
==================
bc0d26d1d;Yossi Synett;2020-11-06 18:34:48 +0000;[All Seq2Seq model + CLM models that can be used with EncoderDecoder] Add cross-attention weights to outputs (#8071)
* Output cross-attention with decoder attention output

* Update src/transformers/modeling_bert.py

* add cross-attention for t5 and bart as well

* fix tests

* correct typo in docs

* add sylvains and sams comments

* correct typo

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
==

docs/source/main_classes/output.rst
src/transformers/modeling_bart.py
src/transformers/modeling_bert.py
src/transformers/modeling_bert_generation.py
src/transformers/modeling_electra.py
src/transformers/modeling_encoder_decoder.py
src/transformers/modeling_fsmt.py
src/transformers/modeling_gpt2.py
src/transformers/modeling_layoutlm.py
src/transformers/modeling_outputs.py
src/transformers/modeling_prophetnet.py
src/transformers/modeling_roberta.py
src/transformers/modeling_t5.py
tests/test_modeling_common.py
tests/test_modeling_encoder_decoder.py
tests/test_modeling_prophetnet.py
==================
30f2507a0;hassoudi;2020-11-06 11:45:46 -0500;Update README.md (#8360)
Fix websitr address
==

model_cards/TypicaAI/magbert-ner/README.md
==================
5807ba3fa;Jonathan Chang;2020-11-06 08:19:41 -0800;Fix typo (#8351)

==

examples/distillation/README.md
==================
82146496b;hassoudi;2020-11-06 06:20:58 -0500;Update README.md (#8338)
fixes
==

model_cards/TypicaAI/magbert-ner/README.md
==================
9e5c4d39a;ktrapeznikov;2020-11-06 06:19:59 -0500;Create README.md (#8312)
* Create README.md

* Update model_cards/ktrapeznikov/gpt2-medium-topic-news/README.md

Co-authored-by: Julien Chaumond <chaumond@gmail.com>
==

model_cards/ktrapeznikov/gpt2-medium-topic-news/README.md
==================
06ebc3796;hasantanvir79;2020-11-06 10:34:24 +0200;Create README.md (#8255)
* Create README.md

Initial commit

* Updated Read me

Updated

* Apply suggestions from code review

Co-authored-by: Julien Chaumond <chaumond@gmail.com>
==

model_cards/tartuNLP/EstBERT/README.md
==================
41cd031cf;Karthik Uppuluri;2020-11-06 00:26:07 -0800;Create README.md (#8169)

==

model_cards/kuppuluri/telugu_bertu_pos/README.md
==================
f932ddeff;Karthik Uppuluri;2020-11-06 00:25:52 -0800;Create README.md (#8170)

==

model_cards/kuppuluri/telugu_bertu_tydiqa/README.md
==================
08b92f78f;Karthik Uppuluri;2020-11-06 00:25:33 -0800;Create README.md (#8168)
* Create README.md

* Update README.md
==

model_cards/kuppuluri/telugu_bertu_ner/README.md
==================
77d62e78b;Karthik Uppuluri;2020-11-06 00:24:31 -0800;Create README.md (#8167)
* Create README.md

Telugu BERTU Readme file

* Update model_cards/kuppuluri/telugu_bertu/README.md

Co-authored-by: Julien Chaumond <chaumond@gmail.com>
==

model_cards/kuppuluri/telugu_bertu/README.md
==================
dd6bfcaef;Yifan Peng;2020-11-06 03:22:52 -0500;Create README.md (#8327)

==

model_cards/bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-12/README.md
==================
ddeecf08e;smanjil;2020-11-06 09:21:13 +0100;german medbert model details (#8266)
* model details

* Apply suggestions from code review

Co-authored-by: Julien Chaumond <chaumond@gmail.com>
==

model_cards/smanjil/German-MedBERT/README.md
==================
96baaafd3;Jiaxin Pei;2020-11-06 03:19:12 -0500;Create README.md (#8258)

==

model_cards/pedropei/question-intimacy/README.md
==================
185259c26;Stefan Schweter;2020-11-06 09:17:03 +0100;[model_cards] Update Italian BERT models and introduce new Italian XXL ELECTRA model üéâ (#8343)

==

model_cards/dbmdz/bert-base-italian-cased/README.md
model_cards/dbmdz/bert-base-italian-uncased/README.md
model_cards/dbmdz/bert-base-italian-xxl-cased/README.md
model_cards/dbmdz/bert-base-italian-xxl-uncased/README.md
model_cards/dbmdz/electra-base-italian-xxl-cased-discriminator/README.md
model_cards/dbmdz/electra-base-italian-xxl-cased-generator/README.md
==================
34bbf60bf;Manuel Romero;2020-11-06 09:15:11 +0100;Model card: GPT-2 fine-tuned on CommonGen (#8248)

==

model_cards/mrm8488/GPT-2-finetuned-common_gen/README.md
==================
973218fd3;Manuel Romero;2020-11-06 09:13:45 +0100;Model card: CodeBERT fine-tuned for Insecure Code Detection (#8247)
* Model card: CodeBERT fine-tuned for Insecure Code Detection

* Update model_cards/mrm8488/codebert-base-finetuned-detect-insecure-code/README.md

Co-authored-by: Julien Chaumond <chaumond@gmail.com>
==

model_cards/mrm8488/codebert-base-finetuned-detect-insecure-code/README.md
==================
f833ca418;Manuel Romero;2020-11-06 09:09:55 +0100;Model card: T5-base fine-tuned on QuaRel (#8334)

==

model_cards/mrm8488/t5-base-finetuned-quarel/README.md
==================
9edafaebe;Stas Bekman;2020-11-05 20:15:14 -0800;[s2s] test_bash_script.py - actually learn something (#8318)
* use decorator

* remove hardcoded paths

* make the test use more data and do real quality tests

* shave off 10 secs

* add --eval_beams 2, reformat

* reduce train size, use smaller custom dataset
==

examples/seq2seq/test_bash_script.py
==================
17450397a;Leandro von Werra;2020-11-05 23:20:57 +0100;Docs bart training ref (#8330)
Co-authored-by: Sam Shleifer <sshleifer@gmail.com>
==

docs/source/model_doc/bart.rst
examples/seq2seq/README.md
==================
d787935a1;Stas Bekman;2020-11-05 13:01:15 -0800;[s2s] test_distributed_eval (#8315)
Co-authored-by: Sam Shleifer <sshleifer@gmail.com>
==

docs/source/testing.rst
examples/seq2seq/test_finetune_trainer.py
examples/seq2seq/test_seq2seq_examples_multi_gpu.py
src/transformers/testing_utils.py
==================
04e442d57;Sylvain Gugger;2020-11-05 15:13:51 -0500;Make Trainer evaluation handle dynamic seq_length (#8336)
* Make Trainer evaluation handle dynamic seq_length

* Document behavior.

* Fix test

* Better fix

* Fixes for realsies this time

* Address review comments

* Without forgetting to save...
==

src/transformers/trainer.py
src/transformers/trainer_pt_utils.py
tests/test_trainer.py
==================
27b402cab;Guillaume Filion;2020-11-05 15:10:43 -0500;Output global_attentions in Longformer models (#7562)
* Output global_attentions in Longformer models

* make style

* small refactoring

* fix tests

* make fix-copies

* add for tf as well

* remove comments in test

* make fix-copies

* make style

* add docs

* make docstring pretty

Co-authored-by: patrickvonplaten <patrick.v.platen@gmail.com>
==

docs/source/model_doc/longformer.rst
src/transformers/modeling_longformer.py
src/transformers/modeling_tf_longformer.py
tests/test_modeling_common.py
tests/test_modeling_longformer.py
tests/test_modeling_tf_common.py
tests/test_modeling_tf_longformer.py
==================
7abc1d96d;Sam Shleifer;2020-11-05 11:42:24 -0500;no warn (#8329)

==

examples/seq2seq/utils.py
==================
52f44dd6d;Bobby Donchev;2020-11-05 15:38:30 +0100;change TokenClassificationTask class methods to static methods (#7902)
* change TokenClassificationTask class methods to static methods

Since we do not require self in the class methods of TokenClassificationTask we should probably switch to static methods. Also, since the class TokenClassificationTask does not contain a constructor it is currently unusable as is. By switching to static methods this fixes the issue of having to document the intent of the broken class.

Also, since the get_labels and read_examples_from_file methods are ought to be implemented. Static method definitions are unchanged even after inheritance, which means that it can be overridden, similar to other class methods.

* Trigger Build

Co-authored-by: Lysandre <lysandre.debut@reseau.eseo.fr>
==

examples/token-classification/utils_ner.py
==================
77c8f6c62;Guillem Garc√≠a Subies;2020-11-05 13:48:36 +0100;Corrected typo in readme (#8320)

==

examples/README.md
==================
226b9debb;Patrick von Platen;2020-11-05 09:40:15 +0100;Update PULL_REQUEST_TEMPLATE.md

==

.github/PULL_REQUEST_TEMPLATE.md
==================
6f35c61f9;Patrick von Platen;2020-11-05 09:39:05 +0100;Update bug-report.md

==

.github/ISSUE_TEMPLATE/bug-report.md
==================
638c0b7c5;Yifan Peng;2020-11-05 03:03:19 -0500;Create README.md (#8223)
* Create README.md

* Update README.md

* Apply suggestions from code review

Co-authored-by: Kevin Canwen Xu <canwenxu@126.com>
Co-authored-by: Julien Chaumond <chaumond@gmail.com>
==

model_cards/bionlp/bluebert_pubmed_uncased_L-12_H-768_A-12/README.md
==================
9c4aa4ac1;Sylvain Gugger;2020-11-04 17:24:49 -0500;Clean up data collators and datasets (#8308)
* Clean up data collators and datasets

* Apply suggestions from code review

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* Remove needless clone

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
==

examples/language-modeling/run_mlm.py
src/transformers/__init__.py
src/transformers/data/data_collator.py
src/transformers/data/datasets/language_modeling.py
src/transformers/utils/dummy_pt_objects.py
tests/test_data_collator.py
==================
b1d3e95eb;Manuel Romero;2020-11-04 19:17:57 +0100;Fix path to old run_language_modeling.py script (#8302)

==

examples/language-modeling/README.md
==================
b6e58db27;Sylvain Gugger;2020-11-04 11:51:21 -0500;Speedup doc build (#8301)
* Try -j option

* Try other thing

* Bigger machine

* Test lower sphinx version

* Remove trailing space
==

setup.py
==================
969ccac2e;Victor SANH;2020-11-04 11:41:45 -0500;adding model cards for distilled models (#8300)
* adding model cards for distil models

* forgot the languages
==

model_cards/distilbert-base-cased-README.md
model_cards/distilbert-base-cased-distilled-squad-README.md
model_cards/distilbert-base-multilingual-cased-README.md
model_cards/distilbert-base-uncased-README.md
model_cards/distilbert-base-uncased-distilled-squad-README.md
model_cards/distilbert-base-uncased-finetuned-sst-2-english-README.md
model_cards/distilgpt2-README.md
model_cards/distilroberta-base-README.md
==================
7342d9a58;Nicolas Patry;2020-11-04 17:30:42 +0100;Improve QA pipeline error handling (#8286)
- The issue is that with previous code we would have the following:

```python
qa_pipeline = (...)
qa_pipeline(question="Where was he born ?", context="")
-> IndexError: Dimension out of range (expected to be in range of [-1, 0], but got 1)
```

The goal here is to improve this to actually return a ValueError
wherever possible.

While at it, I tried to simplify QuestionArgumentHandler's code to
make it smaller and more compat while keeping backward compat.
==

src/transformers/pipelines.py
tests/test_pipelines_question_answering.py
==================
38630e7a8;Branden Chan;2020-11-04 17:21:25 +0100;Update model cards of deepset/roberta-base-squad2 v1 and v2 (#8241)
* update deepset/roberta-base-squad2 to v2

* Update model_cards/deepset/roberta-base-squad2/README.md

Co-authored-by: Julien Chaumond <chaumond@gmail.com>
==

model_cards/deepset/roberta-base-squad2-v2/README.md
model_cards/deepset/roberta-base-squad2/README.md
==================
04561ecbe;Manuel Romero;2020-11-04 17:20:15 +0100;Model card: T5-base fine-tuned on QASC (#8299)

==

model_cards/mrm8488/t5-base-finetuned-qasc/README.md
==================
854b44aa3;Sylvain Gugger;2020-11-04 11:13:24 -0500;Revert size change as it doesn't change anything

==

.circleci/config.yml
==================
414985c42;Sylvain Gugger;2020-11-04 10:44:19 -0500;Upgrade resource for doc building

==

.circleci/config.yml
==================
cf8972469;Sylvain Gugger;2020-11-04 10:42:18 -0500;Fix validation file loading in scripts (#8298)

==

examples/language-modeling/run_clm.py
examples/language-modeling/run_mlm.py
examples/language-modeling/run_mlm_wwm.py
examples/language-modeling/run_plm.py
templates/adding_a_new_example_script/{{cookiecutter.directory_name}}/run_{{cookiecutter.example_shortcut}}.py
==================
cb966e640;Patrick von Platen;2020-11-04 15:44:36 +0100;[Generate Test] fix greedy generate test (#8293)
* fix greedy generate test

* delet ipdb
==

tests/test_generation_utils.py
==================
734afa37f;Pengzhi Gao;2020-11-04 22:38:02 +0800;Fix typo in language-modeling README.md (#8287)

==

examples/language-modeling/README.md
==================
7a7e2c260;Stas Bekman;2020-11-04 06:02:28 -0800;[blenderbot] regex fix (#8282)
Fixing:

```
src/transformers/tokenization_blenderbot.py:163: DeprecationWarning: invalid escape sequence \s
    token = re.sub("\s{2,}", " ", token)
```
==

src/transformers/tokenization_blenderbot.py
==================
29b536a73;Ceyda Cinarel;2020-11-04 07:21:04 +0900;[WIP] Ner pipeline grouped_entities fixes (#5970)
* Bug fix: NER pipeline shouldn't group separate entities of same type

* style fix

* [Bug Fix] Shouldn't group entities that are both 'B' even if they are same type
	(B-type1 B-type1) != (B-type1 I-type1)
[Bug Fix] add an option `ignore_subwords` to ignore subsequent ##wordpieces in predictions. Because some models train on only the first token of a word and not on the subsequent wordpieces (BERT NER default). So it makes sense doing the same thing at inference time.
	The simplest fix is to just group the subwords with the first wordpiece.
	[TODO] how to handle ignored scores? just set them to 0 and calculate zero invariant mean ?
	[TODO] handle different wordpiece_prefix ## ? possible approaches:
		get it from tokenizer? but currently most tokenizers dont have a wordpiece_prefix property?
		have an _is_subword(token)
[Feature add] added option to `skip_special_tokens`. Cause It was harder to remove them after grouping.
[Additional Changes] remove B/I prefix on returned grouped_entities
[Feature Request/TODO] Return indexes?
[Bug TODO]  can't use fast tokenizer with grouped_entities ('BertTokenizerFast' object has no attribute 'convert_tokens_to_string')

* use offset_mapping to fix [UNK] token problem

* ignore score for subwords

* modify ner_pipeline test

* modify ner_pipeline test

* modify ner_pipeline test

* ner_pipeline change ignore_subwords default to true

* add ner_pipeline ignore_subword=False test case

* fix offset_mapping index

* fix style again duh

* change is_subword and convert_tokens_to_string logic

* merge tests with new test structure

* change test names

* remove old tests

* ner tests for fast tokenizer

* fast tokenizers have convert_tokens_to_string

* Fix the incorrect merge

Co-authored-by: Ceyda Cinarel <snu-ceyda@users.noreply.github.com>
Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
Co-authored-by: Lysandre <lysandre.debut@reseau.eseo.fr>
==

src/transformers/pipelines.py
tests/test_pipelines_ner.py
==================
1bb4bba53;Stas Bekman;2020-11-03 13:57:12 -0800;[CIs] Better reports everywhere (#8275)
* make it possible to invoke testconf.py in both test suites without crashing on having the same option added

* perl -pi -e 's|--make_reports|--make-reports|' to be consistent with other opts

* add `pytest --make-reports` to all CIs (and artifacts)

* fix
==

.circleci/config.yml
.github/workflows/self-push.yml
.github/workflows/self-scheduled.yml
examples/conftest.py
src/transformers/testing_utils.py
tests/conftest.py
==================
7f556d2e3;Sylvain Gugger;2020-11-03 16:33:27 -0500;Data collator for token classification (#8274)
* Add DataCollatorForTokenClassification and clean tests

* Make quality
==

src/transformers/__init__.py
src/transformers/data/data_collator.py
src/transformers/utils/dummy_pt_objects.py
tests/test_data_collator.py
==================
6a064447f;Philip May;2020-11-03 21:57:17 +0100;improve documentation of training_args.py (#8270)
* improve documentation of training_args.py

- do_train
- do_eval
- do_predict

* fix line too long

* fix style with black on training_args.py

* Update src/transformers/training_args.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/training_args.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/training_args.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* fix line length with utils/style_doc

* black reformatting

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

src/transformers/training_args.py
==================
4c19f3baa;Sylvain Gugger;2020-11-03 15:50:55 -0500;Clean Trainer tests and datasets dep (#8268)

==

.circleci/config.yml
tests/test_trainer.py
==================
068e6b5ed;Patrick von Platen;2020-11-03 21:13:33 +0100;make files independent (#8267)

==

examples/seq2seq/finetune_trainer.py
examples/seq2seq/seq2seq_trainer.py
examples/seq2seq/seq2seq_training_args.py
==================
cd360dcb2;Stas Bekman;2020-11-03 10:17:11 -0800;[examples] minimal version requirement run-time check in PL (#8133)
Co-authored-by: Sam Shleifer <sshleifer@gmail.com>
==

examples/lightning_base.py
==================
971c638ee;Stas Bekman;2020-11-03 09:04:53 -0800;forward the worker stderr to the parent process (#8262)

==

src/transformers/testing_utils.py
==================
eb6313e82;Lysandre;2020-11-03 10:35:00 -0500;Fix Tatoeba skip

==

examples/seq2seq/test_tatoeba_conversion.py
==================
74f6f91a9;guillaume-be;2020-11-03 16:33:01 +0100;Updated ConversationalPipeline to work with encoder-decoder models (#8207)
* Updated ConversationalPipeline to work with encoder-decoder models (e.g. BlenderBot)

* Addition of integration test for EncoderDecoder conversation model

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
==

src/transformers/pipelines.py
tests/test_pipelines_conversational.py
==================
c66ffa3a1;Nicolas Patry;2020-11-03 16:10:22 +0100;[FIX] TextGenerationPipeline is currently broken. (#8256)
* [FIX] TextGenerationPipeline is currently broken.

It's most likely due to #8180.
What's missing is a multi vs single string handler at the beginning of
the pipe.
And also there was no testing of this pipeline.

* Fixing Conversational tests too.
==

src/transformers/pipelines.py
tests/test_pipelines_conversational.py
tests/test_pipelines_text_generation.py
==================
a1bbcf3f6;Patrick von Platen;2020-11-03 16:04:22 +0100;Refactoring the generate() function (#6949)
* first draft

* show design proposition for new generate method

* up

* make better readable

* make first version

* gpt2 tests pass

* make beam search for gpt2 work

* add first encoder-decoder code

* delete typo

* make t5 work

* save indermediate

* make bart work with beam search

* finish beam search bart / t5

* add default kwargs

* make more tests pass

* fix no bad words sampler

* some fixes and tests for all distribution processors

* fix test

* fix rag slow tests

* merge to master

* add nograd to generate

* make all slow tests pass

* speed up generate

* fix edge case bug

* small fix

* correct typo

* add type hints and docstrings

* fix typos in tests

* add beam search tests

* add tests for beam scorer

* fix test rag

* finish beam search tests

* move generation tests in seperate file

* fix generation tests

* more tests

* add aggressive generation tests

* fix tests

* add gpt2 sample test

* add more docstring

* add more docs

* finish doc strings

* apply some more of sylvains and sams comments

* fix some typos

* make fix copies

* apply lysandres and sylvains comments

* final corrections on examples

* small fix for reformer
==

docs/source/index.rst
docs/source/internal/generation_utils.rst
docs/source/main_classes/model.rst
src/transformers/__init__.py
src/transformers/generation_beam_search.py
src/transformers/generation_logits_process.py
src/transformers/generation_utils.py
src/transformers/modeling_bart.py
src/transformers/modeling_ctrl.py
src/transformers/modeling_encoder_decoder.py
src/transformers/modeling_fsmt.py
src/transformers/modeling_prophetnet.py
src/transformers/modeling_rag.py
src/transformers/modeling_reformer.py
src/transformers/modeling_t5.py
src/transformers/modeling_transfo_xl.py
src/transformers/modeling_xlnet.py
src/transformers/testing_utils.py
src/transformers/utils/dummy_pt_objects.py
tests/test_generation_beam_search.py
tests/test_generation_logits_process.py
tests/test_generation_utils.py
tests/test_modeling_bart.py
tests/test_modeling_bert.py
tests/test_modeling_bert_generation.py
tests/test_modeling_blenderbot.py
tests/test_modeling_common.py
tests/test_modeling_ctrl.py
tests/test_modeling_fsmt.py
tests/test_modeling_gpt2.py
tests/test_modeling_openai.py
tests/test_modeling_prophetnet.py
tests/test_modeling_reformer.py
tests/test_modeling_roberta.py
tests/test_modeling_t5.py
tests/test_modeling_transfo_xl.py
tests/test_modeling_xlm.py
tests/test_modeling_xlnet.py
==================
b63beb743;Sam Shleifer;2020-11-03 09:49:29 -0500;Skip tatoeba tests if Tatoeba-Challenge not cloned (#8260)

==

examples/seq2seq/test_tatoeba_conversion.py
==================
9f1747f99;Patrick von Platen;2020-11-03 13:56:41 +0100;[Seq2Seq] Correct import in Seq2Seq Trainer (#8254)

==

examples/seq2seq/seq2seq_trainer.py
==================
504ff7bb1;Stas Bekman;2020-11-02 15:50:26 -0800;2 SinusoidalPositionalEmbedding fixes (#8226)

==

src/transformers/modeling_bart.py
tests/test_modeling_bart.py
==================
f744b8157;Patrick von Platen;2020-11-02 20:21:55 +0100;add new notebooks (#8246)

==

notebooks/README.md
==================
dc26726df;Patrick von Platen;2020-11-02 20:12:34 +0100;fix encoder decoder bug (#8243)

==

src/transformers/configuration_encoder_decoder.py
==================
9a23af4af;Lysandre Debut;2020-11-02 19:10:09 +0000;Add XLMProphetNetTokenizer to tokenization auto (#8245)

==

src/transformers/tokenization_auto.py
==================
5b178f3c8;Patrick von Platen;2020-11-02 20:03:44 +0100;Create README.md

==

model_cards/patrickvonplaten/roberta_shared_bbc_xsum/README.md
==================
e1b1b614b;Sylvain Gugger;2020-11-02 12:27:04 -0500;Add line by line option to mlm/plm scripts (#8240)
* Make line by line optional in run_mlm

* Add option to disable dynamic padding

* Add option to plm too and update README

* Typos

* More typos

* Even more typos

* Apply suggestions from code review

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
==

examples/language-modeling/README.md
examples/language-modeling/run_mlm.py
examples/language-modeling/run_mlm_wwm.py
examples/language-modeling/run_plm.py
==================
ebec410c7;Patrick von Platen;2020-11-02 17:53:22 +0100;Create README.md

==

model_cards/patrickvonplaten/bert2bert_cnn_daily_mail/README.md
==================
5406f31a1;Sylvain Gugger;2020-11-02 10:43:28 -0500;Fix TensorBoardCallback for older versions of PyTorch (#8239)

==

src/transformers/integrations.py
==================
d1ad4bff4;Sylvain Gugger;2020-11-02 10:26:37 -0500;Fix bad import with PyTorch <= 1.4.1 (#8237)

==

src/transformers/trainer_pt_utils.py
==================
3c8d401cf;Lysandre Debut;2020-11-02 15:26:25 +0000;Patch reports (#8238)

==

.github/workflows/self-scheduled.yml
==================
93354bc77;Martin Monperrus;2020-11-02 13:53:17 +0000;doc: fix typo (#8235)

==

notebooks/README.md
==================
0c92e7d9f;Santiago Castro;2020-11-02 08:47:37 -0500;Fix ignore list behavior in doctests (#8213)

==

src/transformers/modeling_auto.py
src/transformers/modeling_tf_utils.py
src/transformers/modeling_utils.py
tests/test_doc_samples.py
==================
84caa2330;Nicolas Patry;2020-11-02 12:33:50 +0100;Fix the behaviour of DefaultArgumentHandler (removing it). (#8180)
* Some work to fix the behaviour of DefaultArgumentHandler by removing it.

* Fixing specific pipelines argument checking.
==

src/transformers/pipelines.py
tests/test_pipelines_common.py
tests/test_pipelines_fill_mask.py
tests/test_pipelines_zero_shot.py
==================
00cc2d1df;Zhiqi Huang;2020-11-02 13:19:38 +0800;DynaBERT model cards update (#8192)
* Update README.md

* Update README.md
==

model_cards/huawei-noah/DynaBERT_MNLI/README.md
model_cards/huawei-noah/DynaBERT_SST-2/README.md
==================
aa79aa4e7;Kushal;2020-11-02 10:47:43 +0530;Added 12 model cards for Indian Language Models (#8198)
* Create README.md

* added model cards
==

model_cards/neuralspace-reverie/indic-transformers-bn-bert/README.md
model_cards/neuralspace-reverie/indic-transformers-bn-distilbert/README.md
model_cards/neuralspace-reverie/indic-transformers-bn-roberta/README.md
model_cards/neuralspace-reverie/indic-transformers-bn-xlmroberta/README.md
model_cards/neuralspace-reverie/indic-transformers-hi-bert/README.md
model_cards/neuralspace-reverie/indic-transformers-hi-distilbert/README.md
model_cards/neuralspace-reverie/indic-transformers-hi-roberta/README.md
model_cards/neuralspace-reverie/indic-transformers-hi-xlmroberta/README.md
model_cards/neuralspace-reverie/indic-transformers-te-bert/README.md
model_cards/neuralspace-reverie/indic-transformers-te-distilbert/README.md
model_cards/neuralspace-reverie/indic-transformers-te-roberta/README.md
model_cards/neuralspace-reverie/indic-transformers-te-xlmroberta/README.md
==================
9bd30f7cf;Patrick von Platen;2020-11-01 23:31:55 +0100;[Seq2SeqTrainer] Move import to init to make file self-contained (#8194)
* boom boom

* reverse order
==

examples/seq2seq/seq2seq_trainer.py
==================
1f12934df;guillaume-be;2020-11-01 16:21:57 +0100;[Bug fix] Fixed value for BlenderBot pad token (#8205)

==

src/transformers/tokenization_blenderbot.py
==================
8f1c960ee;Abi See;2020-10-30 13:45:38 -0700;Fix two bugs with --logging_first_step (#8193)
* make sure that logging_first_step evaluates

* fix bug with incorrect loss on logging_first_step

* fix style

* logging_first_step only logs, not evals
==

src/transformers/trainer.py
src/transformers/training_args.py
==================
689ff74f9;Avital Oliver;2020-10-30 21:25:39 +0100;Minor style improvements for the Flax BERT and RoBERTa examples (#8178)
* Minor style improvements:

1. Use `@nn.compact` rather than `@compact` (as to not make it seem
   like compact is a standard Python decorator.
2. Move attribute docstrings from two `__call__` methods to comments
   on the attributes themselves. (This was probably a remnant from
   the pre-Linen version where the attributes were arguments to
   `call`.)

* Use black on the Flax modeling code
==

src/transformers/modeling_flax_bert.py
src/transformers/modeling_flax_roberta.py
==================
9eb3a410c;Sylvain Gugger;2020-10-30 15:27:20 -0400;Remove deprecated arguments from new run_clm (#8197)

==

examples/language-modeling/run_clm.py
==================
00112c353;TFUsers;2020-10-30 12:09:10 -0700;Replace swish with silu (#8166)
* Replace swish with silu

* revert nn.silu to nn.swish due to older version

* simplify optimized silu conditional and fix format

* Update activations.py

* Update activations_tf.py

* Update modeling_flax_utils.py

* Update modeling_openai.py

* add swish testcase

* add pytorch swish testcase

* Add more robust python version check

* more formatting fixes

Co-authored-by: TFUsers <TFUsers@gmail.com>
==

src/transformers/activations.py
src/transformers/activations_tf.py
src/transformers/configuration_albert.py
src/transformers/configuration_bart.py
src/transformers/configuration_bert.py
src/transformers/configuration_bert_generation.py
src/transformers/configuration_blenderbot.py
src/transformers/configuration_deberta.py
src/transformers/configuration_distilbert.py
src/transformers/configuration_dpr.py
src/transformers/configuration_electra.py
src/transformers/configuration_fsmt.py
src/transformers/configuration_funnel.py
src/transformers/configuration_gpt2.py
src/transformers/configuration_layoutlm.py
src/transformers/configuration_lxmert.py
src/transformers/configuration_marian.py
src/transformers/configuration_mbart.py
src/transformers/configuration_mobilebert.py
src/transformers/configuration_openai.py
src/transformers/configuration_pegasus.py
src/transformers/configuration_prophetnet.py
src/transformers/configuration_reformer.py
src/transformers/configuration_retribert.py
src/transformers/configuration_squeezebert.py
src/transformers/configuration_xlnet.py
src/transformers/modeling_flax_utils.py
src/transformers/modeling_openai.py
tests/test_activations.py
tests/test_activations_tf.py
==================
cdc48ce92;Sylvain Gugger;2020-10-30 14:20:18 -0400;Finalize lm examples (#8188)
* Finish the cleanup of the language-modeling examples

* Update main README

* Apply suggestions from code review

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* Apply suggestions from code review

Co-authored-by: Thomas Wolf <thomwolf@users.noreply.github.com>

* Propagate changes

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
Co-authored-by: Thomas Wolf <thomwolf@users.noreply.github.com>
==

examples/README.md
examples/contrib/legacy/run_language_modeling.py
examples/language-modeling/README.md
examples/language-modeling/run_clm.py
examples/language-modeling/run_mlm_wwm.py
examples/language-modeling/run_plm.py
examples/text-classification/run_glue.py
templates/adding_a_new_example_script/{{cookiecutter.directory_name}}/run_{{cookiecutter.example_shortcut}}.py
==================
089cc1015;Sylvain Gugger;2020-10-30 12:37:34 -0400;Doc fixes and filter warning in wandb (#8189)

==

src/transformers/integrations.py
src/transformers/modeling_albert.py
src/transformers/modeling_bert.py
src/transformers/modeling_electra.py
src/transformers/modeling_funnel.py
src/transformers/modeling_lxmert.py
src/transformers/modeling_mobilebert.py
src/transformers/modeling_tf_albert.py
src/transformers/modeling_tf_bert.py
src/transformers/modeling_tf_electra.py
src/transformers/modeling_tf_funnel.py
src/transformers/modeling_tf_lxmert.py
src/transformers/modeling_tf_mobilebert.py
==================
566b083eb;Sam Shleifer;2020-10-30 11:23:16 -0400;TFMarian, TFMbart, TFPegasus, TFBlenderbot (#7987)
* Start plumbing

* Marian close

* Small stubs for all children

* Fixed bart

* marian working

* pegasus test is good, but failing

* Checkin tests

* More model files

* Subtle marian, pegasus integration test failures

* Works well

* rm print

* boom boom

* Still failing model2doc

* merge master

* Equivalence test failing, all others fixed

* cleanup

* Fix embed_scale

* Cleanup marian pipeline test

* Undo extra changes

* Smaller delta

* Cleanup model testers

* undo delta

* fix tests import structure

* cross test decorator

* Cleaner set_weights

* Respect authorized_unexpected_keys

* No warnings

* No warnings

* style

* Nest tf import

* black

* Apply suggestions from code review

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* functional dropout

* fixup

* Fixup

* style_doc

* embs

* shape list

* delete slow force_token_id_to_be_generated func

* fixup

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
==

docs/source/model_doc/blenderbot.rst
docs/source/model_doc/marian.rst
docs/source/model_doc/mbart.rst
docs/source/model_doc/pegasus.rst
src/transformers/__init__.py
src/transformers/modeling_bart.py
src/transformers/modeling_tf_auto.py
src/transformers/modeling_tf_bart.py
src/transformers/modeling_tf_blenderbot.py
src/transformers/modeling_tf_marian.py
src/transformers/modeling_tf_mbart.py
src/transformers/modeling_tf_pegasus.py
src/transformers/utils/dummy_tf_objects.py
tests/test_modeling_marian.py
tests/test_modeling_tf_bart.py
tests/test_modeling_tf_blenderbot.py
tests/test_modeling_tf_marian.py
tests/test_modeling_tf_mbart.py
tests/test_modeling_tf_pegasus.py
utils/check_repo.py
==================
6279072f5;Santiago Castro;2020-10-30 11:22:03 -0400;Fix typo: s/languaged/language/ (#8165)

==

src/transformers/modeling_outputs.py
src/transformers/modeling_prophetnet.py
src/transformers/modeling_t5.py
src/transformers/modeling_tf_outputs.py
src/transformers/modeling_tf_t5.py
==================
10f8c6362;Lysandre Debut;2020-10-30 14:25:48 +0000;Ci test tf super slow (#8007)
* Test TF GPU CI

* Change cache

* Fix missing torch requirement

* Fix some model tests


Style

* LXMERT

* MobileBERT

* Longformer skip test

* XLNet

* The rest of the tests

* RAG goes OOM in multi gpu setup

* YAML test files

* Last fixes

* Skip doctests

* Fill mask tests

* Yaml files

* Last test fix

* Style

* Update cache

* Change ONNX tests to slow + use tiny model
==

.github/workflows/self-push.yml
.github/workflows/self-scheduled.yml
src/transformers/modeling_tf_albert.py
src/transformers/modeling_tf_funnel.py
src/transformers/modeling_tf_xlnet.py
tests/test_doc_samples.py
tests/test_modeling_common.py
tests/test_modeling_marian.py
tests/test_modeling_prophetnet.py
tests/test_modeling_rag.py
tests/test_modeling_roberta.py
tests/test_modeling_squeezebert.py
tests/test_modeling_tf_camembert.py
tests/test_modeling_tf_common.py
tests/test_modeling_tf_flaubert.py
tests/test_modeling_tf_longformer.py
tests/test_modeling_tf_lxmert.py
tests/test_modeling_tf_mobilebert.py
tests/test_modeling_tf_t5.py
tests/test_modeling_tf_xlm_roberta.py
tests/test_modeling_transfo_xl.py
tests/test_modeling_xlm_prophetnet.py
tests/test_modeling_xlm_roberta.py
tests/test_onnx.py
tests/test_pipelines_fill_mask.py
==================
7e36deec7;Nicolas Patry;2020-10-30 14:15:41 +0100;Fixing some warnings in DeBerta (#8176)
* Fixing some warnings in DeBerta

* Fixing docs with their rewritten version.
==

src/transformers/modeling_deberta.py
src/transformers/tokenization_deberta.py
==================
053882073;Stas Bekman;2020-10-29 16:30:05 -0700;[CI] Better reports #2 (#8163)

==

.github/workflows/self-scheduled.yml
src/transformers/testing_utils.py
==================
9a21b5061;wlhgtc;2020-10-30 05:08:39 +0800;Fix eval ref miss in Chinese WWM. (#8115)
* ADD: add whole word mask proxy for both eng and chinese

* MOD: adjust format

* MOD: reformat code

* MOD: update import

* MOD: fix bug

* MOD: add import

* MOD: fix bug

* MOD: decouple code and update readme

* MOD: reformat code

* Update examples/language-modeling/README.md

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update examples/language-modeling/README.md

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update examples/language-modeling/run_language_modeling.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update examples/language-modeling/run_language_modeling.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update examples/language-modeling/run_language_modeling.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update examples/language-modeling/run_language_modeling.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* change wwm to whole_word_mask

* reformat code

* reformat

* format

* Code quality

* ADD: update chinese ref readme

* MOD: small changes

* MOD: small changes2

* update readme

* fix eval ref file miss bug

* format file

* MOD: move ref code to contrib

* MOD: add delimeter check

* reformat code

* refomat code

* Update examples/language-modeling/README.md

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
Co-authored-by: Sylvain Gugger <sylvain.gugger@gmail.com>
Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
==

examples/contrib/run_chinese_ref.py
examples/language-modeling/README.md
examples/language-modeling/run_language_modeling.py
src/transformers/data/datasets/language_modeling.py
==================
fdf893c44;Santiago Castro;2020-10-29 17:04:20 -0400;Fix typo: indinces -> indices (#8159)
* Fix typo: indinces -> indices

* Fix some more

* Fix some more

* Fix some more

* Fix CI
==

src/transformers/pipelines.py
src/transformers/tokenization_t5.py
src/transformers/tokenization_t5_fast.py
==================
c83cec44f;Stas Bekman;2020-10-29 11:05:24 -0700;improve error checking (#8157)

==

src/transformers/testing_utils.py
==================
691176283;Sylvain Gugger;2020-10-29 13:38:11 -0400;Add a template for examples and apply it for mlm and plm examples (#8153)
* Add a template for example scripts and apply it to mlm

* Formatting

* Fix test

* Add plm script

* Add a template for example scripts and apply it to mlm

* Formatting

* Fix test

* Add plm script

* Add a template for example scripts and apply it to mlm

* Formatting

* Fix test

* Add plm script

* Styling
==

.circleci/config.yml
Makefile
examples/language-modeling/run_clm.py
examples/language-modeling/run_mlm.py
examples/language-modeling/run_plm.py
examples/test_examples.py
src/transformers/data/data_collator.py
templates/adding_a_new_example_script/README.md
templates/adding_a_new_example_script/cookiecutter.json
templates/adding_a_new_example_script/run_xxx.py
templates/adding_a_new_example_script/utils_xxx.py
templates/adding_a_new_example_script/{{cookiecutter.directory_name}}/run_{{cookiecutter.example_shortcut}}.py
==================
49e4fece5;Sam Shleifer;2020-10-29 12:01:15 -0400;[s2s] distillBART docs for paper replication (#8150)

==

examples/seq2seq/README.md
examples/seq2seq/builtin_trainer/train_distilbart_cnn.sh
examples/seq2seq/precomputed_pseudo_labels.md
examples/seq2seq/train_distilbart_cnn.sh
==================
acf56408d;Sylvain Gugger;2020-10-29 10:56:25 -0400;Smarter prediction loop and no- -> no_ in console args (#8151)
* Smarter prediction loop and no- -> no_ in console args

* Fix test
==

examples/test_examples.py
src/transformers/benchmark/benchmark_args.py
src/transformers/hf_argparser.py
src/transformers/trainer.py
tests/test_hf_argparser.py
==================
b0f1c0ee3;Sylvain Gugger;2020-10-29 10:43:45 -0400;Document tokenizer_class in configurations (#8152)

==

src/transformers/configuration_utils.py
==================
969859d5f;Santiago Castro;2020-10-29 10:33:33 -0400;Fix doc errors and typos across the board (#8139)
* Fix doc errors and typos across the board

* Fix a typo

* Fix the CI

* Fix more typos

* Fix CI

* More fixes

* Fix CI

* More fixes

* More fixes
==

CONTRIBUTING.md
docs/source/installation.md
docs/source/migration.md
docs/source/model_sharing.rst
docs/source/task_summary.rst
examples/adversarial/utils_hans.py
examples/bert-loses-patience/pabee/modeling_pabee_bert.py
examples/deebert/src/modeling_highway_bert.py
examples/distillation/distiller.py
examples/distillation/lm_seqs_dataset.py
examples/distillation/scripts/extract.py
examples/lxmert/modeling_frcnn.py
examples/lxmert/processing_image.py
examples/lxmert/utils.py
examples/movement-pruning/counts_parameters.py
examples/movement-pruning/emmental/modeling_bert_masked.py
examples/movement-pruning/masked_run_glue.py
examples/movement-pruning/masked_run_squad.py
examples/rag/distributed_retriever.py
examples/rag/eval_rag.py
examples/rag/use_own_knowledge_dataset.py
examples/seq2seq/bertabs/configuration_bertabs.py
examples/seq2seq/bertabs/modeling_bertabs.py
examples/seq2seq/convert_model_to_fp16.py
model_cards/aubmindlab/bert-base-arabert/README.md
model_cards/aubmindlab/bert-base-arabertv01/README.md
model_cards/elgeish/cs224n-squad2.0-albert-large-v2/README.md
model_cards/jannesg/takalane_afr_roberta/README.md
model_cards/mrm8488/CodeBERTaPy/README.md
model_cards/mrm8488/TinyBERT-spanish-uncased-finetuned-ner/README.md
model_cards/mrm8488/bert-multi-cased-finetuned-xquadv1/README.md
model_cards/mrm8488/bert-multi-uncased-finetuned-xquadv1/README.md
model_cards/mrm8488/bert-spanish-cased-finetuned-ner/README.md
model_cards/mrm8488/bert-spanish-cased-finetuned-pos/README.md
model_cards/mrm8488/mobilebert-uncased-finetuned-squadv1/README.md
model_cards/mrm8488/mobilebert-uncased-finetuned-squadv2/README.md
model_cards/mrm8488/spanbert-base-finetuned-squadv1/README.md
model_cards/mrm8488/spanbert-base-finetuned-squadv2/README.md
model_cards/mrm8488/spanbert-base-finetuned-tacred/README.md
model_cards/mrm8488/spanbert-large-finetuned-squadv1/README.md
model_cards/mrm8488/spanbert-large-finetuned-squadv2/README.md
model_cards/mrm8488/spanbert-large-finetuned-tacred/README.md
model_cards/mrm8488/t5-base-finetuned-wikiSQL-sql-to-en/README.md
model_cards/mrm8488/t5-base-finetuned-wikiSQL/README.md
model_cards/mrm8488/t5-small-finetuned-wikiSQL/README.md
model_cards/mrm8488/xlm-multi-finetuned-xquadv1/README.md
src/transformers/benchmark/benchmark_utils.py
src/transformers/commands/convert.py
src/transformers/configuration_bart.py
src/transformers/configuration_bert.py
src/transformers/configuration_bert_generation.py
src/transformers/configuration_blenderbot.py
src/transformers/configuration_deberta.py
src/transformers/configuration_distilbert.py
src/transformers/configuration_dpr.py
src/transformers/configuration_electra.py
src/transformers/configuration_flaubert.py
src/transformers/configuration_fsmt.py
src/transformers/configuration_funnel.py
src/transformers/configuration_layoutlm.py
src/transformers/configuration_lxmert.py
src/transformers/configuration_marian.py
src/transformers/configuration_mbart.py
src/transformers/configuration_pegasus.py
src/transformers/configuration_prophetnet.py
src/transformers/configuration_rag.py
src/transformers/configuration_reformer.py
src/transformers/configuration_retribert.py
src/transformers/configuration_squeezebert.py
src/transformers/configuration_transfo_xl.py
src/transformers/convert_graph_to_onnx.py
src/transformers/convert_longformer_original_pytorch_lightning_to_pytorch.py
src/transformers/convert_marian_tatoeba_to_pytorch.py
src/transformers/convert_slow_tokenizers_checkpoints_to_fast.py
src/transformers/convert_xlnet_original_tf_checkpoint_to_pytorch.py
src/transformers/data/data_collator.py
src/transformers/data/datasets/language_modeling.py
src/transformers/data/processors/squad.py
src/transformers/data/processors/utils.py
src/transformers/file_utils.py
src/transformers/generation_tf_utils.py
src/transformers/generation_utils.py
src/transformers/modelcard.py
src/transformers/modeling_auto.py
src/transformers/modeling_bart.py
src/transformers/modeling_deberta.py
src/transformers/modeling_distilbert.py
src/transformers/modeling_dpr.py
src/transformers/modeling_encoder_decoder.py
src/transformers/modeling_flaubert.py
src/transformers/modeling_flax_auto.py
src/transformers/modeling_flax_roberta.py
src/transformers/modeling_funnel.py
src/transformers/modeling_gpt2.py
src/transformers/modeling_longformer.py
src/transformers/modeling_lxmert.py
src/transformers/modeling_mobilebert.py
src/transformers/modeling_rag.py
src/transformers/modeling_reformer.py
src/transformers/modeling_retribert.py
src/transformers/modeling_t5.py
src/transformers/modeling_tf_albert.py
src/transformers/modeling_tf_auto.py
src/transformers/modeling_tf_bart.py
src/transformers/modeling_tf_bert.py
src/transformers/modeling_tf_distilbert.py
src/transformers/modeling_tf_flaubert.py
src/transformers/modeling_tf_funnel.py
src/transformers/modeling_tf_gpt2.py
src/transformers/modeling_tf_longformer.py
src/transformers/modeling_tf_lxmert.py
src/transformers/modeling_tf_mobilebert.py
src/transformers/modeling_tf_openai.py
src/transformers/modeling_tf_pytorch_utils.py
src/transformers/modeling_tf_roberta.py
src/transformers/modeling_tf_t5.py
src/transformers/modeling_tf_transfo_xl.py
src/transformers/modeling_tf_utils.py
src/transformers/modeling_tf_xlm.py
src/transformers/modeling_tf_xlnet.py
src/transformers/modeling_utils.py
src/transformers/modeling_xlm.py
src/transformers/modeling_xlnet.py
src/transformers/optimization_tf.py
src/transformers/retrieval_rag.py
src/transformers/tokenization_bert.py
src/transformers/tokenization_bertweet.py
src/transformers/tokenization_camembert.py
src/transformers/tokenization_camembert_fast.py
src/transformers/tokenization_deberta.py
src/transformers/tokenization_fsmt.py
src/transformers/tokenization_herbert.py
src/transformers/tokenization_herbert_fast.py
src/transformers/tokenization_mbart.py
src/transformers/tokenization_mbart_fast.py
src/transformers/tokenization_phobert.py
src/transformers/tokenization_prophetnet.py
src/transformers/tokenization_transfo_xl.py
src/transformers/tokenization_utils.py
src/transformers/tokenization_utils_base.py
src/transformers/tokenization_utils_fast.py
src/transformers/tokenization_xlm.py
src/transformers/tokenization_xlm_prophetnet.py
src/transformers/tokenization_xlm_roberta.py
src/transformers/tokenization_xlm_roberta_fast.py
src/transformers/tokenization_xlnet.py
src/transformers/tokenization_xlnet_fast.py
src/transformers/trainer.py
src/transformers/trainer_pt_utils.py
src/transformers/training_args.py
src/transformers/utils/notebook.py
templates/adding_a_new_example_script/run_xxx.py
templates/adding_a_new_example_script/utils_xxx.py
templates/adding_a_new_model/configuration_xxx.py
templates/adding_a_new_model/modeling_tf_xxx.py
templates/adding_a_new_model/tokenization_xxx.py
tests/test_logging.py
tests/test_modeling_common.py
tests/test_modeling_tf_lxmert.py
utils/check_copies.py
==================
4731a00c3;Ethan;2020-10-29 20:49:16 +0800;Update widget examples. (#8149)
Co-authored-by: yantan <yantan@effyic.com>
==

model_cards/ethanyt/guwenbert-base/README.md
model_cards/ethanyt/guwenbert-large/README.md
==================
238876068;dartrevan;2020-10-29 15:31:32 +0300;Update README.md (#8090)

==

model_cards/cimm-kzn/rudr-bert/README.md
==================
e566adc09;Branden Chan;2020-10-29 13:29:54 +0100;Add model_cards (#7969)
* add readme

* add readmes

* Add metadata
==

model_cards/deepset/gbert-base/README.md
model_cards/deepset/gbert-large/README.md
model_cards/deepset/gelectra-base-generator/README.md
model_cards/deepset/gelectra-base/README.md
model_cards/deepset/gelectra-large-generator/README.md
model_cards/deepset/gelectra-large/README.md
==================
cc8941d88;dartrevan;2020-10-29 15:23:43 +0300;Create README.md (#8089)

==

model_cards/cimm-kzn/endr-bert/README.md
==================
234a6dc38;dartrevan;2020-10-29 15:23:30 +0300;Create README.md (#8088)
* Create README.md

* metadata

Co-authored-by: Julien Chaumond <chaumond@gmail.com>
==

model_cards/cimm-kzn/enrudr-bert/README.md
==================
5d7685953;gurkan08;2020-10-29 15:22:33 +0300;Create README.md (#8075)
* Create README.md

* Update model_cards/gurkan08/bert-turkish-text-classification/README.md

Co-authored-by: Julien Chaumond <chaumond@gmail.com>
==

model_cards/gurkan08/bert-turkish-text-classification/README.md
==================
b215090ee;Ethan;2020-10-29 20:21:54 +0800;Add two model_cards: ethanyt/guwenbert-base and ethanyt/guwenbert-large (#8041)

==

model_cards/ethanyt/guwenbert-base/README.md
model_cards/ethanyt/guwenbert-large/README.md
==================
ba2ad3a98;Ashwani Tanwar;2020-10-29 12:21:11 +0000;Model Card for Gujarati-XLM-R-Base (#8038)
* Add model card for Gujarati-XLM-R-Base

* Update README.md

Add the model card for the Gujarati-XLM-R-Base.

* Apply suggestions from code review

Co-authored-by: Julien Chaumond <chaumond@gmail.com>
==

model_cards/ashwani-tanwar/Gujarati-XLM-R-Base/README.md
==================
52cea7de7;Manuel Romero;2020-10-29 13:19:47 +0100;Create README.md (#8017)

==

model_cards/mrm8488/squeezebert-finetuned-squadv2/README.md
==================
ff82a2aa9;Manuel Romero;2020-10-29 13:19:35 +0100;Create README.md (#8015)

==

model_cards/mrm8488/squeezebert-finetuned-squadv1/README.md
==================
0a3b9733c;Zhiqi Huang;2020-10-29 20:19:17 +0800;Add model_cards for DynaBERT (#8012)
* Update README.md

* Add dynabert_overview.png

* Update README.md

* Create README.md

* Add dynabert_overview.png

* Update README.md

* Update README.md

* Delete dynabert_overview.png

* Update README.md

* Delete dynabert_overview.png

* Update README.md
==

model_cards/huawei-noah/DynaBERT_MNLI/README.md
model_cards/huawei-noah/DynaBERT_SST-2/README.md
==================
afa21504b;Patrick von Platen;2020-10-29 12:45:55 +0100;add tags (#8147)

==

model_cards/google/bert2bert_L-24_wmt_de_en/README.md
model_cards/google/bert2bert_L-24_wmt_en_de/README.md
model_cards/google/roberta2roberta_L-24_bbc/README.md
model_cards/google/roberta2roberta_L-24_cnn_daily_mail/README.md
model_cards/google/roberta2roberta_L-24_gigaword/README.md
==================
825925dfa;Stas Bekman;2020-10-28 13:50:36 -0700;[s2s test] cleanup (#8131)

==

examples/seq2seq/test_seq2seq_examples_multi_gpu.py
==================
e477eb919;Santiago Castro;2020-10-28 15:52:28 -0400;Fix typo in `AutoModelForMaskedLM` docs (#8129)

==

src/transformers/modeling_auto.py
==================
5e24982e5;Sean Naren;2020-10-28 18:59:14 +0000;Upgrade PyTorch Lightning to 1.0.2 (#7852)
Co-authored-by: Sam Shleifer <sshleifer@gmail.com>
==

examples/lightning_base.py
examples/requirements.txt
examples/seq2seq/callbacks.py
examples/seq2seq/finetune.py
examples/seq2seq/test_bash_script.py
examples/seq2seq/test_seq2seq_examples_multi_gpu.py
examples/text-classification/run_pl_glue.py
examples/token-classification/run_pl_ner.py
==================
1b6c8d481;Lysandre Debut;2020-10-28 17:59:43 +0000;Update CI cache (#8126)

==

.circleci/config.yml
.github/workflows/self-push.yml
.github/workflows/self-scheduled.yml
==================
378142afd;Sylvain Gugger;2020-10-28 13:42:31 -0400;Rename add_start_docstrings_to_callable (#8120)

==

examples/bert-loses-patience/pabee/modeling_pabee_albert.py
examples/bert-loses-patience/pabee/modeling_pabee_bert.py
examples/deebert/src/modeling_highway_bert.py
examples/deebert/src/modeling_highway_roberta.py
examples/movement-pruning/emmental/modeling_bert_masked.py
src/transformers/file_utils.py
src/transformers/modeling_albert.py
src/transformers/modeling_bart.py
src/transformers/modeling_bert.py
src/transformers/modeling_bert_generation.py
src/transformers/modeling_ctrl.py
src/transformers/modeling_deberta.py
src/transformers/modeling_distilbert.py
src/transformers/modeling_dpr.py
src/transformers/modeling_electra.py
src/transformers/modeling_encoder_decoder.py
src/transformers/modeling_flaubert.py
src/transformers/modeling_fsmt.py
src/transformers/modeling_funnel.py
src/transformers/modeling_gpt2.py
src/transformers/modeling_layoutlm.py
src/transformers/modeling_longformer.py
src/transformers/modeling_lxmert.py
src/transformers/modeling_mmbt.py
src/transformers/modeling_mobilebert.py
src/transformers/modeling_openai.py
src/transformers/modeling_prophetnet.py
src/transformers/modeling_rag.py
src/transformers/modeling_reformer.py
src/transformers/modeling_roberta.py
src/transformers/modeling_squeezebert.py
src/transformers/modeling_t5.py
src/transformers/modeling_tf_albert.py
src/transformers/modeling_tf_bart.py
src/transformers/modeling_tf_bert.py
src/transformers/modeling_tf_ctrl.py
src/transformers/modeling_tf_distilbert.py
src/transformers/modeling_tf_electra.py
src/transformers/modeling_tf_flaubert.py
src/transformers/modeling_tf_funnel.py
src/transformers/modeling_tf_gpt2.py
src/transformers/modeling_tf_longformer.py
src/transformers/modeling_tf_lxmert.py
src/transformers/modeling_tf_mobilebert.py
src/transformers/modeling_tf_openai.py
src/transformers/modeling_tf_roberta.py
src/transformers/modeling_tf_t5.py
src/transformers/modeling_tf_transfo_xl.py
src/transformers/modeling_tf_xlm.py
src/transformers/modeling_tf_xlnet.py
src/transformers/modeling_transfo_xl.py
src/transformers/modeling_xlm.py
src/transformers/modeling_xlnet.py
templates/adding_a_new_model/modeling_tf_xxx.py
templates/adding_a_new_model/modeling_xxx.py
==================
6241c873c;Sylvain Gugger;2020-10-28 13:41:56 -0400;Document the various LM Auto models (#8118)

==

docs/source/model_doc/auto.rst
==================
5193172f1;Bram Vanroy;2020-10-28 18:26:12 +0100;[DOC] Improve pipeline() docstrings for config and tokenizer (#8123)
* Improve pipeline() docstrings

* make style

* Update wording for config
==

src/transformers/pipelines.py
==================
b4cacb7a6;Boris Dayma;2020-10-28 11:15:30 -0500;fix(trainer_callback]: typo (#8121)

==

src/transformers/trainer_callback.py
==================
5423f2a9d;Stas Bekman;2020-10-28 08:51:32 -0700;[testing] port test_trainer_distributed to distributed pytest + TestCasePlus enhancements (#8107)
* move the helper code into testing_utils

* port test_trainer_distributed to work with pytest

* improve docs

* simplify notes

* doc

* doc

* style

* doc

* further improvements

* torch might not be available

* real fix

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

docs/source/testing.rst
examples/seq2seq/test_finetune_trainer.py
examples/seq2seq/test_seq2seq_examples_multi_gpu.py
examples/seq2seq/utils.py
src/transformers/testing_utils.py
tests/test_trainer_distributed.py
==================
47dfa65b0;Sylvain Gugger;2020-10-28 10:38:58 -0400;New run_clm script (#8105)
* New run_clm script

* Formatting

* More comments

* Remove unused imports

* Apply suggestions from code review

Co-authored-by: Thomas Wolf <thomwolf@users.noreply.github.com>

* Address review comments

* Change link to the hub

Co-authored-by: Thomas Wolf <thomwolf@users.noreply.github.com>
==

examples/language-modeling/run_clm.py
examples/test_examples.py
examples/text-classification/run_glue.py
==================
8065fea87;Stas Bekman;2020-10-27 22:45:19 -0700;[gh actions] run artifacts job always (#8110)

==

.github/workflows/self-scheduled.yml
==================
1e01db357;Sylvain Gugger;2020-10-27 17:36:13 -0400;Remove header

==

examples/README.md
==================
b715e40ce;Sylvain Gugger;2020-10-27 17:34:05 -0400;Fix typo

==

examples/README.md
==================
41cc5f3f5;Sylvain Gugger;2020-10-27 17:32:20 -0400;Move installation instructions to the top (#8106)

==

examples/README.md
==================
556709ad9;Joe Davison;2020-10-27 17:11:43 -0400;rm multiclass option from model card

==

model_cards/facebook/bart-large-mnli/README.md
==================
c5f3149f9;Sylvain Gugger;2020-10-27 14:39:49 -0400;Adjust setup so that all extras run on Windows (#8102)

==

setup.py
==================
995006eab;Davide Fiocco;2020-10-27 19:21:54 +0100;Add AzureML in integrations via dedicated callback  (#8062)
* first attempt to add AzureML callbacks

* func arg fix

* var name fix, but still won't fix error...

* fixing as in https://discuss.huggingface.co/t/how-to-integrate-an-azuremlcallback-for-logging-in-azure/1713/2

* Avoid lint check of azureml import

* black compliance

* Make isort happy

* Fix point typo in docs

* Add AzureML to Callbacks docs

* Attempt to make sphinx happy

* Format callback docs

* Make documentation style happy

* Make docs compliant to style

Co-authored-by: Davide Fiocco <davide.fiocco@frontiersin.net>
==

docs/source/main_classes/callback.rst
src/transformers/integrations.py
src/transformers/trainer.py
==================
a0906068c;Lysandre Debut;2020-10-27 18:14:13 +0000;Fully remove codecov (#8093)

==

.circleci/config.yml
==================
3e58b6b7b;Joe Davison;2020-10-27 14:09:55 -0400;infer entailment label id on zero shot pipeline (#8059)
* add entailment dim argument

* rename dim -> id

* fix last name change, style

* rm arg, auto-infer only

* typo

* rm superfluous import
==

src/transformers/pipelines.py
tests/test_pipelines_zero_shot.py
==================
9fefdb075;Jason Wolosonovich;2020-10-27 11:09:31 -0700;DEP: pinned sentencepiece to 0.1.91 in setup.py (#8069)
Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
==

setup.py
==================
edd3721cd;Stas Bekman;2020-10-27 10:54:57 -0700;update/add setup targets (#8076)

==

setup.py
==================
55bc0c599;Julien Chaumond;2020-10-27 18:08:05 +0100;[model_cards] Switch to a more explicit domain for the media bucket

==

model_cards/albert-base-v1-README.md
model_cards/albert-xxlarge-v2-README.md
model_cards/aliosm/ComVE-distilgpt2/README.md
model_cards/aliosm/ComVE-gpt2-large/README.md
model_cards/aliosm/ComVE-gpt2-medium/README.md
model_cards/aliosm/ComVE-gpt2/README.md
model_cards/aliosm/ai-soco-c++-roberta-small-clas/README.md
model_cards/aliosm/ai-soco-c++-roberta-small/README.md
model_cards/aliosm/ai-soco-c++-roberta-tiny-96-clas/README.md
model_cards/aliosm/ai-soco-c++-roberta-tiny-96/README.md
model_cards/aliosm/ai-soco-c++-roberta-tiny-clas/README.md
model_cards/aliosm/ai-soco-c++-roberta-tiny/README.md
model_cards/bert-base-cased-README.md
model_cards/bert-base-german-cased-README.md
model_cards/bert-base-uncased-README.md
model_cards/codegram/calbert-base-uncased/README.md
model_cards/codegram/calbert-tiny-uncased/README.md
model_cards/deepset/bert-base-german-cased-oldvocab/README.md
model_cards/distilbert-base-uncased-README.md
model_cards/distilgpt2-README.md
model_cards/distilroberta-base-README.md
model_cards/elgeish/cs224n-squad2.0-albert-base-v2/README.md
model_cards/elgeish/cs224n-squad2.0-albert-large-v2/README.md
model_cards/elgeish/cs224n-squad2.0-albert-xxlarge-v1/README.md
model_cards/gpt2-README.md
model_cards/huggingface/CodeBERTa-language-id/README.md
model_cards/huggingface/CodeBERTa-small-v1/README.md
model_cards/roberta-base-README.md
model_cards/roberta-large-README.md
model_cards/seiya/oubiobert-base-uncased/README.md
model_cards/xlm-mlm-en-2048-README.md
model_cards/xlm-roberta-base-README.md
==================
7bff0af0a;Harutaka Kawamura;2020-10-27 23:37:04 +0900;Fix a bug for `CallbackHandler.callback_list` (#8052)
* Fix callback_list

* Add test

Signed-off-by: harupy <17039389+harupy@users.noreply.github.com>

* Fix test

Signed-off-by: harupy <17039389+harupy@users.noreply.github.com>
==

src/transformers/trainer_callback.py
tests/test_trainer_callback.py
==================
8e28c327f;Harutaka Kawamura;2020-10-27 23:34:51 +0900;Fix assertion error message for MLflowCallback (#8091)

==

src/transformers/integrations.py
==================
3220f21f1;Sylvain Gugger;2020-10-27 10:09:51 -0400;Styling fix

==

src/transformers/testing_utils.py
==================
286dc19a4;Jonathan Chang;2020-10-27 21:52:35 +0800;Fix IterableDataset with __len__ in Trainer (#8095)

==

src/transformers/trainer.py
==================
d93acd6f1;Sam Shleifer;2020-10-27 09:42:07 -0400;Move style_doc to extra_quality_checks (#8081)

==

Makefile
==================
bfd5e370a;Stas Bekman;2020-10-27 06:25:07 -0700;[CI] generate separate report files as artifacts (#7995)
* better reports

* a whole bunch of reports in their own files

* clean up

* improvements

* github artifacts experiment

* style

* complete the report generator with multiple improvements/fixes

* fix

* save all reports under one dir to easy upload

* can remove temp failing tests

* doc fix

* some cleanup
==

.circleci/config.yml
.github/workflows/self-push.yml
.github/workflows/self-scheduled.yml
examples/conftest.py
src/transformers/testing_utils.py
tests/conftest.py
==================
33f6ef733;Lysandre Debut;2020-10-27 13:07:41 +0000;Fix DeBERTa docs (#8092)
* Fix DeBERTa docs

* Tokenizer and config
==

src/transformers/configuration_deberta.py
src/transformers/modeling_deberta.py
src/transformers/tokenization_deberta.py
==================
c42596bc0;Sylvain Gugger;2020-10-27 07:54:50 -0400;Doc styling fixes (#8074)
* Fix a few docstrings

* More fixes

* Styling
==

src/transformers/tokenization_camembert_fast.py
src/transformers/tokenization_dpr.py
src/transformers/tokenization_dpr_fast.py
src/transformers/trainer.py
src/transformers/training_args.py
utils/style_doc.py
==================
1496931b4;Doug Blank;2020-10-27 04:31:07 -0700;Fix comet_ml import and add ensure availability (#7933)
* Fix comet_ml import and add ensure availability

* Make isort happy

* Make flake8 happy

* Don't show comet_ml warn if COMET_MODE=DISABLED

* Make isort happy
==

src/transformers/integrations.py
src/transformers/trainer.py
src/transformers/trainer_tf.py
==================
985bba909;Chengxi Guo;2020-10-27 19:29:25 +0800;fix doc bug (#8082)
Signed-off-by: mymusise <mymusise1@gmail.com>
==

src/transformers/file_utils.py
==================
08f534d2d;Sylvain Gugger;2020-10-26 18:26:02 -0400;Doc styling (#8067)
* Important files

* Styling them all

* Revert "Styling them all"

This reverts commit 7d029395fdae8513b8281cbc2a6c239f8093503e.

* Syling them for realsies

* Fix syntax error

* Fix benchmark_utils

* More fixes

* Fix modeling auto and script

* Remove new line

* Fixes

* More fixes

* Fix more files

* Style

* Add FSMT

* More fixes

* More fixes

* More fixes

* More fixes

* Fixes

* More fixes

* More fixes

* Last fixes

* Make sphinx happy
==

.circleci/config.yml
Makefile
README.md
docs/source/benchmarks.rst
docs/source/bertology.rst
docs/source/converting_tensorflow_models.rst
docs/source/custom_datasets.rst
docs/source/glossary.rst
docs/source/index.rst
docs/source/internal/modeling_utils.rst
docs/source/internal/tokenization_utils.rst
docs/source/internal/trainer_utils.rst
docs/source/main_classes/logging.rst
docs/source/main_classes/model.rst
docs/source/main_classes/pipelines.rst
docs/source/main_classes/processors.rst
docs/source/main_classes/tokenizer.rst
docs/source/main_classes/trainer.rst
docs/source/model_doc/albert.rst
docs/source/model_doc/auto.rst
docs/source/model_doc/bart.rst
docs/source/model_doc/bert.rst
docs/source/model_doc/bertgeneration.rst
docs/source/model_doc/blenderbot.rst
docs/source/model_doc/camembert.rst
docs/source/model_doc/ctrl.rst
docs/source/model_doc/deberta.rst
docs/source/model_doc/dialogpt.rst
docs/source/model_doc/distilbert.rst
docs/source/model_doc/dpr.rst
docs/source/model_doc/electra.rst
docs/source/model_doc/encoderdecoder.rst
docs/source/model_doc/flaubert.rst
docs/source/model_doc/fsmt.rst
docs/source/model_doc/funnel.rst
docs/source/model_doc/gpt.rst
docs/source/model_doc/gpt2.rst
docs/source/model_doc/layoutlm.rst
docs/source/model_doc/longformer.rst
docs/source/model_doc/lxmert.rst
docs/source/model_doc/marian.rst
docs/source/model_doc/mbart.rst
docs/source/model_doc/mobilebert.rst
docs/source/model_doc/pegasus.rst
docs/source/model_doc/prophetnet.rst
docs/source/model_doc/rag.rst
docs/source/model_doc/reformer.rst
docs/source/model_doc/roberta.rst
docs/source/model_doc/squeezebert.rst
docs/source/model_doc/t5.rst
docs/source/model_doc/transformerxl.rst
docs/source/model_doc/xlm.rst
docs/source/model_doc/xlmprophetnet.rst
docs/source/model_doc/xlmroberta.rst
docs/source/model_doc/xlnet.rst
docs/source/model_sharing.rst
docs/source/model_summary.rst
docs/source/multilingual.rst
docs/source/perplexity.rst
docs/source/philosophy.rst
docs/source/preprocessing.rst
docs/source/pretrained_models.rst
docs/source/quicktour.rst
docs/source/serialization.rst
docs/source/task_summary.rst
docs/source/testing.rst
docs/source/tokenizer_summary.rst
docs/source/training.rst
src/transformers/activations.py
src/transformers/activations_tf.py
src/transformers/benchmark/benchmark_args.py
src/transformers/benchmark/benchmark_args_tf.py
src/transformers/benchmark/benchmark_args_utils.py
src/transformers/benchmark/benchmark_utils.py
src/transformers/commands/convert.py
src/transformers/commands/serving.py
src/transformers/configuration_albert.py
src/transformers/configuration_auto.py
src/transformers/configuration_bart.py
src/transformers/configuration_bert.py
src/transformers/configuration_bert_generation.py
src/transformers/configuration_blenderbot.py
src/transformers/configuration_camembert.py
src/transformers/configuration_ctrl.py
src/transformers/configuration_deberta.py
src/transformers/configuration_distilbert.py
src/transformers/configuration_dpr.py
src/transformers/configuration_electra.py
src/transformers/configuration_encoder_decoder.py
src/transformers/configuration_flaubert.py
src/transformers/configuration_fsmt.py
src/transformers/configuration_funnel.py
src/transformers/configuration_gpt2.py
src/transformers/configuration_layoutlm.py
src/transformers/configuration_longformer.py
src/transformers/configuration_lxmert.py
src/transformers/configuration_marian.py
src/transformers/configuration_mbart.py
src/transformers/configuration_mobilebert.py
src/transformers/configuration_openai.py
src/transformers/configuration_pegasus.py
src/transformers/configuration_prophetnet.py
src/transformers/configuration_rag.py
src/transformers/configuration_reformer.py
src/transformers/configuration_retribert.py
src/transformers/configuration_roberta.py
src/transformers/configuration_squeezebert.py
src/transformers/configuration_t5.py
src/transformers/configuration_transfo_xl.py
src/transformers/configuration_utils.py
src/transformers/configuration_xlm.py
src/transformers/configuration_xlm_prophetnet.py
src/transformers/configuration_xlm_roberta.py
src/transformers/configuration_xlnet.py
src/transformers/convert_bert_original_tf2_checkpoint_to_pytorch.py
src/transformers/convert_bert_pytorch_checkpoint_to_original_tf.py
src/transformers/convert_graph_to_onnx.py
src/transformers/convert_marian_tatoeba_to_pytorch.py
src/transformers/convert_marian_to_pytorch.py
src/transformers/convert_slow_tokenizer.py
src/transformers/data/data_collator.py
src/transformers/data/datasets/glue.py
src/transformers/data/datasets/language_modeling.py
src/transformers/data/datasets/squad.py
src/transformers/data/metrics/squad_metrics.py
src/transformers/data/processors/glue.py
src/transformers/data/processors/squad.py
src/transformers/data/processors/utils.py
src/transformers/data/processors/xnli.py
src/transformers/file_utils.py
src/transformers/generation_tf_utils.py
src/transformers/generation_utils.py
src/transformers/hf_api.py
src/transformers/hf_argparser.py
src/transformers/integrations.py
src/transformers/modelcard.py
src/transformers/modeling_albert.py
src/transformers/modeling_auto.py
src/transformers/modeling_bart.py
src/transformers/modeling_bert.py
src/transformers/modeling_bert_generation.py
src/transformers/modeling_blenderbot.py
src/transformers/modeling_camembert.py
src/transformers/modeling_ctrl.py
src/transformers/modeling_deberta.py
src/transformers/modeling_distilbert.py
src/transformers/modeling_dpr.py
src/transformers/modeling_electra.py
src/transformers/modeling_encoder_decoder.py
src/transformers/modeling_flaubert.py
src/transformers/modeling_flax_auto.py
src/transformers/modeling_flax_bert.py
src/transformers/modeling_flax_roberta.py
src/transformers/modeling_flax_utils.py
src/transformers/modeling_fsmt.py
src/transformers/modeling_funnel.py
src/transformers/modeling_gpt2.py
src/transformers/modeling_layoutlm.py
src/transformers/modeling_longformer.py
src/transformers/modeling_lxmert.py
src/transformers/modeling_marian.py
src/transformers/modeling_mbart.py
src/transformers/modeling_mmbt.py
src/transformers/modeling_mobilebert.py
src/transformers/modeling_openai.py
src/transformers/modeling_outputs.py
src/transformers/modeling_pegasus.py
src/transformers/modeling_prophetnet.py
src/transformers/modeling_rag.py
src/transformers/modeling_reformer.py
src/transformers/modeling_retribert.py
src/transformers/modeling_roberta.py
src/transformers/modeling_squeezebert.py
src/transformers/modeling_t5.py
src/transformers/modeling_tf_albert.py
src/transformers/modeling_tf_auto.py
src/transformers/modeling_tf_bart.py
src/transformers/modeling_tf_bert.py
src/transformers/modeling_tf_camembert.py
src/transformers/modeling_tf_ctrl.py
src/transformers/modeling_tf_distilbert.py
src/transformers/modeling_tf_electra.py
src/transformers/modeling_tf_flaubert.py
src/transformers/modeling_tf_funnel.py
src/transformers/modeling_tf_gpt2.py
src/transformers/modeling_tf_longformer.py
src/transformers/modeling_tf_lxmert.py
src/transformers/modeling_tf_mobilebert.py
src/transformers/modeling_tf_openai.py
src/transformers/modeling_tf_outputs.py
src/transformers/modeling_tf_pytorch_utils.py
src/transformers/modeling_tf_roberta.py
src/transformers/modeling_tf_t5.py
src/transformers/modeling_tf_transfo_xl.py
src/transformers/modeling_tf_transfo_xl_utilities.py
src/transformers/modeling_tf_utils.py
src/transformers/modeling_tf_xlm.py
src/transformers/modeling_tf_xlm_roberta.py
src/transformers/modeling_tf_xlnet.py
src/transformers/modeling_transfo_xl.py
src/transformers/modeling_transfo_xl_utilities.py
src/transformers/modeling_utils.py
src/transformers/modeling_xlm.py
src/transformers/modeling_xlm_prophetnet.py
src/transformers/modeling_xlm_roberta.py
src/transformers/modeling_xlnet.py
src/transformers/optimization.py
src/transformers/optimization_tf.py
src/transformers/pipelines.py
src/transformers/retrieval_rag.py
src/transformers/testing_utils.py
src/transformers/tokenization_albert.py
src/transformers/tokenization_albert_fast.py
src/transformers/tokenization_auto.py
src/transformers/tokenization_bart.py
src/transformers/tokenization_bart_fast.py
src/transformers/tokenization_bert.py
src/transformers/tokenization_bert_fast.py
src/transformers/tokenization_bert_japanese.py
src/transformers/tokenization_bertweet.py
src/transformers/tokenization_blenderbot.py
src/transformers/tokenization_camembert.py
src/transformers/tokenization_camembert_fast.py
src/transformers/tokenization_ctrl.py
src/transformers/tokenization_deberta.py
src/transformers/tokenization_dpr.py
src/transformers/tokenization_dpr_fast.py
src/transformers/tokenization_flaubert.py
src/transformers/tokenization_fsmt.py
src/transformers/tokenization_funnel.py
src/transformers/tokenization_funnel_fast.py
src/transformers/tokenization_gpt2.py
src/transformers/tokenization_herbert.py
src/transformers/tokenization_herbert_fast.py
src/transformers/tokenization_layoutlm_fast.py
src/transformers/tokenization_longformer.py
src/transformers/tokenization_mbart.py
src/transformers/tokenization_mbart_fast.py
src/transformers/tokenization_openai.py
src/transformers/tokenization_pegasus.py
src/transformers/tokenization_pegasus_fast.py
src/transformers/tokenization_phobert.py
src/transformers/tokenization_prophetnet.py
src/transformers/tokenization_reformer.py
src/transformers/tokenization_reformer_fast.py
src/transformers/tokenization_roberta.py
src/transformers/tokenization_roberta_fast.py
src/transformers/tokenization_squeezebert.py
src/transformers/tokenization_squeezebert_fast.py
src/transformers/tokenization_t5.py
src/transformers/tokenization_t5_fast.py
src/transformers/tokenization_transfo_xl.py
src/transformers/tokenization_utils.py
src/transformers/tokenization_utils_base.py
src/transformers/tokenization_utils_fast.py
src/transformers/tokenization_xlm.py
src/transformers/tokenization_xlm_prophetnet.py
src/transformers/tokenization_xlm_roberta.py
src/transformers/tokenization_xlm_roberta_fast.py
src/transformers/tokenization_xlnet.py
src/transformers/tokenization_xlnet_fast.py
src/transformers/trainer.py
src/transformers/trainer_callback.py
src/transformers/trainer_pt_utils.py
src/transformers/trainer_tf.py
src/transformers/trainer_utils.py
src/transformers/training_args.py
src/transformers/training_args_tf.py
src/transformers/utils/logging.py
src/transformers/utils/notebook.py
utils/style_doc.py
==================
04a17f855;Sylvain Gugger;2020-10-26 15:01:09 -0400;Doc fixes in preparation for the docstyle PR (#8061)
* Fixes in preparation for doc styling

* More fixes

* Better syntax

* Fixes

* Style

* More fixes

* More fixes
==

docs/source/main_classes/processors.rst
docs/source/model_doc/bertgeneration.rst
docs/source/model_doc/blenderbot.rst
docs/source/model_doc/gpt.rst
src/transformers/benchmark/benchmark_utils.py
src/transformers/commands/convert.py
src/transformers/commands/serving.py
src/transformers/commands/train.py
src/transformers/commands/user.py
src/transformers/convert_bert_pytorch_checkpoint_to_original_tf.py
src/transformers/convert_marian_tatoeba_to_pytorch.py
src/transformers/data/data_collator.py
src/transformers/file_utils.py
src/transformers/generation_tf_utils.py
src/transformers/modeling_gpt2.py
src/transformers/modeling_longformer.py
src/transformers/modeling_openai.py
src/transformers/modeling_roberta.py
src/transformers/modeling_tf_gpt2.py
src/transformers/modeling_tf_longformer.py
src/transformers/modeling_tf_openai.py
src/transformers/modeling_tf_roberta.py
src/transformers/modeling_tf_xlnet.py
src/transformers/modeling_xlnet.py
src/transformers/testing_utils.py
src/transformers/tokenization_bertweet.py
src/transformers/tokenization_deberta.py
==================
8bbb74f21;Philip May;2020-10-26 19:48:26 +0100;[Model Card] new cross lingual sentence model for German and English (#8026)
* mc for new cross lingual sentence model

* fat text

* url spelling fix

* more url spelling fixes

* slight thanks change

* small improvements in text

* multilingual word xchange

* change colab link

* xval fold number

* add model links

* line break in model names

* Update README.md

* Update README.md

* new examples link

* new examples link

* add evaluation dataset name

* add more about multi lingual

* typo fix

* typo

* typos

* hyperparameter typos

* hyperparameter typo

* add metadata

* add metadata

* Update README.md

* typo fix

* Small improvement
==

model_cards/T-Systems-onsite/bert-german-dbmdz-uncased-sentence-stsb/README.md
model_cards/T-Systems-onsite/cross-en-de-roberta-sentence-transformer/README.md
model_cards/T-Systems-onsite/german-roberta-sentence-transformer-v2/README.md
==================
3a1076457;Lysandre Debut;2020-10-26 18:39:25 +0000;Fix TF training arguments instantiation (#8063)

==

src/transformers/training_args.py
==================
bc9332b54;Sam Shleifer;2020-10-26 13:53:27 -0400;[TF] from_pt should respect authorized_unexpected_keys (#8056)

==

src/transformers/modeling_tf_pytorch_utils.py
==================
7ff7c4934;Stas Bekman;2020-10-26 10:19:10 -0700;fixing crash (#8057)

==

src/transformers/modeling_deberta.py
==================
cbad90d86;Lysandre Debut;2020-10-26 16:32:27 +0000;Fix + Test (#8049)

==

src/transformers/tokenization_blenderbot.py
tests/test_tokenization_blenderbot.py
==================
664c7ec45;Patrick von Platen;2020-10-26 17:28:16 +0100;[Seq2Seq Trainer] Make sure padding is implemented for models without pad_token (#8043)
* make sure padding is implemented for non-padding tokens models as well

* add better error message

* add better warning

* remove results files

* Update examples/seq2seq/seq2seq_trainer.py

* remove unnecessary copy line

* correct usage of labels

* delete test files
==

examples/seq2seq/seq2seq_trainer.py
examples/seq2seq/test_finetune_trainer.py
==================
098ddc224;mohammadreza-Banaei73;2020-10-26 17:00:18 +0100;Update README.md (#8050)
--wwm cant be used as an argument given run_language_modeling.py and should be changed to --whole_word_mask
==

examples/language-modeling/README.md
==================
fbcddb854;Joe Davison;2020-10-26 11:07:51 -0400;add mutliclass field to default zero shot example

==

model_cards/facebook/bart-large-mnli/README.md
==================
a9ac1db27;Yusuke Mori;2020-10-27 00:05:16 +0900;Minor error fix of 'bart-large-cnn' details in the pretrained_models doc (#8053)

==

docs/source/pretrained_models.rst
==================
fc2d6eac3;Samuel;2020-10-26 14:22:29 +0000;Minor typo fixes to the preprocessing tutorial in the docs (#8046)
* Fix minor typos

Fix minor typos in the docs.

* Update docs/source/preprocessing.rst

Clearer data structure description.

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

docs/source/preprocessing.rst
==================
b0a907615;Joe Davison;2020-10-26 08:04:20 -0600;minor model card description updates (#8051)

==

model_cards/joeddav/xlm-roberta-large-xnli/README.md
==================
c48b16b8d;noise-field;2020-10-26 16:41:58 +0300;Mlflow integration callback (#8016)
* Add MLflow integration class

Add integration code for MLflow in integrations.py along with the code
that checks that MLflow is installed.

* Add MLflowCallback import

Add import of MLflowCallback in trainer.py

* Handle model argument

Allow the callback to handle model argument and store model config items as hyperparameters.

* Log parameters to MLflow in batches

MLflow cannot log more than a hundred parameters at once.
Code added to split the parameters into batches of 100 items and log the batches one by one.

* Fix style

* Add docs on MLflow callback

* Fix issue with unfinished runs

The "fluent" api used in MLflow integration allows only one run to be active at any given moment. If the Trainer is disposed off and a new one is created, but the training is not finished, it will refuse to log the results when the next trainer is created.

* Add MLflow integration class

Add integration code for MLflow in integrations.py along with the code
that checks that MLflow is installed.

* Add MLflowCallback import

Add import of MLflowCallback in trainer.py

* Handle model argument

Allow the callback to handle model argument and store model config items as hyperparameters.

* Log parameters to MLflow in batches

MLflow cannot log more than a hundred parameters at once.
Code added to split the parameters into batches of 100 items and log the batches one by one.

* Fix style

* Add docs on MLflow callback

* Fix issue with unfinished runs

The "fluent" api used in MLflow integration allows only one run to be active at any given moment. If the Trainer is disposed off and a new one is created, but the training is not finished, it will refuse to log the results when the next trainer is created.
==

docs/source/main_classes/callback.rst
src/transformers/integrations.py
src/transformers/trainer.py
==================
8be9cb0ae;Lysandre Debut;2020-10-26 13:29:56 +0000;Tiny TF Bart fixes (#8023)

==

src/transformers/modeling_tf_bart.py
==================
077478637;Sylvain Gugger;2020-10-26 09:23:12 -0400;Fix label name in DataCollatorForNextSentencePrediction test (#8048)

==

tests/test_data_collator.py
==================
8bbe8247f;Sam Shleifer;2020-10-26 08:59:06 -0400;Cleanup pytorch tests (#8033)

==

tests/test_modeling_marian.py
tests/test_modeling_mbart.py
tests/test_modeling_pegasus.py
==================
20a0894d1;suliuzh;2020-10-26 20:56:56 +0800;update version for scipy (#7998)

==

examples/distillation/requirements.txt
==================
f20aec1de;Sam Shleifer;2020-10-26 08:32:36 -0400;fsmt slow test uses lists (#8031)

==

tests/test_tokenization_fsmt.py
==================
101186bc1;Stas Bekman;2020-10-26 05:15:05 -0700;[docs] [testing] distributed training (#7993)
* distributed training

* fix

* fix formatting

* wording
==

docs/source/testing.rst
==================
c153bcc5c;luyug;2020-10-26 08:12:31 -0400;Add mixed precision evaluation (#8036)
* Add mixed precision evaluation

* use original flag
==

src/transformers/trainer.py
==================
9aa282668;Samuel;2020-10-26 12:08:33 +0000;Minor typo fixes to the tokenizer summary (#8045)
Minor typo fixes to the tokenizer summary
==

docs/source/tokenizer_summary.rst
==================
829b9f8cc;Lysandre;2020-10-26 08:05:02 -0400;Remove codecov.yml

==

codecov.yml
==================
79eb39158;Thomas Wolf;2020-10-26 10:27:48 +0100;[tokenizers] Fixing #8001 - Adding tests on tokenizers serialization (#8006)
* fixing #8001

* make T5 tokenizer serialization more robust - style
==

src/transformers/tokenization_albert.py
src/transformers/tokenization_bert.py
src/transformers/tokenization_bertweet.py
src/transformers/tokenization_deberta.py
src/transformers/tokenization_fsmt.py
src/transformers/tokenization_gpt2.py
src/transformers/tokenization_marian.py
src/transformers/tokenization_prophetnet.py
src/transformers/tokenization_t5.py
src/transformers/tokenization_t5_fast.py
src/transformers/tokenization_transfo_xl.py
src/transformers/tokenization_utils_base.py
src/transformers/tokenization_utils_fast.py
src/transformers/tokenization_xlm.py
src/transformers/tokenization_xlm_prophetnet.py
src/transformers/tokenization_xlnet.py
tests/test_tokenization_common.py
==================
7087d9b1c;Julien Chaumond;2020-10-26 09:38:21 +0100;[model_cards] bert-base-danish Fixup
#8030

==

model_cards/DJSammy/bert-base-danish-uncased_BotXO,ai/README.md
==================
efc4a21ff;Julien Chaumond;2020-10-26 09:32:07 +0100;Fixup #8025
Close #8030

==

model_cards/DJSammy/bert-base-danish-uncased_BotXO,ai/README.md
==================
5148f4330;Sam Longenbach;2020-10-25 03:20:46 -0400;[Model Card] DJSammy/bert-base-danish-uncased_BotXO,ai (#8025)
* Create README.md

* Update README.md
==

model_cards/DJSammy/bert-base-danish-uncased_BotXO,ai/README.md
==================
38f6739cd;Suraj Patil;2020-10-25 01:03:47 +0530;[doc prepare_seq2seq_batch] fix docs (#8013)

==

src/transformers/tokenization_bart.py
src/transformers/tokenization_utils_base.py
==================
00602f784;Yixin Nie;2020-10-24 03:16:07 -0400;Create model card for pre-trained NLI models. (#7864)
* Create README.md

* Update model_cards/ynie/roberta-large-snli_mnli_fever_anli_R1_R2_R3-nli/README.md

Co-authored-by: Julien Chaumond <chaumond@gmail.com>

* Add Meta information for dataset identifier.

Co-authored-by: Julien Chaumond <chaumond@gmail.com>
==

model_cards/ynie/roberta-large-snli_mnli_fever_anli_R1_R2_R3-nli/README.md
==================
3c682ea15;Patrick von Platen;2020-10-23 23:05:51 +0200;[Examples] Allow EncoderDecoderModels to be trained with Seq2Seq (#7809)
* Make Seq2Seq Trainer more similar to Trainer

* fix typo

* fix seq2seq trainer

* remove from tests

* remove lock

* remove train files

* delete test files

* correct typo

* check at init

* make sure trainer is not slowed down on TPU

* correct isort

* remove use cache

* fix use cache

* add last use chache = false
==

examples/seq2seq/finetune_trainer.py
examples/seq2seq/seq2seq_trainer.py
examples/seq2seq/test_finetune_trainer.py
==================
59b5953d8;Sacha Arbonel;2020-10-23 16:58:05 +0200;Create model card for bert-italian-cased-finetuned-pos (#8003)
* Create README.md

* Update model_cards/sachaarbonel/bert-italian-cased-finetuned-pos/README.md

* Apply suggestions from code review

Co-authored-by: Julien Chaumond <chaumond@gmail.com>
==

model_cards/sachaarbonel/bert-italian-cased-finetuned-pos/README.md
==================
6e07c1f44;Zhiqi Huang;2020-10-23 22:53:53 +0800;Add model cards for DynaBERT (#7999)

==
==================
43fdafef8;Zhiqi Huang;2020-10-23 22:53:37 +0800;Create README.md (#7997)

==

model_cards/huawei-noah/DynaBERT_SST-2/README.md
==================
627e81373;Blaise Cruz;2020-10-23 22:52:21 +0800;Added model cards for Tagalog ELECTRA models (#7996)
Co-authored-by: Jan Christian Blaise Cruz <jcblaise@Blaises-MacBook-Pro.local>
==

model_cards/jcblaise/electra-tagalog-base-cased-discriminator/README.md
model_cards/jcblaise/electra-tagalog-base-cased-generator/README.md
model_cards/jcblaise/electra-tagalog-base-uncased-discriminator/README.md
model_cards/jcblaise/electra-tagalog-base-uncased-generator/README.md
model_cards/jcblaise/electra-tagalog-small-cased-discriminator/README.md
model_cards/jcblaise/electra-tagalog-small-cased-generator/README.md
model_cards/jcblaise/electra-tagalog-small-uncased-discriminator/README.md
model_cards/jcblaise/electra-tagalog-small-uncased-generator/README.md
==================
9865e1fe5;Philip May;2020-10-23 16:45:54 +0200;model card for German Sentence Embeddings V2 (#7952)
* model card German Sentence Embeddings V2

- for German RoBERTa for Sentence Embeddings V2
- marked old as outdated

* small correction

* small improvement in description

* small spelling fix

* spelling fix

* add evaluation results

* spearman explanation

* add number of trials
==

model_cards/T-Systems-onsite/bert-german-dbmdz-uncased-sentence-stsb/README.md
model_cards/T-Systems-onsite/german-roberta-sentence-transformer-v2/README.md
==================
d39da5a2a;Ethan Perez;2020-10-23 07:34:06 -0700;Handling longformer model_type (#7990)
Updating the run_squad training script to handle the "longformer" `model_type`. The longformer is trained in the same was as RoBERTa, so I've added the "longformer" `model_type` (that's the right hugginface name for the LongFormer model, right?) everywhere there was a "roberta" `model_type` reference. The longformer (like RoBERTa) doesn't use `token_type_ids` (as I understand from looking at the [longformer notebook](https://github.com/patil-suraj/Notebooks/blob/master/longformer_qa_training.ipynb), which is what gets updated after this change.

This fix might be related to [this issue](https://github.com/huggingface/transformers/issues/7249) with SQuAD training when using run_squad.py
==

examples/question-answering/run_squad.py
==================
5e323017a;Anthony MOI;2020-10-23 10:29:37 -0400;Fix BatchEncoding.word_to_tokens for removed tokens (#7939)

==

src/transformers/tokenization_utils_base.py
tests/test_tokenization_utils.py
==================
4acfd1a8d;Patrick von Platen;2020-10-23 16:29:15 +0200;[Reformer] remove reformer pad_token_id (#7991)
* remove reformer pad_token_id

* fix pegasus
==

src/transformers/tokenization_pegasus.py
src/transformers/tokenization_reformer.py
src/transformers/tokenization_reformer_fast.py
tests/test_tokenization_reformer.py
==================
3a40cdf58;Thomas Wolf;2020-10-23 15:58:19 +0200;[tests|tokenizers] Refactoring pipelines test backbone - Small tokenizers improvements - General tests speedups (#7970)
* WIP refactoring pipeline tests - switching to fast tokenizers

* fix dialog pipeline and fill-mask

* refactoring pipeline tests backbone

* make large tests slow

* fix tests (tf Bart inactive for now)

* fix doc...

* clean up for merge

* fixing tests - remove bart from summarization until there is TF

* fix quality and RAG

* Add new translation pipeline tests - fix JAX tests

* only slow for dialog

* Fixing the missing TF-BART imports in modeling_tf_auto

* spin out pipeline tests in separate CI job

* adding pipeline test to CI YAML

* add slow pipeline tests

* speed up tf and pt join test to avoid redoing all the standalone pt and tf tests

* Update src/transformers/tokenization_utils_base.py

Co-authored-by: Sam Shleifer <sshleifer@gmail.com>

* Update src/transformers/pipelines.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/pipelines.py

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* Update src/transformers/testing_utils.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* add require_torch and require_tf in is_pt_tf_cross_test

Co-authored-by: Sam Shleifer <sshleifer@gmail.com>
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
==

.circleci/config.yml
.github/workflows/self-push.yml
.github/workflows/self-scheduled.yml
src/transformers/convert_slow_tokenizers_checkpoints_to_fast.py
src/transformers/data/processors/squad.py
src/transformers/modeling_tf_auto.py
src/transformers/modeling_tf_utils.py
src/transformers/modeling_utils.py
src/transformers/pipelines.py
src/transformers/testing_utils.py
src/transformers/tokenization_utils.py
src/transformers/tokenization_utils_base.py
src/transformers/tokenization_utils_fast.py
tests/conftest.py
tests/test_modeling_tf_bart.py
tests/test_modeling_tf_common.py
tests/test_modeling_tf_pytorch.py
tests/test_pipelines.py
tests/test_pipelines_common.py
tests/test_pipelines_conversational.py
tests/test_pipelines_dialog.py
tests/test_pipelines_feature_extraction.py
tests/test_pipelines_fill_mask.py
tests/test_pipelines_ner.py
tests/test_pipelines_question_answering.py
tests/test_pipelines_sentiment_analysis.py
tests/test_pipelines_summarization.py
tests/test_pipelines_text2text_generation.py
tests/test_pipelines_text_generation.py
tests/test_pipelines_translation.py
tests/test_pipelines_zero_shot.py
tests/test_tokenization_common.py
==================
88b3a91e6;Lalit Pagaria;2020-10-23 15:54:45 +0200;Handle the case when title is None (#7941)

==

examples/rag/use_own_knowledge_dataset.py
==================
023f0f370;Stas Bekman;2020-10-22 14:26:22 -0700;[s2s trainer] tests to use distributed on multi-gpu machine (#7965)

==

examples/seq2seq/test_finetune_trainer.py
examples/seq2seq/test_seq2seq_examples_multi_gpu.py
examples/seq2seq/utils.py
==================
64b24bb3c;Joe Davison;2020-10-22 15:19:41 -0600;change zero shot widget default example (#7992)

==

model_cards/facebook/bart-large-mnli/README.md
==================
0397619ac;Sam Shleifer;2020-10-22 16:13:49 -0400;Move NoLayerEmbedTokens (#7945)
* Move NoLayerEmbedTokens

* TFWrappedEmbeddings

* Add comment
==

src/transformers/modeling_tf_bart.py
src/transformers/modeling_tf_t5.py
src/transformers/modeling_tf_utils.py
==================
5ac07513e;Sam Shleifer;2020-10-22 16:10:15 -0400;[gh ci] less output ( --durations=50) (#7989)

==

.github/workflows/self-scheduled.yml
==================
5ae935d23;Sylvain Gugger;2020-10-22 15:48:52 -0400;Reload checkpoint (#7984)
* Fix checkpoint loading in Trainer

* Fix typo
==

src/transformers/trainer.py
src/transformers/trainer_callback.py
src/transformers/trainer_pt_utils.py
==================
467573ddd;Lysandre;2020-10-22 15:37:51 -0400;Fix documentation redirect

==

.circleci/deploy.sh
==================
077c99bb5;Joe Davison;2020-10-22 13:01:23 -0600;add zero shot pipeline tags & examples (#7983)
* add zero shot pipeline tags

* rm default and fix yaml format

* rm DS_Store

* add bart large default

* don't add more typos

Co-authored-by: Julien Chaumond <chaumond@gmail.com>

* add multiple multilingual examples

* improve multilingual examples for single-label

Co-authored-by: Julien Chaumond <chaumond@gmail.com>
==

model_cards/facebook/bart-large-mnli/README.md
model_cards/joeddav/bart-large-mnli-yahoo-answers/README.md
model_cards/joeddav/xlm-roberta-large-xnli/README.md
model_cards/valhalla/distilbart-mnli-12-1/README.md
model_cards/valhalla/distilbart-mnli-12-3/README.md
model_cards/valhalla/distilbart-mnli-12-6/README.md
model_cards/valhalla/distilbart-mnli-12-9/README.md
==================
06fc3954a;Sylvain Gugger;2020-10-22 14:26:55 -0400;Only log total_flos at the end of training (#7981)
* Only log total_flos at the end of training

* Fix test
==

src/transformers/trainer.py
src/transformers/trainer_utils.py
tests/test_trainer_callback.py
==================
ff65beafa;Julien Chaumond;2020-10-22 18:54:25 +0200;FillMaskPipeline: support passing top_k on __call__ (#7971)
* FillMaskPipeline: support passing top_k on __call__

Also move from topk to top_k

* migrate to new param name in tests

* Review from @sgugger
==

src/transformers/pipelines.py
tests/test_pipelines.py
==================
2e5052d4f;Sylvain Gugger;2020-10-22 11:42:22 -0400;New run glue script (#7917)
* Start simplification

* More progress

* Finished script

* Address comments and update tests instructions

* Wrong test

* Accept files as inputs and fix test

* Update src/transformers/trainer_utils.py

Co-authored-by: Julien Chaumond <chaumond@gmail.com>

* Fix labels and add combined score

* Add special labels

* Update TPU command

* Revert to old label strategy

* Use model labels

* Fix for STT-B

* Styling

* Apply suggestions from code review

Co-authored-by: Thomas Wolf <thomwolf@users.noreply.github.com>

* Code styling

* Fix review comments

Co-authored-by: Julien Chaumond <chaumond@gmail.com>
Co-authored-by: Thomas Wolf <thomwolf@users.noreply.github.com>
==

examples/test_examples.py
examples/test_xla_examples.py
examples/text-classification/README.md
examples/text-classification/run_glue.py
src/transformers/trainer.py
src/transformers/trainer_utils.py
tests/fixtures/tests_samples/MRPC/dev.csv
tests/fixtures/tests_samples/MRPC/train.csv
==================
18ce6b8ff;Nicolas Patry;2020-10-22 17:16:21 +0200;Fixing the "translation", "translation_XX_to_YY" pipelines. (#7975)
* Actually make the "translation", "translation_XX_to_YY" task behave correctly.

Background:
- Currently "translation_cn_to_ar" does not work. (only 3 pairs are
supported)
- Some models, contain in their config the correct values for the (src,
tgt) pair they can translate. It's usually just one pair, and we can
infer it automatically from the `model.config.task_specific_params`. If
it's not defined we can still probably load the TranslationPipeline
nevertheless.

Proposed fix:
- A simplified version of what could become more general which is
a `parametrized` task. "translation" + (src, tgt) in this instance
it what we need in the general case. The way we go about it for now
is simply parsing "translation_XX_to_YY". If cases of parametrized task arise
we should preferably go in something closer to what `datasets` propose
which is having a secondary argument `task_options`? that will be close
to what that task requires.
- Should be backward compatible in all cases for instance
`pipeline(task="translation_en_to_de") should work out of the box.
- Should provide a warning when a specific translation pair has been
selected on behalf of the user using
`model.config.task_specific_params`.

* Update src/transformers/pipelines.py

Co-authored-by: Julien Chaumond <chaumond@gmail.com>

Co-authored-by: Julien Chaumond <chaumond@gmail.com>
==

src/transformers/pipelines.py
tests/test_pipelines.py
==================
901e9b8ed;Funtowicz Morgan;2020-10-22 16:41:41 +0200;Remove the else branch adding 0 to the hidden state if token_type_embeds is None. (#7977)
Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>
==

src/transformers/modeling_gpt2.py
==================
f34372a9f;Patrick von Platen;2020-10-22 15:39:01 +0200;[PretrainedConfig] Fix save pretrained config for edge case  (#7943)
* fix config save

* add test

* add config class variable and another test

* line break

* fix fsmt and typo

* god am I making many errors today :-/

* Update src/transformers/configuration_utils.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

src/transformers/configuration_encoder_decoder.py
src/transformers/configuration_fsmt.py
src/transformers/configuration_rag.py
src/transformers/configuration_utils.py
tests/test_configuration_common.py
tests/test_modeling_prophetnet.py
==================
cc2e312ca;Peter Bayerle;2020-10-22 06:30:50 -0700;adding text classification with DistilBERT/tf notebook (#7964)
Looking at the current community notebooks, it seems that few are targeted for absolute beginners and even fewer are written with TensorFlow. This notebook describes absolutely everything a beginner would need to know, including how to save/load their model and use it for new predictions (this is often omitted in tutorials)

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
==

notebooks/README.md
==================
a16e568f2;wlhgtc;2020-10-22 21:19:00 +0800;# Add whole word mask support for lm fine-tune (#7925)
* ADD: add whole word mask proxy for both eng and chinese

* MOD: adjust format

* MOD: reformat code

* MOD: update import

* MOD: fix bug

* MOD: add import

* MOD: fix bug

* MOD: decouple code and update readme

* MOD: reformat code

* Update examples/language-modeling/README.md

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update examples/language-modeling/README.md

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update examples/language-modeling/run_language_modeling.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update examples/language-modeling/run_language_modeling.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update examples/language-modeling/run_language_modeling.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update examples/language-modeling/run_language_modeling.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* change wwm to whole_word_mask

* reformat code

* reformat

* format

* Code quality

* ADD: update chinese ref readme

* MOD: small changes

* MOD: small changes2

* update readme

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
Co-authored-by: Sylvain Gugger <sylvain.gugger@gmail.com>
==

examples/language-modeling/README.md
examples/language-modeling/chinese_ref.py
examples/language-modeling/run_language_modeling.py
src/transformers/__init__.py
src/transformers/data/data_collator.py
src/transformers/data/datasets/__init__.py
src/transformers/data/datasets/language_modeling.py
src/transformers/utils/dummy_pt_objects.py
==================
64b4d25cf;Stas Bekman;2020-10-22 06:14:54 -0700;[fsmt test] basic config test with online model + super tiny model (#7860)
* basic config test with online model

* typo

* style

* better test
==

scripts/fsmt/fsmt-make-super-tiny-model.py
scripts/fsmt/fsmt-make-tiny-model.py
tests/test_tokenization_fsmt.py
==================
3479787ed;Julien Chaumond;2020-10-22 15:08:37 +0200;Disable inference API for t5-11b (#7978)

==

model_cards/t5-11b-README.md
==================
a7db81c33;Julien Chaumond;2020-10-22 14:35:31 +0200;[model_card] t5-11b move disclaimer to top of page
cc @Narsil @patrickvonplaten

==

model_cards/t5-11b-README.md
==================
f774b2e8c;Haebin Shin;2020-10-22 20:55:31 +0900;support relative path for best_model_checkpoint (#7973)

==

src/transformers/trainer.py
==================
834810569;Stas Bekman;2020-10-22 03:34:05 -0700;[testing] slow tests should be marked as slow (#7895)
* slow tests should be slow

* exception note

* style

* integrate LysandreJik's notes with some expansions

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* another slow test

* fix link, and prose

* clarify.

* note from Sam

* typo

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

docs/source/testing.rst
tests/test_modeling_marian.py
tests/test_pipelines.py
tests/test_tokenization_auto.py
==================
95792a948;rmroczkowski;2020-10-22 11:48:29 +0200;Herbert tokenizer auto load (#7968)

==

src/transformers/tokenization_auto.py
==================
4abb7ffc1;zolekode;2020-10-22 11:02:12 +0200;added qg evaluation notebook (#7958)
* added qg evaluation notebook

* Update notebooks/README.md

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
==

notebooks/README.md
==================
8b3817339;Stas Bekman;2020-10-21 14:20:53 -0700;[seq2seq testing] multigpu test run via subprocess (#7281)
Co-authored-by: Sam Shleifer <sshleifer@gmail.com>
==

examples/lightning_base.py
examples/seq2seq/distillation.py
examples/seq2seq/finetune.py
examples/seq2seq/finetune_trainer.py
examples/seq2seq/test_seq2seq_examples_multi_gpu.py
examples/seq2seq/utils.py
==================
f8d3695e8;Julien Chaumond;2020-10-21 14:17:56 -0400;[model_cards] camembert: dataset = oscar
Hat/tip @pjox
==

model_cards/camembert-base-README.md
==================
16da87713;Evan Pete Walsh;2020-10-21 10:57:44 -0700;fix 'encode_plus' docstring for 'special_tokens_mask' (0s and 1s were reversed) (#7949)
* fix docstring for 'special_tokens_mask'

* revert auto formatter changes

* revert another auto format

* revert another auto format
==

src/transformers/tokenization_utils_base.py
==================
52decab37;Patrick von Platen;2020-10-21 19:06:23 +0200;fix test (#7947)

==

tests/test_modeling_gpt2.py
==================
9b6610f7f;Patrick von Platen;2020-10-21 17:27:20 +0200;[ProphetNet] Correct Doc string example (#7944)
* correct xlm prophetnet auto model and examples

* fix line-break docs
==

src/transformers/modeling_auto.py
src/transformers/modeling_prophetnet.py
src/transformers/modeling_xlm_prophetnet.py
==================
e174bfeb3;Fran√ßois Lagunas;2020-10-21 17:18:52 +0200;TensorBoard/Wandb/optuna/raytune integration improvements. (#7935)
Improved TensorBoard and Wandb integration, as well as optuna and ray/tune support, with minor modifications to trainer core code.

==

src/transformers/integrations.py
src/transformers/testing_utils.py
src/transformers/trainer.py
src/transformers/trainer_callback.py
src/transformers/trainer_utils.py
src/transformers/utils/hp_naming.py
tests/test_trainer.py
==================
bf162ce8c;Ali Hamdi Ali Fadel;2020-10-21 16:24:43 +0300;Add AI-SOCO models (#7867)

==

model_cards/aliosm/ai-soco-c++-roberta-small-clas/README.md
model_cards/aliosm/ai-soco-c++-roberta-small/README.md
model_cards/aliosm/ai-soco-c++-roberta-tiny-96-clas/README.md
model_cards/aliosm/ai-soco-c++-roberta-tiny-96/README.md
model_cards/aliosm/ai-soco-c++-roberta-tiny-clas/README.md
model_cards/aliosm/ai-soco-c++-roberta-tiny/README.md
==================
58fb25f25;Fangyu Liu;2020-10-21 13:41:41 +0100;Create README.md (#7857)
* Create README.md

model card for cambridgeltl/BioRedditBERT-uncased.

* Update model_cards/cambridgeltl/BioRedditBERT-uncased/README.md

Co-authored-by: Julien Chaumond <chaumond@gmail.com>
==

model_cards/cambridgeltl/BioRedditBERT-uncased/README.md
==================
2b07ec782;Manuel Romero;2020-10-21 14:31:41 +0200;Model card for German BERT fine-tuned for LER/NER (#7855)

==

model_cards/mrm8488/bert-base-german-finetuned-ler/README.md
==================
35d2ad5b8;MichalPleban;2020-10-21 14:30:01 +0200;Create README.md (#7819)

==

model_cards/Michau/t5-base-en-generate-headline/README.md
==================
bdda4f224;Wuwei Lan;2020-10-21 08:29:39 -0400;Create README.md (#7625)
* Create README.md

* Update model_cards/lanwuwei/GigaBERT-v3-Arabic-and-English/README.md

* Update model_cards/lanwuwei/GigaBERT-v3-Arabic-and-English/README.md

Co-authored-by: Julien Chaumond <chaumond@gmail.com>
==

model_cards/lanwuwei/GigaBERT-v3-Arabic-and-English/README.md
==================
8e2374964;Manuel Romero;2020-10-21 14:24:12 +0200;Add missing comma (#7870)

==

model_cards/mrm8488/t5-base-finetuned-question-generation-ap/README.md
==================
3eaa007d7;Manuel Romero;2020-10-21 14:23:55 +0200;Create README.md (#7899)

==

model_cards/mrm8488/t5-base-finetuned-common_gen/README.md
==================
758572cad;Julien Chaumond;2020-10-21 14:13:17 +0200;[model_cards] move hatmimoha/arabic-ner to correct location
see https://github.com/huggingface/transformers/commit/16d3cc187ded95946231956460e9004a236474e2 and https://github.com/huggingface/transformers/pull/7836

==

model_cards/hatmimoha/arabic-ner/README.md
==================
57516c0cc;Stas Bekman;2020-10-21 05:06:07 -0700;[multiple models] skip saving/loading deterministic state_dict keys (#7878)
* make the save_load special key tests common

* handle mbart

* cleaner solution

* fix

* move test_save_load_missing_keys back into fstm for now

* restore

* style

* add marian

* add pegasus

* blenderbot

* revert - no static embed
==

src/transformers/modeling_marian.py
src/transformers/modeling_mbart.py
src/transformers/modeling_pegasus.py
tests/test_modeling_common.py
tests/test_modeling_fsmt.py
tests/test_modeling_marian.py
tests/test_modeling_mbart.py
tests/test_modeling_pegasus.py
==================
006a16483;quentinheinrich;2020-10-21 14:05:53 +0200;update model cards of Illuin models (#7930)

==

model_cards/illuin/camembert-base-fquad/README.md
model_cards/illuin/camembert-large-fquad/README.md
model_cards/illuin/lepetit/README.md
==================
16d3cc187;hatmimoha;2020-10-21 08:02:40 -0400;model card for arabic-ner model (#7836)
* Create README.md

README file for the Arabic NER model

* Update README.md

* Update README.md

* Update hatmimoha/arabic-ner/README.md

Co-authored-by: Julien Chaumond <chaumond@gmail.com>
==

hatmimoha/arabic-ner/README.md
==================
829842159;Sam Shleifer;2020-10-21 07:10:16 -0400;Add TFBartForConditionalGeneration (#5411)
* half done

* doc improvement

* Cp test file

* brokedn

* broken test

* undo some mess

* ckpt

* borked

* Halfway

* 6 passing

* boom boom

* Much progress but still 6

* boom boom

* merged master

* 10 passing

* boom boom

* Style

* no t5 changes

* 13 passing

* Integration test failing, but not gibberish

* Frustrated

* Merged master

* 4 fail

* 4 fail

* fix return_dict

* boom boom

* Still only 4

* prepare method

* prepare method

* before delete classif

* Skip tests to avoid adding boilerplate

* boom boom

* fast tests passing

* style

* boom boom

* Switch to supporting many input types

* remove FIXMENORM

* working

* Fixed past_key_values/decoder_cached_states confusion

* new broken test

* Fix attention mask kwarg name

* undo accidental

* Style and reviewers

* style

* Docs and common tests

* Cleaner assert messages

* copy docs

* style issues

* Sphinx fix

* Simplify caching logic

* test does not require torch

* copy _NoLayerEmbedTokens

* Update src/transformers/modeling_tf_bart.py

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* Update tests/test_modeling_tf_bart.py

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* Update src/transformers/modeling_tf_bart.py

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* Update src/transformers/modeling_tf_bart.py

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* Update src/transformers/modeling_tf_bart.py

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* Line length and dont document None

* Add pipeline test coverage

* assert msg

* At parity

* Assert messages

* mark slow

* Update compile test

* back in init

* Merge master

* Fix tests

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
==

docs/source/model_doc/bart.rst
src/transformers/__init__.py
src/transformers/convert_pytorch_checkpoint_to_tf2.py
src/transformers/generation_tf_utils.py
src/transformers/modeling_bart.py
src/transformers/modeling_blenderbot.py
src/transformers/modeling_marian.py
src/transformers/modeling_tf_auto.py
src/transformers/modeling_tf_bart.py
src/transformers/modeling_tf_bert.py
src/transformers/modeling_tf_pytorch_utils.py
src/transformers/modeling_tf_t5.py
src/transformers/modeling_tf_utils.py
src/transformers/pipelines.py
src/transformers/utils/dummy_tf_objects.py
tests/test_modeling_bart.py
tests/test_modeling_tf_bart.py
tests/test_modeling_tf_common.py
tests/test_modeling_tf_t5.py
tests/test_pipelines.py
==================
5cd9e2cba;Patrick von Platen;2020-10-21 12:43:42 +0200;Update README.md

==

model_cards/microsoft/xprophetnet-large-wiki100-cased-xglue-ntg/README.md
==================
220b5f97c;Patrick von Platen;2020-10-21 12:34:46 +0200;Create README.md

==

model_cards/microsoft/prophetnet-large-uncased-squad-qg/README.md
==================
8ffd7fb12;Patrick von Platen;2020-10-21 12:27:09 +0200;Update README.md

==

model_cards/microsoft/prophetnet-large-uncased-cnndm/README.md
==================
613ab364e;Patrick von Platen;2020-10-21 12:23:17 +0200;Update README.md

==

model_cards/microsoft/prophetnet-large-uncased/README.md
==================
f7eb17dc4;Patrick von Platen;2020-10-21 12:19:44 +0200;Update README.md

==

model_cards/microsoft/xprophetnet-large-wiki100-cased/README.md
==================
29792864c;Patrick von Platen;2020-10-21 11:49:58 +0200;[ProphetNet] Add Question Generation Model + Test (#7942)
* new prophetnet model

* correct name

* make style
==

tests/test_modeling_prophetnet.py
==================
13842e413;Joe Davison;2020-10-20 16:17:39 -0600;PPL guide minor code snippet fix (#7938)

==

docs/source/perplexity.rst
==================
0e24e4c13;Stas Bekman;2020-10-20 12:07:52 -0700;[s2s] create doc for pegasus/fsmt replication (#7934)

==

examples/seq2seq/README.md
==================
96f4828ac;Lysandre Debut;2020-10-20 17:02:47 +0200;Respect the 119 line chars (#7928)

==

docs/source/model_summary.rst
==================
ef0ac063c;Lysandre;2020-10-20 16:29:00 +0200;Docs for v3.4.0

==

.circleci/deploy.sh
docs/source/_static/js/custom.js
==================
eb0e0ce2a;Lysandre;2020-10-20 16:21:11 +0200;Release: v3.4.0

==

README.md
docs/source/conf.py
docs/source/index.rst
setup.py
src/transformers/__init__.py
==================
026404866;Patrick von Platen;2020-10-20 16:13:49 +0200;Update README.md

==

model_cards/google/roberta2roberta_L-24_discofuse/README.md
==================
ffd675b42;Patrick von Platen;2020-10-20 16:11:02 +0200;add summary (#7927)

==

docs/source/model_summary.rst
==================
5547b40b1;Lysandre Debut;2020-10-20 15:50:47 +0200;labels and decoder_input_ids to Glossary (#7906)
* labels and decoder_input_ids to Glossary

* Formatting fixes

* Update docs/source/glossary.rst

Co-authored-by: Sam Shleifer <sshleifer@gmail.com>

* sam's comments

Co-authored-by: Sam Shleifer <sshleifer@gmail.com>
==

docs/source/glossary.rst
==================
f3312515b;Patrick von Platen;2020-10-20 15:42:29 +0200;Add note for WikiSplit

==

model_cards/google/roberta2roberta_L-24_wikisplit/README.md
==================
0724c0f3a;Patrick von Platen;2020-10-20 15:13:22 +0200;Fix EncoderDecoder WikiSplit Example

==

model_cards/google/roberta2roberta_L-24_wikisplit/README.md
==================
ca37db055;Stas Bekman;2020-10-20 04:55:40 -0700;[flax] fix repo_check (#7914)
* [flax] fix repo_check

Unless, this is actually a problem, this adds `modeling_flax_utils` to ignore list. otherwise currently it expects to have a 'tests/test_modeling_flax_utils.py' for it.
for context please see: https://github.com/huggingface/transformers/pull/3722#issuecomment-712360415

* fix 2 more issues

* merge https://github.com/huggingface/transformers/pull/7919/
==

.circleci/config.yml
utils/check_repo.py
==================
048dd6cf1;Shai Erera;2020-10-20 14:50:47 +0300;Fix bug in _sorted_checkpoints (#7880)
I'm using transformers 3.3.1 and run a training script with `--save_total_limit 3`. I hit the exception below, and after debugging the code found that it wrongly tries to index into the `best_model_checkpoint`'s *str* rather than the `sorted_checkpoints` array. When running without the fix I got this exception:

```
Traceback (most recent call last):
  File "/<HOME>/.conda/envs/transformers/lib/python3.7/site-packages/transformers/trainer.py", line 921, in _save_training
    self._rotate_checkpoints(use_mtime=True)
  File "/<HOME>/.conda/envs/transformers/lib/python3.7/site-packages/transformers/trainer.py", line 1283, in _rotate_checkpoints
    checkpoints_sorted = self._sorted_checkpoints(use_mtime=use_mtime)
  File "/<HOME>/.conda/envs/transformers/lib/python3.7/site-packages/transformers/trainer.py", line 1274, in _sorted_checkpoints
    checkpoints_sorted[best_model_index],
TypeError: 'str' object does not support item assignment
```
==

src/transformers/trainer.py
==================
6d4f8bd02;Sylvain Gugger;2020-10-20 07:45:48 -0400;Add Flax dummy objects (#7918)

==

src/transformers/__init__.py
src/transformers/file_utils.py
src/transformers/utils/dummy_flax_objects.py
utils/check_dummies.py
==================
3e31e7f95;Stas Bekman;2020-10-20 01:39:13 -0700;[testing] rename skip targets + docs (#7863)
* rename skip targets + docs

* fix quotes

* style

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* small improvements

* fix

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

docs/source/testing.rst
examples/seq2seq/test_seq2seq_examples.py
src/transformers/testing_utils.py
templates/adding_a_new_model/tests/test_modeling_xxx.py
tests/test_modeling_common.py
tests/test_modeling_layoutlm.py
tests/test_modeling_reformer.py
tests/test_modeling_transfo_xl.py
tests/test_skip_decorators.py
==================
c912ba5f6;Patrick von Platen;2020-10-19 22:02:42 +0200;[EncoderDecoder] Fix Typo (#7915)
* fix encoder decoder models

* add .gitignore
==

.gitignore
src/transformers/modeling_encoder_decoder.py
==================
55bcd0cb5;Bram Vanroy;2020-10-19 21:59:30 +0200;Raise error when using AMP on non-CUDA device (#7869)
* Raise error when using AMP on non-CUDA device

* make style

* make style
==

src/transformers/training_args.py
==================
e3d2bee8d;Patrick von Platen;2020-10-19 21:49:47 +0200;fix t5 training docstring (#7911)

==

docs/source/model_doc/t5.rst
==================
df1ddcedf;Ayub Subhaniya;2020-10-19 23:18:49 +0530;`decoder_config` used before intialisation (#7903)
Seeing error when sending `decoder_config` as a parameter while initializing a encoder-decoder model from pretrained. 
fixed "UnboundLocalError: local variable 'decoder_config' referenced before assignment"
==

src/transformers/modeling_encoder_decoder.py
==================
033f29c62;Quentin Lhoest;2020-10-19 19:42:45 +0200;Allow Custom Dataset in RAG Retriever (#7763)
* add CustomHFIndex

* typo in config

* update tests

* add custom dataset example

* clean script

* update test data

* minor in test

* docs

* docs

* style

* fix imports

* allow to pass the indexed dataset directly

* update tests

* use multiset DPR

* address thom and patrick's comments

* style

* update dpr tokenizer

* add output_dir flag in use_own_knowledge_dataset.py

* allow custom datasets in examples/rag/finetune.py

* add test for custom dataset in distributed rag retriever
==

docs/source/model_doc/rag.rst
examples/rag/distributed_retriever.py
examples/rag/finetune.py
examples/rag/test_data/my_knowledge_dataset.csv
examples/rag/test_distributed_retriever.py
examples/rag/use_own_knowledge_dataset.py
src/transformers/configuration_dpr.py
src/transformers/configuration_rag.py
src/transformers/modeling_dpr.py
src/transformers/retrieval_rag.py
src/transformers/tokenization_dpr.py
src/transformers/tokenization_dpr_fast.py
tests/test_retrieval_rag.py
==================
a09fe140c;Julien Rossi;2020-10-19 17:57:39 +0200;Trainer with Iterable Dataset (#7858)
* fix 5990

* accomodate iterable dataset without predefined length
* set it as 1 use case: provide max_steps, and NO num_epochs
* Is a merge of master and PR 5995

* fix trainer test under TF

* fix only for torch
* TF trainer untouched
* trainer tests are skipped when no torch

* address comments

* fix quality checks

* remove torch.dataset from test_trainer

* unnecessary inheritance
* RegressionDataset implements all needed methods __len__ and __getitem__

* fix quality checks

* restore RegressionDataset

* was wrongly under is_torch_available()
==

src/transformers/trainer.py
tests/test_trainer.py
==================
2422cda01;Weizhen;2020-10-19 23:36:09 +0800;ProphetNet (#7157)
* add new model prophetnet

prophetnet modified

modify codes as suggested v1

add prophetnet test files

* still bugs, because of changed output formats of encoder and decoder

* move prophetnet into the latest version

* clean integration tests

* clean tokenizers

* add xlm config to init

* correct typo in init

* further refactoring

* continue refactor

* save parallel

* add decoder_attention_mask

* fix use_cache vs. past_key_values

* fix common tests

* change decoder output logits

* fix xlm tests

* make common tests pass

* change model architecture

* add tokenizer tests

* finalize model structure

* no weight mapping

* correct n-gram stream attention mask as discussed with qweizhen

* remove unused import

* fix index.rst

* fix tests

* delete unnecessary code

* add fast integration test

* rename weights

* final weight remapping

* save intermediate

* Descriptions for Prophetnet Config File

* finish all models

* finish new model outputs

* delete unnecessary files

* refactor encoder layer

* add dummy docs

* code quality

* fix tests

* add model pages to doctree

* further refactor

* more refactor, more tests

* finish code refactor and tests

* remove unnecessary files

* further clean up

* add docstring template

* finish tokenizer doc

* finish prophetnet

* fix copies

* fix typos

* fix tf tests

* fix fp16

* fix tf test 2nd try

* fix code quality

* add test for each model

* merge new tests to branch

* Update model_cards/microsoft/prophetnet-large-uncased-cnndm/README.md

Co-authored-by: Sam Shleifer <sshleifer@gmail.com>

* Update model_cards/microsoft/prophetnet-large-uncased-cnndm/README.md

Co-authored-by: Sam Shleifer <sshleifer@gmail.com>

* Update src/transformers/modeling_prophetnet.py

Co-authored-by: Sam Shleifer <sshleifer@gmail.com>

* Update utils/check_repo.py

Co-authored-by: Sam Shleifer <sshleifer@gmail.com>

* apply sams and sylvains comments

* make style

* remove unnecessary code

* Update README.md

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update README.md

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/configuration_prophetnet.py

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* implement lysandres comments

* correct docs

* fix isort

* fix tokenizers

* fix copies

Co-authored-by: weizhen <weizhen@mail.ustc.edu.cn>
Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
Co-authored-by: Sam Shleifer <sshleifer@gmail.com>
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
==

README.md
docs/source/index.rst
docs/source/model_doc/encoderdecoder.rst
docs/source/model_doc/prophetnet.rst
docs/source/model_doc/xlmprophetnet.rst
model_cards/microsoft/prophetnet-large-uncased-cnndm/README.md
model_cards/microsoft/prophetnet-large-uncased/README.md
model_cards/microsoft/xprophetnet-large-wiki100-cased-xglue-ntg/README.md
model_cards/microsoft/xprophetnet-large-wiki100-cased-xglue-qg/README.md
model_cards/microsoft/xprophetnet-large-wiki100-cased/README.md
src/transformers/__init__.py
src/transformers/configuration_auto.py
src/transformers/configuration_lxmert.py
src/transformers/configuration_prophetnet.py
src/transformers/configuration_xlm_prophetnet.py
src/transformers/convert_prophetnet_original_pytorch_checkpoint_to_pytorch.py
src/transformers/modeling_auto.py
src/transformers/modeling_lxmert.py
src/transformers/modeling_prophetnet.py
src/transformers/modeling_tf_lxmert.py
src/transformers/modeling_utils.py
src/transformers/modeling_xlm_prophetnet.py
src/transformers/tokenization_auto.py
src/transformers/tokenization_deberta.py
src/transformers/tokenization_fsmt.py
src/transformers/tokenization_prophetnet.py
src/transformers/tokenization_xlm_prophetnet.py
src/transformers/tokenization_xlm_roberta.py
src/transformers/utils/dummy_pt_objects.py
src/transformers/utils/dummy_sentencepiece_objects.py
tests/test_modeling_common.py
tests/test_modeling_encoder_decoder.py
tests/test_modeling_prophetnet.py
tests/test_modeling_t5.py
tests/test_modeling_xlm_prophetnet.py
tests/test_tokenization_prophetnet.py
tests/test_tokenization_xlm_prophetnet.py
utils/check_repo.py
==================
8f8f8d99f;Funtowicz Morgan;2020-10-19 15:55:41 +0200;Integrate Bert-like model on Flax runtime. (#3722)
* WIP flax bert

* Initial commit Bert Jax/Flax implementation.

* Embeddings working and equivalent to PyTorch.

* Move embeddings in its own module BertEmbeddings

* Added jax.jit annotation on forward call

* BertEncoder on par with PyTorch ! :D

* Add BertPooler on par with PyTorch !!

* Working Jax+Flax implementation of BertModel with < 1e-5 differences on the last layer.

* Fix pooled output to take only the first token of the sequence.

* Refactoring to use BertConfig from transformers.

* Renamed FXBertModel to FlaxBertModel

* Model is now initialized in FlaxBertModel constructor and reused.

* WIP JaxPreTrainedModel

* Cleaning up the code of FlaxBertModel

* Added ability to load Flax model saved through save_pretrained()

* Added ability to convert Pytorch Bert model to FlaxBert

* FlaxBert can now load every Pytorch Bert model with on-the-fly conversion

* Fix hardcoded shape values in conversion scripts.

* Improve the way we handle LayerNorm conversion from PyTorch to Flax.

* Added positional embeddings as parameter of BertModel with default to np.arange.

* Let's roll FlaxRoberta !

* Fix missing position_ids parameters on predict for Bert

* Flax backend now supports batched inputs

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Make it possible to load msgpacked model on convert from pytorch in last resort.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Moved save_pretrained to Jax base class along with more constructor parameters.

* Use specialized, model dependent conversion functio.

* Expose `is_flax_available` in file_utils.

* Added unittest for Flax models.

* Added run_tests_flax to the CI.

* Introduce FlaxAutoModel

* Added more unittests

* Flax model reference the _MODEL_ARCHIVE_MAP from PyTorch model.

* Addressing review comments.

* Expose seed in both Bert and Roberta

* Fix typo suggested by @stefan-it

Co-Authored-By: Stefan Schweter <stefan@schweter.it>

* Attempt to make style

* Attempt to make style in tests too

* Added jax & jaxlib to the flax optional dependencies.

* Attempt to fix flake8 warnings ...

* Redo black again and again

* When black and flake8 fight each other for a space ... :boom: :boom: :boom:

* Try removing trailing comma to make both black and flake happy!

* Fix invalid is_<framework>_available call, thanks @LysandreJik :tada:

* Fix another invalid import in flax_roberta test

* Bump and pin flax release to 0.1.0.

* Make flake8 happy, remove unused jax import

* Change the type of the catch for msgpack.

* Remove unused import.

* Put seed as optional constructor parameter.

* trigger ci again

* Fix too much parameters in BertAttention.

* Formatting.

* Simplify Flax unittests to avoid machine crashes.

* Fix invalid number of arguments when raising issue for an unknown model.

* Address @bastings comment in PR, moving jax.jit decorated outside of __call__

* Fix incorrect path to require_flax/require_pytorch functions.

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* Attempt to make style.

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* Correct rebasing of circle-ci dependencies

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* Fix import sorting.

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* Fix unused imports.

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* Again import sorting...

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* Installing missing nlp dependency for flax unittests.

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* Fix laoding of model for Flax implementations.

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* jit the inner function call to make JAX-compatible

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* Format !

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* Flake one more time :notes:

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* Rewrites BERT in Flax to the new Linen API (#7211)

* Rewrite Flax HuggingFace PR to Linen

* Some fixes

* Fix tests

* Fix CI with change of name of nlp (#7054)

* nlp -> datasets

* More nlp -> datasets

* Woopsie

* More nlp -> datasets

* One last

* Expose `is_flax_available` in file_utils.

* Added run_tests_flax to the CI.

* Attempt to make style

* trigger ci again

* Fix import sorting.

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* Revert "Rewrites BERT in Flax to the new Linen API (#7211)"

This reverts commit 23703a5eb3364e26a1cbc3ee34b4710d86a674b0.

* Remove jnp.lax references

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* Make style.

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* Reintroduce Linen changes ...

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* Make style.

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* Use jax native's gelu function.

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* Renaming BertModel to BertModule to highlight the fact this is the Flax Module object.

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* Rewrite FlaxAutoModel test to not rely on pretrained_model_archive_map

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* Remove unused variable in BertModule.

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* Remove unused variable in BertModule again

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* Attempt to have is_flax_available working again.

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* Introduce JAX TensorType

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Improve ImportError message when trying to convert to various TensorType format.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Makes Flax model jittable.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Ensure flax models are jittable in unittests.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Remove unused imports.

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* Ensure jax imports are guarded behind is_flax_available.

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* Make style.

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* Make style again

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* Make style again again

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* Make style again again again

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* Update src/transformers/file_utils.py

Co-authored-by: Marc van Zee <marcvanzee@gmail.com>

* Bump flax to it's latest version

Co-authored-by: Marc van Zee <marcvanzee@gmail.com>

* Bump jax version to at least 0.2.0

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* Style.

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* Update the unittest to use TensorType.JAX

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* isort import in tests.

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* Match new flax parameters name "params"

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* Remove unused imports.

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* Add flax models to transformers __init__

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* Attempt to address all CI related comments.

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* Correct circle.yml indent.

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* Correct circle.yml indent (2)

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* Remove coverage from flax tests

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* Addressing many naming suggestions from comments

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* Simplify for loop logic to interate over layers in FlaxBertLayerCollection

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* use f-string syntax for formatting logs.

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* Use config property from FlaxPreTrainedModel.

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* use "cls_token" instead of "first_token" variable name.

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* use "hidden_state" instead of "h" variable name.

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* Correct class reference in docstring to link to Flax related modules.

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* Added HF + Google Flax team copyright.

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* Make Roberta independent from Bert

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* Move activation functions to flax_utils.

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* Move activation functions to flax_utils for bert.

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* Added docstring for BERT

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* Update import for Bert and Roberta tokenizers

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* Make style.

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* fix-copies

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* Correct FlaxRobertaLayer to match PyTorch.

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* Use the same store_artifact for flax unittest

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* Style.

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* Make sure gradient are disabled only locally for flax unittest using torch equivalence.

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* Use relative imports

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

Co-authored-by: Stefan Schweter <stefan@schweter.it>
Co-authored-by: Marc van Zee <marcvanzee@gmail.com>
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

.circleci/config.yml
setup.py
src/transformers/__init__.py
src/transformers/file_utils.py
src/transformers/modeling_flax_auto.py
src/transformers/modeling_flax_bert.py
src/transformers/modeling_flax_roberta.py
src/transformers/modeling_flax_utils.py
src/transformers/testing_utils.py
src/transformers/tokenization_utils_base.py
tests/test_flax_auto.py
tests/test_modeling_flax_bert.py
tests/test_modeling_flax_roberta.py
==================
0193c8290;Lalit Pagaria;2020-10-19 15:15:52 +0200;[RAG] Propagating of n_docs as parameter to all RagModel's related functions (#7891)
* Propagating n_docs as parameter to all RagModel's related functions that defaults to self.config.n_docs

* Making n_docs parameter's default value to None in marginalize function

* Fixing code quality issues

* Handle the special case when generator is of T5PreTrainedModel instance type. T5PreTrainedModel do not have n_docs as parameter

* T5PreTrainedModel do not have n_docs as parameter

* Addressing review comment

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* Correcting comment by addressing review comment

* Adding assert statement verifying that n_docs is correctly set. n_docs should be the same for both retriever and generator.

* Fixing flake8 reported issue

* Correcting test datasets for rag

* Using doc_scores instead of context_input_ids to check assert as in RagSequenceForGeneration context_input_ids can be null

* doc_scores second dimension have number of retrieved docs

* Changing assert comment

* Apply suggestions from code review

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
==

src/transformers/modeling_rag.py
tests/test_modeling_rag.py
==================
7e6b6fbec;Terencio Agozzino;2020-10-19 14:43:25 +0200;style: fix typo in the README (#7882)

==

README.md
==================
805a202e1;Stas Bekman;2020-10-19 05:23:14 -0700;[CIs] report slow tests add --durations=0 to some pytest jobs (#7884)
* add --durations=50 to some pytest runs

* report all tests
==

.circleci/config.yml
.github/workflows/self-scheduled.yml
==================
4eb61f8e8;Stas Bekman;2020-10-19 04:08:34 -0700;remove USE_CUDA (#7861)

==

.github/workflows/self-push.yml
.github/workflows/self-scheduled.yml
docs/source/testing.rst
scripts/fsmt/tests-to-run.sh
src/transformers/testing_utils.py
tests/test_skip_decorators.py
==================
ea1507fb4;Jordi Mas;2020-10-19 12:50:52 +0200;Julibert model card (#7868)
* Julibert model card

* Fix text
==

model_cards/jordimas/julibert/README.md
==================
7c44c864a;Terencio Agozzino;2020-10-19 12:14:53 +0200;style: fix typo (#7883)

==

.github/PULL_REQUEST_TEMPLATE.md
==================
776e82d2b;ayushtiku5;2020-10-19 12:26:08 +0530;Add support to provide initial tokens to decoder of encoder-decoder type models (#7577)
* Add support to provide initial tokens for decoding

* Add docstring

* improve code quality

* code reformat

* code reformat

* minor change

* remove appending decoder start token

Co-authored-by: Ayush Jain <a.jain@sprinklr.com>
==

src/transformers/generation_utils.py
==================
406a49dfe;AndreaSottana;2020-10-19 07:14:29 +0100;Fix small type hinting error (#7820)
* Fix small type hinting error

* Update tokenization_utils_base.py

* Update src/transformers/tokenization_utils_base.py

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
==

src/transformers/tokenization_utils_base.py
==================
b86a71ea3;Sam Shleifer;2020-10-18 20:18:08 -0400;[tests] fix slow bart cnn test, faster marian tests (#7888)

==

tests/test_modeling_bart.py
tests/test_modeling_marian.py
==================
ba8c4d0ac;Thomas Wolf;2020-10-18 20:51:24 +0200;[Dependencies|tokenizers] Make both SentencePiece and Tokenizers optional dependencies (#7659)
* splitting fast and slow tokenizers [WIP]

* [WIP] splitting sentencepiece and tokenizers dependencies

* update dummy objects

* add name_or_path to models and tokenizers

* prefix added to file names

* prefix

* styling + quality

* spliting all the tokenizer files - sorting sentencepiece based ones

* update tokenizer version up to 0.9.0

* remove hard dependency on sentencepiece üéâ

* and removed hard dependency on tokenizers üéâ

* update conversion script

* update missing models

* fixing tests

* move test_tokenization_fast to main tokenization tests - fix bugs

* bump up tokenizers

* fix bert_generation

* update ad fix several tokenizers

* keep sentencepiece in deps for now

* fix funnel and deberta tests

* fix fsmt

* fix marian tests

* fix layoutlm

* fix squeezebert and gpt2

* fix T5 tokenization

* fix xlnet tests

* style

* fix mbart

* bump up tokenizers to 0.9.2

* fix model tests

* fix tf models

* fix seq2seq examples

* fix tests without sentencepiece

* fix slow => fast  conversion without sentencepiece

* update auto and bert generation tests

* fix mbart tests

* fix auto and common test without tokenizers

* fix tests without tokenizers

* clean up tests lighten up when tokenizers + sentencepiece are both off

* style quality and tests fixing

* add sentencepiece to doc/examples reqs

* leave sentencepiece on for now

* style quality split hebert and fix pegasus

* WIP Herbert fast

* add sample_text_no_unicode and fix hebert tokenization

* skip FSMT example test for now

* fix style

* fix fsmt in example tests

* update following Lysandre and Sylvain's comments

* Update src/transformers/testing_utils.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/testing_utils.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/tokenization_utils_base.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/tokenization_utils_base.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

.circleci/config.yml
.github/workflows/github-torch-hub.yml
.gitignore
docs/source/task_summary.rst
examples/requirements.txt
setup.py
src/transformers/__init__.py
src/transformers/configuration_auto.py
src/transformers/configuration_utils.py
src/transformers/convert_slow_tokenizer.py
src/transformers/convert_slow_tokenizers_checkpoints_to_fast.py
src/transformers/data/data_collator.py
src/transformers/data/datasets/glue.py
src/transformers/file_utils.py
src/transformers/modeling_tf_utils.py
src/transformers/modeling_utils.py
src/transformers/testing_utils.py
src/transformers/tokenization_albert.py
src/transformers/tokenization_albert_fast.py
src/transformers/tokenization_auto.py
src/transformers/tokenization_bart.py
src/transformers/tokenization_bart_fast.py
src/transformers/tokenization_bert.py
src/transformers/tokenization_bert_fast.py
src/transformers/tokenization_bert_generation.py
src/transformers/tokenization_bertweet.py
src/transformers/tokenization_blenderbot.py
src/transformers/tokenization_camembert.py
src/transformers/tokenization_camembert_fast.py
src/transformers/tokenization_ctrl.py
src/transformers/tokenization_deberta.py
src/transformers/tokenization_distilbert.py
src/transformers/tokenization_distilbert_fast.py
src/transformers/tokenization_dpr.py
src/transformers/tokenization_dpr_fast.py
src/transformers/tokenization_electra.py
src/transformers/tokenization_electra_fast.py
src/transformers/tokenization_fsmt.py
src/transformers/tokenization_funnel.py
src/transformers/tokenization_funnel_fast.py
src/transformers/tokenization_gpt2.py
src/transformers/tokenization_gpt2_fast.py
src/transformers/tokenization_herbert.py
src/transformers/tokenization_herbert_fast.py
src/transformers/tokenization_layoutlm.py
src/transformers/tokenization_layoutlm_fast.py
src/transformers/tokenization_longformer.py
src/transformers/tokenization_longformer_fast.py
src/transformers/tokenization_lxmert.py
src/transformers/tokenization_lxmert_fast.py
src/transformers/tokenization_marian.py
src/transformers/tokenization_mbart.py
src/transformers/tokenization_mbart_fast.py
src/transformers/tokenization_mobilebert.py
src/transformers/tokenization_mobilebert_fast.py
src/transformers/tokenization_openai.py
src/transformers/tokenization_openai_fast.py
src/transformers/tokenization_pegasus.py
src/transformers/tokenization_pegasus_fast.py
src/transformers/tokenization_phobert.py
src/transformers/tokenization_reformer.py
src/transformers/tokenization_reformer_fast.py
src/transformers/tokenization_retribert.py
src/transformers/tokenization_retribert_fast.py
src/transformers/tokenization_roberta.py
src/transformers/tokenization_roberta_fast.py
src/transformers/tokenization_squeezebert.py
src/transformers/tokenization_squeezebert_fast.py
src/transformers/tokenization_t5.py
src/transformers/tokenization_t5_fast.py
src/transformers/tokenization_transfo_xl.py
src/transformers/tokenization_utils.py
src/transformers/tokenization_utils_base.py
src/transformers/tokenization_utils_fast.py
src/transformers/tokenization_xlm.py
src/transformers/tokenization_xlm_roberta.py
src/transformers/tokenization_xlm_roberta_fast.py
src/transformers/tokenization_xlnet.py
src/transformers/tokenization_xlnet_fast.py
src/transformers/trainer.py
src/transformers/utils/dummy_pt_objects.py
src/transformers/utils/dummy_sentencepiece_objects.py
src/transformers/utils/dummy_tokenizers_objects.py
templates/adding_a_new_model/tokenization_xxx.py
tests/fixtures/sample_text_no_unicode.txt
tests/test_modeling_bart.py
tests/test_modeling_blenderbot.py
tests/test_modeling_camembert.py
tests/test_modeling_deberta.py
tests/test_modeling_fsmt.py
tests/test_modeling_funnel.py
tests/test_modeling_longformer.py
tests/test_modeling_marian.py
tests/test_modeling_mbart.py
tests/test_modeling_mobilebert.py
tests/test_modeling_pegasus.py
tests/test_modeling_rag.py
tests/test_modeling_reformer.py
tests/test_modeling_roberta.py
tests/test_modeling_squeezebert.py
tests/test_modeling_t5.py
tests/test_modeling_tf_camembert.py
tests/test_modeling_tf_flaubert.py
tests/test_modeling_tf_longformer.py
tests/test_modeling_tf_roberta.py
tests/test_modeling_tf_t5.py
tests/test_modeling_tf_xlm_roberta.py
tests/test_modeling_xlm_roberta.py
tests/test_onnx.py
tests/test_pipelines.py
tests/test_retrieval_rag.py
tests/test_tokenization_albert.py
tests/test_tokenization_auto.py
tests/test_tokenization_bart.py
tests/test_tokenization_bert.py
tests/test_tokenization_bert_generation.py
tests/test_tokenization_bert_japanese.py
tests/test_tokenization_camembert.py
tests/test_tokenization_common.py
tests/test_tokenization_distilbert.py
tests/test_tokenization_dpr.py
tests/test_tokenization_fast.py
tests/test_tokenization_funnel.py
tests/test_tokenization_gpt2.py
tests/test_tokenization_herbert.py
tests/test_tokenization_layoutlm.py
tests/test_tokenization_lxmert.py
tests/test_tokenization_marian.py
tests/test_tokenization_mbart.py
tests/test_tokenization_openai.py
tests/test_tokenization_pegasus.py
tests/test_tokenization_reformer.py
tests/test_tokenization_roberta.py
tests/test_tokenization_squeezebert.py
tests/test_tokenization_t5.py
tests/test_tokenization_utils.py
tests/test_tokenization_xlm_roberta.py
tests/test_tokenization_xlnet.py
tests/test_trainer.py
utils/check_dummies.py
==================
c65863ce5;Raza Habib;2020-10-17 22:31:53 +0100;Remove duplicated mish activation function (#7856)
* Remove duplicated mish activation function

* Update activations.py
==

src/transformers/activations.py
==================
f5c45a19e;Patrick von Platen;2020-10-17 22:46:47 +0200;Fix Rag example docstring (#7872)
* fix rag examples

* fix token generate example
==

src/transformers/modeling_rag.py
==================
9f7b2b243;Stas Bekman;2020-10-17 11:33:21 -0700;[s2s testing] turn all to unittests, use auto-delete temp dirs (#7859)

==

examples/seq2seq/test_bash_script.py
examples/seq2seq/test_data/wmt_en_ro/train.len
examples/seq2seq/test_data/wmt_en_ro/val.len
examples/seq2seq/test_datasets.py
examples/seq2seq/test_finetune_trainer.py
examples/seq2seq/test_seq2seq_examples.py
==================
dc552b9b7;Patrick von Platen;2020-10-16 16:05:06 +0200;Fix typo in sequence model card

==

model_cards/facebook/rag-sequence-nq/README.md
==================
1652ddad3;Stas Bekman;2020-10-16 06:05:29 -0700;[seq2seq testing] improve readability (#7845)

==

examples/seq2seq/test_finetune_trainer.py
==================
466115b27;Quentin Lhoest;2020-10-16 10:15:49 +0200;Fix missing reference titles in retrieval evaluation of RAG (#7817)

==

examples/rag/eval_rag.py
==================
464b53f5e;Stas Bekman;2020-10-16 00:35:39 -0700;[testing] disable FutureWarning in examples tests (#7842)
* [testing] disable FutureWarning in examples tests

same as tests/conftest.py, we can't resolve those warning, so turn the noise off.

* fix
==

examples/conftest.py
==================
eb186bc14;Sylvain Gugger;2020-10-16 03:23:44 -0400;Small fixes to HP search (#7839)

==

src/transformers/trainer.py
src/transformers/trainer_utils.py
==================
d8ca57d2c;Stas Bekman;2020-10-16 00:19:51 -0700;fix/hide warnings (#7837)
s
==

tests/test_trainer_callback.py
==================
c6e865ac2;vblagoje;2020-10-16 03:12:10 -0400;Remove masked_lm_labels from returned dictionary (#7818)

==

src/transformers/data/data_collator.py
==================
96e47d922;Sam Shleifer;2020-10-16 03:11:18 -0400;[cleanup] assign todos, faster bart-cnn test (#7835)
* 2 beam output

* unassign/remove TODOs

* remove one more
==

examples/lightning_base.py
examples/seq2seq/test_bash_script.py
examples/seq2seq/test_finetune_trainer.py
examples/seq2seq/test_seq2seq_examples.py
src/transformers/convert_pegasus_tf_to_pytorch.py
src/transformers/tokenization_pegasus.py
tests/test_modeling_bart.py
tests/test_modeling_fsmt.py
==================
7b13bd01d;rmroczkowski;2020-10-16 09:06:51 +0200;Herbert polish model (#7798)
* HerBERT transformer model for Polish language understanding.

* HerbertTokenizerFast generated with HerbertConverter

* Herbert base and large model cards

* Herbert model cards with tags

* Herbert tensorflow models

* Herbert model tests based on Bert test suit

* src/transformers/tokenization_herbert.py edited online with Bitbucket

* src/transformers/tokenization_herbert.py edited online with Bitbucket

* docs/source/model_doc/herbert.rst edited online with Bitbucket

* Herbert tokenizer tests and bug fixes

* src/transformers/configuration_herbert.py edited online with Bitbucket

* Copyrights and tests for TFHerbertModel

* model_cards/allegro/herbert-base-cased/README.md edited online with Bitbucket

* model_cards/allegro/herbert-large-cased/README.md edited online with Bitbucket

* Bug fixes after testing

* Reformat modified_only_fixup

* Proper order of configuration

* Herbert proper documentation formatting

* Formatting with make modified_only_fixup

* Dummies fixed

* Adding missing models to documentation

* Removing HerBERT model as it is a simple extension of BERT

* Update model_cards/allegro/herbert-base-cased/README.md

Co-authored-by: Julien Chaumond <chaumond@gmail.com>

* Update model_cards/allegro/herbert-large-cased/README.md

Co-authored-by: Julien Chaumond <chaumond@gmail.com>

* HerbertTokenizer deprecated configuration removed

Co-authored-by: Julien Chaumond <chaumond@gmail.com>
==

model_cards/allegro/herbert-base-cased/README.md
model_cards/allegro/herbert-large-cased/README.md
src/transformers/__init__.py
src/transformers/convert_slow_tokenizer.py
src/transformers/tokenization_herbert.py
tests/test_tokenization_herbert.py
==================
99898dcd2;Julien Chaumond;2020-10-16 08:57:02 +0200;[Pipelines] Fix links to model lists (#7826)

==

src/transformers/pipelines.py
==================
52c9e8428;Lysandre Debut;2020-10-16 08:49:13 +0200;Fix DeBERTa integration tests (#7729)

==

src/transformers/modeling_deberta.py
tests/test_modeling_deberta.py
==================
2255c2c7a;Stas Bekman;2020-10-15 21:22:43 -0700;[seq2seq] get_git_info fails gracefully (#7843)
Co-authored-by: Sam Shleifer <sshleifer@gmail.com>
==

examples/seq2seq/utils.py
==================
dfa4c26bc;Katarina Slama;2020-10-15 16:36:31 -0700;Typo and fix the input of labels to `cross_entropy` (#7841)
The current version caused some errors. The changes fixed it for me. Hope this is helpful!
==

docs/source/training.rst
==================
a5a8eeb77;Stas Bekman;2020-10-15 13:21:09 -0700;fix DeprecationWarning (#7834)
in `tests/test_utils_check_copies.py` I was getting intermittently:
```
utils/check_copies.py:52
  /mnt/nvme1/code/transformers-comet/utils/check_copies.py:52: DeprecationWarning: invalid escape sequence \s
    while line_index < len(lines) and re.search(f"^{indent}(class|def)\s+{name}", lines[line_index]) is None:
```
So this should fix it.
==

utils/check_copies.py
==================
9c71cca31;David S. Lim;2020-10-15 12:55:00 -0700;model card for bert-base-NER (#7799)
* model card for bert-base-NER

* add meta data up top

Co-authored-by: Julien Chaumond <chaumond@gmail.com>

Co-authored-by: Julien Chaumond <chaumond@gmail.com>
==

model_cards/dslim/bert-base-NER/README.md
==================
4dbca5002;Stas Bekman;2020-10-15 12:23:24 -0700;fix wandb/comet problems (#7830)
* fix wandb/comet problems

* simplify

* Update src/transformers/integrations.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

src/transformers/integrations.py
==================
e7aa64838;Julien Chaumond;2020-10-15 19:02:10 +0200;[model_cards] facebook/bart-large-mnli: register ZSC for the inference API
cc @Narsil @mfuntowicz @joeddav

==

model_cards/facebook/bart-large-mnli/README.md
==================
2ce3ddab2;Sylvain Gugger;2020-10-15 10:30:34 -0400;Small fixes to NotebookProgressCallback (#7813)

==

src/transformers/file_utils.py
src/transformers/utils/notebook.py
==================
6f45dd2fa;Julien Chaumond;2020-10-15 16:14:08 +0200;[model_cards] Fix yaml for Facebook/wmt19-*
see d99ed7ad618037ae878f0758157ed0764bd7f935

==

model_cards/facebook/wmt19-de-en/README.md
model_cards/facebook/wmt19-en-de/README.md
model_cards/facebook/wmt19-en-ru/README.md
model_cards/facebook/wmt19-ru-en/README.md
==================
d99ed7ad6;Julien Chaumond;2020-10-15 12:53:29 +0200;[model_cards] Facebook: add thumbnail

==

model_cards/facebook/bart-large-cnn/README.md
model_cards/facebook/bart-large-mnli/README.md
model_cards/facebook/bart-large/README.md
model_cards/facebook/rag-sequence-base/README.md
model_cards/facebook/rag-sequence-nq/README.md
model_cards/facebook/rag-token-base/README.md
model_cards/facebook/rag-token-nq/README.md
model_cards/facebook/wmt19-de-en/README.md
model_cards/facebook/wmt19-en-de/README.md
model_cards/facebook/wmt19-en-ru/README.md
model_cards/facebook/wmt19-ru-en/README.md
==================
2485b8b0a;Lysandre;2020-10-15 12:34:29 +0200;Set XLA example time to 500s

==

examples/test_xla_examples.py
==================
2dba7d570;Lysandre;2020-10-15 12:21:32 +0200;Notebook catch all errors

==

src/transformers/file_utils.py
==================
9ade8e749;Nicolas Patry;2020-10-15 11:26:08 +0200;Upgrading TFAutoModelWithLMHead to (#7730)
- TFAutoModelForCausalLM
- TFAutoModelForMaskedLM
- TFAutoModelForSeq2SeqLM

as per deprecation warning. No tests as it simply removes current
warnings from tests.
==

src/transformers/pipelines.py
==================
62b5622e6;Sylvain Gugger;2020-10-15 05:05:08 -0400;Add specific notebook ProgressCalback (#7793)

==

src/transformers/file_utils.py
src/transformers/trainer.py
src/transformers/utils/notebook.py
==================
0911b6bd8;Nicolas Patry;2020-10-15 09:42:07 +0200;Improving Pipelines by defaulting to framework='tf' when pytorch seems unavailable. (#7728)
* Improving Pipelines by defaulting to framework='tf' when

pytorch seems unavailable.

* Actually changing the default resolution order to account for model
defaults

Adding a new tests for each pipeline to check that pipeline(task) works
too without manually adding the framework too.
==

src/transformers/pipelines.py
tests/test_pipelines.py
==================
3a134f7c6;Julien Plu;2020-10-14 23:48:50 +0200;Fix TF savedmodel in Roberta (#7795)
* Remove wrong parameter.

* Same in Longformer
==

src/transformers/modeling_tf_longformer.py
src/transformers/modeling_tf_roberta.py
==================
3032de936;Nils Reimers;2020-10-14 19:30:58 +0200;Model Card (#7752)
* Create README.md

* Update model_cards/sentence-transformers/LaBSE/README.md

Co-authored-by: Julien Chaumond <chaumond@gmail.com>

Co-authored-by: Julien Chaumond <chaumond@gmail.com>
==

model_cards/sentence-transformers/LaBSE/README.md
==================
3fdbeba83;sarahlintang;2020-10-15 00:10:31 +0700;[model_cards] sarahlintang/IndoBERT (#7748)
* Create README.md

* Update model_cards/sarahlintang/IndoBERT/README.md

Co-authored-by: Julien Chaumond <chaumond@gmail.com>
==

model_cards/sarahlintang/IndoBERT/README.md
==================
ba654270b;Julien Chaumond;2020-10-14 19:02:48 +0200;[model_cards] rename to correct model name

==

model_cards/cooelf/limitbert/README.md
==================
08978487e;Zhuosheng Zhang;2020-10-15 00:56:12 +0800;Create README.md (#7722)

==

model_cards/cooelf/limit-bert/README.md
==================
355750912;Sagor Sarker;2020-10-14 22:50:43 +0600;added evaluation results for classification task (#7790)

==

model_cards/sagorsarker/bangla-bert-base/README.md
==================
bb9559a7f;Sylvain Gugger;2020-10-14 12:05:02 -0400;Don't use `store_xxx` on optional bools (#7786)
* Don't use `store_xxx` on optional bools

* Refine test

* Refine test
==

examples/test_xla_examples.py
examples/text-classification/README.md
src/transformers/hf_argparser.py
src/transformers/training_args.py
valohai.yaml
==================
a1d1b332d;Sylvain Gugger;2020-10-14 11:41:45 -0400;Add predict step accumulation (#7767)
* Add eval_accumulation_step and clean distributed eval

* Add TPU test

* Add TPU stuff

* Fix arg name

* Fix Seq2SeqTrainer

* Fix total_size

* Update src/transformers/trainer_pt_utils.py

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* Doc and add test to TPU

* Add unit test

* Adapt name

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
==

docs/source/internal/trainer_utils.rst
examples/seq2seq/seq2seq_trainer.py
examples/test_xla_examples.py
src/transformers/trainer.py
src/transformers/trainer_pt_utils.py
src/transformers/training_args.py
tests/test_trainer.py
tests/test_trainer_distributed.py
tests/test_trainer_tpu.py
tests/test_trainer_utils.py
==================
8feb0cc96;Sam Shleifer;2020-10-14 11:35:00 -0400;fix examples/rag imports, tests (#7712)

==

examples/rag/README.md
examples/rag/__init__.py
examples/rag/eval_rag.py
examples/rag/finetune.py
examples/rag/test_distributed_retriever.py
==================
890e790e1;XiaoqiJiao;2020-10-14 21:31:01 +0800;[model_cards] TinyBERT (HUAWEI Noah's Ark Lab) (#7775)

==

model_cards/huawei-noah/TinyBERT_General_4L_312D/README.md
==================
121dd4332;Jonathan Chang;2020-10-14 04:40:24 -0700;Add batch inferencing support for GPT2LMHeadModel (#7552)
* Add support for gpt2 batch inferencing

* add test

* remove typo

Co-authored-by: patrickvonplaten <patrick.v.platen@gmail.com>
==

src/transformers/modeling_gpt2.py
tests/test_modeling_gpt2.py
==================
0c64b1884;Quentin Lhoest;2020-10-14 11:30:02 +0200;Fix bert position ids in DPR convert script (#7776)
* fix bert position ids in DPR convert script

* style
==

src/transformers/convert_dpr_original_checkpoint_to_pytorch.py
==================
7968051ab;Sylvain Gugger;2020-10-13 17:30:46 -0400;Fix typo

==

tests/test_trainer.py
==================
2977bd528;Sam Shleifer;2020-10-13 16:22:29 -0400;Faster pegasus tokenization test with reduced data size (#7762)

==

tests/test_tokenization_pegasus.py
==================
2d6e2ad4f;Fran√ßois Lagunas;2020-10-13 17:07:02 +0200;Adding optional trial argument to model_init (#7759)
* Adding optional trial argument to model_init

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

src/transformers/trainer.py
==================
7e73c1280;Tiger;2020-10-13 09:00:20 -0500;fixed lots of typos. (#7758)

==

docs/source/main_classes/callback.rst
docs/source/main_classes/trainer.rst
docs/source/philosophy.rst
examples/seq2seq/bertabs/README.md
src/transformers/configuration_mmbt.py
src/transformers/configuration_utils.py
src/transformers/convert_graph_to_onnx.py
src/transformers/data/processors/squad.py
src/transformers/file_utils.py
src/transformers/integrations.py
src/transformers/modeling_auto.py
src/transformers/modeling_tf_auto.py
src/transformers/modeling_tf_utils.py
src/transformers/modeling_utils.py
src/transformers/optimization_tf.py
src/transformers/pipelines.py
src/transformers/tokenization_utils.py
src/transformers/tokenization_utils_base.py
src/transformers/tokenization_utils_fast.py
src/transformers/trainer.py
src/transformers/training_args.py
src/transformers/training_args_tf.py
==================
8cb4ecca2;Noam Wies;2020-10-13 16:46:44 +0300;Avoid unnecessary DDP synchronization when gradient_accumulation_steps > 1 (#7742)
* use DDP no_sync when possible

* fix is_nlp_available addition mistake

* reformat trainer.py

* reformat trainer.py

* drop support for pytorch < 1.2

* return support for pytorch < 1.2
==

src/transformers/trainer.py
==================
52f7d7439;Lysandre Debut;2020-10-13 15:42:27 +0200;Do not softmax when num_labels==1 (#7726)
* Do not softmax when num_labels==1

* Update src/transformers/pipelines.py

Co-authored-by: Funtowicz Morgan <mfuntowicz@users.noreply.github.com>

Co-authored-by: Funtowicz Morgan <mfuntowicz@users.noreply.github.com>
==

src/transformers/pipelines.py
==================
82b09a848;Patrick von Platen;2020-10-13 14:34:22 +0200;[Rag] Fix loading of pretrained Rag Tokenizer (#7756)
* fix rag

* Update tokenizer save_pretrained

Co-authored-by: Thomas Wolf <thomwolf@users.noreply.github.com>
==

src/transformers/tokenization_utils_base.py
tests/test_tokenization_rag.py
==================
2d4e928d9;Patrick von Platen;2020-10-13 12:18:31 +0200;Update PULL_REQUEST_TEMPLATE.md
Putting my name on a couple more issues to directly redirect them to me
==

.github/PULL_REQUEST_TEMPLATE.md
==================
dcba9ee03;Felipe Curti;2020-10-13 06:06:15 -0300;Gpt1 for sequence classification (#7683)
* Add Documentation for GPT-1 Classification

* Add GPT-1 with Classification head

* Add tests for GPT-1 Classification

* Add GPT-1 For Classification to auto models

* Remove authorized missing keys, change checkpoint to openai-gpt
==

docs/source/model_doc/gpt.rst
src/transformers/__init__.py
src/transformers/modeling_auto.py
src/transformers/modeling_openai.py
src/transformers/utils/dummy_pt_objects.py
tests/test_modeling_openai.py
==================
f34b4cd1b;Lysandre Debut;2020-10-13 10:50:41 +0200;ElectraTokenizerFast (#7754)

==

src/transformers/convert_slow_tokenizer.py
==================
9c2b2db2c;Sam Shleifer;2020-10-12 12:24:25 -0400;[marian] Automate Tatoeba-Challenge conversion (#7709)

==

.gitignore
examples/seq2seq/test_tatoeba_conversion.py
scripts/tatoeba/README.md
src/transformers/convert_marian_tatoeba_to_pytorch.py
src/transformers/convert_marian_to_pytorch.py
==================
aacac8f70;Alex Combessie;2020-10-12 17:56:10 +0200;Add license info to nlptown/bert-base-multilingual-uncased-sentiment (#7738)

==

model_cards/nlptown/bert-base-multilingual-uncased-sentiment/README.md
==================
1f1d950b2;Lysandre Debut;2020-10-12 15:10:52 +0200;Fix #7331 (#7732)

==

src/transformers/modeling_gpt2.py
==================
d9ffb87ef;Julien Plu;2020-10-12 14:45:15 +0200;Fix tf text class (#7724)
* Fix test

* fix generic text classification

* fix test

* Fix tests
==

examples/text-classification/run_tf_text_classification.py
==================
d6175a426;sgugger;2020-10-12 08:22:27 -0400;Fix code quality

==

examples/language-modeling/run_language_modeling.py
==================
1d5ea34f6;Jonathan Chang;2020-10-12 04:45:12 -0700;Fix trainer callback (#7720)
Fix a bug that happends when subclassing Trainer and
overwriting evaluate() without calling prediciton_loop()
==

src/transformers/trainer_callback.py
==================
f176e7072;Kelvin;2020-10-12 12:44:02 +0100;The input training data files (multiple files in glob format). (#7717)
Very often splitting large files to smaller files can prevent tokenizer going out of memory in environment like Colab that does not have swap memory
==

examples/language-modeling/run_language_modeling.py
==================
34fcfb44e;AndreaSottana;2020-10-12 11:09:20 +0100;Update tokenization_utils_base.py (#7696)
Minor spelling corrections in docstrings. "information" is uncountable in English and has no plural.
==

src/transformers/tokenization_utils_base.py
==================
2f34bcf3e;fteufel;2020-10-12 10:10:17 +0200;check for tpu availability in save_pretrained (#7699)
Added is_torch_tpu_available() to the condition
for saving a model as xla model. "xla_device"
property of config can also be True on a non-xla
device, when loading a checkpointthat was trained
on xla before.

Resolves #7695
==

src/transformers/modeling_utils.py
==================
13c185771;Sylvain Gugger;2020-10-12 04:06:59 -0400;Fix typo in all model docs (#7714)

==

src/transformers/modeling_albert.py
src/transformers/modeling_bart.py
src/transformers/modeling_bert_generation.py
src/transformers/modeling_ctrl.py
src/transformers/modeling_distilbert.py
src/transformers/modeling_dpr.py
src/transformers/modeling_electra.py
src/transformers/modeling_encoder_decoder.py
src/transformers/modeling_flaubert.py
src/transformers/modeling_fsmt.py
src/transformers/modeling_funnel.py
src/transformers/modeling_gpt2.py
src/transformers/modeling_longformer.py
src/transformers/modeling_lxmert.py
src/transformers/modeling_mmbt.py
src/transformers/modeling_mobilebert.py
src/transformers/modeling_openai.py
src/transformers/modeling_rag.py
src/transformers/modeling_reformer.py
src/transformers/modeling_retribert.py
src/transformers/modeling_roberta.py
src/transformers/modeling_squeezebert.py
src/transformers/modeling_t5.py
src/transformers/modeling_tf_albert.py
src/transformers/modeling_tf_bert.py
src/transformers/modeling_tf_ctrl.py
src/transformers/modeling_tf_distilbert.py
src/transformers/modeling_tf_electra.py
src/transformers/modeling_tf_flaubert.py
src/transformers/modeling_tf_funnel.py
src/transformers/modeling_tf_gpt2.py
src/transformers/modeling_tf_longformer.py
src/transformers/modeling_tf_lxmert.py
src/transformers/modeling_tf_mobilebert.py
src/transformers/modeling_tf_openai.py
src/transformers/modeling_tf_roberta.py
src/transformers/modeling_tf_t5.py
src/transformers/modeling_tf_xlm.py
src/transformers/modeling_tf_xlnet.py
src/transformers/modeling_xlm.py
src/transformers/modeling_xlnet.py
templates/adding_a_new_model/modeling_tf_xxx.py
templates/adding_a_new_model/modeling_xxx.py
==================
83086858f;Berowne;2020-10-12 18:58:58 +1100;fixed typo in warning line 207. (#7718)
replace 'men_len' with 'mem_len' to match parameter name
==

src/transformers/configuration_xlnet.py
==================
03ec02a66;Miguel Victor;2020-10-12 04:45:00 +0800;Corrected typo: maked ‚Üí masked (#7703)

==

src/transformers/modeling_bert.py
==================
827c51949;Sam Shleifer;2020-10-11 16:39:38 -0400;[examples] bump pl=0.9.0 (#7053)

==

examples/lightning_base.py
examples/requirements.txt
examples/seq2seq/README.md
examples/seq2seq/distillation.py
examples/seq2seq/finetune.py
examples/seq2seq/test_seq2seq_examples.py
examples/test_examples.py
==================
ba4bbd92b;Alexandr Maslov;2020-10-11 04:08:08 +0300;Fix docstring in AutoModel class (#7694)

==

src/transformers/modeling_auto.py
==================
26d5475d4;Andrew Kane;2020-10-10 00:55:11 -0700;Added license information for default and distilbert models (#7688)

==

model_cards/distilbert-base-cased-README.md
model_cards/distilbert-base-cased-distilled-squad-README.md
model_cards/distilbert-base-german-cased-README.md
model_cards/distilbert-base-uncased-distilled-squad-README.md
model_cards/distilbert-base-uncased-finetuned-sst-2-english-README.md
model_cards/facebook/bart-large-mnli/README.md
==================
c6e18de9f;Sylvain Gugger;2020-10-09 20:01:15 -0400;Fix flaky test in test_trainer (#7689)

==

tests/test_trainer.py
==================
2c9e83f7b;Sylvain Gugger;2020-10-09 19:24:10 -0400;Fix title level in Blenderbot doc (#7687)

==

docs/source/model_doc/blenderbot.rst
==================
9618cd696;Doug Blank;2020-10-09 09:13:22 -0700;Import integration libraries first (#7650)
* Import intergration libraries first

* isort and black happiness

* flake8 happiness

* Add a test

* Black reformat

* Ignore import order in tests

* A heavy-handed method of disabling comet for tests

* Remove comet_ml tests

* Run black on setup.py
==

setup.py
src/transformers/integrations.py
==================
4dcc424de;sgugger;2020-10-09 12:12:03 -0400;Complete release instruction

==

setup.py
==================
a3cea6a8c;Sylvain Gugger;2020-10-09 11:17:16 -0400;Better links for models in READMED and doc index (#7680)

==

README.md
docs/source/index.rst
setup.py
utils/check_copies.py
==================
0af53b1ef;Sam Shleifer;2020-10-09 11:16:35 -0400;Delete extra test file (#7681)

==

test_tokenization_blenderbot.py
==================
b0f05e0c4;Stas Bekman;2020-10-09 08:10:32 -0700;[pegasus] Faster tokenizer tests (#7672)

==

scripts/pegasus/build_test_sample_spm_no_bos.py
src/transformers/testing_utils.py
src/transformers/tokenization_pegasus.py
src/transformers/tokenization_reformer.py
src/transformers/tokenization_utils.py
tests/fixtures/test_sentencepiece_no_bos.model
tests/test_tokenization_pegasus.py
tests/test_tokenization_t5.py
==================
bc00b37a0;sgugger;2020-10-09 10:54:40 -0400;Revert "Better model links in the README and index"
This reverts commit 76e05518bb11e29c8532d6ac529e72ce9a105495.

==

README.md
docs/source/index.rst
setup.py
utils/check_copies.py
==================
76e05518b;sgugger;2020-10-09 10:54:40 -0400;Better model links in the README and index

==

README.md
docs/source/index.rst
setup.py
utils/check_copies.py
==================
9ad830596;Julien Plu;2020-10-09 16:38:25 +0200;Fix dataset cardinality (#7678)
* Fix test

* Fix cardinality issue

* Fix test
==

examples/text-classification/run_tf_text_classification.py
==================
a1ac08287;Joe Davison;2020-10-09 09:16:06 -0400;add license to xlm-roberta-large-xnli card

==

model_cards/joeddav/xlm-roberta-large-xnli/README.md
==================
21ed3a6b9;Funtowicz Morgan;2020-10-09 14:07:28 +0200;Reintroduce clean_text on BertTokenizer call which was removed by mistake in #4723 (#5749)
* Reintroduce clean_text call which was removed by mistake in #4723

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* Added unittest for clean_text parameter on Bert tokenizer.

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* Better unittest name.

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* Adapt unittest to use untrained tokenizer.

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* Code quality + update test

Co-authored-by: Lysandre <lysandre.debut@reseau.eseo.fr>
==

src/transformers/tokenization_bert.py
tests/test_tokenization_bert.py
==================
5668fdb09;Noah Trenaman;2020-10-09 02:16:58 -0700;Update XLM-RoBERTa details (#7669)

==

docs/source/pretrained_models.rst
==================
0578a9130;guhur;2020-10-09 11:15:08 +0200;fix nn.DataParallel compatibility with PyTorch 1.5 (#7671)
The same type of errors as in https://github.com/huggingface/transformers/pull/4300
==

src/transformers/modeling_lxmert.py
==================
297233fa9;Sam Shleifer;2020-10-08 21:22:22 -0400;[s2s] Switch README urls to cdn (#7670)

==

examples/seq2seq/README.md
examples/seq2seq/finetune_bart_tiny.sh
==================
a1ecc90d6;Sam Shleifer;2020-10-08 14:12:39 -0400;[pseudo] Switch URLS to CDN (#7661)

==

examples/seq2seq/precomputed_pseudo_labels.md
==================
06a973fd2;Suraj Patil;2020-10-08 22:36:35 +0530;[s2s] configure lr_scheduler from command line (#7641)

==

examples/seq2seq/finetune_trainer.py
examples/seq2seq/seq2seq_trainer.py
==================
4a00613c2;Lysandre Debut;2020-10-08 14:36:00 +0200;Fix RobertaForCausalLM docs (#7642)
* Fix RobertaForCausalLM docs

* Apply review suggestion

Co-authored-by: sgugger <sylvain.gugger@gmail,com>

Co-authored-by: sgugger <sylvain.gugger@gmail,com>
==

src/transformers/modeling_roberta.py
==================
55cb2ee62;Thomas Wolf;2020-10-08 13:21:15 +0200;Green tests: update torch-hub test dependencies (add protobuf and pin tokenizer 0.9.0-RC2) (#7658)
* pin torch-hub test

* add protobuf dep
==

.github/workflows/github-torch-hub.yml
==================
9aeacb58b;Thomas Wolf;2020-10-08 11:32:16 +0200;Adding Fast tokenizers for SentencePiece based tokenizers - Breaking: remove Transfo-XL fast tokenizer (#7141)
* [WIP] SP tokenizers

* fixing tests for T5

* WIP tokenizers

* serialization

* update T5

* WIP T5 tokenization

* slow to fast conversion script

* Refactoring to move tokenzier implementations inside transformers

* Adding gpt - refactoring - quality

* WIP adding several tokenizers to the fast world

* WIP Roberta - moving implementations

* update to dev4 switch file loading to in-memory loading

* Updating and fixing

* advancing on the tokenizers - updating do_lower_case

* style and quality

* moving forward with tokenizers conversion and tests

* MBart, T5

* dumping the fast version of transformer XL

* Adding to autotokenizers + style/quality

* update init and space_between_special_tokens

* style and quality

* bump up tokenizers version

* add protobuf

* fix pickle Bert JP with Mecab

* fix newly added tokenizers

* style and quality

* fix bert japanese

* fix funnel

* limite tokenizer warning to one occurence

* clean up file

* fix new tokenizers

* fast tokenizers deep tests

* WIP adding all the special fast tests on the new fast tokenizers

* quick fix

* adding more fast tokenizers in the fast tests

* all tokenizers in fast version tested

* Adding BertGenerationFast

* bump up setup.py for CI

* remove BertGenerationFast (too early)

* bump up tokenizers version

* Clean old docstrings

* Typo

* Update following Lysandre comments

Co-authored-by: Sylvain Gugger <sylvain.gugger@gmail.com>
==

docs/source/model_doc/transformerxl.rst
setup.py
src/transformers/__init__.py
src/transformers/convert_slow_tokenizer.py
src/transformers/tokenization_albert.py
src/transformers/tokenization_auto.py
src/transformers/tokenization_bart.py
src/transformers/tokenization_bert.py
src/transformers/tokenization_bert_japanese.py
src/transformers/tokenization_bertweet.py
src/transformers/tokenization_camembert.py
src/transformers/tokenization_distilbert.py
src/transformers/tokenization_dpr.py
src/transformers/tokenization_electra.py
src/transformers/tokenization_fsmt.py
src/transformers/tokenization_funnel.py
src/transformers/tokenization_gpt2.py
src/transformers/tokenization_longformer.py
src/transformers/tokenization_lxmert.py
src/transformers/tokenization_mbart.py
src/transformers/tokenization_mobilebert.py
src/transformers/tokenization_openai.py
src/transformers/tokenization_pegasus.py
src/transformers/tokenization_phobert.py
src/transformers/tokenization_reformer.py
src/transformers/tokenization_retribert.py
src/transformers/tokenization_roberta.py
src/transformers/tokenization_t5.py
src/transformers/tokenization_transfo_xl.py
src/transformers/tokenization_utils.py
src/transformers/tokenization_utils_base.py
src/transformers/tokenization_utils_fast.py
src/transformers/tokenization_xlm.py
src/transformers/tokenization_xlm_roberta.py
src/transformers/tokenization_xlnet.py
src/transformers/utils/sentencepiece_model_pb2.py
tests/test_tokenization_albert.py
tests/test_tokenization_bart.py
tests/test_tokenization_bert.py
tests/test_tokenization_bert_japanese.py
tests/test_tokenization_camembert.py
tests/test_tokenization_common.py
tests/test_tokenization_ctrl.py
tests/test_tokenization_distilbert.py
tests/test_tokenization_dpr.py
tests/test_tokenization_fast.py
tests/test_tokenization_funnel.py
tests/test_tokenization_gpt2.py
tests/test_tokenization_lxmert.py
tests/test_tokenization_marian.py
tests/test_tokenization_mbart.py
tests/test_tokenization_openai.py
tests/test_tokenization_pegasus.py
tests/test_tokenization_reformer.py
tests/test_tokenization_roberta.py
tests/test_tokenization_t5.py
tests/test_tokenization_transfo_xl.py
tests/test_tokenization_xlm.py
tests/test_tokenization_xlm_roberta.py
tests/test_tokenization_xlnet.py
==================
4d04120c6;Piero Molino;2020-10-08 01:16:10 -0700;Replaced torch.load for loading the pretrained vocab of TransformerXL tokenizer to pickle.load (#6935)
* Replaced torch.load for loading the pretrained vocab of TransformerXL to pickle.load

* Replaced torch.save with pickle.dump when saving the vocabulary

* updating transformer-xl

* uploaded on S3 - compatibility

* fix tests

* style

* Address review comments

Co-authored-by: Thomas Wolf <thomwolf@users.noreply.github.com>
Co-authored-by: Lysandre <lysandre.debut@reseau.eseo.fr>
==

src/transformers/file_utils.py
src/transformers/tokenization_transfo_xl.py
==================
aba4e2294;Sam Shleifer;2020-10-07 23:04:18 -0400;[pseudolabels] cleanup markdown table (#7653)

==

examples/seq2seq/precomputed_pseudo_labels.md
==================
e3e651735;Sam Shleifer;2020-10-07 22:05:03 -0400;Fix 3 failing slow bart/blender tests (#7652)

==

tests/test_modeling_bart.py
tests/test_modeling_blenderbot.py
==================
960faaaf2;Sam Shleifer;2020-10-07 19:09:23 -0400;Blenderbot (#7418)
Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

docs/source/index.rst
docs/source/model_doc/blenderbot.rst
src/transformers/__init__.py
src/transformers/configuration_auto.py
src/transformers/configuration_bart.py
src/transformers/configuration_blenderbot.py
src/transformers/convert_blenderbot_original_pytorch_checkpoint_to_pytorch.py
src/transformers/modeling_auto.py
src/transformers/modeling_bart.py
src/transformers/modeling_blenderbot.py
src/transformers/modeling_mbart.py
src/transformers/tokenization_auto.py
src/transformers/tokenization_blenderbot.py
src/transformers/utils/dummy_pt_objects.py
test_tokenization_blenderbot.py
tests/test_modeling_auto.py
tests/test_modeling_bart.py
tests/test_modeling_blenderbot.py
tests/test_modeling_common.py
tests/test_modeling_mbart.py
tests/test_tokenization_blenderbot.py
==================
aee7967fc;Blaise Cruz;2020-10-08 04:49:20 +0800;Added model cards for Tagalog BERT models (#7603)

==

model_cards/jcblaise/bert-tagalog-base-cased-WWM/README.md
model_cards/jcblaise/bert-tagalog-base-cased/README.md
model_cards/jcblaise/bert-tagalog-base-uncased-WWM/README.md
model_cards/jcblaise/bert-tagalog-base-uncased/README.md
model_cards/jcblaise/distilbert-tagalog-base-cased/README.md
==================
b1c06140f;Bobby Donchev;2020-10-07 22:46:03 +0200;Create README.md for IsRoBERTa language model (#7640)
* Create README.md

* Update README.md

* Apply suggestions from code review

Co-authored-by: Julien Chaumond <chaumond@gmail.com>
==

model_cards/neurocode/IsRoBERTa/README.md
==================
e10d38956;Keshan;2020-10-08 02:10:52 +0530;[Model card] SinhalaBERTo model. (#7558)
* [Model card] SinhalaBERTo model.

This is the model card for keshan/SinhalaBERTo model.

* Update model_cards/keshan/SinhalaBERTo/README.md

Co-authored-by: Julien Chaumond <chaumond@gmail.com>
==

model_cards/keshan/SinhalaBERTo/README.md
==================
167bce56f;Amine Abdaoui;2020-10-07 22:38:14 +0200;[model_card] bert-base-5lang-cased (#7573)
Co-authored-by: Amin <amin.geotrend@gmail.com>
==

model_cards/amine/bert-base-5lang-cased/README.md
==================
923dd4e5e;Abed khooli;2020-10-07 23:37:40 +0300;Create README.md (#7581)

==

model_cards/akhooli/xlm-r-large-arabic-toxic/README.md
==================
85ead0fec;dartrevan;2020-10-07 23:37:10 +0300;Update README.md (#7590)

==

model_cards/cimm-kzn/rudr-bert/README.md
==================
c6b9c72ea;Ilias Chalkidis;2020-10-07 23:36:08 +0300;Update README.md (#7629)
Minor changes: Add arxiv link + Layout improvement + fix typos
==

model_cards/nlpaueb/legal-bert-base-uncased/README.md
==================
048b4bd2c;Abhilash Majumder;2020-10-08 02:05:28 +0530;Create Model Card For "abhilash1910/french-roberta" Model (#7544)

==

model_cards/abhilash1910/french-roberta/README.md
==================
c2e0d8ac5;Julien Chaumond;2020-10-07 16:28:47 -0400;[model_card] nikokons/gpt2-greek
by @nikkon3
==

model_cards/nikokons/gpt2-greek/README.md
==================
e2bb9abb6;Sam Shleifer;2020-10-07 11:20:44 -0400;[s2s] release pseudolabel links and instructions (#7639)

==

examples/seq2seq/make_student.py
examples/seq2seq/precomputed_pseudo_labels.md
==================
08ba4b490;Sylvain Gugger;2020-10-07 10:50:21 -0400;Trainer callbacks (#7596)
* Initial callback proposal

* Finish various callbacks

* Post-rebase conflicts

* Fix tests

* Don't use something that's not set

* Documentation

* Remove unwanted print.

* Document all models can work

* Add tests + small fixes

* Update docs/source/internal/trainer_utils.rst

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* Address review comments

* Fix TF tests

* Real fix this time

* This one should work

* Fix typo

* Really fix typo

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
==

docs/source/index.rst
docs/source/internal/trainer_utils.rst
docs/source/main_classes/callback.rst
docs/source/main_classes/trainer.rst
examples/seq2seq/seq2seq_trainer.py
examples/seq2seq/test_finetune_trainer.py
src/transformers/__init__.py
src/transformers/integrations.py
src/transformers/trainer.py
src/transformers/trainer_callback.py
src/transformers/trainer_pt_utils.py
src/transformers/trainer_utils.py
src/transformers/training_args.py
src/transformers/utils/dummy_pt_objects.py
tests/test_trainer_callback.py
==================
8fa0c956b;Lysandre Debut;2020-10-07 11:20:05 +0200;Add GPT2 to sequence classification auto model (#7630)

==

src/transformers/modeling_auto.py
==================
e084089eb;Gabriele Picco;2020-10-06 23:16:00 +0100;Fix tokenizer UnboundLocalError when padding is set to PaddingStrategy.MAX_LENGTH (#7610)
* Fix UnboundLocalError when PaddingStrategy is MAX_LENGTH

* Fix UnboundLocalError for TruncationStrategy
==

src/transformers/tokenization_utils_base.py
==================
adfe6ace8;Philipp;2020-10-07 00:02:29 +0200;Fix wrong reference name/filename in docstring (#7616)
Resolves: #7613
==

src/transformers/data/processors/squad.py
==================
f0d20ad32;Lysandre;2020-10-06 23:44:03 +0200;Fix-copies

==

src/transformers/utils/dummy_pt_objects.py
==================
598243181;Lysandre Debut;2020-10-06 23:31:21 +0200;Add GPT2ForSequenceClassification based on DialogRPT (#7501)
* Add GPT2ForSequenceClassification based on DialogRPT

* Better documentation

* Code quality
==

docs/source/model_doc/gpt2.rst
src/transformers/__init__.py
src/transformers/modeling_gpt2.py
src/transformers/modeling_outputs.py
tests/test_modeling_gpt2.py
==================
500be01c5;Sam Shleifer;2020-10-06 16:11:56 -0400;[s2s] save first batch to json for debugging purposes (#6810)

==

examples/seq2seq/finetune.py
examples/seq2seq/test_seq2seq_examples.py
==================
2b574e7c6;Sam Shleifer;2020-10-06 11:33:51 -0400;[bart] fix config.classif_dropout (#7593)

==

notebooks/05-benchmark.ipynb
src/transformers/configuration_bart.py
src/transformers/modeling_bart.py
==================
aa6c3c14b;Ahmed Elnaggar;2020-10-06 15:32:52 +0200;typo fix (#7611)
It should be T5-3B not T5-3M.
==

model_cards/Rostlab/prot_t5_xl_bfd/README.md
==================
98fb71857;Adrien David-Sivelle;2020-10-06 09:23:32 -0400;Docker GPU Images: Add NVIDIA/apex to the cuda images with pytorch (#7598)
- Use cuda:10.2 image instead of 10.1 (to address version mismatch
  warning with pytorch)
- Use devel version that is built on the runtime and includes headers
  and development tools (was otherwise failing to build apex)
==

docker/transformers-gpu/Dockerfile
docker/transformers-pytorch-gpu/Dockerfile
==================
4d541f516;George Mihaila;2020-10-06 08:12:04 -0500;fix return dicitonary labels from masked_lm_labels to labels (#7595)

==

src/transformers/data/data_collator.py
==================
8d2c248df;cedspam;2020-10-06 14:46:55 +0200;Update README.md (#7612)

==

model_cards/cedpsam/chatbot_fr/README.md
==================
1c80b2c60;Ilias Chalkidis;2020-10-06 15:46:17 +0300;Create README.md (LEGAL-BERT Model card) (#7607)
* Create README.md

Model description for all LEGAL-BERT models, published as part of  "LEGAL-BERT: The Muppets straight out of Law School". Chalkidis et al., 2018, In Findings of EMNLP 2020

* Update model_cards/nlpaueb/legal-bert-base-uncased/README.md

Co-authored-by: Julien Chaumond <chaumond@gmail.com>
==

model_cards/nlpaueb/legal-bert-base-uncased/README.md
==================
eda27f449;Siddharth Jain;2020-10-06 16:17:16 +0530;[TF generation] Fix typo (#7582)
* Fixing top_k and min_length assertions, and a typo fix

* Apply suggestions from code review

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
==

src/transformers/generation_tf_utils.py
==================
0257992e4;Lysandre Debut;2020-10-06 12:22:04 +0200;Fix squeezebert docs (#7587)
* Configuration

* Modeling

* Tokenization

* Obliterate the trailing spaces

* From underlines to long underlines
==

docs/source/model_doc/squeezebert.rst
src/transformers/configuration_squeezebert.py
src/transformers/modeling_squeezebert.py
src/transformers/tokenization_squeezebert.py
==================
66c72082d;Ahmed Elnaggar;2020-10-06 12:19:21 +0200;Add ProtT5-XL-BFD model card (#7606)
* Add ProtT5-XL-BFD model card

* Apply suggestions from code review

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
==

model_cards/Rostlab/prot_t5_xl_bfd/README.md
==================
b21a30bdd;Stas Bekman;2020-10-06 02:25:21 -0700;[makefile] check only .py files (#7588)
* check only .py files

* better choice of words
==

Makefile
==================
d5d2744aa;Sam Shleifer;2020-10-05 21:31:48 -0400;Support T5 Distillation w/hidden state supervision (#7599)

==

examples/seq2seq/distillation.py
examples/seq2seq/test_seq2seq_examples.py
==================
818c294fd;Lysandre Debut;2020-10-05 17:23:57 +0200;The toggle actually sticks (#7586)

==

docs/source/_static/js/custom.js
==================
03835af70;Sylvain Gugger;2020-10-05 11:01:03 -0400;Documentation fixes (#7585)

==

src/transformers/configuration_utils.py
src/transformers/file_utils.py
src/transformers/modeling_funnel.py
src/transformers/modeling_roberta.py
src/transformers/modeling_xlm.py
==================
9cf7b23b9;Julien Plu;2020-10-05 15:58:45 +0200;Custom TF weights loading (#7422)
* First try

* Fix TF utils

* Handle authorized unexpected keys when loading weights

* Add several more authorized unexpected keys

* Apply style

* Fix test

* Address Patrick's comments.

* Update src/transformers/modeling_tf_utils.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/modeling_tf_utils.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Apply style

* Make return_dict the default behavior and display a warning message

* Revert

* Replace wrong keyword

* Revert code

* Add forgot key

* Fix bug in loading PT models from a TF one.

* Fix sort

* Add a test for custom load weights in BERT

* Apply style

* Remove unused import

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

src/transformers/modeling_tf_bert.py
src/transformers/modeling_tf_pytorch_utils.py
src/transformers/modeling_tf_utils.py
tests/test_modeling_tf_bert.py
==================
d3adb985d;Sylvain Gugger;2020-10-05 09:45:47 -0400;Expand test to locate flakiness (#7580)

==

tests/test_trainer.py
==================
b2b7fc781;Sylvain Gugger;2020-10-05 09:40:45 -0400;Check and update model list in index.rst automatically (#7527)
* Check and update model list in index.rst automatically

* Check and update model list in index.rst automatically

* Adapt template
==

docs/source/index.rst
templates/adding_a_new_model/README.md
utils/check_copies.py
==================
ca05c2a47;Sylvain Gugger;2020-10-05 09:19:16 -0400;Fix post_init of some TrainingArguments (#7525)

==

src/transformers/training_args.py
==================
3bd3d8b54;Sylvain Gugger;2020-10-05 09:13:47 -0400;Add new dummy PT objects

==

src/transformers/utils/dummy_pt_objects.py
==================
28d183c90;Sylvain Gugger;2020-10-05 09:12:04 -0400;Allow soft dependencies in the namespace with ImportErrors at use (#7537)
* PoC on RAG

* Format class name/obj name

* Better name in message

* PoC on one TF model

* Add PyTorch and TF dummy objects + script

* Treat scikit-learn

* Bad copy pastes

* Typo
==

.circleci/config.yml
Makefile
src/transformers/__init__.py
src/transformers/data/__init__.py
src/transformers/data/metrics/__init__.py
src/transformers/file_utils.py
src/transformers/retrieval_rag.py
src/transformers/utils/dummy_pt_objects.py
src/transformers/utils/dummy_tf_objects.py
utils/check_dummies.py
==================
1a00f46c7;Joshua H;2020-10-05 14:21:21 +0200;Update Code example according to deprecation of AutoModeWithLMHead (#7555)
'The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.'
I dont know how to change the 'How to use this model directly from the ü§ó/transformers library:' part since it is not part of the model-paper
==

model_cards/microsoft/DialoGPT-large/README.md
model_cards/microsoft/DialoGPT-medium/README.md
model_cards/microsoft/DialoGPT-small/README.md
==================
0d79de732;Amine Abdaoui;2020-10-05 13:50:56 +0200;docs(pretrained_models): fix num parameters (#7575)
* docs(pretrained_models): fix num parameters

* fix(pretrained_models): correct typo

Co-authored-by: Amin <amin.geotrend@gmail.com>
==

docs/source/pretrained_models.rst
==================
ba5ea66e3;Malte Pietsch;2020-10-05 12:34:13 +0200;Fix tokenization in SQuAD for RoBERTa, Longformer, BART (#7387)
* fix squad tokenization for roberta & co

* change to pure type based check

* sort imports
==

src/transformers/data/processors/squad.py
==================
0270256b2;Sylvain Gugger;2020-10-05 06:33:15 -0400;Allow nested tensors in predicted logits (#7542)

==

src/transformers/trainer.py
src/transformers/trainer_utils.py
==================
60de910e6;Cola;2020-10-05 18:16:29 +0900;Add `power` argument for TF PolynomialDecay (#5732)
* :triangular_flag_on_post: Add `power` argument for TF PolynomialDecay

* :triangular_flag_on_post: Create default optimizer with power

* :triangular_flag_on_post: Add argument to training args

* :rotating_light: Clean code format

* :rotating_light: Fix black warning

* :rotating_light: Fix code format
==

src/transformers/optimization_tf.py
src/transformers/trainer_tf.py
src/transformers/training_args_tf.py
==================
41c3a3b98;Lysandre Debut;2020-10-05 10:49:39 +0200;Add Electra unexpected keys (#7569)

==

src/transformers/modeling_electra.py
==================
071970feb;Nathan Cooper;2020-10-05 04:49:17 -0400;[Model card] Java Code Summarizer model (#7568)
* Create README.md

* Update model_cards/ncoop57/bart-base-code-summarizer-java-v0/README.md

Co-authored-by: Julien Chaumond <chaumond@gmail.com>
==

model_cards/ncoop57/bart-base-code-summarizer-java-v0/README.md
==================
02ef825be;Forrest Iandola;2020-10-05 01:25:43 -0700;SqueezeBERT architecture (#7083)
* configuration_squeezebert.py

thin wrapper around bert tokenizer

fix typos

wip sb model code

wip modeling_squeezebert.py. Next step is to get the multi-layer-output interface working

set up squeezebert to use BertModelOutput when returning results.

squeezebert documentation

formatting

allow head mask that is an array of [None, ..., None]

docs

docs cont'd

path to vocab

docs and pointers to cloud files (WIP)

line length and indentation

squeezebert model cards

formatting of model cards

untrack modeling_squeezebert_scratchpad.py

update aws paths to vocab and config files

get rid of stub of NSP code, and advise users to pretrain with mlm only

fix rebase issues

redo rebase of modeling_auto.py

fix issues with code formatting

more code format auto-fixes

move squeezebert before bert in tokenization_auto.py and modeling_auto.py because squeezebert inherits from bert

tests for squeezebert modeling and tokenization

fix typo

move squeezebert before bert in modeling_auto.py to fix inheritance problem

disable test_head_masking, since squeezebert doesn't yet implement head masking

fix issues exposed by the test_modeling_squeezebert.py

fix an issue exposed by test_tokenization_squeezebert.py

fix issue exposed by test_modeling_squeezebert.py

auto generated code style improvement

issue that we inherited from modeling_xxx.py: SqueezeBertForMaskedLM.forward() calls self.cls(), but there is no self.cls, and I think the goal was actually to call self.lm_head()

update copyright

resolve failing 'test_hidden_states_output' and remove unused encoder_hidden_states and encoder_attention_mask

docs

add integration test. rename squeezebert-mnli --> squeezebert/squeezebert-mnli

autogenerated formatting tweaks

integrate feedback from patrickvonplaten and sgugger to programming style and documentation strings

* tiny change to order of imports
==

README.md
docs/source/index.rst
docs/source/model_doc/squeezebert.rst
docs/source/pretrained_models.rst
model_cards/squeezebert/squeezebert-mnli-headless/README.md
model_cards/squeezebert/squeezebert-mnli/README.md
model_cards/squeezebert/squeezebert-uncased/README.md
src/transformers/__init__.py
src/transformers/configuration_auto.py
src/transformers/configuration_squeezebert.py
src/transformers/modeling_auto.py
src/transformers/modeling_squeezebert.py
src/transformers/tokenization_auto.py
src/transformers/tokenization_squeezebert.py
templates/adding_a_new_model/README.md
templates/adding_a_new_model/modeling_xxx.py
tests/test_modeling_squeezebert.py
tests/test_tokenization_squeezebert.py
==================
e2c935f56;Sylvain Gugger;2020-10-05 04:22:12 -0400;Cleanup documentation for BART, Marian, MBART and Pegasus (#7523)
* Cleanup documentation for BART, Marian, MBART and Pegasus

* Cleanup documentation for BART, Marian, MBART and Pegasus
==

docs/source/model_doc/bart.rst
docs/source/model_doc/marian.rst
docs/source/model_doc/mbart.rst
docs/source/model_doc/pegasus.rst
src/transformers/configuration_bart.py
src/transformers/configuration_marian.py
src/transformers/configuration_mbart.py
src/transformers/configuration_pegasus.py
src/transformers/modeling_bart.py
src/transformers/modeling_marian.py
src/transformers/modeling_mbart.py
src/transformers/modeling_pegasus.py
src/transformers/tokenization_bart.py
src/transformers/tokenization_marian.py
src/transformers/tokenization_mbart.py
src/transformers/tokenization_pegasus.py
==================
5e941bece;Alexandr;2020-10-05 11:17:14 +0300;LayoutLM: add exception handling for bbox values (#7452)
* LayoutLM: add exception handling for bbox values

To replicate unhandled error:

- In `test_modelling_layoutlm.py` set `range_bbox=1025`, i.e. greater 1024
- Run `pytest tests/test_modeling_layoutlm.py`

Requirement for bbox values to be within the range 0-1000 is documented
but if it is violated then it isa not clear what is the issue from error
message.

* Update src/transformers/modeling_layoutlm.py

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
==

src/transformers/modeling_layoutlm.py
==================
2ca0fae9a;Dhaval Taunk;2020-10-05 13:27:15 +0530;added script for fine-tuning roberta for sentiment analysis task (#7505)

==

notebooks/README.md
==================
95f792afb;Sylvain Gugger;2020-10-04 17:39:23 -0400;Remove labels from the RagModel example (#7560)

==

src/transformers/modeling_rag.py
==================
99cb924bf;Suraj Patil;2020-10-04 22:12:30 +0530;[s2s] add config params like Dropout in Seq2SeqTrainingArguments (#7532)

==

examples/seq2seq/finetune_trainer.py
examples/seq2seq/seq2seq_trainer.py
examples/seq2seq/test_finetune_trainer.py
examples/seq2seq/utils.py
==================
9bdce3a4f;Sam Shleifer;2020-10-02 15:58:14 -0400;[s2s] fix lockfile and peg distillation constants (#7545)

==

examples/seq2seq/make_student.py
examples/seq2seq/sentence_splitter.py
==================
de4d7b004;Sam Shleifer;2020-10-01 17:27:45 -0400;[s2s] Adafactor support for builtin trainer (#7522)

==

examples/seq2seq/finetune_trainer.py
examples/seq2seq/seq2seq_trainer.py
examples/seq2seq/test_finetune_trainer.py
==================
d3a9601a1;Sam Shleifer;2020-10-01 17:18:47 -0400;[s2s] trainer scripts: Remove --run_name, thanks sylvain! (#7521)

==

examples/seq2seq/builtin_trainer/train_distil_marian_enro.sh
examples/seq2seq/builtin_trainer/train_distil_marian_enro_tpu.sh
examples/seq2seq/builtin_trainer/train_distilbart_cnn.sh
examples/seq2seq/builtin_trainer/train_mbart_cc25_enro.sh
==================
bdcc4b78a;Sylvain Gugger;2020-10-01 14:13:29 -0400;Fix seq2seq example test (#7518)
* Fix seq2seq example test

* Fix bad copy-paste

* Also save the state
==

examples/seq2seq/finetune_trainer.py
examples/seq2seq/test_finetune_trainer.py
src/transformers/trainer.py
==================
29baa8fab;Sylvain Gugger;2020-10-01 13:07:04 -0400;Clean the Trainer state (#7490)
* Trainer should not modify its TrainingArguments

* Trainer should not modify its TrainingArguments

* Trainer should not modify its TrainingArguments

* Add test of resumed training

* Fixes

* Non multiGPU test

* Clean Trainer state

* Add more to the state

* Documentation

* One last test

* Make resume training test more complete

* Unwanted changes
==

src/transformers/__init__.py
src/transformers/trainer.py
src/transformers/trainer_utils.py
tests/test_trainer.py
==================
2a358f45e;Sam Shleifer;2020-10-01 12:51:09 -0400;[s2s] fix nltk pytest race condition with FileLock (#7515)

==

examples/seq2seq/sentence_splitter.py
==================
72d363d97;Suraj Patil;2020-10-01 21:49:29 +0530;[examples/s2s] clean up finetune_trainer (#7509)

==

examples/seq2seq/finetune_trainer.py
examples/seq2seq/seq2seq_trainer.py
examples/seq2seq/utils.py
==================
bd2621583;Patrick von Platen;2020-10-01 18:15:41 +0200;fix data type (#7513)

==

src/transformers/modeling_utils.py
==================
62f5ae68e;Patrick von Platen;2020-10-01 17:38:50 +0200;[Seq2Seq] Fix a couple of bugs and clean examples (#7474)
* clean T5

* fix t5 tests

* fix index typo

* fix tf common test

* fix examples

* change positional ordering for Bart and FSTM

* add signature test

* clean docs and add tests

* add docs to encoder decoder

* clean docs

* correct two doc strings

* remove sig test for TF Elektra & Funnel

* fix tf t5 slow tests

* fix input_ids to inputs in tf

* Update src/transformers/modeling_bart.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/modeling_bart.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* implement lysandre results

* make style

* fix encoder decoder typo

* fix tf slow tests

* fix slow tests

* renaming

* remove unused input

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

src/transformers/modeling_bart.py
src/transformers/modeling_ctrl.py
src/transformers/modeling_encoder_decoder.py
src/transformers/modeling_fsmt.py
src/transformers/modeling_funnel.py
src/transformers/modeling_gpt2.py
src/transformers/modeling_outputs.py
src/transformers/modeling_rag.py
src/transformers/modeling_reformer.py
src/transformers/modeling_t5.py
src/transformers/modeling_tf_electra.py
src/transformers/modeling_tf_funnel.py
src/transformers/modeling_tf_gpt2.py
src/transformers/modeling_tf_outputs.py
src/transformers/modeling_tf_t5.py
src/transformers/modeling_tf_xlnet.py
src/transformers/modeling_utils.py
src/transformers/modeling_xlnet.py
tests/test_modeling_bart.py
tests/test_modeling_common.py
tests/test_modeling_fsmt.py
tests/test_modeling_gpt2.py
tests/test_modeling_t5.py
tests/test_modeling_tf_common.py
tests/test_modeling_tf_gpt2.py
tests/test_modeling_tf_t5.py
tests/test_modeling_xlnet.py
==================
a42f62d34;Muhammad Harris;2020-10-01 19:54:29 +0500;Train T5 in Tensoflow 2 Community Notebook (#7428)
* t5 t5 community notebook added

* author link updated

* t5 t5 community notebook added

* author link updated

* new colab link updated

Co-authored-by: harris <muhammad.harris@visionx.io>
==

notebooks/README.md
==================
5fc3b5cba;Kai Fricke;2020-10-01 15:34:31 +0100;Fix Tune progress_reporter kwarg (#7508)

==

src/transformers/integrations.py
==================
dabc85d1b;Kai Fricke;2020-10-01 14:52:36 +0100;Report Tune metrics in final evaluation (#7507)

==

src/transformers/integrations.py
==================
9a92afb6d;Alexandr;2020-10-01 16:11:42 +0300;Update LayoutLM doc (#7388)
Co-authored-by: Alexandr Maslov <avmaslov3@gmail.com>
==

docs/source/model_doc/layoutlm.rst
==================
e32390931;Julien Chaumond;2020-10-01 09:08:49 -0400;[model_card] distilbert-base-german-cased

==

model_cards/distilbert-base-german-cased-README.md
==================
9a4e163b5;Julien Chaumond;2020-10-01 08:54:06 -0400;[model_card] Fix metadata, adalbertojunior/PTT5-SMALL-SUM

==

model_cards/adalbertojunior/PTT5-SMALL-SUM/README.md
==================
8435e10e2;Adalberto;2020-10-01 09:52:28 -0300;Create README.md (#7299)
* Create README.md

* language metadata

Co-authored-by: Julien Chaumond <chaumond@gmail.com>
==

model_cards/adalbertojunior/PTT5-SMALL-SUM/README.md
==================
d72743207;Martin M√ºller;2020-10-01 14:51:26 +0200;Update README.md (#7459)

==

model_cards/digitalepidemiologylab/covid-twitter-bert/README.md
==================
664da5b07;allenyummy;2020-10-01 20:50:26 +0800;Create README.md (#7468)

==

model_cards/allenyummy/chinese-bert-wwm-ehr-ner-sl/README.md
==================
f745f61c9;ahotrod;2020-10-01 05:50:07 -0700;Update README.md (#7491)
Model now fine-tuned on Transformers 3.1.0, previous out-of-date model was fine-tuned on Transformers 2.3.0.
==

model_cards/ahotrod/albert_xxlargev1_squad2_512/README.md
==================
6ef7658c0;Abed khooli;2020-10-01 15:48:51 +0300;Create README.md (#7349)
Model card for akhooli/personachat-arabic
==

model_cards/akhooli/personachat-arabic/README.md
==================
15ab3f049;Bayartsogt Yadamsuren;2020-10-01 20:46:27 +0800;Creating readme for bert-base-mongolian-cased (#7439)
* Creating readme for bert-base-mongolian-cased

* Update model_cards/bayartsogt/bert-base-mongolian-cased/README.md

Co-authored-by: Julien Chaumond <chaumond@gmail.com>
==

model_cards/bayartsogt/bert-base-mongolian-cased/README.md
==================
0c2b9fa83;Bayartsogt Yadamsuren;2020-10-01 20:45:22 +0800;creating readme for bert-base-mongolian-uncased (#7440)

==

model_cards/bayartsogt/bert-base-mongolian-uncased/README.md
==================
381443c09;Akshay Gupta;2020-10-01 17:12:07 +0530;Update README.md (#7498)
Making transformers readme more robust.
==

README.md
==================
85d2d8c92;Lysandre Debut;2020-10-01 11:06:02 +0200;Fix local_files_only for TF (#6091)

==

src/transformers/file_utils.py
==================
9e80f972f;Sam Shleifer;2020-10-01 04:48:37 -0400;Enable pegasus fp16 by clamping large activations (#7243)
* Clean clamp

* boom boom

* Take some other changes

* boom boom

* boom boom

* boom boom

* one chg

* fix test

* Use finfo

* style
==

src/transformers/modeling_bart.py
tests/test_modeling_pegasus.py
==================
be51c1039;Sylvain Gugger;2020-10-01 04:41:29 -0400;Add forgotten return_dict argument in the docs (#7483)

==

docs/source/task_summary.rst
==================
48f23f92a;Sam Shleifer;2020-10-01 00:33:01 -0400;[s2sTrainer] test + code cleanup (#7467)

==

examples/seq2seq/finetune.py
examples/seq2seq/finetune_trainer.py
examples/seq2seq/seq2seq_trainer.py
examples/seq2seq/test_finetune_trainer.py
examples/seq2seq/utils.py
==================
097049b81;Sam Shleifer;2020-09-30 22:14:14 -0400;Distributed Trainer: 2 little fixes (#7461)
* reset model.config

* Update src/transformers/trainer.py

* use lower case tensor

* Just tensor change
==

src/transformers/trainer_utils.py
==================
0acd1ffa0;Julien Chaumond;2020-09-30 17:31:08 -0400;[doc] rm Azure buttons as not implemented yet

==

examples/README.md
==================
03e46c1de;Sam Shleifer;2020-09-30 17:00:06 -0400;[s2s] fix kwargs style (#7488)

==

examples/seq2seq/run_distributed_eval.py
==================
6fe8a693e;Sam Shleifer;2020-09-30 16:58:03 -0400;[s2s] Fix t5 warning for distributed eval (#7487)

==

examples/seq2seq/run_distributed_eval.py
==================
4c6728460;Sylvain Gugger;2020-09-30 13:44:58 -0400;Bump isort version. (#7484)

==

setup.py
==================
c031d0102;Amanpreet Singh;2020-09-30 13:27:48 -0400;Seq2SeqDataset: avoid passing src_lang everywhere (#7470)
Co-authored-by: Sam Shleifer <sshleifer@gmail.com>
==

examples/seq2seq/test_datasets.py
examples/seq2seq/utils.py
==================
08939cfdf;Suraj Patil;2020-09-30 22:09:13 +0530;[s2strainer] fix eval dataset loading (#7477)

==

examples/seq2seq/finetune_trainer.py
==================
a97a73e0e;Sylvain Gugger;2020-09-30 12:12:03 -0400;Small QOL improvements to TrainingArguments (#7475)
* Small QOL improvements to TrainingArguments

* With the self.
==

src/transformers/training_args.py
==================
dc7d2daa4;Sylvain Gugger;2020-09-30 10:43:58 -0400;Alphabetize model lists (#7478)

==

README.md
docs/source/index.rst
==================
fdccf82e2;Sylvain Gugger;2020-09-30 09:03:25 -0400;Remove config assumption in Trainer (#7464)
* Remove config assumption in Trainer

* Initialize for eval
==

src/transformers/trainer.py
src/transformers/trainer_utils.py
==================
cc4eff808;Fran√ßois REMY;2020-09-30 13:44:40 +0200;Make transformers install check positive (#7473)
When transformers is correctly installed, you should get a positive message ^_^
==

docs/source/installation.md
==================
7a0cf0ec9;Pengcheng He;2020-09-30 04:07:30 -0700;Add DeBERTa model (#5929)
* Add DeBERTa model

* Remove dependency of deberta

* Address comments

* Patch DeBERTa
Documentation
Style

* Add final tests

* Style

* Enable tests + nitpicks

* position IDs

* BERT -> DeBERTa

* Quality

* Style

* Tokenization

* Last updates.

* @patrickvonplaten's comments

* Not everything can be a copy

* Apply most of @sgugger's review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Last reviews

* DeBERTa -> Deberta

Co-authored-by: Lysandre <lysandre.debut@reseau.eseo.fr>
Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

README.md
docs/source/index.rst
docs/source/model_doc/deberta.rst
docs/source/pretrained_models.rst
model_cards/microsoft/DeBERTa-base/README.md
model_cards/microsoft/DeBERTa-large/README.md
src/transformers/__init__.py
src/transformers/activations.py
src/transformers/configuration_auto.py
src/transformers/configuration_deberta.py
src/transformers/modeling_auto.py
src/transformers/modeling_deberta.py
src/transformers/tokenization_auto.py
src/transformers/tokenization_deberta.py
tests/test_modeling_deberta.py
tests/test_tokenization_deberta.py
==================
44a93c981;Lysandre Debut;2020-09-30 12:53:20 +0200;Number of GPUs for multi-gpu (#7472)

==

.github/workflows/self-push.yml
.github/workflows/self-scheduled.yml
==================
886ef35ce;Lysandre Debut;2020-09-30 12:41:24 +0200;Fix LXMERT with DataParallel (#7471)

==

src/transformers/modeling_lxmert.py
==================
35e94c68d;Lysandre;2020-09-30 12:29:26 +0200;Number of GPUs

==

.github/workflows/self-push.yml
.github/workflows/self-scheduled.yml
==================
056723ad1;Lysandre Debut;2020-09-30 11:53:34 +0200;Multi-GPU setup (#7453)

==

.github/workflows/self-push.yml
.github/workflows/self-scheduled.yml
==================
4ba248748;Sylvain Gugger;2020-09-30 04:05:14 -0400;Get a better error when check_copies fails (#7457)
* Get a better error when check_copies fails

* Fix tests
==

tests/test_utils_check_copies.py
utils/check_copies.py
==================
bef017516;Sam Shleifer;2020-09-29 15:16:43 -0400;remove codecov PR comments (#7400)

==

codecov.yml
==================
a1c2ef7bd;Sylvain Gugger;2020-09-29 14:31:43 -0400;Add documentation for v3.3.1

==

docs/source/_static/js/custom.js
==================
1ba08dc22;Sylvain Gugger;2020-09-29 14:17:34 -0400;Release: v3.3.1

==

docs/source/conf.py
setup.py
src/transformers/__init__.py
==================
8546dc55c;Sylvain Gugger;2020-09-29 14:06:41 -0400;Fix Trainer tests in a multiGPU env (#7458)

==

tests/test_trainer.py
==================
d0fd7154c;Sylvain Gugger;2020-09-29 13:42:09 -0400;Catch import datasets common errors (#7456)

==

src/transformers/file_utils.py
==================
f1220c5fe;Sylvain Gugger;2020-09-29 13:38:47 -0400;Add a code of conduct (#7433)

==

CODE_OF_CONDUCT.md
CONTRIBUTING.md
README.md
==================
9e9a1fb8c;Teven;2020-09-29 18:26:26 +0200;Adding gradient checkpointing to GPT2 (#7446)
* GPT2 gradient checkpointing

* find_unused_parameters removed if checkpointing

* find_unused_parameters removed if checkpointing

* Update src/transformers/configuration_gpt2.py

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* Added a test for generation with checkpointing

* Update src/transformers/configuration_gpt2.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

src/transformers/configuration_gpt2.py
src/transformers/modeling_gpt2.py
src/transformers/trainer.py
tests/test_modeling_gpt2.py
==================
52e8392b7;Sylvain Gugger;2020-09-29 10:41:18 -0400;Add automatic best model loading to Trainer (#7431)
* Add automatic best model loading to Trainer

* Some small fixes

* Formatting
==

src/transformers/trainer.py
src/transformers/trainer_utils.py
src/transformers/training_args.py
tests/test_trainer.py
==================
1fc4de69e;Sylvain Gugger;2020-09-29 03:56:57 -0400;Document new features of make fixup (#7434)

==

CONTRIBUTING.md
==================
205bf0b7e;GmailB;2020-09-29 09:18:01 +0200;Update README.md (#7444)
Hi, just corrected the example code, add 2 links and fixed some typos
==

model_cards/unideeplearning/polibert_sa/README.md
==================
74d8d69bd;Sam Shleifer;2020-09-28 23:20:03 -0400;[s2s] consistent output format across eval scripts (#7435)

==

examples/seq2seq/rouge_cli.py
examples/seq2seq/run_eval.py
==================
671b278e2;Typicasoft;2020-09-28 18:25:25 -0400;Create README.md (#7436)
* Create README.md

MagBERT-NER : Added widget (Text)

* Rename model_cards/README.md to model_cards/TypicaAI/magbert-ner/README.md
==

model_cards/TypicaAI/magbert-ner/README.md
==================
a1a8ffa51;Manuel Romero;2020-09-28 19:40:09 +0200;Update README.md (#7429)
Add links to models fine-tuned on a downstream task
==

model_cards/mrm8488/electricidad-base-discriminator/README.md
==================
f62f2ffdc;Stas Bekman;2020-09-28 07:45:42 -0700;[makefile] 10x speed up checking/fixing  (#7403)
* [makefile] check/fix only modified since branching files

* fix phonies

* parametrize dirs

* have only one source for dirs to check

* look ma, no autoformatters here
==

Makefile
==================
16c213820;Lysandre;2020-09-28 16:32:00 +0200;Update docs to version v3.3.0

==

.circleci/deploy.sh
docs/source/_static/js/custom.js
==================
0613f0522;Lysandre;2020-09-28 15:30:57 +0200;Release: v3.3.0

==

docs/source/conf.py
setup.py
src/transformers/__init__.py
==================
ca3fc36de;Sylvain Gugger;2020-09-28 10:22:58 -0400;Reorganize documentation navbar (#7423)
* Reorganize documentation navbar

* Update css to have clear sections
==

docs/source/_static/css/huggingface.css
docs/source/index.rst
==================
7f4115c09;Lysandre Debut;2020-09-28 15:51:49 +0200;Pull request template (#7392)
co-authored-by: sgugger <sylvain.gugger@gmail.com>

Co-authored-by: sgugger <sylvain.gugger@gmail.com>
==

.github/PULL_REQUEST_TEMPLATE.md
==================
0611eab5e;Sylvain Gugger;2020-09-28 08:31:46 -0400;Document RAG again (#7377)
Do not merge before Monday
==

docs/source/index.rst
docs/source/model_doc/rag.rst
docs/source/model_summary.rst
utils/check_repo.py
==================
7563d5a3c;Sylvain Gugger;2020-09-28 08:20:10 -0400;Catch PyTorch warning when saving/loading scheduler (#7401)

==

src/transformers/trainer.py
==================
1749ca317;Boris Dayma;2020-09-28 07:17:30 -0500;docs: fix model sharing file names (#5855)
* docs: fix model sharing file names

* Update docs/source/model_sharing.rst

Co-authored-by: Julien Chaumond <chaumond@gmail.com>

* docs(model_sharing.rst): fix new line

Co-authored-by: Julien Chaumond <chaumond@gmail.com>
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

docs/source/model_sharing.rst
==================
827947150;Patrick von Platen;2020-09-28 11:08:39 +0200;correct RAG model cards (#7420)

==

model_cards/facebook/rag-sequence-nq/README.md
model_cards/facebook/rag-token-base/README.md
model_cards/facebook/rag-token-nq/README.md
==================
4083a55ab;Marcin Zab≈Çocki;2020-09-28 10:09:26 +0200;Flos fix (#7384)

==

src/transformers/trainer.py
tests/test_trainer.py
==================
ae3e84f3b;Ola Piktus;2020-09-28 09:06:39 +0100;[RAG] Clean Rag readme in examples (#7413)
* Improve README + consolidation script

* Reformat README

* Reformat README

Co-authored-by: Your Name <you@example.com>
==

examples/rag/README.md
examples/rag/consolidate_rag_checkpoint.py
==================
748425d47;Sam Shleifer;2020-09-28 03:08:04 -0400;[T5] allow config.decoder_layers to control decoder size (#7409)
* Working assymmetrical T5

* rename decoder_layers -> num_decoder_layers

* Fix docstring

* Allow creation of asymmetric t5 students
==

examples/seq2seq/make_student.py
examples/seq2seq/test_make_student.py
src/transformers/configuration_t5.py
src/transformers/modeling_t5.py
tests/test_modeling_t5.py
==================
7296fea1d;Sam Shleifer;2020-09-27 16:27:19 -0400;[s2s] rougeLSum expects \n between sentences (#7410)
Co-authored-by: Swetha Mandava <smandava@nvidia.com>
==

examples/requirements.txt
examples/seq2seq/rouge_cli.py
examples/seq2seq/run_eval_search.py
examples/seq2seq/sentence_splitter.py
examples/seq2seq/test_calculate_rouge.py
examples/seq2seq/test_seq2seq_examples.py
examples/seq2seq/utils.py
==================
eab5f5968;Suraj Patil;2020-09-28 00:40:46 +0530;[s2s] add create student script (#7290)
Co-authored-by: Suraj Patil <surajp815@gmail.com>
Co-authored-by: Sam Shleifer <sshleifer@gmail.com>
==

examples/seq2seq/README.md
examples/seq2seq/distillation.py
examples/seq2seq/initialization_utils.py
examples/seq2seq/make_student.py
examples/seq2seq/save_randomly_initialized_model.py
examples/seq2seq/test_data/wmt_en_ro/train.len
examples/seq2seq/test_data/wmt_en_ro/val.len
examples/seq2seq/test_make_student.py
examples/seq2seq/train_distilbart_xsum.sh
==================
e50a931c1;Patrick von Platen;2020-09-25 20:33:21 +0200;[Longformer, Bert, Roberta, ...] Fix multi gpu training (#7272)
* fix multi-gpu

* fix longformer

* force to delete unnecessary layers

* fix notifications

* fix warning

* fix roberta

* fix tests

* remove hasattr

* fix tests

* fix roberta

* merge and clean authorized keys
==

src/transformers/configuration_longformer.py
src/transformers/configuration_utils.py
src/transformers/modeling_albert.py
src/transformers/modeling_bert.py
src/transformers/modeling_longformer.py
src/transformers/modeling_mobilebert.py
src/transformers/modeling_roberta.py
src/transformers/modeling_tf_albert.py
src/transformers/modeling_tf_bert.py
src/transformers/modeling_tf_longformer.py
src/transformers/modeling_tf_mobilebert.py
src/transformers/modeling_tf_pytorch_utils.py
src/transformers/modeling_tf_roberta.py
src/transformers/modeling_tf_utils.py
src/transformers/modeling_utils.py
tests/test_modeling_auto.py
==================
2c8ecdf8a;Patrick von Platen;2020-09-25 19:47:12 +0200;fix rag retriever save pretrained (#7399)

==

src/transformers/retrieval_rag.py
tests/test_retrieval_rag.py
==================
1a14687e6;Patrick von Platen;2020-09-25 19:43:48 +0200;Update README.md

==

model_cards/facebook/rag-token-base/README.md
==================
3327c2b0f;Patrick von Platen;2020-09-25 19:43:36 +0200;Update README.md

==

model_cards/facebook/rag-sequence-base/README.md
==================
fe326bd5c;Ola Piktus;2020-09-25 17:20:49 +0100;Remove dependency on examples/seq2seq from rag (#7395)
Co-authored-by: Your Name <you@example.com>
==

examples/rag/callbacks.py
examples/rag/finetune.py
examples/rag/utils.py
==================
ad39271ae;Sylvain Gugger;2020-09-25 12:20:39 -0400;Fix FP16 and attention masks in FunnelTransformer (#7374)
* Fix #7371

* Fix training

* Fix test values

* Apply the fix to TF as well
==

src/transformers/modeling_funnel.py
src/transformers/modeling_tf_funnel.py
tests/test_modeling_funnel.py
==================
4e5b036bd;Patrick von Platen;2020-09-25 18:16:46 +0200;Update README.md

==

model_cards/facebook/rag-token-nq/README.md
==================
55eccfbb4;Patrick von Platen;2020-09-25 18:16:44 +0200;Update README.md

==

model_cards/facebook/rag-sequence-nq/README.md
==================
e2e77f02c;Sylvain Gugger;2020-09-25 11:48:13 -0400;Fix BartModel output documentation (#7390)

==

src/transformers/modeling_bart.py
==================
bbb07830f;Sylvain Gugger;2020-09-25 11:47:22 -0400;Speedup check_copies script (#7394)

==

utils/check_copies.py
==================
8859c4f84;Stas Bekman;2020-09-25 08:37:40 -0700;[code quality] new make target that combines style and quality targets (#7310)
* [code quality] merge style and quality targets

Any reason why we don't run `flake8` in `make style`? I find myself needing to run `make style` and `make quality` all the time, but I need the latter just for the last 2 checks. Since we have no control over the source code why bother with separating checking and fixing - let's just have one target that fixes and then performs the remaining checks, as we know the first two have been done already.

This PR suggests to merge the 2 targets into one efficient target.

I will edit the docs if this change resonates with the team.

* move checks into style, re-use target

* better name

* add fixup target

* document new target
==

CONTRIBUTING.md
Makefile
==================
38a1b03f4;Sam Shleifer;2020-09-25 11:01:07 -0400;Remove unhelpful bart warning (#7391)

==

src/transformers/modeling_bart.py
==================
5ff0d6d7d;Patrick von Platen;2020-09-25 16:58:29 +0200;Update README.md

==

model_cards/facebook/rag-token-nq/README.md
==================
cf1c88e09;Quentin Lhoest;2020-09-25 16:12:46 +0200;[RAG] Fix retrieval offset in RAG's HfIndex and better integration tests (#7372)
* Fix retrieval offset in RAG's HfIndex

* update slow tests

* style

* fix new test

* style

* add better tests

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
==

examples/rag/test_distributed_retriever.py
src/transformers/retrieval_rag.py
tests/test_modeling_rag.py
tests/test_retrieval_rag.py
==================
571c7a11c;Patrick von Platen;2020-09-25 14:35:49 +0200;[Rag] Fix wrong usage of `num_beams` and `bos_token_id` in Rag Sequence generation (#7386)
* fix_rag_sequence

* add second bug fix
==

src/transformers/modeling_rag.py
==================
415071b4c;Suraj Patil;2020-09-25 17:30:36 +0530;doc changes (#7385)

==

examples/README.md
examples/seq2seq/README.md
==================
2dd652d75;Patrick von Platen;2020-09-25 11:23:55 +0200;[RAG] Add missing doc and attention_mask to rag (#7382)
* add docs

* add missing docs and attention_mask in fine-tune
==

examples/rag/finetune.py
src/transformers/modeling_rag.py
==================
7cdd9da5b;Lysandre Debut;2020-09-25 11:09:09 +0200;Check config type using `type` instead of `isinstance` (#7363)
* Check config type instead of instance


Bad merge

* Remove for loops

* Style
==

src/transformers/modeling_auto.py
src/transformers/modeling_tf_auto.py
src/transformers/tokenization_auto.py
==================
3c6bf8998;Sam Shleifer;2020-09-25 04:24:14 -0400;modeling_bart: 3 small cleanups that dont change outputs (#7381)
* Mbart passing

* boom boom

* cleaner assert

* add assert

* Fix tests
==

src/transformers/modeling_bart.py
tests/test_modeling_mbart.py
==================
9e68d075a;Suraj Patil;2020-09-25 04:16:58 +0530;Seq2SeqTrainer (#6769)
Co-authored-by: Sam Shleifer <sshleifer@gmail.com>
==

examples/seq2seq/README.md
examples/seq2seq/builtin_trainer/finetune.sh
examples/seq2seq/builtin_trainer/finetune_tpu.sh
examples/seq2seq/builtin_trainer/train_distil_marian_enro.sh
examples/seq2seq/builtin_trainer/train_distil_marian_enro_tpu.sh
examples/seq2seq/builtin_trainer/train_distilbart_cnn.sh
examples/seq2seq/builtin_trainer/train_mbart_cc25_enro.sh
examples/seq2seq/finetune_trainer.py
examples/seq2seq/seq2seq_trainer.py
examples/seq2seq/test_finetune_trainer.py
examples/seq2seq/xla_spawn.py
==================
d9d0f1140;Sam Shleifer;2020-09-24 17:30:09 -0400;[s2s] distributed eval allows num_return_sequences > 1 (#7254)

==

examples/seq2seq/README.md
examples/seq2seq/run_distributed_eval.py
examples/seq2seq/run_eval.py
examples/seq2seq/test_seq2seq_examples.py
examples/seq2seq/utils.py
==================
0804d077c;Patrick von Platen;2020-09-24 23:22:04 +0200;correct attention mask (#7373)

==

examples/rag/eval_rag.py
src/transformers/modeling_rag.py
tests/test_modeling_rag.py
==================
a8cbc4269;Stas Bekman;2020-09-24 14:10:26 -0700;[fsmt] build/test scripts (#7257)
Co-authored-by: Sam Shleifer <sshleifer@gmail.com>
==

scripts/fsmt/convert-allenai-wmt16.sh
scripts/fsmt/convert-allenai-wmt19.sh
scripts/fsmt/convert-facebook-wmt19.sh
scripts/fsmt/eval-allenai-wmt16.sh
scripts/fsmt/eval-allenai-wmt19.sh
scripts/fsmt/eval-facebook-wmt19.sh
scripts/fsmt/s3-move.sh
scripts/fsmt/tests-to-run.sh
==================
a8e7982f8;Sylvain Gugger;2020-09-24 17:07:14 -0400;Remove mentions of  RAG from the docs (#7376)
* Remove mentions of  RAG from the docs

* Deactivate check
==

docs/source/index.rst
docs/source/model_doc/rag.rst
docs/source/model_summary.rst
utils/check_repo.py
==================
eadd870b2;Stas Bekman;2020-09-24 12:23:48 -0700;[seq2seq] make it easier to run the scripts (#7274)

==

examples/seq2seq/README.md
examples/seq2seq/convert_model_to_fp16.py
examples/seq2seq/convert_pl_checkpoint_to_hf.py
examples/seq2seq/distillation.py
examples/seq2seq/download_wmt.py
examples/seq2seq/finetune.py
examples/seq2seq/finetune.sh
examples/seq2seq/minify_dataset.py
examples/seq2seq/pack_dataset.py
examples/seq2seq/run_distributed_eval.py
examples/seq2seq/run_eval.py
examples/seq2seq/run_eval_search.py
examples/seq2seq/save_len_file.py
examples/seq2seq/test_bash_script.py
examples/seq2seq/test_data/wmt_en_ro/train.len
examples/seq2seq/test_data/wmt_en_ro/val.len
examples/seq2seq/test_datasets.py
examples/seq2seq/test_fsmt_bleu_score.py
==================
8d3bb781e;Lysandre Debut;2020-09-24 16:59:21 +0200;Formatter (#7368)
* Formatter

* Docs
==

docs/source/main_classes/logging.rst
src/transformers/utils/logging.py
==================
7dfdf793b;Teven;2020-09-24 15:56:40 +0200;Fixing case in which `Trainer` hung while saving model in distributed training (#7365)
* remote debugging

* remote debugging

* moved _store_flos call

* moved _store_flos call

* moved _store_flos call

* removed debugging artefacts
==

src/transformers/trainer.py
==================
0ccb6f5c6;Sylvain Gugger;2020-09-24 09:24:41 -0400;Clean RAG docs and template docs (#7348)
* Clean RAG docs and template docs

* Fix typo

* Better doc
==

docs/source/model_doc/rag.rst
src/transformers/configuration_rag.py
src/transformers/modeling_rag.py
src/transformers/retrieval_rag.py
src/transformers/tokenization_rag.py
templates/adding_a_new_model/configuration_xxx.py
templates/adding_a_new_model/modeling_tf_xxx.py
templates/adding_a_new_model/modeling_xxx.py
templates/adding_a_new_model/tokenization_xxx.py
==================
27174bd4f;Sylvain Gugger;2020-09-24 08:53:54 -0400;Make PyTorch model files independent from each other (#7352)

==

src/transformers/activations.py
src/transformers/modeling_albert.py
src/transformers/modeling_bert.py
src/transformers/modeling_electra.py
src/transformers/modeling_funnel.py
src/transformers/modeling_layoutlm.py
src/transformers/modeling_longformer.py
src/transformers/modeling_lxmert.py
src/transformers/modeling_mobilebert.py
src/transformers/modeling_reformer.py
src/transformers/modeling_retribert.py
src/transformers/modeling_roberta.py
src/transformers/modeling_xlnet.py
==================
d161ed168;Julien Plu;2020-09-24 14:30:59 +0200;Update the TF models to remove their interdependencies (#7238)
* Refacto the models to remove their interdependencies

* Fix Flaubert model

* Fix Flaubert

* Fix XLM

* Fix Albert

* Fix Roberta

* Fix Albert

* Fix Flaubert

* Apply style + remove unused imports

* Fix Distilbert

* remove unused import

* fix Distilbert

* Fix Flaubert

* Apply style

* Fix Flaubert

* Add the copy comments for the check_copies script

* Fix MobileBert model name

* Address Morgan's comments

* Fix typo

* Oops typo
==

src/transformers/modeling_tf_albert.py
src/transformers/modeling_tf_bert.py
src/transformers/modeling_tf_distilbert.py
src/transformers/modeling_tf_electra.py
src/transformers/modeling_tf_flaubert.py
src/transformers/modeling_tf_longformer.py
src/transformers/modeling_tf_mobilebert.py
src/transformers/modeling_tf_roberta.py
src/transformers/modeling_tf_xlm.py
==================
0cffa424f;Jabin Huang;2020-09-24 18:52:10 +0800;Updata tokenization_auto.py (#6870)
Updata tokenization_auto.py to handle Inherited tokenizer
==

src/transformers/tokenization_auto.py
==================
03fb8e79c;Daquan Lin;2020-09-24 17:37:29 +0800;Update modeling_tf_longformer.py (#7359)
correct a very small mistake
==

src/transformers/modeling_tf_longformer.py
==================
1ff5bd38a;Sylvain Gugger;2020-09-24 04:54:37 -0400;Check decorator order (#7326)
* Check decorator order

* Adapt for parametrized decorators

* Fix typos
==

tests/test_tokenization_bert_generation.py
tests/test_tokenization_common.py
tests/test_tokenization_reformer.py
utils/check_repo.py
==================
0be5f4a00;Sylvain Gugger;2020-09-24 04:34:18 -0400;Expand a bit the documentation doc (#7350)

==

docs/README.md
==================
38f170379;Sam Shleifer;2020-09-23 18:11:06 -0400;wip: Code to add lang tags to marian model cards (#6586)

==

src/transformers/convert_marian_to_pytorch.py
==================
129fdae04;Theo Linnemann;2020-09-23 13:56:21 -0400;Remove reference to args in XLA check (#7344)
Previously, the TFTrainingArguments object did a check to see if XLA was enabled, but did this by referencing `self.args.xla`, when it should be `self.xla`, because it is the args object. This can be verified a few lines above, where the XLA field is set.
==

src/transformers/training_args_tf.py
==================
d26661363;Felipe Curti;2020-09-23 14:25:24 -0300;[Benchmarks] Change all args to from `no_...` to their positive form (#7075)
* Changed name to all no_... arguments and all references to them, inverting the boolean condition

* Change benchmark tests to use new Benchmark Args

* Update src/transformers/benchmark/benchmark_args_utils.py

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* Update src/transformers/benchmark/benchmark.py

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* Fix Style. Add --no options in help

* fix some part of tests

* Update src/transformers/benchmark/benchmark_args_utils.py

* Update src/transformers/benchmark/benchmark_args_utils.py

* Update src/transformers/benchmark/benchmark_args_utils.py

* fix all tests

* make style

* add backwards compability

* make backwards compatible

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
Co-authored-by: fmcurti <fcurti@DESKTOP-RRQURBM.localdomain>
==

examples/benchmarking/run_benchmark.py
examples/benchmarking/run_benchmark_tf.py
src/transformers/benchmark/benchmark.py
src/transformers/benchmark/benchmark_args.py
src/transformers/benchmark/benchmark_args_tf.py
src/transformers/benchmark/benchmark_args_utils.py
src/transformers/benchmark/benchmark_tf.py
src/transformers/benchmark/benchmark_utils.py
tests/test_benchmark.py
tests/test_benchmark_tf.py
==================
8c697d58e;Doug Blank;2020-09-23 13:23:45 -0400;Ensure that integrations are imported before transformers or ml libs (#7330)
* Ensure that intergrations are imported before transformers or ml libs

* Black reformatter wanted a newline

* isort requests

* black requests

* flake8 requests
==

src/transformers/__init__.py
src/transformers/integrations.py
==================
3323146e9;Sylvain Gugger;2020-09-23 13:20:45 -0400;Models doc (#7345)
* Clean up model documentation

* Formatting

* Preparation work

* Long lines

* Main work on rst files

* Cleanup all config files

* Syntax fix

* Clean all tokenizers

* Work on first models

* Models beginning

* FaluBERT

* All PyTorch models

* All models

* Long lines again

* Fixes

* More fixes

* Update docs/source/model_doc/bert.rst

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* Update docs/source/model_doc/electra.rst

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* Last fixes

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
==

docs/source/benchmarks.rst
docs/source/bertology.rst
docs/source/converting_tensorflow_models.rst
docs/source/custom_datasets.rst
docs/source/glossary.rst
docs/source/index.rst
docs/source/internal/modeling_utils.rst
docs/source/internal/pipelines_utils.rst
docs/source/internal/tokenization_utils.rst
docs/source/main_classes/configuration.rst
docs/source/main_classes/logging.rst
docs/source/main_classes/model.rst
docs/source/main_classes/optimizer_schedules.rst
docs/source/main_classes/output.rst
docs/source/main_classes/pipelines.rst
docs/source/main_classes/processors.rst
docs/source/main_classes/tokenizer.rst
docs/source/main_classes/trainer.rst
docs/source/model_doc/albert.rst
docs/source/model_doc/auto.rst
docs/source/model_doc/bart.rst
docs/source/model_doc/bert.rst
docs/source/model_doc/bertgeneration.rst
docs/source/model_doc/camembert.rst
docs/source/model_doc/ctrl.rst
docs/source/model_doc/dialogpt.rst
docs/source/model_doc/distilbert.rst
docs/source/model_doc/dpr.rst
docs/source/model_doc/electra.rst
docs/source/model_doc/encoderdecoder.rst
docs/source/model_doc/flaubert.rst
docs/source/model_doc/fsmt.rst
docs/source/model_doc/funnel.rst
docs/source/model_doc/gpt.rst
docs/source/model_doc/gpt2.rst
docs/source/model_doc/longformer.rst
docs/source/model_doc/lxmert.rst
docs/source/model_doc/marian.rst
docs/source/model_doc/mbart.rst
docs/source/model_doc/mobilebert.rst
docs/source/model_doc/pegasus.rst
docs/source/model_doc/reformer.rst
docs/source/model_doc/retribert.rst
docs/source/model_doc/roberta.rst
docs/source/model_doc/t5.rst
docs/source/model_doc/transformerxl.rst
docs/source/model_doc/xlm.rst
docs/source/model_doc/xlmroberta.rst
docs/source/model_doc/xlnet.rst
docs/source/model_sharing.rst
docs/source/model_summary.rst
docs/source/multilingual.rst
docs/source/perplexity.rst
docs/source/philosophy.rst
docs/source/preprocessing.rst
docs/source/pretrained_models.rst
docs/source/quicktour.rst
docs/source/serialization.rst
docs/source/task_summary.rst
docs/source/testing.rst
docs/source/tokenizer_summary.rst
docs/source/training.rst
src/transformers/configuration_albert.py
src/transformers/configuration_bert.py
src/transformers/configuration_bert_generation.py
src/transformers/configuration_ctrl.py
src/transformers/configuration_distilbert.py
src/transformers/configuration_dpr.py
src/transformers/configuration_electra.py
src/transformers/configuration_encoder_decoder.py
src/transformers/configuration_flaubert.py
src/transformers/configuration_fsmt.py
src/transformers/configuration_funnel.py
src/transformers/configuration_gpt2.py
src/transformers/configuration_longformer.py
src/transformers/configuration_lxmert.py
src/transformers/configuration_mmbt.py
src/transformers/configuration_mobilebert.py
src/transformers/configuration_openai.py
src/transformers/configuration_reformer.py
src/transformers/configuration_retribert.py
src/transformers/configuration_roberta.py
src/transformers/configuration_t5.py
src/transformers/configuration_transfo_xl.py
src/transformers/configuration_xlm.py
src/transformers/configuration_xlnet.py
src/transformers/file_utils.py
src/transformers/modeling_albert.py
src/transformers/modeling_bert.py
src/transformers/modeling_bert_generation.py
src/transformers/modeling_camembert.py
src/transformers/modeling_ctrl.py
src/transformers/modeling_distilbert.py
src/transformers/modeling_dpr.py
src/transformers/modeling_electra.py
src/transformers/modeling_encoder_decoder.py
src/transformers/modeling_flaubert.py
src/transformers/modeling_fsmt.py
src/transformers/modeling_funnel.py
src/transformers/modeling_gpt2.py
src/transformers/modeling_layoutlm.py
src/transformers/modeling_longformer.py
src/transformers/modeling_lxmert.py
src/transformers/modeling_mmbt.py
src/transformers/modeling_mobilebert.py
src/transformers/modeling_openai.py
src/transformers/modeling_outputs.py
src/transformers/modeling_reformer.py
src/transformers/modeling_retribert.py
src/transformers/modeling_roberta.py
src/transformers/modeling_t5.py
src/transformers/modeling_tf_albert.py
src/transformers/modeling_tf_bert.py
src/transformers/modeling_tf_camembert.py
src/transformers/modeling_tf_ctrl.py
src/transformers/modeling_tf_distilbert.py
src/transformers/modeling_tf_electra.py
src/transformers/modeling_tf_flaubert.py
src/transformers/modeling_tf_funnel.py
src/transformers/modeling_tf_gpt2.py
src/transformers/modeling_tf_longformer.py
src/transformers/modeling_tf_lxmert.py
src/transformers/modeling_tf_mobilebert.py
src/transformers/modeling_tf_openai.py
src/transformers/modeling_tf_outputs.py
src/transformers/modeling_tf_roberta.py
src/transformers/modeling_tf_t5.py
src/transformers/modeling_tf_transfo_xl.py
src/transformers/modeling_tf_xlm.py
src/transformers/modeling_tf_xlm_roberta.py
src/transformers/modeling_tf_xlnet.py
src/transformers/modeling_transfo_xl.py
src/transformers/modeling_utils.py
src/transformers/modeling_xlm.py
src/transformers/modeling_xlm_roberta.py
src/transformers/modeling_xlnet.py
src/transformers/tokenization_albert.py
src/transformers/tokenization_bert.py
src/transformers/tokenization_bert_generation.py
src/transformers/tokenization_bertweet.py
src/transformers/tokenization_camembert.py
src/transformers/tokenization_ctrl.py
src/transformers/tokenization_distilbert.py
src/transformers/tokenization_dpr.py
src/transformers/tokenization_electra.py
src/transformers/tokenization_flaubert.py
src/transformers/tokenization_fsmt.py
src/transformers/tokenization_funnel.py
src/transformers/tokenization_gpt2.py
src/transformers/tokenization_longformer.py
src/transformers/tokenization_lxmert.py
src/transformers/tokenization_marian.py
src/transformers/tokenization_mbart.py
src/transformers/tokenization_mobilebert.py
src/transformers/tokenization_openai.py
src/transformers/tokenization_phobert.py
src/transformers/tokenization_reformer.py
src/transformers/tokenization_retribert.py
src/transformers/tokenization_roberta.py
src/transformers/tokenization_t5.py
src/transformers/tokenization_transfo_xl.py
src/transformers/tokenization_utils_base.py
src/transformers/tokenization_xlm.py
src/transformers/tokenization_xlm_roberta.py
src/transformers/tokenization_xlnet.py
==================
58405a527;Wissam Antoun;2020-09-23 20:17:00 +0300;Fixed evaluation_strategy on epoch end bug (#7340)
* Fixed evaluation_strategy on epoch end bug

move the evaluation script outside the the iteration loop

* black formatting
==

src/transformers/trainer.py
==================
28cf87303;Stas Bekman;2020-09-23 02:16:19 -0700;[testing] skip decorators: docs, tests, bugs (#7334)
* skip decorators: docs, tests, bugs

* another important note

* style

* bloody style

* add @pytest.mark.parametrize

* add note

* no idea what it wants :(
==

docs/source/testing.rst
src/transformers/testing_utils.py
tests/test_skip_decorators.py
==================
df5364380;Stas Bekman;2020-09-22 19:12:36 -0700;[code quality] fix confused flake8 (#7309)
* fix confused flake

We run `black  --target-version py35 ...` but flake8 doesn't know that, so currently with py38 flake8 fails suggesting that black should have reformatted 63 files. Indeed if I run:

```
black --line-length 119 --target-version py38 examples templates tests src utils
```
it indeed reformats 63 files.

The only solution I found is to create a black config file as explained at https://github.com/psf/black#configuration-format, which is what this PR adds.

Now flake8 knows that py35 is the standard and no longer gets confused regardless of the user's python version.

* adjust the other files that will now rely on black's config file
==

.circleci/config.yml
Makefile
pyproject.toml
==================
78387cc63;Sam Shleifer;2020-09-22 18:27:28 -0400;[s2s] only save metrics.json from rank zero (#7331)

==

examples/seq2seq/callbacks.py
examples/seq2seq/finetune.py
==================
e53138a1b;Sam Shleifer;2020-09-22 18:26:37 -0400;[s2s] add src_lang kwarg for distributed eval (#7300)

==

examples/seq2seq/run_distributed_eval.py
src/transformers/tokenization_mbart.py
==================
a9c7849cf;blinovpd;2020-09-23 01:26:13 +0300;[model_cards] blinoff/roberta-base-russian-v0 (#7317)

==

model_cards/blinoff/roberta-base-russian-v0/README.md
==================
f5518e563;Sylvain Gugger;2020-09-22 14:55:12 -0400;Formatting

==

src/transformers/training_args.py
==================
17099ebd5;Chady Kamar;2020-09-22 14:44:42 -0400;Add num workers cli arg (#7322)
* Add dataloader_num_workers to TrainingArguments

This argument is meant to be used to set the
number of workers for the PyTorch DataLoader.

* Pass num_workers argument on DataLoader init
==

src/transformers/trainer.py
src/transformers/training_args.py
==================
25b0463d0;Sam Shleifer;2020-09-22 13:09:35 -0400;[s2s] add supported architecures to MD (#7252)

==

examples/seq2seq/README.md
==================
d6bc72c46;Pavel Soriano;2020-09-22 18:39:07 +0200;Fixed results of SQuAD-FR evaluation (#7313)
The score for the F1 metric was reported as the Exact Match and vice-versa.
==

model_cards/etalab-ia/camembert-base-squadFR-fquad-piaf/README.md
==================
6303b5a71;Huang Lianzhe;2020-09-23 00:31:21 +0800;[Bug Fix] The actual batch_size is inconsistent with the settings. (#7235)
* [bug fix] fixed the bug that the actual batch_size is inconsistent with the parameter settings

* reformat

* reformat

* reformat

* add support for dict and BatchEncoding

* add support for dict and BatchEncoding

* add documentation for DataCollatorForNextSentencePrediction

* Some more nits for the docstring

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Some more nits for the docstring

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Some more nits for the docstring

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Some more nits for the docstring

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Some more nits for the docstring

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* rename variables

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

src/transformers/data/data_collator.py
src/transformers/data/datasets/language_modeling.py
==================
c754c41c6;Ola Piktus;2020-09-22 17:29:58 +0100;RAG (#6813)
* added rag WIP

* path fix

* Formatting / renaming prior to actual work

* added rag WIP

* path fix

* Formatting / renaming prior to actual work

* added rag WIP

* path fix

* Formatting / renaming prior to actual work

* added rag WIP

* Formatting / renaming prior to actual work

* First commit

* improve comments

* Retrieval evaluation scripts

* refactor to include modeling outputs + MPI retriever

* Fix rag-token model + refactor

* Various fixes + finetuning logic

* use_bos fix

* Retrieval refactor

* Finetuning refactoring and cleanup

* Add documentation and cleanup

* Remove set_up_rag_env.sh file

* Fix retrieval wit HF index

* Fix import errors

* Fix quality errors

* Refactor as per suggestions in https://github.com/huggingface/transformers/pull/6813#issuecomment-687208867

* fix quality

* Fix RAG Sequence generation

* minor cleanup plus initial tests

* fix test

* fix tests 2

* Comments fix

* post-merge fixes

* Improve readme + post-rebase refactor

* Extra dependencied for tests

* Fix tests

* Fix tests 2

* Refactor test requirements

* Fix tests 3

* Post-rebase refactor

* rename nlp->datasets

* RAG integration tests

* add tokenizer to slow integration test and allow retriever to run on cpu

* add tests; fix position ids warning

* change structure

* change structure

* add from encoder generator

* save working solution

* make all integration tests pass

* add RagTokenizer.save/from_pretrained and RagRetriever.save/from_pretrained

* don't save paths

* delete unnecessary imports

* pass config to AutoTokenizer.from_pretrained for Rag tokenizers

* init wiki_dpr only once

* hardcode legacy index and passages paths (todo: add the right urls)

* finalize config

* finalize retriver api and config api

* LegacyIndex index download refactor

* add dpr to autotokenizer

* make from pretrained more flexible

* fix ragfortokengeneration

* small name changes in tokenizer

* add labels to models

* change default index name

* add retrieval tests

* finish token generate

* align test with previous version and make all tests pass

* add tests

* finalize tests

* implement thoms suggestions

* add first version of test

* make first tests work

* make retriever platform agnostic

* naming

* style

* add legacy index URL

* docstrings + simple retrieval test for distributed

* clean model api

* add doc_ids to retriever's outputs

* fix retrieval tests

* finish model outputs

* finalize model api

* fix generate problem for rag

* fix generate for other modles

* fix some tests

* save intermediate

* set generate to default

* big refactor generate

* delete rag_api

* correct pip faiss install

* fix auto tokenization test

* fix faiss install

* fix test

* move the distributed logic to examples

* model page

* docs

* finish tests

* fix dependencies

* fix import in __init__

* Refactor eval_rag and finetune scripts

* start docstring

* add psutil to test

* fix tf test

* move require torch to top

* fix retrieval test

* align naming

* finish automodel

* fix repo consistency

* test ragtokenizer save/load

* add rag model output docs

* fix ragtokenizer save/load from pretrained

* fix tokenizer dir

* remove torch in retrieval

* fix docs

* fixe finetune scripts

* finish model docs

* finish docs

* remove auto model for now

* add require torch

* remove solved todos

* integrate sylvains suggestions

* sams comments

* correct mistake on purpose

* improve README

* Add generation test cases

* fix rag token

* clean token generate

* fix test

* add note to test

* fix attention mask

* add t5 test for rag

* Fix handling prefix in finetune.py

* don't overwrite index_name

Co-authored-by: Patrick Lewis <plewis@fb.com>
Co-authored-by: Aleksandra Piktus <piktus@devfair0141.h2.fair>
Co-authored-by: Aleksandra Piktus <piktus@learnfair5102.h2.fair>
Co-authored-by: Aleksandra Piktus <piktus@learnfair5067.h2.fair>
Co-authored-by: Your Name <you@example.com>
Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
Co-authored-by: Quentin Lhoest <lhoest.q@gmail.com>
==

.gitignore
docs/source/index.rst
docs/source/model_doc/rag.rst
docs/source/model_summary.rst
examples/lightning_base.py
examples/longform-qa/eli5_app.py
examples/longform-qa/eli5_utils.py
examples/rag/README.md
examples/rag/__init__.py
examples/rag/callbacks.py
examples/rag/distributed_retriever.py
examples/rag/eval_rag.py
examples/rag/finetune.py
examples/rag/finetune.sh
examples/rag/parse_dpr_relevance_data.py
examples/rag/requirements.txt
examples/rag/test_distributed_retriever.py
examples/rag/utils.py
examples/requirements.txt
setup.cfg
setup.py
src/transformers/__init__.py
src/transformers/configuration_auto.py
src/transformers/configuration_dpr.py
src/transformers/configuration_rag.py
src/transformers/file_utils.py
src/transformers/modeling_auto.py
src/transformers/modeling_dpr.py
src/transformers/modeling_rag.py
src/transformers/retrieval_rag.py
src/transformers/testing_utils.py
src/transformers/tokenization_auto.py
src/transformers/tokenization_rag.py
tests/test_modeling_rag.py
tests/test_modeling_t5.py
tests/test_retrieval_rag.py
tests/test_tokenization_rag.py
==================
1ee2194fb;Sylvain Gugger;2020-09-22 12:21:52 -0400;Mark big downloads slow (#7325)
* Make big downloads as slow

* Add import

* Right order for slow decorator

* More slow tests
==

tests/test_data_collator.py
tests/test_pipelines.py
tests/test_trainer.py
==================
585217c87;Julien Plu;2020-09-22 18:05:05 +0200;Add generic text classification example in TF (#5716)
* Add new example with nlp

* Update README

* replace nlp by datasets

* Update examples/text-classification/README.md

Add Lysandre's suggestion.

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
==

examples/text-classification/README.md
examples/text-classification/run_tf_text_classification.py
==================
6e21f2422;Lysandre;2020-09-22 18:04:39 +0200;Documentation version

==

.circleci/deploy.sh
docs/source/_static/js/custom.js
==================
3ebb1b3a2;Lysandre;2020-09-22 17:36:51 +0200;Release: v3.2.0

==

docs/source/conf.py
setup.py
src/transformers/__init__.py
==================
01f0fd0ba;Sylvain Gugger;2020-09-22 10:37:11 -0400;Fixes for LayoutLM (#7318)

==

src/transformers/configuration_layoutlm.py
src/transformers/modeling_layoutlm.py
src/transformers/modeling_roberta.py
==================
702a76ff9;Julien Plu;2020-09-22 16:19:34 +0200;Create an XLA parameter and fix the mixed precision (#7311)
* Create an XLA parameter and fix mixed precision creation

* Fix issue brought by intellisense

* Complete docstring
==

src/transformers/trainer_tf.py
src/transformers/training_args_tf.py
==================
596342c2b;Sylvain Gugger;2020-09-22 10:17:48 -0400;Support for Windows in check_copies (#7316)

==

utils/check_copies.py
==================
89edf504b;Sylvain Gugger;2020-09-22 09:52:29 -0400;Add possibility to evaluate every epoch (#7302)
* Add possibility to evaluate every epoch

* Remove multitype arg

* Remove needless import

* Use a proper enum

* Apply suggestions from @LysandreJik

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* One else and formatting

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
==

src/transformers/trainer.py
src/transformers/trainer_utils.py
src/transformers/training_args.py
==================
21ca14809;Sylvain Gugger;2020-09-22 09:34:35 -0400;is_pretokenized -> is_split_into_words (#7236)
* is_pretokenized -> is_split_into_words

* Fix tests
==

docs/source/custom_datasets.rst
docs/source/preprocessing.rst
src/transformers/tokenization_gpt2.py
src/transformers/tokenization_roberta.py
src/transformers/tokenization_utils.py
src/transformers/tokenization_utils_base.py
src/transformers/tokenization_utils_fast.py
tests/test_tokenization_common.py
tests/test_tokenization_fast.py
==================
324f361e9;Julien Plu;2020-09-22 15:31:13 +0200;Fix saving TF custom models (#7291)
* Fix #7277

* Apply style

* Add a full training pipeline test

* Apply style
==

src/transformers/modeling_tf_utils.py
tests/test_modeling_tf_common.py
tests/test_modeling_tf_funnel.py
==================
cd9a0585e;Minghao Li;2020-09-22 21:28:02 +0800;Add LayoutLM Model (#7064)
* first version

* finish test docs readme model/config/tokenization class

* apply make style and make quality

* fix layoutlm GitHub link

* fix conflict in index.rst and add layoutlm to pretrained_models.rst

* fix bug in test_parents_and_children_in_mappings

* reformat modeling_auto.py and tokenization_auto.py

* fix bug in test_modeling_layoutlm.py

* Update docs/source/model_doc/layoutlm.rst

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update docs/source/model_doc/layoutlm.rst

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* remove inh, add tokenizer fast, and update some doc

* copy and rename necessary class from modeling_bert to modeling_layoutlm

* Update src/transformers/configuration_layoutlm.py

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* Update src/transformers/configuration_layoutlm.py

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* Update src/transformers/configuration_layoutlm.py

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* Update src/transformers/configuration_layoutlm.py

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* Update src/transformers/modeling_layoutlm.py

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* Update src/transformers/modeling_layoutlm.py

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* Update src/transformers/modeling_layoutlm.py

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* add mish to activations.py, import ACT2FN and import logging from utils

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
==

README.md
docs/source/index.rst
docs/source/model_doc/layoutlm.rst
docs/source/pretrained_models.rst
src/transformers/__init__.py
src/transformers/activations.py
src/transformers/configuration_auto.py
src/transformers/configuration_layoutlm.py
src/transformers/modeling_auto.py
src/transformers/modeling_layoutlm.py
src/transformers/tokenization_auto.py
src/transformers/tokenization_layoutlm.py
tests/test_modeling_layoutlm.py
tests/test_tokenization_layoutlm.py
==================
244e1b5ba;Sylvain Gugger;2020-09-22 09:20:03 -0400;Fix #7304 (#7305)

==

src/transformers/trainer.py
==================
e46108817;Lysandre Debut;2020-09-22 12:35:51 +0200;Adds FSMT to LM head AutoModel (#7312)

==

src/transformers/modeling_auto.py
==================
e2964b8a1;Stas Bekman;2020-09-22 02:39:06 -0700;[fsmt] no need to pass device (#7292)

==

src/transformers/modeling_fsmt.py
==================
e4b94d8e5;Sylvain Gugger;2020-09-22 05:02:27 -0400;Copy code from Bert to Roberta and add safeguard script (#7219)
* Copy code from Bert to Roberta and add safeguard script

* Fix docstring

* Comment code

* Formatting

* Update src/transformers/modeling_roberta.py

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* Add test and fix bugs

* Fix style and make new comand

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
==

.circleci/config.yml
Makefile
src/transformers/modeling_roberta.py
tests/test_utils_check_copies.py
utils/check_copies.py
utils/check_repo.py
==================
656c27c3a;Sam Shleifer;2020-09-21 17:26:24 -0400;[s2s] save hostname with repo info (#7301)
* save hostname
==

examples/seq2seq/utils.py
==================
34a1b75f0;Thomas Winters;2020-09-21 22:17:28 +0200;Added RobBERT-v2 model card (#7286)
* Added RobBERT-v2 model card

* minor Tweaks

Co-authored-by: Julien Chaumond <chaumond@gmail.com>
==

model_cards/pdelobelle/robbert-v2-dutch-base/README.md
==================
6513d16a4;jjacampos;2020-09-21 22:15:31 +0200;IXAmBERT model card (#7283)
This PR includes the model card for the IXAmBERT model which has been recently uploaded to the huggingface repository.
==

model_cards/ixa-ehu/ixambert-base-cased/README.md
==================
af4b98ed9;Stas Bekman;2020-09-21 12:13:19 -0700;[s2s] adjust finetune + test to work with fsmt (#7263)

==

examples/seq2seq/finetune.py
examples/seq2seq/test_seq2seq_examples.py
==================
8d562a2d1;Stas Bekman;2020-09-21 11:14:26 -0700;[s2s] s/alpha_loss_encoder/alpha_encoder_loss/ (#7298)
fix to match `distillation.py:        self.alpha_encoder_loss`
==

examples/seq2seq/test_seq2seq_examples.py
==================
cbb2f75a1;Stas Bekman;2020-09-21 11:00:40 -0700;[s2s tests] fix test_run_eval_search (#7297)

==

examples/seq2seq/test_seq2seq_examples.py
==================
7a88ed6c2;Suraj Patil;2020-09-21 21:56:18 +0530;[model card] distlbart-mnli model cards (#7278)

==

model_cards/valhalla/distilbart-mnli-12-1/README.md
model_cards/valhalla/distilbart-mnli-12-3/README.md
model_cards/valhalla/distilbart-mnli-12-6/README.md
model_cards/valhalla/distilbart-mnli-12-9/README.md
==================
63276b76d;Sylvain Gugger;2020-09-21 10:31:26 -0400;Fix #7284 (#7289)

==

src/transformers/data/data_collator.py
==================
8d464374b;Rapha√´l Bournhonesque;2020-09-21 15:14:48 +0200;Disable missing weight warning (#7282)

==

src/transformers/modeling_roberta.py
==================
8ff88d25e;Stas Bekman;2020-09-21 06:13:35 -0700;[fsmt] rewrite SinusoidalPositionalEmbedding + USE_CUDA test fixes + new TranslationPipeline test (#7224)
* fix USE_CUDA, add pipeline

* USE_CUDA fix

* recode SinusoidalPositionalEmbedding into nn.Embedding subclass

was needed for torchscript to work - this is now part of the state_dict, so will have to remove these keys during save_pretrained

* back out (ci debug)

* restore

* slow last?

* facilitate not saving certain keys and test

* remove no longer used keys

* style

* fix logging import

* cleanup

* Update src/transformers/modeling_utils.py

Co-authored-by: Sam Shleifer <sshleifer@gmail.com>

* fix bug in max_positional_embeddings

* rename keys to keys_to_never_save per suggestion, improve the setup

* Update src/transformers/modeling_utils.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

Co-authored-by: Sam Shleifer <sshleifer@gmail.com>
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

src/transformers/convert_fsmt_original_pytorch_checkpoint_to_pytorch.py
src/transformers/modeling_fsmt.py
src/transformers/modeling_utils.py
tests/test_modeling_fsmt.py
==================
67c4b0c51;Dat Quoc Nguyen;2020-09-21 17:12:51 +0700;Add model cards for new pre-trained BERTweet-COVID19 models (#7269)
Two new pre-trained models "vinai/bertweet-covid19-base-cased" and "vinai/bertweet-covid19-base-uncased" are resulted by further pre-training the pre-trained model "vinai/bertweet-base" on a  corpus of 23M COVID-19 English Tweets for 40 epochs.
==

model_cards/vinai/bertweet-base/README.md
model_cards/vinai/bertweet-covid19-base-cased/README.md
model_cards/vinai/bertweet-covid19-base-uncased/README.md
model_cards/vinai/phobert-base/README.md
model_cards/vinai/phobert-large/README.md
==================
0cbe1139b;Patrick von Platen;2020-09-21 11:53:08 +0200;Update README.md

==

model_cards/facebook/rag-token-base/README.md
==================
aae4edb5f;Lysandre;2020-09-21 11:37:00 +0200;Addressing review comment

==

examples/text-classification/run_glue.py
==================
43b9d9387;Suraj Patil;2020-09-21 15:04:20 +0530;[example/glue] fix compute_metrics_fn for bart like models (#7248)
* fix compute_metrics_fn

* p.predictions -> preds

* apply suggestions
==

examples/text-classification/run_glue.py
==================
39062d05f;guillaume-be;2020-09-21 10:53:52 +0200;Fixed target_mapping preparation for XLNet when batch size > 1 (incl. beam search) (#7267)

==

src/transformers/modeling_xlnet.py
==================
4b3e55bdc;Nadir El Manouzi;2020-09-21 10:25:22 +0200;Add "Fine-tune ALBERT for sentence-pair classification" notebook to the community notebooks (#7255)

==

notebooks/README.md
==================
7cbf0f722;Stas Bekman;2020-09-20 13:54:42 -0700;examples/seq2seq/__init__.py mutates sys.path (#7194)

==

examples/seq2seq/__init__.py
examples/seq2seq/distillation.py
examples/seq2seq/finetune.py
examples/seq2seq/run_distributed_eval.py
examples/seq2seq/run_eval.py
examples/seq2seq/run_eval_search.py
examples/seq2seq/test_bash_script.py
examples/seq2seq/test_seq2seq_examples.py
==================
a4faeceae;Manuel Romero;2020-09-20 19:12:30 +0200;Fix typo in model name (#7268)

==

README.md
==================
47ab3e826;Stas Bekman;2020-09-20 06:17:29 -0700;@slow has to be last (#7251)
Found an issue when `@slow` isn't the last decorator (gets ignored!), so documenting this significance.
==

docs/source/testing.rst
==================
4f6e52574;Stas Bekman;2020-09-19 14:02:05 -0700;model card improvements (#7221)

==

model_cards/allenai/wmt16-en-de-12-1/README.md
model_cards/allenai/wmt16-en-de-dist-12-1/README.md
model_cards/allenai/wmt16-en-de-dist-6-1/README.md
model_cards/allenai/wmt19-de-en-6-6-base/README.md
model_cards/allenai/wmt19-de-en-6-6-big/README.md
model_cards/facebook/wmt19-de-en/README.md
model_cards/facebook/wmt19-en-de/README.md
model_cards/facebook/wmt19-en-ru/README.md
model_cards/facebook/wmt19-ru-en/README.md
scripts/fsmt/gen-card-allenai-wmt16.py
scripts/fsmt/gen-card-allenai-wmt19.py
scripts/fsmt/gen-card-facebook-wmt19.py
==================
eb074af75;Stas Bekman;2020-09-19 11:37:12 -0700;fsmt tiny model card + script (#7244)

==

model_cards/stas/tiny-wmt19-en-de/README.md
scripts/fsmt/fsmt-make-tiny-model.py
==================
1d90d0f38;Manuel Romero;2020-09-19 08:10:45 +0200;Add title to model card (#7240)

==

model_cards/mrm8488/electricidad-base-finetuned-pawsx-es/README.md
==================
c9b7ef042;Manuel Romero;2020-09-19 08:09:29 +0200;Create README.md (#7239)

==

model_cards/mrm8488/bert-base-german-dbmdz-cased-finetuned-pawsx-de/README.md
==================
83dba10b8;Sam Shleifer;2020-09-18 15:46:01 -0400;[s2s] distributed_eval.py saves better speed info (#7242)

==

examples/seq2seq/run_distributed_eval.py
==================
af2322c7a;Dat Quoc Nguyen;2020-09-19 00:16:43 +0700;Add new pre-trained models BERTweet and PhoBERT (#6129)
* Add BERTweet and PhoBERT models

* Update modeling_auto.py

Re-add `bart` to LM_MAPPING

* Update tokenization_auto.py

Re-add `from .configuration_mobilebert import MobileBertConfig`
not sure why it's replaced by `from transformers.configuration_mobilebert import MobileBertConfig`

* Add BERTweet and PhoBERT to pretrained_models.rst

* Update tokenization_auto.py

Remove BertweetTokenizer and PhobertTokenizer out of tokenization_auto.py (they are currently not supported by AutoTokenizer.

* Update BertweetTokenizer - without nltk

* Update model card for BERTweet

* PhoBERT - with Auto mode - without import fastBPE

* PhoBERT - with Auto mode - without import fastBPE

* BERTweet - with Auto mode - without import fastBPE

* Add PhoBERT and BERTweet to TF modeling auto

* Improve Docstrings for PhobertTokenizer and BertweetTokenizer

* Update PhoBERT and BERTweet model cards

* Fixed a merge conflict in tokenization_auto

* Used black to reformat BERTweet- and PhoBERT-related files

* Used isort to reformat BERTweet- and PhoBERT-related files

* Reformatted BERTweet- and PhoBERT-related files based on flake8

* Updated test files

* Updated test files

* Updated tf test files

* Updated tf test files

* Updated tf test files

* Updated tf test files

* Update commits from huggingface

* Delete unnecessary files

* Add tokenizers to auto and init files

* Add test files for tokenizers

* Revised model cards

* Update save_vocabulary function in BertweetTokenizer and PhobertTokenizer and test files

* Revised test files

* Update orders of Phobert and Bertweet tokenizers in auto tokenization file
==

model_cards/vinai/bertweet-base/README.md
model_cards/vinai/phobert-base/README.md
model_cards/vinai/phobert-large/README.md
src/transformers/__init__.py
src/transformers/tokenization_auto.py
src/transformers/tokenization_bertweet.py
src/transformers/tokenization_phobert.py
tests/test_tokenization_bertweet.py
tests/test_tokenization_phobert.py
==================
9397436ea;Patrick von Platen;2020-09-18 16:52:00 +0200;Create README.md

==

model_cards/facebook/rag-sequence-base/README.md
==================
7eeca4d39;Patrick von Platen;2020-09-18 16:44:02 +0200;Create README.md

==

model_cards/facebook/rag-token-base/README.md
==================
31516c776;Patrick von Platen;2020-09-18 16:37:14 +0200;Update README.md

==

model_cards/facebook/rag-sequence-nq/README.md
==================
4c14669a7;Patrick von Platen;2020-09-18 16:35:11 +0200;Update README.md

==

model_cards/facebook/rag-token-nq/README.md
==================
3a03bab9d;Yih-Dar;2020-09-18 15:28:56 +0200;Fix a few countings (steps / epochs) in trainer_tf.py (#7175)

==

src/transformers/trainer_tf.py
==================
ee9eae4e0;Stefan Schweter;2020-09-18 12:18:06 +0200;token-classification: update url of GermEval 2014 dataset (#6571)

==

examples/token-classification/README.md
==================
eef8d94d1;Julien Chaumond;2020-09-18 12:09:24 +0200;[model_cards]
We use ISO 639-1 cc @gentaiscool

==

model_cards/indobenchmark/indobert-base-p1/README.md
model_cards/indobenchmark/indobert-base-p2/README.md
model_cards/indobenchmark/indobert-large-p1/README.md
model_cards/indobenchmark/indobert-large-p2/README.md
model_cards/indobenchmark/indobert-lite-base-p1/README.md
model_cards/indobenchmark/indobert-lite-base-p2/README.md
model_cards/indobenchmark/indobert-lite-large-p1/README.md
model_cards/indobenchmark/indobert-lite-large-p2/README.md
==================
afd6a9f82;Patrick von Platen;2020-09-18 11:41:12 +0200;Create README.md

==

model_cards/facebook/rag-sequence-nq/README.md
==================
9f1544b9e;Patrick von Platen;2020-09-18 11:37:20 +0200;Create README.md

==

model_cards/facebook/rag-token-nq/README.md
==================
5c1d5ea66;Sameer Zahid;2020-09-18 12:52:43 +0400;Fixed typo in README (#7233)

==

README.md
==================
7719ecd19;Yuta Hayashibe;2020-09-18 17:23:33 +0900;Fix a typo (#7225)

==

src/transformers/pipelines.py
==================
4a26e8ac5;Manuel Romero;2020-09-18 09:24:30 +0200;Create README.md (#7205)

==

model_cards/mrm8488/GuaPeTe-2-tiny/README.md
==================
94320c5b8;Manuel Romero;2020-09-18 09:24:23 +0200;Add customized text to widget (#7204)

==

model_cards/mrm8488/electricidad-base-generator/README.md
==================
3aefb24b2;Manuel Romero;2020-09-18 09:24:10 +0200;Create README.md (#7209)

==

model_cards/mrm8488/electricidad-base-finetuned-pawsx-es/README.md
==================
a22e7a8dd;Manuel Romero;2020-09-18 09:23:58 +0200;Create README.md (#7210)

==

model_cards/mrm8488/RuPERTa-base-finetuned-pawsx-es/README.md
==================
c028b2648;Manuel Romero;2020-09-18 09:23:49 +0200;Create README.md (#7212)

==

model_cards/mrm8488/camembert-base-finetuned-pawsx-fr/README.md
==================
c7cdd7b4f;Genta Indra Winata;2020-09-18 15:22:32 +0800;Create README.md for indobert-lite-base-p1 (#7182)

==

model_cards/indobenchmark/indobert-lite-base-p1/README.md
==================
bfb9150b8;Genta Indra Winata;2020-09-18 15:22:11 +0800;Create README.md for indobert-lite-large-p1 (#7184)
* Create README.md

* Update README.md
==

model_cards/indobenchmark/indobert-lite-large-p1/README.md
==================
d19359340;Genta Indra Winata;2020-09-18 15:21:54 +0800;Create README.md (#7183)

==

model_cards/indobenchmark/indobert-lite-base-p2/README.md
==================
e65d84667;Genta Indra Winata;2020-09-18 15:21:39 +0800;Create README.md (#7185)

==

model_cards/indobenchmark/indobert-lite-large-p2/README.md
==================
e27d86d48;Genta Indra Winata;2020-09-18 15:21:28 +0800;Create README.md for indobert-large-p2 model card (#7181)

==

model_cards/indobenchmark/indobert-large-p2/README.md
==================
881c0783e;Genta Indra Winata;2020-09-18 15:21:16 +0800;Create README.md for indobert-large-p1 model card (#7180)

==

model_cards/indobenchmark/indobert-large-p1/README.md
==================
e0d58a5c8;Genta Indra Winata;2020-09-18 15:20:59 +0800;Create README.md (#7179)

==

model_cards/indobenchmark/indobert-base-p1/README.md
==================
1313a1d2a;Genta Indra Winata;2020-09-18 15:20:29 +0800;Create README.md for indobert-base-p2 (#7178)

==

model_cards/indobenchmark/indobert-base-p2/README.md
==================
cf24f43e7;tuner007;2020-09-18 12:49:45 +0530;Create README.md (#7095)
Create model card for Pegasus QA
==

model_cards/tuner007/pegasus_qa/README.md
==================
67d9fc50d;Sam Shleifer;2020-09-17 18:32:31 -0400;[s2s] remove double assert (#7223)

==

examples/seq2seq/finetune.py
==================
edbaad2c5;Stas Bekman;2020-09-17 13:57:06 -0700;[model cards] fix metadata - 3rd attempt (#7218)

==

model_cards/allenai/wmt16-en-de-12-1/README.md
model_cards/allenai/wmt16-en-de-dist-12-1/README.md
model_cards/allenai/wmt16-en-de-dist-6-1/README.md
model_cards/allenai/wmt19-de-en-6-6-base/README.md
model_cards/allenai/wmt19-de-en-6-6-big/README.md
model_cards/facebook/wmt19-de-en/README.md
model_cards/facebook/wmt19-en-de/README.md
model_cards/facebook/wmt19-en-ru/README.md
model_cards/facebook/wmt19-ru-en/README.md
scripts/fsmt/gen-card-allenai-wmt16.py
scripts/fsmt/gen-card-allenai-wmt19.py
scripts/fsmt/gen-card-facebook-wmt19.py
==================
999a1c957;Stas Bekman;2020-09-17 13:53:14 -0700;skip failing FSMT CUDA tests until investigated (#7220)

==

tests/test_modeling_fsmt.py
==================
51c4adf54;Stas Bekman;2020-09-17 12:29:39 -0700;[model cards] fix dataset yaml (#7216)

==

model_cards/allenai/wmt16-en-de-12-1/README.md
model_cards/allenai/wmt16-en-de-dist-12-1/README.md
model_cards/allenai/wmt16-en-de-dist-6-1/README.md
model_cards/allenai/wmt19-de-en-6-6-base/README.md
model_cards/allenai/wmt19-de-en-6-6-big/README.md
model_cards/facebook/wmt19-de-en/README.md
model_cards/facebook/wmt19-en-de/README.md
model_cards/facebook/wmt19-en-ru/README.md
model_cards/facebook/wmt19-ru-en/README.md
scripts/fsmt/gen-card-allenai-wmt16.py
scripts/fsmt/gen-card-allenai-wmt19.py
scripts/fsmt/gen-card-facebook-wmt19.py
==================
a5638b2b3;Sam Shleifer;2020-09-17 15:19:34 -0400;[s2s] dynamic batch size with --max_tokens_per_batch (#7030)

==

examples/seq2seq/README.md
examples/seq2seq/dynamic_bs_example.sh
examples/seq2seq/finetune.py
examples/seq2seq/save_len_file.py
examples/seq2seq/test_data/wmt_en_ro/train.len
examples/seq2seq/test_data/wmt_en_ro/train.source
examples/seq2seq/test_data/wmt_en_ro/train.target
examples/seq2seq/test_data/wmt_en_ro/val.len
examples/seq2seq/test_datasets.py
examples/seq2seq/test_seq2seq_examples.py
examples/seq2seq/utils.py
==================
efeab6a3f;Stas Bekman;2020-09-17 11:26:38 -0700;[s2s] run_eval/run_eval_search tweaks (#7192)
Co-authored-by: Sam Shleifer <sshleifer@gmail.com>
==

examples/seq2seq/run_eval_search.py
examples/seq2seq/test_seq2seq_examples.py
==================
9c5bcab5b;Stas Bekman;2020-09-17 11:11:17 -0700;[model cards] fix yaml in cards (#7207)

==

model_cards/allenai/wmt16-en-de-12-1/README.md
model_cards/allenai/wmt16-en-de-dist-12-1/README.md
model_cards/allenai/wmt16-en-de-dist-6-1/README.md
model_cards/allenai/wmt19-de-en-6-6-base/README.md
model_cards/allenai/wmt19-de-en-6-6-big/README.md
model_cards/facebook/wmt19-de-en/README.md
model_cards/facebook/wmt19-en-de/README.md
model_cards/facebook/wmt19-en-ru/README.md
model_cards/facebook/wmt19-ru-en/README.md
scripts/fsmt/gen-card-allenai-wmt16.py
scripts/fsmt/gen-card-allenai-wmt19.py
scripts/fsmt/gen-card-facebook-wmt19.py
src/transformers/modeling_fsmt.py
==================
e643a2972;Sohee Yang;2020-09-18 01:30:45 +0900;Change to use relative imports in some files & Add python prompt symbols to example codes (#7202)
* Move 'from transformers' statements to relative imports in some files

* Add python prompt symbols in front of the example codes

* Reformat the code

* Add one missing space

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

src/transformers/integrations.py
src/transformers/modeling_auto.py
src/transformers/modeling_bart.py
src/transformers/modeling_dpr.py
src/transformers/modeling_openai.py
src/transformers/modeling_tf_albert.py
src/transformers/modeling_tf_auto.py
src/transformers/modeling_tf_bert.py
src/transformers/modeling_tf_electra.py
src/transformers/modeling_tf_lxmert.py
src/transformers/modeling_tf_utils.py
src/transformers/modeling_tf_xlnet.py
src/transformers/modeling_utils.py
src/transformers/modeling_xlnet.py
src/transformers/pipelines.py
src/transformers/testing_utils.py
src/transformers/tokenization_auto.py
src/transformers/tokenization_dpr.py
src/transformers/tokenization_pegasus.py
src/transformers/trainer.py
==================
0fe6e435b;Stas Bekman;2020-09-17 08:58:49 -0700;[model cards] ported allenai Deep Encoder, Shallow Decoder models (#7153)
* [model cards] ported allenai Deep Encoder, Shallow Decoder models

* typo

* fix references

* add allenai/wmt19-de-en-6-6 model cards

* fill-in the missing info for the build script as provided by the searcher.
==

model_cards/allenai/wmt16-en-de-12-1/README.md
model_cards/allenai/wmt16-en-de-dist-12-1/README.md
model_cards/allenai/wmt16-en-de-dist-6-1/README.md
model_cards/allenai/wmt19-de-en-6-6-base/README.md
model_cards/allenai/wmt19-de-en-6-6-big/README.md
scripts/fsmt/gen-card-allenai-wmt19.py
==================
1eeb206be;Stas Bekman;2020-09-17 08:31:29 -0700;[ported model] FSMT (FairSeq MachineTranslation) (#6940)
* ready for PR

* cleanup

* correct FSMT_PRETRAINED_MODEL_ARCHIVE_LIST

* fix

* perfectionism

* revert change from another PR

* odd, already committed this one

* non-interactive upload workaround

* backup the failed experiment

* store langs in config

* workaround for localizing model path

* doc clean up as in https://github.com/huggingface/transformers/pull/6956

* style

* back out debug mode

* document: run_eval.py --num_beams 10

* remove unneeded constant

* typo

* re-use bart's Attention

* re-use EncoderLayer, DecoderLayer from bart

* refactor

* send to cuda and fp16

* cleanup

* revert (moved to another PR)

* better error message

* document run_eval --num_beams

* solve the problem of tokenizer finding the right files when model is local

* polish, remove hardcoded config

* add a note that the file is autogenerated to avoid losing changes

* prep for org change, remove unneeded code

* switch to model4.pt, update scores

* s/python/bash/

* missing init (but doesn't impact the finetuned model)

* cleanup

* major refactor (reuse-bart)

* new model, new expected weights

* cleanup

* cleanup

* full link

* fix model type

* merge porting notes

* style

* cleanup

* have to create a DecoderConfig object to handle vocab_size properly

* doc fix

* add note (not a public class)

* parametrize

* - add bleu scores integration tests

* skip test if sacrebleu is not installed

* cache heavy models/tokenizers

* some tweaks

* remove tokens that aren't used

* more purging

* simplify code

* switch to using decoder_start_token_id

* add doc

* Revert "major refactor (reuse-bart)"

This reverts commit 226dad15ca6a9ef4e26178526e878e8fc5c85874.

* decouple from bart

* remove unused code #1

* remove unused code #2

* remove unused code #3

* update instructions

* clean up

* move bleu eval to examples

* check import only once

* move data+gen script into files

* reuse via import

* take less space

* add prepare_seq2seq_batch (auto-tested)

* cleanup

* recode test to use json instead of yaml

* ignore keys not needed

* use the new -y in transformers-cli upload -y

* [xlm tok] config dict: fix str into int to match definition (#7034)

* [s2s] --eval_max_generate_length (#7018)

* Fix CI with change of name of nlp (#7054)

* nlp -> datasets

* More nlp -> datasets

* Woopsie

* More nlp -> datasets

* One last

* extending to support allen_nlp wmt models

- allow a specific checkpoint file to be passed
- more arg settings
- scripts for allen_nlp models

* sync with changes

* s/fsmt-wmt/wmt/ in model names

* s/fsmt-wmt/wmt/ in model names (p2)

* s/fsmt-wmt/wmt/ in model names (p3)

* switch to a better checkpoint

* typo

* make non-optional args such - adjust tests where possible or skip when there is no other choice

* consistency

* style

* adjust header

* cards moved (model rename)

* use best custom hparams

* update info

* remove old cards

* cleanup

* s/stas/facebook/

* update scores

* s/allen_nlp/allenai/

* url maps aren't needed

* typo

* move all the doc / build /eval generators to their own scripts

* cleanup

* Apply suggestions from code review

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* Apply suggestions from code review

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* fix indent

* duplicated line

* style

* use the correct add_start_docstrings

* oops

* resizing can't be done with the core approach, due to 2 dicts

* check that the arg is a list

* style

* style

Co-authored-by: Sam Shleifer <sshleifer@gmail.com>
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
==

docs/source/index.rst
docs/source/model_doc/fsmt.rst
examples/seq2seq/test_data/fsmt/build-eval-data.py
examples/seq2seq/test_data/fsmt/fsmt_val_data.json
examples/seq2seq/test_fsmt_bleu_score.py
model_cards/facebook/wmt19-de-en/README.md
model_cards/facebook/wmt19-en-de/README.md
model_cards/facebook/wmt19-en-ru/README.md
model_cards/facebook/wmt19-ru-en/README.md
src/transformers/__init__.py
src/transformers/configuration_auto.py
src/transformers/configuration_fsmt.py
src/transformers/convert_fsmt_original_pytorch_checkpoint_to_pytorch.py
src/transformers/modeling_auto.py
src/transformers/modeling_fsmt.py
src/transformers/tokenization_auto.py
src/transformers/tokenization_fsmt.py
tests/test_modeling_fsmt.py
tests/test_tokenization_fsmt.py
==================
492bb6aa4;Sylvain Gugger;2020-09-17 08:15:37 -0400;Trainer multi label (#7191)
* Trainer accep multiple labels

* Missing import

* Fix dosctrings
==

src/transformers/trainer.py
src/transformers/trainer_utils.py
src/transformers/training_args.py
tests/test_trainer.py
==================
709745927;RafaelWO;2020-09-17 12:10:34 +0200;Transformer-XL: Remove unused parameters (#7087)
* Removed 'tgt_len' and 'ext_len' from Transfomer-XL

 * Some changes are still to be done

* Removed 'tgt_len' and 'ext_len' from Transfomer-XL (2)

 * Removed comments
 * Fixed quality

* Changed warning to info
==

examples/contrib/run_transfo_xl.py
src/transformers/configuration_transfo_xl.py
src/transformers/modeling_tf_transfo_xl.py
src/transformers/modeling_transfo_xl.py
==================
c183d81e2;Dhaval Taunk;2020-09-17 15:28:57 +0530;added multilabel text classification notebook using distilbert to community notebooks (#7201)
* added multilabel classification using distilbert notebook to community notebooks

* added multilabel classification using distilbert notebook to community notebooks
==

notebooks/README.md
==================
79111b77d;Stas Bekman;2020-09-17 02:52:12 -0700;remove deprecated flag (#7171)
```
/home/circleci/.local/lib/python3.6/site-packages/isort/main.py:915: UserWarning: W0501: The following deprecated CLI flags were used and ignored: --recursive!
  "W0501: The following deprecated CLI flags were used and ignored: "
```
==

.circleci/config.yml
==================
0cdafbf7e;Stas Bekman;2020-09-17 02:51:40 -0700;remove duplicated code (#7173)

==

src/transformers/tokenization_marian.py
==================
45b0b1ff2;Sam Shleifer;2020-09-16 21:58:57 -0400;[s2s] fix kwarg typo (#7196)

==

examples/seq2seq/run_distributed_eval.py
==================
0203ad43b;Sam Shleifer;2020-09-16 15:38:37 -0400;[s2s] distributed eval cleanup (#7186)

==

examples/seq2seq/README.md
examples/seq2seq/run_distributed_eval.py
examples/seq2seq/run_eval.py
examples/seq2seq/utils.py
==================
3babef815;sgugger;2020-09-16 14:57:09 -0400;Formatting

==

examples/seq2seq/test_seq2seq_examples.py
==================
42049b8e1;Stas Bekman;2020-09-16 11:40:35 -0700;use the correct add_start_docstrings (#7174)

==

src/transformers/tokenization_marian.py
src/transformers/tokenization_mbart.py
src/transformers/tokenization_pegasus.py
==================
fdaf8ab34;Stas Bekman;2020-09-16 10:59:57 -0700;[s2s run_eval] new features (#7109)
Co-authored-by: Sam Shleifer <sshleifer@gmail.com>
==

examples/seq2seq/README.md
examples/seq2seq/run_eval.py
examples/seq2seq/run_eval_search.py
examples/seq2seq/test_seq2seq_examples.py
examples/seq2seq/utils.py
==================
df165065c;Antoine Louis;2020-09-16 16:16:01 +0000;[model_cards] antoiloui/belgpt2 üáßüá™ (#7166)
* Create README.md

* Update README.md
==

model_cards/antoiloui/belgpt2/README.md
==================
108c9aefc;Sylvain Gugger;2020-09-16 12:12:12 -0400;Update README (#7133)
* Rewrite and update README

* Typo and migration guide

* Apply suggestions from code review

Co-authored-by: Thomas Wolf <thomwolf@users.noreply.github.com>

* Address Clem's comments

Co-authored-by: Thomas Wolf <thomwolf@users.noreply.github.com>
==

README.md
==================
9e376e156;Donna Choi;2020-09-16 06:15:10 -0700;Add condition (#7161)

==

src/transformers/data/data_collator.py
==================
f8590c56e;Stas Bekman;2020-09-16 05:45:50 -0700;[doc] improve/expand the Parametrization section (#7156)

==

docs/source/testing.rst
==================
d3391c87f;Stas Bekman;2020-09-16 05:41:26 -0700;build/eval/gen-card scripts for fsmt (#7155)
* build/eval/gen-card scripts for fsmt

* adjust for model renames
==

scripts/fsmt/convert-allenai-wmt16.sh
scripts/fsmt/convert-allenai-wmt19.sh
scripts/fsmt/convert-facebook-wmt19.sh
scripts/fsmt/eval-allenai-wmt16.sh
scripts/fsmt/eval-allenai-wmt19.sh
scripts/fsmt/eval-facebook-wmt19.sh
scripts/fsmt/gen-card-allenai-wmt16.py
scripts/fsmt/gen-card-allenai-wmt19.py
scripts/fsmt/gen-card-facebook-wmt19.py
==================
08bfc1718;Xi Ye;2020-09-16 06:40:57 -0500;fix the warning message of overflowed sequence (#7151)

==

src/transformers/tokenization_utils_base.py
==================
af8425b74;Julien Plu;2020-09-16 13:03:47 +0200;Refactoring the TF activations functions (#7150)
* Refactoring the activations functions into a common file

* Apply style

* remove unused import

* fix tests

* Fix tests.
==

src/transformers/activations_tf.py
src/transformers/modeling_tf_albert.py
src/transformers/modeling_tf_bert.py
src/transformers/modeling_tf_distilbert.py
src/transformers/modeling_tf_electra.py
src/transformers/modeling_tf_funnel.py
src/transformers/modeling_tf_gpt2.py
src/transformers/modeling_tf_lxmert.py
src/transformers/modeling_tf_mobilebert.py
src/transformers/modeling_tf_openai.py
src/transformers/modeling_tf_roberta.py
src/transformers/modeling_tf_xlm.py
src/transformers/modeling_tf_xlnet.py
tests/test_activations_tf.py
==================
b00cafbde;Stas Bekman;2020-09-15 16:25:25 -0700;[docs] add testing documentation (#7101)
* [docs] add testing documentation

* Update docs/source/testing.rst

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* tweaks as suggested

* Update docs/source/testing.rst

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update docs/source/testing.rst

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update docs/source/testing.rst

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update docs/source/testing.rst

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update docs/source/testing.rst

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update docs/source/testing.rst

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update docs/source/testing.rst

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update docs/source/testing.rst

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update docs/source/testing.rst

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update docs/source/testing.rst

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update docs/source/testing.rst

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* tweaks

* Update docs/source/testing.rst

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update docs/source/testing.rst

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* more tweaks

* suggestions from @LysandreJik

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

docs/source/index.rst
docs/source/testing.rst
==================
85ffda96f;Patrick von Platen;2020-09-15 21:10:07 +0200;fix encoder decoder kwargs (#7131)

==

src/transformers/modeling_encoder_decoder.py
==================
4c62c6021;Yih-Dar;2020-09-15 17:51:50 +0200;fix ZeroDivisionError and epoch counting (#7125)
* fix ZeroDivisionError and epoch counting

* Add test for num_train_epochs calculation in trainer.py

* Remove @require_non_multigpu for test_num_train_epochs_in_training
==

src/transformers/trainer.py
tests/test_trainer.py
==================
7af2791d7;Patrick von Platen;2020-09-15 16:47:36 +0200;Create README.md

==

model_cards/facebook/rag-token-nq_new/README.md
==================
153ec2f15;Sylvain Gugger;2020-09-15 10:40:57 -0400;Funnel model cards (#7147)

==

model_cards/funnel-transformer/intermediate-base/README.md
model_cards/funnel-transformer/intermediate/README.md
model_cards/funnel-transformer/large-base/README.md
model_cards/funnel-transformer/large/README.md
model_cards/funnel-transformer/medium-base/README.md
model_cards/funnel-transformer/medium/README.md
model_cards/funnel-transformer/small-base/README.md
model_cards/funnel-transformer/small/README.md
model_cards/funnel-transformer/xlarge-base/README.md
model_cards/funnel-transformer/xlarge/README.md
==================
7186ca624;Sylvain Gugger;2020-09-15 10:27:24 -0400;Multi predictions trainer (#7126)
* Allow multiple outputs

* Formatting

* Move the unwrapping before metrics

* Fix typo

* Add test for non-supported config options
==

src/transformers/trainer.py
src/transformers/trainer_utils.py
tests/test_trainer.py
==================
52d250f6a;Pedro Lima;2020-09-15 13:54:12 +0100;[model_cards] pvl/labse_bert model card
From **Language-Agnostic BERT Sentence Embedding**

https://ai.googleblog.com/2020/08/language-agnostic-bert-sentence.html
==

model_cards/pvl/labse_bert/README.md
==================
84d64805b;tuner007;2020-09-15 18:18:25 +0530;Create README.md (#7097)
Model card for PEGASUS finetuned for paraphrasing task
==

model_cards/tuner007/pegasus_paraphrase/README.md
==================
52bb7ccce;Philip May;2020-09-15 14:48:13 +0200;German electra model card v3 update (#7089)
* changed eval table model order

* Update install

* update mc
==

model_cards/german-nlp-group/electra-base-german-uncased/README.md
==================
1a85299a5;Siddharth Jain;2020-09-15 17:48:42 +0530;Tiny typo fix (#7143)

==

src/transformers/generation_tf_utils.py
==================
e29c3f1b1;Paul O'Leary McCann;2020-09-15 20:04:50 +0900;Add quotes to paths in MeCab arguments (#7142)
Without quotes directories with spaces in them will fail to be processed
correctly.
==

src/transformers/tokenization_bert_japanese.py
==================
cb061e78e;Yih-Dar;2020-09-15 11:41:00 +0200;Fix TF Trainer loss calculation (#6998)
* create branch for issue #6968

* First attempt to fix incorrect tf trainer loss calculation

* Fix training loss in metric

* fix tf trainer evaluation loss

* apply count_instances_in_batch() for eval and test datasets

* prototype of using a new argument in trainer_tf.py to fix loss issue

* some renaming and fix, in particular for evaluation methods

* fix bugs to have a running version

* change to @staticmethod

* apply style
==

src/transformers/trainer_tf.py
==================
b0cbcdb05;Stas Bekman;2020-09-15 01:01:14 -0700;[logging] remove no longer needed verbosity override (#7100)

==

examples/bert-loses-patience/run_glue_with_pabee.py
examples/contrib/mm-imdb/run_mmimdb.py
examples/contrib/run_swag.py
examples/deebert/run_glue_deebert.py
examples/distillation/run_squad_w_distillation.py
examples/movement-pruning/masked_run_glue.py
examples/movement-pruning/masked_run_squad.py
examples/question-answering/run_squad.py
examples/text-classification/run_xnli.py
templates/adding_a_new_example_script/run_xxx.py
==================
2bf70e215;Sylvain Gugger;2020-09-15 03:32:44 -0400;Fix reproducible tests in Trainer (#7119)
* Fix reproducible tests in Trainer

* Deal with multiple GPUs
==

tests/test_trainer.py
==================
9e89390ce;Sam Shleifer;2020-09-14 20:33:08 -0400;[QOL] add signature for prepare_seq2seq_batch (#7108)

==

src/transformers/tokenization_bart.py
src/transformers/tokenization_marian.py
src/transformers/tokenization_mbart.py
src/transformers/tokenization_t5.py
src/transformers/tokenization_utils.py
tests/test_tokenization_common.py
==================
33d479d2b;Sam Shleifer;2020-09-14 15:57:56 -0400;[s2s] distributed eval in one command (#7124)

==

examples/seq2seq/aggregate_distributed_results.py
examples/seq2seq/romanian_postprocessing.md
examples/seq2seq/run_distributed_eval.py
examples/seq2seq/utils.py
==================
206b78d48;sgugger;2020-09-14 14:08:51 -0400;Pin version of TF and torch

==

setup.py
==================
90cde2e93;Kevin Canwen Xu;2020-09-14 23:50:22 +0800;Add Mirror Option for Downloads (#6679)
* Add Tuna Mirror for Downloads from China

* format fix

* Use preset instead of hardcoding URL

* Fix

* make style

* update the mirror option doc

* update the mirror
==

src/transformers/configuration_utils.py
src/transformers/file_utils.py
src/transformers/modelcard.py
src/transformers/modeling_tf_utils.py
src/transformers/modeling_utils.py
src/transformers/tokenization_utils_base.py
==================
e0e0675ac;Antonio V Mendoza;2020-09-14 10:07:04 -0400;Demoing LXMERT with raw images by incorporating the FRCNN model for roi-pooled extraction and bounding-box predction on the GQA answer set. (#6986)
* adding demo

* Update examples/lxmert/requirements.txt

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* Update examples/lxmert/checkpoint.sh

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* added user input for .py demo

* updated model loading, data extrtaction, checkpoints, and lots of other automation

* adding normalizing for bounding boxes

* Update requirements.txt

* some optimizations for extracting data

* added data extracting file

* added data extraction file

* minor fixes to reqs and readme

* Style

* remove options

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
Co-authored-by: Lysandre <lysandre.debut@reseau.eseo.fr>
==

examples/lxmert/README.md
examples/lxmert/demo.ipynb
examples/lxmert/extracting_data.py
examples/lxmert/modeling_frcnn.py
examples/lxmert/processing_image.py
examples/lxmert/requirements.txt
examples/lxmert/utils.py
examples/lxmert/visualizing_image.py
==================
5636cbb25;sgugger;2020-09-14 09:37:55 -0400;Extra )

==

docs/source/model_doc/auto.rst
==================
ccc8e30c8;Sylvain Gugger;2020-09-14 09:26:41 -0400;Clean up autoclass doc (#7081)

==

docs/source/model_doc/auto.rst
src/transformers/configuration_auto.py
src/transformers/modeling_auto.py
src/transformers/modeling_tf_auto.py
src/transformers/tokenization_auto.py
==================
3ca1874ca;Stas Bekman;2020-09-14 05:54:23 -0700;[examples testing] restore code (#7099)
For some reason https://github.com/huggingface/transformers/pull/5512 re-added temp dir creation code that was removed by
https://github.com/huggingface/transformers/pull/6494 defeating the purpose of that PR for those tests.
==

examples/requirements.txt
examples/test_examples.py
==================
4d3914841;Stas Bekman;2020-09-14 04:51:19 -0700;fix deprecation warnings (#7033)
* fix deprecation warnings

* remove tests/test_tokenization_common.py's test_padding_to_max_length

* revert test_padding_to_max_length
==

src/transformers/modeling_funnel.py
src/transformers/modeling_tf_utils.py
tests/test_tokenization_common.py
==================
576eec98e;Stas Bekman;2020-09-14 04:50:51 -0700;ignore FutureWarning in tests (#7079)

==

tests/conftest.py
==================
15d18e030;Bartosz Telenczuk;2020-09-14 13:43:40 +0200;fix link to paper (#7116)

==

docs/source/model_doc/encoderdecoder.rst
==================
bb3106f74;Lysandre Debut;2020-09-14 13:42:13 +0200;Temporarily skip failing tests due to dependency change (#7118)
* Temporarily skip failing tests due to dependency change

* Remove trace
==

examples/test_examples.py
tests/test_trainer.py
==================
0fab39695;Sam Shleifer;2020-09-14 00:03:59 -0400;[s2s distill] allow pegasus-12-12 (#7104)

==

examples/seq2seq/distillation.py
==================
de9e29796;Sam Shleifer;2020-09-13 23:40:38 -0400;[s2s] distributed eval cleanup (#7110)

==

examples/seq2seq/run_distributed_eval.py
examples/seq2seq/utils.py
==================
54395d87a;Sam Shleifer;2020-09-13 20:48:47 -0400;Update xsum length penalty to better values (#7107)

==

src/transformers/configuration_pegasus.py
==================
e7f8d2ab6;Sam Shleifer;2020-09-13 17:28:18 -0400;[s2s] two stage run_distributed_eval.py (#7105)

==

examples/seq2seq/aggregate_distributed_results.py
examples/seq2seq/run_distributed_eval.py
examples/seq2seq/utils.py
==================
0ec63afec;Sam Shleifer;2020-09-13 15:11:47 -0400;fix bug in pegasus converter (#7094)

==

src/transformers/convert_pegasus_tf_to_pytorch.py
==================
b76cb1c3d;Sam Shleifer;2020-09-12 01:08:21 -0400;[s2s] run_eval supports --prefix clarg. (#6953)

==

examples/seq2seq/run_eval.py
examples/seq2seq/test_bash_script.py
==================
563ffb3dc;ÊùéÊòéÊµ©;2020-09-12 03:21:05 +0800;Create README.md (#7066)

==

model_cards/microsoft/layoutlm-base-uncased/README.md
==================
1ad49cde3;ÊùéÊòéÊµ©;2020-09-12 03:20:54 +0800;Create README.md (#7067)

==

model_cards/microsoft/layoutlm-large-uncased/README.md
==================
4753816e3;Sagor Sarker;2020-09-12 01:17:25 +0600;added bangla-bert-base model card and also modified other model cards (#7071)
* added bangla-bert-base

* Apply suggestions from code review

Co-authored-by: Julien Chaumond <chaumond@gmail.com>
==

model_cards/sagorsarker/bangla-bert-base/README.md
model_cards/sagorsarker/codeswitch-hineng-lid-lince/README.md
model_cards/sagorsarker/codeswitch-hineng-ner-lince/README.md
model_cards/sagorsarker/codeswitch-hineng-pos-lince/README.md
model_cards/sagorsarker/codeswitch-nepeng-lid-lince/README.md
model_cards/sagorsarker/codeswitch-spaeng-lid-lince/README.md
model_cards/sagorsarker/codeswitch-spaeng-ner-lince/README.md
model_cards/sagorsarker/codeswitch-spaeng-pos-lince/README.md
model_cards/sagorsarker/codeswitch-spaeng-sentiment-analysis-lince/README.md
==================
0a8c17d53;Suraj Patil;2020-09-11 23:48:45 +0530;[T5Tokenizer] remove prefix_tokens (#7078)

==

src/transformers/tokenization_t5.py
tests/test_tokenization_t5.py
==================
4cbd50e61;Sylvain Gugger;2020-09-11 12:06:31 -0400;Compute loss method (#7074)

==

docs/source/main_classes/trainer.rst
src/transformers/trainer.py
==================
ae736163d;Sylvain Gugger;2020-09-11 12:01:33 -0400;Add tests and fix various bugs in ModelOutput (#7073)
* Add tests and fix various bugs in ModelOutput

* Update tests/test_model_output.py

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
==

src/transformers/file_utils.py
tests/test_model_output.py
==================
e841b75de;Sylvain Gugger;2020-09-11 10:42:09 -0400;Automate the lists in auto-xxx docs (#7061)
* More readable dict

* More nlp -> datasets

* Revert "More nlp -> datasets"

This reverts commit 3cd1883d226c63c4a686fc1fed35f2cd586ebe45.

* Automate the lists in auto-xxx docs

* More readable dict

* Revert "More nlp -> datasets"

This reverts commit 3cd1883d226c63c4a686fc1fed35f2cd586ebe45.

* Automate the lists in auto-xxx docs

* nlp -> datasets

* Fix new key
==

docs/source/main_classes/configuration.rst
src/transformers/configuration_auto.py
src/transformers/modeling_auto.py
src/transformers/modeling_tf_auto.py
src/transformers/tokenization_auto.py
==================
0054a48cd;Sylvain Gugger;2020-09-11 04:43:19 -0400;Add dep on datasets (#7058)

==

CONTRIBUTING.md
==================
221d4c63a;Patrick von Platen;2020-09-11 09:57:53 +0200;clean naming (#7068)

==

src/transformers/configuration_auto.py
src/transformers/configuration_bert_generation.py
src/transformers/modeling_bert_generation.py
src/transformers/tokenization_bert_generation.py
tests/test_modeling_encoder_decoder.py
tests/test_tokenization_bert_generation.py
==================
8fcbe486e;Stas Bekman;2020-09-10 15:52:55 -0700;these tests require non-multigpu env (#7059)
* these tests require non-multigpu env

* cleanup

* clarify
==

src/transformers/testing_utils.py
tests/test_trainer.py
==================
77950c485;Sam Shleifer;2020-09-10 15:23:44 -0400;[wip/s2s] DistributedSortishSampler (#7056)

==

examples/seq2seq/finetune.py
examples/seq2seq/test_seq2seq_examples.py
examples/seq2seq/utils.py
==================
514486739;Sylvain Gugger;2020-09-10 14:51:08 -0400;Fix CI with change of name of nlp (#7054)
* nlp -> datasets

* More nlp -> datasets

* Woopsie

* More nlp -> datasets

* One last
==

.circleci/config.yml
.github/workflows/self-push.yml
.github/workflows/self-scheduled.yml
examples/longform-qa/README.md
examples/longform-qa/eli5_app.py
examples/longform-qa/eli5_utils.py
examples/requirements.txt
examples/seq2seq/download_wmt.py
setup.cfg
src/transformers/__init__.py
src/transformers/file_utils.py
src/transformers/trainer.py
tests/test_trainer.py
==================
e9a2f772b;Sam Shleifer;2020-09-10 14:11:34 -0400;[s2s] --eval_max_generate_length (#7018)

==

examples/seq2seq/distil_marian_enro_teacher.sh
examples/seq2seq/distil_marian_no_teacher.sh
examples/seq2seq/finetune.py
examples/seq2seq/test_seq2seq_examples.py
==================
df4594a9d;Stas Bekman;2020-09-10 10:31:01 -0700;[xlm tok] config dict: fix str into int to match definition (#7034)

==

src/transformers/tokenization_xlm.py
==================
d6c08b07a;Julien Chaumond;2020-09-10 17:19:01 +0200;[AutoTokenizer] Correct error message

==

src/transformers/tokenization_auto.py
==================
db38f7ce2;Patrick von Platen;2020-09-10 17:12:33 +0200;[BertGeneration, Docs] Fix another old name in docs (#7050)
* correct docs for bert generation

* upload
==

docs/source/model_doc/bertgeneration.rst
==================
3bd95b0fa;Patrick von Platen;2020-09-10 17:08:40 +0200;correct docs for bert generation (#7048)

==

docs/source/model_doc/bertgeneration.rst
==================
eb2feb5d9;Patrick von Platen;2020-09-10 17:05:50 +0200;Create README.md

==

model_cards/google/bert2bert_L-24_wmt_en_de/README.md
==================
66a5a6fda;Ashwin Geet Dsa;2020-09-10 17:04:03 +0200;fix to ensure that returned tensors after the tokenization is Long (#7039)
* fix to ensure that returned tensors after the tokenization is Long

* fix to ensure that returned tensors after the tokenization is Long

Co-authored-by: Ashwin Geet Dsa <adsa@grvingt-6.nancy.grid5000.fr>
==

src/transformers/data/data_collator.py
==================
9ccdb1d51;Patrick von Platen;2020-09-10 17:01:19 +0200;Update README.md

==

model_cards/google/bert2bert_L-24_wmt_de_en/README.md
==================
60698936f;Patrick von Platen;2020-09-10 17:00:10 +0200;Create README.md

==

model_cards/google/bert2bert_L-24_wmt_de_en/README.md
==================
e0c3bc8ee;Patrick von Platen;2020-09-10 16:51:15 +0200;Create README.md

==

model_cards/google/roberta2roberta_L-24_discofuse/README.md
==================
c356b9878;Patrick von Platen;2020-09-10 16:45:44 +0200;Create README.md

==

model_cards/google/roberta2roberta_L-24_wikisplit/README.md
==================
5afd3f619;Patrick von Platen;2020-09-10 16:44:47 +0200;Create README.md

==

model_cards/google/roberta2roberta_L-24_cnn_daily_mail/README.md
==================
15a189049;Sylvain Gugger;2020-09-10 10:41:56 -0400;Add TF Funnel Transformer (#7029)
* Add TF Funnel Transformer

* Proper dummy input

* Formatting

* Update src/transformers/modeling_tf_funnel.py

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* Address review comments

* One review comment forgotten

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
==

docs/source/model_doc/funnel.rst
src/transformers/__init__.py
src/transformers/file_utils.py
src/transformers/modeling_funnel.py
src/transformers/modeling_tf_auto.py
src/transformers/modeling_tf_funnel.py
tests/test_modeling_tf_common.py
tests/test_modeling_tf_funnel.py
==================
7fd1febf3;Patrick von Platen;2020-09-10 16:40:51 +0200;Add "Leveraging Pretrained Checkpoints for Generation" Seq2Seq models. (#6594)
* add conversion script

* improve conversion script

* make style

* add tryout files

* fix

* update

* add causal bert

* better names

* add tokenizer file as well

* finish causal_bert

* fix small bugs

* improve generate

* change naming

* renaming

* renaming

* renaming

* remove leftover files

* clean files

* add fix tokenizer

* finalize

* correct slow test

* update docs

* small fixes

* fix link

* adapt check repo

* apply sams and sylvains recommendations

* fix import

* implement Lysandres recommendations

* fix logger warn
==

docs/source/index.rst
docs/source/model_doc/bertgeneration.rst
src/transformers/__init__.py
src/transformers/configuration_auto.py
src/transformers/configuration_bert_generation.py
src/transformers/convert_tf_hub_seq_to_seq_bert_to_pytorch.py
src/transformers/file_utils.py
src/transformers/generation_utils.py
src/transformers/modeling_auto.py
src/transformers/modeling_bert_generation.py
src/transformers/modeling_encoder_decoder.py
src/transformers/tokenization_auto.py
src/transformers/tokenization_bert_generation.py
src/transformers/tokenization_camembert.py
src/transformers/tokenization_t5.py
tests/test_modeling_bert_generation.py
tests/test_modeling_encoder_decoder.py
tests/test_tokenization_bert_generation.py
tests/test_tokenization_t5.py
utils/check_repo.py
==================
d1691d90e;Sylvain Gugger;2020-09-10 10:36:02 -0400;Samell fixed in tf template (#7044)

==

templates/adding_a_new_model/modeling_tf_xxx.py
==================
63e539459;Patrick von Platen;2020-09-10 16:34:28 +0200;Update README.md

==

model_cards/google/roberta2roberta_L-24_gigaword/README.md
==================
054db06b1;Patrick von Platen;2020-09-10 16:30:46 +0200;Create README.md

==

model_cards/google/roberta2roberta_L-24_gigaword/README.md
==================
b482ad474;Lysandre Debut;2020-09-10 14:45:52 +0200;Fix template (#7040)

==

templates/adding_a_new_model/modeling_tf_xxx.py
==================
762cba3bd;Yu Liu;2020-09-10 04:56:29 -0700;Albert pretrain datasets/ datacollator (#6168)
* add dataset for albert pretrain

* datacollator for albert pretrain

* naming, comprehension, file reading change

* data cleaning is no needed after this modification

* delete prints

* fix a bug

* file structure change

* add tests for albert datacollator

* remove random seed

* add back len and get item function

* sample file for testing and test code added

* format change for black

* more format change

* Style

* var assignment issue resolve

* add back wrongly deleted DataCollatorWithPadding in init file

* Style

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
Co-authored-by: Lysandre <lysandre.debut@reseau.eseo.fr>
==

src/transformers/__init__.py
src/transformers/data/data_collator.py
src/transformers/data/datasets/__init__.py
src/transformers/data/datasets/language_modeling.py
tests/fixtures/tests_samples/wiki_text/wiki_00
tests/test_data_collator.py
==================
49e9be063;Johann C. Rocholl;2020-09-10 02:31:59 -0700;Fix confusing warnings during TF2 import from PyTorch (#6623)
1. Swapped missing_keys and unexpected_keys.

2. Copy&paste error caused these warnings to say "from TF 2.0" when it's actually "from PyTorch".
==

src/transformers/modeling_tf_pytorch_utils.py
==================
4ee1053dc;Stas Bekman;2020-09-10 01:58:29 -0700;add -y to bypass prompt for transformers-cli upload (#7035)

==

docs/source/model_sharing.rst
src/transformers/commands/user.py
==================
76818cc4c;Patrick von Platen;2020-09-09 16:26:35 +0200;Create README.md

==

model_cards/google/roberta2roberta_L-24_bbc/README.md
==================
15478c128;Lysandre Debut;2020-09-09 12:55:17 +0200;Batch encore plus and overflowing tokens fails when non existing overflowing tokens for a sequence (#6677)
* Patch and test

* Fix tests
==

src/transformers/tokenization_utils_base.py
tests/test_tokenization_common.py
==================
9fd11bf1a;Henry Dashwood;2020-09-09 09:56:40 +0100;replace torch.triu with onnx compatible code (#6929)

==

src/transformers/modeling_bart.py
==================
ed71c21d6;Julien Chaumond;2020-09-09 10:22:59 +0200;[from_pretrained] Allow tokenizer_type ‚â† model_type (#6995)

==

src/transformers/configuration_utils.py
src/transformers/testing_utils.py
src/transformers/tokenization_auto.py
tests/test_tokenization_auto.py
==================
03e363f9a;Stas Bekman;2020-09-09 01:08:36 -0700;[generation] consistently add eos tokens (#6982)
Currently beam search returns inconsistent outputs - if hypos have different lengths we get eos, if they are the same - we don't.

This PR makes the output consistent.

Also why not also replace:

```
            if sent_lengths[i] < max_length:
                decoded[i, sent_lengths[i]] = eos_token_id
```
with:
```
            decoded[i, sent_lengths[i]] = eos_token_id
```
Shouldn't eos always be there? If the data gets truncated, the caller needs to user a larger `max_length`.

Please correct me if my logic is flawed.
==

src/transformers/generation_utils.py
==================
d0963486c;Stas Bekman;2020-09-09 01:08:01 -0700;adding TRANSFORMERS_VERBOSITY env var (#6961)
* introduce TRANSFORMERS_VERBOSITY env var + test + test helpers

* cleanup

* remove helper function
==

docs/source/main_classes/logging.rst
src/transformers/testing_utils.py
src/transformers/utils/logging.py
tests/test_logging.py
==================
f0fc0aea6;Sam Shleifer;2020-09-08 13:29:16 -0400;pegasus.rst: fix expected output (#7017)

==

docs/source/model_doc/pegasus.rst
==================
120176ea2;Patrick von Platen;2020-09-08 18:51:28 +0200;[Longformer] Fix longformer documentation (#7016)
* fix longformer

* allow position ids to not be initialized
==

src/transformers/modeling_longformer.py
==================
5c4eb4b1a;Lysandre Debut;2020-09-08 16:51:58 +0200;Fixing FLOPS merge by checking if torch is available (#7013)
* Should check if `torch` is available

* fixed samples_count error, distributed_concat arguments

* style

* Import torch at beginning of file

Co-authored-by: TevenLeScao <teven.lescao@gmail.com>
==

src/transformers/trainer.py
src/transformers/trainer_utils.py
==================
01d340adf;Teven;2020-09-08 16:00:56 +0200;Floating-point operations logging in trainer (#6768)
* neFLOs calculation, logging, and reloading (#1)

* testing distributed consecutive batches

* fixed AttributeError from DataParallel

* removed verbosity

* rotate with use_mtime=True

* removed print

* fixed interaction with gradient accumulation

* indent formatting

* distributed neflo counting

* fixed typo

* fixed typo

* mean distributed losses

* exporting log history

* moved a few functions

* floating_point_ops clarification for transformers with parameter-reuse

* code quality

* double import

* made flo estimation more task-agnostic

* only logging flos if computed

* code quality

* unused import

* Update src/transformers/trainer.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/modeling_utils.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Sylvain review

* Update src/transformers/modeling_utils.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* black

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

src/transformers/modeling_utils.py
src/transformers/trainer.py
src/transformers/trainer_utils.py
==================
d155b38d6;Sylvain Gugger;2020-09-08 08:08:08 -0400;Funnel transformer (#6908)
* Initial model

* Fix upsampling

* Add special cls token id and test

* Formatting

* Test and fist FunnelTokenizerFast

* Common tests

* Fix the check_repo script and document Funnel

* Doc fixes

* Add all models

* Write doc

* Fix test

* Initial model

* Fix upsampling

* Add special cls token id and test

* Formatting

* Test and fist FunnelTokenizerFast

* Common tests

* Fix the check_repo script and document Funnel

* Doc fixes

* Add all models

* Write doc

* Fix test

* Fix copyright

* Forgot some layers can be repeated

* Apply suggestions from code review

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* Update src/transformers/modeling_funnel.py

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* Address review comments

* Update src/transformers/modeling_funnel.py

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>

* Address review comments

* Update src/transformers/modeling_funnel.py

Co-authored-by: Sam Shleifer <sshleifer@gmail.com>

* Slow integration test

* Make small integration test

* Formatting

* Add checkpoint and separate classification head

* Formatting

* Expand list, fix link and add in pretrained models

* Styling

* Add the model in all summaries

* Typo fixes

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
Co-authored-by: Sam Shleifer <sshleifer@gmail.com>
==

README.md
docs/source/index.rst
docs/source/model_doc/funnel.rst
docs/source/model_summary.rst
docs/source/pretrained_models.rst
src/transformers/__init__.py
src/transformers/commands/convert.py
src/transformers/configuration_auto.py
src/transformers/configuration_funnel.py
src/transformers/convert_funnel_original_tf_checkpoint_to_pytorch.py
src/transformers/modeling_auto.py
src/transformers/modeling_funnel.py
src/transformers/tokenization_auto.py
src/transformers/tokenization_funnel.py
tests/test_modeling_common.py
tests/test_modeling_funnel.py
tests/test_tokenization_funnel.py
utils/check_repo.py
==================
25afb4ea5;Stuart Mesham;2020-09-08 14:07:33 +0200;fixed trainer tr_loss memory leak (#6999)
* fixed trainer tr_loss memory leak

* detached returned training loss from computation graph in the Trainer class' training_step() method

* Revert "fixed trainer tr_loss memory leak"

This reverts commit 47226e4e
==

src/transformers/trainer.py
==================
1b76936d1;Manuel Romero;2020-09-08 10:22:57 +0200;Fix typo (#6994)

==

examples/distillation/lm_seqs_dataset.py
==================
8235426ee;Philipp Schmid;2020-09-08 09:42:20 +0200;New Community NB "Fine tune GPT-2 with Trainer class" (#7005)

==

notebooks/README.md
==================
c18f5916a;Stas Bekman;2020-09-07 22:22:20 -0700;typo (#7001)
apologies for the tiny PRs, just sending those as I find them.
==

src/transformers/tokenization_utils_base.py
==================
60fc03290;Mehrdad Farahani;2020-09-08 01:13:50 +0430;README for HooshvareLab/bert-fa-base-uncased (#6990)
ParsBERT v2.0 is a fine-tuned and vocab-reconstructed version of ParsBERT, and it's able to be used in other scopes!

It includes these features:
- We added some unused-vocab for use in summarization and other scopes.
- We fine-tuned the model on vast styles of writing in the Persian language.
==

model_cards/HooshvareLab/bert-fa-base-uncased/README.md
==================
90ec78b51;Jangwon Park;2020-09-07 21:35:41 +0900;Add missing arguments for BertWordPieceTokenizer (#5810)

==

src/transformers/tokenization_bert.py
==================
77cd0e13d;Lysandre Debut;2020-09-07 14:31:06 +0200;Conversion scripts shouldn't have relative imports (#6991)

==

src/transformers/convert_albert_original_tf_checkpoint_to_pytorch.py
src/transformers/convert_bart_original_pytorch_checkpoint_to_pytorch.py
src/transformers/convert_bert_original_tf2_checkpoint_to_pytorch.py
src/transformers/convert_bert_original_tf_checkpoint_to_pytorch.py
src/transformers/convert_electra_original_tf_checkpoint_to_pytorch.py
src/transformers/convert_gpt2_original_tf_checkpoint_to_pytorch.py
src/transformers/convert_mobilebert_original_tf_checkpoint_to_pytorch.py
src/transformers/convert_openai_original_tf_checkpoint_to_pytorch.py
src/transformers/convert_pytorch_checkpoint_to_tf2.py
src/transformers/convert_reformer_trax_checkpoint_to_pytorch.py
src/transformers/convert_roberta_original_pytorch_checkpoint_to_pytorch.py
src/transformers/convert_t5_original_tf_checkpoint_to_pytorch.py
src/transformers/convert_transfo_xl_original_tf_checkpoint_to_pytorch.py
src/transformers/convert_xlm_original_pytorch_checkpoint_to_pytorch.py
src/transformers/convert_xlnet_original_tf_checkpoint_to_pytorch.py
==================
1650130b0;Lysandre;2020-09-07 14:16:59 +0200;Remove misleading docstring

==

examples/text-classification/run_glue.py
==================
159ef07e4;Stas Bekman;2020-09-07 05:12:25 -0700;match CI's version of flake8 (#6941)
my flake8 wasn't up-to-date enough `make quality` wasn't reporting the same things CI did - this PR adds the actual required version.

Thinking more about some of these minimal versions - CI will always install afresh and thus will always run the latest version. Is there a way to tell pip to always install the latest versions of certain dependencies on `pip install -i ".[dev]"`, rather than hardcoding the minimals which quickly become outdated?
==

setup.py
==================
e9d0d4c75;Abed khooli;2020-09-07 14:31:22 +0300;Create README.md (#6974)

==

model_cards/akhooli/mbart-large-cc25-ar-en/README.md
==================
848fbe1e3;Stas Bekman;2020-09-07 04:28:06 -0700;[gen utils] missing else case (#6980)
* [gen utils] missing else case

1. `else` is missing - I hit that case while porting a model. Probably needs to assert there?
2. also the comment on top seems to be outdated (just vocab_size is being set there)

* typo
==

src/transformers/generation_utils.py
==================
f7e80721e;tznurmin;2020-09-07 10:12:22 +0000;Fixed the default number of attention heads in Reformer Configuration (#6973)

==

src/transformers/configuration_reformer.py
==================
e20d8895b;Richard Bownes;2020-09-07 11:01:40 +0100;Create README.md model card (#6964)
* Create README.md

* Add some custom prompts

Co-authored-by: Julien Chaumond <chaumond@gmail.com>
==

model_cards/rjbownes/Magic-The-Generating/README.md
==================
b4a9c95f1;Stas Bekman;2020-09-07 02:50:18 -0700;[testing] add dependency: parametrize (#6958)
unittest doesn't support pytest's super-handy `@pytest.mark.parametrize`, I researched and there are many proposed workarounds, most tedious at best. If we include https://pypi.org/project/parameterized/ in dev dependencies - it will provide a very easy to write parameterization in tests. Same as pytest's fixture, plus quite a few other ways. 

Example:
```
from parameterized import parameterized
@parameterized([
    (2, 2, 4),
    (2, 3, 8),
    (1, 9, 1),
    (0, 9, 0),
])
def test_pow(base, exponent, expected):
   assert_equal(math.pow(base, exponent), expected)
```
(extra `self`var if inside a test class)

To remind the pytest style is slightly different:
```
    @pytest.mark.parametrize("test_input,expected", [("3+5", 8), ("2+4", 6), ("6*9", 42)])
    def test_eval(test_input, expected):
```
More examples here: https://pypi.org/project/parameterized

May I suggest that it will make it much easier to write some types of tests?
==

setup.py
==================
acfaad74a;Stas Bekman;2020-09-07 02:36:16 -0700;[docstring] missing arg (#6933)
* [docstring] missing arg

add the missing `tie_word_embeddings` entry

* cleanup

* Update src/transformers/configuration_reformer.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

src/transformers/configuration_reformer.py
==================
c3317e1f8;Stas Bekman;2020-09-07 02:16:24 -0700;typo (#6959)
there is no var `decoder_input_ids`, but there is `input_ids` for decoder :)
==

src/transformers/generation_utils.py
==================
10c6f94ad;Julien Chaumond;2020-09-07 05:03:40 -0400;[model_card] register jplu/tf-xlm-r-ner-40-lang as multilingual

==

model_cards/jplu/tf-xlm-r-ner-40-lang/README.md
==================
9ef9c3972;Lysandre Debut;2020-09-07 10:56:08 +0200;Cannot index `None` (#6984)

==

src/transformers/modeling_bert.py
==================
08de989a0;Sylvain Gugger;2020-09-07 04:54:00 -0400;Trainer with grad accum (#6930)
* Add warning for gradient accumulation

* Formatting
==

src/transformers/training_args.py
src/transformers/training_args_tf.py
==================
d4aa7284c;Julien Chaumond;2020-09-07 04:33:15 -0400;[model_card] jplu/tf-xlm-r-ner-40-lang: Fix link
cc @jplu
==

model_cards/jplu/tf-xlm-r-ner-40-lang/README.md
==================
995a958dd;Boris Dayma;2020-09-07 02:03:45 -0500;feat: allow prefix for any generative model (#5885)
* feat: allow padding_text for any generative model

* docs(pipelines.py): correct typo

* Update src/transformers/pipelines.py

Co-authored-by: Sam Shleifer <sshleifer@gmail.com>

* feat: rename padding_text to prefix

* fix: cannot tokenize empty text

* fix: pass prefix arg to pipeline

* test: add prefix to text-generetation pipeline

* style: fix style

* style: clean code and variable name more explicit

* set arg docstring to optional

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

Co-authored-by: Sam Shleifer <sshleifer@gmail.com>
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

examples/text-generation/run_generation.py
src/transformers/pipelines.py
tests/test_pipelines.py
==================
ce37be9d9;Sam Shleifer;2020-09-06 20:41:29 -0400;[s2s] warn if --fp16 for torch 1.6 (#6977)

==

examples/seq2seq/finetune.py
==================
f72fe1f31;Patrick von Platen;2020-09-06 13:26:56 +0200;Correct wrong spacing in README

==

model_cards/patrickvonplaten/bert2bert-cnn_dailymail-fp16/README.md
==================
d31031f60;Steven Liu;2020-09-05 09:50:19 -0700;create model card for astroGPT (#6960)
* create model card for astroGPT

* Hotlink to actual image file

Co-authored-by: Julien Chaumond <chaumond@gmail.com>
==

model_cards/stevhliu/astroGPT/README.md
==================
56742e9f6;Naveenkhasyap;2020-09-05 03:54:32 +0530;Create Readme.MD for KanBERTo (#6942)
* Create Readme.MD for KanBERTo

KanBERTo language model readme for Kannada language.

* Update model_cards/Naveen-k/KanBERTo/README.md

Co-authored-by: Julien Chaumond <chaumond@gmail.com>
==

model_cards/Naveen-k/KanBERTo/README.md
==================
48ff6d510;Stas Bekman;2020-09-04 15:22:25 -0700;[doc] remove the implied defaults to :obj:`None`, s/True/ :obj:`True/, etc. (#6956)
* remove the implied defaults to :obj:`None`

* fix bug in the original

* replace to :obj:`True`, :obj:`False`
==

examples/bert-loses-patience/pabee/modeling_pabee_albert.py
examples/bert-loses-patience/pabee/modeling_pabee_bert.py
examples/deebert/src/modeling_highway_bert.py
examples/deebert/src/modeling_highway_roberta.py
examples/movement-pruning/emmental/modeling_bert_masked.py
src/transformers/configuration_bart.py
src/transformers/configuration_bert.py
src/transformers/configuration_electra.py
src/transformers/configuration_flaubert.py
src/transformers/configuration_gpt2.py
src/transformers/configuration_lxmert.py
src/transformers/configuration_mobilebert.py
src/transformers/configuration_openai.py
src/transformers/configuration_reformer.py
src/transformers/configuration_retribert.py
src/transformers/configuration_xlm.py
src/transformers/configuration_xlnet.py
src/transformers/modeling_albert.py
src/transformers/modeling_bart.py
src/transformers/modeling_bert.py
src/transformers/modeling_ctrl.py
src/transformers/modeling_distilbert.py
src/transformers/modeling_dpr.py
src/transformers/modeling_electra.py
src/transformers/modeling_encoder_decoder.py
src/transformers/modeling_flaubert.py
src/transformers/modeling_gpt2.py
src/transformers/modeling_longformer.py
src/transformers/modeling_lxmert.py
src/transformers/modeling_mmbt.py
src/transformers/modeling_mobilebert.py
src/transformers/modeling_openai.py
src/transformers/modeling_reformer.py
src/transformers/modeling_retribert.py
src/transformers/modeling_roberta.py
src/transformers/modeling_t5.py
src/transformers/modeling_tf_albert.py
src/transformers/modeling_tf_bert.py
src/transformers/modeling_tf_ctrl.py
src/transformers/modeling_tf_distilbert.py
src/transformers/modeling_tf_electra.py
src/transformers/modeling_tf_flaubert.py
src/transformers/modeling_tf_gpt2.py
src/transformers/modeling_tf_longformer.py
src/transformers/modeling_tf_lxmert.py
src/transformers/modeling_tf_mobilebert.py
src/transformers/modeling_tf_openai.py
src/transformers/modeling_tf_roberta.py
src/transformers/modeling_tf_t5.py
src/transformers/modeling_tf_transfo_xl.py
src/transformers/modeling_tf_xlm.py
src/transformers/modeling_tf_xlnet.py
src/transformers/modeling_transfo_xl.py
src/transformers/modeling_xlm.py
src/transformers/modeling_xlnet.py
src/transformers/testing_utils.py
src/transformers/tokenization_albert.py
src/transformers/tokenization_bert.py
src/transformers/tokenization_camembert.py
src/transformers/tokenization_dpr.py
src/transformers/tokenization_mbart.py
src/transformers/tokenization_pegasus.py
src/transformers/tokenization_reformer.py
src/transformers/tokenization_roberta.py
src/transformers/tokenization_t5.py
src/transformers/tokenization_xlm.py
src/transformers/tokenization_xlm_roberta.py
src/transformers/tokenization_xlnet.py
templates/adding_a_new_model/modeling_tf_xxx.py
templates/adding_a_new_model/modeling_xxx.py
templates/adding_a_new_model/tokenization_xxx.py
==================
eff274d62;Stas Bekman;2020-09-04 13:14:37 -0700;typo (#6952)

==

src/transformers/tokenization_utils_base.py
==================
a4fc0c80b;Sam Shleifer;2020-09-04 14:19:31 -0400;[s2s] run_eval.py parses generate_kwargs (#6948)

==

examples/seq2seq/run_eval.py
examples/seq2seq/test_seq2seq_examples.py
examples/seq2seq/utils.py
==================
6078b1209;Sam Shleifer;2020-09-04 14:05:56 -0400;[s2s] distill: --normalize_hidden --supervise_forward (#6834)

==

examples/seq2seq/distil_marian_enro_teacher.sh
examples/seq2seq/distillation.py
examples/seq2seq/test_seq2seq_examples.py
==================
c5d43a872;Stas Bekman;2020-09-04 07:09:42 -0700;[docstring] misc arg doc corrections (#6932)
* correct bool types

fix docstring s/int/bool/

* fix description

* fix num_labels to match reality
==

src/transformers/configuration_bart.py
==================
e3990d137;Patrick von Platen;2020-09-04 16:08:54 +0200;fix (#6946)

==

tests/test_modeling_lxmert.py
==================
a75e31981;Yih-Dar;2020-09-04 14:29:57 +0200;Fix mixed precision issue in TF DistilBert (#6915)
* Remove hard-coded uses of float32 to fix mixed precision use in TF Distilbert

* fix style

* fix gelu dtype issue in TF Distilbert

* fix numeric overflow while using half precision
==

src/transformers/modeling_tf_distilbert.py
==================
e95d262f2;Sam Shleifer;2020-09-03 17:31:35 -0400;[s2s] support early stopping based on loss, rather than rouge (#6927)

==

examples/seq2seq/callbacks.py
examples/seq2seq/finetune.py
examples/seq2seq/test_seq2seq_examples.py
==================
207ed8cb7;Sam Shleifer;2020-09-03 12:42:09 -0400;[s2s] use --eval_beams command line arg (#6926)

==

examples/seq2seq/finetune.py
==================
0f360d3d1;krfricke;2020-09-03 16:49:14 +0100;move wandb/comet logger init to train() to allow parallel logging (#6850)
* move wandb/comet logger init to train() to allow parallel logging

* Setup wandb/comet loggers on first call to log()
==

src/transformers/trainer.py
==================
39ed68d59;Sam Shleifer;2020-09-03 11:11:40 -0400;[s2s] allow task_specific_params=summarization_xsum (#6923)

==

examples/seq2seq/callbacks.py
examples/seq2seq/finetune.py
examples/seq2seq/test_seq2seq_examples.py
==================
5a318f075;Sam Shleifer;2020-09-03 09:47:00 -0400;[s2s]: script to convert pl checkpoints to hf checkpoints (#6911)
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

examples/seq2seq/convert_pl_checkpoint_to_hf.py
examples/seq2seq/distillation.py
examples/seq2seq/test_seq2seq_examples.py
==================
b8e4906c9;brett koonce;2020-09-03 08:29:01 -0500;tweak tar command in readme (#6919)

==

examples/seq2seq/README.md
==================
a66db7d82;Stefan Engl;2020-09-03 15:23:42 +0200;Corrected link to paper (#6905)

==

model_cards/oliverguhr/german-sentiment-bert/README.md
==================
55d61ce8d;David Mark Nemeskey;2020-09-03 15:20:03 +0200;Added a link to the thesis. (#6906)

==

model_cards/SZTAKI-HLT/hubert-base-cc/README.md
==================
653a79cca;abdullaholuk-loodos;2020-09-03 16:13:43 +0300;Loodos model cards had errors on "Usage" section. It is fixed. Also "electra-base-turkish-uncased" model removed from s3 and re-uploaded as "electra-base-turkish-uncased-discriminator". Its README added. (#6921)
Co-authored-by: Abdullah Oluk <abdullaholuk123@gmail.com>
==

model_cards/loodos/albert-base-turkish-uncased/README.md
model_cards/loodos/bert-base-turkish-uncased/README.md
model_cards/loodos/electra-base-turkish-64k-uncased-discriminator/README.md
model_cards/loodos/electra-base-turkish-uncased-discriminator/README.md
model_cards/loodos/electra-small-turkish-cased-discriminator/README.md
model_cards/loodos/electra-small-turkish-uncased-discriminator/README.md
==================
5a3aec90a;Julien Chaumond;2020-09-03 08:57:32 -0400;[model_card] link to correctly cased piaf dataset
cc @psorianom @rachelker
==

model_cards/etalab-ia/camembert-base-squadFR-fquad-piaf/README.md
==================
722b5807d;Sylvain Gugger;2020-09-03 04:14:58 -0400;Template updates (#6914)

==

templates/adding_a_new_model/README.md
templates/adding_a_new_model/tests/test_modeling_tf_xxx.py
templates/adding_a_new_model/tests/test_modeling_xxx.py
==================
ea2c6f1af;Antonio V Mendoza;2020-09-03 04:02:25 -0400;Adding the LXMERT pretraining model (MultiModal  languageXvision)  to HuggingFace's suite of models (#5793)
* added template files for LXMERT and competed the configuration_lxmert.py

* added modeling, tokization, testing, and finishing touched for lxmert [yet to be tested]

* added model card for lxmert

* cleaning up lxmert code

* Update src/transformers/modeling_lxmert.py

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* Update src/transformers/modeling_tf_lxmert.py

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* Update src/transformers/modeling_tf_lxmert.py

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* Update src/transformers/modeling_lxmert.py

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* tested torch lxmert, changed documtention, updated outputs, and other small fixes

* Update src/transformers/convert_pytorch_checkpoint_to_tf2.py

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* Update src/transformers/convert_pytorch_checkpoint_to_tf2.py

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* Update src/transformers/convert_pytorch_checkpoint_to_tf2.py

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* renaming, other small issues, did not change TF code in this commit

* added lxmert question answering model in pytorch

* added capability to edit number of qa labels for lxmert

* made answer optional for lxmert question answering

* add option to return hidden_states for lxmert

* changed default qa labels for lxmert

* changed config archive path

* squshing 3 commits: merged UI + testing improvments + more UI and testing

* changed some variable names for lxmert

* TF LXMERT

* Various fixes to LXMERT

* Final touches to LXMERT

* AutoTokenizer order

* Add LXMERT to index.rst and README.md

* Merge commit test fixes + Style update

* TensorFlow 2.3.0 sequential model changes variable names

Remove inherited test

* Update src/transformers/modeling_tf_pytorch_utils.py

* Update docs/source/model_doc/lxmert.rst

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update docs/source/model_doc/lxmert.rst

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/modeling_tf_lxmert.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* added suggestions

* Fixes

* Final fixes for TF model

* Fix docs

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
Co-authored-by: Lysandre <lysandre.debut@reseau.eseo.fr>
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

README.md
docs/source/index.rst
docs/source/model_doc/lxmert.rst
docs/source/pretrained_models.rst
model_cards/uncnlp/lxmert-base-uncased/LICENSE
model_cards/uncnlp/lxmert-base-uncased/README.md
model_cards/uncnlp/lxmert-base-uncased/lxmert_model-1.jpg
src/transformers/__init__.py
src/transformers/commands/convert.py
src/transformers/configuration_auto.py
src/transformers/configuration_lxmert.py
src/transformers/convert_lxmert_original_tf_checkpoint_to_pytorch.py
src/transformers/convert_pytorch_checkpoint_to_tf2.py
src/transformers/modeling_auto.py
src/transformers/modeling_lxmert.py
src/transformers/modeling_tf_lxmert.py
src/transformers/modeling_tf_mobilebert.py
src/transformers/modeling_tf_utils.py
src/transformers/tokenization_auto.py
src/transformers/tokenization_lxmert.py
tests/test_modeling_lxmert.py
tests/test_modeling_tf_lxmert.py
tests/test_tokenization_lxmert.py
==================
4ebb52afd;Puneetha Pai;2020-09-02 20:24:40 +0530;test_tf_common: remove un_used mixin class parameters (#6866)

==

tests/test_modeling_tf_common.py
tests/test_modeling_tf_distilbert.py
tests/test_modeling_tf_longformer.py
tests/test_modeling_tf_transfo_xl.py
tests/test_modeling_tf_xlnet.py
==================
e71f32c0e;Stas Bekman;2020-09-02 07:18:17 -0700;[testing] fix ambiguous test (#6898)
Since `generate()` does:
```
        num_beams = num_beams if num_beams is not None else self.config.num_beams
```
This test fails if `model.config.num_beams > 1` (which is the case in the model I'm porting).

This fix makes the test setup unambiguous by passing an explicit `num_beams=1` to `generate()`.

Thanks.
==

tests/test_modeling_common.py
==================
8f2723caf;Sylvain Gugger;2020-09-02 08:11:45 -0400;Output attention takes an s (#6903)
* Fix output_attention -> output_attentions

* Formatting

* One unsaved file
==

hubconf.py
src/transformers/configuration_auto.py
src/transformers/configuration_utils.py
src/transformers/modelcard.py
src/transformers/modeling_auto.py
src/transformers/modeling_distilbert.py
src/transformers/modeling_encoder_decoder.py
src/transformers/modeling_tf_auto.py
src/transformers/modeling_tf_distilbert.py
src/transformers/modeling_tf_utils.py
src/transformers/modeling_utils.py
==================
485da7222;Yohei Tamura;2020-09-02 20:36:32 +0900;fix error class instantiation (#6634)

==

src/transformers/tokenization_bert_japanese.py
==================
4230d30f7;Suraj Patil;2020-09-02 17:04:35 +0530;[pipelines] Text2TextGenerationPipeline (#6744)
* add Text2TextGenerationPipeline

* remove max length warning

* remove comments

* remove input_length

* fix typo

* add tests

* use TFAutoModelForSeq2SeqLM

* doc

* typo

* add the doc below TextGenerationPipeline

* doc nit

* style

* delete comment
==

docs/source/main_classes/pipelines.rst
src/transformers/__init__.py
src/transformers/pipelines.py
tests/test_pipelines.py
==================
6b2428122;Prajjwal Bhargava;2020-09-02 16:25:37 +0530;fix typo in comments (#6838)

==

src/transformers/modeling_bert.py
==================
7351ef83c;Stas Bekman;2020-09-02 03:51:51 -0700;[doc] typos (#6867)
* [doc] typos

fixed typos

* Update README.md
==

templates/adding_a_new_model/README.md
==================
ee1bff06f;Harry Wang;2020-09-02 06:45:19 -0400;minor docs grammar fixes (#6889)

==

docs/source/glossary.rst
docs/source/quicktour.rst
==================
8abd7f69f;Patrick von Platen;2020-09-02 12:44:51 +0200;fix warning for position ids (#6884)

==

src/transformers/modeling_electra.py
==================
7cb0572c6;Parthe Pandit;2020-09-02 03:39:01 -0700;Update modeling_bert.py (#6897)
outptus -> outputs in example of BertForPreTraining
==

src/transformers/modeling_bert.py
==================
e3c55ceb8;David Mark Nemeskey;2020-09-02 10:50:10 +0200;Model card for huBERT (#6893)
* Create README.md

Model card for huBERT.

* Update README.md

lowercase h

* Update model_cards/SZTAKI-HLT/hubert-base-cc/README.md

Co-authored-by: Julien Chaumond <chaumond@gmail.com>
==

model_cards/SZTAKI-HLT/hubert-base-cc/README.md
==================
1889e96c8;Patrick von Platen;2020-09-02 09:53:09 +0200;fix QA example for PT (#6890)

==

src/transformers/file_utils.py
==================
d822ab636;Julien Chaumond;2020-09-02 00:02:40 +0200;[model_cards] Fix file path for flexudy/t5-base-multi-sentence-doctor

==

model_cards/flexudy/t5-base-multi-sentence-doctor/README.md
model_cards/flexudy/t5-base-multi-sentence-doctor/sent-banner.png
==================
ad5fb33c9;Rohan Rajpal;2020-09-02 03:29:15 +0530;Create README.md (#6598)

==

model_cards/rohanrajpal/bert-base-en-es-codemix-cased/README.md
==================
f9dadcd85;Rohan Rajpal;2020-09-02 03:28:43 +0530;Create README.md (#6602)

==

model_cards/rohanrajpal/bert-base-en-hi-codemix-cased/README.md
==================
f5d69c75f;Igli Manaj;2020-09-01 23:56:19 +0200;Update multilingual passage rereanking model card (#6788)
Fix range of possible score, add inference .
==

model_cards/amberoad/bert-multilingual-passage-reranking-msmarco/README.md
==================
5d820f3ca;Tom Grek;2020-09-01 14:52:32 -0700;Model card for primer/BART-Squad2 (#6801)

==

model_cards/Primer/bart-squad2/README.md
==================
8b884dadc;zolekode;2020-09-01 23:38:55 +0200;added model card for flexudys t5 model (#6759)
Co-authored-by: zolekode <pascal.zoleko@fau.de>
==

model_cards/flexudy/README.md
model_cards/flexudy/sent-banner.png
==================
bff6d517c;hakan;2020-09-02 00:35:24 +0300;loodos turkish model cards added (#6840)

==

model_cards/loodos/albert-base-turkish-uncased/README.md
model_cards/loodos/bert-base-turkish-uncased/README.md
model_cards/loodos/electra-base-turkish-64k-uncased-discriminator/README.md
model_cards/loodos/electra-base-turkish-uncased/README.md
model_cards/loodos/electra-small-turkish-cased-discriminator/README.md
model_cards/loodos/electra-small-turkish-uncased-discriminator/README.md
==================
502d194b9;Manuel Romero;2020-09-01 23:09:10 +0200;Create README.md (#6887)
Add language meta attribute
==

model_cards/dccuchile/bert-base-spanish-wwm-cased/README.md
==================
d082edf21;Manuel Romero;2020-09-01 23:09:02 +0200;Create README.md (#6888)
Add language meta attribute
==

model_cards/dccuchile/bert-base-spanish-wwm-uncased/README.md
==================
dacbee9a5;Abed khooli;2020-09-02 00:06:15 +0300;Create README.md (#6886)
* Create README.md

model card for  akhooli/xlm-r-large-arabic-sent

* Update model_cards/akhooli/xlm-r-large-arabic-sent/README.md

Co-authored-by: Julien Chaumond <chaumond@gmail.com>
==

model_cards/akhooli/xlm-r-large-arabic-sent/README.md
==================
e2971e61b;Abed khooli;2020-09-01 23:57:48 +0300;Create README.md (#6885)

==

model_cards/akhooli/mbart-large-cc25-en-ar/README.md
==================
4d1a3ffde;Patrick von Platen;2020-09-01 21:56:39 +0200;[EncoderDecoder] Add xlm-roberta to encoder decoder (#6878)
* finish xlm-roberta

* finish docs

* expose XLMRobertaForCausalLM
==

docs/source/model_doc/xlmroberta.rst
src/transformers/__init__.py
src/transformers/modeling_auto.py
src/transformers/modeling_xlm_roberta.py
==================
311992630;Patrick von Platen;2020-09-01 19:24:45 +0200;Create README.md (#6883)
* Create README.md

* Update README.md
==

model_cards/patrickvonplaten/longformer2roberta-cnn_dailymail-fp16/README.md
==================
21d719238;Jin Young (Daniel) Sohn;2020-09-01 08:42:17 -0700;Add cache_dir to save features TextDataset (#6879)
* Add cache_dir to save features TextDataset

This is in case the dataset is in a RO filesystem, for which is the case
in tests (GKE TPU tests).

* style
==

examples/language-modeling/run_language_modeling.py
src/transformers/data/datasets/language_modeling.py
==================
1461aac8d;Lysandre Debut;2020-09-01 11:02:24 -0400;Update docs stable version

==

docs/source/_static/js/custom.js
==================
3726754a6;Lysandre;2020-09-01 14:39:07 +0200;v3.1.0 documentation

==

.circleci/deploy.sh
docs/source/_static/js/custom.js
==================
4b3ee9cbc;Lysandre;2020-09-01 14:27:52 +0200;Release: v3.1.0

==

docs/source/conf.py
setup.py
src/transformers/__init__.py
==================
afc4ece46;Patrick von Platen;2020-09-01 12:38:25 +0200;[Generate] Facilitate PyTorch generate using `ModelOutputs` (#6735)
* fix generate for GPT2 Double Head

* fix gpt2 double head model

* fix  bart / t5

* also add for no beam search

* fix no beam search

* fix encoder decoder

* simplify t5

* simplify t5

* fix t5 tests

* fix BART

* fix transfo-xl

* fix conflict

* integrating sylvains and sams comments

* fix tf past_decoder_key_values

* fix enc dec test
==

docs/source/model_doc/encoderdecoder.rst
src/transformers/generation_utils.py
src/transformers/modeling_bart.py
src/transformers/modeling_encoder_decoder.py
src/transformers/modeling_gpt2.py
src/transformers/modeling_openai.py
src/transformers/modeling_outputs.py
src/transformers/modeling_t5.py
src/transformers/modeling_tf_gpt2.py
src/transformers/modeling_tf_openai.py
src/transformers/modeling_tf_outputs.py
src/transformers/modeling_tf_t5.py
src/transformers/modeling_transfo_xl.py
tests/test_modeling_encoder_decoder.py
tests/test_modeling_gpt2.py
tests/test_modeling_openai.py
tests/test_modeling_t5.py
tests/test_modeling_tf_gpt2.py
tests/test_modeling_tf_openai.py
tests/test_modeling_tf_t5.py
==================
397f81961;Funtowicz Morgan;2020-09-01 11:35:35 +0200;Restore PaddingStrategy.MAX_LENGTH on QAPipeline while no v2. (#6875)
Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>
==

src/transformers/pipelines.py
==================
a32d85f0d;Sam Shleifer;2020-09-01 03:43:27 -0400;delete reinit (#6862)

==

src/transformers/configuration_utils.py
==================
d5f1ffa0d;Sylvain Gugger;2020-09-01 03:16:34 -0400;Logging doc (#6852)
* Add logging doc

* Foamtting

* Update docs/source/main_classes/logging.rst

* Update src/transformers/utils/logging.py

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
==

docs/source/index.rst
docs/source/main_classes/logging.rst
src/transformers/utils/logging.py
==================
59a6a32a6;Stas Bekman;2020-08-31 19:47:23 -0700;add a final report to all pytest jobs (#6861)
we had it added for one job, please add it to all pytest jobs - we need the output of what tests were run to debug the codecov issue. thank you!
==

.circleci/config.yml
==================
431ab19d7;Sam Shleifer;2020-08-31 17:59:34 -0400;[fix] typo in available in helper function (#6859)

==

examples/test_examples.py
==================
367235ee5;Sam Shleifer;2020-08-31 16:16:47 -0400;Bart can make decoder_input_ids from labels (#6758)

==

src/transformers/modeling_bart.py
==================
b9772897e;Sam Shleifer;2020-08-31 16:16:10 -0400;[s2s] command line args for faster val steps (#6833)

==

examples/seq2seq/distillation.py
examples/seq2seq/finetune.py
examples/seq2seq/test_seq2seq_examples.py
==================
8af1970e4;Sam Shleifer;2020-08-31 16:10:43 -0400;Fix marian slow test (#6854)

==

tests/test_modeling_marian.py
==================
bbdba0a76;Funtowicz Morgan;2020-08-31 21:28:00 +0200;Update ONNX notebook to include section on quantization. (#6831)
* Update ONNX notebook to include section on quantization.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Addressing ONNX team comments
==

notebooks/04-onnx-export.ipynb
==================
a59bcefbb;Sylvain Gugger;2020-08-31 15:16:39 -0400;Split hp search methods (#6857)
* Split the run_hp_search by backend

* Unused import
==

src/transformers/integrations.py
src/transformers/trainer.py
==================
23f9611c1;krfricke;2020-08-31 19:38:46 +0100;Add checkpointing to Ray Tune HPO (#6747)
* Introduce HPO checkpointing for PBT

* Moved checkpoint saving

* Fixed checkpoint subdir pass

* Fixed style

* Enable/disable checkpointing, check conditions for various tune schedulers incl. PBT

* Adjust number of GPUs to number of jobs

* Avoid mode pickling in ray

* Move hp search to integrations
==

src/transformers/integrations.py
src/transformers/trainer.py
src/transformers/trainer_utils.py
==================
61b7ba93f;Sam Shleifer;2020-08-31 13:48:26 -0400;Marian distill scripts + integration test (#6799)

==

examples/seq2seq/distil_marian_enro_teacher.sh
examples/seq2seq/distil_marian_no_teacher.sh
examples/seq2seq/test_bash_script.py
examples/test_examples.py
==================
02d09c8fc;Jin Young (Daniel) Sohn;2020-08-31 08:35:51 -0700;Only access loss tensor every logging_steps (#6802)
* Only access loss tensor every logging_steps

* tensor.item() was being called every step. This must not be done
for XLA:TPU tensors as it's terrible for performance causing TPU<>CPU
communication at each step. On RoBERTa MLM for example, it reduces step
time by 30%, should be larger for smaller step time models/tasks.
* Train batch size was not correct in case a user uses the
`per_gpu_train_batch_size` flag
* Avg reduce loss accross eval shards

* Fix style (#6803)

* t5 model should make decoder_attention_mask (#6800)

* [s2s] Test hub configs in self-scheduled CI (#6809)

* [s2s] round runtime in run_eval (#6798)

* Pegasus finetune script: add --adafactor (#6811)

* [bart] rename self-attention -> attention (#6708)

* [tests] fix typos in inputs (#6818)

* Fixed open in colab link (#6825)

* Add model card for singbert lite. Update widget for singbert and singbert-large. (#6827)

* BR_BERTo model card (#6793)

* clearly indicate shuffle=False (#6312)

* Clarify shuffle

* clarify shuffle

Co-authored-by: Kevin Canwen Xu <canwenxu@126.com>

* [s2s README] Add more dataset download instructions (#6737)

* Style

* Patch logging issue

* Set default logging level to `WARNING` instead of `INFO`

* TF Flaubert w/ pre-norm (#6841)

* Dataset and DataCollator for BERT Next Sentence Prediction (NSP) task (#6644)

* add datacollator and dataset for next sentence prediction task

* bug fix (numbers of special tokens & truncate sequences)

* bug fix (+ dict inputs support for data collator)

* add padding for nsp data collator; renamed cached files to avoid conflict.

* add test for nsp data collator

* Style

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
Co-authored-by: Lysandre <lysandre.debut@reseau.eseo.fr>

* Fix in Adafactor docstrings (#6845)

* Fix resuming training for Windows (#6847)

* Only access loss tensor every logging_steps

* tensor.item() was being called every step. This must not be done
for XLA:TPU tensors as it's terrible for performance causing TPU<>CPU
communication at each step. On RoBERTa MLM for example, it reduces step
time by 30%, should be larger for smaller step time models/tasks.
* Train batch size was not correct in case a user uses the
`per_gpu_train_batch_size` flag
* Avg reduce loss accross eval shards

* comments

Co-authored-by: Sam Shleifer <sshleifer@gmail.com>
Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>
Co-authored-by: Thomas Ashish Cherian <6967017+PandaWhoCodes@users.noreply.github.com>
Co-authored-by: Zane Lim <zyuanlim@gmail.com>
Co-authored-by: Rodolfo De Nadai <rdenadai@gmail.com>
Co-authored-by: xujiaze13 <37360975+xujiaze13@users.noreply.github.com>
Co-authored-by: Kevin Canwen Xu <canwenxu@126.com>
Co-authored-by: Lysandre <lysandre.debut@reseau.eseo.fr>
Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
Co-authored-by: Huang Lianzhe <hlz@pku.edu.cn>
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

src/transformers/trainer.py
==================
c48546c7f;Sylvain Gugger;2020-08-31 11:02:30 -0400;Fix resuming training for Windows (#6847)

==

src/transformers/trainer.py
==================
d2f9cb838;Sylvain Gugger;2020-08-31 10:52:47 -0400;Fix in Adafactor docstrings (#6845)

==

src/transformers/optimization.py
==================
2de7ee038;Huang Lianzhe;2020-08-31 20:25:00 +0800;Dataset and DataCollator for BERT Next Sentence Prediction (NSP) task (#6644)
* add datacollator and dataset for next sentence prediction task

* bug fix (numbers of special tokens & truncate sequences)

* bug fix (+ dict inputs support for data collator)

* add padding for nsp data collator; renamed cached files to avoid conflict.

* add test for nsp data collator

* Style

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
Co-authored-by: Lysandre <lysandre.debut@reseau.eseo.fr>
==

src/transformers/__init__.py
src/transformers/data/data_collator.py
src/transformers/data/datasets/__init__.py
src/transformers/data/datasets/language_modeling.py
tests/test_data_collator.py
==================
895d39466;Lysandre Debut;2020-08-31 10:53:20 +0200;TF Flaubert w/ pre-norm (#6841)

==

src/transformers/modeling_tf_flaubert.py
==================
4561f05c5;Lysandre;2020-08-31 09:56:12 +0200;Set default logging level to `WARNING` instead of `INFO`

==

src/transformers/utils/logging.py
==================
05c321415;Lysandre;2020-08-31 09:37:08 +0200;Patch logging issue

==

src/transformers/file_utils.py
==================
dfa10a41b;Sam Shleifer;2020-08-30 16:29:24 -0400;[s2s README] Add more dataset download instructions (#6737)

==

examples/seq2seq/README.md
==================
32fe44086;xujiaze13;2020-08-30 04:26:10 -0700;clearly indicate shuffle=False (#6312)
* Clarify shuffle

* clarify shuffle

Co-authored-by: Kevin Canwen Xu <canwenxu@126.com>
==

examples/lightning_base.py
==================
0eecaceac;Rodolfo De Nadai;2020-08-30 08:02:46 -0300;BR_BERTo model card (#6793)

==

model_cards/rdenadai/BR_BERTo/README.md
==================
d176aaad7;Zane Lim;2020-08-30 18:21:49 +0800;Add model card for singbert lite. Update widget for singbert and singbert-large. (#6827)

==

model_cards/zanelim/singbert-large-sg/README.md
model_cards/zanelim/singbert-lite-sg/README.md
model_cards/zanelim/singbert/README.md
==================
a5847619e;Thomas Ashish Cherian;2020-08-30 15:51:00 +0530;Fixed open in colab link (#6825)

==

notebooks/03-pipelines.ipynb
==================
563485bf9;Stas Bekman;2020-08-30 03:19:57 -0700;[tests] fix typos in inputs (#6818)

==

tests/test_tokenization_bart.py
tests/test_tokenization_t5.py
==================
22933e661;Sam Shleifer;2020-08-29 18:03:08 -0400;[bart] rename self-attention -> attention (#6708)

==

src/transformers/modeling_bart.py
==================
0f58903bb;Sam Shleifer;2020-08-29 17:43:32 -0400;Pegasus finetune script: add --adafactor (#6811)

==

examples/seq2seq/finetune_pegasus_xsum.sh
src/transformers/configuration_pegasus.py
src/transformers/convert_pegasus_tf_to_pytorch.py
tests/test_modeling_pegasus.py
==================
ac47458a0;Sam Shleifer;2020-08-29 17:36:31 -0400;[s2s] round runtime in run_eval (#6798)

==

examples/seq2seq/run_eval.py
==================
5ab21b072;Sam Shleifer;2020-08-28 17:05:52 -0400;[s2s] Test hub configs in self-scheduled CI (#6809)

==

examples/seq2seq/test_seq2seq_examples.py
==================
3cac867fa;Sam Shleifer;2020-08-28 15:22:33 -0400;t5 model should make decoder_attention_mask (#6800)

==

src/transformers/tokenization_t5.py
tests/test_tokenization_t5.py
==================
20f778645;Sam Shleifer;2020-08-28 15:02:25 -0400;Fix style (#6803)

==

tests/test_tokenization_fast.py
==================
9336086ab;Sam Shleifer;2020-08-28 11:15:17 -0400;prepare_seq2seq_batch makes labels/ decoder_input_ids made later. (#6654)
* broken test

* batch parity

* tests pass

* boom boom

* boom boom

* split out bart tokenizer tests

* fix tests

* boom boom

* Fixed dataset bug

* Fix marian

* Undo extra

* Get marian working

* Fix t5 tok tests

* Test passing

* Cleanup

* better assert msg

* require torch

* Fix mbart tests

* undo extra decoder_attn_mask change

* Fix import

* pegasus tokenizer can ignore src_lang kwargs

* unused kwarg test cov

* boom boom

* add todo for pegasus issue

* cover one word translation edge case

* Cleanup

* doc
==

examples/seq2seq/README.md
examples/seq2seq/distillation.py
examples/seq2seq/finetune.py
examples/seq2seq/run_eval.py
examples/seq2seq/test_seq2seq_examples.py
examples/seq2seq/utils.py
src/transformers/modeling_bart.py
src/transformers/modeling_t5.py
src/transformers/tokenization_bart.py
src/transformers/tokenization_marian.py
src/transformers/tokenization_mbart.py
src/transformers/tokenization_pegasus.py
src/transformers/tokenization_t5.py
tests/test_modeling_bart.py
tests/test_tokenization_bart.py
tests/test_tokenization_common.py
tests/test_tokenization_mbart.py
tests/test_tokenization_pegasus.py
tests/test_tokenization_roberta.py
tests/test_tokenization_t5.py
==================
cb276b41d;RafaelWO;2020-08-28 15:56:17 +0200;Transformer-XL: Improved tokenization with sacremoses (#6322)
* Improved tokenization with sacremoses

 * The TransfoXLTokenizer is now using sacremoses for tokenization
 * Added tokenization of comma-separated and floating point numbers.
 * Removed prepare_for_tokenization() from tokenization_transfo_xl.py because punctuation is handled by sacremoses
 * Added corresponding tests
 * Removed test comapring TransfoXLTokenizer and TransfoXLTokenizerFast
 * Added deprecation warning to TransfoXLTokenizerFast

* isort change

Co-authored-by: Teven <teven.lescao@gmail.com>
Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
==

src/transformers/tokenization_transfo_xl.py
tests/test_tokenization_fast.py
tests/test_tokenization_transfo_xl.py
==================
930153e7d;Ahmed Elnaggar;2020-08-28 06:12:28 +0200;Add ProtBert model card (#6764)

==

model_cards/Rostlab/prot_bert/README.md
==================
743d131d7;Stas Bekman;2020-08-27 20:38:09 -0700;[style] set the minimal required version for `black` (#6784)
`make style` with `black` < 20.8b1 is a no go (in case some other package forced a lower version) - so make it explicit to avoid confusion
==

setup.py
==================
fb78a90d6;Sam Shleifer;2020-08-27 22:19:46 -0400;PL: --adafactor option (#6776)

==

examples/lightning_base.py
examples/seq2seq/test_seq2seq_examples.py
==================
92ac2fa7d;Stas Bekman;2020-08-27 17:01:17 -0700;[transformers-cli] fix logger getter (#6777)

==

src/transformers/commands/serving.py
tests/test_cli.py
==================
42fddacd1;Lysandre;2020-08-27 18:31:51 +0200;Format

==

tests/test_optimization.py
==================
70fccc5cf;Stas Bekman;2020-08-27 09:25:16 -0700;new Makefile target: docs (#6510)
* [doc] multiple corrections to "Summary of the tasks"

* add a new "docs" target to validate docs and document it

* fix mixup
==

CONTRIBUTING.md
Makefile
==================
dbfe34f2f;Stas Bekman;2020-08-27 09:23:28 -0700;[test schedulers] adjust to test the first step's reading (#6429)
* [test schedulers] small improvement

* cleanup
==

tests/test_optimization.py
==================
e6b811f0a;Stas Bekman;2020-08-27 09:22:18 -0700;[testing] replace hardcoded paths to allow running tests from anywhere (#6523)
* [testing] replace hardcoded paths to allow running tests from anywhere

* fix the merge conflict
==

src/transformers/testing_utils.py
tests/test_tokenization_fast.py
tests/test_trainer.py
==================
9d1b4db2a;Sam Shleifer;2020-08-27 11:08:14 -0400;add nlp install (#6767)

==

.github/workflows/self-scheduled.yml
==================
c225e872e;Tom Grek;2020-08-27 06:04:50 -0700;Fix it to work with BART (#6756)

==

examples/question-answering/run_squad.py
==================
0d2c111a0;Lysandre;2020-08-27 14:56:47 +0200;Format

==

src/transformers/trainer_tf.py
==================
6f289dc97;Julien Plu;2020-08-27 14:45:34 +0200;Fix the TF Trainer gradient accumulation and the TF NER example (#6713)
* Align TF NER example over the PT one

* Fix Dataset call

* Fix gradient accumulation training

* Apply style

* Address Sylvain's comments

* Address Sylvain's comments

* Apply style
==

examples/token-classification/run_tf_ner.py
examples/token-classification/utils_ner.py
src/transformers/modeling_tf_utils.py
src/transformers/trainer_tf.py
==================
41aa2b4ef;Lysandre Debut;2020-08-27 11:16:50 +0200;Adafactor docs (#6765)

==

docs/source/main_classes/optimizer_schedules.rst
src/transformers/optimization.py
==================
971d1802d;Nikolai Yakovenko;2020-08-27 04:58:13 -0400;Add AdaFactor optimizer from fairseq (#6722)
* AdaFactor optimizer ported from fairseq. Tested for T5 finetuning and MLM -- reduced memory consumption compared to ADAM.

* update PR fixes, add basic test

* bug -- incorrect params in test

* bugfix -- import Adafactor into test

* bugfix -- removed accidental T5 include

* resetting T5 to master

* bugfix -- include Adafactor in __init__

* longer loop for adafactor test

* remove double error class declare

* lint

* black

* isort

* Update src/transformers/optimization.py

Co-authored-by: Sam Shleifer <sshleifer@gmail.com>

* single docstring

* Cleanup docstring

Co-authored-by: Nikolai Y <nikolai.yakovenko@point72.com>
Co-authored-by: Sam Shleifer <sshleifer@gmail.com>
==

src/transformers/__init__.py
src/transformers/optimization.py
tests/test_optimization.py
==================
4bd7be9a4;Sam Shleifer;2020-08-26 23:25:11 -0400;s2s distillation uses AutoModelForSeqToSeqLM (#6761)

==

examples/seq2seq/distillation.py
examples/seq2seq/test_seq2seq_examples.py
==================
05e7150a5;Ahmed Elnaggar;2020-08-27 02:19:19 +0200;create ProtBert-BFD model card. (#6724)

==

model_cards/Rostlab/prot_bert_bfd/README.md
==================
61518e2df;Sam Shleifer;2020-08-26 18:59:20 -0400;[s2s] run_eval.py QOL improvements and cleanup(#6746)

==

examples/seq2seq/run_eval.py
examples/seq2seq/test_seq2seq_examples.py
==================
434936f34;Igli Manaj;2020-08-27 00:00:27 +0200;Model Card for Multilingual Passage Reranking BERT (#6755)

==

model_cards/amberoad/bert-multilingual-passage-reranking-msmarco/README.md
==================
10a34501f;Joe Davison;2020-08-26 17:51:10 -0400;add __init__.py to utils (#6754)

==

src/transformers/utils/__init__.py
==================
61b9ed807;Ali Safaya;2020-08-27 00:27:56 +0300;Model card for kuisailab/albert-large-arabic (#6730)
* Create README.md

* Update README.md
==

model_cards/kuisailab/albert-large-arabic/README.md
==================
8e0d51e4f;Ali Safaya;2020-08-27 00:27:42 +0300;Model card for kuisailab/albert-xlarge-arabic (#6731)
* Create README.md

* Update README.md
==

model_cards/kuisailab/albert-xlarge-arabic/README.md
==================
70c96a10e;Ali Safaya;2020-08-27 00:27:34 +0300;Model card for kuisailab/albert-base-arabic (#6729)
* Create README.md

* Update README.md
==

model_cards/kuisailab/albert-base-arabic/README.md
==================
cc4ba79f6;Sagor Sarker;2020-08-27 03:26:32 +0600;added model card for codeswitch-spaeng-sentiment-analysis-lince (#6727)
* added model card for codeswitch-spaeng-sentiment-analysis-lince model also update other model card

* fixed typo

* fixed typo

* fixed typo

* fixed typo

* fixed typo

* fixed typo

* fixed typo

* Update README.md
==

model_cards/sagorsarker/codeswitch-hineng-lid-lince/README.md
model_cards/sagorsarker/codeswitch-hineng-ner-lince/README.md
model_cards/sagorsarker/codeswitch-hineng-pos-lince/README.md
model_cards/sagorsarker/codeswitch-nepeng-lid-lince/README.md
model_cards/sagorsarker/codeswitch-spaeng-lid-lince/README.md
model_cards/sagorsarker/codeswitch-spaeng-ner-lince/README.md
model_cards/sagorsarker/codeswitch-spaeng-pos-lince/README.md
model_cards/sagorsarker/codeswitch-spaeng-sentiment-analysis-lince/README.md
==================
e10fb9cbe;Tanmay Thakur;2020-08-27 02:52:25 +0530;Create model card for lordtt13/COVID-SciBERT (#6718)

==

model_cards/lordtt13/COVID-SciBERT/README.md
==================
baeba53e8;Adam Montgomerie;2020-08-27 06:20:55 +0900;Adding model cards for 5 models (#6703)
* Added model cards for 4 models

Added model cards for:
- roberta-base-bulgarian
- roberta-base-bulgarian-pos
- roberta-small-bulgarian
- roberta-small-bulgarian-pos

* fixed link text

* Update README.md

* Create README.md

* removed trailing bracket

* Add language metadata

Co-authored-by: Julien Chaumond <chaumond@gmail.com>
==

model_cards/iarfmoose/bert-base-cased-qa-evaluator/README.md
model_cards/iarfmoose/roberta-base-bulgarian-pos/README.md
model_cards/iarfmoose/roberta-base-bulgarian/README.md
model_cards/iarfmoose/roberta-small-bulgarian-pos/README.md
model_cards/iarfmoose/roberta-small-bulgarian/README.md
==================
3242e4d94;Julien Chaumond;2020-08-26 23:16:06 +0200;[model_cards] Fix tiny typos

==

README.md
model_cards/joeddav/xlm-roberta-large-xnli/README.md
src/transformers/modeling_marian.py
tests/test_modeling_tf_common.py
==================
99407f9d1;Joe Davison;2020-08-26 16:05:59 -0400;add xlm-roberta-large-xnli model card (#6723)
* add xlm-roberta-large-xnli model card

* update pt example

* typo
==

model_cards/joeddav/xlm-roberta-large-xnli/README.md
==================
858b7d587;Patrick von Platen;2020-08-26 20:55:41 +0200;[TF Longformer] Improve Speed for TF Longformer (#6447)
* add tf graph compile tests

* fix conflict

* remove more tf transpose statements

* fix conflicts

* fix comment typos

* move function to class function

* fix black

* fix black

* make style
==

src/transformers/modeling_tf_bert.py
src/transformers/modeling_tf_electra.py
src/transformers/modeling_tf_longformer.py
tests/test_modeling_tf_common.py
tests/test_modeling_tf_longformer.py
==================
a75c64d80;Lysandre;2020-08-26 17:20:22 +0200;Black 20 release

==

examples/adversarial/utils_hans.py
examples/benchmarking/plot_csv_file.py
examples/bert-loses-patience/pabee/modeling_pabee_albert.py
examples/bert-loses-patience/pabee/modeling_pabee_bert.py
examples/bert-loses-patience/run_glue_with_pabee.py
examples/bertology/run_bertology.py
examples/contrib/mm-imdb/utils_mmimdb.py
examples/contrib/run_camembert.py
examples/contrib/run_openai_gpt.py
examples/contrib/run_swag.py
examples/deebert/run_glue_deebert.py
examples/deebert/src/modeling_highway_bert.py
examples/deebert/src/modeling_highway_roberta.py
examples/distillation/run_squad_w_distillation.py
examples/distillation/utils.py
examples/language-modeling/run_language_modeling.py
examples/lightning_base.py
examples/longform-qa/eli5_app.py
examples/longform-qa/eli5_utils.py
examples/movement-pruning/emmental/modeling_bert_masked.py
examples/movement-pruning/masked_run_glue.py
examples/movement-pruning/masked_run_squad.py
examples/multiple-choice/utils_multiple_choice.py
examples/question-answering/run_squad_trainer.py
examples/question-answering/run_tf_squad.py
examples/seq2seq/bertabs/configuration_bertabs.py
examples/seq2seq/bertabs/convert_bertabs_original_pytorch_checkpoint.py
examples/seq2seq/bertabs/modeling_bertabs.py
examples/seq2seq/bertabs/run_summarization.py
examples/seq2seq/bertabs/test_utils_summarization.py
examples/seq2seq/bertabs/utils_summarization.py
examples/seq2seq/callbacks.py
examples/seq2seq/distillation.py
examples/seq2seq/test_seq2seq_examples.py
examples/seq2seq/utils.py
examples/text-classification/run_pl_glue.py
examples/text-classification/run_xnli.py
examples/text-generation/pplm/run_pplm.py
examples/text-generation/pplm/run_pplm_discrim_train.py
examples/text-generation/run_generation.py
examples/token-classification/utils_ner.py
src/transformers/activations.py
src/transformers/benchmark/benchmark.py
src/transformers/benchmark/benchmark_args_tf.py
src/transformers/benchmark/benchmark_tf.py
src/transformers/benchmark/benchmark_utils.py
src/transformers/configuration_albert.py
src/transformers/configuration_auto.py
src/transformers/configuration_bart.py
src/transformers/configuration_bert.py
src/transformers/configuration_ctrl.py
src/transformers/configuration_distilbert.py
src/transformers/configuration_dpr.py
src/transformers/configuration_electra.py
src/transformers/configuration_encoder_decoder.py
src/transformers/configuration_flaubert.py
src/transformers/configuration_gpt2.py
src/transformers/configuration_longformer.py
src/transformers/configuration_mobilebert.py
src/transformers/configuration_openai.py
src/transformers/configuration_pegasus.py
src/transformers/configuration_reformer.py
src/transformers/configuration_retribert.py
src/transformers/configuration_roberta.py
src/transformers/configuration_t5.py
src/transformers/configuration_transfo_xl.py
src/transformers/configuration_utils.py
src/transformers/configuration_xlm.py
src/transformers/configuration_xlnet.py
src/transformers/convert_dialogpt_original_pytorch_checkpoint_to_pytorch.py
src/transformers/convert_graph_to_onnx.py
src/transformers/convert_marian_to_pytorch.py
src/transformers/convert_pytorch_checkpoint_to_tf2.py
src/transformers/convert_reformer_trax_checkpoint_to_pytorch.py
src/transformers/data/datasets/glue.py
src/transformers/data/datasets/language_modeling.py
src/transformers/data/datasets/squad.py
src/transformers/data/metrics/squad_metrics.py
src/transformers/data/processors/glue.py
src/transformers/data/processors/utils.py
src/transformers/generation_tf_utils.py
src/transformers/generation_utils.py
src/transformers/modelcard.py
src/transformers/modeling_albert.py
src/transformers/modeling_auto.py
src/transformers/modeling_bart.py
src/transformers/modeling_bert.py
src/transformers/modeling_camembert.py
src/transformers/modeling_ctrl.py
src/transformers/modeling_distilbert.py
src/transformers/modeling_dpr.py
src/transformers/modeling_electra.py
src/transformers/modeling_encoder_decoder.py
src/transformers/modeling_flaubert.py
src/transformers/modeling_gpt2.py
src/transformers/modeling_longformer.py
src/transformers/modeling_mmbt.py
src/transformers/modeling_mobilebert.py
src/transformers/modeling_openai.py
src/transformers/modeling_reformer.py
src/transformers/modeling_retribert.py
src/transformers/modeling_roberta.py
src/transformers/modeling_t5.py
src/transformers/modeling_tf_albert.py
src/transformers/modeling_tf_auto.py
src/transformers/modeling_tf_bert.py
src/transformers/modeling_tf_camembert.py
src/transformers/modeling_tf_ctrl.py
src/transformers/modeling_tf_distilbert.py
src/transformers/modeling_tf_electra.py
src/transformers/modeling_tf_gpt2.py
src/transformers/modeling_tf_longformer.py
src/transformers/modeling_tf_mobilebert.py
src/transformers/modeling_tf_openai.py
src/transformers/modeling_tf_pytorch_utils.py
src/transformers/modeling_tf_roberta.py
src/transformers/modeling_tf_t5.py
src/transformers/modeling_tf_transfo_xl.py
src/transformers/modeling_tf_transfo_xl_utilities.py
src/transformers/modeling_tf_utils.py
src/transformers/modeling_tf_xlm.py
src/transformers/modeling_tf_xlm_roberta.py
src/transformers/modeling_tf_xlnet.py
src/transformers/modeling_transfo_xl.py
src/transformers/modeling_transfo_xl_utilities.py
src/transformers/modeling_utils.py
src/transformers/modeling_xlm.py
src/transformers/modeling_xlm_roberta.py
src/transformers/modeling_xlnet.py
src/transformers/optimization_tf.py
src/transformers/pipelines.py
src/transformers/testing_utils.py
src/transformers/tokenization_auto.py
src/transformers/tokenization_bert.py
src/transformers/tokenization_ctrl.py
src/transformers/tokenization_dpr.py
src/transformers/tokenization_marian.py
src/transformers/tokenization_reformer.py
src/transformers/tokenization_t5.py
src/transformers/tokenization_transfo_xl.py
src/transformers/tokenization_utils_fast.py
src/transformers/tokenization_xlm.py
src/transformers/training_args.py
src/transformers/training_args_tf.py
templates/adding_a_new_example_script/utils_xxx.py
templates/adding_a_new_model/configuration_xxx.py
templates/adding_a_new_model/modeling_tf_xxx.py
templates/adding_a_new_model/modeling_xxx.py
tests/test_modeling_albert.py
tests/test_modeling_bart.py
tests/test_modeling_bert.py
tests/test_modeling_camembert.py
tests/test_modeling_common.py
tests/test_modeling_ctrl.py
tests/test_modeling_distilbert.py
tests/test_modeling_dpr.py
tests/test_modeling_electra.py
tests/test_modeling_encoder_decoder.py
tests/test_modeling_flaubert.py
tests/test_modeling_gpt2.py
tests/test_modeling_longformer.py
tests/test_modeling_mobilebert.py
tests/test_modeling_openai.py
tests/test_modeling_reformer.py
tests/test_modeling_roberta.py
tests/test_modeling_t5.py
tests/test_modeling_tf_camembert.py
tests/test_modeling_tf_common.py
tests/test_modeling_tf_ctrl.py
tests/test_modeling_tf_distilbert.py
tests/test_modeling_tf_electra.py
tests/test_modeling_tf_flaubert.py
tests/test_modeling_tf_gpt2.py
tests/test_modeling_tf_longformer.py
tests/test_modeling_tf_openai.py
tests/test_modeling_tf_roberta.py
tests/test_modeling_tf_t5.py
tests/test_modeling_tf_transfo_xl.py
tests/test_modeling_tf_xlm.py
tests/test_modeling_tf_xlnet.py
tests/test_modeling_transfo_xl.py
tests/test_modeling_xlm.py
tests/test_modeling_xlnet.py
tests/test_pipelines.py
tests/test_tokenization_common.py
tests/test_tokenization_fast.py
tests/test_tokenization_mbart.py
tests/test_tokenization_reformer.py
tests/test_tokenization_t5.py
tests/test_trainer.py
utils/link_tester.py
==================
e78c11033;Lysandre;2020-08-26 17:13:49 +0200;isort 5

==

src/transformers/configuration_transfo_xl.py
==================
02e8cd558;Julien Plu;2020-08-26 17:12:44 +0200;Fix optimizer (#6717)

==

src/transformers/optimization_tf.py
==================
77abd1e79;Lysandre Debut;2020-08-26 11:10:36 -0400;Centralize logging (#6434)
* Logging

* Style

* hf_logging > utils.logging

* Address @thomwolf's comments

* Update test

* Update src/transformers/benchmark/benchmark_utils.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Revert bad change

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

src/transformers/__init__.py
src/transformers/activations.py
src/transformers/benchmark/benchmark.py
src/transformers/benchmark/benchmark_args.py
src/transformers/benchmark/benchmark_args_tf.py
src/transformers/benchmark/benchmark_args_utils.py
src/transformers/benchmark/benchmark_tf.py
src/transformers/benchmark/benchmark_utils.py
src/transformers/commands/convert.py
src/transformers/commands/run.py
src/transformers/commands/serving.py
src/transformers/commands/train.py
src/transformers/configuration_auto.py
src/transformers/configuration_bart.py
src/transformers/configuration_bert.py
src/transformers/configuration_camembert.py
src/transformers/configuration_ctrl.py
src/transformers/configuration_distilbert.py
src/transformers/configuration_dpr.py
src/transformers/configuration_electra.py
src/transformers/configuration_encoder_decoder.py
src/transformers/configuration_flaubert.py
src/transformers/configuration_gpt2.py
src/transformers/configuration_longformer.py
src/transformers/configuration_mbart.py
src/transformers/configuration_mmbt.py
src/transformers/configuration_mobilebert.py
src/transformers/configuration_openai.py
src/transformers/configuration_pegasus.py
src/transformers/configuration_reformer.py
src/transformers/configuration_retribert.py
src/transformers/configuration_roberta.py
src/transformers/configuration_t5.py
src/transformers/configuration_transfo_xl.py
src/transformers/configuration_utils.py
src/transformers/configuration_xlm.py
src/transformers/configuration_xlm_roberta.py
src/transformers/configuration_xlnet.py
src/transformers/convert_albert_original_tf_checkpoint_to_pytorch.py
src/transformers/convert_bart_original_pytorch_checkpoint_to_pytorch.py
src/transformers/convert_bert_original_tf2_checkpoint_to_pytorch.py
src/transformers/convert_bert_original_tf_checkpoint_to_pytorch.py
src/transformers/convert_electra_original_tf_checkpoint_to_pytorch.py
src/transformers/convert_gpt2_original_tf_checkpoint_to_pytorch.py
src/transformers/convert_mobilebert_original_tf_checkpoint_to_pytorch.py
src/transformers/convert_openai_original_tf_checkpoint_to_pytorch.py
src/transformers/convert_pytorch_checkpoint_to_tf2.py
src/transformers/convert_reformer_trax_checkpoint_to_pytorch.py
src/transformers/convert_roberta_original_pytorch_checkpoint_to_pytorch.py
src/transformers/convert_t5_original_tf_checkpoint_to_pytorch.py
src/transformers/convert_transfo_xl_original_tf_checkpoint_to_pytorch.py
src/transformers/convert_xlm_original_pytorch_checkpoint_to_pytorch.py
src/transformers/convert_xlnet_original_tf_checkpoint_to_pytorch.py
src/transformers/data/datasets/glue.py
src/transformers/data/datasets/language_modeling.py
src/transformers/data/datasets/squad.py
src/transformers/data/metrics/squad_metrics.py
src/transformers/data/processors/glue.py
src/transformers/data/processors/squad.py
src/transformers/data/processors/utils.py
src/transformers/data/processors/xnli.py
src/transformers/file_utils.py
src/transformers/generation_tf_utils.py
src/transformers/generation_utils.py
src/transformers/modelcard.py
src/transformers/modeling_albert.py
src/transformers/modeling_auto.py
src/transformers/modeling_bart.py
src/transformers/modeling_bert.py
src/transformers/modeling_camembert.py
src/transformers/modeling_ctrl.py
src/transformers/modeling_distilbert.py
src/transformers/modeling_dpr.py
src/transformers/modeling_electra.py
src/transformers/modeling_encoder_decoder.py
src/transformers/modeling_flaubert.py
src/transformers/modeling_gpt2.py
src/transformers/modeling_longformer.py
src/transformers/modeling_mmbt.py
src/transformers/modeling_mobilebert.py
src/transformers/modeling_openai.py
src/transformers/modeling_reformer.py
src/transformers/modeling_retribert.py
src/transformers/modeling_roberta.py
src/transformers/modeling_t5.py
src/transformers/modeling_tf_albert.py
src/transformers/modeling_tf_auto.py
src/transformers/modeling_tf_bert.py
src/transformers/modeling_tf_camembert.py
src/transformers/modeling_tf_ctrl.py
src/transformers/modeling_tf_distilbert.py
src/transformers/modeling_tf_electra.py
src/transformers/modeling_tf_flaubert.py
src/transformers/modeling_tf_gpt2.py
src/transformers/modeling_tf_longformer.py
src/transformers/modeling_tf_mobilebert.py
src/transformers/modeling_tf_openai.py
src/transformers/modeling_tf_pytorch_utils.py
src/transformers/modeling_tf_roberta.py
src/transformers/modeling_tf_t5.py
src/transformers/modeling_tf_transfo_xl.py
src/transformers/modeling_tf_utils.py
src/transformers/modeling_tf_xlm.py
src/transformers/modeling_tf_xlm_roberta.py
src/transformers/modeling_tf_xlnet.py
src/transformers/modeling_transfo_xl.py
src/transformers/modeling_utils.py
src/transformers/modeling_xlm.py
src/transformers/modeling_xlm_roberta.py
src/transformers/modeling_xlnet.py
src/transformers/optimization.py
src/transformers/pipelines.py
src/transformers/tokenization_albert.py
src/transformers/tokenization_auto.py
src/transformers/tokenization_bart.py
src/transformers/tokenization_bert.py
src/transformers/tokenization_bert_japanese.py
src/transformers/tokenization_camembert.py
src/transformers/tokenization_ctrl.py
src/transformers/tokenization_distilbert.py
src/transformers/tokenization_dpr.py
src/transformers/tokenization_flaubert.py
src/transformers/tokenization_gpt2.py
src/transformers/tokenization_longformer.py
src/transformers/tokenization_mbart.py
src/transformers/tokenization_mobilebert.py
src/transformers/tokenization_openai.py
src/transformers/tokenization_reformer.py
src/transformers/tokenization_retribert.py
src/transformers/tokenization_roberta.py
src/transformers/tokenization_t5.py
src/transformers/tokenization_transfo_xl.py
src/transformers/tokenization_utils.py
src/transformers/tokenization_utils_base.py
src/transformers/tokenization_utils_fast.py
src/transformers/tokenization_xlm.py
src/transformers/tokenization_xlm_roberta.py
src/transformers/tokenization_xlnet.py
src/transformers/trainer.py
src/transformers/trainer_tf.py
src/transformers/training_args.py
src/transformers/training_args_tf.py
src/transformers/utils/logging.py
tests/test_logging.py
==================
461ae8681;Jay Yip;2020-08-26 17:15:35 +0800;Fix tf boolean mask in graph mode (#6741)

==

src/transformers/modeling_tf_utils.py
==================
925f34bbb;Patrick von Platen;2020-08-26 10:58:21 +0200;Add "tie_word_embeddings" config param (#6692)
* add tie_word_embeddings

* correct word embeddings in modeling utils

* make style

* make config param only relevant for torch

* make style

* correct typo

* delete deprecated arg in transo-xl
==

src/transformers/configuration_reformer.py
src/transformers/configuration_transfo_xl.py
src/transformers/configuration_utils.py
src/transformers/modeling_albert.py
src/transformers/modeling_mobilebert.py
src/transformers/modeling_reformer.py
src/transformers/modeling_transfo_xl.py
src/transformers/modeling_utils.py
==================
fa8ee8e85;Patrick von Platen;2020-08-26 10:51:56 +0200;fix torchscript docs (#6740)

==

docs/source/serialization.rst
==================
64c7c2bc1;Sylvain Gugger;2020-08-25 14:58:38 -0400;Install nlp for github actions test (#6728)

==

.github/workflows/self-push.yml
==================
624495706;Sam Shleifer;2020-08-25 14:56:08 -0400;T5Tokenizer adds EOS token if not already added (#5866)
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

src/transformers/tokenization_t5.py
tests/test_modeling_t5.py
tests/test_tokenization_t5.py
==================
e11d923bf;Sam Shleifer;2020-08-25 14:06:28 -0400;Fix pegasus-xsum integration test (#6726)

==

tests/test_modeling_pegasus.py
==================
7e6397a7d;Tomo Lazovich;2020-08-25 13:32:56 -0400;[squad] make examples and dataset accessible from SquadDataset object (#6710)
* [squad] make examples and dataset accessible from SquadDataset object

* [squad] add support for legacy cache files
==

src/transformers/data/datasets/squad.py
==================
ac9702c28;Funtowicz Morgan;2020-08-25 19:24:40 +0200;Fix ONNX test_quantize unittest (#6716)

==

.github/workflows/self-push.yml
.github/workflows/self-scheduled.yml
setup.py
src/transformers/convert_graph_to_onnx.py
==================
074340339;Zane Lim;2020-08-26 00:11:24 +0800;Create README.md (#6721)
add model card for singbert large
==

model_cards/zanelim/singbert-large-sg/README.md
==================
d17cce227;Patrick von Platen;2020-08-25 17:38:51 +0200;add missing keys (#6719)

==

src/transformers/modeling_albert.py
==================
a25c9fc8e;Arnav Sharma;2020-08-25 19:09:02 +0530;Selected typo fix (#6687)

==
==================
625318f52;Funtowicz Morgan;2020-08-25 14:12:54 +0200;tensor.nonzero() is deprecated in PyTorch 1.6 (#6715)
Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>
==

src/transformers/pipelines.py
==================
124c3d6ad;Sylvain Gugger;2020-08-25 07:47:09 -0400;Add tokenizer to Trainer (#6689)

==

src/transformers/trainer.py
==================
abc020219;Sylvain Gugger;2020-08-25 07:07:36 -0400;More tests to Trainer (#6699)
* More tests to Trainer

* Add warning in the doc
==

.circleci/config.yml
src/transformers/trainer.py
tests/test_trainer.py
==================
f5bad031b;Sylvain Gugger;2020-08-25 07:06:58 -0400;Use generators tqdm progressbars (#6696)

==

src/transformers/trainer.py
==================
a99d09c6f;Sam Shleifer;2020-08-25 06:26:29 -0400;add new line to make examples run (#6706)

==

.github/workflows/self-scheduled.yml
==================
4db2fa77d;Joel Hanson;2020-08-25 10:02:07 +0000;Allow tests in examples to use cuda or fp16,if they are available (#5512)
* Allow tests in examples to use cuda or fp16,if they are available

The tests in examples didn't use the cuda or fp16 even if they where available.
- The text classification example (`run_glue.py`) didn't use the fp16 even if it was available but
  the device was take based on the availablity(cuda/cpu).
- The language-modeling example (`run_language_modeling.py`) was having `--no_cuda` argument
  which made the test to work without cuda. This example is having issue when running with fp16
  thus it not enabled (got an assertion error for perplexity due to it higher value).
- The cuda and fp16 is not enabled for question-answering example (`run_squad.py`) as it is having a
  difference in the f1 score.
- The text-generation example (`run_generation.py`) will take the cuda or fp16 whenever it is available.

Resolves some of: #5057

* Unwanted import of is_apex_available was removed

* Made changes to test examples file to have the pass --fp16 only if cuda and apex is avaliable
- run_glue.py: Removed the check for cuda and fp16.
- run_generation.py: Removed the check for cuda and fp16 also removed unwanted flag creation.

* Incorrectly sorted imports fixed

* The model needs to be converted to half precision

* Formatted single line if condition statement to multiline

* The torch_device also needed to be checked before running the test on examples
- The tests in examples which uses cuda should also depend from the USE_CUDA flag,
  similarly to the rest of the test suite. Even if we decide to set USE_CUDA to
  True by default, setting USE_CUDA to False should result in the examples not using CUDA

* Format some of the code in test_examples file

* The improper import of is_apex_available was sorted

* Formatted the code to keep the style standards

* The comma at the end of list giving a flake8 issue was fixed

* Import sort was fixed

* Removed the clean_test_dir function as its not used right now
==

examples/test_examples.py
examples/text-generation/run_generation.py
==================
841f07156;Yohei Tamura;2020-08-25 17:57:08 +0900;Add typing.overload for convert_ids_tokens (#6637)
* add overload for type checker

* black
==

src/transformers/tokenization_utils.py
==================
0f16dd0ac;Quentin Lhoest;2020-08-25 09:57:28 +0200;Add DPR to models summary (#6690)
* add dpr to models summary

* minor

* minor

* Update docs/source/model_summary.rst

qa -> question answering

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update docs/source/model_summary.rst

qa -> question ansering (cont'd)

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

docs/source/model_summary.rst
==================
4fca874ea;Jay;2020-08-25 00:42:32 -0700;Remove hard-coded uses of float32 to fix mixed precision use (#6648)

==

src/transformers/modeling_tf_bert.py
src/transformers/modeling_tf_electra.py
==================
0344428f7;Sam Shleifer;2020-08-25 00:33:11 -0400;[s2s] round bleu, rouge to 4 digits (#6704)

==

examples/seq2seq/distillation.py
examples/seq2seq/finetune.py
examples/seq2seq/run_eval.py
examples/seq2seq/utils.py
==================
b6512d235;Zane Lim;2020-08-25 10:09:13 +0800;Add model card for singbert. (#6674)
* Add model card for singbert.

Adding a model card for singbert- bert for singlish and manglish.

* Update README.md

Add additional tags and model name.

* Update README.md

Fix tag for malay.

* Update model_cards/zanelim/singbert/README.md

Fix language

Co-authored-by: Kevin Canwen Xu <canwenxu@126.com>

* Add examples and custom widget input.

Add examples and custom widget input.

Co-authored-by: Kevin Canwen Xu <canwenxu@126.com>
==

model_cards/zanelim/singbert/README.md
==================
d20cbb886;Sylvain Gugger;2020-08-24 21:04:08 -0400;Fix hyperparameter_search doc (#6695)

==

src/transformers/trainer.py
==================
0ebc9699f;Sam Shleifer;2020-08-24 15:54:57 -0400;[fixdoc] Add import to pegasus usage doc (#6698)

==

docs/source/model_doc/pegasus.rst
==================
6b4c61766;Sylvain Gugger;2020-08-24 13:20:03 -0400;Move unused args to kwargs (#6694)

==

src/transformers/trainer.py
==================
912a21ec7;Stas Bekman;2020-08-24 09:42:34 -0700;remove BartForConditionalGeneration.generate (#6659)
As suggested here: https://github.com/huggingface/transformers/issues/6651#issuecomment-678594233
this removes generic `generate` doc with examples not-relevant to bart.
==

docs/source/model_doc/bart.rst
==================
a8d6716ec;Stas Bekman;2020-08-24 09:30:38 -0700;Create PULL_REQUEST_TEMPLATE.md (#6660)
* Create PULL_REQUEST_TEMPLATE.md

Proposing to copy this neat feature from pytorch. This is a small template that let's a PR submitter tell which issue that PR closes.

* Update .github/PULL_REQUEST_TEMPLATE.md

Co-authored-by: Kevin Canwen Xu <canwenxu@126.com>

Co-authored-by: Kevin Canwen Xu <canwenxu@126.com>
==

.github/PULL_REQUEST_TEMPLATE.md
==================
8f98faf93;Sylvain Gugger;2020-08-24 12:15:00 -0400;Lat fix for Ray HP search (#6691)

==

src/transformers/trainer.py
==================
3a7fdd3f5;Sylvain Gugger;2020-08-24 11:48:45 -0400;Add hyperparameter search to Trainer (#6576)
* Add optuna hyperparameter search to Trainer

* @julien-c suggestions

Co-authored-by: Julien Chaumond <chaumond@gmail.com>

* Make compute_objective an arg function

* Formatting

* Rework to make it easier to add ray

* Formatting

* Initial support for Ray

* Formatting

* Polish and finalize

* Add trial id to checkpoint with Ray

* Smaller default

* Use GPU in ray if available

* Formatting

* Fix test

* Update install instruction

Co-authored-by: Richard Liaw <rliaw@berkeley.edu>

* Address review comments

* Formatting post-merge

Co-authored-by: Julien Chaumond <chaumond@gmail.com>
Co-authored-by: Richard Liaw <rliaw@berkeley.edu>
==

src/transformers/__init__.py
src/transformers/integrations.py
src/transformers/trainer.py
src/transformers/trainer_utils.py
src/transformers/training_args.py
==================
dd522da00;vblagoje;2020-08-24 11:30:06 -0400;Fix PL token classification examples (#6682)

==

examples/token-classification/run.sh
examples/token-classification/run_pl.sh
examples/token-classification/run_pl_ner.py
==================
a57377790;Sylvain Gugger;2020-08-24 11:03:01 -0400;Update repo to isort v5 (#6686)
* Run new isort

* More changes

* Update CI, CONTRIBUTING and benchmarks
==

.circleci/config.yml
CONTRIBUTING.md
Makefile
examples/adversarial/utils_hans.py
examples/multiple-choice/utils_multiple_choice.py
examples/seq2seq/bertabs/run_summarization.py
examples/seq2seq/distillation.py
examples/seq2seq/finetune.py
examples/seq2seq/run_eval.py
examples/test_examples.py
examples/token-classification/utils_ner.py
setup.cfg
setup.py
src/transformers/__init__.py
src/transformers/benchmark/benchmark.py
src/transformers/benchmark/benchmark_tf.py
src/transformers/commands/serving.py
src/transformers/commands/user.py
src/transformers/convert_graph_to_onnx.py
src/transformers/convert_pytorch_checkpoint_to_tf2.py
src/transformers/data/datasets/glue.py
src/transformers/data/datasets/language_modeling.py
src/transformers/data/datasets/squad.py
src/transformers/data/metrics/__init__.py
src/transformers/data/test_generation_utils.py
src/transformers/file_utils.py
src/transformers/hf_api.py
src/transformers/modeling_electra.py
src/transformers/modeling_gpt2.py
src/transformers/modeling_mobilebert.py
src/transformers/modeling_openai.py
src/transformers/modeling_t5.py
src/transformers/modeling_tf_pytorch_utils.py
src/transformers/pipelines.py
src/transformers/tokenization_transfo_xl.py
src/transformers/tokenization_utils_base.py
templates/adding_a_new_model/modeling_xxx.py
templates/adding_a_new_model/tests/test_modeling_tf_xxx.py
templates/adding_a_new_model/tests/test_modeling_xxx.py
tests/test_activations.py
tests/test_benchmark.py
tests/test_benchmark_tf.py
tests/test_hf_api.py
tests/test_modeling_albert.py
tests/test_modeling_auto.py
tests/test_modeling_bart.py
tests/test_modeling_camembert.py
tests/test_modeling_common.py
tests/test_modeling_ctrl.py
tests/test_modeling_distilbert.py
tests/test_modeling_electra.py
tests/test_modeling_encoder_decoder.py
tests/test_modeling_flaubert.py
tests/test_modeling_gpt2.py
tests/test_modeling_longformer.py
tests/test_modeling_marian.py
tests/test_modeling_mbart.py
tests/test_modeling_mobilebert.py
tests/test_modeling_openai.py
tests/test_modeling_reformer.py
tests/test_modeling_roberta.py
tests/test_modeling_t5.py
tests/test_modeling_tf_albert.py
tests/test_modeling_tf_auto.py
tests/test_modeling_tf_bert.py
tests/test_modeling_tf_camembert.py
tests/test_modeling_tf_common.py
tests/test_modeling_tf_ctrl.py
tests/test_modeling_tf_distilbert.py
tests/test_modeling_tf_electra.py
tests/test_modeling_tf_flaubert.py
tests/test_modeling_tf_gpt2.py
tests/test_modeling_tf_longformer.py
tests/test_modeling_tf_mobilebert.py
tests/test_modeling_tf_openai.py
tests/test_modeling_tf_roberta.py
tests/test_modeling_tf_t5.py
tests/test_modeling_tf_transfo_xl.py
tests/test_modeling_tf_xlm.py
tests/test_modeling_tf_xlm_roberta.py
tests/test_modeling_tf_xlnet.py
tests/test_modeling_transfo_xl.py
tests/test_modeling_xlm.py
tests/test_modeling_xlm_roberta.py
tests/test_modeling_xlnet.py
tests/test_optimization_tf.py
tests/test_tokenization_common.py
tests/test_tokenization_reformer.py
tests/test_tokenization_transfo_xl.py
==================
d329c9b05;Teven;2020-08-24 15:31:44 +0200;Fixed DataCollatorForLanguageModeling not accepting lists of lists (#6685)
* Fixed DataCollatorForLanguageModeling + PermutationLanguageModeling not accepting lists of lists

* Update data_collator.py

* black was grumpy
==

src/transformers/data/data_collator.py
==================
0a850d210;sgugger;2020-08-24 09:23:06 -0400;Missing commit

==

src/transformers/trainer.py
==================
b30879fe0;Sylvain Gugger;2020-08-24 09:22:03 -0400;Don't reset the dataset type + plug for rm unused columns (#6683)
* Don't reset the type of the dataset

* Formatting

* Update trainer.py

Co-authored-by: Teven <teven.lescao@gmail.com>
==

src/transformers/trainer.py
src/transformers/training_args.py
==================
1a779ad7e;Jared T Nielsen;2020-08-24 04:27:58 -0700;Specify config filename (#6626)

==

src/transformers/hf_argparser.py
==================
a622705ef;Sagor Sarker;2020-08-24 15:08:32 +0600;added multiple model_cards for below models (#6666)
* Create README.md

* Update README.md

* Create README.md

* Update README.md

* added multiple codeswitch model
==

model_cards/sagorsarker/codeswitch-hineng-lid-lince/README.md
model_cards/sagorsarker/codeswitch-hineng-ner-lince/README.md
model_cards/sagorsarker/codeswitch-hineng-pos-lince/README.md
model_cards/sagorsarker/codeswitch-nepeng-lid-lince/README.md
model_cards/sagorsarker/codeswitch-spaeng-lid-lince/README.md
model_cards/sagorsarker/codeswitch-spaeng-ner-lince/README.md
model_cards/sagorsarker/codeswitch-spaeng-pos-lince/README.md
==================
16e38940b;Patrick von Platen;2020-08-23 17:02:22 +0200;Add Roberta2Roberta shared

==

model_cards/patrickvonplaten/roberta2roberta-share-cnn_dailymail-fp16/README.md
==================
f230a6409;Sam Shleifer;2020-08-23 10:03:41 -0400;new paper bibtex (#6656)

==

README.md
==================
f235ee216;Patrick von Platen;2020-08-23 10:01:58 +0200;Add Roberta2Roberta model card

==

model_cards/patrickvonplaten/roberta2roberta-cnn_dailymail-fp16/README.md
==================
068df740b;Sagor Sarker;2020-08-22 22:13:21 +0600;added model_card for model codeswitch-hineng-lid-lince and codeswitch-spaeng-lid-lince (#6663)
* Create README.md

* Update README.md

* Create README.md

* Update README.md
==

model_cards/sagorsarker/codeswitch-hineng-lid-lince/README.md
model_cards/sagorsarker/codeswitch-spaeng-lid-lince/README.md
==================
97bb2497a;Patrick von Platen;2020-08-22 13:44:20 +0200;Correct bug in bert2bert-cnn_dailymail
Model was trained with the wrong tokenizer. Retrained with correct tokenizer - thanks for spotting @lhoestq !
==

model_cards/patrickvonplaten/bert2bert-cnn_dailymail-fp16/README.md
==================
0f94151dc;Manuel Romero;2020-08-21 20:18:15 +0200;Add model card for electricidad-base-generator (#6650)
I works like a charm!
Look at the output of the example code!
==

model_cards/mrm8488/electricidad-base-generator/README.md
==================
cbda72932;Suraj Patil;2020-08-21 23:12:59 +0530;[Doc model summary] add MBart model summary (#6649)

==

docs/source/model_summary.rst
==================
9e8c494da;Patrick von Platen;2020-08-21 18:11:18 +0200;Add T5-11B disclaimer
@julien-c
==

model_cards/t5-11b-README.md
==================
a4db4e303;Patrick von Platen;2020-08-21 16:22:10 +0200;[Docs model summaries] Add pegasus to docs (#6640)
* add pegasus to docs

* Update docs/source/model_summary.rst
==

docs/source/model_summary.rst
==================
d0e42a7be;Suraj Patil;2020-08-21 17:22:54 +0530;CamembertForCausalLM (#6577)
* added CamembertForCausalLM

* add in __init__ and auto model

* style

* doc
==

docs/source/model_doc/camembert.rst
src/transformers/__init__.py
src/transformers/modeling_auto.py
src/transformers/modeling_camembert.py
==================
bdf7e5de9;josephrocca;2020-08-21 19:07:32 +1000;Remove accidental comment (#6629)

==

src/transformers/generation_tf_utils.py
==================
efc746055;Manuel Romero;2020-08-21 11:04:29 +0200;model card for Spanish electra base (#6633)

==

model_cards/mrm8488/electricidad-base-discriminator/README.md
==================
b105f2c6b;Morgan Funtowicz;2020-08-21 10:37:09 +0200;Update ONNX doc to match the removal of --optimize argument.
Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

==

docs/source/serialization.rst
==================
e5f452275;Sylvain Gugger;2020-08-20 16:29:14 -0400;Trainer automatically drops unused columns in nlp datasets (#6449)
* Add a classmethod to easily build a Trainer from nlp dataset and metric

* Fix docstrings

* Split train/eval

* Formatting

* Log dropped columns + docs

* Authorize callable activations

* Poc for auto activation

* Be framework-agnostic

* Formatting

* Remove class method

* Remove unnecessary code
==

src/transformers/__init__.py
src/transformers/file_utils.py
src/transformers/trainer.py
==================
5bf4465e6;Sam Shleifer;2020-08-20 15:34:43 -0400;Regression test for pegasus bugfix (#6606)

==

src/transformers/configuration_pegasus.py
src/transformers/convert_pegasus_tf_to_pytorch.py
src/transformers/modeling_pegasus.py
tests/test_modeling_pegasus.py
==================
86c07e634;sgugger;2020-08-20 14:23:09 -0400;One last threshold to raise

==

tests/test_modeling_tf_common.py
==================
e8af90c05;Sylvain Gugger;2020-08-20 13:59:40 -0400;Move threshold up for flaky test with Electra (#6622)
* Move threshold up for flaky test with Electra

* Update above as well
==

tests/test_modeling_tf_common.py
==================
953958372;Ivan Dolgov;2020-08-20 20:34:23 +0300;XLNet Bug when training with apex 16-bit precision (#6567)
* xlnet fp16 bug fix

* comment cast added

* Update modeling_xlnet.py

Co-authored-by: Kevin Canwen Xu <canwenxu@126.com>
==

src/transformers/modeling_xlnet.py
==================
505f2d749;Patrick von Platen;2020-08-20 19:23:47 +0200;[Tests] fix attention masks in Tests (#6621)
* fix distilbert

* fix typo
==

tests/test_modeling_albert.py
tests/test_modeling_bert.py
tests/test_modeling_common.py
tests/test_modeling_ctrl.py
tests/test_modeling_distilbert.py
tests/test_modeling_dpr.py
tests/test_modeling_electra.py
tests/test_modeling_flaubert.py
tests/test_modeling_gpt2.py
tests/test_modeling_longformer.py
tests/test_modeling_mobilebert.py
tests/test_modeling_reformer.py
tests/test_modeling_roberta.py
tests/test_modeling_xlm.py
tests/test_modeling_xlnet.py
==================
c9454507c;Denisa Roberts;2020-08-20 12:58:44 -0400;Add tests for Reformer tokenizer (#6485)

==

tests/test_tokenization_reformer.py
==================
f9d280a95;Joe Davison;2020-08-20 12:11:36 -0400;TFTrainer dataset doc & fix evaluation bug (#6618)
* TFTrainer dataset doc & fix evaluation bug

discussed in #6551

* add docstring to test/eval datasets
==

src/transformers/trainer_tf.py
==================
573bdb0a5;Sylvain Gugger;2020-08-20 11:13:50 -0400;Add tests to Trainer (#6605)
* Add tests to Trainer

* Test if removing long breaks everything

* Remove ugly hack

* Fix distributed test

* Use float for number of epochs
==

src/transformers/data/data_collator.py
src/transformers/trainer.py
src/transformers/training_args.py
tests/test_data_collator.py
tests/test_trainer.py
tests/test_trainer_distributed.py
==================
039d8d65f;Joe Davison;2020-08-20 10:32:51 -0400;add intro to nlp lib & dataset links to custom datasets tutorial (#6583)
* add intro to nlp lib + links

* unique links...
==

docs/source/custom_datasets.rst
==================
b3e54698d;sgugger;2020-08-20 08:34:02 -0400;Fix CI

==

src/transformers/trainer.py
==================
33bf42649;Prajjwal Bhargava;2020-08-20 17:53:35 +0530;removed redundant arg in prepare_inputs (#6614)
* removed redundant arg in prepare_inputs

* made same change in prediction_loop
==

src/transformers/trainer.py
==================
cabfdfafc;Romain Rigaux;2020-08-20 02:35:06 -0700;Docs copy button misses ... prefixed code (#6518)
Tested in a local build of the docs.

e.g. Just above https://huggingface.co/transformers/task_summary.html#causal-language-modeling

Copy will copy the full code, e.g.

for token in top_5_tokens:
     print(sequence.replace(tokenizer.mask_token, tokenizer.decode([token])))

Instead of currently only:

for token in top_5_tokens:


>>> for token in top_5_tokens:
...     print(sequence.replace(tokenizer.mask_token, tokenizer.decode([token])))
Distilled models are smaller than the models they mimic. Using them instead of the large versions would help reduce our carbon footprint.
Distilled models are smaller than the models they mimic. Using them instead of the large versions would help increase our carbon footprint.
Distilled models are smaller than the models they mimic. Using them instead of the large versions would help decrease our carbon footprint.
Distilled models are smaller than the models they mimic. Using them instead of the large versions would help offset our carbon footprint.
Distilled models are smaller than the models they mimic. Using them instead of the large versions would help improve our carbon footprint.

Docs for the option fix:
https://sphinx-copybutton.readthedocs.io/en/latest/
==

docs/source/conf.py
==================
61b5ee11e;Stas Bekman;2020-08-20 02:24:25 -0700;lighter 'make test' (#6512)

==

CONTRIBUTING.md
==================
3c3c46f56;Siddharth Jain;2020-08-20 13:47:16 +0530;Typo fix in 04-onnx-export (#6595)

==

notebooks/04-onnx-export.ipynb
==================
93c5c9a52;Oren Amsalem;2020-08-20 07:33:36 +0300;[cleanup] remove confusing newline (#6603)

==

src/transformers/generation_utils.py
==================
18ca0e914;Sylvain Gugger;2020-08-19 13:04:33 -0400;Fix #6575 (#6596)

==

docs/source/preprocessing.rst
src/transformers/tokenization_utils_base.py
==================
7581884de;Suraj Patil;2020-08-19 20:07:48 +0530;[BartTokenizerFast] add prepare_seq2seq_batch (#6543)

==

src/transformers/tokenization_bart.py
tests/test_modeling_bart.py
==================
8bcceacef;Patrick von Platen;2020-08-19 16:18:51 +0200;fix model outputs test (#6593)

==

tests/test_modeling_common.py
==================
9a86321b1;Sam Shleifer;2020-08-19 09:37:45 -0400;tf generation utils: remove unused kwargs (#6591)

==

src/transformers/generation_tf_utils.py
==================
2a7402cbd;Pradhy729;2020-08-19 05:31:10 -0700;Feed forward chunking others (#6365)
* Feed forward chunking for Distilbert & Albert

* Added ff chunking for many other models

* Change model signature

* Added chunking for XLM

* Cleaned up by removing some variables.

* remove test_chunking flag

Co-authored-by: patrickvonplaten <patrick.v.platen@gmail.com>
==

src/transformers/configuration_reformer.py
src/transformers/configuration_utils.py
src/transformers/modeling_albert.py
src/transformers/modeling_bert.py
src/transformers/modeling_distilbert.py
src/transformers/modeling_longformer.py
src/transformers/modeling_reformer.py
src/transformers/modeling_utils.py
src/transformers/modeling_xlm.py
src/transformers/modeling_xlnet.py
tests/test_modeling_bert.py
tests/test_modeling_common.py
tests/test_modeling_reformer.py
==================
fe0b85e77;Patrick von Platen;2020-08-19 14:23:45 +0200;[EncoderDecoder] Add functionality to tie encoder decoder weights (#6538)
* start adding tie encoder to decoder functionality

* finish model tying

* make style

* Apply suggestions from code review

* fix t5 list including cross attention

* apply sams suggestions

* Update src/transformers/modeling_encoder_decoder.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* add max depth break point

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

src/transformers/configuration_encoder_decoder.py
src/transformers/configuration_utils.py
src/transformers/modeling_encoder_decoder.py
src/transformers/modeling_t5.py
src/transformers/modeling_utils.py
tests/test_modeling_encoder_decoder.py
tests/test_modeling_t5.py
==================
ab42d7485;Sam Shleifer;2020-08-18 21:28:10 -0400;Fix bart base test (#6587)

==

tests/test_modeling_bart.py
tests/test_modeling_marian.py
==================
1529bf968;Sam Shleifer;2020-08-18 19:15:50 -0400;add BartConfig.force_bos_token_to_be_generated (#6526)
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

src/transformers/configuration_bart.py
src/transformers/modeling_bart.py
src/transformers/modeling_marian.py
tests/test_modeling_bart.py
==================
974bb4af2;Patrick von Platen;2020-08-18 19:28:17 +0200;[Model card] Bert2GPT2 EncoderDecoder model (#6569)
* Bert2GPT2 EncoderDecoder model

* Update README.md
==

model_cards/patrickvonplaten/bert2gpt2-cnn_dailymail-fp16/README.md
==================
6f972e142;Suraj Patil;2020-08-18 22:40:47 +0530;update xnli-mt url (#6580)

==

examples/text-classification/README.md
==================
fb6844aff;Suraj Patil;2020-08-18 22:17:47 +0530;[Pegasus Doc] minor typo (#6579)
Minor typo correction
@sshleifer
==

docs/source/model_doc/pegasus.rst
==================
aaab9ab18;Manuel Romero;2020-08-18 18:43:20 +0200;Create README.md (#6556)

==

model_cards/mrm8488/t5-base-finetuned-break_data/README.md
==================
1dfce0f08;Manuel Romero;2020-08-18 18:42:14 +0200;Create README.md (#6557)

==

model_cards/mrm8488/t5-base-finetuned-break_data-question-retrieval/README.md
==================
7516bcf27;Romain Rigaux;2020-08-18 07:23:25 -0700;[docs] Fix number of 'ug' occurrences in tokenizer_summary (#6574)

==

docs/source/tokenizer_summary.rst
==================
5a5af22ed;Romain Rigaux;2020-08-18 07:22:43 -0700;[docs] Fix wrong newline in the middle of a paragraph (#6573)

==

docs/source/preprocessing.rst
==================
7659a8eb3;Stas Bekman;2020-08-18 07:21:13 -0700;fix incorrect codecov reports (#6553)
As discussed at https://github.com/huggingface/transformers/issues/6317 codecov currently sends an invalid report when it fails to find a code coverage report for the base it checks against, so this gets fixed by:

-  require_base: yes        # don't report if there is no base coverage report

let's add this for clarity, this supposedly is already the default.

-  require_head: yes        # don't report if there is no head coverage report 

and perhaps no point reporting on doc changes as they don't make any difference and it just generates noise:

-  require_changes: true    # only comment if there was change in coverage
==

codecov.yml
==================
cfa26d2b4;Stefan Schweter;2020-08-18 14:38:54 +0200;github: add @stefan-it to bug-report template for all token-classification related bugs (#6489)

==

.github/ISSUE_TEMPLATE/bug-report.md
==================
1fdf372f8;Philip May;2020-08-18 14:21:52 +0200;Small typo fixes for model card: electra-base-german-uncased (#6555)
* Update README.md

* Update model_cards/german-nlp-group/electra-base-german-uncased/README.md

Co-authored-by: Julien Chaumond <chaumond@gmail.com>
==

model_cards/german-nlp-group/electra-base-german-uncased/README.md
==================
5a81195ea;Ali Modarressi;2020-08-18 16:39:39 +0430;Fixed label datatype for STS-B (#6492)
* fixed label datatype for sts-b

* naming update

* make style

* make style
==

src/transformers/data/processors/glue.py
==================
12d762419;Sam Shleifer;2020-08-17 23:55:42 -0400;[marian] converter supports models from new Tatoeba project (#6342)

==

docs/source/model_doc/marian.rst
src/transformers/convert_marian_to_pytorch.py
tests/test_modeling_marian.py
==================
fb7330b30;Jim Regan;2020-08-17 21:48:05 +0100;update with #s of sentences/tokens (#6546)

==

model_cards/jimregan/BERTreach/README.md
==================
63144701e;onepointconsulting;2020-08-17 21:24:10 +0100;Added first model card (#6530)
* Added first model card

* Add metadata

Co-authored-by: Julien Chaumond <chaumond@gmail.com>
==

model_cards/gilf/french-camembert-postag-model/README.md
==================
98ee80202;Ikram Ali;2020-08-18 01:04:29 +0500;[model_cards] Add model cards for Urduhack model (roberta-urdu-small) (#6536)
* [model_cards] roberta-urdu-small added.

* [model_cards] typo fixed.

* Tweak license format (yaml expects a simple string)

Co-authored-by: Ikram Ali <mrikram1989>
Co-authored-by: Julien Chaumond <chaumond@gmail.com>
==

model_cards/urduhack/roberta-urdu-small/README.md
==================
3a302904c;Jim Regan;2020-08-17 20:56:56 +0100;[model_cards] Add a new model for Irish (#6544)

==

model_cards/jimregan/BERTreach/README.md
==================
07971d8b1;Julien Chaumond;2020-08-17 21:33:32 +0200;[model_cards] Fix yaml for cedpsam/chatbot_fr

==

model_cards/cedpsam/chatbot_fr/README.md
==================
407da12ef;Suraj Patil;2020-08-17 23:27:19 +0530;[T5Tokenizer] add prepare_seq2seq_batch method (#6122)
* tests
==

src/transformers/tokenization_t5.py
tests/test_tokenization_t5.py
==================
c9564f534;Suraj Patil;2020-08-17 22:00:26 +0530;[Doc] add more MBart and other doc (#6490)
* add mbart example

* add Pegasus and MBart in readme

* typo

* add MBart in Pretrained models

* add pre-proc doc

* add DPR in readme

* fix indent

* doc fix
==

README.md
docs/source/index.rst
docs/source/model_doc/mbart.rst
docs/source/pretrained_models.rst
src/transformers/modeling_mbart.py
==================
f68c87310;Stas Bekman;2020-08-17 09:27:02 -0700;replace _ with __ rst links (#6541)

==

docs/source/task_summary.rst
==================
7ca6ab67f;sgugger;2020-08-17 12:20:40 -0400;Fix CI

==

src/transformers/tokenization_bart.py
==================
b732e7e11;Stas Bekman;2020-08-17 08:49:16 -0700;[doc] multiple corrections to "Summary of the tasks" (#6509)
* [doc] multiple corrections to "Summary of the tasks"

* fix indentation

* correction

* fix links, add links to examples/seq2seq/README.md instead of non-existing script
==

docs/source/task_summary.rst
==================
2a77813d5;Suraj Patil;2020-08-17 21:14:46 +0530;[BartTokenizer] add prepare s2s batch (#6212)
Co-authored-by: sgugger <sylvain.gugger@gmail.com>
==

src/transformers/tokenization_bart.py
tests/test_modeling_bart.py
==================
84d33317a;Stas Bekman;2020-08-17 08:07:58 -0700;[doc] make the text more readable, fix some typos, add some disambiguation (#6508)
* [doc] make the text more readable, fix some typos, add some disambiguation

* Update docs/source/glossary.rst

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

docs/source/glossary.rst
==================
d0c2389f4;Joe Davison;2020-08-17 09:15:34 -0400;add custom datasets tutorial (#6466)
* add custom datasets tutorial

* python -> bash code blocks

* Apply suggestions from code review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* minor review feedback changes

* add working native QA snippet

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

docs/source/_static/js/custom.js
docs/source/custom_datasets.rst
docs/source/index.rst
==================
d2da2cb23;Sam Shleifer;2020-08-17 09:06:35 -0400;allow spaces in bash args with "$@" (#6521)

==

examples/seq2seq/finetune_bart_tiny.sh
examples/seq2seq/finetune_pegasus_xsum.sh
examples/seq2seq/finetune_t5.sh
examples/seq2seq/run_distiller.sh
examples/seq2seq/train_distilbart_cnn.sh
examples/seq2seq/train_distilbart_xsum.sh
examples/seq2seq/train_mbart_cc25_enro.sh
==================
b41cc0b86;Funtowicz Morgan;2020-08-17 15:04:35 +0200;Fix flaky ONNX tests (#6531)

==

tests/test_onnx.py
==================
39c3b1d9d;Stas Bekman;2020-08-17 05:33:12 -0700;[sched] polynomial_decay_schedule use default power=1.0 (#6473)

==

src/transformers/optimization.py
==================
9dbe4094f;Stas Bekman;2020-08-17 05:12:19 -0700;[testing] a new TestCasePlus subclass + get_auto_remove_tmp_dir()  (#6494)
* [testing] switch to a new TestCasePlus + get_auto_remove_tmp_dir() for auto-removal of tmp dirs

* respect after=True for tempfile, simplify code

* comments

* comment fix

* put `before` last in args, so can make debug even faster
==

examples/bert-loses-patience/test_run_glue_with_pabee.py
examples/test_examples.py
src/transformers/testing_utils.py
==================
36010cb1e;Patrick von Platen;2020-08-17 12:24:43 +0200;fix pegasus doc (#6533)

==

docs/source/model_doc/pegasus.rst
==================
37709b590;Kevin Canwen Xu;2020-08-17 17:13:58 +0800;Remove deprecated assertEquals (#6532)
`assertEquals` is deprecated: https://stackoverflow.com/questions/930995/assertequals-vs-assertequal-in-python/931011
This PR replaces these deprecated methods.
==

tests/test_tokenization_fast.py
==================
49d8076fa;Stas Bekman;2020-08-17 01:04:53 -0700;[doc] Summary of the models fixes (#6511)
* [doc] Summary of the models fixes

* correction
==

docs/source/model_summary.rst
==================
72911c893;Cahya Wirawan;2020-08-17 09:42:25 +0200;Create model cards for indonesian models (#6522)
* added model cards for indonesian gpt2-small, bert-base and roberta-base models

* removed bibtex entries
==

model_cards/cahya/bert-base-indonesian-522M/README.md
model_cards/cahya/gpt2-small-indonesian-522M/README.md
model_cards/cahya/roberta-base-indonesian-522M/README.md
==================
48c6c6139;Masatoshi Suzuki;2020-08-17 13:00:23 +0900;Support additional dictionaries for BERT Japanese tokenizers (#6515)
* Update BERT Japanese tokenizers

* Update CircleCI config to download unidic

* Specify to use the latest dictionary packages
==

.circleci/config.yml
setup.py
src/transformers/tokenization_bert_japanese.py
tests/test_tokenization_bert_japanese.py
==================
423eb5b1d;Stas Bekman;2020-08-16 20:11:40 -0700;[doc] fix invalid env vars (#6504)
- remove invalid `ENV_` prefix.
- add a few ':' while at it
==

docs/source/installation.md
==================
3c72f5584;Philip May;2020-08-17 05:02:32 +0200;Add Model Card for electra-base-german-uncased (#6496)
* Add Model Card for electra-base-german-uncased

* Update README.md

Co-authored-by: Kevin Canwen Xu <canwenxu@126.com>
==

model_cards/german-nlp-group/electra-base-german-uncased/README.md
==================
df15c7c22;Stas Bekman;2020-08-16 19:57:36 -0700;typos (#6505)

==

docs/source/philosophy.rst
==================
6d38ab1cc;fabiocapsouza;2020-08-16 23:49:49 -0300;Update bert-base-portuguese-cased and bert-large-portuguese-cased model cards (#6527)
Co-authored-by: Fabio Souza <fabiosouza@neuralmind.ai>
==

model_cards/neuralmind/bert-base-portuguese-cased/README.md
model_cards/neuralmind/bert-large-portuguese-cased/README.md
==================
84c265ffc;Sam Shleifer;2020-08-16 22:49:41 -0400;[lightning_base] fix s2s logging, only make train_loader once (#6404)

==

examples/lightning_base.py
examples/seq2seq/distillation.py
examples/seq2seq/finetune.py
examples/text-classification/run_pl_glue.py
examples/token-classification/run_pl_ner.py
examples/token-classification/test_ner_examples.py
==================
72add6c98;Sam Shleifer;2020-08-16 20:31:22 -0400;[s2s] docs, document desired filenames nicely (#6525)

==

examples/seq2seq/README.md
==================
206018112;Kyle Piira;2020-08-16 13:36:38 -0400;Fixes paths with spaces in seq2seq example (#6493)

==

examples/seq2seq/finetune.sh
==================
fe61c05b8;Kevin Canwen Xu;2020-08-16 16:30:16 +0800;Add examples/bert-loses-patience who can help (#6499)

==

.github/ISSUE_TEMPLATE/bug-report.md
==================
24107c2c8;Jin Young (Daniel) Sohn;2020-08-14 09:47:37 -0700;Fix TPU Convergence bug introduced by PR#6151 (#6488)
Currently with the bug introduced we're taking two optimizer steps per
batch: one global one, where `xm.optimizer_step` injects a CRS between
all cores in training, and one without. This has been affecting training
accuracy (for example, XLNet GLUE on MNLI is not converging, etc.).
==

src/transformers/trainer.py
==================
895ed8f45;Sylvain Gugger;2020-08-14 09:46:39 -0400;Generation doc (#6470)
* Generation doc

* MBartForConditionalGeneration (#6441)

* add MBartForConditionalGeneration

* style

* rebase and fixes

* add mbart test in TEST_FILES_WITH_NO_COMMON_TESTS

* fix docs

* don't ignore mbart

* doc

* fix mbart fairseq link

* put mbart before bart

* apply doc suggestions

* Use hash to clean the test dirs (#6475)

* Use hash to clean the test dirs

* Use hash to clean the test dirs

* Use hash to clean the test dirs

* fix

* [EncoderDecoder] Add Cross Attention for GPT2 (#6415)

* add cross attention layers for gpt2

* make gpt2 cross attention work

* finish bert2gpt2

* add explicit comments

* remove attention mask since not yet supported

* revert attn mask in pipeline

* Update src/transformers/modeling_gpt2.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/modeling_encoder_decoder.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Sort unique_no_split_tokens to make it deterministic (#6461)

* change unique_no_split_tokens's type to set

* use sorted list instead of set

* style

* Import accuracy_score (#6480)

* Apply suggestions from code review

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* Address comments

* Styling

* Generation doc

* Apply suggestions from code review

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* Address comments

* Styling

Co-authored-by: Suraj Patil <surajp815@gmail.com>
Co-authored-by: Kevin Canwen Xu <canwenxu@126.com>
Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
Co-authored-by: Quentin Lhoest <42851186+lhoestq@users.noreply.github.com>
Co-authored-by: gijswijnholds <gijswijnholds@gmail.com>
Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
==

docs/source/main_classes/model.rst
src/transformers/configuration_utils.py
src/transformers/generation_tf_utils.py
src/transformers/generation_utils.py
==================
b5ba758ba;gijswijnholds;2020-08-14 14:16:16 +0200;Import accuracy_score (#6480)

==

docs/source/training.rst
==================
9a8c168f5;Quentin Lhoest;2020-08-14 10:36:58 +0200;Sort unique_no_split_tokens to make it deterministic (#6461)
* change unique_no_split_tokens's type to set

* use sorted list instead of set

* style
==

src/transformers/tokenization_utils.py
==================
1d6e71e11;Patrick von Platen;2020-08-14 09:43:29 +0200;[EncoderDecoder] Add Cross Attention for GPT2 (#6415)
* add cross attention layers for gpt2

* make gpt2 cross attention work

* finish bert2gpt2

* add explicit comments

* remove attention mask since not yet supported

* revert attn mask in pipeline

* Update src/transformers/modeling_gpt2.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/modeling_encoder_decoder.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

src/transformers/generation_utils.py
src/transformers/modeling_encoder_decoder.py
src/transformers/modeling_gpt2.py
tests/test_modeling_encoder_decoder.py
tests/test_modeling_gpt2.py
==================
eb613b566;Kevin Canwen Xu;2020-08-14 15:34:39 +0800;Use hash to clean the test dirs (#6475)
* Use hash to clean the test dirs

* Use hash to clean the test dirs

* Use hash to clean the test dirs

* fix
==

examples/bert-loses-patience/test_run_glue_with_pabee.py
examples/test_examples.py
==================
680f1337c;Suraj Patil;2020-08-14 12:51:16 +0530;MBartForConditionalGeneration (#6441)
* add MBartForConditionalGeneration

* style

* rebase and fixes

* add mbart test in TEST_FILES_WITH_NO_COMMON_TESTS

* fix docs

* don't ignore mbart

* doc

* fix mbart fairseq link

* put mbart before bart

* apply doc suggestions
==

docs/source/index.rst
docs/source/model_doc/bart.rst
docs/source/model_doc/mbart.rst
src/transformers/__init__.py
src/transformers/configuration_auto.py
src/transformers/configuration_bart.py
src/transformers/configuration_mbart.py
src/transformers/modeling_auto.py
src/transformers/modeling_mbart.py
src/transformers/tokenization_auto.py
src/transformers/tokenization_bart.py
src/transformers/tokenization_mbart.py
tests/test_modeling_mbart.py
utils/check_repo.py
==================
05810cd80;Manuel Romero;2020-08-13 21:01:08 +0200;Fix typo (#6469)

==

model_cards/mrm8488/t5-small-finetuned-squadv2/README.md
==================
7bc00569d;Kevin Canwen Xu;2020-08-14 00:34:03 +0800;Clean directory after script testing (#6453)
* Clean Dir after testing

* remove pabee ignore
==

examples/bert-loses-patience/test_run_glue_with_pabee.py
examples/test_examples.py
==================
e92efcf72;Sam Shleifer;2020-08-13 12:15:54 -0400;Mult rouge by 100: standard units (#6359)

==

examples/seq2seq/utils.py
==================
eda07efaa;vblagoje;2020-08-13 12:09:51 -0400;Add POS tagging and Phrase chunking token classification examples (#6457)
* Add more token classification examples

* POS tagging example

* Phrase chunking example

* PR review fixes

* Add conllu to third party list (used in token classification examples)
==

examples/requirements.txt
examples/token-classification/run.sh
examples/token-classification/run_chunk.sh
examples/token-classification/run_ner.py
examples/token-classification/run_pl_ner.py
examples/token-classification/run_pos.sh
examples/token-classification/run_pos_pl.sh
examples/token-classification/tasks.py
examples/token-classification/utils_ner.py
setup.cfg
==================
f51161e23;Suraj Patil;2020-08-13 21:38:11 +0530;add BartTokenizerFast in AutoTokenizer (#6464)
Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
==

src/transformers/tokenization_auto.py
==================
a442f87ad;Suraj Patil;2020-08-13 21:36:43 +0530;add LongformerTokenizerFast in AutoTokenizer (#6463)

==

src/transformers/tokenization_auto.py
==================
f7cbc13db;Lysandre Debut;2020-08-13 11:59:35 -0400;Test model outputs equivalence (#6445)
* Test model outputs equivalence

* Fix failing tests

* From dict to kwargs

* DistilBERT

* Addressing @sgugger and @patrickvonplaten's comments
==

src/transformers/modeling_tf_longformer.py
tests/test_modeling_common.py
tests/test_modeling_t5.py
tests/test_modeling_tf_common.py
==================
54c687e97;Prajjwal Bhargava;2020-08-13 19:06:48 +0530;typo fix (#6462)

==

src/transformers/modeling_utils.py
==================
9d94aecd5;Zhu Baohe;2020-08-13 19:12:16 +0800;Fix docs and bad word tokens generation_utils.py (#6387)
* fix

* fix2

* fix3
==

src/transformers/generation_tf_utils.py
src/transformers/generation_utils.py
==================
0ed7c00ba;cedspam;2020-08-13 11:01:17 +0200;Update README.md (#6435)
* Update README.md

* Update README.md

* Update README.md
==

model_cards/cedpsam/chatbot_fr/README.md
==================
e983da0e7;Stas Bekman;2020-08-13 01:29:06 -0700;cleanup tf unittests: part 2 (#6260)
* cleanup torch unittests: part 2

* remove trailing comma added by isort, and which breaks flake

* one more comma

* revert odd balls

* part 3: odd cases

* more ["key"] -> .key refactoring

* .numpy() is not needed

* more unncessary .numpy() removed

* more simplification
==

templates/adding_a_new_model/tests/test_modeling_tf_xxx.py
tests/test_modeling_ctrl.py
tests/test_modeling_gpt2.py
tests/test_modeling_mbart.py
tests/test_modeling_t5.py
tests/test_modeling_tf_albert.py
tests/test_modeling_tf_bert.py
tests/test_modeling_tf_ctrl.py
tests/test_modeling_tf_distilbert.py
tests/test_modeling_tf_electra.py
tests/test_modeling_tf_flaubert.py
tests/test_modeling_tf_gpt2.py
tests/test_modeling_tf_mobilebert.py
tests/test_modeling_tf_openai.py
tests/test_modeling_tf_roberta.py
tests/test_modeling_tf_t5.py
tests/test_modeling_tf_transfo_xl.py
tests/test_modeling_tf_xlm.py
tests/test_modeling_tf_xlnet.py
tests/test_modeling_transfo_xl.py
tests/test_modeling_xlnet.py
==================
bc820476a;Joe Davison;2020-08-12 12:48:29 -0400;add targets arg to fill-mask pipeline (#6239)
* add targets arg to fill-mask pipeline

* add tests and more error handling

* quality

* update docstring
==

src/transformers/pipelines.py
tests/test_pipelines.py
==================
0735def8e;Patrick von Platen;2020-08-12 18:23:30 +0200;[EncoderDecoder] Add encoder-decoder for roberta/ vanilla longformer (#6411)
* add encoder-decoder for roberta

* fix headmask

* apply Sylvains suggestions

* fix typo

* Apply suggestions from code review
==

docs/source/model_doc/roberta.rst
src/transformers/__init__.py
src/transformers/modeling_auto.py
src/transformers/modeling_bert.py
src/transformers/modeling_encoder_decoder.py
src/transformers/modeling_roberta.py
src/transformers/modeling_tf_bert.py
tests/test_modeling_bert.py
tests/test_modeling_encoder_decoder.py
tests/test_modeling_roberta.py
==================
fd3de2000;zcain117;2020-08-12 08:46:24 -0700;Get GKE logs via kubectl logs instead of gcloud logging read. (#6446)

==

.circleci/config.yml
==================
f94a52cd7;Sam Shleifer;2020-08-12 11:41:04 -0400;[s2s] add BartTranslationDistiller for distilling mBART (#6363)

==

examples/seq2seq/distillation.py
examples/seq2seq/test_seq2seq_examples.py
==================
d2370e1bd;Sylvain Gugger;2020-08-12 11:32:27 -0400;Adding PaddingDataCollator (#6442)
* Data collator with padding

* Add type annotation

* Support tensors as well

* Add comment

* Fix for labels wrong shape

* Data collator with padding

* Add type annotation

* Support tensors as well

* Add comment

* Fix for labels wrong shape

* Remove changes rendered unnecessary
==

src/transformers/__init__.py
src/transformers/data/data_collator.py
==================
96c3329f1;Sylvain Gugger;2020-08-12 08:47:30 -0400;Fix #6428 (#6437)

==

src/transformers/hf_argparser.py
==================
a8db954cd;Sylvain Gugger;2020-08-12 08:42:14 -0400;Activate check on the CI (#6427)
* Activate check on the CI

* Fix repo inconsistencies

* Don't document too much
==

.circleci/config.yml
docs/source/model_doc/pegasus.rst
==================
34fabe169;Sylvain Gugger;2020-08-12 08:03:45 -0400;Move prediction_loss_only to TrainingArguments (#6426)

==

docs/source/model_doc/pegasus.rst
src/transformers/trainer.py
src/transformers/trainer_tf.py
src/transformers/training_args.py
utils/check_repo.py
==================
e9c303146;Sylvain Gugger;2020-08-12 08:00:56 -0400;Fixes to make life easier with the nlp library (#6423)
* allow using tokenizer.pad as a collate_fn in pytorch

* allow using tokenizer.pad as a collate_fn in pytorch

* Add documentation and tests

* Make attention mask the right shape

* Better test

Co-authored-by: Thomas Wolf <thomwolf@users.noreply.github.com>
==

src/transformers/pipelines.py
src/transformers/tokenization_utils_base.py
tests/test_tokenization_utils.py
==================
87b359439;Stas Bekman;2020-08-12 04:54:28 -0700;[test] replace capsys with the more refined CaptureStderr/CaptureStdout (#6422)
* replace capsys with the more refined CaptureStderr/CaptureStdout

* Update examples/seq2seq/test_seq2seq_examples.py

Co-authored-by: Sam Shleifer <sshleifer@gmail.com>
==

examples/seq2seq/test_seq2seq_examples.py
==================
ac5bcf236;Jared T Nielsen;2020-08-12 04:52:42 -0700;Fix FFN dropout in TFAlbertLayer, and split dropout in TFAlbertAttent‚Ä¶ (#4323)
* Fix FFN dropout in TFAlbertLayer, and split dropout in TFAlbertAttention into two separate dropout layers.

* Same dropout fixes for PyTorch.
==

src/transformers/modeling_albert.py
src/transformers/modeling_tf_albert.py
==================
4ffea5ce2;Lysandre Debut;2020-08-12 02:52:50 -0400;Disabled pabee test (#6431)

==

examples/bert-loses-patience/test_run_glue_with_pabee.py
==================
155288f04;Rohan Rajpal;2020-08-12 04:08:18 +0530;[model_card] rohanrajpal/bert-base-codemixed-uncased-sentiment (#6324)
* Create README.md

* Update model_cards/rohanrajpal/bert-base-codemixed-uncased-sentiment/README.md

* Update model_cards/rohanrajpal/bert-base-codemixed-uncased-sentiment/README.md

Co-authored-by: Julien Chaumond <chaumond@gmail.com>
==

model_cards/rohanrajpal/bert-base-codemixed-uncased-sentiment/README.md
==================
4e6245fc7;Manuel Romero;2020-08-12 00:35:27 +0200;Create model card T5-base fine-tuned on event2Mind for Intent Prediction (#6412)

==

model_cards/mrm8488/t5-base-finetuned-e2m-intent/README.md
==================
46e3a0a6e;Manuel Romero;2020-08-12 00:34:11 +0200;Create README.md (#6381)

==

model_cards/mrm8488/t5-small-finetuned-quora-for-paraphrasing/README.md
==================
31dfde742;Manuel Romero;2020-08-12 00:32:37 +0200;Create README.md (#6378)

==

model_cards/mrm8488/t5-base-finetuned-question-generation-ap/README.md
==================
25e29150a;Manuel Romero;2020-08-12 00:32:29 +0200;Add metadata to be indexed properly (#6380)

==

model_cards/mrm8488/RuPERTa-base-finetuned-squadv1/README.md
==================
471be5f27;Manuel Romero;2020-08-12 00:32:18 +0200;Change metadata to be indexed correctly (#6379)

==

model_cards/mrm8488/RuPERTa-base-finetuned-squadv2/README.md
==================
42ee0bc63;Rohan Rajpal;2020-08-12 04:01:34 +0530;Create README.md (#6346)
* Create README.md

* add results on SAIL dataset

* Update model_cards/rohanrajpal/bert-base-multilingual-codemixed-cased-sentiment/README.md

Co-authored-by: Julien Chaumond <chaumond@gmail.com>

Co-authored-by: Julien Chaumond <chaumond@gmail.com>
==

model_cards/rohanrajpal/bert-base-multilingual-codemixed-cased-sentiment/README.md
==================
3f071c4b6;Sam Shleifer;2020-08-11 17:58:09 -0400;[examples] add pytest dependency (#6425)

==

examples/requirements.txt
==================
ece0903e1;Stas Bekman;2020-08-11 14:56:41 -0700;lr_schedulers: add get_polynomial_decay_schedule_with_warmup (#6361)
* [wip] add get_polynomial_decay_schedule_with_warmup

* style

* add assert

* change lr_end to a much smaller default number

* check for exact equality

* [model_cards] electra-base-turkish-cased-ner (#6350)

* for electra-base-turkish-cased-ner

* Add metadata

Co-authored-by: Julien Chaumond <chaumond@gmail.com>

* Temporarily de-activate TPU CI

* Update modeling_tf_utils.py (#6372)

fix typo: ckeckpoint->checkpoint

* the test now works again (#6371)

* correct pl link in readme (#6364)

* refactor almost identical tests (#6339)

* refactor almost identical tests

* important to add a clear assert error message

* make the assert error even more descriptive than the original bt

* Small docfile fixes (#6328)

* Patch models (#6326)

* TFAlbertFor{TokenClassification, MultipleChoice}

* Patch models

* BERT and TF BERT info


s

* Update check_repo

* Ci GitHub caching (#6382)

* Cache Github Actions CI

* Remove useless file

* Colab button (#6389)

* Add colab button

* Add colab link for tutorials

* Fix links for open in colab (#6391)

* Update src/transformers/optimization.py

consistently use lr_end=1e-7 default

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* [wip] add get_polynomial_decay_schedule_with_warmup

* style

* add assert

* change lr_end to a much smaller default number

* check for exact equality

* Update src/transformers/optimization.py

consistently use lr_end=1e-7 default

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* remove dup (leftover from merge)

* convert the test into the new refactored format

* stick to using the current_step as is, without ++

Co-authored-by: M. Yusuf Sarƒ±g√∂z <yusufsarigoz@gmail.com>
Co-authored-by: Julien Chaumond <chaumond@gmail.com>
Co-authored-by: Lysandre <lysandre.debut@reseau.eseo.fr>
Co-authored-by: Alexander Measure <ameasure@gmail.com>
Co-authored-by: Rohit Gupta <rohitgr1998@gmail.com>
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
==

examples/lightning_base.py
src/transformers/__init__.py
src/transformers/optimization.py
tests/test_optimization.py
==================
6c87b73d6;cedspam;2020-08-11 22:56:51 +0200;Create README.md (#6386)
* Create README.md

* Update README.md
==

model_cards/cedpsam/chatbot_fr/README.md
==================
0203d6517;Stas Bekman;2020-08-11 13:27:11 -0700;[pl] restore lr logging behavior for glue, ner examples (#6314)

==

examples/lightning_base.py
examples/text-classification/run_pl_glue.py
==================
be1520d3a;Sam Shleifer;2020-08-11 15:57:07 -0400;rename prepare_translation_batch -> prepare_seq2seq_batch (#6103)

==

docs/source/model_doc/bart.rst
docs/source/model_doc/marian.rst
examples/seq2seq/README.md
examples/seq2seq/utils.py
src/transformers/modeling_marian.py
src/transformers/tokenization_bart.py
src/transformers/tokenization_marian.py
src/transformers/tokenization_pegasus.py
src/transformers/tokenization_utils_base.py
tests/test_modeling_marian.py
tests/test_modeling_mbart.py
tests/test_tokenization_common.py
tests/test_tokenization_marian.py
tests/test_tokenization_mbart.py
==================
66fa8ceae;Sam Shleifer;2020-08-11 14:31:23 -0400;PegasusForConditionalGeneration (torch version) (#6340)
Co-authored-by: Jingqing  Zhang <jingqing.zhang15@imperial.ac.uk>
==

docs/source/index.rst
docs/source/model_doc/bart.rst
docs/source/model_doc/pegasus.rst
docs/source/pretrained_models.rst
examples/seq2seq/distillation.py
examples/seq2seq/finetune_pegasus_xsum.sh
src/transformers/__init__.py
src/transformers/configuration_auto.py
src/transformers/configuration_bart.py
src/transformers/configuration_pegasus.py
src/transformers/convert_pegasus_tf_to_pytorch.py
src/transformers/modeling_auto.py
src/transformers/modeling_marian.py
src/transformers/modeling_pegasus.py
src/transformers/tokenization_auto.py
src/transformers/tokenization_pegasus.py
tests/test_modeling_bart.py
tests/test_modeling_mbart.py
tests/test_modeling_pegasus.py
tests/test_tokenization_pegasus.py
==================
f6cb0f806;Stas Bekman;2020-08-11 09:04:17 -0700;[s2s] wmt download script use less ram (#6405)

==

examples/seq2seq/download_wmt.py
==================
7c6a085eb;Stas Bekman;2020-08-11 07:58:54 -0700;pl version: examples/requirements.txt is single source of truth (#6309)

==

examples/text-classification/run_pl.sh
examples/token-classification/run_pl.sh
==================
1d1d5bec1;Pranav Vadrevu;2020-08-11 16:36:15 +0200;Create Model Card File (#6357)

==

model_cards/pranavpsv/gpt2-genre-story-generator/README.md
==================
00ce881c0;Abed khooli;2020-08-11 17:35:31 +0300;Create README.md (#6413)
* Create README.md

Model card for https://huggingface.co/akhooli/gpt2-small-arabic

* Update model_cards/akhooli/gpt2-small-arabic/README.md

Co-authored-by: Julien Chaumond <chaumond@gmail.com>
==

model_cards/akhooli/gpt2-small-arabic/README.md
==================
3ae30787b;Nick Doiron;2020-08-11 10:34:22 -0400;switch Hindi-BERT to S3 README (#6396)

==

model_cards/monsoon-nlp/hindi-bert/README.md
==================
824e651e1;Abed khooli;2020-08-11 16:03:23 +0300;Create README.md (#6397)
* Create README.md

* Update model_cards/akhooli/gpt2-small-arabic-poetry/README.md

* Update model_cards/akhooli/gpt2-small-arabic-poetry/README.md

* Update model_cards/akhooli/gpt2-small-arabic-poetry/README.md

* Update model_cards/akhooli/gpt2-small-arabic-poetry/README.md

Co-authored-by: Julien Chaumond <chaumond@gmail.com>
==

model_cards/akhooli/gpt2-small-arabic-poetry/README.md
==================
404782912;guillaume-be;2020-08-11 11:56:40 +0200;[Performance improvement] "Bad tokens ids" optimization (#6064)
* Optimized banned token masking

* Avoid duplicate EOS masking if in bad_words_id

* Updated mask generation to handle empty banned token list

* Addition of unit tests for the updated bad_words_ids masking

* Updated timeout handling in `test_postprocess_next_token_scores_large_bad_words_list` unit test

* Updated timeout handling in `test_postprocess_next_token_scores_large_bad_words_list` unit test (timeout does not work on Windows)

* Moving Marian import to the test context to allow TF only environments to run

* Moving imports to torch_available test

* Updated operations device and test

* Updated operations device and test

* Added docstring and comment for in-place scores modification

* Moving test to own test_generation_utils, use of lighter models for testing

* removed unneded imports in test_modeling_common

* revert formatting change for ModelTesterMixin

* Updated caching, simplified eos token id test, removed unnecessary @require_torch

* formatting compliance
==

src/transformers/data/test_generation_utils.py
src/transformers/generation_utils.py
==================
87e124c24;David LaPalomento;2020-08-11 05:31:26 -0400;Warn if debug requested without TPU fixes (#6308) (#6390)
* Warn if debug requested without TPU fixes (#6308)
Check whether a PyTorch compatible TPU is available before attempting to print TPU metrics after training has completed. This way, users who apply `--debug` without reading the documentation aren't suprised by a stacktrace.

* Style

Co-authored-by: Lysandre <lysandre.debut@reseau.eseo.fr>
==

src/transformers/trainer.py
==================
cdf1f7edb;Junyuan Zheng;2020-08-11 04:49:16 -0400;Fix tokenizer saving and loading error (#6026)
* fix tokenizer saving and loading bugs when adding AddedToken to additional special tokens

* Add tokenizer test

* Style

* Style 2

Co-authored-by: Lysandre <lysandre.debut@reseau.eseo.fr>
==

src/transformers/tokenization_utils_base.py
tests/test_tokenization_common.py
==================
83984a61c;Stas Bekman;2020-08-11 00:56:47 -0700;testing utils: capturing std streams context manager (#6231)
* testing utils: capturing std streams context manager

* style

* missing import

* add the origin of this code
==

src/transformers/testing_utils.py
==================
f6c0680d3;Stas Bekman;2020-08-11 00:16:52 -0700;add pl_glue example test (#6034)
* add pl_glue example test

* for now just test that it runs, next validate results of eval or predict?

* complete the run_pl_glue test to validate the actual outcome

* worked on my machine, CI gets less accuracy - trying higher epochs

* match run_pl.sh hparms

* more epochs?

* trying higher lr

* for now just test that the script runs to a completion

* correct the comment

* if cuda is available, add --fp16 --gpus=1 to cover more bases

* style
==

examples/test_examples.py
examples/text-classification/run_pl_glue.py
==================
b25cec13c;Pradhy729;2020-08-11 00:12:45 -0700;Feed forward chunking (#6024)
* Chunked feed forward for Bert

This is an initial implementation to test applying feed forward chunking for BERT.
Will need additional modifications based on output and benchmark results.

* Black and cleanup

* Feed forward chunking in BertLayer class.

* Isort

* add chunking for all models

* fix docs

* Fix typo

Co-authored-by: patrickvonplaten <patrick.v.platen@gmail.com>
==

src/transformers/configuration_reformer.py
src/transformers/configuration_utils.py
src/transformers/modeling_bert.py
tests/test_modeling_bert.py
tests/test_modeling_common.py
tests/test_modeling_reformer.py
==================
8a3db6b30;Lysandre;2020-08-11 08:49:37 +0200;Add TPU testing once again

==

.circleci/config.yml
==================
f65ac1faf;zcain117;2020-08-10 23:48:49 -0700;Add missing docker arg for TPU CI. (#6393)

==

.circleci/config.yml
==================
b9ecd92ee;Sam Shleifer;2020-08-10 22:49:39 -0400;[s2s] Script to save wmt data to disk (#6403)

==

examples/seq2seq/download_wmt.py
==================
00bb0b25e;Patrick von Platen;2020-08-10 23:25:06 +0200;TF Longformer (#5764)
* improve names and tests longformer

* more and better tests for longformer

* add first tf test

* finalize tf basic op functions

* fix merge

* tf shape test passes

* narrow down discrepancies

* make longformer local attn tf work

* correct tf longformer

* add first global attn function

* add more global longformer func

* advance tf longformer

* finish global attn

* upload big model

* finish all tests

* correct false any statement

* fix common tests

* make all tests pass except keras save load

* fix some tests

* fix torch test import

* finish tests

* fix test

* fix torch tf tests

* add docs

* finish docs

* Update src/transformers/modeling_longformer.py

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* Update src/transformers/modeling_tf_longformer.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* apply Lysandres suggestions

* reverse to assert statement because function will fail otherwise

* applying sylvains recommendations

* Update src/transformers/modeling_longformer.py

Co-authored-by: Sam Shleifer <sshleifer@gmail.com>

* Update src/transformers/modeling_tf_longformer.py

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
Co-authored-by: Sam Shleifer <sshleifer@gmail.com>
==

docs/source/model_doc/longformer.rst
src/transformers/__init__.py
src/transformers/modeling_longformer.py
src/transformers/modeling_tf_auto.py
src/transformers/modeling_tf_longformer.py
tests/test_modeling_longformer.py
tests/test_modeling_tf_longformer.py
==================
342593664;Patrick von Platen;2020-08-10 19:46:48 +0200;[EncoderDecoderModel] add a `add_cross_attention` boolean to config (#6377)
* correct encoder decoder model

* Apply suggestions from code review

* apply sylvains suggestions
==

src/transformers/configuration_encoder_decoder.py
src/transformers/configuration_utils.py
src/transformers/modeling_bert.py
src/transformers/modeling_encoder_decoder.py
tests/test_modeling_bert.py
tests/test_modeling_encoder_decoder.py
==================
06bc347c9;Sylvain Gugger;2020-08-10 11:16:17 -0400;Fix links for open in colab (#6391)

==

docs/source/_static/js/custom.js
==================
3e0fe3cf5;Sylvain Gugger;2020-08-10 11:12:29 -0400;Colab button (#6389)
* Add colab button

* Add colab link for tutorials
==

docs/source/_static/css/huggingface.css
docs/source/_static/js/custom.js
==================
79588e6fd;Lysandre Debut;2020-08-10 10:39:31 -0400;Ci GitHub caching (#6382)
* Cache Github Actions CI

* Remove useless file
==

.github/workflows/github-push.yml
.github/workflows/github-torch-hub.yml
.github/workflows/self-push.yml
.github/workflows/self-scheduled.yml
==================
b99098abc;Lysandre Debut;2020-08-10 10:39:17 -0400;Patch models (#6326)
* TFAlbertFor{TokenClassification, MultipleChoice}

* Patch models

* BERT and TF BERT info


s

* Update check_repo
==

src/transformers/modeling_bert.py
src/transformers/modeling_tf_bert.py
tests/test_modeling_tf_albert.py
tests/test_modeling_tf_bert.py
tests/test_modeling_tf_electra.py
utils/check_repo.py
==================
6028ed92b;Sylvain Gugger;2020-08-10 05:37:12 -0400;Small docfile fixes (#6328)

==

docs/source/benchmarks.rst
docs/source/preprocessing.rst
docs/source/quicktour.rst
docs/source/task_summary.rst
docs/source/tokenizer_summary.rst
docs/source/training.rst
==================
1429b920d;Stas Bekman;2020-08-10 02:31:20 -0700;refactor almost identical tests (#6339)
* refactor almost identical tests

* important to add a clear assert error message

* make the assert error even more descriptive than the original bt
==

tests/test_optimization.py
==================
35eb96de4;Rohit Gupta;2020-08-10 12:38:46 +0530;correct pl link in readme (#6364)

==

examples/README.md
==================
0830e7951;Stas Bekman;2020-08-09 23:55:52 -0700;the test now works again (#6371)

==

examples/seq2seq/test_seq2seq_examples.py
==================
3a556b0fb;Alexander Measure;2020-08-10 02:55:11 -0400;Update modeling_tf_utils.py (#6372)
fix typo: ckeckpoint->checkpoint
==

src/transformers/modeling_tf_utils.py
==================
1bbc54a87;Lysandre;2020-08-10 08:11:40 +0200;Temporarily de-activate TPU CI

==

.circleci/config.yml
==================
6e8a38568;M. Yusuf Sarƒ±g√∂z;2020-08-09 10:39:51 +0300;[model_cards] electra-base-turkish-cased-ner (#6350)
* for electra-base-turkish-cased-ner

* Add metadata

Co-authored-by: Julien Chaumond <chaumond@gmail.com>
==

model_cards/mys/electra-base-turkish-cased-ner/README.md
==================
9a5ef8374;Sam Shleifer;2020-08-08 21:51:37 -0400;[s2s] fix --gpus clarg collision (#6358)

==

examples/lightning_base.py
examples/text-classification/run_pl_glue.py
examples/token-classification/run_pl_ner.py
==================
1aec99164;Patrick von Platen;2020-08-08 20:37:29 +0200;[GPT2] Correct typo in docs (#6352)

==

src/transformers/configuration_gpt2.py
==================
9f57e39f7;elsanns;2020-08-08 11:47:33 +0200;Add notebook on fine-tuning and interpreting Electra (#6321)
Co-authored-by: eliska <3648991+elisans@users.noreply.github.com>
==

notebooks/README.md
==================
9bed35544;Suraj Patil;2020-08-08 13:51:12 +0530;[s2s] fix label_smoothed_nll_loss (#6344)

==

examples/seq2seq/utils.py
==================
99f73bcc7;Sam Shleifer;2020-08-08 02:45:55 -0400;[s2s] tiny QOL improvement: run_eval prints scores (#6341)

==

examples/seq2seq/pack_dataset.py
examples/seq2seq/run_eval.py
==================
322dffc6c;Stas Bekman;2020-08-07 18:30:39 -0700;remove a TODO item to use a tiny model (#6338)
as discussed with @sshleifer, removing this TODO to switch to a tiny model, since it won't be able to test the results of the evaluation (i.e. the results are meaningless).
==

examples/test_examples.py
==================
1f8e82651;Sam Shleifer;2020-08-07 18:40:21 -0400;[CI] Self-scheduled runner also pins torch (#6332)

==

.github/workflows/self-scheduled.yml
==================
1b8a7ffcf;zcain117;2020-08-07 08:17:07 -0700;Add setup for TPU CI to run every hour. (#6219)
* Add setup for TPU CI to run every hour.

* Re-organize config.yml

Co-authored-by: Lysandre <lysandre.debut@reseau.eseo.fr>
==

.circleci/config.yml
docker/transformers-pytorch-tpu/Dockerfile
docker/transformers-pytorch-tpu/bert-base-cased.jsonnet
docker/transformers-pytorch-tpu/dataset.yaml
docker/transformers-pytorch-tpu/docker-entrypoint.sh
examples/test_xla_examples.py
==================
6695450a2;Stas Bekman;2020-08-07 07:36:32 -0700;[examples] consistently use --gpus, instead of --n_gpu (#6315)

==

examples/distillation/README.md
examples/distillation/train.py
examples/text-classification/README.md
examples/token-classification/README.md
==================
0e36e5151;Julien Plu;2020-08-07 15:30:57 +0200;Fix the tests for Electra (#6284)
* Fix the tests for Electra

* Apply style
==

src/transformers/modeling_electra.py
tests/test_modeling_tf_electra.py
==================
6ba540b74;Sylvain Gugger;2020-08-07 09:18:37 -0400;Add a script to check all models are tested and documented (#6298)
* Add a script to check all models are tested and documented

* Apply suggestions from code review

Co-authored-by: Kevin Canwen Xu <canwenxu@126.com>

* Address comments

Co-authored-by: Kevin Canwen Xu <canwenxu@126.com>
==

Makefile
docs/source/model_doc/bert.rst
docs/source/model_doc/camembert.rst
docs/source/model_doc/flaubert.rst
docs/source/model_doc/reformer.rst
docs/source/model_doc/xlm.rst
src/transformers/modeling_electra.py
src/transformers/modeling_tf_albert.py
tests/test_modeling_electra.py
tests/test_modeling_tf_albert.py
tests/test_modeling_tf_openai.py
utils/check_repo.py
==================
e1638dce1;Stas Bekman;2020-08-07 06:17:32 -0700;fix the slow tests doc (#6167)
remove unnecessary duplication wrt `RUN_SLOW=yes`
==

CONTRIBUTING.md
==================
7e9861f7f;Binny Mathew;2020-08-07 15:21:03 +0530;dehate-bert Model Card (#6248)
Added citation and paper links.
==

model_cards/Hate-speech-CNERG/dehatebert-mono-spanish/README.md
==================
f6df6d98d;Binny Mathew;2020-08-07 15:18:38 +0530;dehate-bert Model Card (#6249)
Added citation and paper links.
==

model_cards/Hate-speech-CNERG/dehatebert-mono-portugese/README.md
==================
26691ecba;Binny Mathew;2020-08-07 15:18:09 +0530;dehate-bert Model Card (#6250)
Added citation and paper links.
==

model_cards/Hate-speech-CNERG/dehatebert-mono-polish/README.md
==================
60657b295;Binny Mathew;2020-08-07 15:17:42 +0530;dehate-bert Model Card (#6251)
Added citation and paper links.
==

model_cards/Hate-speech-CNERG/dehatebert-mono-italian/README.md
==================
721826199;Binny Mathew;2020-08-07 15:17:26 +0530;dehate-bert Model Card (#6252)
Added citation and paper links.
==

model_cards/Hate-speech-CNERG/dehatebert-mono-indonesian/README.md
==================
396d227cd;Binny Mathew;2020-08-07 15:17:04 +0530;dehate-bert Model Card (#6253)
Added citation and paper links.
==

model_cards/Hate-speech-CNERG/dehatebert-mono-german/README.md
==================
8be260f18;Binny Mathew;2020-08-07 15:16:27 +0530;dehate-bert Model Card (#6254)
Added citation and paper links.
==

model_cards/Hate-speech-CNERG/dehatebert-mono-french/README.md
==================
dce7278cd;Binny Mathew;2020-08-07 15:15:52 +0530;dehate-bert Model Card (#6255)
Added citation and paper links.
==

model_cards/Hate-speech-CNERG/dehatebert-mono-english/README.md
==================
3be2d0488;idoh;2020-08-07 12:44:28 +0300;fix consistency CrossEntropyLoss in modeling_bart (#6265)

==

src/transformers/modeling_bart.py
==================
c72f9c90a;Lysandre;2020-08-07 09:07:22 +0200;Remove --no-cache-dir from github CI

==

.github/workflows/github-torch-hub.yml
.github/workflows/self-push.yml
.github/workflows/self-scheduled.yml
==================
0d9328f2e;Lysandre Debut;2020-08-07 02:58:15 -0400;Patch GPU failures (#6281)
* Pin to 1.5.0

* Patch XLM GPU test
==

.github/workflows/self-push.yml
src/transformers/modeling_xlm.py
==================
80a0676a5;Lysandre Debut;2020-08-07 02:48:59 -0400;CI dependency wheel caching (#6287)
* Single workflow cache test




Remove cache dir, re-trigger cache


Only pip archives


Not sudo when pip

* All workflow cache

Remove no-cache-dir instruction


Remove last sudo occurrences


v0.3
==

.circleci/config.yml
==================
175cd45e1;Stas Bekman;2020-08-06 17:32:28 -0700;fix the shuffle agrument usage and the default (#6307)

==

examples/seq2seq/test_seq2seq_examples.py
examples/text-classification/run_pl_glue.py
==================
ffceef204;Bhashithe Abeysinghe;2020-08-06 15:46:43 -0400;[Fix] text-classification PL example (#6027)
Co-authored-by: Sam Shleifer <sshleifer@gmail.com>
==

examples/lightning_base.py
examples/text-classification/run_pl.sh
examples/text-classification/run_pl_glue.py
==================
eb2bd8d6e;xujiaze13;2020-08-06 12:43:45 -0700;Remove redundant line in run_pl_glue.py (#6305)

==

examples/text-classification/run_pl_glue.py
==================
118ecfd42;Patrick von Platen;2020-08-06 21:14:46 +0200;fix for pytorch < 1.6 (#6300)

==

src/transformers/modeling_reformer.py
==================
2804fff83;Sam Shleifer;2020-08-06 14:58:38 -0400;[s2s]Use prepare_translation_batch for Marian finetuning (#6293)
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

examples/seq2seq/README.md
examples/seq2seq/finetune.py
examples/seq2seq/test_seq2seq_examples.py
examples/seq2seq/utils.py
src/transformers/tokenization_marian.py
==================
2f2aa0c89;Teven;2020-08-06 17:47:32 +0200;added `n_inner` argument to gpt2 config (#6296)

==

src/transformers/configuration_gpt2.py
src/transformers/modeling_gpt2.py
src/transformers/modeling_tf_gpt2.py
==================
0a0d53dcf;Manuel Romero;2020-08-06 17:42:43 +0200;Update model card (#6290)
Add links to RuPERTa models fine-tuned on Spanish SQUAD datasets
==

model_cards/mrm8488/RuPERTa-base/README.md
==================
b923871bb;Doug Blank;2020-08-06 08:31:30 -0700;Adds comet_ml to the list of auto-experiment loggers (#6176)
* Support for Comet.ml

* Need to import comet first

* Log this model, not the one in the backprop step

* Log args as hyperparameters; use framework to allow fine control

* Log hyperparameters with context

* Apply black formatting

* isort fix integrations

* isort fix __init__

* Update src/transformers/trainer.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/trainer.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/trainer_tf.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Address review comments

* Style + Quality, remove Tensorboard import test

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
Co-authored-by: Lysandre <lysandre.debut@reseau.eseo.fr>
==

examples/README.md
src/transformers/__init__.py
src/transformers/integrations.py
src/transformers/trainer.py
src/transformers/trainer_tf.py
src/transformers/trainer_utils.py
==================
d5bc32ce9;Philip May;2020-08-06 12:52:28 +0200;Add strip_accents to basic BertTokenizer. (#6280)
* Add strip_accents to basic tokenizer

* Add tests for strip_accents.

* fix style with black

* Fix strip_accents test

* empty commit to trigger CI

* Improved strip_accents check

* Add code quality with is not False
==

src/transformers/tokenization_bert.py
tests/test_tokenization_bert.py
==================
31da35cc8;JME-P;2020-08-05 17:36:24 +0100;Create README.md (#6273)
I am adding a descriptive README.md file to my recently uploaded twitter classification model: shrugging-grace/tweetclassifier.
==

model_cards/shrugging-grace/tweetclassifier/README.md
==================
a8bdba232;JME-P;2020-08-05 17:27:46 +0100;Create README.md for uploaded classifier (#6272)
I am adding a descriptive README.md file to my recently uploaded twitter classification model: shrugging-grace/tweetclassifier.
==

model_cards/jme-p/shrugging-grace-tweet-classifier/README.md
==================
a23a535c1;HUSEIN ZOLKEPLI;2020-08-06 00:27:27 +0800;added t5 bahasa summarization readme (#6269)

==

model_cards/huseinzol05/t5-base-bahasa-summarization-cased/README.md
model_cards/huseinzol05/t5-small-bahasa-summarization-cased/README.md
==================
c67d1a025;Sylvain Gugger;2020-08-05 11:34:39 -0400;Tf model outputs (#6247)
* TF outputs and test on BERT

* Albert to DistilBert

* All remaining TF models except T5

* Documentation

* One file forgotten

* TF outputs and test on BERT

* Albert to DistilBert

* All remaining TF models except T5

* Documentation

* One file forgotten

* Add new models and fix issues

* Quality improvements

* Add T5

* A bit of cleanup

* Fix for slow tests

* Style
==

docs/source/model_doc/albert.rst
docs/source/model_doc/bert.rst
docs/source/model_doc/electra.rst
docs/source/model_doc/gpt.rst
docs/source/model_doc/gpt2.rst
docs/source/model_doc/mobilebert.rst
docs/source/model_doc/transformerxl.rst
docs/source/model_doc/xlnet.rst
src/transformers/file_utils.py
src/transformers/modeling_albert.py
src/transformers/modeling_bert.py
src/transformers/modeling_electra.py
src/transformers/modeling_mobilebert.py
src/transformers/modeling_t5.py
src/transformers/modeling_tf_albert.py
src/transformers/modeling_tf_bert.py
src/transformers/modeling_tf_camembert.py
src/transformers/modeling_tf_ctrl.py
src/transformers/modeling_tf_distilbert.py
src/transformers/modeling_tf_electra.py
src/transformers/modeling_tf_flaubert.py
src/transformers/modeling_tf_gpt2.py
src/transformers/modeling_tf_mobilebert.py
src/transformers/modeling_tf_openai.py
src/transformers/modeling_tf_outputs.py
src/transformers/modeling_tf_roberta.py
src/transformers/modeling_tf_t5.py
src/transformers/modeling_tf_transfo_xl.py
src/transformers/modeling_tf_xlm.py
src/transformers/modeling_tf_xlm_roberta.py
src/transformers/modeling_tf_xlnet.py
src/transformers/modeling_xlnet.py
templates/adding_a_new_model/modeling_tf_xxx.py
templates/adding_a_new_model/tests/test_modeling_tf_xxx.py
tests/test_modeling_tf_albert.py
tests/test_modeling_tf_bert.py
tests/test_modeling_tf_camembert.py
tests/test_modeling_tf_common.py
tests/test_modeling_tf_ctrl.py
tests/test_modeling_tf_distilbert.py
tests/test_modeling_tf_electra.py
tests/test_modeling_tf_flaubert.py
tests/test_modeling_tf_gpt2.py
tests/test_modeling_tf_mobilebert.py
tests/test_modeling_tf_openai_gpt.py
tests/test_modeling_tf_roberta.py
tests/test_modeling_tf_t5.py
tests/test_modeling_tf_transfo_xl.py
tests/test_modeling_tf_xlm.py
tests/test_modeling_tf_xlm_roberta.py
tests/test_modeling_tf_xlnet.py
==================
bd0eab351;Teven;2020-08-05 15:05:52 +0200;Trainer + wandb quality of life logging tweaks (#6241)
* added `name` argument for wandb logging, also logging model config with trainer arguments

* Update src/transformers/training_args.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* added tf, post-review changes

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

src/transformers/trainer.py
src/transformers/trainer_tf.py
src/transformers/training_args.py
src/transformers/training_args_tf.py
==================
33966811b;Julien Plu;2020-08-05 15:04:27 +0200;Add SequenceClassification and MultipleChoice TF models to Electra (#6227)
* Add SequenceClassification and MultipleChoice TF models to Electra

* Apply style

* Add summary_proj_to_labels to Electra config

* Finally mirroring the PT version of these models

* Apply style

* Fix Electra test
==

src/transformers/__init__.py
src/transformers/modeling_tf_auto.py
src/transformers/modeling_tf_electra.py
tests/test_modeling_tf_electra.py
==================
376c02e9a;Stas Bekman;2020-08-05 06:01:17 -0700;[WIP] lightning_base: support --lr_scheduler with multiple possibilities (#6232)
* support --lr_scheduler with multiple possibilities

* correct the error message

* add a note about supported schedulers

* cleanup

* cleanup2

* needs the argument default

* style

* add another assert in the test

* implement requested changes

* cleanups

* fix relative import

* cleanup
==

examples/lightning_base.py
examples/seq2seq/test_seq2seq_examples.py
==================
d89acd07c;Zhu Baohe;2020-08-05 19:37:57 +0800;fix (#6257)

==

src/transformers/configuration_xlnet.py
==================
24c5a6e35;Ninnart Fuengfusin;2020-08-05 20:34:57 +0900;Update optimization.py (#6261)

==

src/transformers/optimization.py
==================
ed6b8f312;Lilian Bordeau;2020-08-05 13:23:55 +0200;Update to match renamed attributes in fairseq master (#5972)
* Update to match renamed attributes in fairseq master

RobertaModel no longer have model.encoder and args.num_classes attributes as of 5/28/20.

* Quality

Co-authored-by: Lysandre <lysandre.debut@reseau.eseo.fr>
==

src/transformers/convert_roberta_original_pytorch_checkpoint_to_pytorch.py
==================
d9149f00d;Ali Safaya;2020-08-05 00:44:14 +0300;Update README.md (#6201)

==

model_cards/asafaya/bert-large-arabic/README.md
==================
ddfdbb86c;Ali Safaya;2020-08-05 00:44:05 +0300;Update README.md (#6200)

==

model_cards/asafaya/bert-medium-arabic/README.md
==================
4f6795566;Ali Safaya;2020-08-05 00:43:48 +0300;Update README.md (#6199)

==

model_cards/asafaya/bert-base-arabic/README.md
==================
869ec441c;Ali Safaya;2020-08-05 00:43:38 +0300;Update README.md (#6198)

==

model_cards/asafaya/bert-mini-arabic/README.md
==================
5177dca63;Adam Montgomerie;2020-08-05 06:42:53 +0900;Create README.md (#6123)

==

model_cards/iarfmoose/t5-base-question-generator/README.md
==================
3f30ebe6c;Manuel Romero;2020-08-04 23:41:23 +0200;Create README.md (#6075)

==

model_cards/mrm8488/gpt2-finetuned-recipes-cooking/README.md
==================
aa7c22a28;Binny Mathew;2020-08-05 03:10:47 +0530;Update Model Card (#6246)
Added citation and paper links.
==

model_cards/Hate-speech-CNERG/dehatebert-mono-arabic/README.md
==================
972535ea7;Joe Davison;2020-08-04 16:37:49 -0400;fix zero shot pipeline docs (#6245)

==

docs/source/main_classes/pipelines.rst
src/transformers/__init__.py
src/transformers/pipelines.py
==================
5920a37a4;Timo Moeller;2020-08-04 19:40:49 +0200;Add license info to German Bert models (#6242)
* Add xlm-r QA model card

* Add tags

* Add license info to german bert
==

model_cards/bert-base-german-cased-README.md
model_cards/deepset/bert-base-german-cased-oldvocab/README.md
==================
6c9ba1d8f;Patrick von Platen;2020-08-04 19:22:43 +0200;[Reformer] Make random seed generator available on random seed and not on model device (#6244)
* improve if else statement random seeds

* Apply suggestions from code review

* Update src/transformers/modeling_reformer.py
==

src/transformers/modeling_reformer.py
==================
d5b0a0e23;Sam Shleifer;2020-08-04 09:53:51 -0400;mBART Conversion script (#6230)

==

src/transformers/convert_bart_original_pytorch_checkpoint_to_pytorch.py
src/transformers/convert_mbart_original_checkpoint_to_pytorch.py
==================
268bf3463;Stas Bekman;2020-08-04 06:31:49 -0700;typo (#6225)

==

src/transformers/benchmark/benchmark_args_utils.py
==================
7f65daa2e;Patrick von Platen;2020-08-04 13:02:25 +0200;fix reformer fp16 (#6237)

==

tests/test_modeling_reformer.py
==================
7ea9b2db3;Andr√©s Felipe Cruz;2020-08-04 00:23:28 -0700;Encoder decoder config docs (#6195)
* Adding docs for how to load encoder_decoder pretrained model with individual config objects

* Adding docs for loading encoder_decoder config from pretrained folder

* Fixing  W293 blank line contains whitespace

* Update src/transformers/modeling_encoder_decoder.py

* Update src/transformers/modeling_encoder_decoder.py

* Update src/transformers/modeling_encoder_decoder.py

* Apply suggestions from code review

model file should only show examples for how to load save model

* Update src/transformers/configuration_encoder_decoder.py

* Update src/transformers/configuration_encoder_decoder.py

* fix space

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
==

src/transformers/configuration_encoder_decoder.py
src/transformers/modeling_encoder_decoder.py
==================
1d5c3a3d9;Lysandre Debut;2020-08-04 03:20:19 -0400;Test with --no-cache-dir (#6235)

==

.circleci/config.yml
.github/workflows/github-torch-hub.yml
.github/workflows/self-push.yml
.github/workflows/self-scheduled.yml
==================
6730ecdd3;Sam Shleifer;2020-08-04 02:59:21 -0400;Remove redundant coverage (#6224)

==

tests/test_tokenization_common.py
==================
5deed37f9;Stas Bekman;2020-08-03 23:42:56 -0700;cleanup torch unittests (#6196)
* improve unit tests

this is a sample of one test according to the request in https://github.com/huggingface/transformers/issues/5973
before I apply it to the rest

* batch 1

* batch 2

* batch 3

* batch 4

* batch 5

* style

* non-tf template

* last deletion of check_loss_output
==

templates/adding_a_new_model/tests/test_modeling_xxx.py
tests/test_modeling_albert.py
tests/test_modeling_bert.py
tests/test_modeling_ctrl.py
tests/test_modeling_distilbert.py
tests/test_modeling_dpr.py
tests/test_modeling_electra.py
tests/test_modeling_encoder_decoder.py
tests/test_modeling_flaubert.py
tests/test_modeling_gpt2.py
tests/test_modeling_longformer.py
tests/test_modeling_mobilebert.py
tests/test_modeling_openai.py
tests/test_modeling_reformer.py
tests/test_modeling_roberta.py
tests/test_modeling_t5.py
tests/test_modeling_xlm.py
tests/test_modeling_xlnet.py
==================
b390a5672;Gong Linyuan;2020-08-04 14:38:30 +0800;Make the order of additional special tokens deterministic (#5704)
* Make the order of additional special tokens deterministic regardless of hash seeds

* Fix
==

src/transformers/tokenization_utils_base.py
==================
d740351f7;Lysandre Debut;2020-08-04 02:37:12 -0400;Upgrade pip when doing CI (#6234)
* Upgrade pip when doing CI

* Don't forget Github CI
==

.circleci/config.yml
.github/workflows/github-torch-hub.yml
.github/workflows/self-push.yml
.github/workflows/self-scheduled.yml
==================
57eb1cb68;Sam Shleifer;2020-08-03 18:22:31 -0400;[s2s] Document better mbart finetuning command (#6229)
* Document better MT command

* improve multigpu command
==

examples/seq2seq/README.md
examples/seq2seq/train_mbart_cc25_enro.sh
==================
0513f8d27;Victor SANH;2020-08-03 15:02:51 -0400;correct label extraction + add note on discrepancies on trained MNLI model and HANS (#6221)

==

examples/adversarial/utils_hans.py
==================
3c289fb38;Kevin Canwen Xu;2020-08-04 01:17:56 +0800;Remove outdated BERT tips (#6217)
* Remove out-dated BERT tips

* Update modeling_outputs.py

* Update bert.rst

* Update bert.rst
==

docs/source/model_doc/bert.rst
src/transformers/modeling_outputs.py
==================
e4920c92d;Sylvain Gugger;2020-08-03 11:44:46 -0400;Doc pipelines (#6175)
* Init work on pipelines doc

* Work in progress

* Work in progress

* Doc pipelines

* Rm unwanted default

* Apply suggestions from code review

Lysandre comments

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
==

docs/source/index.rst
docs/source/internal/pipelines_utils.rst
docs/source/main_classes/model.rst
docs/source/main_classes/pipelines.rst
src/transformers/pipelines.py
==================
b6b2f2270;Sam Shleifer;2020-08-03 10:36:26 -0400;s2s: fix LR logging, remove some dead code. (#6205)

==

examples/lightning_base.py
examples/seq2seq/callbacks.py
examples/seq2seq/train_mbart_cc25_enro.sh
==================
06f1692b0;Maurice Gonzenbach;2020-08-03 16:21:23 +0200;Fix _shift_right function in TFT5PreTrainedModel (#6214)

==

src/transformers/modeling_tf_t5.py
==================
0b4186735;Suraj Patil;2020-08-03 19:49:35 +0530;fix labels (#6213)

==

src/transformers/data/data_collator.py
==================
cedc547e7;Jay Mody;2020-08-03 09:00:39 -0400;Adds train_batch_size, eval_batch_size, and n_gpu to to_sanitized_dict output for logging. (#5331)
* Adds train_batch_size, eval_batch_size, and n_gpu to to_sanitized_dict() output

* Update wandb config logging to use to_sanitized_dict

* removed n_gpu from sanitized dict

* fix quality check errors
==

src/transformers/trainer.py
src/transformers/training_args.py
==================
9996f697e;Julien Plu;2020-08-03 14:10:40 +0200;Fix saved model creation (#5468)
* Fix TF Serving when output_hidden_states and output_attentions are True

* Add tests for saved model creation + bug fix for multiple choices models

* remove unused import

* Fix the input for several layers

* Fix test

* Fix conflict printing

* Apply style

* Fix XLM and Flaubert for TensorFlow

* Apply style

* Fix TF check version

* Apply style

* Trigger CI
==

src/transformers/modeling_tf_albert.py
src/transformers/modeling_tf_bert.py
src/transformers/modeling_tf_ctrl.py
src/transformers/modeling_tf_distilbert.py
src/transformers/modeling_tf_electra.py
src/transformers/modeling_tf_flaubert.py
src/transformers/modeling_tf_gpt2.py
src/transformers/modeling_tf_mobilebert.py
src/transformers/modeling_tf_openai.py
src/transformers/modeling_tf_roberta.py
src/transformers/modeling_tf_t5.py
src/transformers/modeling_tf_transfo_xl.py
src/transformers/modeling_tf_transfo_xl_utilities.py
src/transformers/modeling_tf_utils.py
src/transformers/modeling_tf_xlm.py
src/transformers/modeling_tf_xlnet.py
src/transformers/trainer_tf.py
tests/test_modeling_tf_common.py
tests/test_modeling_tf_xlnet.py
==================
5a0dac53b;Teven;2020-08-03 10:19:03 +0200;Empty assert hunt (#6056)
* Fixed empty asserts

* black-reformatted stragglers in templates

* More code quality checks

* Update src/transformers/convert_marian_to_pytorch.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/convert_marian_to_pytorch.py

Co-authored-by: Sam Shleifer <sshleifer@gmail.com>

* removed unused line as per @sshleifer

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
Co-authored-by: Sam Shleifer <sshleifer@gmail.com>
==

src/transformers/commands/train.py
src/transformers/convert_marian_to_pytorch.py
src/transformers/data/datasets/language_modeling.py
src/transformers/data/metrics/__init__.py
src/transformers/data/metrics/squad_metrics.py
src/transformers/data/processors/utils.py
src/transformers/data/processors/xnli.py
src/transformers/modeling_albert.py
src/transformers/modeling_bert.py
src/transformers/modeling_electra.py
src/transformers/modeling_gpt2.py
src/transformers/modeling_mobilebert.py
src/transformers/modeling_openai.py
src/transformers/modeling_t5.py
src/transformers/modeling_tf_albert.py
src/transformers/modeling_tf_distilbert.py
src/transformers/modeling_tf_flaubert.py
src/transformers/modeling_tf_openai.py
src/transformers/modeling_tf_xlnet.py
src/transformers/modeling_transfo_xl.py
src/transformers/modeling_xlnet.py
src/transformers/tokenization_transfo_xl.py
src/transformers/tokenization_utils_base.py
src/transformers/trainer.py
templates/adding_a_new_example_script/utils_xxx.py
templates/adding_a_new_model/modeling_xxx.py
==================
16c224016;Martin M√ºller;2020-08-03 09:53:38 +0200;Add script to convert tf2.x checkpoint to PyTorch (#5791)
* Add script to convert tf2.x checkpoint to pytorch

The script converts the newer TF2.x checkpoints (as published on their official GitHub: https://github.com/tensorflow/models/tree/master/official/nlp/bert) to Pytorch.

* rename file in order to stay consistent with naming convention
==

src/transformers/convert_bert_original_tf2_checkpoint_to_pytorch.py
==================
82a0e2b67;Philip May;2020-08-02 09:58:26 +0200;Fix docstring for BertTokenizerFast (#6185)
- remove duplicate doc-entry for tokenize_chinese_chars
- add doc for strip_accents and wordpieces_prefix
==

src/transformers/tokenization_bert.py
==================
d8dbf3b75;Stas Bekman;2020-08-01 11:51:07 -0700;[s2s] clean up + doc (#6184)
Co-authored-by: Sam Shleifer <sshleifer@gmail.com>
==

examples/seq2seq/README.md
examples/seq2seq/finetune.sh
==================
a39dfe4fb;Faiaz Rahman;2020-08-01 03:20:48 -0700;Fixed typo in Longformer (#6180)

==

docs/source/model_doc/longformer.rst
==================
8edfaaa81;Joe Davison;2020-07-31 10:56:32 -0400;bart-large-mnli-yahoo-answers model card (#6133)
* Add bart-large-mnli-yahoo-answers model card

* Add examples

* Add widget example

* Rm bart tag

Co-authored-by: Julien Chaumond <chaumond@gmail.com>

Co-authored-by: Julien Chaumond <chaumond@gmail.com>
==

model_cards/joeddav/bart-large-mnli-yahoo-answers/README.md
==================
d951c14ae;Sylvain Gugger;2020-07-31 09:44:37 -0400;Model output test (#6155)
* Use return_dict=True in all tests

* Formatting
==

src/transformers/modeling_encoder_decoder.py
src/transformers/modeling_openai.py
src/transformers/modeling_reformer.py
templates/adding_a_new_model/tests/test_modeling_xxx.py
tests/test_modeling_albert.py
tests/test_modeling_bart.py
tests/test_modeling_bert.py
tests/test_modeling_camembert.py
tests/test_modeling_common.py
tests/test_modeling_ctrl.py
tests/test_modeling_distilbert.py
tests/test_modeling_dpr.py
tests/test_modeling_electra.py
tests/test_modeling_flaubert.py
tests/test_modeling_gpt2.py
tests/test_modeling_longformer.py
tests/test_modeling_mbart.py
tests/test_modeling_mobilebert.py
tests/test_modeling_openai.py
tests/test_modeling_reformer.py
tests/test_modeling_roberta.py
tests/test_modeling_t5.py
tests/test_modeling_transfo_xl.py
tests/test_modeling_xlm.py
tests/test_modeling_xlm_roberta.py
tests/test_modeling_xlnet.py
==================
86caab1e0;Sylvain Gugger;2020-07-31 09:43:23 -0400;Harmonize both Trainers API (#6157)
* Harmonize both Trainers API

* Fix test

* main_prcess -> process_zero
==

docs/source/main_classes/trainer.rst
src/transformers/trainer.py
src/transformers/trainer_tf.py
==================
603cd81a0;Mehrdad Farahani;2020-07-31 12:19:06 +0200;readme m3hrdadfi/albert-fa-base-v2 (#6153)
* readme m3hrdadfi/albert-fa-base-v2

model_card readme for m3hrdadfi/albert-fa-base-v2

* Update model_cards/m3hrdadfi/albert-fa-base-v2/README.md

Co-authored-by: Julien Chaumond <chaumond@gmail.com>
==

model_cards/m3hrdadfi/albert-fa-base-v2/README.md
==================
838dc06ff;Suraj Patil;2020-07-31 14:14:23 +0530;parse arguments from dict (#4869)
* add parse_dict to parse arguments from dict

* add unit test for parse_dict
==

src/transformers/hf_argparser.py
tests/test_hf_argparser.py
==================
cf3cf304c;Paul O'Leary McCann;2020-07-31 17:41:14 +0900;Replace mecab-python3 with fugashi for Japanese tokenization (#6086)
* Replace mecab-python3 with fugashi

This replaces mecab-python3 with fugashi for Japanese tokenization. I am
the maintainer of both projects.

Both projects are MeCab wrappers, so the underlying C++ code is the
same. fugashi is the newer wrapper and doesn't use SWIG, so for basic
use of the MeCab API it's easier to use.

This code insures the use of a version of ipadic installed via pip,
which should make versioning and tracking down issues easier.

fugashi has wheels for Windows, OSX, and Linux, which will help with
issues with installing old versions of mecab-python3 on Windows.
Compared to mecab-python3, because fugashi doesn't use SWIG, it doesn't
require a C++ runtime to be installed on Windows.

In adding this change I removed some code dealing with `cursor`,
`token_start`, and `token_end` variables. These variables didn't seem to
be used for anything, it is unclear to me why they were there.

I ran the tests and they passed, though I couldn't figure out how to run
the slow tests (`--runslow` gave an error) and didn't try testing with
Tensorflow.

* Style fix

* Remove unused variable

Forgot to delete this...

* Adapt doc with install instructions

* Fix typo

Co-authored-by: sgugger <sylvain.gugger@gmail.com>
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

.circleci/config.yml
docs/source/pretrained_models.rst
setup.cfg
setup.py
src/transformers/tokenization_bert_japanese.py
==================
f250beb8a;Stas Bekman;2020-07-31 01:34:46 -0700;enable easy checkout switch (#5645)
* enable easy checkout switch

allow having multiple repository checkouts and not needing to remember to rerun 'pip install -e .[dev]' when switching between checkouts and running tests.

* make isort happy

* examples needs one too
==

examples/conftest.py
tests/conftest.py
==================
7d50af4b0;kolk;2020-07-31 13:58:35 +0530;Create README.md (#6169)

==

model_cards/deepset/minilm-uncased-squad2/README.md
==================
0034a1d24;Prajjwal Bhargava;2020-07-31 13:53:29 +0530;Add Pytorch Native AMP support in Trainer (#6151)
* fixed type; add Pytorch Native CUDA AMP support

* reverted commit on modeling_utils

* confirming to HF black formatting rule

* changed bool value of _use_apex

* scaler support for gradient clipping

* fix inplace operation of clip_grad_norm

* removed not while version comparison
==

src/transformers/trainer.py
==================
7231f7b50;Funtowicz Morgan;2020-07-31 09:45:13 +0200;Enable ONNX/ONNXRuntime optimizations through converter script (#6131)
* Add onnxruntime transformers optimization support

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Added Optimization section in ONNX/ONNXRuntime documentation.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Improve note reference

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Fixing imports order.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Add warning about different level of optimization between torch and tf export.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Address @LysandreJik wording suggestion

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* Address @LysandreJik wording suggestion

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* Always optimize model before quantization for maximum performances.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Address comments on the documentation.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Improve TensorFlow optimization message as suggested by @yufenglee

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Removed --optimize parameter

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Warn the user about current quantization limitation when model is larger than 2GB.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Trigger CI for last check

* Small change in print for the optimization section.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
==

docs/source/serialization.rst
src/transformers/convert_graph_to_onnx.py
==================
c0b93a1c7;Stas Bekman;2020-07-30 15:00:02 -0700;correct the correction (#6163)

==

src/transformers/benchmark/benchmark_utils.py
==================
a2f6d521c;Stas Bekman;2020-07-30 14:18:27 -0700;typos (#6162)
* 2 small typos

* more typos

* correct path
==

src/transformers/benchmark/benchmark_utils.py
tests/test_benchmark.py
==================
f3065abdb;Sylvain Gugger;2020-07-30 14:51:19 -0400;Doc tokenizer (#6110)
* Start doc tokenizers

* Tokenizer documentation

* Start doc tokenizers

* Tokenizer documentation

* Formatting after rebase

* Formatting after merge

* Update docs/source/main_classes/tokenizer.rst

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* Address comment

* Update src/transformers/tokenization_utils_base.py

Co-authored-by: Thomas Wolf <thomwolf@users.noreply.github.com>

* Address Thom's comments

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
Co-authored-by: Thomas Wolf <thomwolf@users.noreply.github.com>
==

docs/source/index.rst
docs/source/internal/tokenization_utils.rst
docs/source/main_classes/tokenizer.rst
src/transformers/modeling_utils.py
src/transformers/tokenization_utils.py
src/transformers/tokenization_utils_base.py
src/transformers/tokenization_utils_fast.py
==================
e642c7890;guillaume-be;2020-07-30 20:11:39 +0200;Addition of a DialoguePipeline (#5516)
* initial commit for pipeline implementation

Addition of input processing and history concatenation

* Conversation pipeline tested and working for single & multiple conversation inputs

* Added docstrings for dialogue pipeline

* Addition of dialogue pipeline integration tests

* Delete test_t5.py

* Fixed max code length

* Updated styling

* Fixed test broken by formatting tools

* Removed unused import

* Added unit test for DialoguePipeline

* Fixed Tensorflow compatibility

* Fixed multi-framework support using framework flag

* - Fixed docstring
- Added `min_length_for_response` as an initialization parameter
- Renamed `*args` to `conversations`, `conversations` being a `Conversation` or a `List[Conversation]`
- Updated truncation to truncate entire segments of conversations, instead of cutting in the middle of a user/bot input

* - renamed pipeline name from dialogue to conversational
- removed hardcoded default value of 1000 and use config.max_length instead
- added `append_response` and `set_history` method to the Conversation class to avoid direct fields mutation
- fixed bug in history truncation method

* - Updated ConversationalPipeline to accept only active conversations (otherwise a ValueError is raised)

* - Simplified input tensor conversion

* - Updated attention_mask value for Tensorflow compatibility

* - Updated last dialogue reference to conversational & fixed integration tests

* Fixed conflict with master

* Updates following review comments

* Updated formatting

* Added Conversation and ConversationalPipeline to the library __init__, addition of docstrings for Conversation, added both to the docs

* Update src/transformers/pipelines.py

Updated docsting following review

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

docs/source/main_classes/pipelines.rst
src/transformers/__init__.py
src/transformers/pipelines.py
tests/test_pipelines.py
==================
ec0267475;Lysandre Debut;2020-07-30 11:11:48 -0400;Fix FlauBERT GPU test (#6142)
* Fix GPU test

* Remove legacy constructor
==

src/transformers/modeling_flaubert.py
==================
91cb95461;Sylvain Gugger;2020-07-30 09:17:00 -0400;Switch from return_tuple to return_dict (#6138)
* Switch from return_tuple to return_dict

* Fix test

* [WIP] Test TF Flaubert + Add {XLM, Flaubert}{TokenClassification, MultipleC‚Ä¶ (#5614)

* Test TF Flaubert + Add {XLM, Flaubert}{TokenClassification, MultipleChoice} models and tests

* AutoModels


Tiny tweaks

* Style

* Final changes before merge

* Re-order for simpler review

* Final fixes

* Addressing @sgugger's comments

* Test MultipleChoice

* Rework TF trainer (#6038)

* Fully rework training/prediction loops

* fix method name

* Fix variable name

* Fix property name

* Fix scope

* Fix method name

* Fix tuple index

* Fix tuple index

* Fix indentation

* Fix variable name

* fix eval before log

* Add drop remainder for test dataset

* Fix step number + fix logging datetime

* fix eval loss value

* use global step instead of step + fix logging at step 0

* Fix logging datetime

* Fix global_step usage

* Fix breaking loop + logging datetime

* Fix step in prediction loop

* Fix step breaking

* Fix train/test loops

* Force TF at least 2.2 for the trainer

* Use assert_cardinality to facilitate the dataset size computation

* Log steps per epoch

* Make tfds compliant with TPU

* Make tfds compliant with TPU

* Use TF dataset enumerate instead of the Python one

* revert previous commit

* Fix data_dir

* Apply style

* rebase on master

* Address Sylvain's comments

* Address Sylvain's and Lysandre comments

* Trigger CI

* Remove unused import

* Switch from return_tuple to return_dict

* Fix test

* Add recent model

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
Co-authored-by: Julien Plu <plu.julien@gmail.com>
==

docs/source/quicktour.rst
docs/source/training.rst
examples/question-answering/run_squad.py
examples/seq2seq/test_seq2seq_examples.py
src/transformers/configuration_utils.py
src/transformers/file_utils.py
src/transformers/modeling_albert.py
src/transformers/modeling_bart.py
src/transformers/modeling_bert.py
src/transformers/modeling_camembert.py
src/transformers/modeling_ctrl.py
src/transformers/modeling_distilbert.py
src/transformers/modeling_dpr.py
src/transformers/modeling_electra.py
src/transformers/modeling_encoder_decoder.py
src/transformers/modeling_flaubert.py
src/transformers/modeling_gpt2.py
src/transformers/modeling_longformer.py
src/transformers/modeling_mmbt.py
src/transformers/modeling_mobilebert.py
src/transformers/modeling_openai.py
src/transformers/modeling_outputs.py
src/transformers/modeling_reformer.py
src/transformers/modeling_roberta.py
src/transformers/modeling_t5.py
src/transformers/modeling_transfo_xl.py
src/transformers/modeling_utils.py
src/transformers/modeling_xlm.py
src/transformers/modeling_xlm_roberta.py
src/transformers/modeling_xlnet.py
src/transformers/pipelines.py
src/transformers/trainer.py
templates/adding_a_new_model/modeling_xxx.py
tests/test_modeling_common.py
tests/test_modeling_t5.py
==================
562b6369c;Sylvain Gugger;2020-07-30 09:13:16 -0400;Tf trainer cleanup (#6143)
* Clean up TFTrainer

* Add import

* Fix conflicts
==

src/transformers/trainer_tf.py
==================
c127d055e;Oren Amsalem;2020-07-30 15:53:35 +0300;add another e.g. to avoid confusion (#6055)

==

src/transformers/modeling_tf_pytorch_utils.py
==================
d24ea708d;Oren Amsalem;2020-07-30 13:13:29 +0300;Actually the extra_id are from 0-99 and not from 1-100 (#5967)
a = tokenizer.encode("we got a <extra_id_99>", return_tensors='pt',add_special_tokens=True)
print(a)
>tensor([[   62,   530,     3,     9, 32000]])
a = tokenizer.encode("we got a <extra_id_100>", return_tensors='pt',add_special_tokens=True)
print(a)
>tensor([[   62,   530,     3,     9,     3,     2, 25666,   834,    23,    26,
           834,  2915,  3155]])
==

docs/source/model_doc/t5.rst
==================
3212b8850;Stas Bekman;2020-07-29 22:09:46 -0700;[s2s] add support for overriding config params (#6149)

==

examples/lightning_base.py
examples/seq2seq/README.md
examples/seq2seq/finetune.sh
examples/seq2seq/test_seq2seq_examples.py
==================
54f9fbeff;Julien Plu;2020-07-29 20:32:01 +0200;Rework TF trainer (#6038)
* Fully rework training/prediction loops

* fix method name

* Fix variable name

* Fix property name

* Fix scope

* Fix method name

* Fix tuple index

* Fix tuple index

* Fix indentation

* Fix variable name

* fix eval before log

* Add drop remainder for test dataset

* Fix step number + fix logging datetime

* fix eval loss value

* use global step instead of step + fix logging at step 0

* Fix logging datetime

* Fix global_step usage

* Fix breaking loop + logging datetime

* Fix step in prediction loop

* Fix step breaking

* Fix train/test loops

* Force TF at least 2.2 for the trainer

* Use assert_cardinality to facilitate the dataset size computation

* Log steps per epoch

* Make tfds compliant with TPU

* Make tfds compliant with TPU

* Use TF dataset enumerate instead of the Python one

* revert previous commit

* Fix data_dir

* Apply style

* rebase on master

* Address Sylvain's comments

* Address Sylvain's and Lysandre comments

* Trigger CI

* Remove unused import
==

examples/README.md
examples/multiple-choice/utils_multiple_choice.py
examples/question-answering/run_tf_squad.py
examples/text-classification/run_tf_glue.py
examples/token-classification/run_tf_ner.py
examples/token-classification/utils_ner.py
src/transformers/modeling_tf_utils.py
src/transformers/trainer_tf.py
src/transformers/training_args_tf.py
==================
3f94170a1;Lysandre Debut;2020-07-29 14:26:26 -0400;[WIP] Test TF Flaubert + Add {XLM, Flaubert}{TokenClassification, MultipleC‚Ä¶ (#5614)
* Test TF Flaubert + Add {XLM, Flaubert}{TokenClassification, MultipleChoice} models and tests

* AutoModels


Tiny tweaks

* Style

* Final changes before merge

* Re-order for simpler review

* Final fixes

* Addressing @sgugger's comments

* Test MultipleChoice
==

src/transformers/__init__.py
src/transformers/modeling_auto.py
src/transformers/modeling_flaubert.py
src/transformers/modeling_tf_flaubert.py
src/transformers/modeling_tf_xlm.py
src/transformers/modeling_xlm.py
tests/test_modeling_common.py
tests/test_modeling_flaubert.py
tests/test_modeling_tf_common.py
tests/test_modeling_tf_flaubert.py
tests/test_modeling_tf_xlm.py
tests/test_modeling_xlm.py
==================
8a8ae2761;Sylvain Gugger;2020-07-29 12:28:12 -0400;Use google style to document properties (#6130)
* Use google style to document properties

* Update src/transformers/configuration_utils.py

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
==

src/transformers/configuration_utils.py
src/transformers/modeling_tf_utils.py
src/transformers/modeling_utils.py
==================
fc64559c4;Julien Plu;2020-07-29 18:20:00 +0200;Fix TF CTRL model naming (#6134)

==

setup.py
src/transformers/modeling_tf_ctrl.py
==================
641b873c1;Lysandre Debut;2020-07-29 11:38:15 -0400;XLNet PLM Readme (#6121)

==

examples/language-modeling/README.md
==================
8d157c930;Timo Moeller;2020-07-29 17:34:16 +0200;add deepset/xlm-roberta-large-squad2 model card (#6128)
* Add xlm-r QA model card

* Add tags
==

model_cards/deepset/xlm-roberta-large-squad2/README.md
==================
6c002853a;Funtowicz Morgan;2020-07-29 13:21:29 +0200;Added capability to quantize a model while exporting through ONNX. (#6089)
* Added capability to quantize a model while exporting through ONNX.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

We do not support multiple extensions

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Reformat files

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* More quality

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Ensure test_generate_identified_name compares the same object types

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Added documentation everywhere on ONNX exporter

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Use pathlib.Path instead of plain-old string

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Use f-string everywhere

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Use the correct parameters for black formatting

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Use Python 3 super() style.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Use packaging.version to ensure installed onnxruntime version match requirements

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Fixing imports sorting order.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Missing raise(s)

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Added quantization documentation

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Fix some spelling.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Fix bad list header format

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>
==

docs/source/serialization.rst
src/transformers/convert_graph_to_onnx.py
tests/test_onnx.py
==================
25de74ccf;Sylvain Gugger;2020-07-29 05:20:53 -0400;Use FutureWarning to deprecate (#6111)

==

src/transformers/modeling_albert.py
src/transformers/tokenization_utils_base.py
==================
640550fc7;Funtowicz Morgan;2020-07-29 11:02:35 +0200;ONNX documentation (#5992)
* Move torchscript and add ONNX documentation under modle_export

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* Let's follow guidelines by the gurus: Renamed torchscript.rst to serialization.rst

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* Remove previously introduced tree element

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* WIP doc

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* ONNX documentation

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Fix invalid link

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Improve spelling

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Final wording pass

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>
==

docs/source/index.rst
docs/source/serialization.rst
==================
92f8ce2ed;Sam Shleifer;2020-07-28 18:30:16 -0400;Fix deebert tests (#6102)

==

examples/deebert/test_glue_deebert.py
==================
c49cd927f;Sam Shleifer;2020-07-28 18:29:35 -0400;[Fix] position_ids tests again (#6100)

==

src/transformers/modeling_bert.py
tests/test_modeling_auto.py
==================
40796c580;Sam Shleifer;2020-07-28 18:29:18 -0400;[fix] add bart to LM_MAPPING (#6099)

==

src/transformers/modeling_auto.py
==================
5abe50381;Sam Shleifer;2020-07-28 18:27:58 -0400;Fix #6096: MBartTokenizer's mask token (#6098)

==

src/transformers/tokenization_bart.py
tests/test_modeling_mbart.py
tests/test_tokenization_mbart.py
==================
b1c8b7690;Joe Davison;2020-07-28 14:46:03 -0400;Fix zero-shot pipeline single seq output shape (#6104)

==

src/transformers/pipelines.py
==================
06834bc33;Lysandre Debut;2020-07-28 12:44:25 -0400;Logs should not be hidden behind a logger.info (#6097)

==

src/transformers/trainer.py
==================
dafa296c9;Sam Shleifer;2020-07-28 11:24:23 -0400;[s2s] Delete useless method, log tokens_per_batch (#6081)

==

examples/seq2seq/finetune.py
examples/seq2seq/utils.py
==================
dc4755c6d;Tanmay Thakur;2020-07-28 19:30:23 +0530;create model-card for lordtt13/emo-mobilebert (#6030)

==

model_cards/lordtt13/emo-mobilebert/README.md
==================
28931f81b;Sylvain Gugger;2020-07-28 09:48:39 -0400;Fix #6092 (#6093)
* Fix #6092

* Format
==

src/transformers/data/data_collator.py
==================
5e97c8294;Manuel Romero;2020-07-28 15:36:00 +0200;Create README.md (#6076)

==

model_cards/mrm8488/gpt2-finetuned-recipes-cooking_v2/README.md
==================
54f49af4a;Clement;2020-07-28 09:14:00 -0400;Add inference widget examples (#5825)

==

README.md
==================
0206efb4c;Sylvain Gugger;2020-07-28 09:08:20 -0400;Make all data collators accept dict (#6065)
* Make all data collators accept dict

* Style
==

src/transformers/data/data_collator.py
==================
31a5486e4;Sam Shleifer;2020-07-28 08:41:27 -0400;github issue template suggests who to tag (#5790)
Co-authored-by: Julien Chaumond <chaumond@gmail.com>
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
Co-authored-by: Teven <teven.lescao@gmail.com>
==

.github/ISSUE_TEMPLATE/bug-report.md
==================
f0c70085c;Stas Bekman;2020-07-28 05:34:58 -0700;link to README.md (#6068)
* add a link to README.md

* Update README.md
==

examples/seq2seq/README.md
==================
4f814fd58;Pavel Soriano;2020-07-28 14:33:52 +0200;[Model Card] camembert-base-squadFR-fquad-piaf (#6087)

==

model_cards/etalab-ia/camembert-base-squadFR-fquad-piaf/README.md
==================
3c7fbf35a;Sam Shleifer;2020-07-28 08:18:11 -0400;MBART: support summarization tasks where max_src_len > max_tgt_len (#6003)
* MBART: support summarization tasks

* fix test

* Style

* add tokenizer test
==

examples/seq2seq/README.md
examples/seq2seq/finetune.py
examples/seq2seq/finetune_t5.sh
examples/seq2seq/test_seq2seq_examples.py
examples/seq2seq/utils.py
src/transformers/tokenization_bart.py
tests/test_tokenization_mbart.py
==================
842eb4560;Tanmay Thakur;2020-07-28 13:55:12 +0530;New Community NB Add (#5824)
Signed-off-by: lordtt13 <thakurtanmay72@yahoo.com>
==

notebooks/README.md
==================
018d61fa2;Andr√©s Felipe Cruz;2020-07-28 01:19:17 -0700;Moving transformers package import statements to relative imports in some files (#5796)
* Moving rom transformers statements to relative imports in some files under src/

* Import order

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
==

src/transformers/configuration_encoder_decoder.py
src/transformers/modeling_encoder_decoder.py
src/transformers/modeling_marian.py
src/transformers/modeling_mobilebert.py
src/transformers/modeling_tf_electra.py
src/transformers/testing_utils.py
src/transformers/tokenization_auto.py
==================
7214954db;Lysandre Debut;2020-07-28 03:14:31 -0400;Should return a tuple for serialization (#6061)

==

src/transformers/pipelines.py
==================
7a68d4013;Sam Shleifer;2020-07-27 20:07:21 -0400;[s2s] Don't mention packed data in README (#6079)

==

examples/seq2seq/README.md
==================
b7345d22d;Sam Shleifer;2020-07-27 20:00:44 -0400;[fix] no warning for position_ids buffer (#6063)

==

src/transformers/modeling_bert.py
src/transformers/modeling_mobilebert.py
src/transformers/modeling_openai.py
src/transformers/modeling_xlm.py
==================
1e00ef681;Sam Shleifer;2020-07-27 18:26:00 -0400;[s2s] dont document packing because it hurts performance (#6077)

==

examples/seq2seq/README.md
==================
9d0d3a664;sgugger;2020-07-27 18:03:09 -0400;Pin TF while we wait for a fix

==

setup.py
==================
769e6ba01;Ramsri Goutham Golla;2020-07-28 01:55:37 +0530;Create README.md (#6032)
Adding model card - readme
==

model_cards/ramsrigouthamg/t5_paraphraser/README.md
==================
fd347e0da;Sylvain Gugger;2020-07-27 15:17:33 -0400;Add fire to setup.cfg to make isort happy (#6066)

==

setup.cfg
==================
11792d782;Sam Shleifer;2020-07-27 12:21:25 -0400;CL util to convert models to fp16 before upload (#5953)

==

examples/seq2seq/convert_model_to_fp16.py
==================
4302ace5b;Sam Shleifer;2020-07-27 12:14:23 -0400;[pack_dataset] don't sort before packing, only pack train (#5954)

==

examples/requirements.txt
examples/seq2seq/minify_dataset.py
examples/seq2seq/pack_dataset.py
==================
c8bdf7f4e;Suraj Patil;2020-07-27 21:20:08 +0530;Add new AutoModel classes in pipeline (#6062)
* use new AutoModel classed

* make style and quality
==

src/transformers/pipelines.py
==================
5779e5434;Cola;2020-07-27 23:55:15 +0900;:pencil2: Fix typo (#5734)

==

src/transformers/trainer_tf.py
==================
d1d15d6f2;Suraj Patil;2020-07-27 19:40:43 +0530;[examples (seq2seq)] fix preparing decoder_input_ids for T5 (#5994)

==

examples/seq2seq/finetune.py
==================
3deffc1d6;Joe Davison;2020-07-27 07:42:58 -0600;Zero shot classification pipeline (#5760)
* add initial zero-shot pipeline

* change default args

* update default template

* add label string splitting

* add str labels support, remove nli from name

* style

* add input validation and working tf defaults

* tests

* quality check

* add docstring to __call__

* add slow tests

* Change truncation to only_first

also lower precision on tests for readibility

* style
==

src/transformers/pipelines.py
tests/test_pipelines.py
==================
1246b20f6;Sylvain Gugger;2020-07-27 09:18:59 -0400;Fix the return documentation rendering for all model outputs (#6022)
* Fix the return documentation rendering for all model outputs

* Formatting
==

src/transformers/file_utils.py
src/transformers/modeling_transfo_xl.py
==================
3b64ad5d5;Sylvain Gugger;2020-07-27 08:31:24 -0400;Remove unused file (#6023)

==

deploy_multi_version_doc.sh
==================
b9b11795c;Xin Wen;2020-07-27 17:34:02 +0800;Update model_summary.rst (#5737)
Add '-' to make the reference of Transformer-XL more accurate and formal.
==

docs/source/model_summary.rst
==================
b21993b36;Gong Linyuan;2020-07-27 17:31:37 +0800;Allow to set Adam beta1, beta2 in TrainingArgs (#5592)
* Add Adam beta1, beta2 to trainier

* Make style consistent
==

src/transformers/optimization_tf.py
src/transformers/trainer.py
src/transformers/trainer_tf.py
src/transformers/training_args.py
==================
7969e96f4;Pavel Soriano;2020-07-27 11:15:08 +0200;draft etalab QA model (#6040)

==

model_cards/etalab-ia/camembert-base-squadFR-fquad-piaf/README.md
==================
a9585fd10;Vamsi995;2020-07-27 14:42:45 +0530;Model card for Vamsi/T5_Paraphrase_Paws (#6037)
* Model card for Vamsi/T5_Paraphrase_Paws

* Update model_cards/Vamsi/T5_Paraphrase_Paws/README.md

Co-authored-by: Julien Chaumond <chaumond@gmail.com>
==

model_cards/Vamsi/T5_Paraphrase_Paws/README.md
==================
f7f03b22d;Rodolfo De Nadai;2020-07-26 18:31:49 -0300;Update README.md of my model (#6042)

==

model_cards/rdenadai/BR_BERTo/README.md
==================
fb0589a03;Stas Bekman;2020-07-26 11:29:54 -0700;don't complain about missing W&B when WANDB_DISABLED=true (#6036)
* don't complain about missing W&B when WANDB_DISABLED=true

* reformat to elif

* typo
==

src/transformers/trainer.py
src/transformers/trainer_tf.py
==================
daa5dd120;Stas Bekman;2020-07-26 11:09:14 -0700;add a summary report flag for run_examples on CI (#6035)
Currently, it's hard to derive which example tests were run on CI, and which weren't. Adding `-rA` flag to `pytest`, will now include a summary like:

```
==================================================================== short test summary info =====================================================================
PASSED examples/test_examples.py::ExamplesTests::test_generation
PASSED examples/test_examples.py::ExamplesTests::test_run_glue
PASSED examples/test_examples.py::ExamplesTests::test_run_language_modeling
PASSED examples/test_examples.py::ExamplesTests::test_run_squad
FAILED examples/test_examples.py::ExamplesTests::test_run_pl_glue - AttributeError: 'Namespace' object has no attribute 'gpus'
============================================================ 1 failed, 4 passed, 8 warnings in 42.96s ============================================================
```
which makes it easier to validate whether some example is being covered by CI or not.
==

.circleci/config.yml
==================
c69ea5efc;Sam Shleifer;2020-07-24 15:34:16 -0400;[CI] Don't test apex (#6021)

==

examples/seq2seq/test_bash_script.py
examples/seq2seq/test_seq2seq_examples.py
==================
a884b7fa3;Sylvain Gugger;2020-07-24 14:15:37 -0400;Update the  new model template (#6019)

==

templates/adding_a_new_model/README.md
templates/adding_a_new_model/configuration_xxx.py
templates/adding_a_new_model/modeling_tf_xxx.py
templates/adding_a_new_model/modeling_xxx.py
templates/adding_a_new_model/tokenization_xxx.py
==================
295466aae;Julien Chaumond;2020-07-24 14:14:10 -0400;[model_card] Sample input for rdenadai/BR_BERTo
cc @rdenadai
==

model_cards/rdenadai/BR_BERTo/README.md
==================
518361d69;Manuel Romero;2020-07-24 20:12:29 +0200;Create model card for RuPERTa-base (#6016)
* Update README.md

* Update model_cards/mrm8488/RuPERTa-base/README.md

Co-authored-by: Julien Chaumond <chaumond@gmail.com>

Co-authored-by: Julien Chaumond <chaumond@gmail.com>
==

model_cards/mrm8488/RuPERTa-base/README.md
==================
bd51f0a7a;Manuel Romero;2020-07-24 20:12:14 +0200;Create README.md (#5952)

==

model_cards/mrm8488/mobilebert-uncased-finetuned-squadv1/README.md
==================
87a779dfa;Manuel Romero;2020-07-24 20:12:09 +0200;Create README.md (#5951)

==

model_cards/mrm8488/mobilebert-uncased-finetuned-squadv2/README.md
==================
d115872b3;Rodolfo De Nadai;2020-07-24 15:11:14 -0300;Create README.md (#6020)
* Create README.md

* Update README.md

* Update model_cards/rdenadai/BR_BERTo/README.md

Co-authored-by: Julien Chaumond <chaumond@gmail.com>

* Update model_cards/rdenadai/BR_BERTo/README.md

Co-authored-by: Julien Chaumond <chaumond@gmail.com>
==

model_cards/rdenadai/BR_BERTo/README.md
==================
3996041d0;Sylvain Gugger;2020-07-24 10:04:25 -0400;Fix question template (#6014)

==

.github/ISSUE_TEMPLATE/question-help.md
==================
778e635fc;Victor SANH;2020-07-24 09:45:21 -0400;[model_cards] roberta-base-finetuned-yelp-polarity (#6009)
* [model_cards] roberta-base-finetuned-yelp-polarity

* Update model_cards/VictorSanh/roberta-base-finetuned-yelp-polarity/README.md

Co-authored-by: Julien Chaumond <chaumond@gmail.com>

Co-authored-by: Julien Chaumond <chaumond@gmail.com>
==

model_cards/VictorSanh/roberta-base-finetuned-yelp-polarity/README.md
==================
614fef169;Funtowicz Morgan;2020-07-24 15:37:52 +0200;Ensure OpenAI GPT position_ids is correctly initialized and registered at init. (#5773)
* Ensure OpenAI GPT position_ids is correctly initialized and registered as buffer at init.

This will make it compatible with TorchScript export.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Fix missing slice operator on the tensor data accessor.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Style.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Fixed BertEmbedding position_ids buffer created at forward.

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* Fixed MobileBertEmbedding position_ids buffer created at forward.

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* Fixed XLM position_ids buffer created at forward.

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>
==

src/transformers/modeling_bert.py
src/transformers/modeling_mobilebert.py
src/transformers/modeling_openai.py
src/transformers/modeling_xlm.py
==================
3b44aa935;Sylvain Gugger;2020-07-24 09:16:28 -0400;Model utils doc (#6005)
* Document TF modeling utils

* Document all model utils
==

docs/source/index.rst
docs/source/internal/modeling_utils.rst
docs/source/main_classes/model.rst
setup.cfg
src/transformers/configuration_utils.py
src/transformers/modeling_tf_utils.py
src/transformers/modeling_utils.py
==================
a54040521;sgugger;2020-07-24 09:07:40 -0400;Fix commit hash for stable doc

==

.circleci/deploy.sh
==================
fc0fe2a53;Qingqing Cao;2020-07-24 04:17:52 -0400;fix: model card readme clutter (#6008)
this removes the clutter line in the readme.md of model card `csarron/roberta-base-squad-v1`. It also fixes the result table.
==

model_cards/csarron/roberta-base-squad-v1/README.md
==================
f5b5c5bd7;Sylvain Gugger;2020-07-23 18:13:36 -0400;Avoid unnecessary warnings when loading pretrained model (#5922)
* Avoid unnecessary warnings when loading pretrained model

* Fix test

* Add other keys to ignore

* keys_to_ignore_at_load -> authorized_missing_keys
==

src/transformers/modeling_bart.py
src/transformers/modeling_gpt2.py
src/transformers/modeling_t5.py
src/transformers/modeling_utils.py
tests/test_modeling_gpt2.py
==================
29afb5764;Philip May;2020-07-23 23:56:45 +0200;Bert german dbmdz uncased sentence stsb (#6000)
* Describe usage of sentence model

* fix typo usage

* add use and description to readme

* fix typo in readme

* readme formatting

* add training procedure to readme

* description name and company

* readme formatting

* dataset training readme

* typo

* readme
==

model_cards/T-Systems-onsite/bert-german-dbmdz-uncased-sentence-stsb/README.md
==================
2b5ef9706;Qingqing Cao;2020-07-23 17:53:47 -0400;Model cards: add roberta-base-squad-v1 and bert-base-uncased-squad-v1 (#6006)
* add: bert-base-uncased-squad-v1

* add: roberta-base-squad-v1
==

model_cards/csarron/bert-base-uncased-squad-v1/README.md
model_cards/csarron/roberta-base-squad-v1/README.md
==================
9827d666e;Sam Shleifer;2020-07-23 15:41:14 -0400;MbartTokenizer: do not hardcode vocab size (#5998)

==

src/transformers/tokenization_bart.py
tests/test_tokenization_mbart.py
==================
6e1619551;Sylvain Gugger;2020-07-23 13:51:29 -0400;Fix #5974 (#5999)

==

src/transformers/modeling_transfo_xl.py
==================
e168488a7;Sylvain Gugger;2020-07-23 12:05:41 -0400;Cleanup Trainer and expose customization points (#5982)
* Clean up Trainer and expose customization points

* Formatting

* eval_step -> prediction_step
==

src/transformers/trainer.py
==================
76f52324b;Qingqing Cao;2020-07-23 11:57:29 -0400;add fine-tuned mobilebert squad v1 and squad v2 model cards (#5980)
* add mobilebert-uncased-squad-v2

* fix shell cmd, add creator info

* add mobilebert-uncased-squad-v1
==

model_cards/csarron/mobilebert-uncased-squad-v1/README.md
model_cards/csarron/mobilebert-uncased-squad-v2/README.md
==================
7e251ae03;GmailB;2020-07-23 17:41:33 +0200;Create README.md (#5989)

==

model_cards/unideeplearning/polibert_sa/README.md
==================
33d7506ea;Sylvain Gugger;2020-07-22 18:14:57 -0400;Update doc of the model page (#5985)

==

docs/source/main_classes/model.rst
src/transformers/modeling_tf_utils.py
src/transformers/modeling_utils.py
==================
c3206eef4;Sam Shleifer;2020-07-22 14:34:49 -0400;[test] partial coverage for train_mbart_enro_cc25.sh (#5976)

==

examples/seq2seq/README.md
examples/seq2seq/test_bash_script.py
examples/seq2seq/test_data/wmt_en_ro/test.source
examples/seq2seq/test_data/wmt_en_ro/test.target
examples/seq2seq/test_data/wmt_en_ro/train.source
examples/seq2seq/test_data/wmt_en_ro/train.target
examples/seq2seq/test_data/wmt_en_ro/val.source
examples/seq2seq/test_data/wmt_en_ro/val.target
examples/seq2seq/train_mbart_cc25_enro.sh
==================
2c0da7803;Stas Bekman;2020-07-22 10:22:34 -0700;minor doc fixes (#5831)
* minor doc fixes

correct superclass name and small grammar fixes

* correct the instance name in the error message

It appears to be `BaseTokenizer` from looking at:

`from tokenizers.implementations import BaseTokenizer as BaseTokenizerFast`

and not `Tokenizer` as it currently says.
==

src/transformers/tokenization_utils_fast.py
==================
feeb956a1;Sam Shleifer;2020-07-22 12:48:38 -0400;[docs] Add integration test example to copy pasta template (#5961)
Co-authored-by: Julien Chaumond <chaumond@gmail.com>
==

CONTRIBUTING.md
examples/deebert/test_glue_deebert.py
src/transformers/testing_utils.py
templates/adding_a_new_example_script/README.md
templates/adding_a_new_model/README.md
templates/adding_a_new_model/modeling_xxx.py
templates/adding_a_new_model/tests/test_modeling_xxx.py
templates/adding_a_new_model/tests/test_tokenization_xxx.py
==================
01116d3c5;Sam Shleifer;2020-07-22 11:38:37 -0400;T5 Model Cards (#5759)
* T5 Model Cards

* Fix paths

* Fix tags

* lang-en
==

model_cards/t5-11b-README.md
model_cards/t5-3b-README.md
model_cards/t5-base-README.md
model_cards/t5-large-README.md
model_cards/t5-small-README.md
==================
896300177;Funtowicz Morgan;2020-07-22 16:11:57 +0200;Expose padding_strategy on squad processor to fix QA pipeline performance regression (#5932)
* Attempt to fix the way squad_convert_examples_to_features pad the elements for the QA pipeline.

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* Quality

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* Make the code easier to read and avoid testing multiple test the same thing.

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* missing enum value on truncation_strategy.

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* Rethinking for the easiest fix: expose the padding strategy on squad_convert_examples_to_features.

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* Remove unused imports.

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>
==

src/transformers/data/processors/squad.py
src/transformers/pipelines.py
==================
ae67b2439;Sam Shleifer;2020-07-21 21:07:48 -0400;[CI] Install examples/requirements.txt (#5956)

==

.github/workflows/self-scheduled.yml
==================
e714412fe;Sylvain Gugger;2020-07-21 18:13:55 -0400;Update doc to new model outputs (#5946)
* Update doc to new model outputs

* Fix outputs in quicktour
==

docs/source/quicktour.rst
docs/source/task_summary.rst
docs/source/training.rst
src/transformers/file_utils.py
src/transformers/modeling_albert.py
src/transformers/modeling_bart.py
src/transformers/modeling_bert.py
src/transformers/modeling_distilbert.py
src/transformers/modeling_dpr.py
src/transformers/modeling_electra.py
src/transformers/modeling_gpt2.py
src/transformers/modeling_longformer.py
src/transformers/modeling_mobilebert.py
src/transformers/modeling_openai.py
src/transformers/modeling_xlm.py
src/transformers/modeling_xlnet.py
==================
ddd40b321;Sam Shleifer;2020-07-21 17:01:07 -0400;[CI] self-scheduled runner tests examples/ (#5927)

==

.github/workflows/self-scheduled.yml
==================
9dab39fee;Sam Shleifer;2020-07-21 16:58:45 -0400;seq2seq/run_eval.py can take decoder_start_token_id (#5949)

==

examples/seq2seq/finetune.py
examples/seq2seq/run_eval.py
src/transformers/tokenization_utils_base.py
==================
5b193b39b;Sam Shleifer;2020-07-21 16:51:39 -0400;[examples/seq2seq]: add --label_smoothing option (#5919)

==

examples/seq2seq/README.md
examples/seq2seq/callbacks.py
examples/seq2seq/finetune.py
examples/seq2seq/test_seq2seq_examples.py
examples/seq2seq/train_mbart_cc25_enro.sh
examples/seq2seq/train_mbart_cc25_enro_multigpu.sh
examples/seq2seq/utils.py
==================
95d1962b9;Sam Shleifer;2020-07-21 14:12:48 -0400;[Doc] explaining romanian postprocessing for MBART BLEU hacking (#5943)

==

examples/seq2seq/romanian_postprocessing.md
==================
604a2355d;Jannes;2020-07-21 19:28:22 +0200;Create README.md (#5876)

==

model_cards/jannesg/takalane_tso_roberta/README.md
==================
77c718ede;Jannes;2020-07-21 19:28:06 +0200;Create README.md (#5873)

==

model_cards/jannesg/takalane_sot_roberta/README.md
==================
325b277db;Jannes;2020-07-21 19:27:30 +0200;Create README.md (#5874)

==

model_cards/jannesg/takalane_ssw_roberta/README.md
==================
d15be2216;Jannes;2020-07-21 19:27:13 +0200;Create README.md (#5879)

==

model_cards/jannesg/takalane_zul_roberta/README.md
==================
f3e23dd90;Jannes;2020-07-21 19:20:47 +0200;Create README.md (#5878)

==

model_cards/jannesg/takalane_xho_roberta/README.md
==================
8b01d15c0;Jannes;2020-07-21 19:20:43 +0200;Create README.md (#5877)

==

model_cards/jannesg/takalane_ven_roberta/README.md
==================
05bddf304;Jannes;2020-07-21 19:20:32 +0200;Create README.md (#5875)

==

model_cards/jannesg/takalane_tsn_roberta/README.md
==================
783a0c7ee;Jannes;2020-07-21 19:20:21 +0200;Create README.md (#5872)

==

model_cards/jannesg/takalane_nso_roberta/README.md
==================
e7844d60c;Jannes;2020-07-21 19:19:48 +0200;Create README.md (#5871)

==

model_cards/jannesg/takalane_nbl_roberta/README.md
==================
b1ee69763;tuner007;2020-07-21 22:45:07 +0530;Create README.md (#5864)

==

model_cards/tuner007/t5_abs_qa/README.md
==================
5f809e497;Manuel Romero;2020-07-21 19:14:27 +0200;Update README.md (#5857)
Add nlp dataset used
==

model_cards/mrm8488/t5-small-finetuned-emotion/README.md
==================
4215f59c9;Manuel Romero;2020-07-21 19:11:08 +0200;Update README.md (#5856)
Add dataset used as it is now part of nlp package
==

model_cards/mrm8488/t5-base-finetuned-emotion/README.md
==================
1d72460d5;Ali Hamdi Ali Fadel;2020-07-21 19:54:29 +0300;Add ComVE model cards (#5884)
* Add ComVE model cards

* Apply suggestions from code review

Co-authored-by: Julien Chaumond <chaumond@gmail.com>
==

model_cards/aliosm/ComVE-distilgpt2/README.md
model_cards/aliosm/ComVE-gpt2-large/README.md
model_cards/aliosm/ComVE-gpt2-medium/README.md
model_cards/aliosm/ComVE-gpt2/README.md
==================
ccbf74a68;Aditya Soni;2020-07-21 19:14:59 +0530;typos in seq2seq/readme (#5937)

==

examples/seq2seq/README.md
==================
d32279438;BatJedi;2020-07-21 13:24:57 +0530;Created model card for my extreme summarization model (#5839)
* Created model card for my extreme summarization model

* Update model_cards/yuvraj/xSumm/README.md

Co-authored-by: Julien Chaumond <chaumond@gmail.com>
==

model_cards/yuvraj/xSumm/README.md
==================
abf5c56e9;BatJedi;2020-07-21 13:24:14 +0530;Created model card for my summarization model (#5838)
* Created model card for my summarization model

* Update model_cards/yuvraj/summarizer-cnndm/README.md

Co-authored-by: Julien Chaumond <chaumond@gmail.com>
==

model_cards/yuvraj/summarizer-cnndm/README.md
==================
d73baeebc;Manuel Romero;2020-07-21 09:52:52 +0200;Create README.md (#5921)
- Maybe the result of this query answers the question You did some days ago @julien-c ;-)
==

model_cards/mrm8488/t5-small-finetuned-wikiSQL/README.md
==================
50acfc871;Manuel Romero;2020-07-21 09:41:37 +0200;Create README.md (#5924)

==

model_cards/mrm8488/t5-base-finetuned-wikiSQL-sql-to-en/README.md
==================
724953340;Manuel Romero;2020-07-21 09:31:42 +0200;Create README.md (#5920)

==

model_cards/mrm8488/t5-base-finetuned-wikiSQL/README.md
==================
4781afd04;Sylvain Gugger;2020-07-20 19:47:06 -0400;Clarify arg class (#5916)

==

src/transformers/trainer.py
==================
8e0bcb56e;Qingqing Cao;2020-07-20 17:54:08 -0400;DataParallel fix: multi gpu evaluation (#5926)
The DataParallel training was fixed in https://github.com/huggingface/transformers/pull/5733, this commit also fixes the evaluation. It's more convenient when the user enables both `do_train` and `do_eval`.
==

examples/question-answering/run_squad.py
==================
a20969170;Sylvain Gugger;2020-07-20 17:53:21 -0400;Add AlbertForPretraining to doc (#5914)

==

docs/source/model_doc/albert.rst
==================
f1a4e06f1;Sam Shleifer;2020-07-20 15:18:26 -0400;[Fix] seq2seq pack_dataset.py actually packs (#5913)
Huge MT speedup!
==

examples/seq2seq/pack_dataset.py
examples/seq2seq/test_seq2seq_examples.py
==================
32883b310;Sylvain Gugger;2020-07-20 11:50:41 -0400;Improve doc of use_cache (#5912)
* Improve doc of use_cache

* Update src/transformers/configuration_xlnet.py

Co-authored-by: Teven <teven.lescao@gmail.com>

Co-authored-by: Teven <teven.lescao@gmail.com>
==

src/transformers/configuration_xlnet.py
==================
9ccb45a26;Clement;2020-07-20 11:40:33 -0400;Update gpt2-README.md

==

model_cards/gpt2-README.md
==================
f19751117;Clement;2020-07-20 10:47:42 -0400;Create gpt2-medium-README.md

==

model_cards/gpt2-medium-README.md
==================
511523672;Clement;2020-07-20 10:47:27 -0400;Create gpt2-large-README.md

==

model_cards/gpt2-large-README.md
==================
182c61193;Clement;2020-07-20 10:47:11 -0400;Update gpt2-README.md

==

model_cards/gpt2-README.md
==================
a9ae27cd0;Clement;2020-07-20 10:46:10 -0400;add link to write with transformers to model card

==

model_cards/gpt2-xl-README.md
==================
01c40db4f;Sam Shleifer;2020-07-20 10:44:10 -0400;[cleanup] squad processor (#5868)

==

src/transformers/data/processors/squad.py
==================
35cb101ea;Stas Bekman;2020-07-20 06:29:12 -0700;DataParallel fixes (#5733)
* DataParallel fixes:

1. switched to a more precise check
-        if self.args.n_gpu > 1:
+        if isinstance(model, nn.DataParallel):

2. fix tests - require the same fixup under DataParallel as the training module

* another fix
==

examples/question-answering/run_squad.py
src/transformers/trainer.py
tests/test_modeling_common.py
==================
290b6e18a;Pradhy729;2020-07-20 06:07:37 -0700;Trainer support for iterabledataset (#5834)
* Don't pass sampler for iterable dataset

* Added check for test and eval dataloaders.

* Formatting

* Don't pass sampler for iterable dataset

* Added check for test and eval dataloaders.

* Formatting

* Cleaner if nesting.

* Added test for trainer and iterable dataset

* Formatting for test

* Fixed import when torch is available only.

* Added require torch decorator to helper class

* Moved dataset class inside unittest

* Removed nested if and changed model in test

* Checking torch availability for IterableDataset
==

src/transformers/trainer.py
tests/test_trainer.py
==================
82dd96cae;Julien Chaumond;2020-07-20 12:47:28 +0200;[model_cards] Dataset ids are case-sensitive
cc @lhoestq @thomwolf

Also cc'ing model author @nreimers => Model pages now properly link to the dataset pages (and in the future, eval results, etc.)

==

model_cards/sentence-transformers/bert-base-nli-cls-token/README.md
model_cards/sentence-transformers/bert-base-nli-max-tokens/README.md
model_cards/sentence-transformers/bert-base-nli-mean-tokens/README.md
==================
b01a8844a;Manuel Romero;2020-07-20 10:06:42 +0200;Create README.md (#5813)

==

model_cards/mrm8488/t5-small-finetuned-imdb-sentiment/README.md
==================
223bad242;Alan deLevie;2020-07-20 03:53:03 -0400;fix typo in (#5893)

==

src/transformers/training_args.py
==================
d441f8d29;Alan deLevie;2020-07-20 03:48:22 -0400;fix typo in training_args_tf.py (#5894)

==

src/transformers/training_args_tf.py
==================
09a2f4068;Sam Shleifer;2020-07-18 13:57:33 -0400;Seq2SeqDataset uses linecache to save memory by @Pradhy729 (#5792)
Co-authored-by: Pradhy729 <49659913+Pradhy729@users.noreply.github.com>
==

examples/seq2seq/README.md
examples/seq2seq/distillation.py
examples/seq2seq/finetune.py
examples/seq2seq/test_seq2seq_examples.py
examples/seq2seq/utils.py
src/transformers/tokenization_bart.py
==================
4b506a37e;Teven;2020-07-18 17:33:13 +0200;Xlnet outputs (#5883)
Slightly breaking change, changes functionality for `use_cache` in XLNet: if use_cache is True and mem_len is 0 or None (which is the case in the base model config), the model behaves like GPT-2 and returns mems to be used as past in generation. At training time `use_cache` is overriden and always True.
==

src/transformers/configuration_utils.py
src/transformers/configuration_xlnet.py
src/transformers/modeling_xlnet.py
tests/test_modeling_xlnet.py
==================
a55809241;Teven;2020-07-18 17:15:40 +0200;Revert "Xlnet outputs (#5881)" (#5882)
This reverts commit 13be4872123094c37eb5fab939b38967b0ad2cd0.
==

src/transformers/configuration_utils.py
src/transformers/configuration_xlnet.py
src/transformers/modeling_xlnet.py
tests/test_modeling_xlnet.py
==================
13be48721;Teven;2020-07-18 16:53:29 +0200;Xlnet outputs (#5881)
Slightly breaking change, changes functionality for `use_cache` in XLNet: if use_cache is True and mem_len is 0 or None (which is the case in the base model config), the model behaves like GPT-2 and returns mems to be used as past in generation. At training time `use_cache` is overriden and always True.
==

src/transformers/configuration_utils.py
src/transformers/configuration_xlnet.py
src/transformers/modeling_xlnet.py
tests/test_modeling_xlnet.py
==================
eae6d8d14;Sebastian;2020-07-18 14:20:11 +0200;Update tokenizers to 0.8.1.rc to fix Mac OS X issues (#5867)

==

setup.py
==================
dad5e12e5;Sam Shleifer;2020-07-18 07:43:57 -0400;[seq2seq] distillation.py accepts trainer arguments (#5865)

==

examples/seq2seq/README.md
examples/seq2seq/distillation.py
==================
ba2400189;Sam Shleifer;2020-07-17 22:51:31 -0400;[seq2seq] MAX_LEN env var for MT commands (#5837)

==

examples/seq2seq/README.md
examples/seq2seq/train_mbart_cc25_enro.sh
examples/seq2seq/train_mbart_cc25_enro_multigpu.sh
==================
529850ae7;Nathan Raw;2020-07-17 20:43:06 -0600;Lightning Updates for v0.8.5 (#5798)
Co-authored-by: Sam Shleifer <sshleifer@gmail.com>
==

examples/lightning_base.py
examples/requirements.txt
examples/seq2seq/README.md
examples/seq2seq/finetune.py
examples/seq2seq/finetune.sh
examples/seq2seq/test_seq2seq_examples.py
examples/seq2seq/train_mbart_cc25_enro.sh
==================
615be03f9;Teven;2020-07-17 20:33:44 +0200;Revert "XLNet `use_cache` refactor (#5770)" (#5854)
This reverts commit 0b2da0e592fcb3a361494509031f91e1b700e4ad.
==

src/transformers/configuration_utils.py
src/transformers/configuration_xlnet.py
src/transformers/modeling_xlnet.py
tests/test_modeling_xlnet.py
==================
0b2da0e59;Teven;2020-07-17 20:24:16 +0200;XLNet `use_cache` refactor (#5770)
Slightly breaking change, changes functionality for `use_cache` in XLNet: if use_cache is True and mem_len is 0 or None (which is the case in the base model config), the model behaves like GPT-2 and returns mems to be used as past in generation. At training time `use_cache` is overriden and always True.
==

src/transformers/configuration_utils.py
src/transformers/configuration_xlnet.py
src/transformers/modeling_xlnet.py
tests/test_modeling_xlnet.py
==================
9750e1300;Jannes;2020-07-17 20:03:53 +0200;Create README.md (#5847)

==

model_cards/jannesg/takalane_afr_roberta/README.md
==================
1bca4fbd3;Julien Chaumond;2020-07-17 13:55:37 -0400;[model_card] Fix metadata

==

model_cards/neuraly/bert-base-italian-cased-sentiment/README.md
==================
a9d56a675;Gianpaolo Di Pietro;2020-07-17 19:50:49 +0200;Added model card for neuraly/bert-base-italian-cased-sentiment (#5845)
* Added model card for neuraly/bert-base-italian-cased-sentiment

* Update model_cards/neuraly/bert-base-italian-cased-sentiment/README.md

Co-authored-by: Julien Chaumond <chaumond@gmail.com>

Co-authored-by: Gianpy15 <g.dipietro@neuraly.ai>
Co-authored-by: Julien Chaumond <chaumond@gmail.com>
==

model_cards/neuraly/bert-base-italian-cased-sentiment/README.md
==================
12f14710c;Patrick von Platen;2020-07-17 18:22:05 +0200;[Model card] Bert2Bert
Add Rouge2 results
==

model_cards/patrickvonplaten/bert2bert-cnn_dailymail-fp16/README.md
==================
9d37c56ba;Patrick von Platen;2020-07-17 16:17:42 +0200;[Reformer] - Cache hidden states and buckets to speed up inference (#5578)
* fix merge rebase

* add intermediate reformer code

* save intermediate caching results

* save intermediate

* save intermediate results

* save intermediate

* upload next step

* fix generate tests

* make tests work

* add named tuple output

* Apply suggestions from code review

* fix use_cache for False case

* fix tensor to gpu

* fix tensor to gpu

* refactor

* refactor and make style
==

src/transformers/modeling_reformer.py
src/transformers/modeling_xlnet.py
tests/test_modeling_reformer.py
==================
0b6c255a9;Patrick von Platen;2020-07-17 11:41:56 +0200;[Model card] Bert2Bert (#5841)
* Create README.md

* Update README.md

* Update README.md

* Update README.md
==

model_cards/patrickvonplaten/bert2bert-cnn_dailymail-fp16/README.md
==================
3d9556a72;Sam Shleifer;2020-07-17 02:54:25 -0400;[cleanups] make Marian save as Marian (#5830)

==

src/transformers/__init__.py
src/transformers/modeling_marian.py
==================
e238e3d55;Sam Shleifer;2020-07-17 01:53:25 -0400;[seq2seq] Don't copy self.source in sortishsampler (#5818)

==

examples/seq2seq/utils.py
==================
2e4624b41;Bayartsogt Yadamsuren;2020-07-17 13:40:38 +0800;language tag addition on albert-mongolian (#5828)
* language tag addition on albert-mongolian

* Update model_cards/bayartsogt/albert-mongolian/README.md

Co-authored-by: Julien Chaumond <chaumond@gmail.com>
==

model_cards/bayartsogt/albert-mongolian/README.md
==================
d088d744a;Manuel Romero;2020-07-16 21:18:31 +0200;Create README.md (#5821)

==

model_cards/mrm8488/t5-small-finetuned-emotion/README.md
==================
233072fc1;Nick Doiron;2020-07-16 15:13:51 -0400;dv-wave (#5823)

==

model_cards/monsoon-nlp/dv-wave/README.md
==================
283500ff9;Sam Shleifer;2020-07-16 14:06:49 -0400;[seq2seq] pack_dataset.py rewrites dataset in max_tokens format (#5819)

==

examples/seq2seq/pack_dataset.py
examples/seq2seq/test_seq2seq_examples.py
==================
c45d7a707;Manuel Romero;2020-07-16 16:25:50 +0200;Update README.md (#5812)
Fix missig "-" in meta data
==

model_cards/mrm8488/t5-base-finetuned-imdb-sentiment/README.md
==================
057411c56;Patrick von Platen;2020-07-16 16:19:37 +0200;fix longformer slow down (#5811)

==

src/transformers/modeling_longformer.py
==================
89a78be51;Patrick von Platen;2020-07-16 15:15:10 +0200; fix benchmark for longformer (#5808)

==

src/transformers/benchmark/benchmark.py
src/transformers/benchmark/benchmark_tf.py
tests/test_benchmark.py
==================
aefc0c042;Patrick von Platen;2020-07-16 12:13:10 +0200;fix benchmark non standard model (#5801)

==

src/transformers/benchmark/benchmark.py
src/transformers/benchmark/benchmark_tf.py
==================
8ce610bc9;Martin M√ºller;2020-07-16 11:26:17 +0200;Update README.md (#5789)

==

model_cards/digitalepidemiologylab/covid-twitter-bert/README.md
==================
6b6d035d8;Julien Chaumond;2020-07-16 03:50:47 -0400;[model_card] illuin/lepetit

==

model_cards/illuin/lepetit/README.md
==================
d1f74b9af;HuYong;2020-07-16 11:03:05 +0800;ADD ERNIE model (#5763)
* ERNIE model card

* Update Readme.md

* Update Readme.md

* Update Readme.md

* Rename Readme.md to README.md

* Update README.md

* Update Readme.md

* Update README.md

* Rename Readme.md to README.md

* Update Readme.md

* Update Readme.md

* Rename Readme.md to README.md

* Update and rename Readme.md to README.md

Co-authored-by: Kevin Canwen Xu <canwenxu@126.com>
==

model_cards/nghuyong/ernie-1.0/README.md
model_cards/nghuyong/ernie-2.0-en/README.md
model_cards/nghuyong/ernie-2.0-large-en/README.md
model_cards/nghuyong/ernie-tiny/README.md
==================
3b924fabe;Clement;2020-07-15 17:59:06 -0400;Create distilbert squad tags

==

model_cards/distilbert-base-cased-distilled-squad-README.md
==================
067814102;Clement;2020-07-15 17:50:46 -0400;fix readme

==

model_cards/huggingface/CodeBERTa-language-id/README.md
==================
d179fd69c;Clement;2020-07-15 17:48:22 -0400;test readme change

==

model_cards/huggingface/CodeBERTa-language-id/README.md
==================
63761614e;Manuel Romero;2020-07-15 22:19:21 +0200;Update README.md (#5776)
Add cherry picked example for the widget

Co-authored-by: Julien Chaumond <chaumond@gmail.com>
==

model_cards/mrm8488/RuPERTa-base/README.md
==================
221e23c6c;Manuel Romero;2020-07-15 22:17:25 +0200;Create README.md (#5781)
* Create README.md

* Update model_cards/mrm8488/RoBasquERTa/README.md

Co-authored-by: Julien Chaumond <chaumond@gmail.com>
==

model_cards/mrm8488/RoBasquERTa/README.md
==================
d4cda29af;Manuel Romero;2020-07-15 22:17:19 +0200;Create README.md (#5782)
* Create README.md

* Apply suggestions from code review

Co-authored-by: Julien Chaumond <chaumond@gmail.com>
==

model_cards/mrm8488/RoBERTinha/README.md
==================
62ec28ce4;Julien Chaumond;2020-07-15 22:14:52 +0200;[model_cards] Fix pierreguillou/gpt2-small-portuguese

==

model_cards/pierreguillou/gpt2-small-portuguese/README.md
==================
a946724bb;Pierre Guillou;2020-07-15 17:13:28 -0300;metadata (#5758)
* metadata

* Update model_cards/pierreguillou/gpt2-small-portuguese/README.md

Co-authored-by: Julien Chaumond <chaumond@gmail.com>
==

model_cards/pierreguillou/gpt2-small-portuguese/README.md
==================
015dc51fe;Julien Chaumond;2020-07-15 21:25:52 +0200;[model_card] bert-portuguese: add language meta
cc @rodrigonogueira4 @abiocapsouza @robertoalotufo

Also cc @piegu

Obrigado :)

==

model_cards/neuralmind/bert-base-portuguese-cased/README.md
model_cards/neuralmind/bert-large-portuguese-cased/README.md
==================
1a647abf0;Sam Shleifer;2020-07-15 14:59:38 -0400;[fix] check code quality (#5772)

==

examples/seq2seq/utils.py
tests/test_modeling_t5.py
==================
b23d3a5ad;Julien Chaumond;2020-07-15 18:59:20 +0200;[model_cards] Switch all languages codes to ISO-639-{1,2,3}

==

model_cards/DeepPavlov/bert-base-bg-cs-pl-ru-cased/README.md
model_cards/DeepPavlov/bert-base-cased-conversational/README.md
model_cards/DeepPavlov/rubert-base-cased-conversational/README.md
model_cards/DeepPavlov/rubert-base-cased-sentence/README.md
model_cards/DeepPavlov/rubert-base-cased/README.md
model_cards/KB/albert-base-swedish-cased-alpha/README.md
model_cards/KB/bert-base-swedish-cased-ner/README.md
model_cards/KB/bert-base-swedish-cased/README.md
model_cards/LorenzoDeMattei/GePpeTto/README.md
model_cards/MoseliMotsoehli/TswanaBert/README.md
model_cards/MoseliMotsoehli/zuBERTa/README.md
model_cards/Musixmatch/umberto-commoncrawl-cased-v1/README.md
model_cards/Musixmatch/umberto-wikipedia-uncased-v1/README.md
model_cards/Norod78/hewiki-articles-distilGPT2py-il/README.md
model_cards/Tereveni-AI/gpt2-124M-uk-fiction/README.md
model_cards/TurkuNLP/bert-base-finnish-cased-v1/README.md
model_cards/TurkuNLP/bert-base-finnish-uncased-v1/README.md
model_cards/ViktorAlm/electra-base-norwegian-uncased-discriminator/README.md
model_cards/allegro/herbert-klej-cased-tokenizer-v1/README.md
model_cards/allegro/herbert-klej-cased-v1/README.md
model_cards/asafaya/bert-base-arabic/README.md
model_cards/asafaya/bert-large-arabic/README.md
model_cards/asafaya/bert-medium-arabic/README.md
model_cards/asafaya/bert-mini-arabic/README.md
model_cards/aubmindlab/bert-base-arabert/README.md
model_cards/aubmindlab/bert-base-arabertv01/README.md
model_cards/bashar-talafha/multi-dialect-bert-base-arabic/README.md
model_cards/bert-base-cased-README.md
model_cards/bert-base-chinese-README.md
model_cards/bert-base-german-cased-README.md
model_cards/bert-base-german-dbmdz-cased-README.md
model_cards/bert-base-german-dbmdz-uncased-README.md
model_cards/bert-base-multilingual-uncased-README.md
model_cards/bert-base-uncased-README.md
model_cards/camembert-base-README.md
model_cards/camembert/camembert-base-ccnet-4gb/README.md
model_cards/camembert/camembert-base-ccnet/README.md
model_cards/camembert/camembert-base-oscar-4gb/README.md
model_cards/camembert/camembert-base-wikipedia-4gb/README.md
model_cards/camembert/camembert-large/README.md
model_cards/clue/albert_chinese_small/README.md
model_cards/clue/albert_chinese_tiny/README.md
model_cards/clue/roberta_chinese_3L312_clue_tiny/README.md
model_cards/clue/roberta_chinese_base/README.md
model_cards/clue/roberta_chinese_large/README.md
model_cards/clue/xlnet_chinese_large/README.md
model_cards/daigo/bert-base-japanese-sentiment/README.md
model_cards/dbmdz/bert-base-german-cased/README.md
model_cards/dbmdz/bert-base-german-europeana-cased/README.md
model_cards/dbmdz/bert-base-german-europeana-uncased/README.md
model_cards/dbmdz/bert-base-german-uncased/README.md
model_cards/dbmdz/bert-base-italian-cased/README.md
model_cards/dbmdz/bert-base-italian-uncased/README.md
model_cards/dbmdz/bert-base-italian-xxl-cased/README.md
model_cards/dbmdz/bert-base-italian-xxl-uncased/README.md
model_cards/dbmdz/bert-base-turkish-128k-cased/README.md
model_cards/dbmdz/bert-base-turkish-128k-uncased/README.md
model_cards/dbmdz/bert-base-turkish-cased/README.md
model_cards/dbmdz/bert-base-turkish-uncased/README.md
model_cards/dbmdz/distilbert-base-turkish-cased/README.md
model_cards/dbmdz/electra-base-turkish-cased-discriminator/README.md
model_cards/dbmdz/electra-small-turkish-cased-discriminator/README.md
model_cards/deepset/bert-base-german-cased-oldvocab/README.md
model_cards/distilbert-base-uncased-README.md
model_cards/dkleczek/bert-base-polish-cased-v1/README.md
model_cards/dkleczek/bert-base-polish-uncased-v1/README.md
model_cards/dumitrescustefan/bert-base-romanian-cased-v1/README.md
model_cards/dumitrescustefan/bert-base-romanian-uncased-v1/README.md
model_cards/fmikaelian/camembert-base-fquad/README.md
model_cards/fmikaelian/camembert-base-squad/README.md
model_cards/fmikaelian/flaubert-base-uncased-squad/README.md
model_cards/google/electra-base-discriminator/README.md
model_cards/google/electra-base-generator/README.md
model_cards/google/electra-large-discriminator/README.md
model_cards/google/electra-large-generator/README.md
model_cards/google/electra-small-discriminator/README.md
model_cards/google/electra-small-generator/README.md
model_cards/google/mobilebert-uncased/README.md
model_cards/gpt2-README.md
model_cards/henryk/bert-base-multilingual-cased-finetuned-dutch-squad2/README.md
model_cards/henryk/bert-base-multilingual-cased-finetuned-polish-squad1/README.md
model_cards/henryk/bert-base-multilingual-cased-finetuned-polish-squad2/README.md
model_cards/huseinzol05/albert-base-bahasa-cased/README.md
model_cards/huseinzol05/albert-tiny-bahasa-cased/README.md
model_cards/huseinzol05/bert-base-bahasa-cased/README.md
model_cards/huseinzol05/electra-base-discriminator-bahasa-cased/README.md
model_cards/huseinzol05/electra-base-generator-bahasa-cased/README.md
model_cards/huseinzol05/electra-small-discriminator-bahasa-cased/README.md
model_cards/huseinzol05/electra-small-generator-bahasa-cased/README.md
model_cards/huseinzol05/gpt2-117M-bahasa-cased/README.md
model_cards/huseinzol05/gpt2-345M-bahasa-cased/README.md
model_cards/huseinzol05/t5-base-bahasa-cased/README.md
model_cards/huseinzol05/t5-small-bahasa-cased/README.md
model_cards/huseinzol05/tiny-bert-bahasa-cased/README.md
model_cards/huseinzol05/xlnet-base-bahasa-cased/README.md
model_cards/illuin/camembert-base-fquad/README.md
model_cards/illuin/camembert-large-fquad/README.md
model_cards/ixa-ehu/berteus-base-cased/README.md
model_cards/jannesg/bertsson/README.md
model_cards/julien-c/EsperBERTo-small-pos/README.md
model_cards/julien-c/EsperBERTo-small/README.md
model_cards/krevas/finance-koelectra-base-discriminator/README.md
model_cards/krevas/finance-koelectra-base-generator/README.md
model_cards/krevas/finance-koelectra-small-discriminator/README.md
model_cards/krevas/finance-koelectra-small-generator/README.md
model_cards/lserinol/bert-turkish-question-answering/README.md
model_cards/monologg/koelectra-base-discriminator/README.md
model_cards/monologg/koelectra-base-generator/README.md
model_cards/monologg/koelectra-small-discriminator/README.md
model_cards/monologg/koelectra-small-generator/README.md
model_cards/monsoon-nlp/hindi-bert/README.md
model_cards/mrm8488/GPT-2-finetuned-CORD19/README.md
model_cards/mrm8488/GPT-2-finetuned-covid-bio-medrxiv/README.md
model_cards/mrm8488/RuPERTa-base-finetuned-ner/README.md
model_cards/mrm8488/RuPERTa-base-finetuned-pos/README.md
model_cards/mrm8488/RuPERTa-base/README.md
model_cards/mrm8488/TinyBERT-spanish-uncased-finetuned-ner/README.md
model_cards/mrm8488/bert-base-spanish-wwm-cased-finetuned-spa-squad2-es/README.md
model_cards/mrm8488/bert-italian-finedtuned-squadv1-it-alfa/README.md
model_cards/mrm8488/bert-medium-finetuned-squadv2/README.md
model_cards/mrm8488/bert-mini-finetuned-squadv2/README.md
model_cards/mrm8488/bert-small-finetuned-squadv2/README.md
model_cards/mrm8488/bert-small-finetuned-typo-detection/README.md
model_cards/mrm8488/bert-spanish-cased-finetuned-ner/README.md
model_cards/mrm8488/bert-spanish-cased-finetuned-pos-syntax/README.md
model_cards/mrm8488/bert-spanish-cased-finetuned-pos/README.md
model_cards/mrm8488/bert-tiny-finetuned-squadv2/README.md
model_cards/mrm8488/bert-uncased-finetuned-qnli/README.md
model_cards/mrm8488/distill-bert-base-spanish-wwm-cased-finetuned-spa-squad2-es/README.md
model_cards/mrm8488/electra-base-finetuned-squadv1/README.md
model_cards/mrm8488/electra-small-finetuned-squadv1/README.md
model_cards/mrm8488/electra-small-finetuned-squadv2/README.md
model_cards/mrm8488/electricidad-small-discriminator/README.md
model_cards/mrm8488/electricidad-small-finetuned-squadv1-es/README.md
model_cards/mrm8488/gpt2-imdb-neutral/README.md
model_cards/mrm8488/longformer-base-4096-finetuned-squadv2/README.md
model_cards/mrm8488/roberta-base-1B-1-finetuned-squadv1/README.md
model_cards/mrm8488/roberta-base-1B-1-finetuned-squadv2/README.md
model_cards/mrm8488/spanbert-base-finetuned-squadv1/README.md
model_cards/mrm8488/spanbert-base-finetuned-squadv2/README.md
model_cards/mrm8488/spanbert-base-finetuned-tacred/README.md
model_cards/mrm8488/spanbert-finetuned-squadv1/README.md
model_cards/mrm8488/spanbert-finetuned-squadv2/README.md
model_cards/mrm8488/spanbert-large-finetuned-squadv1/README.md
model_cards/mrm8488/spanbert-large-finetuned-squadv2/README.md
model_cards/mrm8488/spanbert-large-finetuned-tacred/README.md
model_cards/mrm8488/t5-base-finetuned-emotion/README.md
model_cards/mrm8488/t5-base-finetuned-imdb-sentiment/README.md
model_cards/mrm8488/t5-base-finetuned-sarcasm-twitter/README.md
model_cards/mrm8488/t5-base-finetuned-span-sentiment-extraction/README.md
model_cards/mrm8488/t5-base-finetuned-squadv2/README.md
model_cards/mrm8488/t5-base-finetuned-summarize-news/README.md
model_cards/mrm8488/t5-small-finetuned-squadv1/README.md
model_cards/mrm8488/t5-small-finetuned-squadv2/README.md
model_cards/mrm8488/umberto-wikipedia-uncased-v1-finetuned-squadv1-it/README.md
model_cards/nlpaueb/bert-base-greek-uncased-v1/README.md
model_cards/nlptown/bert-base-multilingual-uncased-sentiment/README.md
model_cards/pierreguillou/gpt2-small-portuguese/README.md
model_cards/redewiedergabe/bert-base-historical-german-rw-cased/README.md
model_cards/roberta-base-README.md
model_cards/roberta-large-README.md
model_cards/savasy/bert-base-turkish-ner-cased/README.md
model_cards/savasy/bert-base-turkish-sentiment-cased/README.md
model_cards/savasy/bert-base-turkish-squad/README.md
model_cards/savasy/bert-turkish-text-classification/README.md
model_cards/schmidek/electra-small-cased/README.md
model_cards/sentence-transformers/bert-base-nli-cls-token/README.md
model_cards/sentence-transformers/bert-base-nli-max-tokens/README.md
model_cards/sentence-transformers/bert-base-nli-mean-tokens/README.md
model_cards/severinsimmler/literary-german-bert/README.md
model_cards/surajp/RoBERTa-hindi-guj-san/README.md
model_cards/surajp/SanBERTa/README.md
model_cards/surajp/albert-base-sanskrit/README.md
model_cards/tblard/tf-allocine/README.md
model_cards/voidful/albert_chinese_base/README.md
model_cards/voidful/albert_chinese_large/README.md
model_cards/voidful/albert_chinese_small/README.md
model_cards/voidful/albert_chinese_tiny/README.md
model_cards/voidful/albert_chinese_xlarge/README.md
model_cards/voidful/albert_chinese_xxlarge/README.md
model_cards/xlm-roberta-large-finetuned-conll03-german-README.md
model_cards/yjernite/bart_eli5/README.md
model_cards/youscan/ukr-roberta-base/README.md
==================
d533c7e9b;Funtowicz Morgan;2020-07-15 16:11:22 +0200;[fix] T5 ONNX test: model.to(torch_device) (#5769)
Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>
==

tests/test_modeling_t5.py
==================
d0486c8bc;Sam Shleifer;2020-07-15 08:23:22 -0400;[cleanup] T5 test, warnings (#5761)

==

examples/seq2seq/run_eval.py
examples/seq2seq/utils.py
tests/test_modeling_t5.py
==================
ec0a945cf;Patrick von Platen;2020-07-15 09:51:14 +0200;[AutoModels] Fix config params handling of all PT and TF AutoModels (#5665)
* fix auto model causal lm

* leverage given functionality

* apply unused kwargs to all auto models
==

src/transformers/modeling_auto.py
src/transformers/modeling_tf_auto.py
==================
8ab565a4b;Julien Chaumond;2020-07-14 22:27:07 +0200;[model_card] Fix syntax

==

model_cards/cimm-kzn/rudr-bert/README.md
==================
92dc95922;Bashar Talafha;2020-07-14 22:48:59 +0300;Update README.md (#5752)

==

model_cards/bashar-talafha/multi-dialect-bert-base-arabic/README.md
==================
baf93b02c;Bashar Talafha;2020-07-14 19:51:57 +0300;Update README.md (#5696)

==

model_cards/bashar-talafha/multi-dialect-bert-base-arabic/README.md
==================
5d178954c;Joe Davison;2020-07-14 10:39:44 -0600;tiny ppl doc typo fix (#5751)

==

docs/source/perplexity.rst
==================
ac921f038;Manuel Romero;2020-07-14 16:58:45 +0200;RuPERTa model card (#5743)
* Customize inference widget input

* Update model_cards/mrm8488/RuPERTa-base/README.md

Co-authored-by: Kevin Canwen Xu <canwenxu@126.com>
==

model_cards/mrm8488/RuPERTa-base/README.md
==================
21c1fe529;dartrevan;2020-07-14 17:51:53 +0300;RuDR-BERT model card (#5698)

==

model_cards/cimm-kzn/rudr-bert/README.md
==================
2db1cc807;Doron Adler;2020-07-14 17:50:44 +0300;Norod78/hewiki-articles-distilGPT2py-il model card (#5735)
Model card for hewiki-articles-distilGPT2py-il
A tiny GPT2 model for generating Hebrew text
==

model_cards/Norod78/hewiki-articles-distilGPT2py-il/README.md
==================
dae244ad8;Pierre Guillou;2020-07-14 11:48:52 -0300;GPorTuguese-2 model card (#5744)

==

model_cards/pierreguillou/gpt2-small-portuguese/README.md
==================
b2505f7db;Sam Shleifer;2020-07-14 06:13:05 -0400;Cleanup bart caching logic (#5640)

==

src/transformers/modeling_bart.py
==================
838950ee4;Sam Shleifer;2020-07-14 06:12:24 -0400;[fix] mbart_en_ro_generate test now identical to fairseq (#5731)

==

tests/test_modeling_mbart.py
==================
4d5a8d655;Boris Dayma;2020-07-14 04:12:33 -0500;docs(wandb): explain how to use W&B integration (#5607)
* docs(wandb): explain how to use W&B integration

fix #5262

* Also mention TensorBoard

Co-authored-by: Julien Chaumond <chaumond@gmail.com>
==

examples/README.md
==================
cd30f98fd;Gunnlaugur Thor Briem;2020-07-14 07:47:41 +0000;doc: fix apparent copy-paste error in docstring (#5626)

==

src/transformers/commands/train.py
==================
f867000f5;as-stevens;2020-07-14 03:16:22 -0400;[Reformer classification head] Implement the reformer model classification head for text classification (#5198)
* Reformer model head classification implementation for text classification

* Reformat the reformer model classification code

* PR review comments, and test case implementation for reformer for classification head changes

* CI/CD reformer for classification head test import error fix

* CI/CD test case implementation  added ReformerForSequenceClassification to all_model_classes

* Code formatting- fixed

* Normal test cases added for reformer classification head

* Fix test cases implementation for the reformer classification head

* removed token_type_id parameter from the reformer classification head

* fixed the test case for reformer classification head

* merge conflict with master fixed

* merge conflict, changed reformer classification to accept the choice_label parameter added in latest code

* refactored the the reformer classification head test code

* reformer classification head, common transform test cases fixed

* final set of the review comment, rearranging the reformer classes and docstring add to classification forward method

* fixed the compilation error and text case fix for reformer classification head

* Apply suggestions from code review

Remove unnecessary dup

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
==

src/transformers/__init__.py
src/transformers/modeling_reformer.py
tests/test_modeling_reformer.py
==================
f0bda06f4;Gaurav Mishra;2020-07-13 21:02:03 -0700;Update tokenization_t5.py (#5717)
Minor doc fix.
==

src/transformers/tokenization_t5.py
==================
c3c61ea01;Sam Shleifer;2020-07-13 17:12:18 -0400;[Fix] github actions CI by reverting #5138 (#5686)

==

.github/workflows/self-push.yml
.github/workflows/self-scheduled.yml
==================
45addfe96;Stas Bekman;2020-07-13 11:59:53 -0700;FlaubertForTokenClassification  (#5644)
* implement FlaubertForTokenClassification as a subclass of XLMForTokenClassification

* fix mapping order

* add the doc

* add common tests
==

docs/source/model_doc/flaubert.rst
src/transformers/__init__.py
src/transformers/modeling_auto.py
src/transformers/modeling_flaubert.py
tests/test_modeling_flaubert.py
==================
7096e4751;Patrick von Platen;2020-07-13 17:23:22 +0200;[Longformer] fix longformer global attention output (#5659)
* fix longformer global attention output

* fix multi gpu problem

* replace -10000 with 0

* better comment

* make attention output equal local and global

* Update src/transformers/modeling_longformer.py
==

src/transformers/modeling_longformer.py
==================
ce374ba87;Sylvain Gugger;2020-07-13 08:37:38 -0400;Fix Trainer in DataParallel setting (#5685)
* Fix Trainer in DataParallel setting

* Fix typo

Co-authored-by: Sam Shleifer <sshleifer@gmail.com>
==

src/transformers/trainer.py
==================
0a19a49df;Stas Bekman;2020-07-13 03:10:17 -0700;doc improvements (#5688)

==

docs/source/quicktour.rst
==================
443b0cad9;Stas Bekman;2020-07-13 03:09:49 -0700;rename the function to match the rest of the test convention (#5692)

==

tests/test_modeling_xlm.py
==================
74843695e;onepointconsulting;2020-07-13 07:53:48 +0100;Added first description of the model (#5672)
Added general description, information about the tags and also some example usage code.
==

model_cards/gilf/french-postag-model/README.md
==================
0befb5132;Kevin Canwen Xu;2020-07-12 12:34:21 +0800;Pipeline model type check (#5679)
* Add model type check for pipelines

* Add model type check for pipelines

* rename func

* Fix the init parameters

* Fix format

* rollback unnecessary refactor
==

src/transformers/pipelines.py
==================
dc31a72f5;Kevin Canwen Xu;2020-07-11 21:37:30 +0800;Add Microsoft's CodeBERT (#5683)
* Add Microsoft's CodeBERT

* link style

* single modal

* unused import
==

model_cards/microsoft/codebert-base-mlm/README.md
model_cards/microsoft/codebert-base/README.md
==================
7fad617dc;Sylvain Gugger;2020-07-10 17:31:02 -0400;Document model outputs (#5673)
* Document model outputs

* Update docs/source/main_classes/output.rst

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
==

docs/source/index.rst
docs/source/main_classes/output.rst
docs/source/model_doc/albert.rst
docs/source/model_doc/bert.rst
docs/source/model_doc/dpr.rst
docs/source/model_doc/electra.rst
docs/source/model_doc/gpt.rst
docs/source/model_doc/gpt2.rst
docs/source/model_doc/mobilebert.rst
docs/source/model_doc/transformerxl.rst
docs/source/model_doc/xlm.rst
docs/source/model_doc/xlnet.rst
src/transformers/file_utils.py
src/transformers/modeling_albert.py
src/transformers/modeling_bert.py
src/transformers/modeling_electra.py
src/transformers/modeling_mobilebert.py
src/transformers/modeling_outputs.py
==================
df983b748;Sylvain Gugger;2020-07-10 17:25:52 -0400;Deprecate old past arguments (#5671)

==

src/transformers/modeling_albert.py
src/transformers/modeling_bart.py
src/transformers/modeling_bert.py
src/transformers/modeling_ctrl.py
src/transformers/modeling_distilbert.py
src/transformers/modeling_electra.py
src/transformers/modeling_gpt2.py
src/transformers/modeling_longformer.py
src/transformers/modeling_openai.py
src/transformers/modeling_roberta.py
src/transformers/modeling_t5.py
==================
cdf4cd706;Tomo Lazovich;2020-07-10 16:34:21 -0400;[squad] add version tag to squad cache (#5669)

==

src/transformers/data/datasets/squad.py
==================
223084e42;Patrick von Platen;2020-07-10 18:34:25 +0200;Add Reformer to notebooks

==

notebooks/README.md
==================
201d23f28;Julien Chaumond;2020-07-10 18:07:29 +0200;Update The Big Table of Tasks
Co-Authored-By: Suraj Patil <surajp815@gmail.com>
Co-Authored-By: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

==

examples/README.md
==================
82f7bbbd9;Bashar Talafha;2020-07-10 18:43:27 +0300;Update README.md (#5617)
* Update README.md

* Update README.md
==

model_cards/bashar-talafha/multi-dialect-bert-base-arabic/README.md
==================
bf497376e;Manuel Romero;2020-07-10 17:42:49 +0200;Create README.md (#5572)

==

model_cards/mrm8488/umberto-wikipedia-uncased-v1-finetuned-squadv1-it/README.md
==================
3653d01f2;kolk;2020-07-10 21:09:44 +0530;Create README.md for electra-base-squad2 (#5574)

==

model_cards/deepset/electra-base-squad2/README.md
==================
aa69c81f2;Txus;2020-07-10 17:39:04 +0200;Add freshly trained `base` version (#5621)

==

model_cards/codegram/calbert-base-uncased/README.md
==================
227e0a406;Teven;2020-07-10 17:38:36 +0200;Fixed use of memories in XLNet (caching for language generation + warning when loading improper memoryless model) (#5632)
* Pytorch gpu => cpu proper device

* Memoryless XLNet warning + fixed memories during generation

* Revert "Pytorch gpu => cpu proper device"

This reverts commit 93489b36

* made black happy

* TF generation with memories

* dim => axis

* added padding_text to TF XL models

* Added comment, added TF
==

src/transformers/configuration_xlnet.py
src/transformers/modeling_tf_xlnet.py
src/transformers/modeling_xlnet.py
src/transformers/pipelines.py
==================
3b7b64656;Manuel Romero;2020-07-10 17:38:23 +0200;Create README.md (#5638)

==

model_cards/mrm8488/t5-small-finetuned-squadv1/README.md
==================
0039b965d;Manuel Romero;2020-07-10 17:38:11 +0200;Create model card (#5655)
Create model card for T5-small fine-tuned on SQUAD v2
==

model_cards/mrm8488/t5-small-finetuned-squadv2/README.md
==================
46982d612;Nils Reimers;2020-07-10 17:38:03 +0200;Create README.md - Model card (#5657)
Model card for sentence-transformers/bert-base-nli-cls-token
==

model_cards/sentence-transformers/bert-base-nli-cls-token/README.md
==================
c483803d1;Nils Reimers;2020-07-10 17:37:56 +0200;Create README.md - Model card (#5658)
Model card for sentence-transformers/bert-base-nli-max-tokens
==

model_cards/sentence-transformers/bert-base-nli-max-tokens/README.md
==================
edfd82f5f;Sylvain Gugger;2020-07-10 11:36:53 -0400;Change model outputs types to self-document outputs (#5438)
* [WIP] Proposal for model outputs

* All Bert models

* Make CI green maybe?

* Fix ONNX test

* Isolate ModelOutput from pt and tf

* Formatting

* Add Electra models

* Auto-generate docstrings from outputs

* Add TF outputs

* Add some BERT models

* Revert TF side

* Remove last traces of TF changes

* Fail with a clear error message

* Add Albert and work through Bart

* Add CTRL and DistilBert

* Formatting

* Progress on Bart

* Renames and finish Bart

* Formatting

* Fix last test

* Add DPR

* Finish Electra and add FlauBERT

* Add GPT2

* Add Longformer

* Add MMBT

* Add MobileBert

* Add GPT

* Formatting

* Add Reformer

* Add Roberta

* Add T5

* Add Transformer XL

* Fix test

* Add XLM + fix XLMForTokenClassification

* Style + XLMRoberta

* Add XLNet

* Formatting

* Add doc of return_tuple arg
==

src/transformers/benchmark/benchmark_utils.py
src/transformers/configuration_utils.py
src/transformers/convert_graph_to_onnx.py
src/transformers/file_utils.py
src/transformers/modeling_albert.py
src/transformers/modeling_bart.py
src/transformers/modeling_bert.py
src/transformers/modeling_camembert.py
src/transformers/modeling_ctrl.py
src/transformers/modeling_distilbert.py
src/transformers/modeling_dpr.py
src/transformers/modeling_electra.py
src/transformers/modeling_encoder_decoder.py
src/transformers/modeling_flaubert.py
src/transformers/modeling_gpt2.py
src/transformers/modeling_longformer.py
src/transformers/modeling_mmbt.py
src/transformers/modeling_mobilebert.py
src/transformers/modeling_openai.py
src/transformers/modeling_outputs.py
src/transformers/modeling_reformer.py
src/transformers/modeling_roberta.py
src/transformers/modeling_t5.py
src/transformers/modeling_transfo_xl.py
src/transformers/modeling_utils.py
src/transformers/modeling_xlm.py
src/transformers/modeling_xlm_roberta.py
src/transformers/modeling_xlnet.py
tests/test_modeling_common.py
tests/test_modeling_t5.py
tests/test_modeling_tf_common.py
tests/test_modeling_xlm.py
tests/test_modeling_xlnet.py
==================
fa265230a;Suraj Parmar;2020-07-10 21:04:23 +0530;Create Model card for RoBERTa-hindi-guj-san (#5661)

==

model_cards/surajp/RoBERTa-hindi-guj-san/README.md
==================
b2747af54;Sylvain Gugger;2020-07-10 10:31:47 -0400;Improvements to PretrainedConfig documentation (#5642)
* Update PretrainedConfig doc

* Formatting

* Small fixes

* Forgotten args and more cleanup
==

docs/source/main_classes/configuration.rst
src/transformers/configuration_utils.py
==================
bfacb2e34;Julien Chaumond;2020-07-10 08:10:24 -0400;[model_card] BART for ELI5
cc @yjernite
==

model_cards/yjernite/bart_eli5/README.md
==================
2e6bb0e9c;Nils Reimers;2020-07-10 11:41:10 +0200;Create README.md (#5652)

==

model_cards/sentence-transformers/bert-base-nli-mean-tokens/README.md
==================
552e4591f;Julien Chaumond;2020-07-10 05:07:33 -0400;[model_card] Add meta + fix link to image
(hotlinking to image works on GitHub but not on external sites)

cc @bashartalafha
==

model_cards/bashar-talafha/multi-dialect-bert-base-arabic/README.md
==================
02a0b4301;Teven;2020-07-09 22:29:32 +0200;Fixed TextGenerationPipeline on torch + GPU (#5629)
* Pytorch gpu => cpu proper device

* Memoryless XLNet warning + fixed memories during generation

* Revert "Memoryless XLNet warning + fixed memories during generation"

This reverts commit 3d3251ff

* Took the operations on the generated_sequence out of the ensure_device scope
==

src/transformers/pipelines.py
==================
760f726e5;Sylvain Gugger;2020-07-09 15:13:22 -0400;Add forum link in the docs (#5637)

==

docs/source/_static/js/custom.js
==================
bfeaae223;Stas Bekman;2020-07-09 12:12:29 -0700;fix 404 (#5616)

==

CONTRIBUTING.md
==================
b25f7802d;Lysandre Debut;2020-07-09 13:54:32 -0400;Should check that torch TPU is available (#5636)

==

src/transformers/modeling_utils.py
==================
3cc23eee0;Lysandre Debut;2020-07-09 13:35:21 -0400;More explicit error when failing to tensorize overflowing tokens (#5633)

==

src/transformers/tokenization_utils_base.py
==================
b9d8af07e;Lysandre;2020-07-09 11:06:23 -0400;Update stable doc

==

.circleci/deploy.sh
==================
1158e5655;Lysandre Debut;2020-07-09 11:03:07 -0400;Correct extension (#5631)

==

docs/source/model_summary.rst
==================
5c82bf683;Lysandre;2020-07-09 10:16:13 -0400;Update stable doc

==

.circleci/deploy.sh
==================
0533cf470;Lysandre Debut;2020-07-09 09:19:19 -0400;Test XLA examples (#5583)
* Test XLA examples

* Style

* Using `require_torch_tpu`

* Style

* No need for pytest
==

examples/test_xla_examples.py
src/transformers/testing_utils.py
==================
3bd55199c;Funtowicz Morgan;2020-07-09 15:11:40 +0200;QA pipeline BART compatible (#5496)
* Ensure padding and question cannot have higher probs than context.

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* Add bart the the list of tokenizers adding two <sep> tokens for squad_convert_example_to_feature

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* Format.

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* Addressing @patrickvonplaten comments.

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* Addressing @patrickvonplaten comments about masking non-context element when generating the answer.

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* Addressing @sshleifer comments.

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* Make sure we mask CLS after handling impossible answers

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* Mask in the correct vectors ...

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>
==

src/transformers/data/processors/squad.py
src/transformers/pipelines.py
==================
fa5423b16;Stas Bekman;2020-07-08 16:52:44 -0700;doc fixes (#5613)

==

docs/source/tokenizer_summary.rst
==================
7d0ef0042;Txus;2020-07-08 23:54:51 +0200;Add newly trained `calbert-tiny-uncased` (#5599)
* Create README.md

Add newly trained `calbert-tiny-uncased` (complete rewrite with SentencePiece)

* Add Exbert link

* Apply suggestions from code review

Co-authored-by: Julien Chaumond <chaumond@gmail.com>
==

model_cards/codegram/calbert-tiny-uncased/README.md
==================
0cc4eae0e;Lorenzo Ampil;2020-07-09 04:18:17 +0800;Fix Inconsistent NER Grouping (Pipeline) (#4987)
* Add B I handling to grouping

* Add fix to include separate entity as last token

* move last_idx definition outside loop

* Use first entity in entity group as reference for entity type

* Add test cases

* Take out extra class accidentally added

* Return tf ner grouped test to original

* Take out redundant last entity

* Get last_idx safely

Co-authored-by: ColleterVi <36503688+ColleterVi@users.noreply.github.com>

* Fix first entity comment

* Create separate functions for group_sub_entities and group_entities (splitting call method to testable functions)

* Take out unnecessary last_idx

* Remove additional forward pass test

* Move token classification basic tests to separate class

* Move token classification basic tests back to monocolumninputtestcase

* Move base ner tests to nerpipelinetests

* Take out unused kwargs

* Add back mandatory_keys argument

* Add unitary tests for group_entities in _test_ner_pipeline

* Fix last entity handling

* Fix grouping fucntion used

* Add typing to group_sub_entities and group_entities

Co-authored-by: ColleterVi <36503688+ColleterVi@users.noreply.github.com>
==

src/transformers/pipelines.py
tests/test_pipelines.py
==================
82ce8488b;Suraj Patil;2020-07-09 01:38:56 +0530;create model cards for qg models (#5610)

==

model_cards/valhalla/t5-base-e2e-qg/README.md
model_cards/valhalla/t5-base-qa-qg-hl/README.md
model_cards/valhalla/t5-base-qg-hl/README.md
model_cards/valhalla/t5-samll-qg-prepend/README.md
model_cards/valhalla/t5-small-e2e-qg/README.md
model_cards/valhalla/t5-small-qa-qg-hl/README.md
model_cards/valhalla/t5-small-qg-hl/README.md
==================
d6b6ab11f;Bashar Talafha;2020-07-08 23:07:48 +0300;Create README.md (#5601)

==

model_cards/bashar-talafha/multi-dialect-bert-base-arabic/README.md
==================
40d98ebf5;Patrick von Platen;2020-07-08 16:03:59 +0200;Update benchmark notebook (#5603)
* Cr√©√© avec Colaboratory

* delete old file
==

notebooks/05-benchmark.ipynb
==================
281e39488;Sylvain Gugger;2020-07-08 08:46:35 -0400;Update question template (#5585)

==

.github/ISSUE_TEMPLATE/question-help.md
==================
f82a2a5e8;Patrick von Platen;2020-07-08 12:11:09 +0200;[Benchmark] Add benchmarks for TF Training (#5594)
* tf_train

* adapt timing for tpu

* fix timing

* fix timing

* fix timing

* fix timing

* update notebook

* add tests
==

notebooks/05-benchmark.ipynb
src/transformers/benchmark/benchmark.py
src/transformers/benchmark/benchmark_tf.py
tests/test_benchmark_tf.py
==================
cfbb98297;Ji Xin;2020-07-07 20:17:59 -0400;Add DeeBERT (entropy-based early exiting for *BERT) (#5477)
* Add deebert code

* Add readme of deebert

* Add test for deebert

Update test for Deebert

* Update DeeBert (README, class names, function refactoring); remove requirements.txt

* Format update

* Update test

* Update readme and model init methods
==

examples/deebert/README.md
examples/deebert/entropy_eval.sh
examples/deebert/eval_deebert.sh
examples/deebert/run_glue_deebert.py
examples/deebert/src/__init__.py
examples/deebert/src/modeling_highway_bert.py
examples/deebert/src/modeling_highway_roberta.py
examples/deebert/test_glue_deebert.py
examples/deebert/train_deebert.sh
==================
b4b33fdf2;Joe Davison;2020-07-07 16:04:15 -0600;Guide to fixed-length model perplexity evaluation (#5449)
* add first draft ppl guide

* upload imgs

* expand on strides

* ref typo

* rm superfluous past var

* add tokenization disclaimer
==

docs/source/imgs/ppl_chunked.gif
docs/source/imgs/ppl_full.gif
docs/source/imgs/ppl_sliding.gif
docs/source/index.rst
docs/source/perplexity.rst
==================
fde217c67;Patrick von Platen;2020-07-07 23:21:23 +0200;readme for benchmark (#5363)

==

examples/benchmarking/README.md
==================
d6eab5305;Sam Shleifer;2020-07-07 13:46:05 -0400;mbart.prepare_translation_batch: pass through kwargs (#5581)

==

src/transformers/tokenization_bart.py
==================
353b8f1e7;Sam Shleifer;2020-07-07 13:23:01 -0400;Add mbart-large-cc25, support translation finetuning (#5129)
improve unittests for finetuning, especially w.r.t testing frozen parameters
fix freeze_embeds for T5
add streamlit setup.cfg
==

docs/source/model_doc/bart.rst
examples/longform-qa/eli5_app.py
examples/seq2seq/README.md
examples/seq2seq/finetune.py
examples/seq2seq/finetune_t5.sh
examples/seq2seq/test_seq2seq_examples.py
examples/seq2seq/train_mbart_cc25_enro.sh
examples/seq2seq/utils.py
setup.cfg
src/transformers/tokenization_bart.py
tests/test_modeling_bart.py
tests/test_modeling_mbart.py
tests/test_tokenization_common.py
tests/test_tokenization_mbart.py
==================
141492448;Julien Chaumond;2020-07-07 13:15:10 -0400;Create xlm-roberta-large-finetuned-conll03-german-README.md
cc @BramVanroy
==

model_cards/xlm-roberta-large-finetuned-conll03-german-README.md
==================
4dc65591b;Patrick von Platen;2020-07-07 18:15:53 +0200;[Almost all TF models] TF clean up: add missing CLM / MLM loss; fix T5 naming and keras compile (#5395)
* add first version of clm tf

* make style

* add more tests for bert

* update tf clm loss

* fix tests

* correct tf ner script

* add mlm loss

* delete bogus file

* clean tf auto model + add tests

* finish adding clm loss everywhere

* fix training in distilbert

* fix flake8

* save intermediate

* fix tf t5 naming

* remove prints

* finish up

* up

* fix tf gpt2

* fix new test utils import

* fix flake8

* keep backward compatibility

* Update src/transformers/modeling_tf_albert.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/modeling_tf_auto.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/modeling_tf_electra.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/modeling_tf_roberta.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/modeling_tf_mobilebert.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/modeling_tf_auto.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/modeling_tf_bert.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update src/transformers/modeling_tf_distilbert.py

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* apply sylvains suggestions

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

examples/token-classification/run_tf_ner.py
src/transformers/__init__.py
src/transformers/modeling_auto.py
src/transformers/modeling_tf_albert.py
src/transformers/modeling_tf_auto.py
src/transformers/modeling_tf_bert.py
src/transformers/modeling_tf_ctrl.py
src/transformers/modeling_tf_distilbert.py
src/transformers/modeling_tf_electra.py
src/transformers/modeling_tf_gpt2.py
src/transformers/modeling_tf_mobilebert.py
src/transformers/modeling_tf_openai.py
src/transformers/modeling_tf_roberta.py
src/transformers/modeling_tf_t5.py
src/transformers/modeling_tf_utils.py
src/transformers/modeling_tf_xlnet.py
src/transformers/modeling_xlnet.py
tests/test_modeling_distilbert.py
tests/test_modeling_tf_auto.py
tests/test_modeling_tf_bert.py
tests/test_modeling_tf_common.py
tests/test_modeling_tf_distilbert.py
tests/test_modeling_tf_t5.py
==================
33e43eddd;Suraj Patil;2020-07-07 20:36:12 +0530;[docs] fix model_doc links in model summary (#5566)
* fix model_doc links

* update model links
==

docs/source/model_summary.rst
==================
4fedc1256;Quentin Lhoest;2020-07-07 16:35:12 +0200;Fix tests imports dpr (#5576)
* fix test imports

* fix max_length

* style

* fix tests
==

src/transformers/tokenization_dpr.py
tests/test_modeling_dpr.py
tests/test_tokenization_dpr.py
==================
d4886173b;Sam Shleifer;2020-07-07 10:06:48 -0400;[Bart] enable test_torchscript, update test_tie_weights (#5457)
* Passing all but one torchscript test

* Style

* move comment

* remove unneeded assert
==

tests/test_modeling_bart.py
tests/test_modeling_common.py
==================
e49393c36;Suraj Patil;2020-07-07 18:27:08 +0530;[examples] Add trainer support for question-answering (#4829)
* add SquadDataset

* add DataCollatorForQuestionAnswering

* update __init__

* add run_squad with  trainer

* add DataCollatorForQuestionAnswering in __init__

* pass data_collator to trainer

* doc tweak

* Update run_squad_trainer.py

* Update __init__.py

* Update __init__.py

Co-authored-by: Julien Chaumond <chaumond@gmail.com>
Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

examples/question-answering/README.md
examples/question-answering/run_squad_trainer.py
src/transformers/__init__.py
src/transformers/data/datasets/__init__.py
src/transformers/data/datasets/squad.py
==================
fbd879219;Quentin Lhoest;2020-07-07 14:56:12 +0200;Add DPR model (#5279)
* beginning of dpr modeling

* wip

* implement forward

* remove biencoder + better init weights

* export dpr model to embed model for nlp lib

* add new api

* remove old code

* make style

* fix dumb typo

* don't load bert weights

* docs

* docs

* style

* move the `k` parameter

* fix init_weights

* add pretrained configs

* minor

* update config names

* style

* better config

* style

* clean code based on PR comments

* change Dpr to DPR

* fix config

* switch encoder config to a dict

* style

* inheritance -> composition

* add messages in assert startements

* add dpr reader tokenizer

* one tokenizer per model

* fix base_model_prefix

* fix imports

* typo

* add convert script

* docs

* change tokenizers conf names

* style

* change tokenizers conf names

* minor

* minor

* fix wrong names

* minor

* remove unused convert functions

* rename convert script

* use return_tensors in tokenizers

* remove n_questions dim

* move generate logic to tokenizer

* style

* add docs

* docs

* quality

* docs

* add tests

* style

* add tokenization tests

* DPR full tests

* Stay true to the attention mask building

* update docs

* missing param in bert input docs

* docs

* style

Co-authored-by: Lysandre <lysandre.debut@reseau.eseo.fr>
==

docs/source/index.rst
docs/source/model_doc/dpr.rst
src/transformers/__init__.py
src/transformers/configuration_dpr.py
src/transformers/convert_dpr_original_checkpoint_to_pytorch.py
src/transformers/modeling_bert.py
src/transformers/modeling_dpr.py
src/transformers/tokenization_dpr.py
src/transformers/tokenization_utils_base.py
tests/test_modeling_dpr.py
tests/test_tokenization_dpr.py
==================
d2a939911;Sava≈ü Yƒ±ldƒ±rƒ±m;2020-07-07 13:43:49 +0300;Update model card (#5491)

==

model_cards/savasy/bert-base-turkish-sentiment-cased/README.md
==================
2e653d89d;Sava≈ü Yƒ±ldƒ±rƒ±m;2020-07-07 13:43:34 +0300;Update model card (#5492)

==

model_cards/savasy/bert-base-turkish-ner-cased/README.md
==================
beaf60e58;Sava≈ü Yƒ±ldƒ±rƒ±m;2020-07-07 13:43:09 +0300;bert-turkish-text-classification model card (#5493)

==

model_cards/savasy/bert-turkish-text-classification/README.md
==================
e6eba8419;Manuel Romero;2020-07-07 12:41:42 +0200;electra-small-finetuned-squadv1 model card (#5430)
* Create model card

Create model card for electra-small-discriminator finetuned on SQUAD v1.1

* Set right model path in code example
==

model_cards/mrm8488/electra-small-finetuned-squadv1/README.md
==================
43b7ad5df;Vitalii Radchenko;2020-07-07 13:40:23 +0300;ukr-roberta-base model card (#5514)

==

model_cards/youscan/ukr-roberta-base/README.md
==================
87aa857d7;Manuel Romero;2020-07-07 12:39:09 +0200;roberta-base-1B-1-finetuned-squadv1 model card (#5515)

==

model_cards/mrm8488/roberta-base-1B-1-finetuned-squadv1/README.md
==================
c7d96b60e;Moseli Motsoehli;2020-07-07 00:38:15 -1000;zuBERTa model card (#5536)
* Create README

* Update README.md

Co-authored-by: Kevin Canwen Xu <canwenxu@126.com>
==

model_cards/MoseliMotsoehli/zuBERTa/README.md
==================
b95dfcf11;Manuel Romero;2020-07-07 12:33:42 +0200;roberta-base-1B-1-finetuned-squadv2 model card (#5523)

==

model_cards/mrm8488/roberta-base-1B-1-finetuned-squadv2/README.md
==================
691226571;Abel;2020-07-07 11:32:29 +0200;Make T5 compatible with ONNX (#5518)
* Default decoder inputs to encoder ones for T5 if neither are specified.

* Fixing typo, now all tests are passing.

* Changing einsum to operations supported by onnx

* Adding a test to ensure T5 can be exported to onnx op>9

* Modified test for onnx export to make it faster

* Styling changes.

* Styling changes.

* Changing notation for matrix multiplication

Co-authored-by: Abel Riboulot <tkai@protomail.com>
==

src/transformers/modeling_t5.py
tests/test_modeling_t5.py
==================
989ae326b;Patrick von Platen;2020-07-07 10:48:06 +0200;[Reformer] Adapt Reformer MaskedLM Attn mask (#5560)
* fix attention mask

* fix slow test

* refactor attn masks

* fix fp16 generate test
==

src/transformers/modeling_reformer.py
tests/test_modeling_reformer.py
==================
3dcb748e3;Shashank Gupta;2020-07-07 13:47:37 +0530;Added data collator for permutation (XLNet) language modeling and related calls (#5522)
* Added data collator for XLNet language modeling and related calls

Added DataCollatorForXLNetLanguageModeling in data/data_collator.py
to generate necessary inputs for language modeling training with
XLNetLMHeadModel. Also added related arguments, logic and calls in
examples/language-modeling/run_language_modeling.py.

Resolves: #4739, #2008 (partially)

* Changed name to `DataCollatorForPermutationLanguageModeling`

Changed the name of `DataCollatorForXLNetLanguageModeling` to the more general `DataCollatorForPermutationLanguageModelling`.
Removed the `--mlm` flag requirement for the new collator and defined a separate `--plm_probability` flag for its use.
CTRL uses a CLM loss just like GPT and GPT-2, so should work out of the box with this script (provided `past` is taken care of
similar to `mems` for XLNet).
Changed calls and imports appropriately.

* Added detailed comments, changed variable names

Added more detailed comments to `DataCollatorForPermutationLanguageModeling` in `data/data_collator.py` to explain working. Also cleaned up variable names and made them more informative.

* Added tests for new data collator

Added tests in `tests/test_trainer.py` for DataCollatorForPermutationLanguageModeling based on those in DataCollatorForLanguageModeling. A specific test has been added to check for odd-length sequences.

* Fixed styling issues
==

examples/language-modeling/run_language_modeling.py
src/transformers/__init__.py
src/transformers/data/data_collator.py
tests/test_trainer.py
==================
1d2332861;Lysandre;2020-07-06 18:56:44 -0400;Post v3.0.2 release commit

==

.circleci/deploy.sh
docs/source/_static/js/custom.js
setup.py
==================
b0892fa0e;Lysandre;2020-07-06 18:49:32 -0400;Release: v3.0.2

==

docs/source/conf.py
setup.py
src/transformers/__init__.py
==================
f1e2e423a;Sylvain Gugger;2020-07-06 18:45:01 -0400;Fix fast tokenizers too (#5562)

==

src/transformers/tokenization_gpt2.py
src/transformers/tokenization_roberta.py
==================
5787e4c15;Anthony MOI;2020-07-06 18:27:53 -0400;Various tokenizers fixes (#5558)
* BertTokenizerFast - Do not specify strip_accents by default

* Bump tokenizers to new version

* Add test for AddedToken serialization
==

setup.py
src/transformers/tokenization_bert.py
tests/test_tokenization_common.py
tests/test_tokenization_fast.py
==================
21f28c34b;Sylvain Gugger;2020-07-06 17:26:48 -0400;Fix #5507 (#5559)
* Fix #5507

* Fix formatting
==

src/transformers/tokenization_gpt2.py
src/transformers/tokenization_roberta.py
==================
9d9b872b6;Lysandre Debut;2020-07-06 12:17:05 -0400;The `add_space_before_punct_symbol` is only for TransfoXL (#5549)

==

examples/text-generation/run_generation.py
==================
d6b0b9d45;Lysandre Debut;2020-07-06 11:33:57 -0400;GPT2 tokenizer should not output token type IDs (#5546)
* GPT2 tokenizer should not output token type IDs

* Same for OpenAIGPT
==

src/transformers/tokenization_gpt2.py
src/transformers/tokenization_openai.py
==================
7833b21a5;Sylvain Gugger;2020-07-06 11:22:24 -0400;Fix #5544 (#5551)

==

src/transformers/tokenization_utils.py
==================
c47348408;Thomas Wolf;2020-07-06 17:15:25 +0200;Fix the tokenization warning noted in #5505 (#5550)
* fix warning

* style and quality
==

src/transformers/tokenization_utils.py
==================
1bbc28bee;Lysandre;2020-07-06 10:27:00 -0400;Imports organization

==

src/transformers/convert_pytorch_checkpoint_to_tf2.py
==================
1bc13697b;Mohamed Taher Alrefaie;2020-07-06 15:55:10 +0200;Update convert_pytorch_checkpoint_to_tf2.py (#5531)
fixed ImportError: cannot import name 'hf_bucket_url'
==

src/transformers/convert_pytorch_checkpoint_to_tf2.py
==================
b2309cc6b;Arnav Sharma;2020-07-06 18:45:22 +0530;Typo fix in `training` doc (#5495)

==

docs/source/training.rst
==================
7ecff0ccb;ELanning;2020-07-06 06:14:57 -0700;Fix typo in training (#5510)

==

docs/source/training.rst
==================
58cca47c1;Sam Shleifer;2020-07-03 14:27:49 -0400;[cleanup] TF T5 tests only init t5-base once. (#5410)

==

src/transformers/modeling_tf_t5.py
tests/test_modeling_tf_t5.py
==================
991172922;Patrick von Platen;2020-07-03 19:25:25 +0200;better error message (#5497)

==

src/transformers/generation_tf_utils.py
src/transformers/generation_utils.py
==================
b58a15a31;Thomas Wolf;2020-07-03 17:38:39 +0200;unpining specific git versions in setup.py

==

setup.py
==================
fedabcd15;Thomas Wolf;2020-07-03 17:02:44 +0200;Release: 3.0.1

==

setup.py
src/transformers/__init__.py
==================
17ade127b;Lysandre Debut;2020-07-03 10:51:21 -0400;Exposing prepare_for_model for both slow & fast tokenizers (#5479)
* Exposing prepare_for_model for both slow & fast tokenizers

* Update method signature

* The traditional style commit

* Hide the warnings behind the verbose flag

* update default truncation strategy and prepare_for_model

* fix tests and prepare_for_models methods

Co-authored-by: Thomas Wolf <thomwolf@users.noreply.github.com>
==

src/transformers/tokenization_utils.py
src/transformers/tokenization_utils_base.py
tests/test_tokenization_common.py
tests/test_tokenization_fast.py
==================
814ed7ee7;Manuel Romero;2020-07-03 14:29:09 +0200;Create model card (#5396)
Create model card for electicidad-small (Spanish Electra) fine-tuned on SQUAD-esv1
==

model_cards/mrm8488/electricidad-small-finetuned-squadv1-es/README.md
==================
49281ac93;Moseli Motsoehli;2020-07-03 02:25:57 -1000;grammar corrections and train data update (#5448)
- fixed grammar and spelling
- added an intro
- updated Training data references
==

model_cards/MoseliMotsoehli/TswanaBert/README.md
==================
97355339f;chrisliu;2020-07-03 05:16:27 -0700;Update upstream (#5456)

==

model_cards/chrisliu298/arxiv_ai_gpt2/README.md
==================
55b932a81;Manuel Romero;2020-07-03 12:19:49 +0200;Create model card (#5464)
Create model card for electra-small-discriminator fine-tuned on SQUAD v2.0
==

model_cards/mrm8488/electra-small-finetuned-squadv2/README.md
==================
21cd8c408;Funtowicz Morgan;2020-07-03 10:29:20 +0200;QA Pipelines fixes (#5429)
* Make QA pipeline supports models with more than 2 outputs such as BART assuming start/end are the two first outputs.

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* When using the new padding/truncation paradigm setting padding="max_length" + max_length=X actually pads the input up to max_length.

This result in every sample going through QA pipelines to be of size 384 whatever the actual input size is making the overall pipeline very slow.

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* Mask padding & question before applying softmax. Softmax has been refactored to operate in log space for speed and stability.

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* Format.

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* Use PaddingStrategy.LONGEST instead of DO_NOT_PAD

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* Revert "When using the new padding/truncation paradigm setting padding="max_length" + max_length=X actually pads the input up to max_length."

This reverts commit 1b00a9a2

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* Trigger CI after unattended failure

* Trigger CI
==

src/transformers/pipelines.py
==================
8438bab38;Pierric Cistac;2020-07-02 19:23:55 -0400;Fix roberta model ordering for TFAutoModel (#5414)

==

src/transformers/modeling_tf_auto.py
==================
6b735a725;Sylvain Gugger;2020-07-02 17:07:42 -0400;Tokenizer summary (#5467)
* Work on tokenizer summary

* Finish tutorial

* Link to it

* Apply suggestions from code review

Co-authored-by: Anthony MOI <xn1t0x@gmail.com>
Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* Add vocab definition

Co-authored-by: Anthony MOI <xn1t0x@gmail.com>
Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
==

docs/source/index.rst
docs/source/quicktour.rst
docs/source/tokenizer_summary.rst
==================
ef0e9d806;Shen;2020-07-02 12:57:33 -0500;Update: ElectraDiscriminatorPredictions forward. (#5471)
`ElectraDiscriminatorPredictions.forward` should not need `attention_mask`.
==

src/transformers/modeling_electra.py
==================
13a8588f2;Manuel Romero;2020-07-02 16:16:30 +0200;Create model card (#5432)
Create model card for electra-base-discriminator fine-tuned on SQUAD v1.1
==

model_cards/mrm8488/electra-base-finetuned-squadv1/README.md
==================
a0a6387a0;Julien Chaumond;2020-07-02 10:04:02 -0400;[model_cards] roberta-large-mnli: fix sep_token

==

model_cards/roberta-large-mnli-README.md
==================
215db688d;Julien Chaumond;2020-07-02 09:43:54 -0400;Create roberta-large-mnli-README.md

==

model_cards/roberta-large-mnli-README.md
==================
69d313e80;Lysandre Debut;2020-07-02 09:23:00 -0400;Bans SentencePiece 0.1.92 (#5418)

==

setup.py
==================
84e56669a;George Ho;2020-07-02 09:19:33 -0400;Fix typo in glossary (#5466)

==

docs/source/glossary.rst
==================
c6a510c6f;Teven;2020-07-02 13:53:33 +0200;Fixing missing arguments for TransfoXL tokenizer when using TextGenerationPipeline (#5465)
* overriding _parse_and_tokenize in `TextGenerationPipeine` to allow for TransfoXl tokenizer arguments
==

src/transformers/pipelines.py
==================
6726416e4;Teven;2020-07-02 11:56:44 +0200;Changed expected_output_ids in TransfoXL generation test (#5462)
* Changed expected_output_ids in TransfoXL generation test to match #4826 generation PR.

* making black happy

* making isort happy
==

tests/test_modeling_transfo_xl.py
==================
812def00c;tommccoy;2020-07-02 02:19:07 -0700;fix use of mems in Transformer-XL (#4826)
Fixed duplicated memory use in Transformer-XL generation leading to bad predictions and performance.
==

src/transformers/modeling_transfo_xl.py
==================
306f1a269;Patrick von Platen;2020-07-02 00:20:49 +0200;Add Reformer MLM notebook (#5450)
* Add Reformer MLM notebook

* Update notebooks/README.md
==

notebooks/README.md
==================
d16e36c7e;Patrick von Platen;2020-07-01 22:43:18 +0200;[Reformer] Add Masked LM Reformer (#5426)
* fix conflicts

* fix

* happy rebasing
==

docs/source/model_doc/reformer.rst
src/transformers/__init__.py
src/transformers/modeling_auto.py
src/transformers/modeling_reformer.py
tests/test_modeling_reformer.py
==================
f4323dbf8;Funtowicz Morgan;2020-07-01 20:30:42 +0200;Don't discard entity_group when token is the latest in the sequence. (#5439)
Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>
==

src/transformers/pipelines.py
==================
35befd9ce;Joe Davison;2020-07-01 10:40:14 -0600;Fix tensor label type inference in default collator (#5250)
* allow tensor label inputs to default collator

* replace try/except with type check
==

src/transformers/data/data_collator.py
tests/test_trainer.py
==================
fe81f7d12;Patrick von Platen;2020-07-01 18:27:14 +0200;finish reformer qa head (#5433)

==

docs/source/model_doc/reformer.rst
src/transformers/__init__.py
src/transformers/modeling_auto.py
src/transformers/modeling_reformer.py
tests/test_modeling_reformer.py
==================
d697b6ca7;Patrick von Platen;2020-07-01 17:43:32 +0200;[Longformer] Major Refactor (#5219)
* refactor naming

* add small slow test

* refactor

* refactor naming

* rename selected to extra

* big global attention refactor

* make style

* refactor naming

* save intermed

* refactor functions

* finish function refactor

* fix tests

* fix longformer

* fix longformer

* fix longformer

* fix all tests but one

* finish longformer

* address sams and izs comments

* fix transpose
==

src/transformers/modeling_longformer.py
tests/test_modeling_common.py
tests/test_modeling_longformer.py
==================
e0d58ddb6;Sam Shleifer;2020-07-01 11:42:22 -0400;[fix] Marian tests import (#5442)

==

tests/test_tokenization_marian.py
==================
608d5a7c4;Funtowicz Morgan;2020-07-01 17:27:47 +0200;Raises PipelineException on FillMaskPipeline when there are != 1 mask_token in the input (#5389)
* Added PipelineException

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* fill-mask pipeline raises exception when more than one mask_token detected.

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* Put everything in a function.

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* Added tests on pipeline fill-mask when input has != 1 mask_token

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* Fix numel() computation for TF

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* Addressing PR comments.

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* Remove function typing to avoid import on specific framework.

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* Quality.

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* Retry typing with @julien-c tip.

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* Quality¬≤.

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* Simplify fill-mask mask_token checking.

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* Trigger CI
==

src/transformers/pipelines.py
tests/test_pipelines.py
==================
6c55e9fc3;Sylvain Gugger;2020-07-01 11:02:59 -0400;Fix dropdown bug in searches (#5440)
* Trigger CI

* Fix dropdown bug in searches
==

docs/source/_static/js/custom.js
==================
734a28a76;Sylvain Gugger;2020-07-01 11:00:20 -0400;Clean up diffs in Trainer/TFTrainer (#5417)
* Cleanup and unify Trainer/TFTrainer

* Forgot to adapt TFTrainingArgs

* In tf scripts n_gpu -> n_replicas

* Update src/transformers/training_args.py

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* Address review comments

* Formatting

* Fix typo

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
==

examples/multiple-choice/run_tf_multiple_choice.py
examples/question-answering/run_tf_squad.py
examples/text-classification/run_tf_glue.py
examples/token-classification/run_tf_ner.py
src/transformers/__init__.py
src/transformers/trainer.py
src/transformers/trainer_tf.py
src/transformers/trainer_utils.py
src/transformers/training_args.py
src/transformers/training_args_tf.py
==================
43cb03a93;Sam Shleifer;2020-07-01 10:32:50 -0400;MarianTokenizer.prepare_translation_batch uses new tokenizer API (#5182)

==

src/transformers/tokenization_marian.py
tests/test_tokenization_marian.py
==================
13deb95a4;Sam Shleifer;2020-07-01 10:31:17 -0400;Move tests/utils.py -> transformers/testing_utils.py (#5350)

==

examples/seq2seq/finetune_bart_tiny.sh
examples/seq2seq/test_seq2seq_examples.py
src/transformers/testing_utils.py
tests/test_activations.py
tests/test_benchmark.py
tests/test_benchmark_tf.py
tests/test_configuration_auto.py
tests/test_doc_samples.py
tests/test_modeling_albert.py
tests/test_modeling_auto.py
tests/test_modeling_bart.py
tests/test_modeling_bert.py
tests/test_modeling_camembert.py
tests/test_modeling_common.py
tests/test_modeling_ctrl.py
tests/test_modeling_distilbert.py
tests/test_modeling_electra.py
tests/test_modeling_encoder_decoder.py
tests/test_modeling_flaubert.py
tests/test_modeling_gpt2.py
tests/test_modeling_longformer.py
tests/test_modeling_marian.py
tests/test_modeling_mobilebert.py
tests/test_modeling_openai.py
tests/test_modeling_reformer.py
tests/test_modeling_roberta.py
tests/test_modeling_t5.py
tests/test_modeling_tf_albert.py
tests/test_modeling_tf_auto.py
tests/test_modeling_tf_bert.py
tests/test_modeling_tf_camembert.py
tests/test_modeling_tf_common.py
tests/test_modeling_tf_ctrl.py
tests/test_modeling_tf_distilbert.py
tests/test_modeling_tf_electra.py
tests/test_modeling_tf_flaubert.py
tests/test_modeling_tf_gpt2.py
tests/test_modeling_tf_mobilebert.py
tests/test_modeling_tf_openai_gpt.py
tests/test_modeling_tf_roberta.py
tests/test_modeling_tf_t5.py
tests/test_modeling_tf_transfo_xl.py
tests/test_modeling_tf_xlm.py
tests/test_modeling_tf_xlm_roberta.py
tests/test_modeling_tf_xlnet.py
tests/test_modeling_transfo_xl.py
tests/test_modeling_xlm.py
tests/test_modeling_xlm_roberta.py
tests/test_modeling_xlnet.py
tests/test_onnx.py
tests/test_optimization.py
tests/test_optimization_tf.py
tests/test_pipelines.py
tests/test_tokenization_auto.py
tests/test_tokenization_bert.py
tests/test_tokenization_bert_japanese.py
tests/test_tokenization_common.py
tests/test_tokenization_distilbert.py
tests/test_tokenization_fast.py
tests/test_tokenization_roberta.py
tests/test_tokenization_transfo_xl.py
tests/test_tokenization_utils.py
tests/test_tokenization_xlm.py
tests/test_tokenization_xlm_roberta.py
tests/test_tokenization_xlnet.py
tests/test_trainer.py
==================
9c219305f;sgugger;2020-07-01 10:22:50 -0400;Trigger CI

==
==================
64e3d966b;Sylvain Gugger;2020-07-01 08:11:55 -0400;Add support for past states (#5399)
* Add support for past states

* Style and forgotten self

* You mean, documenting is not enough? I have to actually add it too?

* Add memory support during evaluation

* Fix tests in eval and add TF support

* No need to change this line anymore
==

src/transformers/trainer.py
src/transformers/trainer_tf.py
src/transformers/training_args.py
src/transformers/training_args_tf.py
==================
4ade7491f;Sylvain Gugger;2020-07-01 08:11:25 -0400;Fix examples titles and optimization doc page (#5408)

==

docs/source/main_classes/optimizer_schedules.rst
examples/README.md
src/transformers/optimization.py
src/transformers/optimization_tf.py
==================
d60d231ea;Moseli Motsoehli;2020-06-30 23:01:51 -1000;Create README.md (#5422)
* Create README.md

* Update model_cards/MoseliMotsoehli/TswanaBert/README.md

Co-authored-by: Julien Chaumond <chaumond@gmail.com>
==

model_cards/MoseliMotsoehli/TswanaBert/README.md
==================
298bdab18;Jay;2020-07-01 01:01:56 -0700;Create model card for schmidek/electra-small-cased (#5400)

==

model_cards/schmidek/electra-small-cased/README.md
==================
fcf065246;Julien Plu;2020-07-01 01:49:11 +0200;Fix TensorFlow dataset generator (#4881)
* fix TensorFlow generator

* Better features handling

* Apply style

* Apply style

* Fix squad as well

* Apply style

* Better factorization of TF Tensors creation
==

src/transformers/data/processors/glue.py
src/transformers/data/processors/squad.py
==================
501040fd3;Hong Xu;2020-06-30 16:45:35 -0700;In the run_ner.py example, give the optional label arg a default value (#5326)
Otherwise, if label is not specified, the following error occurs:

	Traceback (most recent call last):
	  File "run_ner.py", line 303, in <module>
	    main()
	  File "run_ner.py", line 101, in main
	    model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))
	  File "/home/user/anaconda3/envs/bert/lib/python3.7/site-packages/transformers/hf_argparser.py", line 159, in parse_json_file
	    obj = dtype(**inputs)
	TypeError: __init__() missing 1 required positional argument: 'labels'
==

examples/token-classification/run_ner.py
==================
b45e65efa;Sam Shleifer;2020-06-30 16:41:43 -0400;Avoid deprecation warning for F.tanh (#5413)

==

src/transformers/modeling_mobilebert.py
==================
23231c0f7;Sam Shleifer;2020-06-30 16:17:12 -0400;[GH Runner] fix yaml indent (#5412)

==

.github/workflows/self-push.yml
==================
ac6111459;Sam Shleifer;2020-06-30 16:12:14 -0400;[CI] gh runner doesn't use -v, cats new result (#5409)

==

.circleci/config.yml
.github/workflows/self-push.yml
.github/workflows/self-scheduled.yml
==================
27a7fe7a8;Sam Shleifer;2020-06-30 15:29:13 -0400;examples/seq2seq: never override $WANDB_PROJECT (#5407)

==

examples/seq2seq/README.md
examples/seq2seq/finetune.py
==================
32d203145;Sam Shleifer;2020-06-30 15:28:15 -0400;[fix] slow fill_mask test failure (#5406)

==

src/transformers/pipelines.py
tests/test_pipelines.py
==================
80aa4b8aa;Sam Shleifer;2020-06-30 15:01:53 -0400;[CI] GH-runner stores artifacts like CircleCI (#5318)

==

.github/workflows/self-push.yml
.github/workflows/self-scheduled.yml
==================
87716a6d0;Sylvain Gugger;2020-06-30 11:43:43 -0400;Documentation for the Trainer API (#5383)
* Documentation for the Trainer API

* Address review comments

* Address comments
==

docs/source/index.rst
docs/source/main_classes/trainer.rst
src/transformers/__init__.py
src/transformers/trainer.py
src/transformers/trainer_tf.py
src/transformers/trainer_utils.py
src/transformers/training_args.py
src/transformers/training_args_tf.py
==================
c4d4e8bdb;Yacine Jernite;2020-06-30 10:42:08 -0400;Move GenerationMixin to separate file (#5254)
* separate_generation_code

* isort

* renamed

* rename_files

* move_shapelit
==

src/transformers/__init__.py
src/transformers/generation_tf_utils.py
src/transformers/generation_utils.py
src/transformers/modeling_tf_utils.py
src/transformers/modeling_utils.py
==================
90d13954c;Lysandre;2020-06-30 09:16:36 -0400;Repin versions

==

setup.py
==================
0607b8894;Sylvain Gugger;2020-06-30 08:59:32 -0400;How to share model cards with the CLI (#5374)
* How to share model cards

* Switch the two options

* Fix bad copy/cut

* Julien's suggestion
==

docs/source/model_sharing.rst
==================
331d8d293;Kevin Canwen Xu;2020-06-30 18:11:11 +0800;Upload DistilBART artwork (#5394)

==

examples/seq2seq/README.md
==================
09e841490;Manuel Romero;2020-06-30 12:02:24 +0200;Model Card Fixing (#5369)
- Fix missing ```-``` in language meta
- T5 pic uploaded to a more permanent place
==

model_cards/mrm8488/t5-base-finetuned-sarcasm-twitter/README.md
==================
4c5bed192;Manuel Romero;2020-06-30 12:01:45 +0200;Model Card Fixing (#5373)
- T5 pic uploaded to a more permanent place
==

model_cards/mrm8488/t5-base-finetuned-squadv2/README.md
==================
02509d4b0;Manuel Romero;2020-06-30 12:01:11 +0200;Model Card Fixing (#5371)
- Model pic uploaded to a more permanent place
==

model_cards/mrm8488/t5-base-finetuned-summarize-news/README.md
==================
79f0118c7;Manuel Romero;2020-06-30 12:00:29 +0200;Model Card Fixing (#5370)
- Fix missing ```-``` in language meta
- T5 pic uploaded to a more permanent place
==

model_cards/mrm8488/t5-base-finetuned-emotion/README.md
==================
9a473f1e4;MichaelJanz;2020-06-30 08:05:01 +0200;Update Bertabs example to work again (#5355)
* Fix the bug 'Attempted relative import with no known parent package' when using the bertabs example. Also change the used model from bertabs-finetuned-cnndm, since it seems not be accessible anymore

* Update run_summarization.py

Co-authored-by: Kevin Canwen Xu <canwenxu@126.com>
==

examples/seq2seq/bertabs/run_summarization.py
==================
7f60e93ac;Sylvain Gugger;2020-06-29 18:27:36 -0400;Mention openAI model card and merge content (#5378)
* Mention openAI model card and merge content

* Fix sentence
==

model_cards/gpt2-README.md
==================
482a5993c;chrisliu;2020-06-29 09:54:30 -0700;Fix model card folder name so that it is consistent with model hub (#5368)
* Merge upstream

* Merge upstream

* Add generate.py link

* Merge upstream

* Merge upstream

* Fix folder name
==

model_cards/chrisliu298/arxiv_ai_gpt2/README.md
==================
97f24303e;chrisliu;2020-06-29 08:34:52 -0700;Add link to file and fix typos in model card (#5367)
* Merge upstream

* Merge upstream

* Add generate.py link
==

model_cards/chrisliu298/arxiv-ai-gpt2/README.md
==================
b9ee87f5c;Lysandre Debut;2020-06-29 11:08:54 -0400;Doc for v3.0.0 (#5366)
* Doc for v3.0.0

* Update docs/source/_static/js/custom.js

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Update docs/source/_static/js/custom.js

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

.circleci/deploy.sh
docs/source/_static/js/custom.js
==================
b62ca5952;Lysandre;2020-06-29 10:38:34 -0400;Release: v3.0.0

==

docs/source/conf.py
setup.py
src/transformers/__init__.py
==================
a316a6aaa;Sam Shleifer;2020-06-29 10:36:04 -0400;[seq2seq docs] Move evaluation down, fix typo (#5365)

==

examples/seq2seq/README.md
==================
4bcc35cd6;Patrick von Platen;2020-06-29 16:08:57 +0200;[Docs] Benchmark docs (#5360)
* first doc version

* add benchmark docs

* fix typos

* improve README

* Update docs/source/benchmarks.rst

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* fix naming and docs

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
==

docs/source/benchmarks.md
docs/source/benchmarks.rst
examples/benchmarking/run_benchmark_tf.py
examples/benchmarking/time_xla_1.csv
notebooks/05-benchmark.ipynb
src/transformers/__init__.py
src/transformers/benchmark/benchmark_args_tf.py
src/transformers/benchmark/benchmark_tf.py
src/transformers/benchmark/benchmark_utils.py
tests/test_benchmark_tf.py
==================
482c9178d;Sylvain Gugger;2020-06-29 09:51:13 -0400;Pin mecab for now (#5362)

==

setup.py
==================
2513fe0d0;Clement;2020-06-29 09:05:08 -0400;added subtitle for recent contributors in readme (#5130)

==

README.md
==================
30245c0c6;Manuel Romero;2020-06-29 15:02:33 +0200;Fix table format fot test tesults (#5357)

==

model_cards/mrm8488/t5-base-finetuned-emotion/README.md
==================
c34010551;Manuel Romero;2020-06-29 15:01:55 +0200;Create model card (#5356)

==

model_cards/mrm8488/t5-base-finetuned-sarcasm-twitter/README.md
==================
01aa0b852;Ali Safaya;2020-06-29 15:58:30 +0300;Create README.md (#5353)

==

model_cards/asafaya/bert-large-arabic/README.md
==================
96907367f;chrisliu;2020-06-29 05:53:20 -0700;arxiv-ai-gpt2 model card (#5337)
* Add model card and generation script for model arxiv_ai_gpt2

* Update arxiv-ai-gpt2 model card

Remove unnecessary lines

* Delete code in model cards
==

model_cards/chrisliu298/arxiv-ai-gpt2/README.md
==================
3cdf8b7ec;Ali Safaya;2020-06-29 15:41:41 +0300;Create model card for asafaya/bert-mini-arabic (#5352)
* Create README.md

* Update model_cards/asafaya/bert-mini-arabic/README.md

Co-authored-by: Julien Chaumond <chaumond@gmail.com>
==

model_cards/asafaya/bert-mini-arabic/README.md
==================
9db1f4160;Ali Safaya;2020-06-29 15:36:00 +0300;Create README.md (#5351)

==

model_cards/asafaya/bert-medium-arabic/README.md
==================
c950fef54;Julien Chaumond;2020-06-29 14:24:33 +0200;[docs] Small tweaks to #5323

==

docs/source/model_sharing.rst
==================
4544f906e;Sylvain Gugger;2020-06-29 05:06:05 -0400;model cards for roberta and bert-multilingual (#5324)
* More model cards (cc @myleott)

* Apply suggestions from code review

Co-authored-by: Julien Chaumond <chaumond@gmail.com>
==

model_cards/bert-base-multilingual-cased-README.md
model_cards/bert-base-multilingual-uncased-README.md
model_cards/roberta-base-README.md
model_cards/roberta-large-README.md
==================
92671532e;sgugger;2020-06-26 10:07:46 -0400;More model cards

==

model_cards/bert-base-cased-README.md
model_cards/bert-base-uncased-README.md
model_cards/distilbert-base-uncased-README.md
==================
9209d36f9;Pradhy729;2020-06-29 01:29:14 -0700;Added a model card README.md for my pretrained model. (#5325)
* Create README.md

* Removed unnecessary link from README.md

* Update README.md
==

model_cards/pradhyra/AWSBlogBert/README.md
==================
7cb52f53e;Julien Plu;2020-06-29 08:38:32 +0200;Fix LR decay in TF Trainer (#5269)
* Recover old PR

* Apply style

* Trigger CI
==

src/transformers/trainer_tf.py
==================
321c05aba;krevas;2020-06-29 14:47:44 +0900;Model cards for finance-koelectra models (#5313)
* Add finance-koelectra readme card

* Add finance-koelectra readme card

* Add finance-koelectra readme card

* Add finance-koelectra readme card
==

model_cards/krevas/finance-koelectra-base-discriminator/README.md
model_cards/krevas/finance-koelectra-base-generator/README.md
model_cards/krevas/finance-koelectra-small-discriminator/README.md
model_cards/krevas/finance-koelectra-small-generator/README.md
==================
28a690a80;Sam Shleifer;2020-06-28 15:08:28 -0400;[mBART] skip broken forward pass test, stronger integration test (#5327)

==

src/transformers/tokenization_bart.py
tests/test_modeling_bart.py
==================
45e26125d;Sam Shleifer;2020-06-28 14:53:47 -0400;save_pretrained: mkdir(exist_ok=True) (#5258)
* all save_pretrained methods mkdir if not os.path.exists
==

examples/bert-loses-patience/run_glue_with_pabee.py
examples/contrib/mm-imdb/run_mmimdb.py
examples/contrib/run_swag.py
examples/distillation/run_squad_w_distillation.py
examples/movement-pruning/masked_run_glue.py
examples/movement-pruning/masked_run_squad.py
examples/question-answering/run_squad.py
examples/seq2seq/distillation.py
examples/text-classification/run_xnli.py
src/transformers/configuration_utils.py
src/transformers/convert_pytorch_checkpoint_to_tf2.py
src/transformers/modeling_tf_utils.py
src/transformers/modeling_utils.py
src/transformers/pipelines.py
src/transformers/tokenization_utils_base.py
templates/adding_a_new_example_script/run_xxx.py
==================
12dfbd4f7;Suraj Patil;2020-06-28 22:24:54 +0530;[examples] fix example links (#5344)

==

examples/README.md
==================
98109464c;Patrick von Platen;2020-06-28 14:32:25 +0200;clean reformer reverse sort (#5343)

==

src/transformers/modeling_reformer.py
==================
1af58c070;Sylvain Gugger;2020-06-27 11:10:02 -0400;New model sharing tutorial (#5323)

==

docs/source/index.rst
docs/source/model_sharing.md
docs/source/model_sharing.rst
docs/source/quicktour.rst
docs/source/serialization.rst
==================
efae6645e;Sylvain Gugger;2020-06-27 11:09:51 -0400;Fix `xxx_length` behavior when using XLNet in pipeline (#5319)

==

src/transformers/pipelines.py
==================
393b8dc09;Sam Shleifer;2020-06-26 19:20:43 -0400;examples/seq2seq/run_eval.py fixes and docs (#5322)

==

examples/seq2seq/README.md
examples/seq2seq/run_eval.py
examples/seq2seq/test_seq2seq_examples.py
examples/seq2seq/utils.py
tests/test_modeling_bart.py
==================
5543b30aa;Sam Shleifer;2020-06-26 15:03:41 -0400;[pl_examples] default warmup steps=0 (#5316)

==

examples/lightning_base.py
examples/seq2seq/README.md
examples/seq2seq/finetune.py
examples/seq2seq/test_seq2seq_examples.py
examples/seq2seq/train_distilbart_cnn.sh
examples/seq2seq/train_distilbart_xsum.sh
==================
bf0d12c22;Sam Shleifer;2020-06-26 13:59:31 -0400;CircleCI stores cleaner output at test_outputs.txt (#5291)

==

.circleci/config.yml
==================
601d4d699;Thomas Wolf;2020-06-26 19:48:14 +0200;[tokenizers] Updates data processors, docstring, examples and model cards to the new API (#5308)
* remove references to old API in docstring - update data processors

* style

* fix tests - better type checking error messages

* better type checking

* include awesome fix by @LysandreJik for #5310

* updated doc and examples
==

README.md
docs/README.md
docs/source/main_classes/tokenizer.rst
docs/source/task_summary.rst
docs/source/training.rst
examples/adversarial/utils_hans.py
examples/longform-qa/eli5_utils.py
examples/movement-pruning/emmental/modeling_bert_masked.py
examples/multiple-choice/utils_multiple_choice.py
examples/seq2seq/run_eval.py
examples/seq2seq/utils.py
model_cards/SparkBeyond/roberta-large-sts-b/README.md
model_cards/a-ware/bart-squadv2/README.md
model_cards/a-ware/xlmroberta-squadv2/README.md
model_cards/google/reformer-enwik8/README.md
model_cards/lserinol/bert-turkish-question-answering/README.md
model_cards/mrm8488/longformer-base-4096-finetuned-squadv2/README.md
model_cards/mrm8488/t5-base-finetuned-squadv2/README.md
model_cards/oliverguhr/german-sentiment-bert/README.md
model_cards/valhalla/bart-large-finetuned-squadv1/README.md
model_cards/valhalla/longformer-base-4096-finetuned-squadv1/README.md
model_cards/valhalla/t5-base-squad/README.md
notebooks/02-transformers.ipynb
notebooks/04-onnx-export.ipynb
src/transformers/convert_graph_to_onnx.py
src/transformers/data/datasets/language_modeling.py
src/transformers/data/processors/glue.py
src/transformers/data/processors/squad.py
src/transformers/file_utils.py
src/transformers/modeling_albert.py
src/transformers/modeling_bert.py
src/transformers/modeling_ctrl.py
src/transformers/modeling_distilbert.py
src/transformers/modeling_electra.py
src/transformers/modeling_flaubert.py
src/transformers/modeling_gpt2.py
src/transformers/modeling_longformer.py
src/transformers/modeling_mobilebert.py
src/transformers/modeling_openai.py
src/transformers/modeling_reformer.py
src/transformers/modeling_retribert.py
src/transformers/modeling_roberta.py
src/transformers/modeling_tf_albert.py
src/transformers/modeling_tf_bert.py
src/transformers/modeling_tf_ctrl.py
src/transformers/modeling_tf_distilbert.py
src/transformers/modeling_tf_electra.py
src/transformers/modeling_tf_flaubert.py
src/transformers/modeling_tf_gpt2.py
src/transformers/modeling_tf_mobilebert.py
src/transformers/modeling_tf_openai.py
src/transformers/modeling_tf_roberta.py
src/transformers/modeling_tf_transfo_xl.py
src/transformers/modeling_tf_xlm.py
src/transformers/modeling_tf_xlnet.py
src/transformers/modeling_transfo_xl.py
src/transformers/modeling_xlm.py
src/transformers/modeling_xlnet.py
src/transformers/pipelines.py
src/transformers/tokenization_albert.py
src/transformers/tokenization_bart.py
src/transformers/tokenization_bert.py
src/transformers/tokenization_camembert.py
src/transformers/tokenization_roberta.py
src/transformers/tokenization_utils.py
src/transformers/tokenization_utils_base.py
src/transformers/tokenization_xlm.py
src/transformers/tokenization_xlm_roberta.py
src/transformers/tokenization_xlnet.py
templates/adding_a_new_model/tokenization_xxx.py
tests/test_modeling_bart.py
tests/test_modeling_t5.py
tests/test_modeling_tf_t5.py
==================
fd405e9a9;Kevin Canwen Xu;2020-06-27 00:53:10 +0800;Add BART-base modeling and configuration (#5315)

==

src/transformers/configuration_bart.py
src/transformers/modeling_bart.py
==================
798dbff6a;Sam Shleifer;2020-06-26 11:43:23 -0400;[pipelines] Change summarization default to distilbart-cnn-12-6 (#5289)

==

src/transformers/pipelines.py
tests/test_pipelines.py
==================
834b6884c;Patrick von Platen;2020-06-26 17:38:13 +0200;Add benchmark notebook (#5312)
* add notebook

* Cr√©√© avec Colaboratory

* move notebook to correct folder

* correct link

* correct filename

* correct filename

* better name
==

notebooks/05-benchmark.ipynb
notebooks/README.md
==================
08c9607c3;Patrick von Platen;2020-06-26 16:58:11 +0200;[Generation] fix docs for decoder_input_ids (#5306)
* fix docs

* Update src/transformers/modeling_utils.py

* Update src/transformers/modeling_tf_utils.py

* Update src/transformers/modeling_tf_utils.py

* Update src/transformers/modeling_utils.py

* Update src/transformers/modeling_tf_utils.py

* Update src/transformers/modeling_utils.py
==

src/transformers/modeling_tf_utils.py
src/transformers/modeling_utils.py
==================
79a82cc06;Patrick von Platen;2020-06-26 15:00:14 +0200;[Benchmarks] improve Example Plotter (#5245)
* improve plotting

* better labels

* fix time plot
==

examples/benchmarking/plot_csv_file.py
examples/benchmarking/time_xla_1.csv
src/transformers/benchmark/benchmark_args_utils.py
==================
88d7f96e3;Sylvain Gugger;2020-06-26 08:08:31 -0400;Gpt2 model card (#5283)
* Bert base model card

* Add metadata

* Adapt examples

* GPT2 model card

* Remove the BERT model card

* Change language code
==

model_cards/gpt2-README.md
==================
fc5bce9e6;Sylvain Gugger;2020-06-26 08:01:19 -0400;Bert base model card (#5276)
* Bert base model card

* Add metadata

* Adapt examples

* Comment on text generation

* Update model_cards/bert-base-uncased-README.md

Co-authored-by: Julien Chaumond <chaumond@gmail.com>
==

model_cards/bert-base-uncased-README.md
==================
135791e8e;Funtowicz Morgan;2020-06-26 11:55:57 +0200;Add pad_to_multiple_of on tokenizers (reimport) (#5054)
* Add new parameter `pad_to_multiple_of` on tokenizers.

* unittest for pad_to_multiple_of

* Add .name when logging enum.

* Fix missing .items() on dict in tests.

* Add special check + warning if the tokenizer doesn't have proper pad_token.

* Use the correct logger format specifier.

* Ensure tokenizer with no pad_token do not modify the underlying padding strategy.

* Skip test if tokenizer doesn't have pad_token

* Fix RobertaTokenizer on empty input

* Format.

Signed-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>

* fix and updating to simpler API

Co-authored-by: Thomas Wolf <thomwolf@users.noreply.github.com>
==

src/transformers/tokenization_roberta.py
src/transformers/tokenization_utils.py
src/transformers/tokenization_utils_base.py
src/transformers/tokenization_utils_fast.py
tests/test_tokenization_common.py
==================
7cc15bdd9;Lysandre Debut;2020-06-25 18:19:21 -0400;Closes #5218

==

examples/README.md
==================
2ffef0d0c;Joe Davison;2020-06-25 15:11:11 -0600;Training & fine-tuning quickstart (#5034)
* add initial fine-tuning guide

* split code blocks to smaller segments

* fix up trianer section of fine-tune doc

* a few last typos

* Update usage -> task summary link

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

docs/source/index.rst
docs/source/training.rst
==================
364a5ae1f;Lysandre Debut;2020-06-25 16:46:00 -0400;Refactor Code samples; Test code samples (#5036)
* Refactor code samples

* Test docstrings

* Style

* Tokenization examples

* Run rust of tests

* First step to testing source docs

* Style and BART comment

* Test the remainder of the code samples

* Style

* let to const

* Formatting fixes

* Ready for merge

* Fix fixture + Style

* Fix last tests

* Update docs/source/quicktour.rst

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>

* Addressing @sgugger's comments + Fix MobileBERT in TF

Co-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>
==

docs/source/_static/css/code-snippets.css
docs/source/_static/css/huggingface.css
docs/source/_static/js/custom.js
docs/source/conf.py
docs/source/glossary.rst
docs/source/multilingual.rst
docs/source/quicktour.rst
docs/source/task_summary.rst
setup.py
src/transformers/configuration_albert.py
src/transformers/configuration_bart.py
src/transformers/configuration_bert.py
src/transformers/configuration_ctrl.py
src/transformers/configuration_distilbert.py
src/transformers/configuration_electra.py
src/transformers/configuration_encoder_decoder.py
src/transformers/configuration_gpt2.py
src/transformers/configuration_longformer.py
src/transformers/configuration_mobilebert.py
src/transformers/configuration_openai.py
src/transformers/configuration_reformer.py
src/transformers/configuration_roberta.py
src/transformers/configuration_transfo_xl.py
src/transformers/configuration_xlm.py
src/transformers/configuration_xlnet.py
src/transformers/data/processors/squad.py
src/transformers/file_utils.py
src/transformers/modeling_albert.py
src/transformers/modeling_auto.py
src/transformers/modeling_bart.py
src/transformers/modeling_bert.py
src/transformers/modeling_camembert.py
src/transformers/modeling_ctrl.py
src/transformers/modeling_distilbert.py
src/transformers/modeling_electra.py
src/transformers/modeling_encoder_decoder.py
src/transformers/modeling_flaubert.py
src/transformers/modeling_gpt2.py
src/transformers/modeling_longformer.py
src/transformers/modeling_marian.py
src/transformers/modeling_mobilebert.py
src/transformers/modeling_openai.py
src/transformers/modeling_reformer.py
src/transformers/modeling_roberta.py
src/transformers/modeling_t5.py
src/transformers/modeling_tf_albert.py
src/transformers/modeling_tf_bert.py
src/transformers/modeling_tf_ctrl.py
src/transformers/modeling_tf_distilbert.py
src/transformers/modeling_tf_electra.py
src/transformers/modeling_tf_gpt2.py
src/transformers/modeling_tf_mobilebert.py
src/transformers/modeling_tf_openai.py
src/transformers/modeling_tf_roberta.py
src/transformers/modeling_tf_t5.py
src/transformers/modeling_tf_transfo_xl.py
src/transformers/modeling_tf_xlm.py
src/transformers/modeling_tf_xlnet.py
src/transformers/modeling_transfo_xl.py
src/transformers/modeling_utils.py
src/transformers/modeling_xlm.py
src/transformers/modeling_xlnet.py
src/transformers/tokenization_bart.py
src/transformers/tokenization_marian.py
src/transformers/tokenization_reformer.py
src/transformers/tokenization_t5.py
tests/test_doc_samples.py
tests/test_modeling_tf_xlm.py
==================
315f464b0;Thomas Wolf;2020-06-25 22:17:14 +0200;[tokenizers] Several small improvements and bug fixes (#5287)
* avoid recursion in id checks for fast tokenizers

* better typings and fix #5232

* align slow and fast tokenizers behaviors for Roberta and GPT2

* style and quality

* fix tests - improve typings
==

src/transformers/tokenization_gpt2.py
src/transformers/tokenization_roberta.py
src/transformers/tokenization_utils_base.py
src/transformers/tokenization_utils_fast.py
tests/test_tokenization_fast.py
tests/test_tokenization_roberta.py
==================
24f46ea3f;Sylvain Gugger;2020-06-25 11:45:05 -0400;Remove links for all docs (#5280)

==

README.md
==================
27cf1d97f;Thomas Wolf;2020-06-25 17:24:28 +0200;[Tokenization] Fix #5181 - make #5155 more explicit - move back the default logging level in tests to WARNING (#5252)
* fix-5181

Padding to max sequence length while truncation to another length was wrong on slow tokenizers

* clean up and fix #5155

* fix XLM test

* Fix tests for Transfo-XL

* logging only above WARNING in tests

* switch slow tokenizers tests in @slow

* fix Marian truncation tokenization test

* style and quality

* make the test a lot faster by limiting the sequence length used in tests
==

src/transformers/tokenization_utils.py
src/transformers/tokenization_utils_base.py
tests/test_modeling_auto.py
tests/test_modeling_common.py
tests/test_modeling_tf_auto.py
tests/test_tokenization_auto.py
tests/test_tokenization_common.py
tests/test_tokenization_fast.py
tests/test_trainer_distributed.py
==================
e008d520b;Sam Shleifer;2020-06-25 10:13:01 -0400;[examples/seq2seq] more README improvements (#5274)

==

examples/seq2seq/README.md
==================
6a495cae0;Julien Chaumond;2020-06-25 15:58:25 +0200;[model_cards] Example of how to specify inputs for the widget

==

model_cards/distilbert-base-uncased-distilled-squad-README.md
model_cards/julien-c/EsperBERTo-small-pos/README.md
model_cards/julien-c/EsperBERTo-small/README.md
==================
0e1fce3c0;Anthony MOI;2020-06-25 02:17:02 -0400;Fix convert_graph_to_onnx (#5230)

==

src/transformers/convert_graph_to_onnx.py
==================
5543efd5c;Moumeneb1;2020-06-25 07:56:07 +0200;Create README.md (#5259)

==

model_cards/moumeneb1/flaubert-base-cased-ecology_crisis/README.md
==================
40457bceb;Sam Shleifer;2020-06-24 23:58:11 -0400;examples/seq2seq supports translation (#5202)

==

examples/seq2seq/README.md
examples/seq2seq/__init__.py
examples/seq2seq/bertabs/README.md
examples/seq2seq/bertabs/__init__.py
examples/seq2seq/bertabs/configuration_bertabs.py
examples/seq2seq/bertabs/convert_bertabs_original_pytorch_checkpoint.py
examples/seq2seq/bertabs/modeling_bertabs.py
examples/seq2seq/bertabs/requirements.txt
examples/seq2seq/bertabs/run_summarization.py
examples/seq2seq/bertabs/test_utils_summarization.py
examples/seq2seq/bertabs/utils_summarization.py
examples/seq2seq/callbacks.py
examples/seq2seq/distillation.py
examples/seq2seq/finetune.py
examples/seq2seq/finetune.sh
examples/seq2seq/finetune_bart_tiny.sh
examples/seq2seq/finetune_t5.sh
examples/seq2seq/initialization_utils.py
examples/seq2seq/run_distiller.sh
examples/seq2seq/run_eval.py
examples/seq2seq/test_seq2seq_examples.py
examples/seq2seq/train_distilbart_cnn.sh
examples/seq2seq/train_distilbart_xsum.sh
examples/seq2seq/utils.py
examples/summarization/README.md
examples/summarization/test_summarization_examples.py
examples/translation/t5/README.md
examples/translation/t5/__init__.py
examples/translation/t5/evaluate_wmt.py
examples/translation/t5/test_t5_examples.py
setup.cfg
src/transformers/tokenization_bart.py
==================
d12ceb48b;Sylvain Gugger;2020-06-24 18:43:20 -0400;Tokenization tutorial (#5257)
* All done

* Link to the tutorial

* Typo fixes

Co-authored-by: Thomas Wolf <thomwolf@users.noreply.github.com>

* Add metnion of the return_xxx args

Co-authored-by: Thomas Wolf <thomwolf@users.noreply.github.com>
==

docs/source/index.rst
docs/source/preprocessing.rst
docs/source/quicktour.rst
==================
7ac911071;Thomas Wolf;2020-06-24 21:53:08 +0200;Add more tests on tokenizers serialization - fix bugs  (#5056)
* update tests for fast tokenizers + fix small bug in saving/loading

* better tests on serialization

* fixing serialization

* comment cleanup
==

src/transformers/tokenization_utils.py
src/transformers/tokenization_utils_base.py
src/transformers/tokenization_utils_fast.py
tests/test_tokenization_common.py
==================
0148c262e;Sylvain Gugger;2020-06-24 15:16:04 -0400;Fix first test (#5255)

==

.circleci/deploy.sh
==================
70c1e1d2d;Sylvain Gugger;2020-06-24 15:06:14 -0400;Use master _static (#5253)
* Use _static from master everywhere

* Copy to existing too
==

.circleci/deploy.sh
==================
4965aee06;Victor SANH;2020-06-24 14:38:15 -0400;[HANS] Fix label_list for RoBERTa/BART (class flipping) (#5196)
* fix weirdness in roberta/bart for mnli trained checkpoints

* black compliance

* isort code check
==

examples/adversarial/run_hans.py
examples/adversarial/utils_hans.py
==================
fc24a93e6;Julien Chaumond;2020-06-24 16:54:00 +0000;[HfApi] Add support for pipeline_tag

==

src/transformers/hf_api.py
==================
0a3d0e02c;Setu Shah;2020-06-24 09:14:50 -0700;Replace labels with -100 to skip loss calc (#4718)

==

src/transformers/data/data_collator.py
==================
6894b486d;Sylvain Gugger;2020-06-24 12:13:43 -0400;Fix version controller links (for realsies) (#5251)

==

docs/source/_static/js/custom.js
==================
1121ce9f9;Sai Saketh Aluru;2020-06-24 21:11:08 +0530;Model cards for Hate-speech-CNERG models (#5236)
* Add dehatebert-mono-arabic readme card

* Update dehatebert-mono-arabic model card

* model cards for Hate-speech-CNERG models
==

model_cards/Hate-speech-CNERG/dehatebert-mono-arabic/README.md
model_cards/Hate-speech-CNERG/dehatebert-mono-english/README.md
model_cards/Hate-speech-CNERG/dehatebert-mono-french/README.md
model_cards/Hate-speech-CNERG/dehatebert-mono-german/README.md
model_cards/Hate-speech-CNERG/dehatebert-mono-indonesian/README.md
model_cards/Hate-speech-CNERG/dehatebert-mono-italian/README.md
model_cards/Hate-speech-CNERG/dehatebert-mono-polish/README.md
model_cards/Hate-speech-CNERG/dehatebert-mono-portugese/README.md
model_cards/Hate-speech-CNERG/dehatebert-mono-spanish/README.md
==================
cf10d4cfd;Lysandre Debut;2020-06-24 11:37:20 -0400;Cleaning TensorFlow models (#5229)
* Cleaning TensorFlow models

Update all classes


stylr

* Don't average loss
==

src/transformers/modeling_tf_albert.py
src/transformers/modeling_tf_bert.py
src/transformers/modeling_tf_distilbert.py
src/transformers/modeling_tf_electra.py
src/transformers/modeling_tf_mobilebert.py
src/transformers/modeling_tf_roberta.py
src/transformers/modeling_tf_xlm.py
src/transformers/modeling_tf_xlnet.py
tests/test_modeling_tf_common.py
tests/test_modeling_tf_distilbert.py
tests/test_modeling_tf_electra.py
tests/test_modeling_tf_roberta.py
tests/test_modeling_tf_xlnet.py
==================
609e0c583;Sylvain Gugger;2020-06-24 11:35:55 -0400;Fix links (#5248)

==

docs/source/_static/js/custom.js
==================
c9163a8d5;Ali Modarressi;2020-06-24 19:48:29 +0430;delay decay schedule until the end of warmup (#4940)

==

src/transformers/optimization_tf.py
==================
f216b6067;Sylvain Gugger;2020-06-24 10:59:06 -0400;Fix deploy doc (#5246)
* Try with the same command

* Try like this
==

.circleci/deploy.sh
==================
49f6e7a3c;Sylvain Gugger;2020-06-24 10:37:01 -0400;Add some prints to debug (#5244)

==

.circleci/deploy.sh
==================
c2a26ec8a;Patrick von Platen;2020-06-24 16:09:17 +0200;[Use cache] Align logic of `use_cache` with output_attentions and output_hidden_states (#5194)
* fix use cache

* add bart use cache

* fix bart

* finish bart
==

src/transformers/modeling_bart.py
src/transformers/modeling_ctrl.py
src/transformers/modeling_gpt2.py
src/transformers/modeling_t5.py
src/transformers/modeling_tf_ctrl.py
src/transformers/modeling_tf_gpt2.py
src/transformers/modeling_tf_t5.py
tests/test_modeling_bart.py
tests/test_modeling_gpt2.py
tests/test_modeling_t5.py
tests/test_modeling_tf_common.py
tests/test_modeling_tf_gpt2.py
tests/test_modeling_tf_t5.py
==================
64c393ee7;Sylvain Gugger;2020-06-24 09:59:07 -0400;Don't recreate old docs (#5243)

==

.circleci/deploy.sh
==================
b29683736;Patrick von Platen;2020-06-24 15:58:49 +0200;fix print in benchmark (#5242)

==

src/transformers/benchmark/benchmark_utils.py
==================
9fe09cec7;Patrick von Platen;2020-06-24 15:11:42 +0200;[Benchmark] Extend Benchmark to all model type extensions (#5241)
* add benchmark for all kinds of models

* improved import

* delete bogus files

* make style
==

examples/benchmarking/plot_csv_file.py
src/transformers/benchmark/benchmark.py
src/transformers/benchmark/benchmark_args_utils.py
src/transformers/benchmark/benchmark_tf.py
src/transformers/benchmark/benchmark_utils.py
tests/test_benchmark.py
tests/test_benchmark_tf.py
==================
7c41057d5;Sylvain Gugger;2020-06-24 07:56:14 -0400;Add hugs (#5225)

==

docs/source/index.rst
docs/source/migration.md
docs/source/model_summary.rst
docs/source/philosophy.rst
docs/source/quicktour.rst
docs/source/task_summary.rst
docs/source/torchscript.rst
examples/README.md
notebooks/README.md
==================
5e85b324e;Sylvain Gugger;2020-06-24 07:55:58 -0400;Use the script in utils (#5224)

==

examples/text-classification/README.md
==================
5e31a98ab;flozi00;2020-06-24 10:45:51 +0200;Create README.md (#5108)
* Create README.md

* Update model_cards/a-ware/roberta-large-squad-classification/README.md

Co-authored-by: Julien Chaumond <chaumond@gmail.com>
==

model_cards/a-ware/roberta-large-squad-classification/README.md
==================
033124e5f;Adriano Diniz;2020-06-24 05:42:46 -0300;Update README.md (#5199)
Fix/add information in README.md
==

model_cards/aodiniz/bert_uncased_L-10_H-512_A-8_cord19-200616/README.md
==================
7ca6627ec;ahotrod;2020-06-24 01:40:50 -0700;Create README.md (#5217)
electra_large_discriminator_squad2_512 Question Answering LM
==

model_cards/ahotrod/electra_large_discriminator_squad2_512/README.md
==================
54e9ce785;Kevin Canwen Xu;2020-06-24 16:10:36 +0800;Fix PABEE division by zero error (#5233)
* Fix PABEE division by zero error

* patience=0 by default
==

examples/bert-loses-patience/run_glue_with_pabee.py
==================
9022ef021;Sylvain Gugger;2020-06-23 17:30:17 -0400;Only put tensors on a device (#5223)
* Only put tensors on a device

* Type hint and unpack list comprehension
==

src/transformers/trainer.py
==================
173528e36;Sylvain Gugger;2020-06-23 17:05:12 -0400;Add version control menu (#5222)
* Add version control menu

* Constify things

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* Apply suggestions from code review

Co-authored-by: Julien Chaumond <chaumond@gmail.com>

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
Co-authored-by: Julien Chaumond <chaumond@gmail.com>
==

docs/source/_static/css/huggingface.css
docs/source/_static/js/custom.js
docs/source/conf.py
==================
76e5af4cf;Sam Shleifer;2020-06-23 16:40:45 -0400;[pl_examples] revert deletion of optimizer_step (#5227)

==

examples/lightning_base.py
examples/summarization/finetune.py
examples/summarization/run_distiller.sh
==================
c01480bba;Julien Chaumond;2020-06-23 18:31:13 +0200;[file_utils] Type user-agent

==

src/transformers/file_utils.py
==================
58918c76f;Sam Shleifer;2020-06-23 11:35:42 -0400;[bart] add config.extra_pos_embeddings to facilitate reuse (#5190)

==

src/transformers/configuration_bart.py
src/transformers/modeling_bart.py
src/transformers/modeling_roberta.py
src/transformers/modeling_utils.py
tests/test_modeling_roberta.py
==================
b28b53713;Thomas Wolf;2020-06-23 13:37:29 +0200;More clear error message in the use-case of #5169 (#5184)

==

src/transformers/tokenization_utils.py
==================
11fdde027;Thomas Wolf;2020-06-23 13:36:57 +0200;Tokenizers API developments (#5103)
* Add return lengths

* make pad a bit more flexible so it can be used as collate_fn

* check all kwargs sent to encoding method are known

* fixing kwargs in encodings

* New AddedToken class in python

This class let you specify specifique tokenization behaviors for some special tokens. Used in particular for GPT2 and Roberta, to control how white spaces are stripped around special tokens.

* style and quality

* switched to hugginface tokenizers library for AddedTokens

* up to tokenizer 0.8.0-rc3 - update API to use AddedToken state

* style and quality

* do not raise an error on additional or unused kwargs for tokenize() but only a warning

* transfo-xl pretrained model requires torch

* Update src/transformers/tokenization_utils.py

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
==

setup.py
src/transformers/tokenization_bert.py
src/transformers/tokenization_gpt2.py
src/transformers/tokenization_roberta.py
src/transformers/tokenization_transfo_xl.py
src/transformers/tokenization_utils.py
src/transformers/tokenization_utils_base.py
src/transformers/tokenization_utils_fast.py
tests/test_tokenization_common.py
tests/test_tokenization_fast.py
tests/test_tokenization_roberta.py
==================
1ae132a07;Patrick von Platen;2020-06-23 10:49:18 +0200;[Reformer] Axial Pos Emb Improve mem usage reformer (#5209)
* improve mem handling

* improve mem for pos ax encodings
==

src/transformers/modeling_reformer.py
==================
514410407;Sam Shleifer;2020-06-22 23:39:04 -0400;[fix] remove unused import (#5206)

==

tests/test_modeling_mobilebert.py
==================
0d158e38c;Sam Shleifer;2020-06-22 23:31:36 -0400;[fix] mobilebert had wrong path, causing slow test failure (#5205)

==

src/transformers/modeling_mobilebert.py
tests/test_modeling_mobilebert.py
==================
f5c2a122e;Sam Shleifer;2020-06-22 20:40:10 -0400;Upgrade examples to pl=0.8.1(#5146)

==

examples/lightning_base.py
examples/requirements.txt
examples/summarization/callbacks.py
examples/summarization/distillation.py
examples/summarization/finetune.py
examples/summarization/run_distiller.sh
examples/summarization/run_eval.py
examples/summarization/test_summarization_examples.py
examples/summarization/utils.py
examples/text-classification/run_pl_glue.py
src/transformers/tokenization_auto.py
==================
06b60c8b0;flozi00;2020-06-23 00:40:19 +0200;[Modelcard] bart-squadv2 (#5011)
* [Modelcard] bart-squadv2

* Update README.md

* Update README.md
==

model_cards/a-ware/bart-squadv2/README.md
==================
35e068725;flozi00;2020-06-23 00:40:00 +0200;Create README.md (#5013)

==

model_cards/a-ware/xlmroberta-squadv2/README.md
==================
22d2c8ea2;Fran Martinez;2020-06-23 00:39:29 +0200;Create README.md for finetuned BERT model (#5009)
* Create README.md

* changes in model usage section

* minor changes in output visualization

* minor errata in readme
==

model_cards/fran-martinez/scibert_scivocab_cased_ner_jnlpba/README.md
==================
258950569;furunkel;2020-06-23 00:39:22 +0200;Add model card for StackOBERTflow-comments-small (#5008)
* Create README.md

* Update README.md
==

model_cards/giganticode/StackOBERTflow-comments-small-v1/README.md
==================
d8c26ed13;bogdankostic;2020-06-23 00:26:12 +0200;Specify dataset used for crossvalidation (#5175)

==

model_cards/deepset/roberta-base-squad2-covid/README.md
==================
a34fb91d5;Adriano Diniz;2020-06-22 19:00:53 -0300;Create README.md (#5149)

==

model_cards/aodiniz/bert_uncased_L-10_H-512_A-8_cord19-200616/README.md
==================
ffabcf524;Adriano Diniz;2020-06-22 18:59:54 -0300;Create README.md (#5160)

==

model_cards/aodiniz/bert_uncased_L-4_H-256_A-4_cord19-200616/README.md
==================
3363a19b1;Adriano Diniz;2020-06-22 18:59:33 -0300;Create README.md (#5152)
* Create README.md

* Apply suggestions from code review

Co-authored-by: Julien Chaumond <chaumond@gmail.com>
==

model_cards/aodiniz/bert_uncased_L-10_H-512_A-8_cord19-200616_squad2/README.md
==================
0cca61925;Micha√´l Benesty;2020-06-22 23:47:33 +0200;Add link to new comunity notebook (optimization) (#5195)
* Add link to new comunity notebook (optimization)

related to https://github.com/huggingface/transformers/issues/4842#event-3469184635

This notebook is about benchmarking model training with/without dynamic padding optimization. 
https://github.com/ELS-RD/transformers-notebook 

Using dynamic padding on MNLI provides a **4.7 times training time reduction**, with max pad length set to 512. The effect is strong because few examples are >> 400 tokens in this dataset. IRL, it will depend of the dataset, but it always bring improvement and, after more than 20 experiments listed in this [article](https://towardsdatascience.com/divide-hugging-face-transformers-training-time-by-2-or-more-21bf7129db9q-21bf7129db9e?source=friends_link&sk=10a45a0ace94b3255643d81b6475f409), it seems to not hurt performance.

Following advice from @patrickvonplaten I do the PR myself :-)

* Update notebooks/README.md

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
==

notebooks/README.md
==================
1c5cd8e5f;Lee Haau-Sing;2020-06-22 17:24:27 -0400;Add README.md (nyu-mll) (#5174)
* nyu-mll: roberta on smaller datasets

* Update README.md

* Update README.md

Co-authored-by: Alex Warstadt <alexwarstadt@gmail.com>
==

model_cards/nyu-mll/roberta-base-100M-1/README.md
model_cards/nyu-mll/roberta-base-100M-2/README.md
model_cards/nyu-mll/roberta-base-100M-3/README.md
model_cards/nyu-mll/roberta-base-10M-1/README.md
model_cards/nyu-mll/roberta-base-10M-2/README.md
model_cards/nyu-mll/roberta-base-10M-3/README.md
model_cards/nyu-mll/roberta-base-1B-1/README.md
model_cards/nyu-mll/roberta-base-1B-2/README.md
model_cards/nyu-mll/roberta-base-1B-3/README.md
model_cards/nyu-mll/roberta-med-small-1M-1/README.md
model_cards/nyu-mll/roberta-med-small-1M-2/README.md
model_cards/nyu-mll/roberta-med-small-1M-3/README.md
model_cards/nyu-mll/roberta_1M_to_1B/README.md
==================
c43975248;Sylvain Gugger;2020-06-22 16:38:53 -0400;Switch master/stable doc and add older releases (#5193)

==

.circleci/deploy.sh
README.md
==================
417e492f1;Sylvain Gugger;2020-06-22 16:08:09 -0400;Quick tour (#5145)
* Quicktour part 1

* Update

* All done

* Typos

Co-authored-by: Thomas Wolf <thomwolf@users.noreply.github.com>

* Address comments in quick tour

* Update docs/source/quicktour.rst

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* Update from feedback

Co-authored-by: Thomas Wolf <thomwolf@users.noreply.github.com>
Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
==

docs/source/index.rst
docs/source/main_classes/pipelines.rst
docs/source/model_summary.rst
docs/source/philosophy.rst
docs/source/quickstart.md
docs/source/quicktour.rst
docs/source/task_summary.rst
==================
75e1eed8d;Thomas Wolf;2020-06-22 21:58:47 +0200;Cleaner warning when loading pretrained models (#4557)
* Cleaner warning when loading pretrained models

This make more explicit logging messages when using the various `from_pretrained` methods. It also make these messages as `logging.warning` because it's a common source of silent mistakes.

* Update src/transformers/modeling_utils.py

Co-authored-by: Julien Chaumond <chaumond@gmail.com>

* Update src/transformers/modeling_utils.py

Co-authored-by: Julien Chaumond <chaumond@gmail.com>

* style and quality

Co-authored-by: Julien Chaumond <chaumond@gmail.com>
==

src/transformers/modeling_tf_pytorch_utils.py
src/transformers/modeling_tf_utils.py
src/transformers/modeling_utils.py
==================
4e741efa9;Lysandre Debut;2020-06-22 15:49:50 -0400;Have documentation fail on warning (#5189)
* Have documentation fail on warning

* Force ci failure

* Revert "Force ci failure"

This reverts commit f0a4666ec2eb4cd00a4da48af3357defc63324a0.
==

.circleci/config.yml
==================
1262495a9;Sylvain Gugger;2020-06-22 14:43:52 -0400;Add TF auto model to the docs + fix sphinx warnings (#5187)

==

docs/source/model_doc/auto.rst
docs/source/model_doc/mobilebert.rst
src/transformers/modeling_albert.py
src/transformers/modeling_bart.py
src/transformers/modeling_bert.py
src/transformers/modeling_ctrl.py
src/transformers/modeling_distilbert.py
src/transformers/modeling_electra.py
src/transformers/modeling_flaubert.py
src/transformers/modeling_gpt2.py
src/transformers/modeling_longformer.py
src/transformers/modeling_mobilebert.py
src/transformers/modeling_openai.py
src/transformers/modeling_reformer.py
src/transformers/modeling_roberta.py
src/transformers/modeling_t5.py
src/transformers/modeling_tf_albert.py
src/transformers/modeling_tf_auto.py
src/transformers/modeling_tf_bert.py
src/transformers/modeling_tf_ctrl.py
src/transformers/modeling_tf_distilbert.py
src/transformers/modeling_tf_electra.py
src/transformers/modeling_tf_gpt2.py
src/transformers/modeling_tf_mobilebert.py
src/transformers/modeling_tf_openai.py
src/transformers/modeling_tf_roberta.py
src/transformers/modeling_tf_t5.py
src/transformers/modeling_tf_transfo_xl.py
src/transformers/modeling_tf_xlm.py
src/transformers/modeling_tf_xlnet.py
src/transformers/modeling_transfo_xl.py
src/transformers/modeling_xlm.py
src/transformers/modeling_xlnet.py
==================
88429c57b;Adriano Diniz;2020-06-22 14:49:14 -0300;Create README.md (#5165)

==

model_cards/aodiniz/bert_uncased_L-2_H-512_A-8_cord19-200616/README.md
==================
76ee9c8bc;Manuel Romero;2020-06-22 19:47:30 +0200;Create README.md (#5107)
* Create README.md

@julien-c check out that dataset meta tag is right

* Fix typo

Co-authored-by: Julien Chaumond <chaumond@gmail.com>
==

model_cards/mrm8488/t5-base-finetuned-imdb-sentiment/README.md
==================
bf493d556;Manuel Romero;2020-06-22 19:45:45 +0200;Model card for t5-base-finetuned-emotion (recognition) (#5179)

==

model_cards/mrm8488/t5-base-finetuned-emotion/README.md
==================
e9ef21175;Patrick von Platen;2020-06-22 19:00:11 +0200;improve doc (#5185)

==

src/transformers/configuration_t5.py
==================
ebc36108d;Thomas Wolf;2020-06-22 17:25:43 +0200;[tokenizers] Fix #5081 and improve backward compatibility (#5125)
* fix #5081 and improve backward compatibility (slightly)

* add nlp to setup.cfg - style and quality

* align default to previous default

* remove test that doesn't generalize
==

src/transformers/tokenization_utils.py
src/transformers/tokenization_utils_base.py
tests/test_tokenization_common.py
==================
d2a7c86dc;Malte;2020-06-22 17:09:05 +0200;Check if `text` is set to avoid IndexError (#4209)
Fix for https://github.com/huggingface/transformers/issues/3809
==

src/transformers/tokenization_roberta.py
==================
90f4b2452;Iz Beltagy;2020-06-22 07:47:14 -0700;Add support for gradient checkpointing in BERT (#4659)
* add support for gradient checkpointing in BERT

* fix unit tests

* isort

* black

* workaround for `torch.utils.checkpoint.checkpoint` not accepting bool

* Revert "workaround for `torch.utils.checkpoint.checkpoint` not accepting bool"

This reverts commit 5eb68bb804f5ffbfc7ba13c45a47717f72d04574.

* workaround for `torch.utils.checkpoint.checkpoint` not accepting bool

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
==

src/transformers/configuration_bert.py
src/transformers/modeling_bert.py
==================
f4e1f0221;Joseph Liu;2020-06-22 22:10:45 +0800;Output hidden states (#4978)
* Configure all models to use output_hidden_states as argument passed to foward()

* Pass all tests

* Remove cast_bool_to_primitive in TF Flaubert model

* correct tf xlnet

* add pytorch test

* add tf test

* Fix broken tests

* Configure all models to use output_hidden_states as argument passed to foward()

* Pass all tests

* Remove cast_bool_to_primitive in TF Flaubert model

* correct tf xlnet

* add pytorch test

* add tf test

* Fix broken tests

* Refactor output_hidden_states for mobilebert

* Reset and remerge to master

Co-authored-by: Joseph Liu <joseph.liu@coinflex.com>
Co-authored-by: patrickvonplaten <patrick.v.platen@gmail.com>
==

src/transformers/modeling_albert.py
src/transformers/modeling_bart.py
src/transformers/modeling_bert.py
src/transformers/modeling_ctrl.py
src/transformers/modeling_distilbert.py
src/transformers/modeling_electra.py
src/transformers/modeling_flaubert.py
src/transformers/modeling_gpt2.py
src/transformers/modeling_longformer.py
src/transformers/modeling_mmbt.py
src/transformers/modeling_mobilebert.py
src/transformers/modeling_openai.py
src/transformers/modeling_reformer.py
src/transformers/modeling_roberta.py
src/transformers/modeling_t5.py
src/transformers/modeling_tf_albert.py
src/transformers/modeling_tf_bert.py
src/transformers/modeling_tf_ctrl.py
src/transformers/modeling_tf_distilbert.py
src/transformers/modeling_tf_electra.py
src/transformers/modeling_tf_flaubert.py
src/transformers/modeling_tf_gpt2.py
src/transformers/modeling_tf_mobilebert.py
src/transformers/modeling_tf_openai.py
src/transformers/modeling_tf_roberta.py
src/transformers/modeling_tf_t5.py
src/transformers/modeling_tf_transfo_xl.py
src/transformers/modeling_tf_xlm.py
src/transformers/modeling_tf_xlnet.py
src/transformers/modeling_transfo_xl.py
src/transformers/modeling_xlm.py
src/transformers/modeling_xlnet.py
tests/test_modeling_common.py
tests/test_modeling_tf_common.py
==================
866a8ccab;Kevin Canwen Xu;2020-06-22 21:48:14 +0800;Add model cards for Microsoft's MiniLM (#5178)
* Add model cards for Microsoft's MiniLM

* XLMRobertaTokenizer

* format

* Add thumbnail

* finishing up
==

model_cards/microsoft/MiniLM-L12-H384-uncased/README.md
model_cards/microsoft/Multilingual-MiniLM-L12-H384/README.md
==================
b99ad457f;RafaelWO;2020-06-22 15:40:52 +0200;Added feature to move added tokens in vocabulary for Transformer-XL (#4953)
* Fixed resize_token_embeddings for transfo_xl model

* Fixed resize_token_embeddings for transfo_xl.

Added custom methods to TransfoXLPreTrainedModel for resizing layers of
the AdaptiveEmbedding.

* Updated docstring

* Fixed resizinhg cutoffs; added check for new size of embedding layer.

* Added test for resize_token_embeddings

* Fixed code quality

* Fixed unchanged cutoffs in model.config

* Added feature to move added tokens in tokenizer.

* Fixed code quality

* Added feature to move added tokens in tokenizer.

* Fixed code quality

* Fixed docstring, renamed sym to 	oken.

Co-authored-by: Rafael Weingartner <rweingartner.its-b2015@fh-salzburg.ac.at>
==

src/transformers/tokenization_transfo_xl.py
tests/test_tokenization_transfo_xl.py
==================
eb0ca71ef;Sylvain Gugger;2020-06-22 08:30:49 -0400;Update glossary (#5148)
* Update glossary

* Update docs/source/glossary.rst

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
==

docs/source/glossary.rst
src/transformers/configuration_electra.py
==================
fa0be6d76;Patrick von Platen;2020-06-22 12:06:56 +0200;Benchmarks (#4912)
* finish benchmark

* fix isort

* fix setup cfg

* retab

* fix time measuring of tf graph mode

* fix tf cuda

* clean code

* better error message
==

examples/benchmarking/run_benchmark.py
examples/benchmarking/run_benchmark_tf.py
examples/longform-qa/eli5_app.py
examples/longform-qa/eli5_utils.py
examples/requirements.txt
setup.cfg
src/transformers/__init__.py
src/transformers/benchmark/__init__.py
src/transformers/benchmark/benchmark.py
src/transformers/benchmark/benchmark_args.py
src/transformers/benchmark/benchmark_args_tf.py
src/transformers/benchmark/benchmark_args_utils.py
src/transformers/benchmark/benchmark_tf.py
src/transformers/benchmark/benchmark_utils.py
src/transformers/file_utils.py
src/transformers/trainer.py
tests/test_benchmark.py
tests/test_benchmark_tf.py
==================
18a0150bf;Zihao Fu;2020-06-22 16:58:28 +0800;fix bart doc (#5132)
fix bart doc
==

src/transformers/modeling_bart.py
==================
3fe75c7f7;Mikael Souza;2020-06-22 04:51:17 -0400;Fixing docs for Encoder Decoder Config (#5171)

==

src/transformers/configuration_encoder_decoder.py
==================
59345cc87;flozi00;2020-06-22 10:49:23 +0200;Typo (#5147)

==

model_cards/google/reformer-crime-and-punishment/README.md
==================
bc3a0c060;Ilya Boytsov;2020-06-21 18:51:21 +0300;[examples] fixes arguments for summarization finetune scripts (#5157)
Authored-by: i.boytsov <i.boytsov@MAC867.local>
==

examples/summarization/finetune.sh
examples/summarization/finetune_bart_tiny.sh
examples/summarization/finetune_t5.sh
==================
68e19f1c2;Tim Suchanek;2020-06-20 17:00:04 +0200;Fix typo in root README (#5073)

==

README.md
==================
c0c577cf8;Kevin Canwen Xu;2020-06-20 22:56:39 +0800;Fix PABEE's result table (#5158)

==

examples/bert-loses-patience/README.md
==================
aa6a29bc2;Julien Chaumond;2020-06-20 09:16:30 +0200;SummarizationPipeline: init required task name (#5086)
* SummarizationPipeline: init required task name

* Update src/transformers/pipelines.py

Co-authored-by: Sam Shleifer <sshleifer@gmail.com>

* Apply suggestions from code review

Co-authored-by: Sam Shleifer <sshleifer@gmail.com>
==

src/transformers/pipelines.py
==================
2fd28d436;Kevin Canwen Xu;2020-06-20 13:41:46 +0800;Add BERT Loses Patience (Patience-based Early Exit) (#5078)
* Add BERT Loses Patience (Patience-based Early Exit)

* update model archive

* update format

* sort import

* flake8

* Add results

* full results

* align the table

* refactor to inherit

* default per gpu eval = 1

* Formatting

* Formatting

* isort

* modify readme

* Add check

* Fix format

* Fix format

* Doc strings

* ALBERT & BERT for sequence classification don't inherit from the original anymore

* Remove incorrect comments

* Remove incorrect comments

* Remove incorrect comments

* Sync up with new code

* Sync up with new code

* Add a test

* Add a test

* Add a test

* Add a test

* Add a test

* Add a test

* Finishing up!
==

examples/bert-loses-patience/README.md
examples/bert-loses-patience/pabee/__init__.py
examples/bert-loses-patience/pabee/modeling_pabee_albert.py
examples/bert-loses-patience/pabee/modeling_pabee_bert.py
examples/bert-loses-patience/run_glue_with_pabee.py
examples/bert-loses-patience/test_run_glue_with_pabee.py
==================
f1679d7c4;Zhu Baohe;2020-06-20 13:21:19 +0800;Fix dropout in TFMobileBert (#5150)

==

src/transformers/modeling_tf_mobilebert.py
==================
5ed94b231;Kevin Canwen Xu;2020-06-20 10:13:34 +0800;Update note to avoid confusion (#5131)

==

model_cards/canwenxu/BERT-of-Theseus-MNLI/README.md
==================
d97b4176e;Lysandre;2020-06-19 21:58:17 -0400;Correct device assignment

==

src/transformers/modeling_mobilebert.py
==================
9a3f91088;Vasily Shamporov;2020-06-19 23:38:36 +0300;Add MobileBert (#4901)
* Add MobileBert

* Quality + Conversion script

* style

* Update src/transformers/modeling_mobilebert.py

* Links to S3

* Style

* TFMobileBert

Slight fixes to the pytorch MobileBert
Style

* MobileBertForMaskedLM (PT + TF)

* MobileBertForNextSentencePrediction (PT + TF)

* MobileFor{MultipleChoice, TokenClassification} (PT + TF)


ss

* Tests + Auto

* Doc

* Tests

* Addressing @sgugger's comments

* Adressing @patrickvonplaten's comments

* Style

* Style

* Integration test

* style

* Model card

Co-authored-by: Lysandre <lysandre.debut@reseau.eseo.fr>
Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
==

docs/source/index.rst
docs/source/model_doc/mobilebert.rst
model_cards/google/mobilebert-uncased/README.md
src/transformers/__init__.py
src/transformers/configuration_auto.py
src/transformers/configuration_mobilebert.py
src/transformers/convert_mobilebert_original_tf_checkpoint_to_pytorch.py
src/transformers/modeling_auto.py
src/transformers/modeling_mobilebert.py
src/transformers/modeling_tf_auto.py
src/transformers/modeling_tf_mobilebert.py
src/transformers/tokenization_auto.py
src/transformers/tokenization_mobilebert.py
tests/test_modeling_mobilebert.py
tests/test_modeling_tf_mobilebert.py
==================
f45e87391;Sam Shleifer;2020-06-19 13:33:24 -0400;[bart-mnli] Fix class flipping bug (#5141)

==

src/transformers/data/datasets/glue.py
==================
e33929ef1;Erick Rocha Fonseca;2020-06-19 14:41:31 +0100;Fix in Reformer Config documentation (#5138)

==

src/transformers/configuration_reformer.py
==================
84be482f6;Sam Shleifer;2020-06-18 20:47:37 -0400;AutoTokenizer supports mbart-large-en-ro (#5121)

==

src/transformers/configuration_auto.py
src/transformers/configuration_bart.py
src/transformers/tokenization_auto.py
tests/test_modeling_bart.py
==================
2db1e2f41;Sam Shleifer;2020-06-18 20:34:48 -0400;[cleanup] remove redundant code in SummarizationDataset (#5119)

==

examples/summarization/utils.py
==================
5f721ad6e;Sylvain Gugger;2020-06-18 19:20:04 -0400;Fix #5114 (#5122)

==

src/transformers/data/data_collator.py
tests/test_trainer.py
==================
a258982af;Pri Oberoi;2020-06-18 19:04:04 -0400;Add missing arg in 02-transformers notebook (#5085)
* Add missing arg when creating model

* Fix typos

* Remove from_tf flag when creating model
==

notebooks/02-transformers.ipynb
==================
32e94cff6;Deniz;2020-06-18 15:41:26 -0700;tf add resize_token_embeddings method (#4351)
* resize token embeddings

* add tokens

* add tokens

* add tokens

* add t5 token method

* add t5 token method

* add t5 token method

* typo

* debugging input

* debugging input

* debug

* debug

* debug

* trying to set embedding tokens properly

* set embeddings for generation head too

* set embeddings for generation head too

* debugging

* debugging

* enable generation

* add base method

* add base method

* add base method

* return logits in the main call

* reverting to generation

* revert back

* set embeddings for the bert main layer

* description

* fix conflicts

* logging

* set base model as self

* refactor

* tf_bert add method

* tf_bert add method

* tf_bert add method

* tf_bert add method

* tf_bert add method

* tf_bert add method

* tf_bert add method

* tf_bert add method

* v0

* v0

* finalize

* final

* black

* add tests

* revert back the emb call

* comments

* comments

* add the second test

* add vocab size condig

* add tf models

* add tf models. add common tests

* remove model specific embedding tests

* stylish

* remove files

* stylez

* Update src/transformers/modeling_tf_transfo_xl.py

change the error.

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* adding unchanged weight test

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
==

src/transformers/modeling_tf_albert.py
src/transformers/modeling_tf_bert.py
src/transformers/modeling_tf_ctrl.py
src/transformers/modeling_tf_distilbert.py
src/transformers/modeling_tf_electra.py
src/transformers/modeling_tf_gpt2.py
src/transformers/modeling_tf_openai.py
src/transformers/modeling_tf_roberta.py
src/transformers/modeling_tf_t5.py
src/transformers/modeling_tf_transfo_xl.py
src/transformers/modeling_tf_utils.py
src/transformers/modeling_tf_xlm.py
src/transformers/modeling_tf_xlnet.py
tests/test_modeling_tf_common.py
tests/test_modeling_tf_distilbert.py
==================
973433260;Lysandre Debut;2020-06-18 18:07:59 -0400;Pin `sphinx-rtd-theme` (#5128)

==

setup.py
==================
8a377c3d6;Sam Shleifer;2020-06-18 18:06:27 -0400;[fix] Move _adjust_logits above postprocess to fix Marian.generate (#5126)

==

src/transformers/modeling_bart.py
src/transformers/modeling_marian.py
src/transformers/modeling_utils.py
==================
3d3e605af;Sam Shleifer;2020-06-18 16:30:24 -0400;[cleanup] generate_beam_search comments (#5115)

==

src/transformers/modeling_tf_utils.py
src/transformers/modeling_utils.py
==================
ca2d0f98c;Suraj Patil;2020-06-19 00:29:35 +0530;ElectraForMultipleChoice (#4954)
* add ElectraForMultipleChoice

* add  test_for_multiple_choice

* add ElectraForMultipleChoice in auto model

* add ElectraForMultipleChoice in all_model_classes

* add SequenceSummary related parameters

* get rid pooler, use SequenceSummary instead

* add electra multiple choice test

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
==

src/transformers/__init__.py
src/transformers/configuration_electra.py
src/transformers/modeling_auto.py
src/transformers/modeling_electra.py
tests/test_modeling_electra.py
==================
279d8e24f;Ori Garin;2020-06-18 20:47:05 +0300;support local_files_only option for tf models (#5116)

==

src/transformers/modeling_tf_utils.py
==================
355954ffc;Julien Chaumond;2020-06-18 05:17:45 -0400;Create distilbert-base-uncased-distilled-squad-README.md

==

model_cards/distilbert-base-uncased-distilled-squad-README.md
==================
18177a1a6;Suraj Patil;2020-06-18 12:46:29 +0530;lm_labels => labels (#5080)

==

docs/source/model_doc/t5.rst
==================
efeb75b80;Lysandre;2020-06-17 18:24:35 -0400;Remove misleading comment
closes #4958
==

examples/question-answering/run_squad.py
==================
bb154ac50;Saurabh Misra;2020-06-17 15:04:11 -0700;Fixing TPU training by disabling wandb.watch gradients logging for TPU (#4926)

==

src/transformers/trainer.py
==================
fb6cccb86;Suraj Patil;2020-06-18 03:24:16 +0530;fix qa example (#4929)

==

src/transformers/modeling_electra.py
==================
38bba9cdd;Karthikeyan Singaravelan;2020-06-18 03:16:58 +0530;Fix deprecation warnings due to invalid escape sequences. (#4924)

==

src/transformers/tokenization_transfo_xl.py
==================
f1a3d0374;Sam Shleifer;2020-06-17 16:39:17 -0400;add pandas to setup.cfg (#5093)

==

examples/longform-qa/eli5_utils.py
setup.cfg
==================
90c833870;Sam Shleifer;2020-06-17 16:31:05 -0400;[MarianTokenizer] Switch to sacremoses for punc normalization (#5092)

==

src/transformers/tokenization_marian.py
==================
049e14f0e;Pranav Dayanand Pawar;2020-06-18 01:38:43 +0530;very minor spelling correction in script command (#5090)
actual script name - counts_parameters.py
==

examples/movement-pruning/README.md
==================
20fa82898;Sylvain Gugger;2020-06-17 15:24:51 -0400;Make default_data_collator more flexible and deprecate old behavior (#5060)
* Make default_data_collator more flexible

* Accept tensors for all features

* Document code

* Refactor

* Formatting
==

src/transformers/data/data_collator.py
src/transformers/trainer.py
tests/test_trainer.py
==================
5e0696339;Yacine Jernite;2020-06-17 14:48:06 -0400;Some changes to simplify the generation function (#5031)
* moving logits post-processing out of beam search

* moving logits post-processing out of beam search

* first step cache

* fix_Encoder_Decoder

* patrick_version_postprocess

* add_keyword_arg
==

src/transformers/modeling_bart.py
src/transformers/modeling_encoder_decoder.py
src/transformers/modeling_t5.py
src/transformers/modeling_utils.py
==================
204ebc25e;Sylvain Gugger;2020-06-17 14:01:10 -0400;Update installation page and add contributing to the doc (#5084)
* Update installation page and add contributing to the doc

* Remove mention of symlinks
==

CONTRIBUTING.md
docs/README.md
docs/source/contributing.md
docs/source/index.rst
docs/source/installation.md
docs/source/model_doc/gpt.rst
==================
043f9f51f;Sam Shleifer;2020-06-17 13:51:34 -0400;[examples] SummarizationModule improvements (#4951)

==

examples/lightning_base.py
examples/requirements.txt
examples/summarization/README.md
examples/summarization/callbacks.py
examples/summarization/distillation.py
examples/summarization/evaluate_cnn.py
examples/summarization/finetune.py
examples/summarization/finetune.sh
examples/summarization/finetune_bart.sh
examples/summarization/initialization_utils.py
examples/summarization/run_distiller.sh
examples/summarization/run_eval.py
examples/summarization/test_summarization_examples.py
examples/summarization/utils.py
src/transformers/modeling_bart.py
==================
cd40f6564;Sylvain Gugger;2020-06-17 11:45:05 -0400;Add header and fix command (#5082)

==

examples/adversarial/README.md
examples/adversarial/run_hans.py
==================
70bc3ead4;Julien Chaumond;2020-06-17 15:09:22 +0000;[TextClassificationPipeline] Hotfix: make json serializable

==

src/transformers/pipelines.py
==================
7291ea0bf;Sylvain Gugger;2020-06-17 07:55:20 -0400;Reorganize documentation (#5064)
* Reorganize topics and add all models

==

docs/source/index.rst
==================
e4aaa4580;Sylvain Gugger;2020-06-16 18:14:58 -0400;Update pipeline examples to doctest syntax (#5030)

==

README.md
==================
011cc0be5;Sylvain Gugger;2020-06-16 16:50:02 -0400;Fix all sphynx warnings (#5068)

==

docs/source/main_classes/optimizer_schedules.rst
docs/source/main_classes/pipelines.rst
docs/source/model_doc/auto.rst
docs/source/model_doc/encoderdecoder.rst
docs/source/model_doc/reformer.rst
docs/source/pretrained_models.rst
docs/source/usage.rst
src/transformers/configuration_auto.py
src/transformers/configuration_t5.py
src/transformers/configuration_xlnet.py
src/transformers/data/processors/utils.py
src/transformers/modeling_auto.py
src/transformers/modeling_electra.py
src/transformers/modeling_longformer.py
src/transformers/modeling_reformer.py
src/transformers/modeling_t5.py
src/transformers/modeling_tf_albert.py
src/transformers/modeling_tf_electra.py
src/transformers/modeling_tf_t5.py
src/transformers/modeling_tf_utils.py
src/transformers/modeling_utils.py
src/transformers/optimization_tf.py
src/transformers/pipelines.py
src/transformers/tokenization_auto.py
src/transformers/tokenization_utils_base.py
==================
af497b567;flozi00;2020-06-16 22:46:20 +0200;Typo (#5069)

==

examples/longform-qa/README.md
==================
49c520252;Yacine Jernite;2020-06-16 16:36:58 -0400;Eli5 examples (#4968)
* add eli5 examples

* add dense query script

* query_di

* merging

* merging

* add_utils

* adds nearest neighbor wikipedia

* batch queries

* training_retriever

* new notebooks

* moved retriever traiing script

* finished wiki40b

* max_len_fix

* train_s2s

* retriever_batch_checkpointing

* cleanup

* merge

* dim_fix

* fix_indexer

* fix_wiki40b_snippets

* fix_embed_for_r

* fp32 index

* fix_sparse_q

* joint_training

* remove obsolete datasets

* add_passage_nn_results

* add_passage_nn_results

* add_batch_nn

* add_batch_nn

* add_data_scripts

* notebook

* notebook

* notebook

* fix_multi_gpu

* add_app

* full_caching

* full_caching

* notebook

* sparse_done

* images

* notebook

* add_image_gif

* with_Gif

* add_contr_image

* notebook

* notebook

* notebook

* train_functions

* notebook

* min_retrieval_length

* pandas_option

* notebook

* min_retrieval_length

* notebook

* notebook

* eval_Retriever

* notebook

* images

* notebook

* add_example

* add_example

* notebook

* fireworks

* notebook

* notebook

* joe's notebook comments

* app_update

* notebook

* notebook_link

* captions

* notebook

* assing RetriBert model

* add RetriBert to Auto

* change AutoLMHead to AutoSeq2Seq

* notebook downloads from hf models

* style_black

* style_black

* app_update

* app_update

* fix_app_update

* style

* style

* isort

* Delete WikiELI5training.ipynb

* Delete evaluate_eli5.py

* Delete WikiELI5explore.ipynb

* Delete ExploreWikiELI5Support.html

* Delete explainlikeimfive.py

* Delete wiki_snippets.py

* children before parent

* children before parent

* style_black

* style_black_only

* isort

* isort_new

* Update src/transformers/modeling_retribert.py

Co-authored-by: Julien Chaumond <chaumond@gmail.com>

* typo fixes

* app_without_asset

* cleanup

* Delete ELI5animation.gif

* Delete ELI5contrastive.svg

* Delete ELI5wiki_index.svg

* Delete choco_bis.svg

* Delete fireworks.gif

* Delete huggingface_logo.jpg

* Delete huggingface_logo.svg

* Delete Long_Form_Question_Answering_with_ELI5_and_Wikipedia.ipynb

* Delete eli5_app.py

* Delete eli5_utils.py

* readme

* Update README.md

* unused imports

* moved_info

* default_beam

* ftuned model

* disclaimer

* Update src/transformers/modeling_retribert.py

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* black

* add_doc

* names

* isort_Examples

* isort_Examples

* Add doc to index

Co-authored-by: Julien Chaumond <chaumond@gmail.com>
Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
Co-authored-by: Lysandre <lysandre.debut@reseau.eseo.fr>
==

docs/source/index.rst
docs/source/model_doc/retribert.rst
examples/longform-qa/README.md
examples/longform-qa/eli5_app.py
examples/longform-qa/eli5_utils.py
src/transformers/__init__.py
src/transformers/configuration_auto.py
src/transformers/configuration_bart.py
src/transformers/configuration_retribert.py
src/transformers/modeling_auto.py
src/transformers/modeling_retribert.py
src/transformers/tokenization_auto.py
src/transformers/tokenization_bart.py
src/transformers/tokenization_retribert.py
==================
c3e607496;Sam Shleifer;2020-06-16 14:06:45 -0400;[cleanup] examples test_run_squad uses tiny model (#5059)

==

examples/test_examples.py
==================
439aa1d6e;Sylvain Gugger;2020-06-16 13:03:41 -0400;Remove old section + caching in install (#5027)

==

docs/source/installation.md
docs/source/serialization.rst
==================
3d495c61e;Sam Shleifer;2020-06-16 09:48:19 -0400;Fix marian tokenizer save pretrained (#5043)

==

src/transformers/tokenization_marian.py
tests/test_tokenization_marian.py
==================
d5477baf7;Sylvain Gugger;2020-06-16 08:06:31 -0400;Convert hans to Trainer (#5025)
* Convert hans to Trainer

* Tick box
==

examples/README.md
examples/adversarial/run_hans.py
examples/adversarial/test_hans.py
examples/adversarial/utils_hans.py
==================
c852036b4;Amil Khare;2020-06-16 17:33:43 +0530;[cleanup] Hoist ModelTester objects to top level (#4939)
Co-authored-by: Sam Shleifer <sshleifer@gmail.com>
==

tests/test_modeling_albert.py
tests/test_modeling_ctrl.py
tests/test_modeling_distilbert.py
tests/test_modeling_electra.py
tests/test_modeling_flaubert.py
tests/test_modeling_gpt2.py
tests/test_modeling_longformer.py
tests/test_modeling_openai.py
tests/test_modeling_roberta.py
tests/test_modeling_t5.py
tests/test_modeling_tf_albert.py
tests/test_modeling_tf_bert.py
tests/test_modeling_tf_ctrl.py
tests/test_modeling_tf_distilbert.py
tests/test_modeling_tf_electra.py
tests/test_modeling_tf_gpt2.py
tests/test_modeling_tf_openai_gpt.py
tests/test_modeling_tf_roberta.py
tests/test_modeling_tf_t5.py
tests/test_modeling_tf_transfo_xl.py
tests/test_modeling_tf_xlm.py
tests/test_modeling_tf_xlnet.py
tests/test_modeling_transfo_xl.py
tests/test_modeling_xlm.py
tests/test_modeling_xlnet.py
==================
0c55a384f;Manuel Romero;2020-06-16 10:19:09 +0200;Add reference to NLP dataset (#5028)
* Add reference to NLP dataset

* Update README.md

Co-authored-by: Julien Chaumond <chaumond@gmail.com>
==

model_cards/mrm8488/t5-base-finetuned-squadv2/README.md
==================
0946d1209;Manuel Romero;2020-06-16 10:17:46 +0200;Add reference to NLP (package) dataset (#5029)
* Add reference to NLP (package) dataset

* Update README.md

Co-authored-by: Julien Chaumond <chaumond@gmail.com>
==

model_cards/mrm8488/longformer-base-4096-finetuned-squadv2/README.md
==================
edcb3ac59;Boris Dayma;2020-06-16 02:40:43 -0500;refactor(wandb): consolidate import (#5044)

==

src/transformers/trainer.py
src/transformers/trainer_tf.py
src/transformers/trainer_utils.py
==================
9e0336499;Funtowicz Morgan;2020-06-16 09:25:25 +0200;Ability to pickle/unpickle BatchEncoding pickle (reimport) (#5039)
* Added is_fast property on BatchEncoding to indicate if the object comes from a Fast Tokenizer.

* Added __get_state__() & __set_state__() to be pickable.

* Correct tokens() return type from List[int] to List[str]

* Added unittest for BatchEncoding pickle/unpickle

* Added unittest for BatchEncoding is_fast

* More careful checking on BatchEncoding unpickle tests.

* Formatting.

* is_fast should assertTrue on Rust tokenizers.

* Ensure tensorflow has correct way of checking array_equal

* More formatting.
==

src/transformers/tokenization_utils_base.py
tests/test_tokenization_utils.py
==================
f9f8a5312;Sylvain Gugger;2020-06-15 18:31:41 -0400;Add DistilBertForMultipleChoice (#5032)
* Add `DistilBertForMultipleChoice`

==

docs/source/model_doc/distilbert.rst
src/transformers/__init__.py
src/transformers/configuration_distilbert.py
src/transformers/modeling_auto.py
src/transformers/modeling_distilbert.py
tests/test_modeling_distilbert.py
==================
36434220f;Anthony MOI;2020-06-15 17:12:51 -0400;[HUGE] Refactoring tokenizers backend - padding - truncation - pre-tokenized pipeline - fast tokenizers - tests (#4510)
* Use tokenizers pre-tokenized pipeline

* failing pretrokenized test

* Fix is_pretokenized in python

* add pretokenized tests

* style and quality

* better tests for batched pretokenized inputs

* tokenizers clean up - new padding_strategy - split the files

* [HUGE] refactoring tokenizers - padding - truncation - tests

* style and quality

* bump up requied tokenizers version to 0.8.0-rc1

* switched padding/truncation API - simpler better backward compat

* updating tests for custom tokenizers

* style and quality - tests on pad

* fix QA pipeline

* fix backward compatibility for max_length only

* style and quality

* Various cleans up - add verbose

* fix tests

* update docstrings

* Fix tests

* Docs reformatted

* __call__ method documented

Co-authored-by: Thomas Wolf <thomwolf@users.noreply.github.com>
Co-authored-by: Lysandre <lysandre.debut@reseau.eseo.fr>
==

docs/source/main_classes/tokenizer.rst
examples/summarization/utils.py
setup.py
src/transformers/__init__.py
src/transformers/modeling_tf_albert.py
src/transformers/tokenization_bert.py
src/transformers/tokenization_gpt2.py
src/transformers/tokenization_openai.py
src/transformers/tokenization_transfo_xl.py
src/transformers/tokenization_utils.py
src/transformers/tokenization_utils_base.py
src/transformers/tokenization_utils_fast.py
templates/adding_a_new_model/tests/test_tokenization_xxx.py
tests/test_tokenization_albert.py
tests/test_tokenization_bert.py
tests/test_tokenization_bert_japanese.py
tests/test_tokenization_common.py
tests/test_tokenization_ctrl.py
tests/test_tokenization_fast.py
tests/test_tokenization_gpt2.py
tests/test_tokenization_marian.py
tests/test_tokenization_openai.py
tests/test_tokenization_roberta.py
tests/test_tokenization_transfo_xl.py
tests/test_tokenization_xlm.py
==================
ebba39e4e;Patrick von Platen;2020-06-15 22:50:09 +0200;[Bart] Question Answering Model is added to tests (#5024)
* fix test

* Update tests/test_modeling_common.py

* Update tests/test_modeling_common.py
==

tests/test_modeling_bart.py
tests/test_modeling_common.py
==================
bbad4c698;Sylvain Gugger;2020-06-15 15:50:17 -0400;Add position_ids (#5021)

==

src/transformers/modeling_tf_electra.py
==================
1bf4098e0;Boris Dayma;2020-06-15 13:06:17 -0500;feat(TFTrainer): improve logging (#4946)
* feat(tftrainer): improve logging

* fix(trainer): consider case with evaluation only

* refactor(tftrainer): address comments

* refactor(tftrainer): move self.epoch_logging to __init__
==

src/transformers/trainer.py
src/transformers/trainer_tf.py
==================
7b5a1e7d5;Funtowicz Morgan;2020-06-15 19:36:57 +0200;Fix importing transformers on Windows (#4997)

==

src/transformers/benchmark/benchmark_utils.py
==================
a9f1fc6c9;Sam Shleifer;2020-06-15 13:29:26 -0400;Add bart-base (#5014)

==

docs/source/pretrained_models.rst
src/transformers/tokenization_bart.py
tests/test_modeling_bart.py
==================
7b685f522;Funtowicz Morgan;2020-06-15 19:13:58 +0200;Increase pipeline support for ONNX export. (#5005)
* Increase pipeline support for ONNX export.

* Style.
==

src/transformers/convert_graph_to_onnx.py
==================
1affde2f1;Sylvain Gugger;2020-06-15 11:58:33 -0400;Make DataCollator a callable (#5015)
* Make DataCollator a callable

* Update src/transformers/data/data_collator.py

Co-authored-by: Julien Chaumond <chaumond@gmail.com>
==

examples/adversarial/test_hans.py
examples/bertology/run_bertology.py
src/transformers/__init__.py
src/transformers/data/data_collator.py
src/transformers/trainer.py
tests/test_trainer.py
tests/test_trainer_distributed.py
==================
f7c93b3ce;Bram Vanroy;2020-06-15 16:10:26 +0200;Possible fix to make AMP work with DDP in the trainer (#4728)
* manually set device in trainer args

* check if current device is cuda before set_device

* Explicitly set GPU ID when using single GPU

This addresses https://github.com/huggingface/transformers/issues/4657#issuecomment-642228099
==

src/transformers/training_args.py
==================
66bcfbb13;ipuneetrathore;2020-06-15 18:13:50 +0530;Create README.md (#4975)
* Create README.md

* Update model_cards/ipuneetrathore/bert-base-cased-finetuned-finBERT/README.md

Co-authored-by: Julien Chaumond <chaumond@gmail.com>
==

model_cards/ipuneetrathore/bert-base-cased-finetuned-finBERT/README.md
==================
d812e6d76;Stefan Schweter;2020-06-15 14:30:40 +0200;NER: fix construction of input examples for RoBERTa (#4943)
* utils_ner: do not add extra sep token for RoBERTa model

* run_pl_ner: do not add extra sep token for RoBERTa model
==

examples/token-classification/run_pl_ner.py
examples/token-classification/utils_ner.py
==================
ebab096e8;Suraj Patil;2020-06-15 15:09:41 +0530;[model card] model card for bart-large-finetuned-squadv1 (#4977)
* [model card] model card for bart-large-finetuned-squadv1

* add metadata link to the dataset
==

model_cards/valhalla/bart-large-finetuned-squadv1/README.md
==================
9ad36ad57;Funtowicz Morgan;2020-06-15 11:04:51 +0200;Improve ONNX logging (#4999)
* Improve ONNX export logging to give more information about the generated graph.

* Correctly handle input and output in the logging.
==

src/transformers/convert_graph_to_onnx.py
==================
9931f817b;ZhuBaohe;2020-06-15 03:36:14 +0800;fix (#4976)

==

src/transformers/modeling_albert.py
src/transformers/modeling_bart.py
src/transformers/modeling_bert.py
src/transformers/modeling_camembert.py
src/transformers/modeling_ctrl.py
src/transformers/modeling_distilbert.py
src/transformers/modeling_electra.py
src/transformers/modeling_flaubert.py
src/transformers/modeling_gpt2.py
src/transformers/modeling_longformer.py
src/transformers/modeling_mmbt.py
src/transformers/modeling_openai.py
src/transformers/modeling_reformer.py
src/transformers/modeling_roberta.py
src/transformers/modeling_t5.py
src/transformers/modeling_tf_albert.py
src/transformers/modeling_tf_bert.py
src/transformers/modeling_tf_camembert.py
src/transformers/modeling_tf_ctrl.py
src/transformers/modeling_tf_distilbert.py
src/transformers/modeling_tf_electra.py
src/transformers/modeling_tf_flaubert.py
src/transformers/modeling_tf_gpt2.py
src/transformers/modeling_tf_openai.py
src/transformers/modeling_tf_roberta.py
src/transformers/modeling_tf_t5.py
src/transformers/modeling_tf_transfo_xl.py
src/transformers/modeling_tf_xlm.py
src/transformers/modeling_tf_xlm_roberta.py
src/transformers/modeling_tf_xlnet.py
src/transformers/modeling_transfo_xl.py
src/transformers/modeling_xlm.py
src/transformers/modeling_xlm_roberta.py
src/transformers/modeling_xlnet.py
==================
9208f57b1;Suraj Patil;2020-06-14 22:34:49 +0530;BartTokenizerFast (#4878)

==

src/transformers/__init__.py
src/transformers/tokenization_bart.py
==================
403d30985;Sylvain Gugger;2020-06-13 09:35:13 -0400;Hans data (#4854)
* Update hans data to be able to use Trainer

* Fixes

* Deal with tokenizer that don't have token_ids

* Clean up things

* Simplify data use

* Fix the input dict

* Formatting + proper path in README
==

examples/adversarial/README.md
examples/adversarial/hans_processors.py
examples/adversarial/test_hans.py
examples/adversarial/utils_hans.py
==================
ca5e1cdf8;Julien Chaumond;2020-06-12 23:19:07 +0200;model_cards: we can now tag datasets
see corresponding model pages to see how it's rendered

==

model_cards/canwenxu/BERT-of-Theseus-MNLI/README.md
model_cards/deepset/roberta-base-squad2/README.md
==================
e93ccb329;Suraj Patil;2020-06-13 01:17:57 +0530;BartForQuestionAnswering (#4908)

==

docs/source/model_doc/bart.rst
src/transformers/__init__.py
src/transformers/modeling_auto.py
src/transformers/modeling_bart.py
tests/test_modeling_bart.py
==================
538531cde;Sylvain Gugger;2020-06-12 14:20:19 -0400;Add AlbertForMultipleChoice (#4959)
* Add AlbertForMultipleChoice

* Make up to date and add all models to common tests

==

docs/source/model_doc/albert.rst
src/transformers/__init__.py
src/transformers/modeling_albert.py
src/transformers/modeling_auto.py
src/transformers/modeling_bert.py
tests/test_modeling_albert.py
==================
fe2413970;Manuel Romero;2020-06-12 15:03:43 +0200;Create README.md (#4865)

==

model_cards/mrm8488/t5-base-finetuned-summarize-news/README.md
==================
9aa219a1f;Yannis Papanikolaou;2020-06-12 16:03:13 +0300;Create README.md (#4872)

==

model_cards/healx/gpt-2-pubmed-large/README.md
==================
86578bb04;Patrick von Platen;2020-06-12 10:01:49 +0200;[AutoModel] Split AutoModelWithLMHead into clm, mlm, encoder-decoder (#4933)
* first commit

* add new auto models

* better naming

* fix bert automodel

* fix automodel for pretraining

* add models to init

* fix name typo

* fix typo

* better naming

* future warning instead of depreciation warning
==

src/transformers/__init__.py
src/transformers/modeling_auto.py
src/transformers/modeling_bert.py
src/transformers/modeling_encoder_decoder.py
tests/test_modeling_auto.py
tests/test_modeling_bert.py
==================
562003311;Sam Shleifer;2020-06-11 22:11:34 -0400;[mbart] Fix fp16 testing logic (#4949)

==

tests/test_modeling_bart.py
==================
473808da0;VictorSanh;2020-06-11 19:42:45 +0000;update `mvmt-pruning/saving_prunebert` (updating torch to 1.5)

==

examples/movement-pruning/Saving_PruneBERT.ipynb
==================
caf374667;Patrick von Platen;2020-06-11 21:28:01 +0200;fix indentation issue (#4941)

==

src/transformers/benchmark/benchmark.py
==================
6293eb04d;Suraj Patil;2020-06-11 22:46:34 +0530;[Model card] model card for electra-base QA model (#4936)

==

model_cards/valhalla/electra-base-discriminator-finetuned_squadv1/README.md
==================
08b59d10e;Sam Shleifer;2020-06-11 13:02:33 -0400;MBartTokenizer:add language codes (#3776)

==

src/transformers/tokenization_bart.py
tests/test_modeling_bart.py
==================
20451195f;Sylvain Gugger;2020-06-11 10:31:26 -0400;Support multiple choice in tf common model tests (#4920)
* Support multiple choice in tf common model tests

* Add the input_embeds test

==

src/transformers/modeling_tf_bert.py
tests/test_modeling_tf_bert.py
tests/test_modeling_tf_common.py
==================
699541c4b;Setu Shah;2020-06-10 23:11:22 -0700;TFTrainer: Add dataloader_drop_last (#4925)

==

src/transformers/trainer.py
src/transformers/trainer_tf.py
==================
e80d6c689;RafaelWO;2020-06-11 01:03:06 +0200;Fix resize_token_embeddings for Transformer-XL (#4759)
* Fixed resize_token_embeddings for transfo_xl model

* Fixed resize_token_embeddings for transfo_xl.

Added custom methods to TransfoXLPreTrainedModel for resizing layers of
the AdaptiveEmbedding.

* Updated docstring

* Fixed resizinhg cutoffs; added check for new size of embedding layer.

* Added test for resize_token_embeddings

* Fixed code quality

* Fixed unchanged cutoffs in model.config

Co-authored-by: Rafael Weingartner <rweingartner.its-b2015@fh-salzburg.ac.at>
==

src/transformers/modeling_transfo_xl.py
tests/test_modeling_transfo_xl.py
==================
d541938c4;Sylvain Gugger;2020-06-10 18:38:34 -0400;Make multiple choice models work with input_embeds (#4921)

==

src/transformers/modeling_bert.py
src/transformers/modeling_longformer.py
src/transformers/modeling_roberta.py
src/transformers/modeling_xlnet.py
tests/test_modeling_common.py
==================
1e2631d6f;Sylvain Gugger;2020-06-10 18:26:42 -0400;Split LMBert model in two (#4874)
* Split LMBert model in two

* Fix example

* Remove lm_labels

* Adapt tests, refactor prepare_for_generation

* Fix merge

* Hide BeartLMHeadModel

==

src/transformers/modeling_bert.py
src/transformers/modeling_encoder_decoder.py
tests/test_modeling_bert.py
tests/test_modeling_encoder_decoder.py
==================
f6da8b220;Matthew Goldey;2020-06-10 18:25:55 -0400;check type before logging in trainer to ensure values are scalars (#4883)
* check type before logging to ensure it's a scalar

* log when Trainer attempts to add a non-scalar value using TensorboardX's writer.add_scalar so we know what kinds of fixes are appropriate

* black it

* rephrase log message to clarify attribute was dropped

Co-authored-by: Julien Chaumond <chaumond@gmail.com>

Co-authored-by: Julien Chaumond <chaumond@gmail.com>
==

src/transformers/trainer.py
==================
1c986f42f;Yannis Papanikolaou;2020-06-11 00:29:41 +0300;Create README.md (#4871)

==

model_cards/healx/gpt-2-pubmed-medium/README.md
==================
3ae2e86ba;Lysandre Debut;2020-06-10 16:28:18 -0400;Run a single wandb instance per TPU run (#4851)
* Run a single wandb instance per TPU run

* wandb: self.is_world_master

* make style

Co-authored-by: Julien Chaumond <chaumond@gmail.com>
==

src/transformers/trainer.py
==================
466aa57a4;Lysandre Debut;2020-06-10 15:53:15 -0400;Don't init TPU device twice (#4916)

==

src/transformers/file_utils.py
==================
ef2dcdcca;Suraj Patil;2020-06-11 00:47:52 +0530;ElectraForQuestionAnswering (#4913)
* ElectraForQuestionAnswering

* udate __init__

* add test for electra qa model

* add ElectraForQuestionAnswering in auto models

* add ElectraForQuestionAnswering in all_model_classes

* fix outputs, input_ids defaults to None

* add ElectraForQuestionAnswering in docs

* remove commented line
==

docs/source/model_doc/electra.rst
src/transformers/__init__.py
src/transformers/modeling_auto.py
src/transformers/modeling_electra.py
tests/test_modeling_electra.py
==================
5d63ca6c3;Amil Khare;2020-06-10 23:36:55 +0530;[ctrl] fix pruning of MultiHeadAttention (#4904)

==

src/transformers/modeling_ctrl.py
tests/test_modeling_ctrl.py
==================
4e10acb3e;Sylvain Gugger;2020-06-10 13:19:53 -0400;Add more models to common tests (#4910)

==

src/transformers/modeling_distilbert.py
src/transformers/modeling_electra.py
src/transformers/modeling_longformer.py
src/transformers/modeling_roberta.py
tests/test_modeling_distilbert.py
tests/test_modeling_electra.py
tests/test_modeling_longformer.py
tests/test_modeling_roberta.py
tests/test_modeling_xlnet.py
==================
3b3619a32;Patrick von Platen;2020-06-10 18:10:59 +0200;[All models] fix docs after adding output attentions to all forward functions (#4909)
* fix doc

* add format file

* add output attentions to all docs

* add also for bart

* fix naming

* re-add doc to config
==

src/transformers/configuration_utils.py
src/transformers/modeling_albert.py
src/transformers/modeling_bart.py
src/transformers/modeling_bert.py
src/transformers/modeling_camembert.py
src/transformers/modeling_ctrl.py
src/transformers/modeling_distilbert.py
src/transformers/modeling_electra.py
src/transformers/modeling_flaubert.py
src/transformers/modeling_gpt2.py
src/transformers/modeling_longformer.py
src/transformers/modeling_mmbt.py
src/transformers/modeling_openai.py
src/transformers/modeling_reformer.py
src/transformers/modeling_roberta.py
src/transformers/modeling_t5.py
src/transformers/modeling_tf_albert.py
src/transformers/modeling_tf_bert.py
src/transformers/modeling_tf_camembert.py
src/transformers/modeling_tf_ctrl.py
src/transformers/modeling_tf_distilbert.py
src/transformers/modeling_tf_electra.py
src/transformers/modeling_tf_flaubert.py
src/transformers/modeling_tf_gpt2.py
src/transformers/modeling_tf_openai.py
src/transformers/modeling_tf_roberta.py
src/transformers/modeling_tf_t5.py
src/transformers/modeling_tf_transfo_xl.py
src/transformers/modeling_tf_xlm.py
src/transformers/modeling_tf_xlm_roberta.py
src/transformers/modeling_tf_xlnet.py
src/transformers/modeling_transfo_xl.py
src/transformers/modeling_xlm.py
src/transformers/modeling_xlm_roberta.py
src/transformers/modeling_xlnet.py
==================
ac99217e9;Sylvain Gugger;2020-06-10 09:26:06 -0400;Fix the CI (#4903)
* Fix CI
==

tests/test_modeling_common.py
==================
0a375f5ab;Sylvain Gugger;2020-06-10 08:10:20 -0400;Deal with multiple choice in common tests (#4886)
* Deal with multiple choice in common tests

==

tests/test_modeling_bert.py
tests/test_modeling_common.py
==================
e8db8b845;Sylvain Gugger;2020-06-09 20:05:09 -0400;Remove unused arguments in Multiple Choice example (#4853)
* Remove unused arguments

* Formatting

* Remove second todo comment
==

examples/multiple-choice/utils_multiple_choice.py
==================
29c36e9f3;songyouwei;2020-06-10 07:14:27 +0800;run_pplm.py bug fix (#4867)
`is_leaf` may become `False` after `.to(device=device)` function call.
==

examples/text-generation/pplm/run_pplm.py
==================
13aa17411;Lysandre;2020-06-09 18:50:56 -0400;uninstalled wandb raises AttributeError

==

src/transformers/trainer.py
==================
6e603cb78;Bharat Raghunathan;2020-06-10 03:09:06 +0530;[All models] Extend config.output_attentions with output_attentions function arguments (#4538)
* DOC: Replace instances of ``config.output_attentions`` with function argument ``output_attentions``

* DOC: Apply Black Formatting

* Fix errors where output_attentions was undefined

* Remove output_attentions in classes per review

* Fix regressions on tests having `output_attention`

* Fix further regressions in tests relating to `output_attentions`

Ensure proper propagation of `output_attentions` as a function parameter
to all model subclasses

* Fix more regressions in `test_output_attentions`

* Fix issues with BertEncoder

* Rename related variables to `output_attentions`

* fix pytorch tests

* fix bert and gpt2 tf

* Fix most TF tests for `test_output_attentions`

* Fix linter errors and more TF tests

* fix conflicts

* DOC: Apply Black Formatting

* Fix errors where output_attentions was undefined

* Remove output_attentions in classes per review

* Fix regressions on tests having `output_attention`

* fix conflicts

* fix conflicts

* fix conflicts

* fix conflicts

* fix pytorch tests

* fix conflicts

* fix conflicts

* Fix linter errors and more TF tests

* fix tf tests

* make style

* fix isort

* improve output_attentions

* improve tensorflow

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
==

src/transformers/configuration_utils.py
src/transformers/modeling_albert.py
src/transformers/modeling_bart.py
src/transformers/modeling_bert.py
src/transformers/modeling_ctrl.py
src/transformers/modeling_distilbert.py
src/transformers/modeling_electra.py
src/transformers/modeling_flaubert.py
src/transformers/modeling_gpt2.py
src/transformers/modeling_longformer.py
src/transformers/modeling_mmbt.py
src/transformers/modeling_openai.py
src/transformers/modeling_reformer.py
src/transformers/modeling_roberta.py
src/transformers/modeling_t5.py
src/transformers/modeling_tf_albert.py
src/transformers/modeling_tf_bert.py
src/transformers/modeling_tf_ctrl.py
src/transformers/modeling_tf_distilbert.py
src/transformers/modeling_tf_electra.py
src/transformers/modeling_tf_flaubert.py
src/transformers/modeling_tf_gpt2.py
src/transformers/modeling_tf_openai.py
src/transformers/modeling_tf_roberta.py
src/transformers/modeling_tf_t5.py
src/transformers/modeling_tf_transfo_xl.py
src/transformers/modeling_tf_utils.py
src/transformers/modeling_tf_xlm.py
src/transformers/modeling_tf_xlnet.py
src/transformers/modeling_transfo_xl.py
src/transformers/modeling_xlm.py
src/transformers/modeling_xlnet.py
templates/adding_a_new_model/modeling_tf_xxx.py
templates/adding_a_new_model/modeling_xxx.py
tests/test_modeling_common.py
tests/test_modeling_longformer.py
tests/test_modeling_tf_common.py
tests/test_modeling_xlnet.py
==================
f90bc44d9;Sam Shleifer;2020-06-09 17:38:28 -0400;[examples] Cleanup summarization docs (#4876)

==

examples/summarization/README.md
examples/summarization/download_cnn_daily_mail.py
examples/summarization/finetune_bart.sh
examples/summarization/finetune_bart_tiny.sh
examples/summarization/finetune_t5.sh
==================
2cfb947f5;Patrick von Platen;2020-06-09 23:12:43 +0200;[Benchmark] add tpu and torchscipt for benchmark (#4850)
* add tpu and torchscipt for benchmark

* fix name in tests

* "fix email"

* make style

* better log message for tpu

* add more print and info for tpu

* allow possibility to print tpu metrics

* correct cpu usage

* fix test for non-install

* remove bugus file

* include psutil in testing

* run a couple of times before tracing in torchscript

* do not allow tpu memory tracing for now

* make style

* add torchscript to env

* better name for torch tpu

Co-authored-by: Patrick von Platen <patrick@huggingface.co>
==

setup.py
src/transformers/__init__.py
src/transformers/benchmark/benchmark.py
src/transformers/benchmark/benchmark_args.py
src/transformers/benchmark/benchmark_utils.py
src/transformers/file_utils.py
src/transformers/trainer.py
src/transformers/training_args.py
tests/test_benchmark.py
==================
f0340b303;Hamza Harkous;2020-06-09 22:14:01 +0200;Removes  from the  of the parent of TFRobertaClassificationHead (#4884)
Co-authored-by: Hamza Harkous <harkous@google.com>
==

src/transformers/modeling_tf_roberta.py
==================
02e5f7966;Amil Khare;2020-06-09 20:44:12 +0530;[examples] consolidate summarization examples (#4837)

==

examples/summarization/README.md
examples/summarization/bart/__init__.py
examples/summarization/bart/evaluate_cnn.py
examples/summarization/download_cnn_daily_mail.py
examples/summarization/evaluate_cnn.py
examples/summarization/finetune.py
examples/summarization/finetune_bart.sh
examples/summarization/finetune_bart_tiny.sh
examples/summarization/finetune_t5.sh
examples/summarization/t5/README.md
examples/summarization/t5/__init__.py
examples/summarization/t5/evaluate_cnn.py
examples/summarization/t5/test_t5_examples.py
examples/summarization/test_summarization_examples.py
examples/summarization/utils.py
==================
9f5d5a531;Julien Plu;2020-06-09 09:44:00 +0200;Fix the __getattr__ method in BatchEncoding (#4772)

==

src/transformers/tokenization_utils.py
==================
41a1d27cd;Sylvain Gugger;2020-06-08 21:22:37 -0400;Add XLMRobertaForQuestionAnswering (#4855)
* Add XLMRobertaForQuestionAnswering

* Formatting

* Make test happy
==

docs/source/model_doc/xlmroberta.rst
src/transformers/__init__.py
src/transformers/modeling_auto.py
src/transformers/modeling_xlm_roberta.py
==================
a139d1a16;Sam Shleifer;2020-06-08 17:08:04 -0400;[cleanup] consolidate some prune_heads logic (#4799)

==

src/transformers/modeling_albert.py
src/transformers/modeling_bert.py
src/transformers/modeling_distilbert.py
src/transformers/modeling_gpt2.py
src/transformers/modeling_openai.py
src/transformers/modeling_t5.py
src/transformers/modeling_utils.py
src/transformers/modeling_xlm.py
==================
4c7f564f9;ZhuBaohe;2020-06-09 00:28:50 +0800;fix (#4839)

==

src/transformers/modeling_longformer.py
==================
37be3786c;Sylvain Gugger;2020-06-08 11:28:19 -0400;Clean documentation (#4849)
* Clean documentation
==

docs/source/model_doc/albert.rst
docs/source/model_doc/bart.rst
docs/source/model_doc/camembert.rst
docs/source/model_doc/ctrl.rst
docs/source/model_doc/distilbert.rst
docs/source/model_doc/electra.rst
docs/source/model_doc/flaubert.rst
docs/source/model_doc/longformer.rst
docs/source/model_doc/marian.rst
docs/source/model_doc/roberta.rst
docs/source/model_doc/t5.rst
docs/source/model_doc/xlm.rst
docs/source/model_doc/xlmroberta.rst
docs/source/model_doc/xlnet.rst
src/transformers/modeling_electra.py
src/transformers/modeling_longformer.py
src/transformers/modeling_roberta.py
src/transformers/modeling_xlnet.py
==================
42860e92a;Lysandre;2020-06-08 09:47:00 -0400;Turn off codecov patch for now

==

codecov.yml
==================
36dfc317b;Julien Plu;2020-06-08 15:45:23 +0200;TF Checkpoints (#4831)
* Align checkpoint dir with the PT trainer

* Use args for max to keep checkpoints
==

src/transformers/trainer_tf.py
==================
439f1cab2;Patrick von Platen;2020-06-08 15:31:32 +0200;[Generate] beam search should generate without replacement (#4845)
* fix flaky beam search

* fix typo
==

src/transformers/modeling_tf_utils.py
==================
c0554776d;Patrick von Platen;2020-06-08 15:31:12 +0200;fix PR (#4810)

==

src/transformers/benchmark/benchmark.py
src/transformers/benchmark/benchmark_args_utils.py
src/transformers/benchmark/benchmark_utils.py
tests/test_benchmark.py
==================
e81774794;Sylvain Gugger;2020-06-08 08:14:32 -0400;Expose classes used in documentation (#4808)
* Expose classes used in documentation

* Format code
==

src/transformers/__init__.py
==================
b6f365a8e;daniel-shan;2020-06-08 02:36:09 -0700;Updates args in tf squad example. (#4820)
Co-authored-by: Daniel Shan <daniel.shan@workday.com>
==

examples/question-answering/README.md
==================
e33fdc93b;Bram Vanroy;2020-06-07 17:55:10 +0200;Export PretrainedBartModel from __init__ (#4819)

==

src/transformers/__init__.py
==================
c58e6c129;Sam Shleifer;2020-06-06 00:52:17 -0400;[marian tests ] pass device to pipeline (#4815)

==

tests/test_modeling_marian.py
==================
ddf9a3dfc;Mr Ruben;2020-06-06 03:16:48 +0200;Updated path "cd examples/text-generation/pplm" (#4778)
https://github.com/huggingface/transformers/issues/4776
==

examples/text-generation/pplm/README.md
==================
2d372a990;Sylvain Gugger;2020-06-05 20:47:02 -0400;Explain how to preview the docs in a PR (#4795)

==

docs/README.md
==================
56d5d160c;Sylvain Gugger;2020-06-05 18:45:42 -0400;Add model and doc badges (#4811)
* Add badges for models and docs
==

docs/source/summary.rst
==================
4ab742459;Sam Shleifer;2020-06-05 18:45:19 -0400;[cleanup/marian] pipelines test and new kwarg (#4812)

==

src/transformers/tokenization_marian.py
tests/test_modeling_marian.py
tests/test_tokenization_marian.py
==================
875288b34;Sam Shleifer;2020-06-05 17:27:31 -0400;[isort] add matplotlib to known 3rd party dependencies (#4800)

==

examples/benchmarking/plot_csv_file.py
setup.cfg
==================
8cca87556;Patrick von Platen;2020-06-05 23:16:37 +0200;[EncoderDecoderConfig] automatically set decoder config to decoder (#4809)
* automatically set decoder config to decoder

* add more tests
==

src/transformers/configuration_encoder_decoder.py
tests/test_modeling_encoder_decoder.py
==================
f1fe18465;Sylvain Gugger;2020-06-05 16:41:46 -0400;Use labels to remove deprecation warnings (#4807)

==

tests/test_modeling_albert.py
tests/test_modeling_bart.py
tests/test_modeling_bert.py
tests/test_modeling_distilbert.py
tests/test_modeling_electra.py
tests/test_modeling_gpt2.py
tests/test_modeling_longformer.py
tests/test_modeling_openai.py
tests/test_modeling_roberta.py
tests/test_modeling_t5.py
==================
5c0cfc2cf;Sylvain Gugger;2020-06-05 15:29:20 -0400;Add link to community models (#4804)

==

docs/source/summary.rst
==================
4dd5cf220;Sylvain Gugger;2020-06-05 15:20:29 -0400;Fix argument label (#4792)
* Fix argument label

* Fix test
==

src/transformers/data/data_collator.py
tests/test_trainer.py
==================
3723f30a1;Sam Shleifer;2020-06-05 14:57:24 -0400;[cleanup] MarianTokenizer: delete unused constants (#4802)

==

src/transformers/tokenization_marian.py
==================
acaa2e626;Sylvain Gugger;2020-06-05 12:36:22 -0400;Clean-up code (#4790)

==

src/transformers/configuration_xlnet.py
==================
fa661ce74;Sylvain Gugger;2020-06-05 12:22:50 -0400;Add model summary (#4789)
* Add model summary

* Add link to pretrained models
==

docs/source/imgs/local_attention_mask.png
docs/source/index.rst
docs/source/summary.rst
==================
79ab881eb;Lysandre Debut;2020-06-05 12:01:43 -0400;No silent error when d_head already in the configuration (#4747)
* No silent error when d_head already in the configuration

* Update src/transformers/configuration_xlnet.py

Co-authored-by: Julien Chaumond <chaumond@gmail.com>

Co-authored-by: Julien Chaumond <chaumond@gmail.com>
==

src/transformers/configuration_xlnet.py
==================
b9109f2de;Julien Chaumond;2020-06-05 14:59:22 +0200;[doc] Make it clearer that `text-generation` does not involve training

==

examples/README.md
==================
ceaab8dd2;Sylvain Gugger;2020-06-05 07:56:11 -0400;Add .vs to gitignore (#4774)

==

.gitignore
==================
f9414f755;Julien Plu;2020-06-05 01:45:53 +0200;Tensorflow improvements (#4530)
* Better None gradients handling

* Apply Style

* Apply Style

* Create a loss class per task to compute its respective loss

* Add loss classes to the ALBERT TF models

* Add loss classes to the BERT TF models

* Add question answering and multiple choice to TF Camembert

* Remove prints

* Add multiple choice model to TF DistilBERT + loss computation

* Add question answering model to TF Electra + loss computation

* Add token classification, question answering and multiple choice models to TF Flaubert

* Add multiple choice model to TF Roberta + loss computation

* Add multiple choice model to TF XLM + loss computation

* Add multiple choice and question answering models to TF XLM-Roberta

* Add multiple choice model to TF XLNet + loss computation

* Remove unused parameters

* Add task loss classes

* Reorder TF imports + add new model classes

* Add new model classes

* Bugfix in TF T5 model

* Bugfix for TF T5 tests

* Bugfix in TF T5 model

* Fix TF T5 model tests

* Fix T5 tests + some renaming

* Fix inheritance issue in the AutoX tests

* Add tests for TF Flaubert and TF XLM Roberta

* Add tests for TF Flaubert and TF XLM Roberta

* Remove unused piece of code in the TF trainer

* bugfix and remove unused code

* Bugfix for TF 2.2

* Apply Style

* Divide TFSequenceClassificationAndMultipleChoiceLoss into their two respective name

* Apply style

* Mirror the PT Trainer in the TF one: fp16, optimizers and tb_writer as class parameter and better dataset handling

* Fix TF optimizations tests and apply style

* Remove useless parameter

* Bugfix and apply style

* Fix TF Trainer prediction

* Now the TF models return the loss such as their PyTorch couterparts

* Apply Style

* Ignore some tests output

* Take into account the SQuAD cls_index, p_mask and is_impossible parameters for the QuestionAnswering task models.

* Fix names for SQuAD data

* Apply Style

* Fix conflicts with 2.11 release

* Fix conflicts with 2.11

* Fix wrongname

* Add better documentation on the new create_optimizer function

* Fix isort

* logging_dir: use same default as PyTorch

Co-authored-by: Julien Chaumond <chaumond@gmail.com>
==

.gitignore
src/transformers/__init__.py
src/transformers/data/processors/squad.py
src/transformers/hf_argparser.py
src/transformers/modeling_tf_albert.py
src/transformers/modeling_tf_auto.py
src/transformers/modeling_tf_bert.py
src/transformers/modeling_tf_camembert.py
src/transformers/modeling_tf_distilbert.py
src/transformers/modeling_tf_electra.py
src/transformers/modeling_tf_flaubert.py
src/transformers/modeling_tf_openai.py
src/transformers/modeling_tf_roberta.py
src/transformers/modeling_tf_t5.py
src/transformers/modeling_tf_transfo_xl.py
src/transformers/modeling_tf_utils.py
src/transformers/modeling_tf_xlm.py
src/transformers/modeling_tf_xlm_roberta.py
src/transformers/modeling_tf_xlnet.py
src/transformers/optimization_tf.py
src/transformers/trainer_tf.py
src/transformers/training_args.py
src/transformers/training_args_tf.py
tests/test_modeling_tf_common.py
tests/test_modeling_tf_flaubert.py
tests/test_modeling_tf_xlm_roberta.py
tests/test_optimization_tf.py
==================
ccd26c286;Th√©ophile Blard;2020-06-05 01:15:07 +0200;Create model card for tblard/allocine (#4775)
https://huggingface.co/tblard/tf-allocine
==

model_cards/tblard/tf-allocine/README.md
==================
2a4b9e09c;Stefan Schweter;2020-06-05 01:13:17 +0200;NER: Add new WNUT‚Äô17 example (#4681)
* ner: add preprocessing script for examples that splits longer sentences

* ner: example shell scripts use local preprocessing now

* ner: add new example section for WNUT‚Äô17 NER task. Remove old English CoNLL-03 results

* ner: satisfy black and isort
==

examples/token-classification/README.md
examples/token-classification/run.sh
examples/token-classification/run_pl.sh
examples/token-classification/scripts/preprocess.py
==================
0e1869cc2;Setu Shah;2020-06-03 22:25:08 -0700;Add drop_last arg for data loader

==

src/transformers/trainer.py
src/transformers/training_args.py
==================
48a05026d;prajjwal1;2020-05-28 00:09:25 +0900;removed deprecared use of Variable api from pplm example

==

examples/text-generation/pplm/run_pplm.py
==================
12d0eb5f3;Sylvain Gugger;2020-06-04 17:57:04 -0400;Don't access pad_token_id if there is no pad_token (#4773)

==

src/transformers/tokenization_utils.py
==================
17a88d319;Manuel Romero;2020-06-04 22:59:56 +0200;Create model card for T5-base fine-tuned for Sentiment Span Extraction (#4737)

==

model_cards/mrm8488/t5-base-finetuned-span-sentiment-extraction/README.md
==================
fb52143cf;Oren Amsalem;2020-06-04 23:59:37 +0300;Create README.md (#4743)

==

model_cards/facebook/bart-large/README.md
==================
5f077a344;Suraj Parmar;2020-06-04 20:58:40 +0000;Model Card for RoBERTa trained on Sanskrit (#4763)
* Model cad for SanBERTa

Model Card for RoBERTa trained on Sanskrit

* Model card for SanBERTa

model card for RoBERTa trained on Sanskrit
==

model_cards/surajp/SanBERTa/README.md
==================
cd4e07a85;Sylvain Gugger;2020-06-04 13:43:14 -0400;Add note about doc generation (#4770)

==

docs/README.md
==================
492b352ab;Jason Phang;2020-06-04 13:41:24 -0400;Remove unnecessary model_type arg in example (#4771)

==

examples/text-classification/README.md
==================
e645b9ab9;Lysandre Debut;2020-06-04 11:44:38 -0400;Codecov setup (#4768)
* Codecov setup

* Understanding codecov
==

codecov.yml
==================
2b8b6c929;Sam Shleifer;2020-06-04 08:13:52 -0400;[cleanup] PretrainedModel.generate: remove unused kwargs (#4761)

==

src/transformers/modeling_utils.py
==================
5bf9afbf3;Funtowicz Morgan;2020-06-04 04:57:01 +0000;Introduce a new tensor type for return_tensors on tokenizer for NumPy (#4585)
* Refactor tensor creation in tokenizers.

* Make sure to convert string to TensorType

* Refactor convert_to_tensors_

* Introduce numpy tensor creation

* Format

* Add unittest for TensorType creation from str

* sorting imports

* Added unittests for numpy tensor conversion.

* Do not use in-place version for squeeze as numpy doesn't provide such feature.

* Added extra parameter prepend_batch_axis: bool on prepare_for_model.

* Ensure test_np_encode_plus_sent_to_model is not executed if encoder/decoder model.

* style.

* numpy tests require_torch for now while flax not merged.

* Hopefully will make flake8 happy.

* One more time :notes:
==

src/transformers/__init__.py
src/transformers/tokenization_utils.py
tests/test_tokenization_common.py
tests/test_tokenization_utils.py
==================
efae15492;Funtowicz Morgan;2020-06-03 20:48:28 +0000;never_split on slow tokenizers should not split (#4723)
* Ensure tokens in never_split are not splitted when using basic tokenizer before wordpiece.

* never_split only use membership attempt to use a set() which is 10x faster for this operation.

* Use union to concatenate two sets.

* Updated docstring for never_split parameter.

* Avoid set.union() if never_split is None

* Added comments.

* Correct docstring format.
==

src/transformers/tokenization_bert.py
==================
2e4de7623;Lysandre Debut;2020-06-03 16:30:59 -0400;Update encode documentation (#4751)

==

src/transformers/tokenization_utils.py
==================
ed4df8557;Patrick von Platen;2020-06-03 18:53:23 +0200;fix beam search bug in tf as well (#4745)

==

src/transformers/modeling_tf_utils.py
src/transformers/modeling_utils.py
==================
1b5820a56;Sylvain Gugger;2020-06-03 09:36:26 -0400;Unify label args (#4722)
* Deprecate masked_lm_labels argument

* Apply to all models

* Better error message
==

src/transformers/modeling_albert.py
src/transformers/modeling_bart.py
src/transformers/modeling_bert.py
src/transformers/modeling_ctrl.py
src/transformers/modeling_distilbert.py
src/transformers/modeling_electra.py
src/transformers/modeling_encoder_decoder.py
src/transformers/modeling_gpt2.py
src/transformers/modeling_longformer.py
src/transformers/modeling_openai.py
src/transformers/modeling_reformer.py
src/transformers/modeling_roberta.py
src/transformers/modeling_t5.py
tests/test_modeling_encoder_decoder.py
==================
3e5928c57;Abhishek Kumar Mishra;2020-06-03 17:07:26 +0800;Adding notebooks for Fine Tuning [Community Notebook] (#4732)
* Added links to more community notebooks

Added links to 3 more community notebooks from the git repo: https://github.com/abhimishra91/transformers-tutorials
Different Transformers models are fine tuned on Dataset using PyTorch

* Update README.md

* Update README.md

* Update README.md

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
==

notebooks/README.md
==================
99207bd11;Julien Chaumond;2020-06-03 03:51:31 -0400;Pipelines: miscellanea of QoL improvements and small features... (#4632)
* [hf_api] Attach all unknown attributes for future-proof compatibility

* [Pipeline] NerPipeline is really a TokenClassificationPipeline

* modelcard.py: I don't think we need to force the download

* Remove config, tokenizer from SUPPORTED_TASKS as we're moving to one model = one weight + one tokenizer

* FillMaskPipeline: also output token in string form

* TextClassificationPipeline: option to return all scores, not just the argmax

* Update docs/source/main_classes/pipelines.rst
==

docs/source/main_classes/pipelines.rst
src/transformers/hf_api.py
src/transformers/modelcard.py
src/transformers/pipelines.py
==================
8ed47aa10;David Mezzetti;2020-06-03 03:40:14 -0400;bert-small-cord19 model cards (#4730)
* Create README.md

* Create README.md

* Create README.md
==

model_cards/NeuML/bert-small-cord19-squad2/README.md
model_cards/NeuML/bert-small-cord19/README.md
model_cards/NeuML/bert-small-cord19qa/README.md
==================
9ca485734;Patrick von Platen;2020-06-02 23:08:39 +0200;[Reformer] Improved memory if input is shorter than chunk length (#4720)
* improve handling of short inputs for reformer

* correct typo in assert statement

* fix other tests
==

src/transformers/modeling_reformer.py
src/transformers/modeling_utils.py
tests/test_modeling_reformer.py
==================
b231a413f;Jin Young Sohn;2020-06-02 10:40:14 -0700;Add cache_dir to save features in GLUE + Differentiate match/mismatch for MNLI metrics (#4621)
* Glue task cleaup

* Enable writing cache to cache_dir in case dataset lives in readOnly
filesystem.
* Differentiate match vs mismatch for MNLI metrics.

* Style

* Fix pytype

* Fix type

* Use cache_dir in mnli mismatch eval dataset

* Small Tweaks

Co-authored-by: Julien Chaumond <chaumond@gmail.com>
==

examples/text-classification/run_glue.py
src/transformers/data/datasets/glue.py
src/transformers/data/metrics/__init__.py
src/transformers/trainer.py
==================
70f742343;Sam Shleifer;2020-06-02 12:59:00 -0400;TFRobertaModelIntegrationTest requires tf (#4726)

==

tests/test_modeling_tf_roberta.py
==================
d976ef262;Lysandre;2020-06-02 10:27:15 -0400;Repin versions

==

setup.py
==================
b42586ea5;Julien Chaumond;2020-06-02 10:21:09 -0400;Fix CI after killing archive maps (#4724)
* üêõ Fix model ids for BART and Flaubert
==

docs/source/model_doc/bart.rst
examples/summarization/bart/evaluate_cnn.py
examples/summarization/bart/test_bart_examples.py
src/transformers/configuration_ctrl.py
src/transformers/convert_bart_original_pytorch_checkpoint_to_pytorch.py
src/transformers/pipelines.py
src/transformers/tokenization_bart.py
src/transformers/tokenization_flaubert.py
tests/test_modeling_bart.py
==================
b43c78e5d;Lysandre;2020-06-02 09:49:00 -0400;Release: v2.11.0

==

docs/source/conf.py
setup.py
src/transformers/__init__.py
==================
d4c2cb402;Julien Chaumond;2020-06-02 09:39:33 -0400;Kill model archive maps (#4636)
* Kill model archive maps

* Fixup

* Also kill model_archive_map for MaskedBertPreTrainedModel

* Unhook config_archive_map

* Tokenizers: align with model id changes

* make style && make quality

* Fix CI
==

docs/source/pretrained_models.rst
examples/adversarial/test_hans.py
examples/contrib/mm-imdb/run_mmimdb.py
examples/contrib/run_swag.py
examples/distillation/run_squad_w_distillation.py
examples/movement-pruning/emmental/configuration_bert_masked.py
examples/movement-pruning/emmental/modeling_bert_masked.py
examples/movement-pruning/masked_run_glue.py
examples/movement-pruning/masked_run_squad.py
examples/question-answering/run_squad.py
examples/summarization/bertabs/configuration_bertabs.py
examples/summarization/bertabs/modeling_bertabs.py
examples/text-classification/README.md
examples/text-classification/run_xnli.py
examples/token-classification/run_ner.py
src/transformers/__init__.py
src/transformers/configuration_albert.py
src/transformers/configuration_auto.py
src/transformers/configuration_bart.py
src/transformers/configuration_bert.py
src/transformers/configuration_camembert.py
src/transformers/configuration_ctrl.py
src/transformers/configuration_distilbert.py
src/transformers/configuration_electra.py
src/transformers/configuration_flaubert.py
src/transformers/configuration_gpt2.py
src/transformers/configuration_longformer.py
src/transformers/configuration_marian.py
src/transformers/configuration_openai.py
src/transformers/configuration_reformer.py
src/transformers/configuration_roberta.py
src/transformers/configuration_t5.py
src/transformers/configuration_transfo_xl.py
src/transformers/configuration_utils.py
src/transformers/configuration_xlm.py
src/transformers/configuration_xlm_roberta.py
src/transformers/configuration_xlnet.py
src/transformers/convert_pytorch_checkpoint_to_tf2.py
src/transformers/modeling_albert.py
src/transformers/modeling_auto.py
src/transformers/modeling_bart.py
src/transformers/modeling_bert.py
src/transformers/modeling_camembert.py
src/transformers/modeling_ctrl.py
src/transformers/modeling_distilbert.py
src/transformers/modeling_electra.py
src/transformers/modeling_flaubert.py
src/transformers/modeling_gpt2.py
src/transformers/modeling_longformer.py
src/transformers/modeling_marian.py
src/transformers/modeling_openai.py
src/transformers/modeling_reformer.py
src/transformers/modeling_roberta.py
src/transformers/modeling_t5.py
src/transformers/modeling_tf_albert.py
src/transformers/modeling_tf_auto.py
src/transformers/modeling_tf_bert.py
src/transformers/modeling_tf_camembert.py
src/transformers/modeling_tf_ctrl.py
src/transformers/modeling_tf_distilbert.py
src/transformers/modeling_tf_electra.py
src/transformers/modeling_tf_flaubert.py
src/transformers/modeling_tf_gpt2.py
src/transformers/modeling_tf_openai.py
src/transformers/modeling_tf_roberta.py
src/transformers/modeling_tf_t5.py
src/transformers/modeling_tf_transfo_xl.py
src/transformers/modeling_tf_utils.py
src/transformers/modeling_tf_xlm.py
src/transformers/modeling_tf_xlm_roberta.py
src/transformers/modeling_tf_xlnet.py
src/transformers/modeling_transfo_xl.py
src/transformers/modeling_utils.py
src/transformers/modeling_xlm.py
src/transformers/modeling_xlm_roberta.py
src/transformers/modeling_xlnet.py
src/transformers/tokenization_auto.py
src/transformers/tokenization_bert.py
src/transformers/tokenization_bert_japanese.py
src/transformers/tokenization_utils.py
templates/adding_a_new_example_script/run_xxx.py
templates/adding_a_new_model/configuration_xxx.py
templates/adding_a_new_model/modeling_tf_xxx.py
templates/adding_a_new_model/modeling_xxx.py
templates/adding_a_new_model/tests/test_modeling_xxx.py
tests/test_modeling_albert.py
tests/test_modeling_auto.py
tests/test_modeling_bart.py
tests/test_modeling_bert.py
tests/test_modeling_common.py
tests/test_modeling_ctrl.py
tests/test_modeling_distilbert.py
tests/test_modeling_electra.py
tests/test_modeling_flaubert.py
tests/test_modeling_gpt2.py
tests/test_modeling_openai.py
tests/test_modeling_reformer.py
tests/test_modeling_roberta.py
tests/test_modeling_t5.py
tests/test_modeling_tf_albert.py
tests/test_modeling_tf_auto.py
tests/test_modeling_tf_bert.py
tests/test_modeling_tf_ctrl.py
tests/test_modeling_tf_distilbert.py
tests/test_modeling_tf_electra.py
tests/test_modeling_tf_gpt2.py
tests/test_modeling_tf_openai_gpt.py
tests/test_modeling_tf_roberta.py
tests/test_modeling_tf_transfo_xl.py
tests/test_modeling_tf_xlm.py
tests/test_modeling_tf_xlnet.py
tests/test_modeling_transfo_xl.py
tests/test_modeling_xlm.py
tests/test_modeling_xlnet.py
tests/test_tokenization_bert_japanese.py
==================
47a551d17;Patrick von Platen;2020-06-02 11:03:46 +0200;[pipeline] Tokenizer should not add special tokens for text generation (#4686)
* allow to not add special tokens

* remove print
==

src/transformers/pipelines.py
==================
f6d5046af;Funtowicz Morgan;2020-06-02 09:02:27 +0000;Override get_vocab for fast tokenizer. (#4717)

==

src/transformers/tokenization_utils.py
==================
88762a2f8;Lysandre Debut;2020-06-02 04:29:28 -0400;Specify PyTorch versions for examples (#4710)

==

README.md
examples/README.md
==================
d3ef14f93;Lorenzo Ampil;2020-06-02 15:59:53 +0800;Add community notebook for sentiment span extraction (#4700)

==

notebooks/README.md
==================
767793631;Sylvain Gugger;2020-06-01 15:22:51 -0400;Make docstring match args (#4711)

==

src/transformers/modeling_bart.py
src/transformers/modeling_gpt2.py
src/transformers/modeling_openai.py
src/transformers/modeling_transfo_xl.py
src/transformers/modeling_xlm.py
==================
6449c494d;Lysandre;2020-06-01 12:57:40 -0400;close #4685

==

src/transformers/modeling_auto.py
src/transformers/modeling_tf_auto.py
==================
ec8717d5d;Julien Chaumond;2020-06-01 16:54:55 +0200;[config] Ensure that id2label always takes precedence over num_labels

==

src/transformers/configuration_utils.py
==================
751a1e089;Julien Chaumond;2020-06-01 16:25:43 +0200;[config] Ensure that id2label always takes precedence over num_labels
Fixes bug reported in https://github.com/huggingface/transformers/issues/4669

See #3967 for context

==

src/transformers/configuration_utils.py
==================
ec62b7d95;Rens;2020-06-01 16:12:48 +0200;Fix onnx export input names order (#4641)
* pass on tokenizer to pipeline

* order input names when convert to onnx

* update style

* remove unused imports

* make ordered inputs list needs to be mutable

* add test custom bert model

* remove unused imports
==

src/transformers/convert_graph_to_onnx.py
tests/test_onnx.py
==================
bf760c80b;Victor SANH;2020-05-29 01:10:56 -0400;finish README

==

examples/movement-pruning/README.md
==================
9d7d9b3ae;Victor SANH;2020-05-29 01:10:41 -0400;weird import

==

examples/movement-pruning/emmental/modeling_bert_masked.py
==================
2a3c88a65;Victor SANH;2020-05-28 10:42:00 -0400;Update examples/movement-pruning/README.md
Co-authored-by: Julien Chaumond <chaumond@gmail.com>
==

examples/movement-pruning/README.md
==================
4ac462bfb;Victor SANH;2020-05-28 10:41:53 -0400;Update examples/movement-pruning/README.md
Co-authored-by: Julien Chaumond <chaumond@gmail.com>
==

examples/movement-pruning/README.md
==================
35fa0bbca;Victor SANH;2020-05-28 00:47:05 -0400;clarify README

==

examples/movement-pruning/README.md
==================
cc746a502;Victor SANH;2020-05-28 00:37:09 -0400;flake8 compliance

==

examples/movement-pruning/bertarize.py
examples/movement-pruning/counts_parameters.py
examples/movement-pruning/emmental/__init__.py
examples/movement-pruning/emmental/modules/__init__.py
==================
b11386e15;Victor SANH;2020-05-28 00:26:56 -0400;less prints in saving prunebert

==

examples/movement-pruning/Saving_PruneBERT.ipynb
==================
8b5d4003a;Victor SANH;2020-05-28 00:26:47 -0400;complete README

==

examples/movement-pruning/README.md
==================
5c8e5b370;Victor SANH;2020-05-28 00:26:39 -0400;commplying with isort

==

examples/movement-pruning/bertarize.py
examples/movement-pruning/counts_parameters.py
examples/movement-pruning/emmental/__init__.py
examples/movement-pruning/emmental/configuration_bert_masked.py
examples/movement-pruning/emmental/modeling_bert_masked.py
examples/movement-pruning/emmental/modules/__init__.py
examples/movement-pruning/emmental/modules/masked_nn.py
examples/movement-pruning/masked_run_glue.py
examples/movement-pruning/masked_run_squad.py
==================
db2a3b2e0;Victor SANH;2020-05-27 20:44:45 -0400;space

==

examples/movement-pruning/bertarize.py
examples/movement-pruning/counts_parameters.py
==================
5f8f2d849;Victor SANH;2020-05-27 18:25:17 -0400;add floppy bert model notebok

==

examples/movement-pruning/Saving_PruneBERT.ipynb
==================
b41948f5c;Victor SANH;2020-05-27 18:24:49 -0400;add requirements

==

examples/movement-pruning/requirements.txt
==================
fb8f4277b;Victor SANH;2020-05-27 18:24:39 -0400;add scripts

==

examples/movement-pruning/bertarize.py
examples/movement-pruning/counts_parameters.py
==================
d489a6d3d;Victor SANH;2020-05-27 18:24:32 -0400;add masked_run_*

==

examples/movement-pruning/masked_run_glue.py
examples/movement-pruning/masked_run_squad.py
==================
e4c07faf0;Victor SANH;2020-05-27 18:24:23 -0400;add sparsity modules

==

examples/movement-pruning/emmental/__init__.py
examples/movement-pruning/emmental/configuration_bert_masked.py
examples/movement-pruning/emmental/modeling_bert_masked.py
examples/movement-pruning/emmental/modules/__init__.py
examples/movement-pruning/emmental/modules/binarizer.py
examples/movement-pruning/emmental/modules/masked_nn.py
==================
667003e44;Mehrdad Farahani;2020-06-01 14:29:09 +0200;Create README.md (#4665)

==

model_cards/HooshvareLab/bert-base-parsbert-ner-uncased/README.md
==================
ed23f5909;Mehrdad Farahani;2020-06-01 14:28:43 +0200;HooshvareLab readme parsbert-armananer (#4666)
Readme for HooshvareLab/bert-base-parsbert-armananer-uncased
==

model_cards/HooshvareLab/bert-base-parsbert-armanner-uncased/README.md
==================
3750b9b0b;Mehrdad Farahani;2020-06-01 14:28:25 +0200;HooshvareLab readme parsbert-peymaner (#4667)
Readme for HooshvareLab/bert-base-parsbert-peymaner-uncased
==

model_cards/HooshvareLab/bert-base-parsbert-peymaner-uncased/README.md
==================
036c2c6b0;Mehrdad Farahani;2020-06-01 14:27:00 +0200;Update HooshvareLab/bert-base-parsbert-uncased (#4687)
mBERT results added regarding NER datasets!
==

model_cards/HooshvareLab/bert-base-parsbert-uncased/README.md
==================
74872c19d;Manuel Romero;2020-06-01 11:45:54 +0200;Create README.md (#4684)

==

model_cards/mrm8488/longformer-base-4096-finetuned-squadv2/README.md
==================
0866669e7;Patrick von Platen;2020-05-30 01:25:19 +0200;[EncoderDecoder] Fix initialization and save/load bug (#4680)
* fix bug

* add more tests
==

src/transformers/modeling_encoder_decoder.py
tests/test_modeling_encoder_decoder.py
==================
6f82aea66;Patrick von Platen;2020-05-29 19:38:56 +0200;Include `nlp` notebook for model evaluation (#4676)

==

notebooks/README.md
==================
33b7532e6;Wei Fang;2020-05-30 00:13:30 +0800;Fix longformer attention mask type casting when using apex (#4574)
* Fix longformer attention mask casting when using apex

* remove extra type casting
==

src/transformers/modeling_longformer.py
==================
56ee2560b;Patrick von Platen;2020-05-29 17:58:42 +0200;[Longformer] Better handling of global attention mask vs local attention mask (#4672)
* better api

* improve automatic setting of global attention mask

* fix longformer bug

* fix global attention mask in test

* fix global attn mask flatten

* fix slow tests

* update docstring

* update docs and make more robust

* improve attention mask
==

docs/source/model_doc/longformer.rst
src/transformers/modeling_longformer.py
tests/test_modeling_longformer.py
==================
e2230ba77;Simon B√∂hm;2020-05-29 17:55:55 +0200;Fix BERT example code for NSP and Multiple Choice (#3953)
Change the example code to use encode_plus since the token_type_id
wasn't being correctly set.
==

src/transformers/modeling_bert.py
src/transformers/modeling_tf_bert.py
==================
3a5d1ea2a;Zhangyx;2020-05-29 23:12:24 +0800;Fix two bugs: 1. Index of test data of SST-2.  2. Label index of MNLI data. (#4546)

==

src/transformers/data/datasets/glue.py
src/transformers/data/processors/glue.py
==================
9c1725644;Patrick von Platen;2020-05-29 13:46:08 +0200;[Longformer] Multiple choice for longformer (#4645)
* add multiple choice for longformer

* add models to docs

* adapt docstring

* add test to longformer

* add longformer for mc in init and modeling auto

* fix tests
==

docs/source/model_doc/albert.rst
docs/source/model_doc/longformer.rst
docs/source/model_doc/roberta.rst
src/transformers/__init__.py
src/transformers/modeling_auto.py
src/transformers/modeling_bert.py
src/transformers/modeling_longformer.py
src/transformers/modeling_roberta.py
src/transformers/modeling_tf_albert.py
src/transformers/modeling_tf_bert.py
src/transformers/modeling_xlnet.py
tests/test_modeling_longformer.py
==================
91487cbb8;Iz Beltagy;2020-05-29 04:12:35 -0700;[Longformer] fix model name in examples (#4653)
* fix longformer model names in examples

* a better name for the notebook
==

notebooks/README.md
src/transformers/modeling_longformer.py
==================
b5015a2a0;flozi00;2020-05-28 22:44:43 +0200;gpt2 typo (#4629)
* gpt2 typo

* Add files via upload
==

src/transformers/modeling_albert.py
src/transformers/modeling_ctrl.py
src/transformers/modeling_flaubert.py
src/transformers/modeling_gpt2.py
src/transformers/modeling_openai.py
src/transformers/modeling_reformer.py
src/transformers/modeling_tf_albert.py
src/transformers/modeling_tf_ctrl.py
src/transformers/modeling_tf_flaubert.py
src/transformers/modeling_tf_gpt2.py
src/transformers/modeling_tf_openai.py
src/transformers/modeling_tf_transfo_xl.py
src/transformers/modeling_tf_xlm.py
src/transformers/modeling_tf_xlnet.py
src/transformers/modeling_transfo_xl.py
src/transformers/modeling_xlm.py
src/transformers/modeling_xlnet.py
==================
fe5cb1a1c;Iz Beltagy;2020-05-28 13:35:15 -0700;Adding community notebook (#4642)
Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
==

notebooks/README.md
==================
aecaaf73a;Suraj Patil;2020-05-29 01:57:22 +0530;[Community notebooks] add longformer-for-qa notebook (#4652)

==

notebooks/README.md
==================
5e737018e;Anthony MOI;2020-05-28 10:54:45 -0400;Fix add_special_tokens on fast tokenizers (#4531)

==

src/transformers/tokenization_utils.py
tests/test_tokenization_fast.py
==================
e444648a3;Suraj Patil;2020-05-28 16:18:18 +0530;LongformerForTokenClassification (#4638)

==

src/transformers/__init__.py
src/transformers/modeling_auto.py
src/transformers/modeling_longformer.py
tests/test_modeling_longformer.py
==================
3cc2c2a15;Lavanya Shukla;2020-05-28 14:48:16 +0530;add 2 colab notebooks (#4505)
Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
==

notebooks/README.md
==================
ef03ae874;Iz Beltagy;2020-05-28 02:11:05 -0700;[Longformer] more models + model cards (#4628)
* adding freeze roberta models

* model cards

* lint
==

model_cards/allenai/longformer-base-4096-extra.pos.embd.only/README.md
model_cards/allenai/longformer-base-4096/README.md
src/transformers/configuration_longformer.py
src/transformers/modeling_longformer.py
src/transformers/tokenization_longformer.py
==================
96f57c9cc;Patrick von Platen;2020-05-27 23:22:16 +0200;[Benchmark] Memory benchmark utils (#4198)
* improve memory benchmarking

* correct typo

* fix current memory

* check torch memory allocated

* better pytorch function

* add total cached gpu memory

* add total gpu required

* improve torch gpu usage

* update memory usage

* finalize memory tracing

* save intermediate benchmark class

* fix conflict

* improve benchmark

* improve benchmark

* finalize

* make style

* improve benchmarking

* correct typo

* make train function more flexible

* fix csv save

* better repr of bytes

* better print

* fix __repr__ bug

* finish plot script

* rename plot file

* delete csv and small improvements

* fix in plot

* fix in plot

* correct usage of timeit

* remove redundant line

* remove redundant line

* fix bug

* add hf parser tests

* add versioning and platform info

* make style

* add gpu information

* ensure backward compatibility

* finish adding all tests

* Update src/transformers/benchmark/benchmark_args.py

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* Update src/transformers/benchmark/benchmark_args_utils.py

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>

* delete csv files

* fix isort ordering

* add out of memory handling

* add better train memory handling

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
==

examples/benchmarking/plot_csv_file.py
examples/benchmarking/run_benchmark.py
examples/benchmarks.py
examples/requirements.txt
src/transformers/__init__.py
src/transformers/benchmark/__init__.py
src/transformers/benchmark/benchmark.py
src/transformers/benchmark/benchmark_args.py
src/transformers/benchmark/benchmark_args_utils.py
src/transformers/benchmark/benchmark_utils.py
src/transformers/file_utils.py
src/transformers/hf_argparser.py
tests/test_benchmark.py
tests/test_hf_argparser.py
==================
ec4cdfdd0;Suraj Patil;2020-05-28 02:00:00 +0530;LongformerForSequenceClassification (#4580)
* LongformerForSequenceClassification

* better naming x=>hidden_states, fix typo in doc

* Update src/transformers/modeling_longformer.py

* Update src/transformers/modeling_longformer.py

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
==

src/transformers/__init__.py
src/transformers/modeling_auto.py
src/transformers/modeling_longformer.py
tests/test_modeling_longformer.py
==================
4402879ee;Suraj Patil;2020-05-27 22:18:03 +0530;[Model Card] model card for longformer-base-4096-finetuned-squadv1 (#4625)

==

model_cards/valhalla/longformer-base-4096-finetuned-squadv1/README.md
==================
6a1768802;Lysandre Debut;2020-05-27 11:36:55 -0400;per_device instead of per_gpu/error thrown when argument unknown (#4618)
* per_device instead of per_gpu/error thrown when argument unknown

* [docs] Restore examples.md symlink

* Correct absolute links so that symlink to the doc works correctly

* Update src/transformers/hf_argparser.py

Co-authored-by: Julien Chaumond <chaumond@gmail.com>

* Warning + reorder

* Docs

* Style

* not for squad

Co-authored-by: Julien Chaumond <chaumond@gmail.com>
==

README.md
docs/source/examples.md
examples/README.md
examples/multiple-choice/README.md
examples/test_examples.py
examples/text-classification/README.md
examples/token-classification/README.md
src/transformers/hf_argparser.py
src/transformers/trainer.py
src/transformers/training_args.py
==================
1381b6d01;Mehrdad Farahani;2020-05-27 17:25:36 +0200;README for HooshvareLab (#4610)
HooshvareLab/bert-base-parsbert-uncased
==

model_cards/HooshvareLab/bert-base-parsbert-uncased/README.md
==================
5acb4edf2;Patrick von Platen;2020-05-27 17:19:11 +0200;Update version command when contributing (#4614)

==

CONTRIBUTING.md
==================
842588c12;Darek K≈Çeczek;2020-05-27 15:50:04 +0200;uncased readme (#4608)
Co-authored-by: kldarek <darekmail>
==

model_cards/dkleczek/bert-base-polish-uncased-v1/README.md
==================
ac1a61217;Darek K≈Çeczek;2020-05-27 15:36:20 +0200;Create README.md (#4607)
Model card for cased model
==

model_cards/dkleczek/bert-base-polish-cased-v1/README.md
==================
07797c4da;Sam Shleifer;2020-05-27 09:10:26 -0400;[testing] LanguageModelGenerationTests require_tf or require_torch (#4616)

==

tests/test_modeling_ctrl.py
tests/test_modeling_gpt2.py
tests/test_modeling_openai.py
tests/test_modeling_tf_ctrl.py
tests/test_modeling_tf_gpt2.py
tests/test_modeling_tf_openai_gpt.py
tests/test_modeling_tf_transfo_xl.py
tests/test_modeling_tf_xlm.py
tests/test_modeling_tf_xlnet.py
tests/test_modeling_xlm.py
tests/test_modeling_xlnet.py
==================
a9aa7456a;Hao Tan;2020-05-26 21:13:07 -0400;Add back --do_lower_case to uncased models (#4245)
The option `--do_lower_case` is currently required by the uncased models (i.e., bert-base-uncased, bert-large-uncased).

Results:
BERT-BASE without --do_lower_case:  'exact': 73.83, 'f1': 82.22
BERT-BASE with --do_lower_case:  'exact': 81.02, 'f1': 88.34
==

examples/question-answering/README.md
==================
a801c7fd7;Bayartsogt Yadamsuren;2020-05-27 04:54:42 +0800;Creating a readme for ALBERT in Mongolian (#4603)
Here I am uploading Mongolian masked language model (ALBERT) on your platform.
https://en.wikipedia.org/wiki/Mongolia
==

model_cards/bayartsogt/albert-mongolian/README.md
==================
6458c0e26;Wissam Antoun;2020-05-26 23:52:43 +0300;updated model cards for both models at aubmindlab (#4604)
* updated aubmindlab/bert-base-arabert/ Model card

* updated aubmindlab/bert-base-arabertv01 model card
==

model_cards/aubmindlab/bert-base-arabert/README.md
model_cards/aubmindlab/bert-base-arabertv01/README.md
==================
ea4e7a53f;Oleksandr Bushkovskyi;2020-05-26 23:51:40 +0300;Improve model card for Tereveni-AI/gpt2-124M-uk-fiction (#4582)
Add language metadata, training and evaluation corpora details.
Add example output. Fix inconsistent use of quotes.
==

model_cards/Tereveni-AI/gpt2-124M-uk-fiction/README.md
==================
937930dca;Manuel Romero;2020-05-26 22:50:08 +0200;Create README.md (#4591)

==

model_cards/mrm8488/t5-base-finetuned-squadv2/README.md
==================
bac1cc4dc;Manuel Romero;2020-05-26 22:38:39 +0200;Remove MD emojis (#4602)

==

model_cards/mrm8488/bert-italian-finedtuned-squadv1-it-alfa/README.md
==================
003c47712;Patrick von Platen;2020-05-26 19:43:58 +0200;[GPT2, CTRL] Allow input of input_ids and past of variable length (#4581)
* revert convenience  method

* clean docs a bit
==

src/transformers/modeling_ctrl.py
src/transformers/modeling_gpt2.py
src/transformers/modeling_tf_ctrl.py
src/transformers/modeling_tf_gpt2.py
==================
5ddd8d653;ohmeow;2020-05-26 07:43:41 -0700;Add BART fine-tuning summarization community notebook (#4539)
* adding BART summarization how-to community notebook

* Update notebooks/README.md

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
==

notebooks/README.md
==================
8cc6807e8;Bram Vanroy;2020-05-26 16:00:51 +0200;Make transformers-cli cross-platform (#4131)
* make transformers-cli cross-platform

Using "scripts" is a useful option in setup.py particularly when you want to get access to non-python scripts. However, in this case we want to have an entry point into some of our own Python scripts. To do this in a concise, cross-platfom way, we can use entry_points.console_scripts. This change is necessary to provide the CLI on different platforms, which "scripts" does not ensure. Usage remains the same, but the "transformers-cli" script has to be moved (be part of the library) and renamed (underscore + extension)

* make style & quality
==

setup.py
src/transformers/commands/transformers_cli.py
==================
c589eae2b;Patrick von Platen;2020-05-26 14:58:47 +0200;[Longformer For Question Answering] Conversion script, doc, small fixes (#4593)
* add new longformer for question answering model

* add new config as well

* fix links

* fix links part 2
==

docs/source/model_doc/longformer.rst
src/transformers/configuration_longformer.py
src/transformers/convert_longformer_original_pytorch_lightning_to_pytorch.py
src/transformers/modeling_longformer.py
src/transformers/tokenization_longformer.py
==================
a163c9ca5;ZhuBaohe;2020-05-26 20:57:24 +0800;[T5] Fix Cross Attention position bias (#4499)
* fix

* fix1
==

src/transformers/modeling_t5.py
src/transformers/modeling_tf_t5.py
==================
1d6902898;ZhuBaohe;2020-05-26 20:51:28 +0800;fix (#4410)

==

src/transformers/modeling_tf_xlnet.py
==================
b86e42e0a;Sam Shleifer;2020-05-25 19:20:50 -0400;[ci] fix 3 remaining slow GPU failures (#4584)

==

src/transformers/configuration_distilbert.py
src/transformers/modeling_encoder_decoder.py
tests/test_modeling_bart.py
tests/test_modeling_tf_electra.py
==================
365d452d4;Julien Chaumond;2020-05-25 17:28:02 -0400;[ci] Slow GPU tests run daily (#4465)

==

.github/workflows/self-scheduled.yml
==================
3e3e55212;Patrick von Platen;2020-05-25 22:04:45 +0200;[Reformer] fix reformer num buckets (#4564)
* fix reformer num buckets

* fix

* adapt docs

* set num buckets in config
==

docs/source/model_doc/reformer.rst
src/transformers/configuration_reformer.py
src/transformers/modeling_reformer.py
==================
3dea40b85;Elman Mansimov;2020-05-25 16:04:30 -0400;fixing tokenization of extra_id symbols in T5Tokenizer. Related to issue 4021 (#4353)

==

src/transformers/tokenization_utils.py
==================
513973362;Suraj Patil;2020-05-26 01:33:55 +0530;LongformerTokenizerFast (#4547)

==

src/transformers/__init__.py
src/transformers/tokenization_longformer.py
==================
c9c385c52;Oliver Guhr;2020-05-25 21:29:50 +0200;Updated the link to the paper (#4570)
I looks like the conference has changed the link to the paper.
==

model_cards/oliverguhr/german-sentiment-bert/README.md
==================
adab7f833;Sho Arora;2020-05-25 12:29:33 -0700;Add nn.Module as superclass (#4533)

==

src/transformers/modeling_mmbt.py
==================
8f7c1c767;Manuel Romero;2020-05-25 21:28:30 +0200;Create model card (#4578)

==

model_cards/mrm8488/bert-italian-finedtuned-squadv1-it-alfa/README.md
==================
4c6b21805;Ali Safaya;2020-05-25 22:12:23 +0300;Update README.md (#4556)

==

model_cards/asafaya/bert-base-arabic/README.md
==================
50d1ce411;Antonis Maronikolakis;2020-05-25 20:50:45 +0200;add DistilBERT to supported models (#4558)

==

examples/language-modeling/README.md
==================
03d8527de;Suraj Patil;2020-05-25 22:13:36 +0530;Longformer for question answering (#4500)
* added LongformerForQuestionAnswering

* add LongformerForQuestionAnswering

* fix import for LongformerForMaskedLM

* add LongformerForQuestionAnswering

* hardcoded sep_token_id

* compute attention_mask if not provided

* combine global_attention_mask with attention_mask when provided

* update example in  docstring

* add assert error messages, better attention combine

* add test for longformerForQuestionAnswering

* typo

* cast gloabl_attention_mask to long

* make style

* Update src/transformers/configuration_longformer.py

* Update src/transformers/configuration_longformer.py

* fix the code quality

* Merge branch 'longformer-for-question-answering' of https://github.com/patil-suraj/transformers into longformer-for-question-answering

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
==

src/transformers/__init__.py
src/transformers/configuration_longformer.py
src/transformers/modeling_auto.py
src/transformers/modeling_longformer.py
tests/test_modeling_longformer.py
==================
a34a9896a;Bharat Raghunathan;2020-05-23 13:40:59 +0000;DOC: Fix typos in modeling_auto (#4534)

==

src/transformers/modeling_auto.py
==================
e19b97815;Bijay Gurung;2020-05-23 04:55:22 +0545;Add Type Hints to modeling_utils.py Closes #3911 (#3948)
* Add Type Hints to modeling_utils.py Closes #3911

Add Type Hints to methods in `modeling_utils.py`

Note: The coverage isn't 100%. Mostly skipped internal methods.

* Reformat according to `black` and `isort`

* Use typing.Iterable instead of Sequence

* Parameterize Iterable by its generic type

* Use typing.Optional when None is the default value

* Adhere to style guideline

* Update src/transformers/modeling_utils.py

* Update src/transformers/modeling_utils.py

Co-authored-by: Julien Chaumond <chaumond@gmail.com>
==

src/transformers/modeling_utils.py
==================
996f393a8;Funtowicz Morgan;2020-05-22 22:08:30 +0000;Warn the user about max_len being on the path to be deprecated. (#4528)
* Warn the user about max_len being on the path to be deprecated.

* Ensure better backward compatibility when max_len is provided to a tokenizer.

* Make sure to override the parameter and not the actual instance value.

* Format & quality
==

src/transformers/tokenization_utils.py
==================
0f6969b7e;Patrick von Platen;2020-05-22 23:51:36 +0200;Better github link for Reformer Colab Notebook

==

notebooks/README.md
==================
ab44630db;Sam Shleifer;2020-05-22 17:49:45 -0400;[Summarization Pipeline]: Fix default tokenizer (#4506)
* Fix pipelines defaults bug

* one liner

* style
==

src/transformers/pipelines.py
==================
2c1ebb8b5;Julien Chaumond;2020-05-22 17:27:47 -0400;Re-apply #4446 + add packaging dependency
As discussed w/ @lysandrejik

packaging is maintained by PyPA (the Python Packaging Authority), and should be lightweight and stable

==

setup.py
src/transformers/trainer.py
==================
e6aeb0d3e;Lysandre;2020-05-22 17:20:03 -0400;Style

==

src/transformers/trainer.py
==================
95a26fcf2;Alexander Measure;2020-05-22 15:17:09 -0400;link to paper was broken (#4526)
changed from https://https://arxiv.org/abs/2001.04451.pdf to https://arxiv.org/abs/2001.04451.pdf
==

docs/source/model_doc/reformer.rst
==================
89d795f18;HUSEIN ZOLKEPLI;2020-05-23 03:04:06 +0800;Added huseinzol05/t5-small-bahasa-cased README.md (#4522)

==

model_cards/huseinzol05/t5-small-bahasa-cased/README.md
==================
35df91148;Anthony MOI;2020-05-22 12:45:10 -0400;Fix convert_token_type_ids_from_sequences for fast tokenizers (#4503)

==

src/transformers/tokenization_bert.py
src/transformers/tokenization_roberta.py
tests/test_tokenization_fast.py
==================
f7677e162;Julien Chaumond;2020-05-22 12:20:54 -0400;[model_cards] bart-large-cnn
cc @sshleifer
==

model_cards/facebook/bart-large-cnn/README.md
==================
12e6afe90;Patrick von Platen;2020-05-22 17:03:34 +0200;Add Reformer colab to community noteboos

==

notebooks/README.md
==================
ef22ba483;Lysandre;2020-05-22 11:03:07 -0400;Re-pin versions

==

setup.py
==================
10d72390c;Lysandre;2020-05-22 10:49:45 -0400;Revert #4446 Since it introduces a new dependency

==

src/transformers/trainer.py
==================
e0db6bbd6;Lysandre;2020-05-22 10:37:44 -0400;Release: v2.10.0

==

docs/source/conf.py
setup.py
src/transformers/__init__.py
==================
bd6e30183;Frankie Liuzzi;2020-05-22 09:48:21 -0400;added functionality for electra classification head (#4257)
* added functionality for electra classification head

* unneeded dropout

* Test ELECTRA for sequence classification

* Style

Co-authored-by: Frankie <frankie@frase.io>
Co-authored-by: Lysandre <lysandre.debut@reseau.eseo.fr>
==

src/transformers/__init__.py
src/transformers/modeling_auto.py
src/transformers/modeling_electra.py
tests/test_modeling_electra.py
==================
a08652772;Lysandre;2020-05-21 09:42:47 -0400;Unused Union should not be imported

==

src/transformers/trainer.py
==================
9d2ce253d;Lysandre Debut;2020-05-21 09:18:27 -0400;TPU hangs when saving optimizer/scheduler (#4467)
* TPU hangs when saving optimizer/scheduler

* Style

* ParallelLoader is not a DataLoader

* Style

* Addressing @julien-c's comments
==

src/transformers/trainer.py
==================
49296533c;Zhangyx;2020-05-21 21:17:44 +0800;Adds predict stage for glue tasks, and generate result files which can be submitted to gluebenchmark.com (#4463)
* Adds predict stage for glue tasks, and generate result files which could be submitted to gluebenchmark.com website.

* Use Split enum + always output the label name

Co-authored-by: Julien Chaumond <chaumond@gmail.com>
==

examples/bertology/run_bertology.py
examples/text-classification/run_glue.py
src/transformers/data/datasets/glue.py
src/transformers/data/processors/glue.py
src/transformers/data/processors/utils.py
tests/test_trainer.py
==================
271bedb48;Tobias Lee;2020-05-21 21:17:03 +0800;[examples] fix no grad in second pruning in run_bertology (#4479)
* fix no grad in second pruning and typo

* fix prune heads attention mismatch problem

* fix

* fix

* fix

* run make style

* run make style
==

examples/bertology/run_bertology.py
==================
865d4d595;Julien Chaumond;2020-05-20 18:27:42 -0400;[ci] Close #4481

==

tests/test_tokenization_bert_japanese.py
==================
a3af8e86c;Julien Chaumond;2020-05-20 18:26:51 -0400;Update test_trainer_distributed.py

==

tests/test_trainer_distributed.py
==================
eacea530c;Cola;2020-05-21 05:48:29 +0900;:rotating_light: Remove warning of deprecation (#4477)
Remove warning of deprecated overload of addcdiv_

Fix #4451
==

src/transformers/optimization.py
==================
fa2fbed3e;Julien Plu;2020-05-20 22:46:21 +0200;Better None gradients handling in TF Trainer (#4469)
* Better None gradients handling

* Apply Style

* Apply Style
==

src/transformers/trainer_tf.py
==================
e708bb75b;Oliver √Östrand;2020-05-20 16:45:59 -0400;Correct TF formatting to exclude LayerNorms from weight decay (#4448)
* Exclude LayerNorms from weight decay

* Include both formats of layer norm
==

src/transformers/optimization_tf.py
==================
49c06132d;Rens;2020-05-20 22:23:21 +0200;pass on tokenizer to pipeline (#4489)

==

src/transformers/convert_graph_to_onnx.py
==================
cacb654c7;Nathan Cooper;2020-05-20 16:17:52 -0400;Add Fine-tune DialoGPT on new datasets notebook (#4473)

==

notebooks/README.md
==================
30a09f382;Timo Moeller;2020-05-20 22:08:29 +0200;Adjust german bert model card, add new model card (#4488)

==

model_cards/bert-base-german-cased-README.md
model_cards/deepset/bert-base-german-cased-oldvocab/README.md
==================
14cb5b35f;Lysandre Debut;2020-05-20 11:59:45 -0400;Fix slow gpu tests lysandre (#4487)
* There is one missing key in BERT

* Correct device for CamemBERT model

* RoBERTa tokenization adding prefix space

* Style
==

tests/test_modeling_auto.py
tests/test_modeling_camembert.py
tests/test_tokenization_roberta.py
==================
6dc52c78d;Manuel Romero;2020-05-20 15:45:50 +0200;Create README.md (#4482)

==

model_cards/mrm8488/RuPERTa-base-finetuned-pos/README.md
==================
ed5456daf;Manuel Romero;2020-05-20 15:45:24 +0200;Model card for RuPERTa-base fine-tuned for NER (#4466)

==

model_cards/mrm8488/RuPERTa-base-finetuned-ner/README.md
==================
c76450e20;Oleksandr Bushkovskyi;2020-05-20 16:44:26 +0300;Model card for Tereveni-AI/gpt2-124M-uk-fiction (#4470)
Create model card for "Tereveni-AI/gpt2-124M-uk-fiction" model
==

model_cards/Tereveni-AI/gpt2-124M-uk-fiction/README.md
==================
9907dc523;Hu Xu;2020-05-20 08:42:35 -0500;add BERT trained from review corpus. (#4405)
* add model_cards for BERT trained on reviews.

* add link to repository.

* refine README.md for each review model
==

model_cards/activebus/BERT-DK_laptop/README.md
model_cards/activebus/BERT-DK_rest/README.md
model_cards/activebus/BERT-PT_laptop/README.md
model_cards/activebus/BERT-PT_rest/README.md
model_cards/activebus/BERT-XD_Review/README.md
model_cards/activebus/BERT_Review/README.md
==================
efbc1c5a9;Sam Shleifer;2020-05-19 19:45:49 -0400;[MarianTokenizer] implement save_vocabulary and other common methods (#4389)

==

src/transformers/tokenization_marian.py
tests/test_modeling_marian.py
tests/test_tokenization_marian.py
==================
956c4c4eb;Sam Shleifer;2020-05-19 19:45:31 -0400;[gpu slow tests] fix mbart-large-enro gpu tests (#4472)

==

tests/test_modeling_bart.py
==================
48c3a70b4;Patrick von Platen;2020-05-19 21:52:36 +0200;[Longformer] Docs and clean API (#4464)
* add longformer docs

* improve docs
==

docs/source/index.rst
docs/source/model_doc/longformer.rst
docs/source/pretrained_models.rst
src/transformers/configuration_longformer.py
src/transformers/modeling_longformer.py
==================
aa925a52f;Patrick von Platen;2020-05-19 21:35:04 +0200;[Tests, GPU, SLOW] fix a bunch of GPU hardcoded tests in Pytorch (#4468)
* fix gpu slow tests in pytorch

* change model to device syntax
==

examples/contrib/run_transfo_xl.py
src/transformers/modeling_utils.py
tests/test_modeling_ctrl.py
tests/test_modeling_encoder_decoder.py
tests/test_modeling_gpt2.py
tests/test_modeling_longformer.py
tests/test_modeling_openai.py
tests/test_modeling_t5.py
tests/test_modeling_transfo_xl.py
tests/test_modeling_xlm.py
tests/test_modeling_xlnet.py
==================
5856999a9;Suraj Patil;2020-05-19 21:56:28 +0530;add T5 fine-tuning notebook [Community notebooks] (#4462)
* add T5 fine-tuning notebook [Community notebooks]

* Update README.md

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
==

notebooks/README.md
==================
07dd7c2fd;Sam Shleifer;2020-05-19 10:46:55 -0400;[cleanup] test_tokenization_common.py (#4390)

==

CONTRIBUTING.md
src/transformers/tokenization_roberta.py
src/transformers/tokenization_utils.py
tests/test_tokenization_albert.py
tests/test_tokenization_bert.py
tests/test_tokenization_bert_japanese.py
tests/test_tokenization_common.py
tests/test_tokenization_distilbert.py
tests/test_tokenization_openai.py
tests/test_tokenization_t5.py
tests/test_tokenization_xlm.py
tests/test_tokenization_xlm_roberta.py
tests/test_tokenization_xlnet.py
==================
8f1d04714;Iz Beltagy;2020-05-19 07:04:43 -0700;Longformer (#4352)
* first commit

* bug fixes

* better examples

* undo padding

* remove wrong VOCAB_FILES_NAMES

* License

* make style

* make isort happy

* unit tests

* integration test

* make `black` happy by undoing `isort` changes!!

* lint

* no need for the padding value

* batch_size not bsz

* remove unused type casting

* seqlen not seq_len

* staticmethod

* `bert` selfattention instead of `n2`

* uint8 instead of bool + lints

* pad inputs_embeds using embeddings not a constant

* black

* unit test with padding

* fix unit tests

* remove redundant unit test

* upload model weights

* resolve todo

* simpler _mask_invalid_locations without lru_cache + backward compatible masked_fill_

* increase unittest coverage
==

README.md
docs/source/pretrained_models.rst
src/transformers/__init__.py
src/transformers/configuration_auto.py
src/transformers/configuration_longformer.py
src/transformers/modeling_auto.py
src/transformers/modeling_longformer.py
src/transformers/tokenization_auto.py
src/transformers/tokenization_longformer.py
tests/test_modeling_longformer.py
==================
31eedff5a;Girishkumar;2020-05-19 19:26:24 +0530;Refactored the README.md file (#4427)

==

model_cards/savasy/bert-base-turkish-sentiment-cased/README.md
==================
384f0eb2f;Shaoyen;2020-05-18 20:16:05 -0700;Map optimizer to correct device after loading from checkpoint. (#4403)
* Map optimizer to correct device after loading from checkpoint.

* Make style test pass

Co-authored-by: Julien Chaumond <chaumond@gmail.com>
==

src/transformers/trainer.py
==================
bf14ef75f;Julien Chaumond;2020-05-18 23:13:33 -0400;[Trainer] move model to device before setting optimizer (#4450)

==

src/transformers/trainer.py
==================
5e7fe8b58;Julien Chaumond;2020-05-18 22:02:39 -0400;Distributed eval: SequentialDistributedSampler + gather all results (#4243)
* Distributed eval: SequentialDistributedSampler + gather all results

* For consistency only write to disk from world_master

Close https://github.com/huggingface/transformers/issues/4272

* Working distributed eval

* Hook into scripts

* Fix #3721 again

* TPU.mesh_reduce: stay in tensor space

Thanks @jysohn23

* Just a small comment

* whitespace

* torch.hub: pip install packaging

* Add test scenarii
==

.github/workflows/github-torch-hub.yml
examples/language-modeling/run_language_modeling.py
examples/multiple-choice/run_multiple_choice.py
examples/text-classification/run_glue.py
examples/token-classification/run_ner.py
src/transformers/trainer.py
tests/test_trainer_distributed.py
==================
4c0689361;Julien Chaumond;2020-05-18 20:34:50 -0400;Fix nn.DataParallel compatibility in PyTorch 1.5 (#4300)
* Test case for #3936

* multigpu tests pass on pytorch 1.4.0

* Fixup

* multigpu tests pass on pytorch 1.5.0

* Update src/transformers/modeling_utils.py

* Update src/transformers/modeling_utils.py

* rename multigpu to require_multigpu

* mode doc
==

src/transformers/modeling_albert.py
src/transformers/modeling_bert.py
src/transformers/modeling_t5.py
src/transformers/modeling_utils.py
src/transformers/modeling_xlnet.py
tests/test_modeling_common.py
tests/test_modeling_ctrl.py
tests/test_modeling_gpt2.py
tests/test_modeling_reformer.py
tests/test_modeling_transfo_xl.py
tests/test_modeling_xlnet.py
tests/utils.py
==================
9de4afa89;Rakesh Chada;2020-05-18 17:17:36 -0700;Make get_last_lr in trainer backward compatible (#4446)
* makes fetching last learning late in trainer backward compatible

* split comment to multiple lines

* fixes black styling issue

* uses version to create a more explicit logic
==

src/transformers/trainer.py
==================
42e8fbfc5;Stefan Dumitrescu;2020-05-19 01:48:56 +0300;Added model cards for Romanian BERT models (#4437)
* Create README.md

* Create README.md

* Update README.md

* Update README.md

* Apply suggestions from code review

Co-authored-by: Julien Chaumond <chaumond@gmail.com>
==

model_cards/dumitrescustefan/bert-base-romanian-cased-v1/README.md
model_cards/dumitrescustefan/bert-base-romanian-uncased-v1/README.md
==================
54065d68b;Oliver Guhr;2020-05-19 00:44:41 +0200;added model card for german-sentiment-bert (#4435)

==

model_cards/oliverguhr/german-sentiment-bert/README.md
==================
e28b7e231;Martin M√ºller;2020-05-19 00:41:34 +0200;Create README.md (#4433)

==

model_cards/digitalepidemiologylab/covid-twitter-bert/README.md
==================
09b933f19;sy-wada;2020-05-19 07:18:17 +0900;Update README.md (model_card) (#4424)
- add a citation.
- modify the table of the BLUE benchmark.

The table of the first version was not displayed correctly on https://huggingface.co/seiya/oubiobert-base-uncased.
Could you please confirm that this fix will allow you to display it correctly?
==

model_cards/seiya/oubiobert-base-uncased/README.md
==================
235777ccc;Manuel Romero;2020-05-19 00:17:33 +0200;Modify example of usage (#4413)
I followed the google example of usage for its electra small model but i have seen it is not meaningful, so i created a better example
==

model_cards/mrm8488/electricidad-small-discriminator/README.md
==================
9ddd3a654;Suraj Patil;2020-05-19 03:47:14 +0530;add model card for t5-base-squad (#4409)
* add model card for t5-base-squad

* Update model_cards/valhalla/t5-base-squad/README.md

Co-authored-by: Julien Chaumond <chaumond@gmail.com>
==

model_cards/valhalla/t5-base-squad/README.md
==================
c5aa11439;HUSEIN ZOLKEPLI;2020-05-19 06:10:23 +0800;Added README huseinzol05/t5-base-bahasa-cased (#4377)
* add bert bahasa readme

* update readme

* update readme

* added xlnet

* added tiny-bert and fix xlnet readme

* added albert base

* added albert tiny

* added electra model

* added gpt2 117m bahasa readme

* added gpt2 345m bahasa readme

* added t5-base-bahasa

* fix readme

* Update model_cards/huseinzol05/t5-base-bahasa-cased/README.md

Co-authored-by: Julien Chaumond <chaumond@gmail.com>
==

model_cards/huseinzol05/t5-base-bahasa-cased/README.md
==================
ca4a3f4da;Funtowicz Morgan;2020-05-18 18:32:33 +0000;Adding optimizations block from ONNXRuntime. (#4431)
* Adding optimizations block from ONNXRuntime.

* Turn off external data format by default for PyTorch export.

* Correct the way use_external_format is passed through the cmdline args.
==

notebooks/04-onnx-export.ipynb
src/transformers/convert_graph_to_onnx.py
==================
24538df91;Patrick von Platen;2020-05-18 20:23:57 +0200;[Community notebooks] General notebooks (#4441)
* Update README.md

* Update README.md

* Update README.md

* Update README.md
==

notebooks/README.md
==================
a699525d2;Sam Shleifer;2020-05-18 12:23:21 -0400;[test_pipelines] Mark tests > 10s @slow, small speedups (#4421)

==

tests/test_pipelines.py
==================
d9ece8233;Boris Dayma;2020-05-18 10:37:35 -0500;fix(run_language_modeling): use arg overwrite_cache (#4407)

==

examples/language-modeling/run_language_modeling.py
==================
d39bf0ac2;Patrick von Platen;2020-05-18 17:34:00 +0200;better naming in tf t5 (#4401)

==

src/transformers/modeling_tf_t5.py
==================
590adb130;Patrick von Platen;2020-05-18 17:31:35 +0200;improve docstring (#4422)

==

src/transformers/configuration_t5.py
==================
026a5d088;Patrick von Platen;2020-05-18 17:25:58 +0200;[T5 fp16] Fix fp16 in T5 (#4436)
* fix fp16 in t5

* make style

* refactor invert_attention_mask fn

* fix typo
==

src/transformers/modeling_t5.py
src/transformers/modeling_utils.py
tests/test_modeling_t5.py
==================
fa6113f9a;Soham Chatterjee;2020-05-18 23:23:29 +0800;Fixed spelling of training (#4416)

==

docs/source/model_doc/albert.rst
==================
757baee84;Julien Chaumond;2020-05-18 11:20:46 -0400;Fix un-prefixed f-string
see https://github.com/huggingface/transformers/pull/4367#discussion_r426356693

Hat/tip @girishponkiya

==

examples/distillation/scripts/extract_distilbert.py
==================
a27c79590;Patrick von Platen;2020-05-18 15:51:40 +0200;fix (#4419)

==

src/transformers/modeling_tf_utils.py
src/transformers/modeling_utils.py
==================
31c799a0c;Funtowicz Morgan;2020-05-18 13:24:41 +0000;Tag onnx export tests as slow (#4432)

==

tests/test_onnx.py
==================
8581a670e;Mehrad Moradshahi;2020-05-18 05:54:04 -0700;[MbartTokenizer] save to sentencepiece.bpe.model (#4335)

==

src/transformers/tokenization_bart.py
==================
18d233d52;Lorenzo Ampil;2020-05-17 15:25:17 +0800;Allow the creation of "entity groups" for NerPipeline #3548 (#3957)
* Add index to be returned by NerPipeline to allow for the creation of

* Add entity groups

* Convert entity list to dict

* Add entity to entity_group_disagg atfter updating entity gorups

* Change 'group' parameter to 'grouped_entities'

* Add unit tests for grouped NER pipeline case

* Correct variable name typo for NER_FINETUNED_MODELS

* Sync grouped tests to recent test updates
==

src/transformers/pipelines.py
tests/test_pipelines.py
==================
3e0f06210;Julien Chaumond;2020-05-15 17:44:17 -0400;Fix addcmul_

==

src/transformers/optimization.py
==================
fc2a4c88c;Julien Chaumond;2020-05-15 17:38:48 -0400;Fix: one more try

==

src/transformers/optimization.py
==================
55bda5255;Julien Chaumond;2020-05-15 17:23:48 -0400;Same fix for `addcmul_`

==

src/transformers/optimization.py
==================
ad02c961c;Julien Chaumond;2020-05-15 17:09:11 -0400;Fix UserWarning: This overload of add_ is deprecated in pytorch==1.5.0

==

src/transformers/optimization.py
==================
15550ce0d;Julien Chaumond;2020-05-15 17:08:38 -0400;[skip ci] remove local rank

==

examples/language-modeling/run_language_modeling.py
src/transformers/configuration_roberta.py
==================
62427d081;Nikita;2020-05-15 17:33:08 +0300;rerun notebook 02-transformers (#4341)

==

notebooks/02-transformers.ipynb
==================
34706ba05;Jared T Nielsen;2020-05-15 07:52:00 -0600;Allow for None gradients in GradientAccumulator. (#4372)

==

src/transformers/optimization_tf.py
==================
edf9ac11d;Lysandre Debut;2020-05-15 09:49:11 -0400;Should return overflowing information for the log (#4385)

==

examples/multiple-choice/utils_multiple_choice.py
==================
b908f2e9d;Funtowicz Morgan;2020-05-15 13:47:15 +0000;Attempt to unpin torch version for Github Action. (#4384)

==

.github/workflows/self-push.yml
==================
af2e6bf87;Julien Chaumond;2020-05-14 20:34:31 -0400;[examples] Streamline doc

==

examples/README.md
==================
7defc6670;Lysandre Debut;2020-05-14 17:07:52 -0400;p_mask in SQuAD pre-processing (#4049)
* Better p_mask building

* Adressing @mfuntowicz comments
==

src/transformers/data/processors/squad.py
==================
84894974b;Morgan Funtowicz;2020-05-14 22:40:59 +0200;Updated ONNX notebook link in README.

==

notebooks/04-onnx-export.ipynb
notebooks/README.md
==================
db0076a9d;Funtowicz Morgan;2020-05-14 20:35:52 +0000;Conversion script to export transformers models to ONNX IR. (#4253)
* Added generic ONNX conversion script for PyTorch model.

* WIP initial TF support.

* TensorFlow/Keras ONNX export working.

* Print framework version info

* Add possibility to check the model is correctly loading on ONNX runtime.

* Remove quantization option.

* Specify ONNX opset version when exporting.

* Formatting.

* Remove unused imports.

* Make functions more generally reusable from other part of the code.

* isort happy.

* flake happy

* Export only feature-extraction for now

* Correctly check inputs order / filter before export.

* Removed task variable

* Fix invalid args call in load_graph_from_args.

* Fix invalid args call in convert.

* Fix invalid args call in infer_shapes.

* Raise exception and catch in caller function instead of exit.

* Add 04-onnx-export.ipynb notebook

* More WIP on the notebook

* Remove unused imports

* Simplify & remove unused constants.

* Export with constant_folding in PyTorch

* Let's try to put function args in the right order this time ...

* Disable external_data_format temporary

* ONNX notebook draft ready.

* Updated notebooks charts + wording

* Correct error while exporting last chart in notebook.

* Adressing @LysandreJik comment.

* Set ONNX opset to 11 as default value.

* Set opset param mandatory

* Added ONNX export unittests

* Quality.

* flake8 happy

* Add keras2onnx dependency on extras["tf"]

* Pin keras2onnx on github master to v1.6.5

* Second attempt.

* Third attempt.

* Use the right repo URL this time ...

* Do the same for onnxconverter-common

* Added keras2onnx and onnxconveter-common to 1.7.0 to supports TF2.2

* Correct commit hash.

* Addressing PR review: Optimization are enabled by default.

* Addressing PR review: small changes in the notebook

* setup.py comment about keras2onnx versioning.
==

notebooks/04-onnx-export.ipynb
notebooks/README.md
setup.py
src/transformers/convert_graph_to_onnx.py
tests/test_onnx.py
==================
2d0548017;Suraj Patil;2020-05-15 00:09:44 +0530;Fix trainer evaluation (#4363)
* fix loss calculation in evaluation

* fix evaluation on TPU when prediction_loss_only is True
==

src/transformers/trainer.py
==================
035678efd;Sava≈ü Yƒ±ldƒ±rƒ±m;2020-05-14 21:07:32 +0300;Create README.md (#4359)
* Create README.md

* Update model_cards/savasy/bert-base-turkish-squad/README.md

Co-authored-by: Julien Chaumond <chaumond@gmail.com>
==

model_cards/savasy/bert-base-turkish-squad/README.md
==================
b9c9e0538;sy-wada;2020-05-15 03:06:10 +0900;Create README.md (#4357)

==

model_cards/seiya/oubiobert-base-uncased/README.md
==================
9535bf197;Sam Shleifer;2020-05-14 13:50:47 -0400;Tokenizer.batch_decode convenience method (#4159)

==

src/transformers/tokenization_marian.py
src/transformers/tokenization_utils.py
==================
7822cd38a;Sam Shleifer;2020-05-14 13:36:02 -0400;[tests] make pipelines tests faster with smaller models (#4238)
covers torch and tf. Also fixes a failing @slow test
==

src/transformers/pipelines.py
tests/test_pipelines.py
==================
448c46725;Julien Chaumond;2020-05-14 13:14:26 -0400;Fix: unpin flake8 and fix cs errors (#4367)
* Fix: unpin flake8 and fix cs errors

* Ok we still need to quote those
==

examples/benchmarks.py
examples/distillation/distiller.py
examples/distillation/scripts/binarized_data.py
examples/distillation/scripts/extract.py
examples/distillation/scripts/extract_distilbert.py
examples/distillation/train.py
setup.cfg
setup.py
src/transformers/convert_marian_to_pytorch.py
src/transformers/data/datasets/glue.py
src/transformers/data/datasets/language_modeling.py
src/transformers/pipelines.py
tests/test_tokenization_common.py
==================
c547f15a1;Julien Chaumond;2020-05-14 11:58:32 -0400;Use Filelock to ensure distributed barriers
see context in https://github.com/huggingface/transformers/pull/4223

==

examples/language-modeling/run_language_modeling.py
examples/multiple-choice/run_multiple_choice.py
examples/multiple-choice/utils_multiple_choice.py
examples/token-classification/run_ner.py
examples/token-classification/utils_ner.py
src/transformers/data/datasets/language_modeling.py
==================
015f7812e;Julien Chaumond;2020-05-14 10:12:18 -0400;[ci skip] Pin isort

==

setup.py
==================
ef46ccb05;Lysandre Debut;2020-05-14 08:59:52 -0400;TPU needs a rendezvous (#4339)

==

src/transformers/trainer.py
==================
94cb73c2d;Viktor Alm;2020-05-14 02:05:15 +0200;Add image and metadata (#4345)
Unfortunately i accidentally orphaned my other PR
==

model_cards/ViktorAlm/electra-base-norwegian-uncased-discriminator/README.md
==================
a0eebdc40;Manuel Romero;2020-05-14 02:04:57 +0200;Add link to W&B to see whole training logs (#4348)

==

model_cards/mrm8488/gpt2-imdb-neg/README.md
==================
7cb203fae;Lysandre;2020-05-13 17:38:50 -0400;Release: v2.9.1

==

docs/source/conf.py
setup.py
src/transformers/__init__.py
==================
9a687ebb7;Sam Shleifer;2020-05-13 17:29:41 -0400;[Marian Fixes] prevent predicting pad_token_id before softmax, support language codes, name multilingual models (#4290)

==

docs/source/model_doc/marian.rst
src/transformers/convert_marian_to_pytorch.py
src/transformers/modeling_bart.py
src/transformers/modeling_marian.py
src/transformers/modeling_utils.py
src/transformers/tokenization_marian.py
tests/test_modeling_marian.py
==================
839bfaedb;Patrick von Platen;2020-05-13 20:24:08 +0200;[Docs, Notebook] Include generation pipeline (#4295)
* add first text for generation

* add generation pipeline to usage

* Created using Colaboratory

* correct docstring

* finish
==

docs/source/usage.rst
notebooks/03-pipelines.ipynb
==================
2d184cb55;Elyes Manai;2020-05-13 16:22:03 +0200;wrong variable name used (#4328)

==

model_cards/google/electra-base-generator/README.md
==================
ca1361868;Julien Plu;2020-05-13 15:22:31 +0200;Question Answering for TF trainer (#4320)
* Add QA trainer example for TF

* Make data_dir optional

* Fix parameter logic

* Fix feature convert

* Update the READMEs to add the question-answering task

* Apply style

* Change 'sequence-classification' to 'text-classification' and prefix with 'eval' all the metric names

* Apply style

* Apply style
==

examples/README.md
examples/question-answering/README.md
examples/question-answering/run_tf_squad.py
src/transformers/trainer_tf.py
src/transformers/training_args_tf.py
==================
1e51bb717;Denis;2020-05-13 14:32:57 +0200;Fix for #3865. PretrainedTokenizer mapped " do not" into " don't" when .decode(...) is called. Removed the " do not" --> " don't" mapping from clean_up_tokenization(...). (#4024)

==

src/transformers/tokenization_utils.py
==================
241759101;Julien Chaumond;2020-05-12 21:52:01 -0400;(v2) Improvements to the wandb integration (#4324)
* Improvements to the wandb integration

* small reorg + no global necessary

* feat(trainer): log epoch and final metrics

* Simplify logging a bit

* Fixup

* Fix crash when just running eval

Co-authored-by: Chris Van Pelt <vanpelt@gmail.com>
Co-authored-by: Boris Dayma <boris.dayma@gmail.com>
==

examples/language-modeling/run_language_modeling.py
examples/test_examples.py
examples/token-classification/test_ner_examples.py
src/transformers/trainer.py
tests/test_trainer.py
==================
7d7fe4997;Funtowicz Morgan;2020-05-12 19:02:46 +0000;Allow BatchEncoding to be initialized empty. (#4316)
* Allow BatchEncoding to be initialized empty.

This is required by recent changes introduced in TF 2.2.

* Attempt to unpin Tensorflow to 2.2 with the previous commit.
==

setup.py
src/transformers/tokenization_utils.py
==================
0a97f6312;Sava≈ü Yƒ±ldƒ±rƒ±m;2020-05-12 22:01:45 +0300;Update README.md (#4313)

==

model_cards/savasy/bert-base-turkish-sentiment-cased/README.md
==================
15a121fec;Sava≈ü Yƒ±ldƒ±rƒ±m;2020-05-12 22:01:34 +0300;Update README.md (#4315)

==

model_cards/savasy/bert-base-turkish-sentiment-cased/README.md
==================
15d45211f;Stefan Schweter;2020-05-12 21:01:17 +0200;[model_cards]: üáπüá∑ Add new ELECTRA small and base models for Turkish (#4318)

==

model_cards/dbmdz/electra-base-turkish-cased-discriminator/README.md
model_cards/dbmdz/electra-small-turkish-cased-discriminator/README.md
==================
8a017cbb5;Viktor Alm;2020-05-12 21:00:56 +0200;Add modelcard with acknowledgements (#4321)

==

model_cards/ViktorAlm/electra-base-norwegian-uncased-discriminator/README.md
==================
4bf504224;Julien Chaumond;2020-05-12 09:11:50 -0400;Fix BART tests on GPU (#4298)

==

src/transformers/modeling_bart.py
tests/test_modeling_bart.py
==================
e4512aab3;Viktor Alm;2020-05-12 14:48:48 +0200;Add MultipleChoice to TFTrainer [WIP] (#4270)
* catch gpu len 1 set to gpu0

* Add mpc to trainer

* Add MPC for TF

* fix TF automodel for MPC and add Albert

* Apply style

* Fix import

* Note to self: double check

* Make shape None, None for datasetgenerator output shapes

* Add from_pt bool which doesnt seem to work

* Original checkpoint dir

* Fix docstrings for automodel

* Update readme and apply style

* Colab should probably not be from users

* Colabs should probably not be from users

* Add colab

* Update README.md

* Update README.md

* Cleanup __intit__

* Cleanup flake8 trailing comma

* Update src/transformers/training_args_tf.py

* Update src/transformers/modeling_tf_auto.py

Co-authored-by: Viktor Alm <viktoralm@pop-os.localdomain>
Co-authored-by: Julien Chaumond <chaumond@gmail.com>
==

examples/README.md
examples/multiple-choice/README.md
examples/multiple-choice/run_tf_multiple_choice.py
examples/multiple-choice/utils_multiple_choice.py
src/transformers/__init__.py
src/transformers/modeling_tf_albert.py
src/transformers/modeling_tf_auto.py
src/transformers/trainer_tf.py
src/transformers/training_args_tf.py
==================
65be574ae;Levent Serinol;2020-05-12 15:34:17 +0300;fixed missing torch module import (#4305)
fixed missing torch module import in example usage code
==

model_cards/lserinol/bert-turkish-question-answering/README.md
==================
31e67dd19;Jangwon Park;2020-05-12 21:32:44 +0900;Remove hard-coded pad token id in distilbert and albert (#3965)

==

src/transformers/modeling_albert.py
src/transformers/modeling_distilbert.py
==================
30e343862;Lysandre Debut;2020-05-11 21:03:30 -0400;pin TF to 2.1 (#4297)
* pin TF to 2.1

* Pin flake8 as well
==

setup.py
==================
56e8ef632;Julien Chaumond;2020-05-11 20:40:41 -0400;[ci] Restrict GPU tests to actual code commits

==

.github/workflows/self-push.yml
==================
ba6f6e44a;Julien Chaumond;2020-05-12 00:05:36 +0000;[ci] Re-enable torch GPU tests

==

.github/workflows/self-push.yml
==================
952495681;Lysandre Debut;2020-05-11 16:43:57 -0400;Documentation specification (#4294)

==

docs/README.md
==================
61d22f9cc;Bram Vanroy;2020-05-11 21:24:02 +0200;Simplify cache vars and allow for TRANSFORMERS_CACHE env (#4226)
* simplify cache vars and allow for TRANSFORMERS_CACHE env

As it currently stands, "TRANSFORMERS_CACHE" is not an accepted variable. It seems that the these variables were not updated when moving from version pytorch_transformers to transformers. In addition, the fallback procedure could be improved. and simplified. Pathlib seems redundant here.

* Update file_utils.py
==

src/transformers/file_utils.py
==================
cd40cb887;Lysandre Debut;2020-05-11 15:05:36 -0400;Fix special token doc (#4292)

==

src/transformers/tokenization_albert.py
src/transformers/tokenization_bert.py
src/transformers/tokenization_camembert.py
src/transformers/tokenization_roberta.py
src/transformers/tokenization_xlm.py
src/transformers/tokenization_xlm_roberta.py
src/transformers/tokenization_xlnet.py
==================
82601f4c1;Tianlei Wu;2020-05-11 11:55:55 -0700;Allow gpt2 to be exported to valid ONNX (#4244)
* allow gpt2 to be exported to valid ONNX model

* cast size from int to float explictly
==

src/transformers/activations.py
src/transformers/modeling_gpt2.py
==================
39994051e;Guo, Quan;2020-05-11 13:35:13 -0400;Add migrating from `pytorch-transformers` (#4273)
"Migrating from pytorch-transformers to transformers" is missing in the main document. It is available in the main `readme` thought. Just move it to the document.
==

docs/source/migration.md
==================
051dcb2a0;Lysandre Debut;2020-05-11 13:31:03 -0400;CamemBERT does not make use of Token Type IDs (#4289)

==

src/transformers/tokenization_camembert.py
==================
41e829121;fgaim;2020-05-11 19:10:00 +0200;Add ALBERT to the Tensorflow to Pytorch model conversion cli (#3933)
* Add ALBERT to convert command of transformers-cli

* Document ALBERT tf to pytorch model conversion
==

docs/source/converting_tensorflow_models.rst
src/transformers/commands/convert.py
==================
3f42eb979;Stefan Schweter;2020-05-11 18:48:21 +0200;Documentation: fix links to NER examples (#4279)
* docs: fix link to token classification (NER) example

* examples: fix links to NER scripts
==

docs/source/examples.md
examples/token-classification/README.md
==================
8fdb7997c;Funtowicz Morgan;2020-05-11 16:45:53 +0000;Align sentiment-analysis' tokenizer (currently uncased) to the model (uncased). (#4264)

==

src/transformers/pipelines.py
==================
4658896ee;Sam Shleifer;2020-05-11 11:47:51 -0400;[Marian] Fix typo in docstring (#4284)

==

src/transformers/modeling_marian.py
==================
bf64b8cf0;Levent Serinol;2020-05-11 18:32:25 +0300;Model card for bert-turkish-question-answering question-answering model (#4281)
* Create README.md

* Update model_cards/lserinol/bert-turkish-question-answering/README.md

Co-authored-by: Julien Chaumond <chaumond@gmail.com>
==

model_cards/lserinol/bert-turkish-question-answering/README.md
==================
94b57bf79;Julien Plu;2020-05-11 17:28:37 +0200;[TF 2.2 compat]  use tf.VariableAggregation.ONLY_FIRST_REPLICA (#4283)
* Fix the issue to properly run the accumulator with TF 2.2

* Apply style

* Fix training_args_tf for TF 2.2

* Fix the TF training args when only one GPU is available

* Remove the fixed version of TF in setup.py
==

setup.py
src/transformers/optimization_tf.py
src/transformers/training_args_tf.py
==================
cffbb3d8e;Sava≈ü Yƒ±ldƒ±rƒ±m;2020-05-11 18:24:41 +0300;Update README.md (#4276)

==

model_cards/savasy/bert-base-turkish-sentiment-cased/README.md
==================
5f50d619d;Julien Plu;2020-05-11 17:24:10 +0200;Fix XTREME link + add number of eval documents + fix usage code (#4280)

==

model_cards/jplu/tf-xlm-r-ner-40-lang/README.md
==================
7751be7ce;theblackcat102;2020-05-11 22:53:42 +0800;fix reformer apex scaling issue (#4242)

==

src/transformers/modeling_reformer.py
==================
ac7d5f67a;Patrick von Platen;2020-05-11 16:38:07 +0200;[Reformer] Add Enwiki8 Reformer Model - Adapt convert script (#4282)
* adapt convert script

* update convert script

* finish

* fix marian pretrained docs
==

docs/source/pretrained_models.rst
src/transformers/configuration_reformer.py
src/transformers/convert_reformer_trax_checkpoint_to_pytorch.py
src/transformers/modeling_reformer.py
==================
336116d96;Patrick von Platen;2020-05-11 16:22:08 +0200;Reformer enwik8 - Model card (#4286)

==

model_cards/google/reformer-enwik8/README.md
==================
b290c32e1;flozi00;2020-05-10 20:07:08 +0200;[docs] fix typo (#4249)

==

src/transformers/modeling_auto.py
==================
3487be75e;Sam Shleifer;2020-05-10 13:54:57 -0400;[Marian] documentation and AutoModel support (#4152)
- MarianSentencepieceTokenizer - > MarianTokenizer
- Start using unk token.
- add docs page
- add better generation params to MarianConfig
- more conversion utilities
==

README.md
docs/source/index.rst
docs/source/model_doc/bart.rst
docs/source/model_doc/marian.rst
docs/source/pretrained_models.rst
src/transformers/__init__.py
src/transformers/configuration_auto.py
src/transformers/configuration_marian.py
src/transformers/convert_marian_to_pytorch.py
src/transformers/modeling_auto.py
src/transformers/modeling_marian.py
src/transformers/tokenization_auto.py
src/transformers/tokenization_marian.py
tests/test_modeling_marian.py
==================
9d2f467bf;Girishkumar;2020-05-10 13:02:36 +0000;[README] Corrected some grammatical mistakes (#4199)

==

docs/source/quickstart.md
==================
7b75aa9fa;Julien Chaumond;2020-05-08 14:10:05 -0400;[TPU] Doc, fix xla_spawn.py, only preprocess dataset once (#4223)
* [TPU] Doc, fix xla_spawn.py, only preprocess dataset once

* Update examples/README.md

* [xla_spawn] Add `_mp_fn` to other Trainer scripts

* [TPU] Fix: eval dataloader was None
==

examples/README.md
examples/bertology/run_bertology.py
examples/language-modeling/run_language_modeling.py
examples/multiple-choice/run_multiple_choice.py
examples/text-classification/README.md
examples/text-classification/run_glue.py
examples/token-classification/run_ner.py
examples/xla_spawn.py
src/transformers/data/datasets/glue.py
src/transformers/trainer.py
==================
274d850d3;Julien Chaumond;2020-05-08 12:39:46 -0400;Fix #4098

==

src/transformers/modeling_gpt2.py
==================
26dad0a9f;Lorenzo De Mattei;2020-05-08 15:45:10 +0200;example updated to use generation pipeline (#4230)
* example updated to use generation pipeline

* Update model_cards/LorenzoDeMattei/GePpeTto/README.md

Co-authored-by: Julien Chaumond <chaumond@gmail.com>
==

model_cards/LorenzoDeMattei/GePpeTto/README.md
==================
9ebb5b2a5;rmroczkowski;2020-05-08 15:42:43 +0200;Model card for allegro/herbert-klej-cased-tokenizer-v1 (#4184)

==

model_cards/allegro/herbert-klej-cased-tokenizer-v1/README.md
==================
9e54efd00;rmroczkowski;2020-05-08 15:42:28 +0200;Model card for allegro/herbert-klej-cased-v1 (#4183)

==

model_cards/allegro/herbert-klej-cased-v1/README.md
==================
a8b798e6c;Manuel Romero;2020-05-08 15:30:15 +0200;Model card for spanish electra small (#4196)

==

model_cards/mrm8488/electricidad-small-discriminator/README.md
==================
242005d76;Sava≈ü Yƒ±ldƒ±rƒ±m;2020-05-08 16:27:29 +0300;Create README.md (#4132)
* Create README.md

* Adding code fence around code block
==

model_cards/savasy/bert-base-turkish-ner-cased/README.md
==================
5940c73bb;Manuel Romero;2020-05-08 15:25:36 +0200;Create README.md (#4179)
model card for my De Novo Drug discovery model using MLM
==

model_cards/mrm8488/chEMBL_smiles_v1/README.md
==================
cf08830c2;Patrick von Platen;2020-05-08 14:30:05 +0200;[Pipeline, Generation]  tf generation pipeline bug (#4217)
* fix PR

* move tests to correct place
==

src/transformers/pipelines.py
tests/test_pipelines.py
==================
8bf731265;Jared T Nielsen;2020-05-07 17:44:51 -0600;Add AlbertForPreTraining and TFAlbertForPreTraining models. (#4057)
* Add AlbertForPreTraining and TFAlbertForPreTraining models.

* PyTorch conversion

* TensorFlow conversion

* style

Co-authored-by: Lysandre <lysandre.debut@reseau.eseo.fr>
==

src/transformers/__init__.py
src/transformers/convert_albert_original_tf_checkpoint_to_pytorch.py
src/transformers/convert_pytorch_checkpoint_to_tf2.py
src/transformers/modeling_albert.py
src/transformers/modeling_auto.py
src/transformers/modeling_tf_albert.py
src/transformers/modeling_tf_auto.py
tests/test_modeling_albert.py
tests/test_modeling_tf_albert.py
==================
c99fe0386;Julien Chaumond;2020-05-07 18:44:18 -0400;[doc] Fix broken links + remove crazy big notebook

==

README.md
docs/source/bertology.rst
docs/source/examples.md
docs/source/main_classes/processors.rst
docs/source/model_doc/xlnet.rst
docs/source/multilingual.rst
examples/README.md
examples/distillation/run_squad_w_distillation.py
examples/language-modeling/README.md
examples/multiple-choice/README.md
examples/question-answering/README.md
examples/text-classification/README.md
examples/text-generation/README.md
examples/text-generation/pplm/run_pplm.py
model_cards/fmikaelian/camembert-base-fquad/README.md
model_cards/fmikaelian/camembert-base-squad/README.md
model_cards/fmikaelian/flaubert-base-uncased-squad/README.md
model_cards/ktrapeznikov/albert-xlarge-v2-squad-v2/README.md
model_cards/ktrapeznikov/biobert_v1.1_pubmed_squad_v2/README.md
model_cards/ktrapeznikov/scibert_scivocab_uncased_squad_v2/README.md
model_cards/mrm8488/GPT-2-finetuned-CORD19/README.md
model_cards/mrm8488/GPT-2-finetuned-covid-bio-medrxiv/README.md
model_cards/mrm8488/TinyBERT-spanish-uncased-finetuned-ner/README.md
model_cards/mrm8488/bert-base-spanish-wwm-cased-finetuned-spa-squad2-es/README.md
model_cards/mrm8488/bert-medium-finetuned-squadv2/README.md
model_cards/mrm8488/bert-mini-finetuned-squadv2/README.md
model_cards/mrm8488/bert-small-finetuned-squadv2/README.md
model_cards/mrm8488/bert-small-finetuned-typo-detection/README.md
model_cards/mrm8488/bert-spanish-cased-finetuned-ner/README.md
model_cards/mrm8488/bert-spanish-cased-finetuned-pos-syntax/README.md
model_cards/mrm8488/bert-spanish-cased-finetuned-pos/README.md
model_cards/mrm8488/bert-tiny-finetuned-squadv2/README.md
model_cards/mrm8488/distilbert-base-multi-cased-finetuned-typo-detection/README.md
model_cards/mrm8488/distilbert-multi-finetuned-for-xqua-on-tydiqa/README.md
model_cards/mrm8488/spanbert-finetuned-squadv1/README.md
model_cards/mrm8488/spanbert-finetuned-squadv2/README.md
src/transformers/modeling_albert.py
src/transformers/modeling_roberta.py
src/transformers/modeling_tf_albert.py
src/transformers/modeling_tf_roberta.py
==================
66113bd62;Sava≈ü Yƒ±ldƒ±rƒ±m;2020-05-08 01:31:22 +0300;Create README.md (#4202)

==

model_cards/savasy/bert-base-turkish-sentiment-cased/README.md
==================
6669915b6;Julien Chaumond;2020-05-07 15:26:58 -0400;[examples] Add column for pytorch-lightning support

==

examples/README.md
==================
612fa1b10;Julien Chaumond;2020-05-07 15:00:06 -0400;Examples readme.md (#4215)
* README

* Update README.md
==

docs/source/usage.rst
examples/README.md
==================
2e5782437;Lysandre;2020-05-07 14:42:00 -0400;Pin isort and tf <= 2.1.0

==

setup.py
==================
e7cfc1a31;Lysandre;2020-05-07 14:15:20 -0400;Release: v2.9.0

==

docs/source/conf.py
setup.py
src/transformers/__init__.py
==================
0ae96ff8a;Julien Chaumond;2020-05-07 13:48:44 -0400;BIG Reorganize examples  (#4213)
* Created using Colaboratory

* [examples] reorganize files

* remove run_tpu_glue.py as superseded by TPU support in Trainer

* Bugfix: int, not tuple

* move files around
==

.gitignore
README.md
docs/source/examples.md
docs/source/main_classes/processors.rst
docs/source/usage.rst
examples/README.md
examples/adversarial/README.md
examples/adversarial/hans_processors.py
examples/adversarial/test_hans.py
examples/adversarial/utils_hans.py
examples/bertology/run_bertology.py
examples/contrib/mm-imdb/README.md
examples/contrib/mm-imdb/run_mmimdb.py
examples/contrib/mm-imdb/utils_mmimdb.py
examples/glue/README.md
examples/language-modeling/README.md
examples/language-modeling/run_language_modeling.py
examples/lightning_base.py
examples/multiple-choice/README.md
examples/multiple-choice/run_multiple_choice.py
examples/multiple-choice/utils_multiple_choice.py
examples/question-answering/README.md
examples/question-answering/run_squad.py
examples/requirements.txt
examples/run_tpu_glue.py
examples/summarization/bart/finetune.py
examples/summarization/bart/run_train.sh
examples/summarization/bart/run_train_tiny.sh
examples/test_examples.py
examples/text-classification/README.md
examples/text-classification/run_glue.py
examples/text-classification/run_pl.sh
examples/text-classification/run_pl_glue.py
examples/text-classification/run_tf_glue.py
examples/text-classification/run_xnli.py
examples/text-generation/README.md
examples/text-generation/pplm/README.md
examples/text-generation/pplm/imgs/headfigure.png
examples/text-generation/pplm/imgs/wooly.png
examples/text-generation/pplm/pplm_classification_head.py
examples/text-generation/pplm/run_pplm.py
examples/text-generation/pplm/run_pplm_discrim_train.py
examples/text-generation/run_generation.py
examples/token-classification/README.md
examples/token-classification/run.sh
examples/token-classification/run_ner.py
examples/token-classification/run_pl.sh
examples/token-classification/run_pl_ner.py
examples/token-classification/run_tf_ner.py
examples/token-classification/test_ner_examples.py
examples/token-classification/utils_ner.py
model_cards/SparkBeyond/roberta-large-sts-b/README.md
src/transformers/trainer.py
tests/fixtures/tests_samples/.gitignore
tests/fixtures/tests_samples/GermEval/dev.txt
tests/fixtures/tests_samples/GermEval/labels.txt
tests/fixtures/tests_samples/GermEval/train.txt
tests/fixtures/tests_samples/MRPC/dev.tsv
tests/fixtures/tests_samples/MRPC/train.tsv
tests/fixtures/tests_samples/SQUAD/dev-v2.0.json
tests/fixtures/tests_samples/SQUAD/train-v2.0.json
tests/fixtures/tests_samples/STS-B/dev.tsv
tests/fixtures/tests_samples/STS-B/train.tsv
tests/test_trainer.py
valohai.yaml
==================
cafa6a9e2;Julien Chaumond;2020-05-07 11:25:26 -0400;[Trainer] Ability to specify optimizer/scheduler at init
cc @patrickvonplaten @thomwolf

==

src/transformers/trainer.py
==================
e4fd5e399;Bram Vanroy;2020-05-07 17:14:56 +0200;Use with_extension to change the extension (#4203)
As per https://github.com/huggingface/transformers/pull/3934#discussion_r421307659
==

src/transformers/hf_argparser.py
==================
ebf80e2e7;Lysandre Debut;2020-05-07 10:34:04 -0400;Tpu trainer (#4146)
* wip

* wip

* a last wip

* Better logging when using TPUs

* Correct argument name

* Tests

* fix

* Metrics in evaluation

* Update src/transformers/training_args.py

* [tpu] Use launcher script instead

* [tpu] lots of tweaks

* Fix formatting

Co-authored-by: Julien Chaumond <chaumond@gmail.com>
==

examples/run_glue.py
examples/xla_spawn.py
src/transformers/trainer.py
src/transformers/training_args.py
==================
026097b9e;Funtowicz Morgan;2020-05-07 14:02:53 +0000;Ensure fast tokenizer can construct tensor without pad token if only one sample is provided. (#4201)

==

src/transformers/tokenization_utils.py
==================
0a6cbea0a;Funtowicz Morgan;2020-05-07 13:52:40 +0000;Rewritten batch support in pipelines. (#4154)
* Rewritten batch support in pipelines.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Fix imports sorting :wrench:

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Set pad_to_max_length=True by default on Pipeline.

* Set pad_to_max_length=False for generation pipelines.

Most of generation models doesn't have padding token.

* Address @joeddav review comment: Uniformized *args.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Address @joeddav review comment: Uniformized *args (second).

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>
==

src/transformers/pipelines.py
tests/test_pipelines.py
==================
99d1a6944;Patrick von Platen;2020-05-07 10:54:48 +0200;fix examples (#4192)

==

src/transformers/modeling_reformer.py
==================
74ffc9ea6;Patrick von Platen;2020-05-07 10:50:11 +0200;[Reformer] Fix example and error message (#4191)
* fix example reformer

* fix error message and example docstring

* improved error message
==

src/transformers/modeling_reformer.py
==================
96c78396c;Patrick von Platen;2020-05-07 10:28:31 +0200;fix docstring reformer (#4190)

==

src/transformers/configuration_reformer.py
==================
dca34695d;Patrick von Platen;2020-05-07 10:17:01 +0200;Reformer (#3351)
* first copy & past commit from Bert and morgans LSH code

* add easy way to compare to trax original code

* translate most of function

* make trax lsh self attention deterministic with numpy seed + copy paste code

* add same config

* add same config

* make layer init work

* implemented hash_vectors function for lsh attention

* continue reformer translation

* hf LSHSelfAttentionLayer gives same output as trax layer

* refactor code

* refactor code

* refactor code

* refactor

* refactor + add reformer config

* delete bogus file

* split reformer attention layer into two layers

* save intermediate step

* save intermediate step

* make test work

* add complete reformer block layer

* finish reformer layer

* implement causal and self mask

* clean reformer test and refactor code

* fix merge conflicts

* fix merge conflicts

* update init

* fix device for GPU

* fix chunk length init for tests

* include morgans optimization

* improve memory a bit

* improve comment

* factorize num_buckets

* better testing parameters

* make whole model work

* make lm model work

* add t5 copy paste tokenizer

* add chunking feed forward

* clean config

* add improved assert statements

* make tokenizer work

* improve test

* correct typo

* extend config

* add complexer test

* add new axial position embeddings

* add local block attention layer

* clean tests

* refactor

* better testing

* save intermediate progress

* clean test file

* make shorter input length work for model

* allow variable input length

* refactor

* make forward pass for pretrained model work

* add generation possibility

* finish dropout and init

* make style

* refactor

* add first version of RevNet Layers

* make forward pass work and add convert file

* make uploaded model forward pass work

* make uploaded model forward pass work

* refactor code

* add namedtuples and cache buckets

* correct head masks

* refactor

* made reformer more flexible

* make style

* remove set max length

* add attention masks

* fix up tests

* fix lsh attention mask

* make random seed optional for the moment

* improve memory in reformer

* add tests

* make style

* make sure masks work correctly

* detach gradients

* save intermediate

* correct backprob through gather

* make style

* change back num hashes

* rename to labels

* fix rotation shape

* fix detach

* update

* fix trainer

* fix backward dropout

* make reformer more flexible

* fix conflict

* fix

* fix

* add tests for fixed seed in reformer layer

* fix trainer typo

* fix typo in activations

* add fp16 tests

* add fp16 training

* support fp16

* correct gradient bug in reformer

* add fast gelu

* re-add dropout for embedding dropout

* better naming

* better naming

* renaming

* finalize test branch

* finalize tests

* add more tests

* finish tests

* fix

* fix type trainer

* fix fp16 tests

* fix tests

* fix tests

* fix tests

* fix issue with dropout

* fix dropout seeds

* correct random seed on gpu

* finalize random seed for dropout

* finalize random seed for dropout

* remove duplicate line

* correct half precision bug

* make style

* refactor

* refactor

* docstring

* remove sinusoidal position encodings for reformer

* move chunking to modeling_utils

* make style

* clean config

* make style

* fix tests

* fix auto tests

* pretrained models

* fix docstring

* update conversion file

* Update pretrained_models.rst

* fix rst

* fix rst

* update copyright

* fix test path

* fix test path

* fix small issue in test

* include reformer in generation tests

* add docs for axial position encoding

* finish docs

* Update convert_reformer_trax_checkpoint_to_pytorch.py

* remove isort

* include sams comments

* remove wrong comment in utils

* correct typos

* fix typo

* Update reformer.rst

* applied morgans optimization

* make style

* make gpu compatible

* remove bogus file

* big test refactor

* add example for chunking

* fix typo

* add to README
==

README.md
docs/source/glossary.rst
docs/source/index.rst
docs/source/main_classes/model.rst
docs/source/model_doc/reformer.rst
docs/source/pretrained_models.rst
src/transformers/__init__.py
src/transformers/activations.py
src/transformers/configuration_auto.py
src/transformers/configuration_reformer.py
src/transformers/convert_reformer_trax_checkpoint_to_pytorch.py
src/transformers/modeling_auto.py
src/transformers/modeling_reformer.py
src/transformers/modeling_utils.py
src/transformers/tokenization_auto.py
src/transformers/tokenization_reformer.py
tests/test_activations.py
tests/test_modeling_common.py
tests/test_modeling_reformer.py
==================
877fc5641;Clement;2020-05-06 16:31:07 -0400;change order pytorch/tf in readme (#4167)

==

README.md
==================
aad50151f;Julien Plu;2020-05-06 18:56:52 +0200;TF version of the trainer (#4017)
* First commit to add a TF version of the trainer.

* Make the TF trainer closer to what looks the PT trainer

* Refactoring common code between the PT and TF trainer into an util file.

* Some bugfix + better similarity with the PT trainer

* Add missing class in transformers init

* Bugfix over prediction + use classification report instead of simple metrics

* Fix name error

* Fix optimization tests + style

* Apply style

* Several bugfix for multi-gpu training

* Apply style

* Apply style

* Add glue example for the TF trainer

* Several bugix + address the reviews

* Fix on the TF training args file

* Add a debug mode

* Bugfix in utils_ner.py when segment_ids is None

* Apply style

* Apply style

* Add TPU strategy

* Fix selection strategy
==

examples/ner/run_tf_ner.py
examples/ner/utils_ner.py
examples/run_tf_glue.py
src/transformers/__init__.py
src/transformers/optimization_tf.py
src/transformers/trainer.py
src/transformers/trainer_tf.py
src/transformers/trainer_utils.py
src/transformers/training_args_tf.py
tests/test_optimization_tf.py
==================
25296b12a;Simone Primarosa;2020-05-06 17:24:49 +0100;Fix overwrite_cache behaviour for pytorch lightning examples (#4093)

==

examples/glue/run_pl_glue.py
examples/ner/run_pl_ner.py
==================
9972562d3;kumapo;2020-05-07 01:00:23 +0900;Include ElectraPreTrainedModel into __init__ (#4173)

==

src/transformers/__init__.py
==================
ff8ed52dd;martindh;2020-05-06 16:41:07 +0200;Camembert-large-fquad model card (#4143)
Description for the model card describing the camembert-large-fquad model.
==

model_cards/illuin/camembert-large-fquad/README.md
==================
4c3be2e71;Julien Plu;2020-05-06 16:40:55 +0200;Add model card for the NER model (#4162)

==

model_cards/jplu/tf-xlm-r-ner-40-lang/README.md
==================
17ae0363d;Manuel Romero;2020-05-06 16:38:29 +0200;Fix markdown to show the results table properly (#4119)

==

model_cards/mrm8488/distilroberta-base-finetuned-sentiment/README.md
==================
a638e986f;Patrick von Platen;2020-05-06 00:42:34 +0200;fix hard wired pad token id (#4138)

==

src/transformers/modeling_roberta.py
==================
fd2174664;Julien Chaumond;2020-05-05 10:59:23 -0400;[Trainer] W&B: Enable model watch
See https://github.com/huggingface/transformers/pull/3916

==

src/transformers/trainer.py
==================
79b1c6966;Lysandre Debut;2020-05-05 10:23:01 -0400;Pytorch 1.5.0 (#3973)
* Standard deviation can no longer be set to 0

* Remove torch pinned version

* 9th instead of 10th, silly me
==

setup.py
tests/test_modeling_common.py
==================
818463ee8;Boris Dayma;2020-05-04 21:42:27 -0500;Trainer: add logging through Weights & Biases (#3916)
* feat: add logging through Weights & Biases

* feat(wandb): make logging compatible with all scripts

* style(trainer.py): fix formatting

* [Trainer] Tweak wandb integration

Co-authored-by: Julien Chaumond <chaumond@gmail.com>
==

.gitignore
src/transformers/trainer.py
src/transformers/training_args.py
==================
858b1d1e5;jaymody;2020-04-27 14:42:57 -0400;allow an already created tensorboard SummaryWriter be passed to Trainer

==

src/transformers/trainer.py
==================
8e67573a6;Patrick von Platen;2020-05-04 02:18:36 +0200;[EncoderDecoder Tests] Improve tests (#4046)
* Hoist bert model tester for patric

* indent

* make tests work

* Update tests/test_modeling_bert.py

Co-authored-by: Julien Chaumond <chaumond@gmail.com>

Co-authored-by: sshleifer <sshleifer@gmail.com>
Co-authored-by: Julien Chaumond <chaumond@gmail.com>
==

tests/test_modeling_bert.py
tests/test_modeling_encoder_decoder.py
==================
6af3306a1;Lorenzo Ampil;2020-05-03 18:40:08 +0800;Add decoder specific error message for T5Stack.forward (#4128)

==

src/transformers/modeling_t5.py
==================
1cdd2ad2a;Zhiyu Lin;2020-05-02 11:20:30 -0400;Fix #2941 (#4109)
* Fix of issue #2941

Reshaped score array to avoid `numpy` ValueError.

* Update src/transformers/pipelines.py

* Update src/transformers/pipelines.py

Co-authored-by: Julien Chaumond <chaumond@gmail.com>
==

src/transformers/pipelines.py
==================
5f4f6b65b;Manuel Romero;2020-05-02 17:19:31 +0200;distilroberta-base-finetuned-sentiment (#4115)
* Create model card

Create Model card for distilroberta-base-finetuned-sentiment

* Update model_cards/mrm8488/distilroberta-base-finetuned-sentiment/README.md

* Update model_cards/mrm8488/distilroberta-base-finetuned-sentiment/README.md

Co-authored-by: Julien Chaumond <chaumond@gmail.com>
==

model_cards/mrm8488/distilroberta-base-finetuned-sentiment/README.md
==================
7da051f13;Suraj Parmar;2020-05-02 20:45:39 +0530;model card for surajp/albert-base-sanskrit (#4114)
* Create README.md

* Update model_cards/surajp/albert-base-sanskrit/README.md

Co-authored-by: Julien Chaumond <chaumond@gmail.com>
==

model_cards/surajp/albert-base-sanskrit/README.md
==================
14911e2e1;Zhen Wang;2020-05-02 22:52:12 +0800;Create README.md (#4112)

==

model_cards/wptoux/albert-chinese-large-qa/README.md
==================
9e97c8753;HUSEIN ZOLKEPLI;2020-05-02 22:51:15 +0800;Added huseinzol05/gpt2-345M-bahasa-cased (#4102)

==

model_cards/huseinzol05/gpt2-345M-bahasa-cased/README.md
==================
4c5bd9218;William Falcon;2020-05-02 10:38:30 -0400;Update run_pl_glue.py (#4117)

==

examples/glue/run_pl_glue.py
==================
5282b31df;William Falcon;2020-05-02 10:38:21 -0400;Update run_pl_ner.py (#4118)

==

examples/ner/run_pl_ner.py
==================
1e616c0af;Stefan Schweter;2020-05-02 16:29:17 +0200;NER: parse args from .args file or JSON (#4110)
* ner: parse args from .args file or JSON

* examples: mention json-based configuration file support for run_ner script
==

examples/ner/README.md
examples/ner/run_ner.py
==================
abb1fa3f3;Patrick von Platen;2020-05-02 10:32:00 +0200;Update README.md

==

model_cards/google/reformer-crime-and-punishment/README.md
==================
0ccbfd286;Patrick von Platen;2020-05-02 10:31:00 +0200;Update Reformer ReadME

==

model_cards/google/reformer-crime-and-punishment/README.md
==================
2d8340a91;Patrick von Platen;2020-05-02 10:25:22 +0200;[Reformer] Move model card to google model (#4113)
* correct model card

* remove model card from patrick von platen
==

model_cards/google/reformer-crime-and-punishment/README.md
==================
d713cfc5e;Julien Chaumond;2020-05-01 11:48:58 -0400;GePpeTto üáÆüáπ: Fixpath to model card

==

model_cards/LorenzoDeMattei/GePpeTto/README.md
==================
f3d44301c;Lorenzo De Mattei;2020-05-01 17:46:42 +0200;GePpeTto model üáÆüáπ (#4099)
* Create GePpeTto.md

* Update model_cards/LorenzoDeMattei/GePpeTto.md

* Update model_cards/LorenzoDeMattei/GePpeTto.md

Co-authored-by: Julien Chaumond <chaumond@gmail.com>
==

model_cards/LorenzoDeMattei/GePpeTto.md
==================
27d55125e;Julien Chaumond;2020-05-01 11:28:55 -0400;Configs: saner num_labels in configs. (#3967)

==

src/transformers/configuration_utils.py
==================
e80be7f1d;Stefan Schweter;2020-05-01 17:06:58 +0200;docs: add xlm-roberta section to multi-lingual section (#4101)

==

docs/source/multilingual.rst
==================
18db92dd9;Sam Shleifer;2020-05-01 09:05:47 -0400;[testing] add timeout_decorator (#3543)

==

setup.cfg
setup.py
tests/test_modeling_bart.py
==================
b8686174b;Julien Chaumond;2020-04-30 22:40:13 -0400;Merge pull request #3934 from huggingface/examples_args_from_files
[qol] example scripts: parse args from .args file or JSON
==

examples/run_glue.py
src/transformers/hf_argparser.py
==================
f39217a5e;Julien Chaumond;2020-04-23 17:16:55 -0400;[tests] Light cleanup of tempfile in tests/

==

tests/test_tokenization_common.py
==================
f54dc3f4d;Julien Chaumond;2020-04-23 17:12:59 -0400;[ci] Load pretrained models into the default (long-lived) cache
There's an inconsistency right now where:
- we load some models into CACHE_DIR
- and some models in the default cache
- and often, in both for the same models

When running the RUN_SLOW tests, this takes a lot of disk space, time, and bandwidth.

I'd rather always use the default cache

==

tests/test_modeling_albert.py
tests/test_modeling_bart.py
tests/test_modeling_bert.py
tests/test_modeling_ctrl.py
tests/test_modeling_distilbert.py
tests/test_modeling_electra.py
tests/test_modeling_flaubert.py
tests/test_modeling_gpt2.py
tests/test_modeling_openai.py
tests/test_modeling_roberta.py
tests/test_modeling_t5.py
tests/test_modeling_tf_albert.py
tests/test_modeling_tf_bert.py
tests/test_modeling_tf_ctrl.py
tests/test_modeling_tf_distilbert.py
tests/test_modeling_tf_electra.py
tests/test_modeling_tf_gpt2.py
tests/test_modeling_tf_openai_gpt.py
tests/test_modeling_tf_roberta.py
tests/test_modeling_tf_t5.py
tests/test_modeling_tf_transfo_xl.py
tests/test_modeling_tf_xlm.py
tests/test_modeling_tf_xlnet.py
tests/test_modeling_transfo_xl.py
tests/test_modeling_xlm.py
tests/test_modeling_xlnet.py
tests/utils.py
==================
6b410bedf;Scottish_Fold007;2020-05-01 10:26:58 +0800;Model Card: gaochangkuan  README.md (#4033)
* Create README.md

* Update README.md

* tweak

Co-authored-by: Julien Chaumond <chaumond@gmail.com>
==

model_cards/gaochangkuan/model_dir/README.md
==================
8829ace4a;husein zolkepli;2020-04-30 20:36:12 +0800;added gpt2 117m bahasa readme
(cherry picked from commit a4a673a1d0bec0bf4085eef021acb788ca1f5eb5)

==

model_cards/huseinzol05/gpt2-117M-bahasa-cased/README.md
==================
1851a64b6;Benjamin Muller;2020-04-29 11:45:34 +0800;create model_card camembert-base-wikipedia-4gb

==

model_cards/camembert/camembert-base-wikipedia-4gb/README.md
==================
443e5e34a;Benjamin Muller;2020-04-29 11:44:12 +0800;Create README.md

==

model_cards/camembert/camembert-base-ccnet/README.md
==================
60e1556a4;Benjamin Muller;2020-04-29 11:42:52 +0800;Create model_card camembert-base-ccnet-4gb

==

model_cards/camembert/camembert-base-ccnet-4gb/README.md
==================
fa9365eca;Benjamin Muller;2020-04-29 11:38:47 +0800;Create README.md

==

model_cards/camembert/camembert-base-oscar-4gb/README.md
==================
afe002b04;Benjamin Muller;2020-04-29 11:31:27 +0800;Create README.md

==

model_cards/camembert/camembert-large/README.md
==================
8b5e5ebcf;Suraj Parmar;2020-05-01 07:44:08 +0530;Continue training args and tqdm in notebooks (#3939)
* Continue training args

* Continue training args

* added explaination

* added explaination

* added explaination

* Fixed tqdm auto

* Update src/transformers/training_args.py

Co-Authored-By: Julien Chaumond <chaumond@gmail.com>

* Update src/transformers/training_args.py

* Update src/transformers/training_args.py

Co-authored-by: Julien Chaumond <chaumond@gmail.com>
==

src/transformers/trainer.py
src/transformers/training_args.py
==================
ab90353f1;Julien Chaumond;2020-04-30 12:51:06 -0400;[cli] {login, upload, s3} display more helpful error messages

==

src/transformers/commands/user.py
tests/test_hf_api.py
==================
452dd0e4d;Julien Chaumond;2020-04-30 12:06:01 -0400;[ci] Align test_hf_api.py with API change

==

tests/test_hf_api.py
==================
7f9193ef0;Jordan;2020-04-30 07:33:09 -0500;Fixed Style Inconsistency (#3976)

==

src/transformers/modeling_bert.py
==================
64070cbb8;Jared T Nielsen;2020-04-30 06:28:56 -0600;Fix TF input docstrings to refer to tf.Tensor rather than torch.FloatTensor. (#4051)

==

README.md
src/transformers/modeling_tf_albert.py
src/transformers/modeling_tf_bert.py
src/transformers/modeling_tf_distilbert.py
src/transformers/modeling_tf_electra.py
src/transformers/modeling_tf_roberta.py
src/transformers/modeling_tf_t5.py
src/transformers/modeling_tf_transfo_xl.py
src/transformers/modeling_tf_utils.py
==================
e73595bd6;Lysandre Debut;2020-04-29 09:53:19 -0400;Remove jitted method so that our models are pickable. (#4050)

==

src/transformers/activations.py
==================
2c7784288;Sam Shleifer;2020-04-29 09:47:20 -0400;[Fix common tests on GPU] send model, ids to torch_device (#4014)

==

tests/test_modeling_common.py
==================
6faca88ee;Julien Chaumond;2020-04-28 20:29:47 -0400;Align MarianMT with #4030
cc @sshleifer

==

src/transformers/modeling_marian.py
==================
211e13081;Julien Chaumond;2020-04-28 20:34:34 -0400;[github] Issue templates: populate some labels
cc @bramvanroy @stefan-it
==

.github/ISSUE_TEMPLATE/--new-model-addition.md
.github/ISSUE_TEMPLATE/migration.md
.github/ISSUE_TEMPLATE/question-help.md
==================
455c63909;Julien Chaumond;2020-04-28 20:27:14 -0400;CDN urls (#4030)
* [file_utils] use_cdn + documentation

* Move to cdn. urls for weights

* [urls] Hotfix for bert-base-japanese
==

examples/summarization/bertabs/modeling_bertabs.py
src/transformers/configuration_bert.py
src/transformers/configuration_utils.py
src/transformers/file_utils.py
src/transformers/modelcard.py
src/transformers/modeling_albert.py
src/transformers/modeling_bart.py
src/transformers/modeling_bert.py
src/transformers/modeling_camembert.py
src/transformers/modeling_distilbert.py
src/transformers/modeling_electra.py
src/transformers/modeling_flaubert.py
src/transformers/modeling_gpt2.py
src/transformers/modeling_openai.py
src/transformers/modeling_roberta.py
src/transformers/modeling_t5.py
src/transformers/modeling_tf_albert.py
src/transformers/modeling_tf_bert.py
src/transformers/modeling_tf_ctrl.py
src/transformers/modeling_tf_distilbert.py
src/transformers/modeling_tf_electra.py
src/transformers/modeling_tf_gpt2.py
src/transformers/modeling_tf_openai.py
src/transformers/modeling_tf_roberta.py
src/transformers/modeling_tf_t5.py
src/transformers/modeling_tf_transfo_xl.py
src/transformers/modeling_tf_utils.py
src/transformers/modeling_tf_xlm.py
src/transformers/modeling_tf_xlnet.py
src/transformers/modeling_transfo_xl.py
src/transformers/modeling_utils.py
src/transformers/modeling_xlm.py
src/transformers/modeling_xlm_roberta.py
src/transformers/modeling_xlnet.py
src/transformers/tokenization_auto.py
src/transformers/tokenization_bert_japanese.py
src/transformers/tokenization_utils.py
templates/adding_a_new_model/modeling_tf_xxx.py
templates/adding_a_new_model/modeling_xxx.py
==================
8ba4c5885;Thomas Wolf;2020-04-29 01:13:59 +0200;Allow a more backward compatible behavior of max_len_single_sentence and max_len_sentences_pair (#3994)
* Allow a more backward compatible behavior of max_len_single_sentence and max_len_sentences_pair and

* The style and quality are now top-notch
==

src/transformers/tokenization_utils.py
==================
847e7f337;Sam Shleifer;2020-04-28 18:22:37 -0400;MarianMTModel.from_pretrained('Helsinki-NLP/opus-marian-en-de') (#3908)
Co-Authored-By: Stefan Schweter <stefan@schweter.it>
==

setup.cfg
src/transformers/__init__.py
src/transformers/configuration_bart.py
src/transformers/configuration_marian.py
src/transformers/convert_marian_to_pytorch.py
src/transformers/modeling_bart.py
src/transformers/modeling_marian.py
src/transformers/modeling_utils.py
src/transformers/tokenization_marian.py
src/transformers/tokenization_utils.py
tests/test_modeling_bart.py
tests/test_modeling_marian.py
==================
d714dfeaa;Sam Shleifer;2020-04-28 17:12:00 -0400;[isort] add known 3rd party to setup.cfg (#4053)
* add known 3rd party to setup.cfg

* comment

* Update CONTRIBUTING.md

Co-authored-by: Julien Chaumond <chaumond@gmail.com>
==

CONTRIBUTING.md
examples/summarization/t5/evaluate_cnn.py
examples/translation/t5/evaluate_wmt.py
setup.cfg
==================
d52b0e294;MichalMalyska;2020-04-28 16:42:15 -0400;Minor Readme Fixes (#4056)
Added contact info and fixed typos.
==

model_cards/NLP4H/ms_bert/README.md
==================
55adefe42;Alex Combessie;2020-04-28 22:40:21 +0200;Add license information to model cards (#3864)
Close #3357
==

model_cards/albert-base-v1-README.md
model_cards/albert-xxlarge-v2-README.md
model_cards/bert-base-cased-README.md
model_cards/bert-base-chinese-README.md
model_cards/bert-base-german-dbmdz-cased-README.md
model_cards/bert-base-german-dbmdz-uncased-README.md
model_cards/bert-base-multilingual-cased-README.md
model_cards/bert-base-multilingual-uncased-README.md
model_cards/bert-base-uncased-README.md
model_cards/bert-large-cased-README.md
model_cards/camembert-base-README.md
model_cards/distilbert-base-multilingual-cased-README.md
model_cards/distilbert-base-uncased-README.md
model_cards/distilgpt2-README.md
model_cards/distilroberta-base-README.md
model_cards/google/electra-base-discriminator/README.md
model_cards/google/electra-base-generator/README.md
model_cards/google/electra-large-discriminator/README.md
model_cards/google/electra-large-generator/README.md
model_cards/google/electra-small-discriminator/README.md
model_cards/google/electra-small-generator/README.md
model_cards/gpt2-README.md
model_cards/iuliaturc/bert_uncased_L-2_H-128_A-2/README.md
model_cards/microsoft/DialoGPT-large/README.md
model_cards/microsoft/DialoGPT-medium/README.md
model_cards/microsoft/DialoGPT-small/README.md
model_cards/roberta-base-README.md
model_cards/t5-11b-README.md
model_cards/t5-3b-README.md
model_cards/t5-base-README.md
model_cards/t5-large-README.md
model_cards/t5-small-README.md
model_cards/xlm-mlm-en-2048-README.md
model_cards/xlm-roberta-base-README.md
==================
0ac6d0bf3;ydaigo;2020-04-28 21:56:17 +0900;Create README.md
I create japanese binary classification.
==

model_cards/daigo/bert-base-japanese-sentiment/README.md
==================
c73c83b0e;Louis MARTIN;2020-04-18 02:53:45 -0700;Small cosmetic changes to CamemBERT model card

==

model_cards/camembert-base-README.md
==================
4a94c062a;Bogdan Kostiƒá;2020-04-27 17:47:31 +0200;Provide model card for roberta-base-squad2-covid

==

model_cards/deepset/roberta-base-squad2-covid/README.md
==================
c7d06b79a;jazzcook15;2020-04-28 12:18:56 -0700;Fix #3954 - GPT2 is not traceable (#3955)
* Update sqrt computation so it can survive a torch.jit.trace

* Update modeling_gpt2.py

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
==

src/transformers/modeling_gpt2.py
==================
9a0a8c1c6;Patrick von Platen;2020-04-28 16:33:23 +0200;add examples to doc (#4045)

==

src/transformers/configuration_encoder_decoder.py
src/transformers/modeling_encoder_decoder.py
==================
fa49b9afe;Patrick von Platen;2020-04-28 15:11:09 +0200;Clean Encoder-Decoder models with Bart/T5-like API and add generate possibility (#3383)
* change encoder decoder style to bart & t5 style

* make encoder decoder generation dummy work for bert

* make style

* clean init config in encoder decoder

* add tests for encoder decoder models

* refactor and add last tests

* refactor and add last tests

* fix attn masks for bert encoder decoder

* make style

* refactor prepare inputs for Bert

* refactor

* finish encoder decoder

* correct typo

* add docstring to config

* finish

* add tests

* better naming

* make style

* fix flake8

* clean docstring

* make style

* rename
==

docs/source/index.rst
docs/source/model_doc/encoderdecoder.rst
src/transformers/__init__.py
src/transformers/configuration_auto.py
src/transformers/configuration_encoder_decoder.py
src/transformers/modeling_auto.py
src/transformers/modeling_bert.py
src/transformers/modeling_encoder_decoder.py
src/transformers/modeling_utils.py
src/transformers/utils_encoder_decoder.py
tests/test_modeling_encoder_decoder.py
==================
180585741;Patrick von Platen;2020-04-28 14:33:15 +0200;[Generation] Generation should allow to start with empty prompt (#3993)
* fix empty prompt

* fix length in generation pipeline
==

examples/run_generation.py
src/transformers/pipelines.py
==================
52679fbc2;Patrick von Platen;2020-04-28 14:32:31 +0200;add dialogpt training tips (#3996)

==

docs/source/model_doc/dialogpt.rst
==================
b5c6d3d4c;Stefan Schweter;2020-04-28 09:12:25 +0200;notebooks: minor fix for community provided models example (#4025)

==

notebooks/02-transformers.ipynb
==================
2fade302a;martindh;2020-04-27 10:30:59 +0200;camembert-base-fquad
Model card for illuin release of camembert-base-fquad
==

model_cards/illuin/camembert-base-fquad/README.md
==================
20c3b8cab;Manuel Romero;2020-04-27 10:26:59 +0200;Create model card

==

model_cards/mrm8488/distilbert-base-multi-cased-finetuned-typo-detection/README.md
==================
b3f272ffc;Manuel Romero;2020-04-27 10:15:15 +0200;Create model card

==

model_cards/mrm8488/bert-small-finetuned-typo-detection/README.md
==================
518f291ee;Nick Doiron;2020-04-26 17:30:48 -0400;add model card for Hindi-BERT

==

model_cards/monsoon-nlp/hindi-bert/README.md
==================
d7b3bf547;monologg;2020-04-27 02:07:26 +0900;Model cards for KoELECTRA

==

model_cards/monologg/koelectra-base-discriminator/README.md
model_cards/monologg/koelectra-base-generator/README.md
model_cards/monologg/koelectra-small-discriminator/README.md
model_cards/monologg/koelectra-small-generator/README.md
==================
db9d56c08;Sai Saketh Aluru;2020-04-28 03:48:54 +0530;Add modelcard for Hate-speech-CNERG/dehatebert-mono-arabic model (#3979)
* Add dehatebert-mono-arabic readme card

* Update dehatebert-mono-arabic model card
==

model_cards/Hate-speech-CNERG/dehatebert-mono-arabic/README.md
==================
41750a6cf;sshleifer;2020-04-22 17:50:18 -0400;Fix typos

==

src/transformers/modeling_utils.py
==================
12bb7fe77;Lorenzo Ampil;2020-04-28 00:27:15 +0800;Fix t5 doc typos (#3978)
* Fix tpo in into and add line under

* Add missing blank line under

* Correct types under
==

docs/source/model_doc/t5.rst
==================
97a375484;Julien Chaumond;2020-04-25 10:43:44 -0400;rm boto3 dependency

==

.github/workflows/github-torch-hub.yml
hubconf.py
setup.py
src/transformers/file_utils.py
==================
4e817ff41;Txus;2020-04-25 15:16:40 +0200;Create README.md (#3966)

==

model_cards/codegram/calbert-base-uncased/README.md
==================
73d6a2f90;Junyi_Li;2020-04-24 16:12:42 -0400;[model_cards] xlnet_chinese_large & roberta_chinese_large

==

model_cards/clue/albert_chinese_small/README.md
model_cards/clue/albert_chinese_tiny/README.md
model_cards/clue/roberta_chinese_3L312_clue_tiny/README.md
model_cards/clue/roberta_chinese_base/README.md
model_cards/clue/roberta_chinese_large/README.md
model_cards/clue/xlnet_chinese_large/README.md
==================
623ba0236;Manuel Romero;2020-04-24 21:57:01 +0200;Create README.md (#3882)

==

model_cards/mrm8488/roberta-large-finetuned-wsc/README.md
==================
f4078e0db;Leandro von Werra;2020-04-24 16:24:28 +0200;Feat/add model card (#3923)
* add model card for gpt2-imdb-ctrl

* fix title

* add sentiment control description
==

model_cards/lvwerra/gpt2-imdb-ctrl/README.md
==================
03322b426;YuvalPeleg;2020-04-24 17:24:00 +0300;Create README.md (#3917)

==

model_cards/SparkBeyond/roberta-large-sts-b/README.md
==================
c81152600;Julien Chaumond;2020-04-24 09:52:42 -0400;[examples] For convenience, also save the tokenizer
Close #3921

==

examples/ner/run_ner.py
examples/run_glue.py
examples/run_language_modeling.py
examples/run_multiple_choice.py
==================
b0167632c;Cola;2020-04-24 20:55:34 +0900;Shuffle train subset for summarization example (#3909)
* Shuffle train subset

* Cleaner shuffle
==

examples/summarization/bart/finetune.py
==================
c53cc018d;Julien Chaumond;2020-04-23 23:59:43 +0000;[Trainer] Fix _rotate_checkpoints
Close #3920

==

src/transformers/trainer.py
==================
cbbb3c43c;Julien Chaumond;2020-04-23 16:27:43 -0400;[hubconf] Modify pythonpath to get canonical imports to work
See https://github.com/huggingface/transformers/pull/3881/files#r412292660

Should we remove SRC_DIR from sys.path right after the imports, @aaugustin?

==

hubconf.py
==================
77b75d2c7;mneilly-et;2020-04-23 12:25:31 -0600;Fix for #3873 to change type of exponent parameter for torch.pow() call from int to float (#3924)

==

src/transformers/activations.py
==================
6ba254ee5;Clement;2020-04-23 14:19:45 -0400;quick fix wording readme for community models (#3900)

==

README.md
==================
a79a9e124;Jared T Nielsen;2020-04-23 11:18:16 -0600;Fix TFAlbertForSequenceClassification classifier dropout probability. It was set to config.hidden_dropout_prob, but should be config.classifier_dropout_prob. (#3928)

==

src/transformers/modeling_tf_albert.py
==================
8e093e598;peterandluc;2020-04-23 17:10:57 +0200;Remove 50k limits bug

==

src/transformers/data/datasets/language_modeling.py
==================
6af5a54c2;Julien Chaumond;2020-04-23 11:02:05 -0400;[Trainer] reuse constant

==

src/transformers/trainer.py
==================
7c2a32ff8;Julien Chaumond;2020-04-23 10:43:22 -0400;[housekeeping] super()

==

src/transformers/modeling_bert.py
src/transformers/modeling_flaubert.py
src/transformers/modeling_tf_flaubert.py
==================
a946b6b51;Julien Chaumond;2020-04-23 10:39:24 -0400;[housekeeping] Upgrade `# type` Python 2 syntax
cc @sshleifer

==

src/transformers/configuration_utils.py
src/transformers/modeling_bart.py
==================
cb3c2212c;Manuel Romero;2020-04-22 20:56:43 +0200;Create model card (#3890)
Model: TinyBERT-spanish-uncased-finetuned-ner
==

model_cards/mrm8488/TinyBERT-spanish-uncased-finetuned-ner/README.md
==================
d698b87f2;Manuel Romero;2020-04-22 20:54:17 +0200;Update comparison table (#3889)

==

model_cards/mrm8488/bert-spanish-cased-finetuned-ner/README.md
==================
13dd2acca;Anthony MOI;2020-04-22 11:02:29 -0400;Bump tokenizers version to final 0.7.0 (#3898)

==

setup.py
==================
f16540fcb;Lorenzo Ampil;2020-04-22 21:37:03 +0800;Pipeline for Text Generation: GenerationPipeline (#3758)
* Add GenerationPipeline

* Fix parameter names

* Correct parameter __call__ parameters

* Add model type attribute and correct function calls for prepare_input

* Take out trailing commas from init attributes

* Remove unnecessary tokenization line

* Implement support for multiple text inputs

* Apply generation support for multiple input text prompts

* Take out tensor coersion

* Take out batch index

* Add text prompt to return sequence

* Squeeze token tensore before decoding

* Return only a single list of sequences if only one prompt was used

* Correct results variable name

* Add GenerationPipeline to SUPPORTED_TASKS with the alias , initalized w GPT2

* Registedred AutoModelWithLMHead for both pt and t

* Update docstring for GenerationPipeline

* Add kwargs parameter to mode.generate

* Take out kwargs parameter after all

* Add generation pipeline example in pipeline docstring

* Fix max length by squeezing tokens tensor

* Apply ensure_tensor_on_device to pytorch tensor

* Include generation step in torch.no_grad

* Take out input from prepare_xlm_input and set 'en' as default xlm_language

* Apply framework specific encoding during prepare_input

* Format w make style

* Move GenerationPipeline import to follow proper import sorting

* Take out training comma from generation dict

* Apply requested changes

* Change name to TextGenerationPipeline

* Apply TextGenerationPipeline rename to __init___

* Changing alias to

* Set input mapping as input to ensure_tensor_on_device

* Fix assertion placement

* Add test_text_generation

* Add TextGenerationPipeline to PipelineCommonTests

* Take out whitespace

* Format __init__ w black

* Fix __init__ style

* Forman __init___

* Add line to end of __init__

* Correct model tokenizer set for test_text_generation

* Ensure to return list of list, not list of string (to pass test)

* Limit test models to only 3 to limit runtime to address circleCI timeout error

* Update src/transformers/pipelines.py

Co-Authored-By: Patrick von Platen <patrick.v.platen@gmail.com>

* Update src/transformers/pipelines.py

Co-Authored-By: Patrick von Platen <patrick.v.platen@gmail.com>

* Update src/transformers/pipelines.py

Co-Authored-By: Patrick von Platen <patrick.v.platen@gmail.com>

* Update src/transformers/pipelines.py

Co-Authored-By: Patrick von Platen <patrick.v.platen@gmail.com>

* Update src/transformers/pipelines.py

Co-Authored-By: Patrick von Platen <patrick.v.platen@gmail.com>

* Update tests/test_pipelines.py

Co-Authored-By: Patrick von Platen <patrick.v.platen@gmail.com>

* Update src/transformers/pipelines.py

Co-Authored-By: Patrick von Platen <patrick.v.platen@gmail.com>

* Update src/transformers/pipelines.py

Co-Authored-By: Patrick von Platen <patrick.v.platen@gmail.com>

* Update src/transformers/pipelines.py

Co-Authored-By: Patrick von Platen <patrick.v.platen@gmail.com>

* Remove argument docstring, __init__, add additional __call__ arguments, and reformat results to list of dict

* Fix blank result list

* Add TextGenerationPipeline to pipelines.rst

* Update src/transformers/pipelines.py

Co-Authored-By: Patrick von Platen <patrick.v.platen@gmail.com>

* Update src/transformers/pipelines.py

Co-Authored-By: Patrick von Platen <patrick.v.platen@gmail.com>

* Fix typos from adding PADDING_TEXT_TOKEN_LENGTH

* Fix incorrectly moved result list

* Update src/transformers/pipelines.py

Co-Authored-By: Patrick von Platen <patrick.v.platen@gmail.com>

* Update src/transformers/pipelines.py

* Update src/transformers/pipelines.py

* Update src/transformers/pipelines.py

* Update src/transformers/pipelines.py

* Update src/transformers/pipelines.py

* Update src/transformers/pipelines.py

* Update src/transformers/pipelines.py

* Update src/transformers/pipelines.py

* Update src/transformers/pipelines.py

* Update src/transformers/pipelines.py

* Update src/transformers/pipelines.py

* Update src/transformers/pipelines.py

Co-Authored-By: Patrick von Platen <patrick.v.platen@gmail.com>

* Add back generation line and make style

* Take out blank whitespace

* Apply new alis, text-generation, to test_pipelines

* Fix text generation alias in test

* Update src/transformers/pipelines.py

Co-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>
Co-authored-by: Julien Chaumond <chaumond@gmail.com>
==

docs/source/main_classes/pipelines.rst
src/transformers/__init__.py
src/transformers/pipelines.py
tests/test_pipelines.py
==================
1dc9b3c78;Julien Chaumond;2020-04-22 01:15:10 +0000;Fixes #3877

==

docs/source/bertology.rst
examples/run_bertology.py
==================
dd9d483d0;Julien Chaumond;2020-04-21 20:11:56 -0400;Trainer (#3800)
* doc

* [tests] Add sample files for a regression task

* [HUGE] Trainer

* Feedback from @sshleifer

* Feedback from @thomwolf + logging tweak

* [file_utils] when downloading concurrently, get_from_cache will use the cached file for subsequent processes

* [glue] Use default max_seq_length of 128 like before

* [glue] move DataTrainingArguments around

* [ner] Change interface of InputExample, and align run_{tf,pl}

* Re-align the pl scripts a little bit

* ner

* [ner] Add integration test

* Fix language_modeling with API tweak

* [ci] Tweak loss target

* Don't break console output

* amp.initialize: model must be on right device before

* [multiple-choice] update for Trainer

* Re-align to 827d6d6ef071029cfe82838a18dab046b5813976
==

.gitignore
README.md
examples/README.md
examples/glue/run_pl.sh
examples/glue/run_pl_glue.py
examples/ner/README.md
examples/ner/run.sh
examples/ner/run_ner.py
examples/ner/run_pl.sh
examples/ner/run_pl_ner.py
examples/ner/run_tf_ner.py
examples/ner/test_ner_examples.py
examples/ner/utils_ner.py
examples/run_glue.py
examples/run_language_modeling.py
examples/run_multiple_choice.py
examples/summarization/bart/finetune.py
examples/summarization/bart/run_train.sh
examples/test_examples.py
examples/tests_samples/.gitignore
examples/tests_samples/GermEval/dev.txt
examples/tests_samples/GermEval/labels.txt
examples/tests_samples/GermEval/train.txt
examples/tests_samples/STS-B/dev.tsv
examples/tests_samples/STS-B/train.tsv
examples/transformer_base.py
examples/utils_multiple_choice.py
src/transformers/__init__.py
src/transformers/configuration_utils.py
src/transformers/data/data_collator.py
src/transformers/data/datasets/__init__.py
src/transformers/data/datasets/glue.py
src/transformers/data/datasets/language_modeling.py
src/transformers/data/processors/glue.py
src/transformers/data/processors/utils.py
src/transformers/file_utils.py
src/transformers/modeling_auto.py
src/transformers/trainer.py
src/transformers/training_args.py
tests/test_hf_argparser.py
tests/test_trainer.py
==================
eb5601b0a;Julien Chaumond;2020-04-21 15:46:18 -0400;[ci] Pin torch version while we update

==

setup.py
==================
53f5ef6df;Spencer Adams;2020-04-21 14:31:36 -0500;create readme for spentaur/yelp model (#3874)
* create readme for spentaur/yelp model

* update spentaur/yelp/README.md

* remove typo
==

model_cards/spentaur/yelp/README.md
==================
d32585a30;Julien Chaumond;2020-04-21 10:49:00 -0400;Fix Torch.hub + Integration test

==

.github/workflows/github-torch-hub.yml
hubconf.py
src/transformers/modeling_albert.py
src/transformers/modeling_electra.py
src/transformers/tokenization_camembert.py
src/transformers/tokenization_xlm_roberta.py
==================
7d40901ce;Bharat Raghunathan;2020-04-21 12:38:20 +0530;Fix Documentation issue in BertForMaskedLM forward (#3855)

==

src/transformers/modeling_bert.py
==================
b1ff0b2ae;Andrey Kulagin;2020-04-17 17:33:24 +0300;Fix bug in examples: double wrap into DataParallel during eval

==

examples/hans/test_hans.py
examples/mm-imdb/run_mmimdb.py
examples/ner/run_ner.py
examples/run_language_modeling.py
examples/run_multiple_choice.py
examples/run_xnli.py
==================
7f23af168;husein zolkepli;2020-04-19 16:47:09 +0800;added electra model
(cherry picked from commit b5f2dc5d627d44b8cbb0ccf8ad2b46bea211a236)

==

model_cards/huseinzol05/electra-base-discriminator-bahasa-cased/README.md
model_cards/huseinzol05/electra-base-generator-bahasa-cased/README.md
model_cards/huseinzol05/electra-small-discriminator-bahasa-cased/README.md
model_cards/huseinzol05/electra-small-generator-bahasa-cased/README.md
==================
03121deba;Punyajoy Saha;2020-04-20 07:39:12 +0530;New model added
The first model added to the repo
==

model_cards/Hate-speech-CNERG/dehatebert-mono-english/README.md
==================
15b9868f8;Manuel Romero;2020-04-14 01:00:01 +0200;Create model card

==

model_cards/mrm8488/gpt2-imdb-neutral/README.md
==================
2c05b8a56;Funtowicz Morgan;2020-04-20 20:58:52 +0000;Remove tqdm logging when using pipelines. (#3833)
Introduce tqdm_enabled parameter on squad_convert_examples_to_features() default to True and set to False in QA pipelines.
==

src/transformers/data/processors/squad.py
src/transformers/pipelines.py
==================
c79b550dd;Jared T Nielsen;2020-04-20 14:08:57 -0600;Add `qas_id` to SquadResult and SquadExample (#3745)
* Add qas_id

* Fix incorrect name in squad.py

* Make output files optional for squad eval
==

examples/run_squad.py
src/transformers/data/metrics/squad_metrics.py
src/transformers/data/processors/squad.py
==================
c4158a631;Patrick von Platen;2020-04-20 20:39:16 +0200;[Pipelines] Encode to max length of input not max length of tokenizer for batch input (#3857)
* remove max_length = tokenizer.max_length when encoding

* make style
==

src/transformers/pipelines.py
==================
857ccdb25;Mohamed El-Geish;2020-04-20 07:54:39 -0700;exbert links for my albert model cards (#3729)
* exbert links for my albert model cards

* Added exbert tag to the metadata block

* Adding "how to cite"
==

model_cards/elgeish/cs224n-squad2.0-albert-base-v2/README.md
model_cards/elgeish/cs224n-squad2.0-albert-large-v2/README.md
model_cards/elgeish/cs224n-squad2.0-albert-xxlarge-v1/README.md
model_cards/elgeish/cs224n-squad2.0-distilbert-base-uncased/README.md
model_cards/elgeish/cs224n-squad2.0-roberta-base/README.md
==================
a504cb49e;Sam Shleifer;2020-04-20 10:49:56 -0400;[examples] fix summarization do_predict (#3866)

==

examples/summarization/bart/finetune.py
examples/summarization/bart/test_bart_examples.py
examples/transformer_base.py
==================
52c85f847;ahotrod;2020-04-19 10:49:24 -0700;Update README.md

==

model_cards/ahotrod/roberta_large_squad2/README.md
==================
a21d4fa41;Patrick von Platen;2020-04-18 18:07:17 +0200;add "by" to ReadMe

==

README.md
==================
827d6d6ef;Thomas Wolf;2020-04-18 13:43:57 +0200;Cleanup fast tokenizers integration (#3706)
* First pass on utility classes and python tokenizers

* finishing cleanup pass

* style and quality

* Fix tests

* Updating following @mfuntowicz comment

* style and quality

* Fix Roberta

* fix batch_size/seq_length inBatchEncoding

* add alignement methods + tests

* Fix OpenAI and Transfo-XL tokenizers

* adding trim_offsets=True default for GPT2 et RoBERTa

* style and quality

* fix tests

* add_prefix_space in roberta

* bump up tokenizers to rc7

* style

* unfortunately tensorfow does like these - removing shape/seq_len for now

* Update src/transformers/tokenization_utils.py

Co-Authored-By: Stefan Schweter <stefan@schweter.it>

* Adding doc and docstrings

* making flake8 happy

Co-authored-by: Stefan Schweter <stefan@schweter.it>
==

docs/source/main_classes/tokenizer.rst
docs/source/model_doc/bert.rst
docs/source/model_doc/distilbert.rst
docs/source/model_doc/electra.rst
docs/source/model_doc/gpt.rst
docs/source/model_doc/gpt2.rst
docs/source/model_doc/roberta.rst
docs/source/model_doc/transformerxl.rst
examples/run_language_modeling.py
setup.py
src/transformers/tokenization_albert.py
src/transformers/tokenization_bert.py
src/transformers/tokenization_bert_japanese.py
src/transformers/tokenization_camembert.py
src/transformers/tokenization_ctrl.py
src/transformers/tokenization_distilbert.py
src/transformers/tokenization_electra.py
src/transformers/tokenization_gpt2.py
src/transformers/tokenization_openai.py
src/transformers/tokenization_roberta.py
src/transformers/tokenization_t5.py
src/transformers/tokenization_transfo_xl.py
src/transformers/tokenization_utils.py
src/transformers/tokenization_xlm.py
src/transformers/tokenization_xlm_roberta.py
src/transformers/tokenization_xlnet.py
templates/adding_a_new_model/tokenization_xxx.py
tests/test_tokenization_fast.py
==================
60a42ef1c;Julien Chaumond;2020-04-17 20:21:15 -0400;[model_cards] Fix CamemBERT table markdown
see https://github.com/huggingface/transformers/pull/3836

==

model_cards/camembert-base-README.md
==================
88aecee6a;Julien Chaumond;2020-04-17 20:16:00 -0400;[ci] GitHub-hosted runner has no space left on device

==

.github/workflows/github-push.yml
==================
73efa694e;Benjamin Muller;2020-04-18 08:08:13 +0800;Update camembert-base-README.md (#3836)

==

model_cards/camembert-base-README.md
==================
e9d0bc027;Patrick von Platen;2020-04-18 02:07:18 +0200;[Config, Serialization] more readable config serialization (#3797)
* better config serialization

* finish configuration utils
==

src/transformers/configuration_utils.py
==================
8b63a01d9;Lysandre Debut;2020-04-17 11:28:55 -0400;XLM tokenizer should encode with bos token (#3791)
* XLM tokenizer should encode with bos token

* Update tests
==

src/transformers/tokenization_xlm.py
tests/test_tokenization_xlm.py
==================
1d4a35b39;Patrick von Platen;2020-04-17 17:26:16 +0200;Higher tolerance for past testing in TF T5 (#3844)

==

tests/test_modeling_tf_t5.py
==================
d13eca11e;Patrick von Platen;2020-04-17 17:25:14 +0200;Higher tolerance for past testing in T5 (#3843)

==

tests/test_modeling_t5.py
==================
b0c9fbb29;Harutaka Kawamura;2020-04-18 00:23:18 +0900;Add workflow to build docs (#3763)

==

.circleci/config.yml
==================
c19727fd3;Santiago Castro;2020-04-17 11:17:21 -0400;Add support for the null answer in `QuestionAnsweringPipeline` (#3441)
* Add support for the null answer in `QuestionAnsweringPipeline`

* black

* Fix min null score computation

* Fix a PR comment
==

src/transformers/pipelines.py
==================
edf0582c0;Simon B√∂hm;2020-04-17 17:14:12 +0200;Fix token_type_id in BERT question-answering example (#3790)
token_type_id is converted into the segment embedding. For question answering,
this needs to highlight whether a token belongs to sequence 0 or 1.
encode_plus takes care of correctly setting this parameter automatically.
==

src/transformers/modeling_bert.py
src/transformers/modeling_tf_bert.py
==================
6d00033e9;Pierric Cistac;2020-04-17 10:45:30 -0400;Question Answering support for Albert and Roberta in TF (#3812)
* Add TFAlbertForQuestionAnswering

* Add TFRobertaForQuestionAnswering

* Update TFAutoModel with Roberta/Albert for QA

* Clean `super` TF Albert calls
==

src/transformers/__init__.py
src/transformers/modeling_tf_albert.py
src/transformers/modeling_tf_auto.py
src/transformers/modeling_tf_roberta.py
tests/test_modeling_tf_albert.py
tests/test_modeling_tf_roberta.py
==================
f399c0061;Patrick von Platen;2020-04-17 09:42:22 +0200;Update README

==

model_cards/patrickvonplaten/reformer-crime-and-punish/README.md
==================
f0c96fafd;Sam Shleifer;2020-04-16 15:15:19 -0400;[examples] summarization/bart/finetune.py supports t5 (#3824)
renames `run_bart_sum.py` to `finetune.py`
==

examples/summarization/bart/finetune.py
examples/summarization/bart/run_train.sh
examples/summarization/bart/run_train_tiny.sh
examples/summarization/bart/test_bart_examples.py
examples/summarization/t5/README.md
==================
0cec4fab7;Jonathan Sum;2020-04-15 08:25:09 -0700;typo: fine-grained token-leven
Changing from "fine-grained token-leven" to "fine-grained token-level"
==

notebooks/02-transformers.ipynb
==================
14cdeee75;Aryansh Omray;2020-04-16 20:53:46 +0530;Tanh torch warnings

==

src/transformers/activations.py
==================
16469fedb;Sam Shleifer;2020-04-16 15:02:43 -0400;[PretrainedTokenizer] Factor out tensor conversion method (#3777)

==

src/transformers/tokenization_utils.py
==================
80a169451;Patrick von Platen;2020-04-16 20:00:41 +0200;[Examples, T5] Change newstest2013 to newstest2014 and clean up  (#3817)
* Refactored use of newstest2013 to newstest2014. Fixed bug where argparse consumed first command line argument as model_size argument rather than using default model_size by forcing explicit --model_size flag inclusion

* More pythonic file handling through 'with' context

* COSMETIC - ran Black and isort

* Fixed reference to number of lines in newstest2014

* Fixed failing test. More pythonic file handling

* finish PR from tholiao

* remove outcommented lines

* make style

* make isort happy

Co-authored-by: Thomas Liao <tholiao@gmail.com>
==

examples/translation/t5/README.md
examples/translation/t5/evaluate_wmt.py
==================
d48679515;Lysandre Debut;2020-04-16 11:19:24 -0400;JIT not compatible with PyTorch/XLA (#3743)

==

src/transformers/activations.py
==================
b1e2368b3;Davide Fiocco;2020-04-16 17:04:32 +0200;Typo fix (#3821)

==

examples/run_glue.py
==================
baca8fa8e;Patrick von Platen;2020-04-16 16:21:34 +0200;clean pipelines (#3795)

==

src/transformers/pipelines.py
tests/test_pipelines.py
==================
38f7461df;Patrick von Platen;2020-04-16 16:14:52 +0200;[TFT5, Cache] Add cache to TFT5 (#3772)
* correct gpt2 test inputs

* make style

* delete modeling_gpt2 change in test file

* translate from pytorch

* correct tests

* fix conflicts

* fix conflicts

* fix conflicts

* fix conflicts

* make tensorflow t5 caching work

* make style

* clean reorder cache

* remove unnecessary spaces

* fix test
==

src/transformers/modeling_t5.py
src/transformers/modeling_tf_t5.py
src/transformers/modeling_tf_utils.py
tests/test_modeling_t5.py
tests/test_modeling_tf_gpt2.py
tests/test_modeling_tf_t5.py
==================
a5b249472;Patrick von Platen;2020-04-16 15:58:57 +0200;change pad token id to config pad token id (#3793)

==

src/transformers/modeling_bert.py
==================
dbd041243;Sam Shleifer;2020-04-16 09:55:25 -0400;[cleanup] factor out get_head_mask, invert_attn_mask, get_exten‚Ä¶ (#3806)
* Delete some copy pasted code
==

src/transformers/modeling_albert.py
src/transformers/modeling_bert.py
src/transformers/modeling_ctrl.py
src/transformers/modeling_distilbert.py
src/transformers/modeling_electra.py
src/transformers/modeling_flaubert.py
src/transformers/modeling_gpt2.py
src/transformers/modeling_mmbt.py
src/transformers/modeling_openai.py
src/transformers/modeling_t5.py
src/transformers/modeling_utils.py
src/transformers/modeling_xlm.py
templates/adding_a_new_model/modeling_xxx.py
==================
d22894dfd;Patrick von Platen;2020-04-16 09:04:32 +0200;[Docs] Add DialoGPT (#3755)
* add dialoGPT

* update README.md

* fix conflict

* update readme

* add code links to docs

* Update README.md

* Update dialo_gpt2.rst

* Update pretrained_models.rst

* Update docs/source/model_doc/dialo_gpt2.rst

Co-Authored-By: Julien Chaumond <chaumond@gmail.com>

* change filename of dialogpt

Co-authored-by: Julien Chaumond <chaumond@gmail.com>
==

README.md
docs/source/index.rst
docs/source/model_doc/albert.rst
docs/source/model_doc/bert.rst
docs/source/model_doc/camembert.rst
docs/source/model_doc/ctrl.rst
docs/source/model_doc/dialogpt.rst
docs/source/model_doc/distilbert.rst
docs/source/model_doc/electra.rst
docs/source/model_doc/flaubert.rst
docs/source/model_doc/gpt.rst
docs/source/model_doc/gpt2.rst
docs/source/model_doc/roberta.rst
docs/source/model_doc/t5.rst
docs/source/model_doc/transformerxl.rst
docs/source/model_doc/xlm.rst
docs/source/model_doc/xlmroberta.rst
docs/source/model_doc/xlnet.rst
docs/source/pretrained_models.rst
==================
c59b1e682;Sam Shleifer;2020-04-15 18:35:01 -0400;[examples] unit test for run_bart_sum (#3544)
- adds pytorch-lightning dependency
==

examples/requirements.txt
examples/summarization/bart/run_bart_sum.py
examples/summarization/bart/run_train.sh
examples/summarization/bart/run_train_tiny.sh
examples/summarization/bart/test_bart_examples.py
examples/transformer_base.py
==================
301bf8d1b;Patrick von Platen;2020-04-15 16:26:24 +0200;Create Modelcard for Reformer Model

==

model_cards/patrickvonplaten/reformer-crime-and-punish/README.md
==================
01c37dcdb;Patrick von Platen;2020-04-14 20:40:28 +0200;[Config, Caching] Remove `output_past` everywhere and replace by `use_cache` argument (#3734)
* remove output_past from pt

* make style

* add optional input length for gpt2

* add use cache to prepare input

* save memory in gpt2

* correct gpt2 test inputs

* make past input optional for gpt2

* finish use_cache for all models

* make style

* delete modeling_gpt2 change in test file

* correct docstring

* correct is true statements for gpt2
==

examples/summarization/bart/run_bart_sum.py
src/transformers/configuration_utils.py
src/transformers/modeling_bart.py
src/transformers/modeling_ctrl.py
src/transformers/modeling_gpt2.py
src/transformers/modeling_t5.py
src/transformers/modeling_tf_ctrl.py
src/transformers/modeling_tf_gpt2.py
src/transformers/modeling_tf_utils.py
src/transformers/modeling_tf_xlnet.py
src/transformers/modeling_utils.py
src/transformers/modeling_xlnet.py
tests/test_modeling_common.py
tests/test_modeling_t5.py
tests/test_modeling_tf_common.py
==================
092cf881a;Patrick von Platen;2020-04-14 04:29:28 +0200;[Generation, EncoderDecoder] Apply Encoder Decoder 1.5GB memory‚Ä¶ (#3778)

==

src/transformers/modeling_tf_utils.py
==================
352d5472b;Teven;2020-04-13 18:11:23 +0200;Shift labels internally within TransfoXLLMHeadModel when called with labels (#3716)
* Shifting labels inside TransfoXLLMHead

* Changed doc to reflect change

* Updated pytorch test

* removed IDE whitespace changes

* black reformat

Co-authored-by: TevenLeScao <teven.lescao@gmail.com>
==

src/transformers/modeling_transfo_xl.py
src/transformers/modeling_transfo_xl_utilities.py
tests/test_modeling_transfo_xl.py
==================
5ebd89895;elk-cloner;2020-04-13 18:41:18 +0430;fix dataset shuffling for Distributed training (#huggingface#3721) (#3766)

==

examples/run_language_modeling.py
==================
7972a4019;HenrykBorzymowski;2020-04-11 12:44:59 +0200;updated dutch squad model card (#3736)
* added model_cards for polish squad models

* corrected mistake in polish design cards

* updated model_cards for squad2_dutch model

* added links to benchmark models

Co-authored-by: Henryk Borzymowski <henryk.borzymowski@pwc.com>
==

model_cards/henryk/bert-base-multilingual-cased-finetuned-dutch-squad2/README.md
==================
f8c1071c5;HUSEIN ZOLKEPLI;2020-04-11 18:42:06 +0800;Added README huseinzol05/albert-tiny-bahasa-cased (#3746)
* add bert bahasa readme

* update readme

* update readme

* added xlnet

* added tiny-bert and fix xlnet readme

* added albert base

* added albert tiny
==

model_cards/huseinzol05/albert-tiny-bahasa-cased/README.md
==================
700ccf6e3;Jin Young Sohn;2020-04-10 13:03:27 -0700;Fix glue_convert_examples_to_features API breakage (#3742)

==

examples/run_tpu_glue.py
==================
b7cf9f43d;Anthony MOI;2020-04-10 14:23:49 -0400;Update tokenizers to 0.7.0-rc5 (#3705)

==

notebooks/01-training-tokenizers.ipynb
setup.py
src/transformers/tokenization_openai.py
src/transformers/tokenization_transfo_xl.py
==================
551b45052;Jin Young Sohn;2020-04-10 09:53:54 -0700;Add `run_glue_tpu.py` that trains models on TPUs (#3702)
* Initial commit to get BERT + run_glue.py on TPU

* Add README section for TPU and address comments.

* Cleanup TPU bits from run_glue.py (#3)

TPU runner is currently implemented in:
https://github.com/pytorch-tpu/transformers/blob/tpu/examples/run_glue_tpu.py.

We plan to upstream this directly into `huggingface/transformers`
(either `master` or `tpu`) branch once it's been more thoroughly tested.

* Cleanup TPU bits from run_glue.py

TPU runner is currently implemented in:
https://github.com/pytorch-tpu/transformers/blob/tpu/examples/run_glue_tpu.py.

We plan to upstream this directly into `huggingface/transformers`
(either `master` or `tpu`) branch once it's been more thoroughly tested.

* No need to call `xm.mark_step()` explicitly (#4)

Since for gradient accumulation we're accumulating on batches from
`ParallelLoader` instance which on next() marks the step itself.

* Resolve R/W conflicts from multiprocessing (#5)

* Add XLNet in list of models for `run_glue_tpu.py` (#6)

* Add RoBERTa to list of models in TPU GLUE (#7)

* Add RoBERTa and DistilBert to list of models in TPU GLUE (#8)

* Use barriers to reduce duplicate work/resources (#9)

* Shard eval dataset and aggregate eval metrics (#10)

* Shard eval dataset and aggregate eval metrics

Also, instead of calling `eval_loss.item()` every time do summation with
tensors on device.

* Change defaultdict to float

* Reduce the pred, label tensors instead of metrics

As brought up during review some metrics like f1 cannot be aggregated
via averaging. GLUE task metrics depends largely on the dataset, so
instead we sync the prediction and label tensors so that the metrics can
be computed accurately on those instead.

* Only use tb_writer from master (#11)

* Apply huggingface black code formatting

* Style

* Remove `--do_lower_case` as example uses cased

* Add option to specify tensorboard logdir

This is needed for our testing framework which checks regressions
against key metrics writtern by the summary writer.

* Using configuration for `xla_device`

* Prefix TPU specific comments.

* num_cores clarification and namespace eval metrics

* Cache features file under `args.cache_dir`

Instead of under `args.data_dir`. This is needed as our test infra uses
data_dir with a read-only filesystem.

* Rename `run_glue_tpu` to `run_tpu_glue`

Co-authored-by: LysandreJik <lysandre.debut@reseau.eseo.fr>
==

examples/README.md
examples/run_tpu_glue.py
src/transformers/configuration_utils.py
src/transformers/modeling_utils.py
==================
cbad305ce;Julien Chaumond;2020-04-10 12:34:04 -0400;[docs] The use of `do_lower_case` in scripts is on its way to deprecation (#3738)

==

README.md
docs/source/serialization.rst
examples/README.md
valohai.yaml
==================
b169ac9c2;Julien Chaumond;2020-04-10 12:21:58 -0400;[examples] Generate argparsers from type hints on dataclasses (#3669)
* [examples] Generate argparsers from type hints on dataclasses

* [HfArgumentParser] way simpler API

* Restore run_language_modeling.py for easier diff

* [HfArgumentParser] final tweaks from code review
==

examples/run_glue.py
src/transformers/__init__.py
src/transformers/hf_argparser.py
src/transformers/training_args.py
tests/test_hf_argparser.py
==================
7a7fdf71f;Sam Shleifer;2020-04-10 11:25:39 -0400;Multilingual BART - (#3602)
- support mbart-en-ro weights
- add MBartTokenizer
==

docs/source/pretrained_models.rst
src/transformers/__init__.py
src/transformers/configuration_bart.py
src/transformers/convert_bart_original_pytorch_checkpoint_to_pytorch.py
src/transformers/modeling_bart.py
src/transformers/tokenization_bart.py
tests/test_modeling_bart.py
==================
f98d0ef2a;Julien Chaumond;2020-04-10 10:20:18 -0400;Big cleanup of `glue_convert_examples_to_features` (#3688)
* Big cleanup of `glue_convert_examples_to_features`

* Use batch_encode_plus

* Cleaner wrapping of glue_convert_examples_to_features for TF

@lysandrejik

* Cleanup syntax, thanks to @mfuntowicz

* Raise explicit error in case of user error
==

examples/glue/run_pl_glue.py
examples/run_glue.py
examples/run_tf_glue.py
examples/run_xnli.py
src/transformers/data/processors/glue.py
src/transformers/data/processors/utils.py
==================
ce2298fb5;Patrick von Platen;2020-04-10 01:02:50 +0200;[T5, generation] Add decoder caching for T5 (#3682)
* initial commit to add decoder caching for T5

* better naming for caching

* finish T5 decoder caching

* correct test

* added extensive past testing for T5

* clean files

* make tests cleaner

* improve docstring

* improve docstring

* better reorder cache

* make style

* Update src/transformers/modeling_t5.py

Co-Authored-By: Yacine Jernite <yjernite@users.noreply.github.com>

* make set output past work for all layers

* improve docstring

* improve docstring

Co-authored-by: Yacine Jernite <yjernite@users.noreply.github.com>
==

src/transformers/modeling_t5.py
src/transformers/modeling_utils.py
tests/test_modeling_common.py
tests/test_modeling_t5.py
==================
9384e5f6d;calpt;2020-04-09 20:44:57 +0200;Fix force_download of files on Windows (#3697)

==

src/transformers/file_utils.py
==================
bc65afc4d;Julien Chaumond;2020-04-09 10:44:42 -0400;[Exbert] Change style of button

==

model_cards/albert-base-v1-README.md
model_cards/albert-xxlarge-v2-README.md
model_cards/bert-base-cased-README.md
model_cards/bert-base-german-cased-README.md
model_cards/bert-base-uncased-README.md
model_cards/distilbert-base-uncased-README.md
model_cards/distilgpt2-README.md
model_cards/distilroberta-base-README.md
model_cards/gpt2-README.md
model_cards/roberta-base-README.md
model_cards/xlm-mlm-en-2048-README.md
model_cards/xlm-roberta-base-README.md
==================
31baeed61;LysandreJik;2020-04-09 09:09:00 -0400;Update quotes
cc @julien-c
==

tests/test_tokenization_common.py
==================
f8208fa45;Teven;2020-04-09 09:03:19 +0200;Correct transformers-cli env call

==

.github/ISSUE_TEMPLATE/bug-report.md
==================
6435b9f90;Lysandre Debut;2020-04-08 16:22:44 -0400;Updating the TensorFlow models to work as expected with tokenizers v3.0.0 (#3684)
* Updating modeling tf files; adding tests

* Merge `encode_plus` and `batch_encode_plus`
==

src/transformers/modeling_tf_albert.py
src/transformers/modeling_tf_bert.py
src/transformers/modeling_tf_ctrl.py
src/transformers/modeling_tf_distilbert.py
src/transformers/modeling_tf_electra.py
src/transformers/modeling_tf_flaubert.py
src/transformers/modeling_tf_gpt2.py
src/transformers/modeling_tf_openai.py
src/transformers/modeling_tf_transfo_xl.py
src/transformers/modeling_tf_xlm.py
src/transformers/modeling_tf_xlnet.py
src/transformers/tokenization_transfo_xl.py
tests/test_tokenization_common.py
tests/test_tokenization_distilbert.py
==================
500aa1231;LysandreJik;2020-04-08 14:32:47 -0400;close #3699

==

src/transformers/modeling_electra.py
==================
a594ee9c8;Julien Chaumond;2020-04-08 12:12:52 -0400;More doc for model cards (#3698)
see https://github.com/huggingface/transformers/pull/3679#pullrequestreview-389368270
==

templates/adding_a_new_model/README.md
==================
83703cd07;Julien Chaumond;2020-04-07 13:44:02 -0400;Update doc for {Summarization,Translation}Pipeline and other tweaks

==

README.md
model_cards/bart-large-cnn/README.md
model_cards/bart-large-xsum/README.md
model_cards/t5-11b-README.md
model_cards/t5-3b-README.md
model_cards/t5-base-README.md
model_cards/t5-large-README.md
model_cards/t5-small-README.md
src/transformers/pipelines.py
==================
a1b3b4167;Seyone Chithrananda;2020-04-08 09:10:20 -0400;Created README.md for model card ChemBERTa (#3666)
* created readme.md

* update readme with fixes

Fixes from PR comments
==

model_cards/seyonec/ChemBERTa-zinc-base-v1/README.md
==================
747907dc5;Lorenzo Ampil;2020-04-03 14:58:53 +0800;Fix typo in FeatureExtractionPipeline docstring

==

src/transformers/pipelines.py
==================
715aa5b13;Sam Shleifer;2020-04-07 19:08:26 -0400;[Bart] Replace config.output_past with use_cache kwarg (#3632)

==

examples/summarization/bart/evaluate_cnn.py
src/transformers/configuration_bart.py
src/transformers/modeling_bart.py
tests/test_modeling_bart.py
==================
e344e3d40;Sam Shleifer;2020-04-07 19:05:58 -0400;[examples] SummarizationDataset cleanup (#3451)

==

examples/summarization/bart/run_bart_sum.py
examples/summarization/bart/test_bart_examples.py
examples/summarization/bart/utils.py
examples/transformer_base.py
==================
b0ad06951;Patrick von Platen;2020-04-07 22:26:31 +0200;[Tokenization] fix edge case for bert tokenization (#3517)
* fix egde gase for bert tokenization

* add Lysandres comments for improvement

* use new is_pretokenized_flag
==

src/transformers/tokenization_utils.py
==================
80fa0f781;Patrick von Platen;2020-04-07 22:25:57 +0200;[Examples, Benchmark] Improve benchmark utils (#3674)
* improve and add features to benchmark utils

* update benchmark style

* remove output files
==

examples/benchmarks.py
==================
05deb52dc;Michael Pang;2020-04-07 15:19:18 -0500;Optimize causal mask using torch.where (#2715)
* Optimize causal mask using torch.where

Instead of multiplying by 1.0 float mask, use torch.where with a bool mask for increased performance.

* Maintain compatiblity with torch 1.0.0 - thanks for PR feedback

* Fix typo

* reformat line for CI
==

src/transformers/modeling_gpt2.py
==================
0a4b1068e;Sam Shleifer;2020-04-07 14:01:30 -0400;Speedup torch summarization tests (#3663)

==

tests/test_pipelines.py
==================
5aa8a278a;Myle Ott;2020-04-07 12:03:23 -0400;Fix roberta checkpoint conversion script (#3642)

==

src/transformers/convert_roberta_original_pytorch_checkpoint_to_pytorch.py
==================
11cc1e168;Julien Chaumond;2020-04-07 10:20:19 -0400;[model_cards] Turn down spurious warnings
Close #3639 + spurious warning mentioned in #3227

cc @lysandrejik @thomwolf

==

src/transformers/modelcard.py
==================
0a9d09b42;Teven;2020-04-07 00:47:51 +0200;fixed TransfoXLLMHeadModel documentation (#3661)
Co-authored-by: TevenLeScao <teven.lescao@gmail.com>
==

src/transformers/modeling_transfo_xl.py
==================
96ab75b8d;Funtowicz Morgan;2020-04-06 22:29:15 +0000;Tokenizers v3.0.0 (#3185)
* Renamed num_added_tokens to num_special_tokens_to_add

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Cherry-Pick: Partially fix space only input without special tokens added to the output #3091

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Added property is_fast on PretrainedTokenizer and PretrainedTokenizerFast

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Make fast tokenizers unittests work on Windows.

* Entirely refactored unittest for tokenizers fast.

* Remove ABC class for CommonFastTokenizerTest

* Added embeded_special_tokens tests from allenai @dirkgr

* Make embeded_special_tokens tests from allenai more generic

* Uniformize vocab_size as a property for both Fast and normal tokenizers

* Move special tokens handling out of PretrainedTokenizer (SpecialTokensMixin)

* Ensure providing None input raise the same ValueError than Python tokenizer + tests.

* Fix invalid input for assert_padding when testing batch_encode_plus

* Move add_special_tokens from constructor to tokenize/encode/[batch_]encode_plus methods parameter.

* Ensure tokenize() correctly forward add_special_tokens to rust.

* Adding None checking on top on encode / encode_batch for TransfoXLTokenizerFast.
Avoid stripping on None values.

* unittests ensure tokenize() also throws a ValueError if provided None

* Added add_special_tokens unittest for all supported models.

* Style

* Make sure TransfoXL test run only if PyTorch is provided.

* Split up tokenizers tests for each model type.

* Fix invalid unittest with new tokenizers API.

* Filter out Roberta openai detector models from unittests.

* Introduce BatchEncoding on fast tokenizers path.

This new structure exposes all the mappings retrieved from Rust.
It also keeps the current behavior with model forward.

* Introduce BatchEncoding on slow tokenizers path.

Backward compatibility.

* Improve error message on BatchEncoding for slow path

* Make add_prefix_space True by default on Roberta fast to match Python in majority of cases.

* Style and format.

* Added typing on all methods for PretrainedTokenizerFast

* Style and format

* Added path for feeding pretokenized (List[str]) input to PretrainedTokenizerFast.

* Style and format

* encode_plus now supports pretokenized inputs.

* Remove user warning about add_special_tokens when working on pretokenized inputs.

* Always go through the post processor.

* Added support for pretokenized input pairs on encode_plus

* Added is_pretokenized flag on encode_plus for clarity and improved error message on input TypeError.

* Added pretokenized inputs support on batch_encode_plus

* Update BatchEncoding methods name to match Encoding.

* Bump setup.py tokenizers dependency to 0.7.0rc1

* Remove unused parameters in BertTokenizerFast

* Make sure Roberta returns token_type_ids for unittests.

* Added missing typings

* Update add_tokens prototype to match tokenizers side and allow AddedToken

* Bumping tokenizers to 0.7.0rc2

* Added documentation for BatchEncoding

* Added (unused) is_pretokenized parameter on PreTrainedTokenizer encode_plus/batch_encode_plus methods.

* Added higher-level typing for tokenize / encode_plus / batch_encode_plus.

* Fix unittests failing because add_special_tokens was defined as a constructor parameter on Rust Tokenizers.

* Fix text-classification pipeline using the wrong tokenizer

* Make pipelines works with BatchEncoding

* Turn off add_special_tokens on tokenize by default.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Remove add_prefix_space from tokenize call in unittest.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Style and quality

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Correct message for batch_encode_plus none input exception.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Fix invalid list comprehension for offset_mapping overriding content every iteration.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* TransfoXL uses Strip normalizer.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Bump tokenizers dependency to 0.7.0rc3

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Support AddedTokens for special_tokens and use left stripping on mask for Roberta.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* SpecilaTokenMixin can use slots to faster access to underlying attributes.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Remove update_special_tokens from fast tokenizers.

* Ensure TransfoXL unittests are run only when torch is available.

* Style.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Style

* Style üôèüôè

* Remove slots on SpecialTokensMixin, need deep dive into pickle protocol.

* Remove Roberta warning on __init__.

* Move documentation to Google style.

Co-authored-by: LysandreJik <lysandre.debut@reseau.eseo.fr>
==

setup.py
src/transformers/pipelines.py
src/transformers/tokenization_bert.py
src/transformers/tokenization_roberta.py
src/transformers/tokenization_transfo_xl.py
src/transformers/tokenization_utils.py
tests/test_pipelines.py
tests/test_tokenization_bert.py
tests/test_tokenization_common.py
tests/test_tokenization_fast.py
tests/test_tokenization_gpt2.py
==================
e52d1258e;Ethan Perez;2020-04-06 15:52:22 -0500;Fix RoBERTa/XLNet Pad Token in run_multiple_choice.py (#3631)
* Fix RoBERTa/XLNet Pad Token in run_multiple_choice.py

`convert_examples_to_fes atures` sets `pad_token=0` by default, which is correct for BERT but incorrect for RoBERTa (`pad_token=1`) and XLNet (`pad_token=5`). I think the other arguments to `convert_examples_to_features` are correct, but it might be helpful if someone checked who is more familiar with this part of the codebase.

* Simplifying change to match recent commits
==

examples/run_multiple_choice.py
==================
0ac33ddd8;ktrapeznikov;2020-04-06 10:00:30 -0400;Create README.md

==

model_cards/ktrapeznikov/biobert_v1.1_pubmed_squad_v2/README.md
==================
326e6ebae;Manuel Romero;2020-04-06 18:30:52 +0200;Add model card

==

model_cards/mrm8488/spanbert-base-finetuned-squadv1/README.md
==================
43eca3f87;Manuel Romero;2020-04-06 18:23:52 +0200;Add model card

==

model_cards/mrm8488/spanbert-base-finetuned-squadv2/README.md
==================
6bec88ca4;Manuel Romero;2020-04-06 18:11:20 +0200;Create README.md

==

model_cards/mrm8488/spanbert-base-finetuned-tacred/README.md
==================
769b60f93;Manuel Romero;2020-04-06 22:29:36 +0200;Add model card (#3655)
* Add model card

* Fix model name in fine-tuning script
==

model_cards/mrm8488/spanbert-large-finetuned-tacred/README.md
==================
c4bcb0190;Manuel Romero;2020-04-06 22:29:25 +0200;Create model card (#3654)
* Create model card

* Fix model name in fine-tuning script
==

model_cards/mrm8488/spanbert-large-finetuned-squadv1/README.md
==================
6903a987b;Manuel Romero;2020-04-06 17:43:57 +0200;Create README.md

==

model_cards/mrm8488/spanbert-large-finetuned-squadv2/README.md
==================
760872dbd;MichalMalyska;2020-04-06 16:27:50 -0400;Create README.md (#3662)

==

model_cards/NLP4H/ms_bert/README.md
==================
47e1334c0;jjacampos;2020-04-06 22:21:25 +0200;Add model card for BERTeus (#3649)
* Add model card for BERTeus

* Update README
==

model_cards/ixa-ehu/berteus-base-cased/README.md
==================
529534dc2;Suchin;2020-04-06 13:12:09 -0700;BioMed Roberta-Base (AllenAI) (#3643)
* added model card

* updated README

* updated README

* updated README

* added evals

* removed pico eval

* Tweaks

Co-authored-by: Julien Chaumond <chaumond@gmail.com>
==

model_cards/allenai/biomed_roberta_base/README.md
==================
261c4ff4e;Lysandre Debut;2020-04-06 14:32:39 -0400;Update notebooks (#3620)
* Update notebooks

* From local to global link

* from local links to *actual* global links
==

docs/source/notebooks.md
docs/source/notebooks.rst
notebooks/README.md
==================
39a34cc37;Julien Chaumond;2020-04-06 11:43:33 -0400;[model_cards] ELECTRA (w/ examples of usage)
Co-Authored-By: Kevin Clark <clarkkev@users.noreply.github.com>
Co-Authored-By: Lysandre Debut <lysandre.debut@reseau.eseo.fr>

==

model_cards/google/electra-base-discriminator/README.md
model_cards/google/electra-base-generator/README.md
model_cards/google/electra-large-discriminator/README.md
model_cards/google/electra-large-generator/README.md
model_cards/google/electra-small-discriminator/README.md
model_cards/google/electra-small-generator/README.md
==================
ea6dba278;LysandreJik;2020-04-06 10:09:54 -0400;Re-pin isort

==

setup.py
==================
11c3257a1;LysandreJik;2020-04-06 10:06:41 -0400;unpin isort for pypi

==

setup.py
==================
36bffc81b;LysandreJik;2020-04-06 10:03:53 -0400;Release: v2.8.0

==

docs/source/conf.py
setup.py
src/transformers/__init__.py
==================
2ee410560;Patrick von Platen;2020-04-06 10:37:05 +0200;[Generate, Test] Split generate test function into beam search, no beam search (#3601)
* split beam search and no beam search test

* fix test

* clean generate tests
==

tests/test_modeling_common.py
tests/test_modeling_tf_common.py
==================
1789c7daf;Patrick von Platen;2020-04-05 12:33:41 +0200;fix argument order (#3637)

==

src/transformers/modeling_tf_transfo_xl.py
==================
b809d2f07;Patrick von Platen;2020-04-05 12:23:09 +0200;Fix TF T5 docstring (#3636)

==

src/transformers/modeling_tf_t5.py
==================
4ab8ab4f5;Timo Moeller;2020-04-03 15:44:21 +0200;Adjust model card to reflect changes to vocabulary
(cherry picked from commit 8e25c4bf2838211378db4d93e7f9722386cc1a04)

==

model_cards/bert-base-german-cased-README.md
==================
ac40eed1a;ktrapeznikov;2020-04-04 12:04:17 -0400;Create README.md
adding readme for 
ktrapeznikov/albert-xlarge-v2-squad-v2
==

model_cards/ktrapeznikov/albert-xlarge-v2-squad-v2/README.md
==================
fd9995ebc;ktrapeznikov;2020-04-04 12:32:51 -0400;Create README.md

==

model_cards/ktrapeznikov/scibert_scivocab_uncased_squad_v2/README.md
==================
5d912e7ed;Julien Chaumond;2020-04-04 15:04:03 -0400;Tweak typing for #3566

==

src/transformers/tokenization_bert_japanese.py
==================
94eb68d74;Julien Chaumond;2020-04-04 15:03:26 -0400;weigths*weights

==

docs/source/migration.md
src/transformers/modeling_transfo_xl.py
src/transformers/modeling_xlnet.py
templates/adding_a_new_model/README.md
==================
243e687be;Manuel Romero;2020-04-04 00:09:50 +0200;Create model card

==

model_cards/mrm8488/gpt2-imdb-neg/README.md
==================
3e4b4dd19;Julien Chaumond;2020-04-03 20:03:29 -0400;[model_cards] Link to ExBERT visualisation
Hat/tip @bhoov @HendrikStrobelt @sebastianGehrmann

Also cc @srush and @thomwolf

==

model_cards/albert-base-v1-README.md
model_cards/albert-xxlarge-v2-README.md
model_cards/bert-base-cased-README.md
model_cards/bert-base-german-cased-README.md
model_cards/bert-base-uncased-README.md
model_cards/camembert-base-README.md
model_cards/distilbert-base-uncased-README.md
model_cards/distilgpt2-README.md
model_cards/distilroberta-base-README.md
model_cards/gpt2-README.md
model_cards/roberta-base-README.md
model_cards/xlm-mlm-en-2048-README.md
model_cards/xlm-roberta-base-README.md
==================
c6acd246e;Max Ryabinin;2020-04-03 22:20:21 +0300;Speed up GELU computation with torch.jit (#2988)
* Compile gelu_new with torchscript

* Compile _gelu_python with torchscript

* Wrap gelu_new with torch.jit for torch>=1.4
==

src/transformers/activations.py
==================
d5d7d8861;Lysandre Debut;2020-04-03 14:10:54 -0400;ELECTRA (#3257)
* Electra wip

* helpers

* Electra wip

* Electra v1

* ELECTRA may be saved/loaded

* Generator & Discriminator

* Embedding size instead of halving the hidden size

* ELECTRA Tokenizer

* Revert BERT helpers

* ELECTRA Conversion script

* Archive maps

* PyTorch tests

* Start fixing tests

* Tests pass

* Same configuration for both models

* Compatible with base + large

* Simplification + weight tying

* Archives

* Auto + Renaming to standard names

* ELECTRA is uncased

* Tests

* Slight API changes

* Update tests

* wip

* ElectraForTokenClassification

* temp

* Simpler arch + tests

Removed ElectraForPreTraining which will be in a script

* Conversion script

* Auto model

* Update links to S3

* Split ElectraForPreTraining and ElectraForTokenClassification

* Actually test PreTraining model

* Remove num_labels from configuration

* wip

* wip

* From discriminator and generator to electra

* Slight API changes

* Better naming

* TensorFlow ELECTRA tests

* Accurate conversion script

* Added to conversion script

* Fast ELECTRA tokenizer

* Style

* Add ELECTRA to README

* Modeling Pytorch Doc + Real style

* TF Docs

* Docs

* Correct links

* Correct model intialized

* random fixes

* style

* Addressing Patrick's and Sam's comments

* Correct links in docs
==

README.md
docs/source/index.rst
docs/source/model_doc/electra.rst
src/transformers/__init__.py
src/transformers/configuration_auto.py
src/transformers/configuration_electra.py
src/transformers/convert_electra_original_tf_checkpoint_to_pytorch.py
src/transformers/convert_pytorch_checkpoint_to_tf2.py
src/transformers/modeling_auto.py
src/transformers/modeling_bert.py
src/transformers/modeling_electra.py
src/transformers/modeling_tf_electra.py
src/transformers/tokenization_auto.py
src/transformers/tokenization_electra.py
tests/test_modeling_electra.py
tests/test_modeling_tf_electra.py
==================
8594dd80d;Yohei Tamura;2020-04-04 00:12:19 +0900;BertJapaneseTokenizer accept options for mecab (#3566)
* BertJapaneseTokenizer accept options for mecab

* black

* fix mecab_option to Option[str]
==

src/transformers/tokenization_bert_japanese.py
tests/test_tokenization_bert_japanese.py
==================
216e167ce;HUSEIN ZOLKEPLI;2020-04-03 21:28:43 +0800;Added albert-base-bahasa-cased README and fixed tiny-bert-bahasa-cased README (#3613)
* add bert bahasa readme

* update readme

* update readme

* added xlnet

* added tiny-bert and fix xlnet readme

* added albert base
==

model_cards/huseinzol05/albert-base-bahasa-cased/README.md
model_cards/huseinzol05/tiny-bert-bahasa-cased/README.md
==================
1ac6a246d;ahotrod;2020-04-03 06:28:25 -0700;Update README.md (#3604)
Update AutoModel & AutoTokernizer loading.
==

model_cards/ahotrod/xlnet_large_squad2_512/README.md
==================
e91692f4a;ahotrod;2020-04-03 06:27:57 -0700;Update README.md (#3603)

==

model_cards/ahotrod/albert_xxlargev1_squad2_512/README.md
==================
8e287d507;HenrykBorzymowski;2020-04-03 15:07:15 +0200;corrected mistake in polish model cards (#3611)
* added model_cards for polish squad models

* corrected mistake in polish design cards

Co-authored-by: Henryk Borzymowski <henryk.borzymowski@pwc.com>
==

model_cards/henryk/bert-base-multilingual-cased-finetuned-polish-squad1/README.md
model_cards/henryk/bert-base-multilingual-cased-finetuned-polish-squad2/README.md
==================
81484b447;redewiedergabe;2020-04-03 03:48:31 +0200;Create README.md (#3568)
* Create README.md

* added meta block (language: german)

* Added additional information about test data
==

model_cards/redewiedergabe/bert-base-historical-german-rw-cased/README.md
==================
9f6349aba;ahotrod;2020-04-01 13:04:43 -0700;Create README.md

==

model_cards/ahotrod/roberta_large_squad2/README.md
==================
ddb1ce741;Henryk Borzymowski;2020-04-02 15:32:06 +0200;added model_cards for polish squad models

==

model_cards/henryk/bert-base-multilingual-cased-finetuned-polish-squad1/README.md
model_cards/henryk/bert-base-multilingual-cased-finetuned-polish-squad2/README.md
==================
f68d22850;Patrick von Platen;2020-04-02 21:49:34 +0200;delete bogus print statement (#3595)

==

src/transformers/modeling_tf_utils.py
==================
c50aa67bf;Nicolas;2020-04-02 15:00:05 -0400;Resizing embedding matrix before sending it to the optimizer. (#3532)
* Resizing embedding matrix after sending it to the optimizer prevents from updating the newly resized matrix.

* Remove space for style matter
==

examples/run_language_modeling.py
==================
1b1015995;Mark Kockerbeck;2020-04-02 11:07:08 -0700;Adding should_continue check for retraining (#3509)

==

examples/run_language_modeling.py
==================
390c12859;Patrick von Platen;2020-04-02 15:18:33 +0200;[Encoder-Decoder] Force models outputs to always have batch_size as their first dim (#3536)
* solve conflicts

* improve comments
==

src/transformers/modeling_bart.py
src/transformers/modeling_t5.py
src/transformers/modeling_utils.py
==================
ab5d06a09;Patrick von Platen;2020-04-02 12:34:05 +0200;[T5, examples] replace heavy t5 models with tiny random models (#3556)
* replace heavy t5 models with tiny random models as was done by sshleifer

* fix isort
==

examples/summarization/t5/test_t5_examples.py
examples/translation/t5/test_t5_examples.py
==================
a4ee4da18;Patrick von Platen;2020-04-01 22:04:20 +0200;[T5, TF 2.2] change tf t5 argument naming (#3547)
* change tf t5 argument naming for TF 2.2

* correct bug in testing
==

src/transformers/modeling_tf_t5.py
tests/test_modeling_tf_common.py
tests/test_modeling_tf_t5.py
==================
06dd59755;Patrick von Platen;2020-04-01 21:59:12 +0200;fix bug in warnings T5 pipelines (#3545)

==

src/transformers/pipelines.py
==================
9de9ceb6c;Anirudh Srinivasan;2020-04-02 00:34:38 +0530;Correct output shape for Bert NSP models in docs (#3482)

==

src/transformers/modeling_bert.py
==================
b815edf69;Patrick von Platen;2020-04-01 18:01:33 +0200;[T5, Testst] Add extensive hard-coded integration tests and make sure PT and TF give equal results (#3550)
* add some t5 integration tests

* finish summarization and translation integration tests for T5 - results loook good

* add tf test

* fix == vs is bug

* fix tf beam search error and make tf t5 tests pass
==

src/transformers/modeling_tf_t5.py
src/transformers/modeling_tf_utils.py
src/transformers/modeling_utils.py
tests/test_modeling_t5.py
tests/test_modeling_tf_t5.py
==================
8538ce904;HUSEIN ZOLKEPLI;2020-04-01 19:15:00 +0800;Add tiny-bert-bahasa-cased model card (#3567)
* add bert bahasa readme

* update readme

* update readme

* added xlnet

* added tiny-bert and fix xlnet readme
==

model_cards/huseinzol05/tiny-bert-bahasa-cased/README.md
model_cards/huseinzol05/xlnet-base-bahasa-cased/README.md
==================
c1a6252be;Manuel Romero;2020-04-01 13:14:23 +0200;Create model card (#3557)
Create model card for: distilbert-multi-finetuned-for-xqua-on-tydiqa
==

model_cards/mrm8488/distilbert-multi-finetuned-for-xqua-on-tydiqa/README.md
==================
50e15c825;Julien Chaumond;2020-04-01 07:13:40 -0400;Tokenizers: Start cleaning examples a little (#3455)
* Start cleaning examples

* Fixup
==

examples/glue/run_pl_glue.py
examples/hans/test_hans.py
examples/ner/run_ner.py
examples/ner/run_pl_ner.py
examples/ner/run_tf_ner.py
examples/run_glue.py
examples/run_multiple_choice.py
examples/run_xnli.py
==================
b38d552a9;Patrick von Platen;2020-03-31 18:42:31 +0200;[Generate] Add bad words list argument to the generate function (#3367)
* add bad words list

* make style

* add bad_words_tokens

* make style

* better naming

* make style

* fix typo
==

src/transformers/configuration_utils.py
src/transformers/modeling_tf_utils.py
src/transformers/modeling_utils.py
tests/test_modeling_common.py
tests/test_modeling_tf_common.py
==================
ae6834e02;Patrick von Platen;2020-03-31 17:54:13 +0200;[Examples] Clean summarization and translation example testing files for T5 and Bart (#3514)
* fix conflicts

* add model size argument to summarization

* correct wrong import

* fix isort

* correct imports

* other isort make style

* make style
==

examples/summarization/bart/evaluate_cnn.py
examples/summarization/bart/test_bart_examples.py
examples/summarization/t5/evaluate_cnn.py
examples/summarization/t5/test_t5_examples.py
examples/translation/t5/evaluate_wmt.py
examples/translation/t5/test_t5_examples.py
==================
0373b60c4;Manuel Romero;2020-03-31 16:40:34 +0200;Update README.md (#3552)
- Show that the last uploaded version was trained on more data (custom_license files)
==

model_cards/mrm8488/GPT-2-finetuned-CORD19/README.md
==================
83d1fbcff;Patrick von Platen;2020-03-31 15:36:03 +0200;[Docs] Add usage examples for translation and summarization (#3538)

==

docs/source/usage.rst
==================
55bcae7f2;Patrick von Platen;2020-03-31 15:32:25 +0200;remove useless and confusing lm_labels line (#3531)

==

src/transformers/modeling_t5.py
==================
42e1e3c67;Patrick von Platen;2020-03-31 15:31:46 +0200;Update usage doc regarding generate fn (#3504)

==

docs/source/usage.rst
==================
57b0fab69;Patrick von Platen;2020-03-31 15:30:17 +0200;Add better explanation to check `docs` locally. (#3459)

==

docs/README.md
==================
a8d4dff0a;Manuel Romero;2020-03-31 14:01:09 +0200;Update README.md (#3470)
Fix typo
==

model_cards/mrm8488/GPT-2-finetuned-CORD19/README.md
==================
4a5663568;Manuel Romero;2020-03-31 14:01:03 +0200;Create card for the model: GPT-2-finetuned-covid-bio-medrxiv (#3453)

==

model_cards/mrm8488/GPT-2-finetuned-covid-bio-medrxiv/README.md
==================
bbedb5967;Branden Chan;2020-03-31 14:00:35 +0200;Create README.md (#3393)
* Create README.md

* Update README.md
==

model_cards/deepset/quora_dedup_bert_base/README.md
==================
c2cf19294;Manuel Romero;2020-03-31 14:00:00 +0200;Add link to 16 POS tags model (#3465)

==

model_cards/mrm8488/bert-spanish-cased-finetuned-pos/README.md
==================
c82ef7215;Gabriele Sarti;2020-03-31 13:59:49 +0200;Added CovidBERT-NLI model card (#3477)

==

model_cards/gsarti/covidbert-nli/README.md
==================
b48a1f08c;Manuel Romero;2020-03-31 13:59:36 +0200;Add text shown in example of usage (#3464)

==

model_cards/mrm8488/bert-spanish-cased-finetuned-ner/README.md
==================
99833a9cb;Manuel Romero;2020-03-31 13:59:22 +0200;Create model card (#3487)

==

model_cards/mrm8488/bert-spanish-cased-finetuned-pos-syntax/README.md
==================
ebceeeacd;Sho Arora;2020-03-31 04:58:48 -0700;Add electra and alectra model cards (#3524)

==

model_cards/shoarora/alectra-small-owt/README.md
model_cards/shoarora/electra-small-owt/README.md
==================
a6c4ee27f;Leandro von Werra;2020-03-31 13:54:45 +0200;Add model cards (#3537)
* feat: add model card bert-imdb

* feat: add model card gpt2-imdb-pos

* feat: add model card gpt2-imdb
==

model_cards/lvwerra/bert-imdb/README.md
model_cards/lvwerra/gpt2-imdb-pos/README.md
model_cards/lvwerra/gpt2-imdb/README.md
==================
e5c393dce;Ethan Perez;2020-03-30 16:06:08 -0500;[Bug fix] Using loaded checkpoint with --do_predict (instead of‚Ä¶ (#3437)
* Using loaded checkpoint with --do_predict

Without this fix, I'm getting near-random validation performance for a trained model, and the validation performance differs per validation run. I think this happens since the `model` variable isn't set with the loaded checkpoint, so I'm using a randomly initialized model. Looking at the model activations, they differ each time I run evaluation (but they don't with this fix).

* Update checkpoint loading

* Fixing model loading
==

examples/glue/run_pl_glue.py
examples/ner/run_pl_ner.py
==================
8deff3acf;Sam Shleifer;2020-03-30 12:28:27 -0400;[bart-tiny-random] Put a 5MB model on S3 to allow faster exampl‚Ä¶ (#3488)

==

examples/summarization/bart/evaluate_cnn.py
examples/summarization/bart/test_bart_examples.py
tests/test_modeling_bart.py
==================
1f7286572;dougian;2020-03-30 17:20:37 +0100;[BART] Update encoder and decoder on set_input_embedding (#3501)
Co-authored-by: Ioannis Douratsos <ioannisd@amazon.com>
==

src/transformers/modeling_bart.py
==================
cc598b312;Julien Chaumond;2020-03-30 10:41:49 -0400;[InputExample] Unfreeze for now, cf. #3423

==

src/transformers/data/processors/utils.py
==================
d38bbb225;Julien Plu;2020-03-30 15:50:12 +0200;Update the NER TF script (#3511)
* Update the NER TF script to remove the softmax and make the pad token label id to -1

* Reformat the quality and style

Co-authored-by: Julien Plu <julien.plu@adevinta.com>
==

examples/ner/run_tf_ner.py
src/transformers/optimization_tf.py
==================
eff757f2e;LysandreJik;2020-03-30 09:00:47 -0400;Re-pin isort version

==

setup.py
==================
a009d751c;LysandreJik;2020-03-30 08:54:56 -0400;Un-pin isort for v2.7.0 pypi

==

setup.py
==================
6f5a12a58;LysandreJik;2020-03-30 08:49:24 -0400;Release: v2.7.0

==

docs/source/conf.py
setup.py
src/transformers/__init__.py
==================
296252c49;Patrick von Platen;2020-03-30 14:26:24 +0200;fix lm lables in docstring (#3529)

==

src/transformers/modeling_t5.py
src/transformers/modeling_tf_t5.py
==================
75ec6c9e3;Patrick von Platen;2020-03-30 13:45:26 +0200;[T5] make decoder input ids optional for t5 training (#3521)
* make decoder input ids optional for t5 training

* lm_lables should not be shifted in t5

* add tests

* finish shift right functionality for PT T5

* move shift right to correct class

* cleaner code

* replace -100 values with pad token id

* add assert statement

* remove unnecessary for loop

* make style
==

src/transformers/modeling_t5.py
src/transformers/modeling_tf_utils.py
src/transformers/modeling_utils.py
tests/test_modeling_bart.py
tests/test_modeling_t5.py
tests/test_modeling_tf_t5.py
==================
5b44e0a31;Patrick von Platen;2020-03-30 13:35:53 +0200;[T5] Add training documenation  (#3507)
* Add clear description of how to train T5

* correct docstring in T5

* correct typo

* correct docstring format

* update t5 model docs

* implement collins feedback

* fix typo and add more explanation for sentinal tokens

* delete unnecessary todos
==

docs/source/model_doc/t5.rst
src/transformers/modeling_bart.py
src/transformers/modeling_t5.py
src/transformers/modeling_tf_t5.py
==================
33ef7002e;Sam Shleifer;2020-03-29 13:25:42 -0400;[Docs] examples/summarization/bart: Simplify CNN/DM preprocessi‚Ä¶ (#3516)

==

examples/summarization/bart/README.md
==================
f6a23d191;Sam Shleifer;2020-03-29 10:51:13 -0400;[BART] add bart-large-xsum weights (#3422)

==

src/transformers/configuration_bart.py
src/transformers/convert_bart_original_pytorch_checkpoint_to_pytorch.py
src/transformers/modeling_bart.py
src/transformers/tokenization_bart.py
tests/test_modeling_bart.py
==================
601ac5b1d;Stefan Schweter;2020-03-27 22:00:03 +0100;[model_cards]: use MIT license for all dbmdz models

==

model_cards/dbmdz/bert-base-german-cased/README.md
model_cards/dbmdz/bert-base-german-europeana-cased/README.md
model_cards/dbmdz/bert-base-german-europeana-uncased/README.md
model_cards/dbmdz/bert-base-german-uncased/README.md
model_cards/dbmdz/bert-base-italian-cased/README.md
model_cards/dbmdz/bert-base-italian-uncased/README.md
model_cards/dbmdz/bert-base-italian-xxl-cased/README.md
model_cards/dbmdz/bert-base-italian-xxl-uncased/README.md
model_cards/dbmdz/bert-base-turkish-128k-cased/README.md
model_cards/dbmdz/bert-base-turkish-128k-uncased/README.md
model_cards/dbmdz/bert-base-turkish-cased/README.md
model_cards/dbmdz/bert-base-turkish-uncased/README.md
model_cards/dbmdz/distilbert-base-turkish-cased/README.md
==================
17dceae7a;Patrick von Platen;2020-03-27 18:01:28 +0100;Fix circle ci flaky fail of wmt example  (#3485)
* force bleu

* fix wrong file name

* rename file

* different filenames for each example test

* test files should clean up after themselves

* test files should clean up after themselves

* do not force bleu

* correct typo

* fix isort
==

examples/summarization/bart/test_bart_examples.py
examples/summarization/t5/test_t5_examples.py
examples/translation/t5/test_t5_examples.py
==================
00ea100e9;Patrick von Platen;2020-03-27 16:05:37 +0100;add summarization and translation to notebook (#3478)

==

notebooks/03-pipelines.ipynb
==================
b08259a12;Funtowicz Morgan;2020-03-27 14:59:55 +0000;run_ner.py / bert-base-multilingual-cased can output empty tokens (#2991)
* Use tokenizer.num_added_tokens to count number of added special_tokens instead of hardcoded numbers.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* run_ner.py - Do not add a label to the labels_ids if word_tokens is empty.

This can happen when using bert-base-multilingual-cased with an input containing an unique space.
In this case, the tokenizer will output just an empty word_tokens thus leading to an non-consistent behavior
over the labels_ids tokens adding one more tokens than tokens vector.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>
==

examples/ner/utils_ner.py
==================
f4f494683;Patrick von Platen;2020-03-27 15:57:58 +0100;Rename `t5-large` to `t5-base` in README.md

==

examples/summarization/t5/README.md
==================
fa9af2468;Patrick von Platen;2020-03-27 15:57:16 +0100;Add T5 to docs (#3461)
* add t5 docs basis

* improve docs

* add t5 docs

* improve t5 docstring

* add t5 tokenizer docstring

* finish docstring

* make style

* add pretrained models

* correct typo

* make examples work

* finalize docs
==

docs/source/index.rst
docs/source/model_doc/t5.rst
docs/source/pretrained_models.rst
src/transformers/modeling_bart.py
src/transformers/modeling_t5.py
src/transformers/modeling_tf_t5.py
src/transformers/tokenization_t5.py
==================
ff80b7315;Lysandre Debut;2020-03-27 10:56:59 -0400;Add option to choose T5 model size. (#3480)
T5-small in test


isort
==

examples/summarization/t5/evaluate_cnn.py
examples/summarization/t5/test_t5_examples.py
==================
e2c05f06e;LysandreJik;2020-03-27 09:28:21 -0400;Correct indentation in docstring
For some reason Sphinx extremely dislikes this and crashes.
==

src/transformers/modeling_tf_utils.py
==================
3ee431dd4;Sam Shleifer;2020-03-26 21:34:15 -0400;[Bart/Memory] Two separate, smaller decoder attention masks (#3371)

==

src/transformers/modeling_bart.py
tests/test_modeling_bart.py
==================
53fe73380;Manuel Romero;2020-03-27 02:33:33 +0100;Model Cards: Fix grammar error (#3467)

==

model_cards/mrm8488/bert-spanish-cased-finetuned-pos/README.md
==================
c10decf7a;Sam Shleifer;2020-03-26 19:33:54 -0400;[Bart: example] drop columns that are exclusively pad_token_id‚Ä¶ (#3400)
* trim seq_len below 1024 if there are columns full of pad_token_id
* Centralize trim_batch so SummarizationDataset can use it too
==

src/transformers/tokenization_utils.py
==================
63f4d8cad;Sam Shleifer;2020-03-26 18:42:39 -0400;[Bart/Memory] SelfAttention only returns weights if config.outp‚Ä¶ (#3369)

==

src/transformers/modeling_bart.py
==================
2b2a2f8df;Sam Shleifer;2020-03-26 18:42:09 -0400;[Bart] Fix: put dummy_inputs on correct device (#3398)
* Dummy inputs to model.device

* Move self.device to ModuleUtilsMixin
==

src/transformers/modeling_bart.py
src/transformers/modeling_utils.py
==================
1a5aefc95;Sam Shleifer;2020-03-26 18:41:19 -0400;[Seq2Seq Generation] Call encoder before expanding input_ids (#3370)

==

src/transformers/modeling_bart.py
src/transformers/modeling_t5.py
src/transformers/modeling_utils.py
==================
39371ee45;Sam Shleifer;2020-03-26 18:40:39 -0400;[Bart/Memory] don't create lm_head (#3323)
* delete lm_head, skips weight tying
* Fixed s3
==

src/transformers/modeling_bart.py
tests/test_modeling_bart.py
tests/test_modeling_common.py
==================
5ad2ea06a;Patrick von Platen;2020-03-26 19:07:59 +0100;Add wmt translation example (#3428)
* add translation example

* make style

* adapt docstring

* add gpu device as input for example

* small renaming

* better README
==

examples/requirements.txt
examples/translation/t5/README.md
examples/translation/t5/__init__.py
examples/translation/t5/evaluate_wmt.py
examples/translation/t5/test_t5_examples.py
==================
b4fb94fe6;Patrick von Platen;2020-03-26 12:50:36 +0100;revert unpin isort commit

==

setup.py
==================
e703e923c;Patrick von Platen;2020-03-26 18:17:55 +0100;Add t5 summarization example (#3411)
* rebase to master

* change tf to pytorch

* change to pytorch

* small fix

* renaming

* add gpu training possibility

* renaming

* improve README

* incoorporate collins feedback

* better Readme

* better README.md
==

examples/requirements.txt
examples/summarization/t5/README.md
examples/summarization/t5/__init__.py
examples/summarization/t5/download_cnn_daily_mail.py
examples/summarization/t5/evaluate_cnn.py
examples/summarization/t5/test_t5_examples.py
==================
1a6c546c6;sakares saengkaew;2020-03-26 21:22:13 +0700;Add missing token classification for XLM (#3277)
* Add the missing token classification for XLM

* fix styling

* Add XLMForTokenClassification to AutoModelForTokenClassification class

* Fix docstring typo for non-existing class

* Add the missing token classification for XLM

* fix styling

* fix styling

* Add XLMForTokenClassification to AutoModelForTokenClassification class

* Fix docstring typo for non-existing class

* Add missing description for AlbertForTokenClassification

* fix styling

* Add missing docstring for AlBert

* Slow tests should be slow

Co-authored-by: Sakares Saengkaew <s.sakares@gmail.com>
Co-authored-by: LysandreJik <lysandre.debut@reseau.eseo.fr>
==

src/transformers/__init__.py
src/transformers/modeling_auto.py
src/transformers/modeling_xlm.py
tests/test_modeling_auto.py
tests/test_modeling_xlm.py
==================
311970546;Patrick von Platen;2020-03-26 14:59:49 +0100;rename string in pipeline

==

src/transformers/pipelines.py
==================
7420a6a9c;Manuel Romero;2020-03-26 14:07:01 +0100;Create card for model GPT-2-finetuned-CORD19

==

model_cards/mrm8488/GPT-2-finetuned-CORD19/README.md
==================
022e8fab9;Patrick von Platen;2020-03-26 13:50:58 +0100;Adds translation pipeline (#3419)
* fix merge conflicts

* add t5 summarization example

* change parameters for t5 summarization

* make style

* add first code snippet for translation

* only add prefixes

* add prefix patterns

* make style

* renaming

* fix conflicts

* remove unused patterns

* solve conflicts

* fix merge conflicts

* remove translation example

* remove summarization example

* make sure tensors are in numpy for float comparsion

* re-add t5 config

* fix t5 import config typo

* make style

* remove unused numpy statements

* update doctstring

* import translation pipeline
==

src/transformers/__init__.py
src/transformers/pipelines.py
tests/test_pipelines.py
==================
3c5c56750;HUSEIN ZOLKEPLI;2020-03-26 19:50:27 +0800;Update model card huseinzol05/bert-base-bahasa-cased (#3425)
* add bert bahasa readme

* update readme

* update readme

* added xlnet
==

model_cards/huseinzol05/bert-base-bahasa-cased/README.md
model_cards/huseinzol05/xlnet-base-bahasa-cased/README.md
==================
9c683ef01;Patrick von Platen;2020-03-26 11:03:13 +0100;Add t5 to pipeline(task='summarization') (#3413)
* solve conflicts

* move warnings below

* incorporate changes

* add pad_to_max_length to pipelines

* add bug fix for T5 beam search

* add prefix patterns

* make style

* fix conflicts

* adapt pipelines for task specific parameters

* improve docstring

* remove unused patterns
==

src/transformers/configuration_utils.py
src/transformers/modeling_tf_utils.py
src/transformers/pipelines.py
tests/test_pipelines.py
==================
ffcffebe8;Lysandre Debut;2020-03-26 04:41:36 -0400;Force the return of token type IDs (#3439)

==

examples/utils_multiple_choice.py
src/transformers/data/processors/squad.py
==================
010e0460b;Travis McGuire;2020-03-25 13:40:03 -0700;Updated/added model cards (#3435)

==

model_cards/twmkn9/albert-base-v2-squad2/README.md
model_cards/twmkn9/bert-base-uncased-squad2/README.md
model_cards/twmkn9/distilbert-base-uncased-squad2/README.md
model_cards/twmkn9/distilroberta-base-squad2/README.md
==================
ffa17fe32;Patrick von Platen;2020-03-25 21:32:04 +0100;Extend config with task specific configs. (#3433)
* add new default configs

* change prefix default to None
==

src/transformers/configuration_utils.py
src/transformers/modeling_tf_utils.py
src/transformers/modeling_utils.py
==================
83272a385;Julien Chaumond;2020-03-25 11:10:20 -0400;Experiment w/ dataclasses (including Py36) (#3423)
* [ci] Also run test_examples in py37

(will revert at the end of the experiment)

* InputExample: use immutable dataclass

* [deps] Install dataclasses for Py<3.7

* [skip ci] Revert "[ci] Also run test_examples in py37"

This reverts commit d29afd9959786b77759b0b8fa4e6b4335b952015.
==

setup.py
src/transformers/data/processors/utils.py
==================
ccbe839ee;Gabriele Sarti;2020-03-25 02:15:55 +0100;Added BioBERT-NLI model card (#3421)

==

model_cards/gsarti/biobert-nli/README.md
==================
3d76df3a1;Andre Carrera;2020-03-24 19:00:24 -0600;BART for summarization training with CNN/DM using pytorch-lightning

==

examples/summarization/bart/README.md
examples/summarization/bart/run_bart_sum.py
examples/summarization/bart/run_train.sh
examples/summarization/bart/utils.py
examples/transformer_base.py
==================
eaabaaf75;Julien Chaumond;2020-03-24 17:56:40 -0400;[run_language_modeling] Fix: initialize a new model from a config object

==

examples/run_language_modeling.py
==================
f8823bad9;Julien Chaumond;2020-03-24 17:46:25 -0400;Expose missing mappings (see #3415)

==

examples/run_language_modeling.py
src/transformers/__init__.py
==================
d0c36a7b7;Julien Chaumond;2020-03-24 12:10:43 -0400;[ci] Partial revert of 18eec3a9847 due to fbc5bf10cfe

==

.circleci/config.yml
==================
fbc5bf10c;LysandreJik;2020-03-24 11:51:24 -0400;v2.6.0 release: isort un-pinned

==

setup.py
==================
b88bda6af;Manuel Romero;2020-03-24 12:03:19 +0100;Add right model and tokenizer path in example

==

model_cards/mrm8488/xlm-multi-finetuned-xquadv1/README.md
==================
b31ef225c;Stefan Schweter;2020-03-24 11:18:21 +0100;[model_cards] üáπüá∑ Add new (uncased, 128k) BERTurk model

==

model_cards/dbmdz/bert-base-turkish-128k-uncased/README.md
==================
b4009cb00;Stefan Schweter;2020-03-24 11:18:04 +0100;[model_cards] üáπüá∑ Add new (cased, 128k) BERTurk model

==

model_cards/dbmdz/bert-base-turkish-128k-cased/README.md
==================
d3283490e;Stefan Schweter;2020-03-24 11:00:32 +0100;[model_cards] üáπüá∑ Add new (uncased) BERTurk model

==

model_cards/dbmdz/bert-base-turkish-uncased/README.md
==================
e279a312d;Mohamed El-Geish;2020-03-24 08:28:33 -0700;Model cards for CS224n SQuAD2.0 models (#3406)
* Model cards for CS224n SQuAD2.0 models

* consistent spacing
==

model_cards/elgeish/cs224n-squad2.0-albert-base-v2/README.md
model_cards/elgeish/cs224n-squad2.0-albert-large-v2/README.md
model_cards/elgeish/cs224n-squad2.0-albert-xxlarge-v1/README.md
model_cards/elgeish/cs224n-squad2.0-distilbert-base-uncased/README.md
model_cards/elgeish/cs224n-squad2.0-roberta-base/README.md
==================
7372e62b2;Gabriele Sarti;2020-03-24 16:01:56 +0100;Added precisions in SciBERT-NLI model card (#3410)

==

model_cards/gsarti/scibert-nli/README.md
==================
471cce24b;LysandreJik;2020-03-24 10:37:32 -0400;Release: v2.6.0

==

docs/source/conf.py
setup.py
src/transformers/__init__.py
==================
e392ba693;Patrick von Platen;2020-03-24 10:18:37 +0100;Add camembert integration tests (#3375)
* add integration tests for camembert

* use jplu/tf-camembert fro the moment

* make style
==

tests/test_modeling_camembert.py
tests/test_modeling_tf_camembert.py
==================
a8e3336a8;Julien Chaumond;2020-03-23 19:30:19 -0400;[examples] Use AutoModels in more examples

==

examples/ner/run_ner.py
examples/ner/run_tf_ner.py
examples/run_glue.py
examples/run_language_modeling.py
examples/run_squad.py
src/transformers/__init__.py
templates/adding_a_new_example_script/run_xxx.py
==================
ec6766a36;Julien Chaumond;2020-03-23 18:38:09 -0400;[deps] scikit-learn's transient issue was fixed

==

setup.py
==================
f7dcf8fce;Julien Chaumond;2020-03-23 13:58:49 -0400;[BertAbs] Move files around for more consistent naming

==

examples/summarization/bertabs/configuration_bertabs.py
examples/summarization/bertabs/convert_bertabs_original_pytorch_checkpoint.py
examples/summarization/bertabs/modeling_bertabs.py
==================
e25c4f402;Julien Chaumond;2020-03-23 13:58:21 -0400;[ALBERT] move things around for more consistent naming
see #3359

cc @lysandrejik

==

src/transformers/configuration_albert.py
src/transformers/modeling_albert.py
src/transformers/tokenization_albert.py
==================
85b324bee;Manuel Romero;2020-03-23 12:27:31 +0100;Add comparison table with older brother in family

==

model_cards/mrm8488/bert-tiny-finetuned-squadv2/README.md
==================
b7aa077a6;Manuel Romero;2020-03-23 12:18:05 +0100;Create card for the model

==

model_cards/mrm8488/bert-medium-finetuned-squadv2/README.md
==================
f740177c8;Manuel Romero;2020-03-23 12:32:25 +0100;Add comparison table with new models

==

model_cards/mrm8488/bert-mini-finetuned-squadv2/README.md
==================
e52482909;LysandreJik;2020-03-23 12:01:23 -0400;Correct order for dev/quality dependencies
cc @julien-c
==

setup.py
==================
28424906c;Gabriele Sarti;2020-03-22 14:25:23 +0100;Added scibert-nli model card

==

model_cards/gsarti/scibert-nli/README.md
==================
18eec3a98;Julien Chaumond;2020-03-23 10:03:22 -0400;[ci] simpler way to load correct version of isort
hat/tip @bramvanroy

==

.circleci/config.yml
setup.py
==================
cf72479bf;Julien Chaumond;2020-03-20 18:05:50 -0400;One last reorder of {scheduler,optimizer}.step()

==

examples/contrib/run_openai_gpt.py
==================
634bf6cf7;Elijah Rippeth;2020-03-20 17:41:32 -0400;fixes lr_scheduler warning
For more details, see https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
==

examples/ner/run_ner.py
==================
265709f5c;Travis McGuire;2020-03-20 09:56:39 -0700;New model, new model cards

==

model_cards/twmkn9/albert-base-v2-squad2/README.md
model_cards/twmkn9/bert-base-uncased-squad2/README.md
==================
115abd216;Bram Vanroy;2020-03-19 14:29:49 +0100;Handle pinned version of isort
The CONTRIBUTING file pins to a specific version of isort, so we might as well install that in `dev` . This makes it easier for contributors so they don't have to manually install the specific commit.
==

setup.py
==================
95e00d080;Patrick von Platen;2020-03-20 21:41:04 +0100;Clean special token init in modeling_....py (#3264)
* make style

* fix conflicts
==

examples/summarization/bart/evaluate_cnn.py
src/transformers/__init__.py
src/transformers/configuration_albert.py
src/transformers/configuration_bart.py
src/transformers/configuration_bert.py
src/transformers/configuration_distilbert.py
src/transformers/configuration_flaubert.py
src/transformers/configuration_gpt2.py
src/transformers/configuration_roberta.py
src/transformers/configuration_t5.py
src/transformers/configuration_transfo_xl.py
src/transformers/configuration_utils.py
src/transformers/configuration_xlm.py
src/transformers/configuration_xlnet.py
src/transformers/modeling_bart.py
src/transformers/modeling_tf_utils.py
src/transformers/modeling_utils.py
tests/test_modeling_bart.py
tests/test_modeling_gpt2.py
tests/test_modeling_tf_gpt2.py
tests/test_modeling_tf_transfo_xl.py
tests/test_modeling_transfo_xl.py
==================
8becb7329;Nitish Shirish Keskar;2020-03-19 15:25:30 -0700;removing torch.cuda.empty_cache() from TF function (#3267)
torch.cuda.empty_cache() was being called from a TF function (even when torch is unavailable)
not sure any replacement is needed if TF OOMs
==

examples/benchmarks.py
==================
ecfd33631;Julien Chaumond;2020-03-19 18:23:03 -0400;Simpler Error message when loading config/model with .from_pretrained() (#3341)

==

src/transformers/configuration_utils.py
==================
8eeefcb57;Kyeongpil Kang;2020-03-20 07:21:49 +0900;Update 01-training-tokenizers.ipynb (typo issue) (#3343)
I found there are two grammar errors or typo issues in the explanation of the encoding properties.

The original sentences:
If your was made of multiple \"parts\" such as (question, context), then this would be a vector with for each token the segment it belongs to
If your has been truncated into multiple subparts because of a length limit (for BERT for example the sequence length is limited to 512), this will contain all the remaining overflowing parts.

I think "input" should be inserted after the phrase "If your".
==

notebooks/01-training-tokenizers.ipynb
==================
bbf26c4e6;Patrick von Platen;2020-03-19 23:18:23 +0100;Support T5 Generation (#3228)
* fix conflicts

* update bart max length test

* correct spelling mistakes

* implemented model specific encode function

* fix merge conflicts

* better naming

* save intermediate state -> need to rethink strucuture a bit

* leave tf problem as it is for now

* current version

* add layers.pop

* remove ipdb

* make style

* clean return cut decoding

* remove ipdbs

* Fix restoring layers in the decoders that doesnt exists.

* push good intermediate solution for now

* fix conflicts

* always good to refuse to merge conflicts when rebasing

* fix small bug

* improve function calls

* remove unused file

* add correct scope behavior for t5_generate

Co-authored-by: Morgan Funtowicz <funtowiczmo@gmail.com>
==

src/transformers/__init__.py
src/transformers/configuration_t5.py
src/transformers/convert_pytorch_checkpoint_to_tf2.py
src/transformers/modeling_auto.py
src/transformers/modeling_bart.py
src/transformers/modeling_t5.py
src/transformers/modeling_tf_auto.py
src/transformers/modeling_tf_pytorch_utils.py
src/transformers/modeling_tf_t5.py
src/transformers/modeling_tf_utils.py
src/transformers/modeling_utils.py
tests/test_modeling_bart.py
tests/test_modeling_common.py
tests/test_modeling_t5.py
tests/test_modeling_tf_common.py
tests/test_modeling_tf_t5.py
==================
656e1386a;Julien Chaumond;2020-03-18 22:08:48 -0400;Fix #3305: run_ner only possible on ModelForTokenClassification models

==

examples/ner/run_ner.py
==================
0c44b1191;husein zolkepli;2020-03-19 20:32:51 +0800;add bert bahasa readme

==

model_cards/huseinzol05/bert-base-bahasa-cased/README.md
==================
e99af3b17;Manuel Romero;2020-03-19 11:51:02 +0100;Create model card for bert-small-finetuned-squadv2

==

model_cards/mrm8488/bert-small-finetuned-squadv2/README.md
==================
39db05526;Manuel Romero;2020-03-19 20:07:39 +0100;Merge pull request #3348 from mrm8488/patch-28
Create card for BERT-Mini finetuned on SQuAD v2
==

model_cards/mrm8488/bert-mini-finetuned-squadv2/README.md
==================
dedc7a8fd;Manuel Romero;2020-03-19 11:19:23 +0100;Create card for BERT-Tiny fine-tuned on SQuAD v2
- Only 17MB of Model weights!!
==

model_cards/mrm8488/bert-tiny-finetuned-squadv2/README.md
==================
676adf862;Manuel Romero;2020-03-19 10:46:15 +0100;Created card for spanbert-finetuned-squadv1

==

model_cards/mrm8488/spanbert-finetuned-squadv1/README.md
==================
11d8bcc9d;Antti Virtanen;2020-03-19 21:06:01 +0200;Add model cards for FinBERT. (#3331)
* Add a model card for FinBERT

This is a copy of https://github.com/TurkuNLP/FinBERT/blob/master/README.md.

* Added a file for uncased.

* Add metadata for cased.

* Added metadata for uncased.
==

model_cards/TurkuNLP/bert-base-finnish-cased-v1/README.md
model_cards/TurkuNLP/bert-base-finnish-uncased-v1/README.md
==================
f049be7ad;Lysandre Debut;2020-03-19 13:53:05 -0400;Export ALBERT main layer in TensorFlow (#3354)

==

src/transformers/__init__.py
==================
3bedfd334;Kyeongpil Kang;2020-03-20 01:22:47 +0900;Fix wrong link for the notebook file (#3344)
For the tutorial of "How to generate text", the URL link was wrong (it was linked to the tutorial of "How to train a language model").

I fixed the URL.
==

notebooks/README.md
==================
b2c2c31c6;Serkan Karakulak;2020-03-19 12:08:31 -0400;Minor Bug Fix for Running Roberta on Glue (#3240)
* added return_token_type_ids argument for tokenizers which do not generate return_type_ids by default

* fixed styling

* Style

Co-authored-by: LysandreJik <lysandre.debut@reseau.eseo.fr>
==

src/transformers/data/processors/glue.py
==================
4e4403c9b;Sam Shleifer;2020-03-19 11:56:54 -0400;[BART] torch 1.0 compatibility (#3322)
* config.activation_function
==

src/transformers/activations.py
src/transformers/configuration_bart.py
src/transformers/modeling_bart.py
==================
c44a17db1;mataney;2020-03-19 17:21:21 +0200;[FIX] not training when epoch is small (#3006)
* solving bug where for small epochs and large gradient_accumulation_steps we never train

* black formatting

* no need to change these files
==

examples/run_glue.py
==================
ad7233fc0;Sam Shleifer;2020-03-19 11:16:51 -0400;[BART] cleanup: remove redundant kwargs, improve docstrings (#3319)

==

src/transformers/modeling_bart.py
tests/test_modeling_bart.py
==================
cd21d8bc0;Mohamed El-Geish;2020-03-19 06:49:25 -0700;Typo in warning message (#3219)
`T5Tokenizer` instead of `XLNetTokenizer`
==

src/transformers/tokenization_t5.py
==================
8d3e218ea;Matthew Goldey;2020-03-19 08:47:54 -0500;fix typo in docstring demonstrating usage (#3213)

==

src/transformers/modeling_encoder_decoder.py
==================
cec3cdda1;Patrick von Platen;2020-03-19 09:55:17 +0100;Fix input ids can be none attn mask (#3345)
* fix issue 3289

* fix attention mask if input_ids None behavior
==

src/transformers/modeling_ctrl.py
src/transformers/modeling_gpt2.py
==================
f6d813aaa;Junyi_Li;2020-03-19 11:05:29 +0800;Create README.md

==

model_cards/clue/albert_chinese_small/README.md
==================
939328111;Junyi_Li;2020-03-19 11:07:23 +0800;Create README.md
roberta_chinese_base card
==

model_cards/clue/roberta_chinese_base/README.md
==================
29442d2ed;Junyi_Li;2020-03-19 11:06:35 +0800;Create README.md
albert_chinese_tiny card
==

model_cards/clue/albert_chinese_tiny/README.md
==================
20139b7c8;Kyle Lo;2020-03-18 12:45:11 -0700;Added model cards for SciBERT models uploaded under AllenAI org (#3330)
* Create README.md

* model card

* add model card for cased
==

model_cards/allenai/scibert_scivocab_cased/README.md
model_cards/allenai/scibert_scivocab_uncased/README.md
==================
cae334c43;Morgan Funtowicz;2020-03-18 17:11:42 +0100;Improve fill-mask pipeline example in 03-pipelines notebook.
Remove hardcoded mask_token and use the value provided by the tokenizer.

==

notebooks/03-pipelines.ipynb
==================
4b1970bb4;Branden Chan;2020-03-18 15:33:46 +0100;Create README.md

==

model_cards/deepset/sentence_bert/README.md
==================
d6afbd323;Lysandre Debut;2020-03-18 09:52:49 -0400;XLM-R Tokenizer now passes common tests + Integration tests (#3198)
* XLM-R now passes common tests + Integration tests

* Correct mask index

* Model input names

* Style

* Remove text preprocessing

* Unneccessary import
==

src/transformers/tokenization_xlm_roberta.py
tests/test_tokenization_xlm_roberta.py
==================
292186a3e;Patrick von Platen;2020-03-18 14:24:27 +0100;Adding LM Head to Transfo-XL and first step to fixing problem with Adaptive Embeddings in TransfoXL (#3286)
* first commit

* work in progress

* make language generation task pass

* update to working version for LM

* delete print

* remove dead code

* make style
==

src/transformers/__init__.py
src/transformers/modeling_tf_transfo_xl.py
src/transformers/modeling_transfo_xl.py
src/transformers/modeling_transfo_xl_utilities.py
tests/test_modeling_tf_common.py
tests/test_modeling_tf_transfo_xl.py
tests/test_modeling_transfo_xl.py
==================
efdb46b6e;Patrick von Platen;2020-03-18 13:24:28 +0100;add link to blog post (#3326)

==

notebooks/README.md
==================
ddb10c644;Patrick von Platen;2020-03-18 13:24:09 +0100;improve doctstring (#3327)

==

src/transformers/modeling_tf_utils.py
src/transformers/modeling_utils.py
==================
d7f98cd3e;Junyi_Li;2020-03-18 11:28:13 +0800;Init card for model

==

model_cards/clue/roberta_chinese_3L312_clue_tiny/README.md
==================
38a555a83;Sam Shleifer;2020-03-17 18:04:21 -0400;Add Summarization to Pipelines (#3128)
* passing

* Undo stupid chg

* docs

* undo rename

* delete-cruft

* only import if you have torch

* Dont rely on dict ordering

* Fix dict ordering upstream

* docstring link

* docstring link

* remove trailing comma for 3.5 compat

* new name

* delegate kwarging

* Update kwargs
==

docs/source/main_classes/pipelines.rst
src/transformers/__init__.py
src/transformers/pipelines.py
tests/test_pipelines.py
==================
2b60a26b4;J.P Lee;2020-03-18 01:30:10 +0900;Update examples/ner/run_ner.py to use AutoModel (#3305)
* Update examples/ner/run_ner.py to use AutoModel

* Fix missing code and apply `make style` command
==

examples/ner/run_ner.py
==================
e41212c71;Manuel Romero;2020-03-17 17:29:11 +0100;Create model card for CodeBERTaPy (#3309)

==

model_cards/mrm8488/CodeBERTaPy/README.md
==================
0f1bc0d68;Julien Chaumond;2020-03-17 12:02:51 -0400;[model_cards] Add google thumbnail

==

model_cards/iuliaturc/bert_uncased_L-2_H-128_A-2/README.md
model_cards/jannesg/bertsson/README.md
==================
930c9412b;Nathan Raw;2020-03-17 09:46:42 -0600;[WIP] Lightning glue example (#3290)
* :sparkles: Alter base pl transformer to use automodels

* :bug: Add batch size env variable to function call

* :lipstick: Apply black code style from Makefile

* :truck: Move lightning base out of ner directory

* :sparkles: Add lightning glue example

* :lipstick: self

* move _feature_file to base class

* :sparkles: Move eval logging to custom callback

* :lipstick: Apply black code style

* :bug: Add parent to pythonpath, remove copy command

* :bug: Add missing max_length kwarg
==

examples/glue/README.md
examples/glue/run_pl.sh
examples/glue/run_pl_glue.py
examples/ner/run_pl.sh
examples/ner/run_pl_ner.py
examples/transformer_base.py
==================
e8f44af5b;Patrick von Platen;2020-03-17 15:52:37 +0100;[generate] do_sample default back to False (#3298)
* change do_samples back

* None better default as boolean

* adapt do_sample to True in test example

* make style
==

examples/summarization/bart/evaluate_cnn.py
src/transformers/modeling_tf_utils.py
src/transformers/modeling_utils.py
tests/test_modeling_bart.py
tests/test_modeling_common.py
tests/test_modeling_tf_common.py
==================
2187c49f5;Thomas Wolf;2020-03-17 15:17:11 +0100;CPU/GPU memory benchmarking utilities - Remove support for python 3.5 (now only 3.6+) (#3186)
* memory benchmark rss

* have both forward pass and line-by-line mem tracing

* cleaned up tracing

* refactored and cleaning up API

* no f-strings yet...

* add GPU mem logging

* fix GPU memory monitoring

* style and quality

* clean up and doc

* update with comments

* Switching to python 3.6+

* fix quality
==

.circleci/config.yml
README.md
docs/source/installation.md
examples/benchmarks.py
examples/requirements.txt
setup.py
src/transformers/__init__.py
src/transformers/benchmark_utils.py
src/transformers/configuration_gpt2.py
src/transformers/modeling_gpt2.py
src/transformers/modeling_utils.py
==================
bd3feddf6;Jannes;2020-03-17 14:05:11 +0100;Create README.md (#3306)
* Create README.md

* Updated README.md
==

model_cards/jannesg/bertsson/README.md
==================
68ef0a111;Julien Chaumond;2020-03-16 23:37:42 -0400;[model_cards] Symlink all Google AI's BERT Miniatures to source model card

==

model_cards/google/bert_uncased_L-10_H-128_A-2/README.md
model_cards/google/bert_uncased_L-10_H-256_A-4/README.md
model_cards/google/bert_uncased_L-10_H-512_A-8/README.md
model_cards/google/bert_uncased_L-10_H-768_A-12/README.md
model_cards/google/bert_uncased_L-12_H-128_A-2/README.md
model_cards/google/bert_uncased_L-12_H-256_A-4/README.md
model_cards/google/bert_uncased_L-12_H-512_A-8/README.md
model_cards/google/bert_uncased_L-12_H-768_A-12/README.md
model_cards/google/bert_uncased_L-2_H-128_A-2/README.md
model_cards/google/bert_uncased_L-2_H-256_A-4/README.md
model_cards/google/bert_uncased_L-2_H-512_A-8/README.md
model_cards/google/bert_uncased_L-2_H-768_A-12/README.md
model_cards/google/bert_uncased_L-4_H-128_A-2/README.md
model_cards/google/bert_uncased_L-4_H-256_A-4/README.md
model_cards/google/bert_uncased_L-4_H-512_A-8/README.md
model_cards/google/bert_uncased_L-4_H-768_A-12/README.md
model_cards/google/bert_uncased_L-6_H-128_A-2/README.md
model_cards/google/bert_uncased_L-6_H-256_A-4/README.md
model_cards/google/bert_uncased_L-6_H-512_A-8/README.md
model_cards/google/bert_uncased_L-6_H-768_A-12/README.md
model_cards/google/bert_uncased_L-8_H-128_A-2/README.md
model_cards/google/bert_uncased_L-8_H-256_A-4/README.md
model_cards/google/bert_uncased_L-8_H-512_A-8/README.md
model_cards/google/bert_uncased_L-8_H-768_A-12/README.md
==================
b2c1a447f;Sam Shleifer;2020-03-16 23:09:10 -0400;[BART] Delete redundant unit test (#3302)

==

tests/test_modeling_bart.py
==================
b2028cc26;iuliaturc-google;2020-03-16 21:51:46 -0400;Add model card for Google AI's BERT Miniatures (#3301)
This model card is intended to be shared among all models under google/bert_uncased_*
(We'll need some support from HuggingFace to get this card cross-linked from all models)
==

model_cards/iuliaturc/bert_uncased_L-2_H-128_A-2/README.md
==================
475917631;Patrick von Platen;2020-03-16 19:38:19 +0100;add camembert for Question answering for examples

==

src/transformers/__init__.py
==================
11573231c;Sam Shleifer;2020-03-16 12:47:53 -0400;[BART] generation_mode as a kwarg not a class attribute (#3278)

==

src/transformers/modeling_bart.py
src/transformers/modeling_utils.py
==================
de697935a;Manuel Romero;2020-03-16 08:58:55 +0100;Create model card for spanbert-finetuned-squadv2

==

model_cards/mrm8488/spanbert-finetuned-squadv2/README.md
==================
3ddd2029b;Manuel Romero;2020-03-16 10:15:42 +0100;Create CodeBERTaJS model card

==

model_cards/mrm8488/codeBERTaJS/README.md
==================
879e1d323;Julien Plu;2020-03-16 14:29:21 +0100;Add TF2 version of FlauBERT (#2700)
* Add TF2 version of FlauBERT

* Add TF2 version of FlauBERT

* Add documentation

* Apply style and quality

* Apply style once again

Co-authored-by: Lysandre Debut <lysandre@huggingface.co>
==

src/transformers/__init__.py
src/transformers/convert_pytorch_checkpoint_to_tf2.py
src/transformers/modeling_tf_camembert.py
src/transformers/modeling_tf_flaubert.py
==================
af471ce5e;Patrick von Platen;2020-03-16 09:48:30 +0100;Improved Error message when loading config/model with .from_pretrained() (#3247)
* better error message

* better error message

* update to model identifier instead of url

* update to model identifier instead of ur
==

src/transformers/configuration_utils.py
==================
5ea8ba67b;Sam Shleifer;2020-03-15 23:00:44 -0400;[BART] Remove unused kwargs (#3279)
* Remove unused kwargs
* dont call forward in tests
==

examples/summarization/bertabs/modeling_bertabs.py
src/transformers/modeling_bart.py
tests/test_modeling_bart.py
==================
3814e167d;Thomas Wolf;2020-03-14 15:08:59 +0100;Merge pull request #3225 from patrickvonplaten/finalize_merge_bart_generate_into_default_generate
Complete merge Seq-2-Seq generation into default generation
==
==================
2bd79e23d;Sam Shleifer;2020-03-13 19:48:26 -0400;[BART] FP16 testing fixes (#3266)

==

src/transformers/modeling_bart.py
tests/test_modeling_bart.py
==================
8320feec0;Julien Chaumond;2020-03-13 18:28:09 -0400;[model_cards] CodeBERTa

==

model_cards/huggingface/CodeBERTa-language-id/README.md
model_cards/huggingface/CodeBERTa-small-v1/README.md
==================
ab756f713;Patrick von Platen;2020-03-13 21:00:54 +0100;add gpt2-xl for tf

==

src/transformers/modeling_tf_gpt2.py
==================
4f75d380a;Patrick von Platen;2020-03-13 16:35:52 +0100;make style

==

examples/summarization/bart/evaluate_cnn.py
==================
c2ee3840a;Patrick von Platen;2020-03-13 16:34:44 +0100;update file to new starting token logic

==

examples/summarization/bart/evaluate_cnn.py
==================
cc4c37952;Benjamin Muller;2020-03-13 20:08:53 +0800;Create camembert-base-README.md

==

model_cards/camembert-base-README.md
==================
afea70c01;dependabot[bot];2020-03-12 18:16:05 +0000;Bump psutil from 5.6.3 to 5.6.6 in /examples/distillation
Bumps [psutil](https://github.com/giampaolo/psutil) from 5.6.3 to 5.6.6.
- [Release notes](https://github.com/giampaolo/psutil/releases)
- [Changelog](https://github.com/giampaolo/psutil/blob/master/HISTORY.rst)
- [Commits](https://github.com/giampaolo/psutil/compare/release-5.6.3...release-5.6.6)

Signed-off-by: dependabot[bot] <support@github.com>
==

examples/distillation/requirements.txt
==================
087465b94;Sam Shleifer;2020-03-12 19:38:05 -0400;add BART to README (#3255)

==

README.md
==================
6a82f774f;Patrick von Platen;2020-03-12 21:10:51 +0100;fix typo

==

tests/test_modeling_bart.py
==================
f1c71da11;Patrick von Platen;2020-03-12 21:00:54 +0100;fix eos_token_ids in test

==

tests/test_modeling_bart.py
==================
6047f46b1;Patrick von Platen;2020-03-12 20:17:50 +0100;re-add eos token to get good bart results

==

src/transformers/modeling_utils.py
tests/test_modeling_bart.py
==================
c11160114;Patrick von Platen;2020-03-11 14:30:07 +0100;small clean-up

==

src/transformers/modeling_utils.py
==================
2e81b9d8d;Sam Shleifer;2020-03-12 10:36:37 -0400;Bart: update example for #3140 compatibility (#3233)
* Update bart example docs
==

examples/summarization/bart/README.md
examples/summarization/bart/evaluate_cnn.py
src/transformers/modeling_bart.py
==================
72768b6b9;Julien Chaumond;2020-03-12 10:03:59 -0400;[model_cards] polbert: simplify usage example with pipelines
Co-Authored-By: Darek K≈Çeczek <darek.kleczek@gmail.com>

==

model_cards/dkleczek/bert-base-polish-uncased-v1/README.md
==================
a4c75f149;Julien Chaumond;2020-03-11 19:11:19 -0400;[ci] last resort

==

.github/workflows/self-push.yml
==================
824e320d9;Julien Chaumond;2020-03-11 18:52:10 -0400;[ci] Fixup c6cf925

==

.github/workflows/self-push.yml
==================
c6cf925ff;Julien Chaumond;2020-03-11 18:49:19 -0400;[ci] last resort
while looking for fix to https://twitter.com/julien_c/status/1237864185821708291

==

.github/workflows/self-push.yml
==================
14e455b71;Stefan Schweter;2020-03-11 22:49:59 +0100;[model_cards] üáπüá∑ Add new (cased) DistilBERTurk model

==

model_cards/dbmdz/distilbert-base-turkish-cased/README.md
==================
f65f74bbc;Emily Alsentzer;2020-03-11 12:37:00 -0400;Create README.md (#3230)

==

model_cards/emilyalsentzer/Bio_Discharge_Summary_BERT/README.md
==================
324292cfc;Emily Alsentzer;2020-03-11 12:36:33 -0400;Add Bio+ Clinical BERT model card (#3229)
* Create README.md

* Update README.md
==

model_cards/emilyalsentzer/Bio_ClinicalBERT/README.md
==================
e43afb1bb;Julien Chaumond;2020-03-11 11:36:47 -0400;[model_cards] DialoGPT: How to use + thumbnail + conversational tag
cc @dreasysnail

Co-Authored-By: Patrick von Platen <patrick.v.platen@gmail.com>

==

model_cards/microsoft/DialoGPT-large/README.md
model_cards/microsoft/DialoGPT-medium/README.md
model_cards/microsoft/DialoGPT-small/README.md
==================
5085df995;Julien Chaumond;2020-03-11 09:29:22 -0400;[model_cards] PolBERT tweaks

==

model_cards/dkleczek/bert-base-polish-uncased-v1/README.md
==================
19a63d824;Darek K≈Çeczek;2020-03-11 14:12:48 +0100;Create Readme.md model card (#3221)

==

model_cards/dkleczek/bert-base-polish-uncased-v1/README.md
==================
dc848c299;Yizhe;2020-03-10 16:38:16 -0700;Create README.md

==

model_cards/microsoft/DialoGPT-large/README.md
==================
6ad221daf;Yizhe;2020-03-10 17:03:06 -0700;Create README.md

==

model_cards/microsoft/DialoGPT-small/README.md
==================
735180aa1;Yizhe;2020-03-10 16:36:55 -0700;Create README.md

==

model_cards/microsoft/DialoGPT-medium/README.md
==================
6c61c0801;Manuel Romero;2020-03-11 00:33:05 +0100;Create README.md

==

model_cards/mrm8488/bert-multi-cased-finedtuned-xquad-tydiqa-goldp/README.md
==================
235616686;Manuel Romero;2020-03-10 22:59:22 +0100;Update README.md
- Update title
- Remove metrics
==

model_cards/mrm8488/xlm-multi-finetuned-xquadv1/README.md
==================
5bb00c817;Manuel Romero;2020-03-10 22:56:15 +0100;Update README.md
Change title to clarify the model description
==

model_cards/mrm8488/bert-multi-cased-finetuned-xquadv1/README.md
==================
601e42475;Manuel Romero;2020-03-10 22:53:32 +0100;Update README.md

==

model_cards/mrm8488/bert-multi-uncased-finetuned-xquadv1/README.md
==================
1b9e765b2;Manuel Romero;2020-03-10 22:51:49 +0100;Update README.md
- Remove metrics until tested on other xquad benchmarks
==

model_cards/mrm8488/bert-multi-cased-finetuned-xquadv1/README.md
==================
db29ffc97;Thomas Wolf;2020-03-11 13:21:53 +0100;Merge pull request #3140 from patrickvonplaten/merge_bart_generate_into_default_generate
Merge bart generate into default generate
==
==================
ac303eae4;Patrick von Platen;2020-03-11 12:24:30 +0100;fix problem with half

==

tests/test_modeling_bart.py
==================
bc9d5d917;Patrick von Platen;2020-03-11 12:15:38 +0100;make all tensors half precision

==

tests/test_modeling_bart.py
==================
a332cc9f7;Patrick von Platen;2020-03-11 11:53:36 +0100;finalize generation merge

==

src/transformers/configuration_bart.py
src/transformers/modeling_bart.py
src/transformers/modeling_utils.py
tests/test_modeling_bart.py
==================
1ba21f96c;Patrick von Platen;2020-03-10 20:29:30 +0100;fix bug in tf no_repeat_ngram_size

==

src/transformers/modeling_tf_utils.py
==================
d997ac781;Patrick von Platen;2020-03-10 20:24:26 +0100;fix typo

==

src/transformers/modeling_utils.py
==================
7351a8dba;Patrick von Platen;2020-03-10 16:53:09 +0100;re-add scoring filtering

==

src/transformers/modeling_utils.py
tests/test_modeling_bart.py
==================
9b8ee8cea;Patrick von Platen;2020-03-10 14:32:21 +0100;delete print and make style

==

src/transformers/modeling_tf_utils.py
==================
ca1330f0b;Patrick von Platen;2020-03-10 14:22:16 +0100;do not mess with the negative sign

==

src/transformers/modeling_tf_utils.py
==================
10989715d;Patrick von Platen;2020-03-09 20:25:09 +0100;rename variable

==

src/transformers/modeling_tf_utils.py
==================
cf0629056;Patrick von Platen;2020-03-09 20:05:41 +0100;remove ipdb

==

src/transformers/modeling_tf_utils.py
==================
374deef48;Patrick von Platen;2020-03-09 20:01:20 +0100;fixed typo

==

src/transformers/modeling_utils.py
tests/test_modeling_bart.py
==================
a2c8e516c;Patrick von Platen;2020-03-09 19:47:45 +0100;fix torch to tf translation

==

src/transformers/modeling_tf_utils.py
==================
ca2047bc3;Patrick von Platen;2020-03-09 19:18:11 +0100;refactor variable naming and improve tf generate in line with torch generate

==

src/transformers/modeling_tf_utils.py
src/transformers/modeling_utils.py
==================
41b437ea3;patrickvonplaten;2020-03-09 00:33:12 +0100;add draft version of propsoed changes for ROGUE score

==

src/transformers/modeling_utils.py
tests/test_modeling_bart.py
==================
a5751f757;patrickvonplaten;2020-03-08 22:27:00 +0100;fix bug with attention_mask as optional input argument

==

tests/test_modeling_bart.py
==================
629aac92e;patrickvonplaten;2020-03-07 11:45:45 +0100;do not allow do_sample and weird force bos token things

==

src/transformers/modeling_utils.py
==================
d880a5fbd;patrickvonplaten;2020-03-07 10:55:23 +0100;finalized PR

==

src/transformers/modeling_utils.py
tests/test_modeling_bart.py
==================
2acfe6396;patrickvonplaten;2020-03-06 22:19:01 +0100;best current version and make style

==

src/transformers/configuration_t5.py
src/transformers/modeling_bart.py
src/transformers/modeling_utils.py
tests/test_modeling_bart.py
==================
c62444da3;patrickvonplaten;2020-03-08 14:26:08 +0100;fix conflicts

==

src/transformers/modeling_bart.py
src/transformers/modeling_utils.py
tests/test_modeling_bart.py
==================
77e677506;Patrick von Platen;2020-03-06 15:14:43 +0100;add current changes

==

tests/test_modeling_bart.py
==================
333affcb8;Patrick von Platen;2020-03-06 15:14:36 +0100;add current changes

==

src/transformers/modeling_utils.py
==================
421216997;Patrick von Platen;2020-03-06 14:51:13 +0100;comment out stuff

==

tests/test_modeling_bart.py
==================
7a11e925c;Patrick von Platen;2020-03-06 14:39:28 +0100;work in progress

==

src/transformers/configuration_bart.py
src/transformers/configuration_t5.py
src/transformers/configuration_utils.py
src/transformers/modeling_bart.py
src/transformers/modeling_utils.py
tests/test_modeling_bart.py
==================
5b3000d93;Patrick von Platen;2020-03-05 17:02:56 +0100;renamed min_len to min_length

==

examples/summarization/bart/evaluate_cnn.py
==================
aceb3fbaf;Patrick von Platen;2020-03-05 16:34:47 +0100;only do output_past=True for language generation in bart

==

tests/test_modeling_bart.py
tests/test_modeling_common.py
==================
7cba11fb9;Patrick von Platen;2020-03-05 15:48:00 +0100;better naming

==

src/transformers/modeling_bart.py
src/transformers/modeling_utils.py
tests/test_modeling_bart.py
==================
ff648221b;Patrick von Platen;2020-03-06 11:31:19 +0100;fix conflicts

==

src/transformers/configuration_utils.py
src/transformers/modeling_bart.py
src/transformers/modeling_utils.py
tests/test_modeling_bart.py
tests/test_modeling_common.py
==================
c0d9dd3ba;Patrick von Platen;2020-03-05 14:56:56 +0100;refactored code a bit and made more generic

==

src/transformers/configuration_utils.py
src/transformers/modeling_utils.py
tests/test_modeling_bart.py
tests/test_modeling_common.py
==================
d8e2b3c54;Patrick von Platen;2020-03-06 11:28:10 +0100;fix conflicts

==

src/transformers/modeling_bart.py
src/transformers/modeling_utils.py
tests/test_modeling_bart.py
tests/test_modeling_common.py
==================
d6de6423b;Julien Chaumond;2020-03-10 16:52:44 -0400;[doc] --organization tweak
Co-Authored-By: Thomas Wolf <thomwolf@users.noreply.github.com>

==

README.md
docs/source/model_sharing.md
==================
0e56dc307;Julien Chaumond;2020-03-10 16:42:01 -0400;[doc] Document the new --organization flag of CLI

==

README.md
docs/source/model_sharing.md
==================
270dfa1c8;Julien Chaumond;2020-03-10 15:09:29 -0400;[dialogpt] conversion script
Reference: https://github.com/huggingface/transformers/pull/1778#issuecomment-567675530

cc @patrickvonplaten and @dreasysnail

==

src/transformers/convert_dialogpt_original_pytorch_checkpoint_to_pytorch.py
==================
2661d8068;Manuel Romero;2020-03-10 05:10:41 +0100;Update README.md
- Clarify that the model is not trained on the evaluation dataset
==

model_cards/mrm8488/bert-multi-cased-finetuned-xquadv1/README.md
==================
6a13448ad;Manuel Romero;2020-03-10 05:06:08 +0100;Update README.md
- Fix path of tokenizer
- Clarify that the model is not trained on the evaluation set
==

model_cards/mrm8488/bert-multi-uncased-finetuned-xquadv1/README.md
==================
e57533cca;Manuel Romero;2020-03-09 21:21:55 +0100;Create README.md

==

model_cards/mrm8488/xlm-multi-finetuned-xquadv1/README.md
==================
31f2437f0;Patrick von Platen;2020-03-10 11:29:17 +0100;Merge pull request #3191 from patrickvonplaten/add_integration_tests_lm_generate_torch_tf
Add integration tests lm generate torch tf
==
==================
5ca356a46;Shubham Agarwal;2020-03-10 00:43:38 +0000;NER - pl example (#3180)
* 1. seqeval required by ner pl example. install from examples/requirements. 2. unrecognized arguments: save_steps

* pl checkpoint callback filenotfound error: make directory and pass

* #3159 pl checkpoint path difference

* 1. Updated Readme for pl 2. pl script now also correct displays logs 3. pass gpu ids compared to number of gpus

* Updated results in readme

* 1. updated readme 2. removing deprecated pl methods 3. finalizing scripts

* comment length check

* using deprecated validation_end for stable results

* style related changes
==

examples/ner/README.md
examples/ner/run_pl.sh
examples/ner/run_pl_ner.py
==================
f51ba059b;Travis McGuire;2020-03-09 16:27:11 -0700;Model card for albert-base-v2-squad2

==

model_cards/twmkn9/albert-base-v2-squad2/README.md
==================
cbf8f5d32;Julien Chaumond;2020-03-09 17:29:49 -0400;[model upload] Support for organizations

==

src/transformers/commands/user.py
src/transformers/hf_api.py
tests/test_hf_api.py
==================
525b6b1c5;Lysandre;2020-03-09 16:52:30 -0400;TFQA pipeline marked as slow test

==

tests/test_pipelines.py
==================
3aca02efb;Sam Shleifer;2020-03-09 15:09:35 -0400;Bart example: model.to(device) (#3194)

==

examples/summarization/bart/evaluate_cnn.py
==================
5164ea91a;Lysandre Debut;2020-03-09 13:48:58 -0400;Skipping outputs (#3116)
* Minimal example

* Proposal 2

* Proposal 2 for fast tokenizers

* Typings

* Docs

* Revert "Docs" for easier review

This reverts commit eaf0f97062e809887704a542144c537f769d5223.

* Remove unnecessary assignments

* Tests

* Fix faulty type

* Remove prints

* return_outputs -> model_input_names

* Revert "Revert "Docs" for easier review"

This reverts commit 6fdc69408102bf695797f2dfddbb6350c6b9e722.

* code quality
==

src/transformers/tokenization_distilbert.py
src/transformers/tokenization_roberta.py
src/transformers/tokenization_utils.py
tests/test_tokenization_common.py
==================
49debe62f;Patrick von Platen;2020-03-09 16:29:57 +0100;Merge pull request #3190 from patrickvonplaten/fix_repetition_penalty_in_tf_generate
fix repetition penalty mask in tf
==
==================
847d37030;Patrick von Platen;2020-03-09 16:18:29 +0100;fix typo

==

src/transformers/configuration_utils.py
==================
eb3e6cb04;Lysandre;2020-03-09 10:54:18 -0400;cased -> uncased in BERT SQuAD example
closes #3183
==

examples/README.md
==================
9050ffe03;Patrick von Platen;2020-03-09 15:43:12 +0100;delete w! -> need to be more careful with vim

==

w!
==================
efb619235;Patrick von Platen;2020-03-09 15:31:21 +0100;add print statement to avoid code quality problem

==

tests/test_modeling_tf_transfo_xl.py
==================
b12541c4d;Patrick von Platen;2020-03-09 13:58:01 +0000;test ctrl

==

tests/test_modeling_ctrl.py
tests/test_modeling_tf_ctrl.py
==================
3e624c64c;Patrick von Platen;2020-03-09 14:55:11 +0100;fix repetition penalty mask in tf

==

src/transformers/configuration_utils.py
src/transformers/modeling_tf_utils.py
==================
b73dd1a0e;Patrick von Platen;2020-03-09 11:34:31 +0100;fix typo in test xlm tf

==

tests/test_modeling_tf_xlm.py
==================
4620caa86;Patrick von Platen;2020-03-09 11:18:54 +0100;fix if use lang embeddings in tf xlm

==

src/transformers/modeling_tf_xlm.py
tests/test_modeling_tf_xlm.py
==================
fbd02d469;patrickvonplaten;2020-03-08 21:45:55 +0100;fixed all tests, still need to check ctrl tf and pt and xlm tf

==

tests/test_modeling_ctrl.py
tests/test_modeling_tf_gpt2.py
tests/test_modeling_tf_openai_gpt.py
tests/test_modeling_tf_transfo_xl.py
tests/test_modeling_tf_xlm.py
tests/test_modeling_tf_xlnet.py
tests/test_modeling_xlm.py
==================
b4a3a6474;patrickvonplaten;2020-03-08 16:25:03 +0100;fix xlnet & transfotests

==

tests/test_modeling_tf_transfo_xl.py
tests/test_modeling_tf_xlnet.py
tests/test_modeling_transfo_xl.py
tests/test_modeling_xlnet.py
==================
b29fed790;Param bhavsar;2020-03-08 15:10:22 +0530;Updated `Tokenw   ise` in print statement to `Token wise`

==

notebooks/02-transformers.ipynb
==================
66c827656;patrickvonplaten;2020-03-08 15:35:08 +0100;fix typo in test gpt2

==

tests/test_modeling_gpt2.py
==================
314bdc7c1;patrickvonplaten;2020-03-08 15:34:20 +0100;fix typo in test

==

tests/test_modeling_gpt2.py
==================
575976144;patrickvonplaten;2020-03-08 15:29:10 +0100;updated all tests

==

tests/test_modeling_ctrl.py
tests/test_modeling_gpt2.py
tests/test_modeling_openai.py
tests/test_modeling_tf_ctrl.py
tests/test_modeling_tf_gpt2.py
tests/test_modeling_tf_openai_gpt.py
tests/test_modeling_tf_transfo_xl.py
tests/test_modeling_tf_xlm.py
tests/test_modeling_tf_xlnet.py
tests/test_modeling_transfo_xl.py
tests/test_modeling_xlm.py
tests/test_modeling_xlnet.py
w!
==================
e03129ad4;Julien Chaumond;2020-03-06 18:07:44 -0500;[model_cards] Small formatting fix
cc @mrm8488

==

model_cards/mrm8488/bert-multi-cased-finetuned-xquadv1/README.md
model_cards/mrm8488/bert-multi-uncased-finetuned-xquadv1/README.md
==================
08a70fb39;Julien Chaumond;2020-03-06 17:50:38 -0500;[model_cards] Fixup d6df9a8f

==

model_cards/voidful/albert_chinese_base/README.md
model_cards/voidful/albert_chinese_large/README.md
model_cards/voidful/albert_chinese_small/README.md
model_cards/voidful/albert_chinese_tiny/README.md
model_cards/voidful/albert_chinese_xlarge/README.md
model_cards/voidful/albert_chinese_xxlarge/README.md
==================
0ae91c80a;Lysandre Debut;2020-03-06 17:26:18 -0500;Change back pipeline signatures (#3105)
* Change back pipeline signatures

* String types for non-imported objects
==

src/transformers/pipelines.py
==================
d6df9a8ff;voidful;2020-03-06 22:22:02 +0800;[model_cards]Add albert chinese model(tiny,small,base,large,xlarge,xxlarge)

==

model_cards/voidful/albert_chinese_base/README.md
model_cards/voidful/albert_chinese_large/README.md
model_cards/voidful/albert_chinese_small/README.md
model_cards/voidful/albert_chinese_tiny/README.md
model_cards/voidful/albert_chinese_xlarge/README.md
model_cards/voidful/albert_chinese_xxlarge/README.md
==================
c52716d46;Manuel Romero;2020-03-05 19:35:37 +0100;Create README.md

==

model_cards/mrm8488/bert-multi-uncased-finetuned-xquadv1/README.md
==================
73a0c2537;Aleksei Lymar;2020-03-05 17:57:53 +0300;remove excess line breaks in DeepPavlov model cards

==

model_cards/DeepPavlov/bert-base-bg-cs-pl-ru-cased/README.md
model_cards/DeepPavlov/bert-base-cased-conversational/README.md
model_cards/DeepPavlov/bert-base-multilingual-cased-sentence/README.md
model_cards/DeepPavlov/rubert-base-cased-conversational/README.md
model_cards/DeepPavlov/rubert-base-cased-sentence/README.md
model_cards/DeepPavlov/rubert-base-cased/README.md
==================
ed37f9fa4;Sam Shleifer;2020-03-06 16:06:36 -0500;[Bart] _prepare_decoder_inputs should use large negative (#3158)

==

src/transformers/modeling_bart.py
tests/test_modeling_bart.py
==================
0416d437f;Thomas Wolf;2020-03-06 22:01:46 +0100;Merge pull request #3148 from patrickvonplaten/refactoring_beam_search_for_tf_2
refactored beam search according to torch implementation
==
==================
db9279ded;Funtowicz Morgan;2020-03-06 18:04:29 +0000;Fix QA models binding for Flaubert, XLNet and XLM. (#3100)
Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

Format & quality

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

Again.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>
==

src/transformers/modeling_auto.py
==================
e58b3ec5d;Sam Shleifer;2020-03-06 11:15:33 -0500;add imports to examples (#3160)

==

src/transformers/modeling_bart.py
==================
6ffe03a0a;Thomas Wolf;2020-03-06 13:06:34 +0100;Merge pull request #3137 from tomhosking/bart-refactor
Refactor BartModel so that input checks are handled within enc/dec
==
==================
3e5da38da;Thomas Wolf;2020-03-06 13:05:52 +0100;Merge pull request #3132 from huggingface/hf_api_model_list
[hf_api] Get the public list of all the models on huggingface
==
==================
9499a3778;Thomas Wolf;2020-03-06 12:59:13 +0100;Merge pull request #3103 from gthb/keras-serialization
Support keras JSON/HDF5 serialization of main layers
==
==================
9362eb4a0;patrickvonplaten;2020-03-05 23:58:43 +0100;refactored beam search according to torch implementation

==

src/transformers/modeling_tf_utils.py
==================
c8035e11e;Patrick von Platen;2020-03-06 00:45:08 +0100;Merge pull request #3149 from patrickvonplaten/fix_renaming_error
fix missed BartForMaskedLM renaming
==
==================
58fc8f97a;patrickvonplaten;2020-03-06 00:35:47 +0100;fix renaming problem

==

tests/test_modeling_bart.py
==================
857e0a0d3;Sam Shleifer;2020-03-05 17:41:18 -0500;Rename BartForMaskedLM -> BartForConditionalGeneration (#3114)
* improved documentation
==

docs/source/model_doc/bart.rst
examples/summarization/bart/evaluate_cnn.py
src/transformers/__init__.py
src/transformers/convert_bart_original_pytorch_checkpoint_to_pytorch.py
src/transformers/modeling_auto.py
src/transformers/modeling_bart.py
tests/test_modeling_bart.py
==================
fa2aa699d;Lysandre Debut;2020-03-05 17:26:48 -0500;Merge pull request #3011 from patrickvonplaten/add_models_special_tokens_to_specific_configs
Add models special tokens to its pretrained configs
==
==================
146c52123;Lysandre Debut;2020-03-05 17:24:42 -0500;Merge branch 'master' into add_models_special_tokens_to_specific_configs

==
==================
b623ddc00;Lysandre Debut;2020-03-05 17:16:57 -0500;Pass kwargs to configuration (#3147)
* Pass kwargs to configuration

* Setter

* test

==

src/transformers/configuration_utils.py
tests/test_configuration_common.py
==================
0001d0568;Lysandre Debut;2020-03-05 17:01:54 -0500;Correct missing keys + test (#3143)

==

src/transformers/modeling_utils.py
tests/test_modeling_common.py
==================
1741d740f;Thomas Wolf;2020-03-05 22:14:35 +0100;Merge pull request #3145 from sshleifer/bartfp16
[Bart] FP16 Support
==
==================
bbabbc161;Thomas Wolf;2020-03-05 22:12:56 +0100;Merge pull request #3135 from patrickvonplaten/refactor_beam_search_generate
Refactoring and bug fixing beam search generate
==
==================
14d40584b;sshleifer;2020-03-05 13:06:35 -0500;remove newline

==

src/transformers/modeling_bart.py
==================
1360dacaa;sshleifer;2020-03-05 12:57:42 -0500;cleanup deltas

==

src/transformers/modeling_bart.py
tests/test_modeling_bart.py
==================
810079de1;sshleifer;2020-03-05 12:48:14 -0500;no ipdb

==

src/transformers/modeling_bart.py
==================
c203509d5;sshleifer;2020-03-05 12:34:08 -0500;undo chg

==

examples/summarization/bertabs/modeling_bertabs.py
==================
c36fdc88d;sshleifer;2020-03-05 12:33:08 -0500;tests pass

==

examples/summarization/bertabs/modeling_bertabs.py
src/transformers/modeling_bart.py
tests/test_modeling_bart.py
==================
7ac47bfe6;Morgan Funtowicz;2020-03-05 16:07:43 +0100;Updated notebook dependencies for Colab.
Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

==

notebooks/02-transformers.ipynb
==================
be02176a4;Morgan Funtowicz;2020-03-05 16:00:38 +0100;Fixing sentiment pipeline in 03-pipelines notebook.
Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

==

notebooks/03-pipelines.ipynb
==================
8a2d9bc9e;Julien Chaumond;2020-03-05 09:34:43 -0500;Add model cards for DeepPavlov models (#3138)
* add empty model cards for every current DeepPavlov model

* fix: replace cyrillic `—Å` with `c`

* docs: add model cards for current DeepPavlov BERT models

* docs: add links for arXiv preprints

==

model_cards/DeepPavlov/bert-base-bg-cs-pl-ru-cased/README.md
model_cards/DeepPavlov/bert-base-cased-conversational/README.md
model_cards/DeepPavlov/bert-base-multilingual-cased-sentence/README.md
model_cards/DeepPavlov/rubert-base-cased-conversational/README.md
model_cards/DeepPavlov/rubert-base-cased-sentence/README.md
model_cards/DeepPavlov/rubert-base-cased/README.md
==================
012cbdb0f;Morgan Funtowicz;2020-03-05 15:34:15 +0100;Updating colab links in notebooks README.
Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

==

notebooks/README.md
==================
31acb8dc5;Tom Hosking;2020-03-05 13:51:30 +0000;Remove rogue .DS_Store

==

src/.DS_Store
==================
06a6cb6f3;Tom Hosking;2020-03-05 13:45:41 +0000;Refactor BartModel so that input checks are handled within BartEncoder and BartDecoder

==

src/.DS_Store
src/transformers/modeling_bart.py
==================
e33ed12c3;Patrick von Platen;2020-03-05 13:41:04 +0100;uncomment expression

==

src/transformers/modeling_utils.py
==================
4220fd52b;Patrick von Platen;2020-03-05 13:36:21 +0100;remove ipdb

==

src/transformers/modeling_utils.py
==================
c47394b0c;Patrick von Platen;2020-03-05 13:12:50 +0100;refactoring and bug fixing beam search generate

==

src/transformers/modeling_utils.py
==================
4c91a3af9;Gunnlaugur Thor Briem;2020-03-05 11:48:10 +0000;Document keras_serializable decorator

==

src/transformers/modeling_tf_utils.py
==================
4be01e5cb;Gunnlaugur Thor Briem;2020-03-05 11:08:45 +0000;Use name transformers_config in Keras serialization
Be explicit that this is config for the transformers package (as these
layers may coexist with other custom stuff in a Keras model, plus the
Keras container itself is called config, and config["config"] is not
great)

Add explicit error handling for initializer calls that have neither
the `config` nor the `transformers_config` argument, or have both.

==

src/transformers/modeling_tf_utils.py
==================
a355f4f0f;Gunnlaugur Thor Briem;2020-03-05 11:11:42 +0000;Add functools.wraps for wrapper initializer
Preserve the original initializer function's metadata. See
https://docs.python.org/3/library/functools.html#functools.update_wrapper

==

src/transformers/modeling_tf_utils.py
==================
d262a5d48;Gunnlaugur Thor Briem;2020-03-05 11:05:29 +0000;fix: remove unused import

==

src/transformers/configuration_auto.py
==================
30624f705;Morgan Funtowicz;2020-03-05 11:40:15 +0100;Fix Colab links + install dependencies first.
Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

==

notebooks/01-training-tokenizers.ipynb
notebooks/02-transformers.ipynb
notebooks/03-pipelines.ipynb
notebooks/README.md
==================
3f067f440;Julien Chaumond;2020-03-04 23:55:46 -0500;[hf_api] slightly more doc

==

src/transformers/hf_api.py
==================
f564f93c8;Julien Chaumond;2020-03-04 23:33:09 -0500;[hf_api] Get the public list of all the models on huggingface

==

src/transformers/hf_api.py
tests/test_hf_api.py
==================
ff9e79ba3;Julien Chaumond;2020-03-04 20:18:07 -0500;make style

==

tests/test_doc_samples.py
==================
07a79db50;Lysandre;2020-03-04 19:11:31 -0500;Fix failing doc samples

==

docs/source/multilingual.rst
src/transformers/modeling_flaubert.py
tests/test_doc_samples.py
==================
4f338ed40;Gunnlaugur Thor Briem;2020-03-04 23:45:29 +0000;Explicit config_class instead of module inspection

==

src/transformers/configuration_auto.py
src/transformers/modeling_tf_albert.py
src/transformers/modeling_tf_bert.py
src/transformers/modeling_tf_ctrl.py
src/transformers/modeling_tf_gpt2.py
src/transformers/modeling_tf_transfo_xl.py
src/transformers/modeling_tf_utils.py
src/transformers/modeling_tf_xlnet.py
==================
6fe1cc087;Gunnlaugur Thor Briem;2020-03-04 23:24:13 +0000;fix: clean up inadvertent change in tf_t5
This was the beginnings of an attempt to address the test failure on
this layer, and instead I backed out of making this layer
keras-serializable at all ... so it was a mistake to commit this.

==

src/transformers/modeling_tf_t5.py
==================
bdd3d0c76;Thomas Wolf;2020-03-04 23:28:00 +0100;Merge pull request #3118 from patrickvonplaten/add_beam_search_to_generation_tf_2_0
Add beam search to generation tf 2 0
==
==================
c440030e9;Julien Chaumond;2020-03-04 16:33:10 -0500;[model_cards] Tag AR model languages

==

model_cards/asafaya/bert-base-arabic/README.md
model_cards/aubmindlab/bert-base-arabert/README.md
model_cards/aubmindlab/bert-base-arabertv01/README.md
==================
3b7f95a50;Thomas Wolf;2020-03-04 21:59:09 +0100;Merge pull request #3115 from gthb/fix-bogus-param-to-layer-init
fix: passing config as Layer trainable param
==
==================
1bca97ec7;Morgan Funtowicz;2020-03-04 21:19:33 +0100;Update notebook link and fix few working issues.
Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

==

notebooks/02-transformers.ipynb
notebooks/README.md
==================
189113d89;Manuel Romero;2020-03-04 18:12:50 +0100;Create README.md

==

model_cards/mrm8488/bert-multi-cased-finetuned-xquadv1/README.md
==================
76111a3d3;Julien Chaumond;2020-03-04 12:55:20 -0500;[model_cards] Add card by @lvwerra
(the current way to submit a model card to have it displayed on the website is to open a PR on the `transformers` repo itself)

Thanks for sharing!

==

model_cards/lvwerra/gpt2-medium-taboo/README.md
==================
a43c388ab;Julien Chaumond;2020-03-04 12:53:02 -0500;[model_cards] Add card by @djstrong
(the current way to submit a model card to have it displayed on the website is to open a PR on the `transformers` repo itself)

Thanks for sharing!

==

model_cards/djstrong/bg_cs_pl_ru_cased_L-12_H-768_A-12/README.md
==================
ec60e0ae7;Manuel Romero;2020-03-03 22:38:01 +0100;Create README.md

==

model_cards/mrm8488/bert-uncased-finetuned-qnli/README.md
==================
6a143bf28;Wissam Antoun;2020-03-04 20:04:39 +0300;model cards for both aubmindlab/bert-base-arabert models (#3113)
* Added readme for AraBERTv0.1

* Added readme to AraBERT

==

model_cards/aubmindlab/bert-base-arabert/README.md
model_cards/aubmindlab/bert-base-arabertv01/README.md
==================
932eab943;Patrick von Platen;2020-03-04 18:03:46 +0100;include tf gpt2 tests for attn mask and past variable (#3122)

==

tests/test_modeling_tf_gpt2.py
==================
256cbbc4a;Julien Chaumond;2020-03-04 12:01:45 -0500;[doc] Fix link to how-to-train Colab

==

notebooks/README.md
==================
006097f8a;Patrick von Platen;2020-03-04 18:01:17 +0100;rename variables named 'word' to 'token' in generate fn (#3119)
* fix conflits

* fixed naming bug

* make style

==

src/transformers/modeling_utils.py
==================
18f4b9274;Gunnlaugur Thor Briem;2020-03-04 16:57:28 +0000;fix: work with Tensorflow < 2.1.0
tf.keras.utils.register_keras_serializable was added in TF 2.1.0, so
don't rely on it being there; just decorate the class with it if it
exists.

==

src/transformers/modeling_tf_utils.py
==================
71c871197;Funtowicz Morgan;2020-03-04 16:45:57 +0000;Adding Docker images for transformers + notebooks (#3051)
* Added transformers-pytorch-cpu and gpu Docker images

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Added automatic jupyter launch for Docker image.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Move image from alpine to Ubuntu to align with NVidia container images.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Added TRANSFORMERS_VERSION argument to Dockerfile.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Added Pytorch-GPU based Docker image

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Added Tensorflow images.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Use python 3.7 as Tensorflow doesnt provide 3.8 compatible wheel.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Remove double FROM instructions on transformers-pytorch-cpu image.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Added transformers-tensorflow-gpu Docker image.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* use the correct ubuntu version for tensorflow-gpu

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Added pipelines example notebook

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Added transformers-cpu and transformers-gpu (including both PyTorch and TensorFlow) images.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Docker images doesnt start jupyter notebook by default.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Tokenizers notebook

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Update images links

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Update Docker images to python 3.7.6 and transformers 2.5.1

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Added 02-transformers notebook.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Trying to realign 02-transformers notebook ?

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Added Transformer image schema

* Some tweaks on tokenizers notebook

* Removed old notebooks.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Attempt to provide table of content for each notebooks

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Second attempt.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Reintroduce transformer image.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Keep trying

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* It's going to fly !

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Remaining of the Table of Content

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Fix inlined elements for the table of content

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Removed anaconda dependencies for Docker images.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Removing notebooks ToC

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Added LABEL to each docker image.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Removed old Dockerfile

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Directly use the context and include transformers from here.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Reduce overall size of compiled Docker images.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Install jupyter by default and use CMD for easier launching of the images.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Reduce number of layers in the images.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Added README.md for notebooks.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Fix notebooks link in README

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Fix some wording issues.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Added blog notebooks too.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Addressing spelling errors in review comments.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

Co-authored-by: MOI Anthony <xn1t0x@gmail.com>

==

docker/Dockerfile
docker/transformers-cpu/Dockerfile
docker/transformers-gpu/Dockerfile
docker/transformers-pytorch-cpu/Dockerfile
docker/transformers-pytorch-gpu/Dockerfile
docker/transformers-tensorflow-cpu/Dockerfile
docker/transformers-tensorflow-gpu/Dockerfile
notebooks/01-training-tokenizers.ipynb
notebooks/02-transformers.ipynb
notebooks/03-pipelines.ipynb
notebooks/Comparing-PT-and-TF-models.ipynb
notebooks/Comparing-TF-and-PT-models-MLM-NSP.ipynb
notebooks/Comparing-TF-and-PT-models-SQuAD.ipynb
notebooks/Comparing-TF-and-PT-models.ipynb
notebooks/README.md
==================
7a89a3e49;Patrick von Platen;2020-03-04 12:02:57 +0100;correct beam search sampling

==

src/transformers/modeling_tf_utils.py
==================
c4c4c9998;Patrick von Platen;2020-03-04 11:09:45 +0100;make GPT2 and CTRL shape consistent between torch and TF

==

src/transformers/modeling_tf_ctrl.py
src/transformers/modeling_tf_gpt2.py
src/transformers/modeling_tf_utils.py
==================
2529b2d37;patrickvonplaten;2020-03-04 00:41:05 +0100;set redorder past sort dimension to its default

==

src/transformers/modeling_tf_utils.py
==================
61fef6e95;patrickvonplaten;2020-03-04 00:32:07 +0100;added beam_search generation for tf 2.0

==

src/transformers/modeling_tf_utils.py
tests/test_modeling_tf_common.py
==================
34de670db;Patrick von Platen;2020-03-04 17:25:23 +0100;fix sklearn release circle ci [temporary] (#3123)

==

setup.py
==================
6701fb785;Patrick von Platen;2020-03-04 15:30:51 +0100;fix beam_search behavior when sampling (#3106)
* fix beam_search behavior when sampling

* delete print

* make correct style

==

src/transformers/modeling_utils.py
==================
b1116fd67;Gunnlaugur Thor Briem;2020-03-03 23:05:40 +0000;fix: passing config as Layer trainable param
Lurking bugs discovered while working on other stuff.

==

src/transformers/modeling_tf_albert.py
src/transformers/modeling_tf_openai.py
==================
96c499016;Gunnlaugur Thor Briem;2020-03-03 22:57:05 +0000;fix unused imports and style

==

src/transformers/modeling_tf_distilbert.py
src/transformers/modeling_tf_openai.py
src/transformers/modeling_tf_roberta.py
src/transformers/modeling_tf_t5.py
src/transformers/modeling_tf_xlm.py
tests/test_modeling_tf_common.py
==================
470753bcf;Gunnlaugur Thor Briem;2020-03-03 22:44:38 +0000;Put @keras_serializable only on layers it works on
And only run the test on TF*MainLayer classes so marked.

==

src/transformers/modeling_tf_distilbert.py
src/transformers/modeling_tf_openai.py
src/transformers/modeling_tf_t5.py
src/transformers/modeling_tf_utils.py
src/transformers/modeling_tf_xlm.py
tests/test_modeling_tf_common.py
==================
0c716ede8;Gunnlaugur Thor Briem;2020-03-03 22:31:38 +0000;Use class decorator instead of superclass
When supplied by Keras deserialization, the config parameter to initializers
will be a dict. So intercept it and convert to PretrainedConfig object (and
store in instance attribute for get_config to get at it) before passing to the
actual initializer. To accomplish this, and repeat as little code as possible,
use a class decorator on TF*MainLayer classes.

==

src/transformers/configuration_auto.py
src/transformers/modeling_tf_albert.py
src/transformers/modeling_tf_bert.py
src/transformers/modeling_tf_ctrl.py
src/transformers/modeling_tf_distilbert.py
src/transformers/modeling_tf_gpt2.py
src/transformers/modeling_tf_openai.py
src/transformers/modeling_tf_roberta.py
src/transformers/modeling_tf_t5.py
src/transformers/modeling_tf_transfo_xl.py
src/transformers/modeling_tf_utils.py
src/transformers/modeling_tf_xlm.py
src/transformers/modeling_tf_xlnet.py
tests/test_modeling_tf_common.py
==================
e9e6efdc4;Sam Shleifer;2020-03-03 15:54:29 -0500;BartForSequenceClassification: fix num_labels, add test (#3110)

==

src/transformers/modeling_bart.py
tests/test_modeling_bart.py
==================
f631e01d2;Julien Chaumond;2020-03-03 15:31:31 -0500;[ci] Re-run integration ground truth from fairseq
Adopted best practice set by @patrickvonplaten of commenting lines run on fairseq, for easy comparison

also see #3020

==

src/transformers/convert_roberta_original_pytorch_checkpoint_to_pytorch.py
tests/test_modeling_roberta.py
tests/test_modeling_tf_roberta.py
tests/test_modeling_xlm_roberta.py
==================
5b396457e;Sam Shleifer;2020-03-03 15:29:59 -0500;Summarization Examples: add Bart CNN Evaluation (#3082)
* Rename and improve example

* Add test

* slightly faster test

* style

* This breaks remy prolly

* shorter test string

* no slow

* newdir structure

* New tree

* Style

* shorter

* docs

* clean

* Attempt future import

* more import hax

==

examples/summarization/__init__.py
examples/summarization/bart/README.md
examples/summarization/bart/__init__.py
examples/summarization/bart/evaluate_cnn.py
examples/summarization/bart/test_bart_examples.py
examples/summarization/bertabs/README.md
examples/summarization/bertabs/__init__.py
examples/summarization/bertabs/configuration_bertabs.py
examples/summarization/bertabs/convert_bertabs_original_pytorch_checkpoint.py
examples/summarization/bertabs/modeling_bertabs.py
examples/summarization/bertabs/requirements.txt
examples/summarization/bertabs/run_summarization.py
examples/summarization/bertabs/test_utils_summarization.py
examples/summarization/bertabs/utils_summarization.py
==================
5c5af879b;Sam Shleifer;2020-03-03 15:14:12 -0500;[Bart] dont call .forward (#3094)

==

src/transformers/modeling_bart.py
==================
b8da16f39;Gunnlaugur Thor Briem;2020-03-03 15:15:30 +0000;Add (failing) tests for Keras save/load

==

tests/test_modeling_tf_common.py
==================
ba2817071;Gunnlaugur Thor Briem;2020-03-03 14:00:30 +0000;Support keras JSON/HDF5 serialization of main layers
Fixes #3101

==

src/transformers/modeling_tf_albert.py
src/transformers/modeling_tf_bert.py
src/transformers/modeling_tf_ctrl.py
src/transformers/modeling_tf_distilbert.py
src/transformers/modeling_tf_gpt2.py
src/transformers/modeling_tf_openai.py
src/transformers/modeling_tf_t5.py
src/transformers/modeling_tf_transfo_xl.py
src/transformers/modeling_tf_utils.py
src/transformers/modeling_tf_xlm.py
src/transformers/modeling_tf_xlnet.py
==================
a088d75e5;Julien Chaumond;2020-03-03 09:52:32 -0500;[model_cards] Fix incorrect path

==

model_cards/asafaya/bert-base-arabic/README.md
==================
413410036;Patrick von Platen;2020-03-03 15:42:15 +0100;Add generate() functionality to TF 2.0 (#3063)
* add first copy past test to tf 2 generate

* add tf top_k_top_p_filter fn

* add generate function for TF

* add generate function for TF

* implemented generate for all models expect transfoXL

* implemented generate for all models expect transfoXL

* implemented generate for all models expect transfoXL

* make style

* change permission of test file to correct ones

* delete ipdb

* delete ipdb

* fix bug and finish simple gpt2 integration test

* clean test file

* clean test file

* make style

* make style

* make style

* make style

* change import style

* change import style

* make style

* make style

* add decorators

* add decorators

* fix tf ctrl bug dim => axis in TF

* make style

* make style

* refactored test file

* refactored test file

* take out test_torch_tf_conversion if nothing is defined

* take out test_torch_tf_conversion if nothing is defined

* remove useless files

* remove useless files

* fix conflicts

* fix conflicts

* fix conflicts

* fix conflicts

* fix conflicts

* solve conflicts

* solve conflicts

* fix conflicts

* fix conflicts

* merge conflicts

* delete ipdb

* exposed top_k_top_p_filtering fns

* delete weirdly created w! file

* add comment to test tf common modeling

* fix conflicts

* fix conflicts

* make style

* merge conflicts

* make style

* change tf.tensor.shape to shape_list(tensor)

==

src/transformers/__init__.py
src/transformers/modeling_ctrl.py
src/transformers/modeling_gpt2.py
src/transformers/modeling_tf_ctrl.py
src/transformers/modeling_tf_gpt2.py
src/transformers/modeling_tf_transfo_xl.py
src/transformers/modeling_tf_utils.py
src/transformers/modeling_tf_xlm.py
src/transformers/modeling_tf_xlnet.py
src/transformers/modeling_transfo_xl.py
src/transformers/modeling_xlnet.py
tests/test_modeling_common.py
tests/test_modeling_gpt2.py
tests/test_modeling_tf_common.py
tests/test_modeling_tf_ctrl.py
tests/test_modeling_tf_gpt2.py
tests/test_modeling_tf_openai_gpt.py
tests/test_modeling_tf_transfo_xl.py
tests/test_modeling_tf_xlm.py
tests/test_modeling_tf_xlnet.py
==================
b31f71501;ali safaya;2020-03-03 16:30:10 +0300;bert-base-arabic model card

==

model_cards/asafaya/base-bert-arabic/README.md
==================
c0c7ec345;Davide Fiocco;2020-03-03 14:59:47 +0100;Don't crash if fine-tuned model doesn't end with a number (#3099)
That's the same fix applied in https://github.com/huggingface/transformers/issues/2258 , but for the GLUE example
==

examples/run_glue.py
==================
eec5ec807;Julien Chaumond;2020-03-02 18:56:17 -0500;[BART] to each its own config + make BART compatible w/ Pipelines
cc @sshleifer

==

src/transformers/configuration_bart.py
src/transformers/pipelines.py
==================
6b1558bad;Felix MIKAELIAN;2020-03-02 23:07:13 +0100;add models cards for camembert-base-fquad camembert-base-squad (#3089)
* add models cards for camembert-base-fquad camembert-base-squad

* typo fix

==

model_cards/fmikaelian/camembert-base-fquad/README.md
model_cards/fmikaelian/camembert-base-squad/README.md
==================
f169957d0;Julien Chaumond;2020-03-02 15:45:25 -0500;TF GPU CI (#3085)
* debug env

* Restrict TF GPU memory

* Fixup

* One more test

* rm debug logs

* Fixup

==

.github/workflows/self-push.yml
.github/workflows/self-scheduled.yml
tests/test_modeling_tf_common.py
tests/utils.py
utils/link_tester.py
==================
d3eb7d23a;Lysandre Debut;2020-03-02 14:07:10 -0500;Pipeline doc (#3055)
* Pipeline doc initial commit

* pipeline abstraction

* Remove modelcard argument from pipeline

* Task-specific pipelines can be instantiated with no model or tokenizer

* All pipelines doc

==

docs/source/index.rst
docs/source/main_classes/pipelines.rst
src/transformers/pipelines.py
tests/test_pipelines.py
==================
2c7749784;Manuel Romero;2020-03-02 19:31:58 +0100;Update README.md
- Add example of usage
- Update metrics
==

model_cards/mrm8488/bert-spanish-cased-finetuned-ner/README.md
==================
0e56b37e8;Julien Chaumond;2020-03-02 12:23:58 -0500;rm bogus file
cc @patrickvonplaten

==

tests/test_modeling_common.py!
==================
2fdc7f6ce;Patrick von Platen;2020-03-02 18:00:09 +0100;correct greedy generation when doing beam search (#3078)
* correct greedy generation when doing beam search

* improve comment

==

src/transformers/modeling_utils.py
tests/test_modeling_common.py
==================
13afb7120;Julien Chaumond;2020-03-02 11:56:45 -0500;[ci] Ensure that TF does not preempt all GPU memory for itself
see https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth

Co-Authored-By: Funtowicz Morgan <mfuntowicz@users.noreply.github.com>
Co-Authored-By: Lysandre Debut <lysandre.debut@reseau.eseo.fr>

==

.github/workflows/self-push.yml
.github/workflows/self-scheduled.yml
==================
c0135194e;Patrick von Platen;2020-03-02 16:53:55 +0100;Force pad_token_id to be set before padding for standard tokenizer (#3035)
* force pad_token_id to be set before padding

* fix tests and forbid padding without having a padding_token_id set

==

src/transformers/tokenization_utils.py
tests/test_tokenization_common.py
==================
b54ef78d0;Sam Shleifer;2020-03-02 10:35:53 -0500;Bart-CNN (#3059)
`generate` code that produces 99% identical summarizations to fairseq on CNN test data, with caching.
==

docs/source/model_doc/bart.rst
docs/source/pretrained_models.rst
src/transformers/configuration_bart.py
src/transformers/convert_bart_original_pytorch_checkpoint_to_pytorch.py
src/transformers/modeling_bart.py
src/transformers/modeling_utils.py
src/transformers/tokenization_bart.py
tests/test_modeling_bart.py
==================
6b1ff2508;Victor SANH;2020-03-02 10:20:21 -0500;fix n_gpu count when no_cuda flag is activated (#3077)
* fix n_gpu count when no_cuda flag is activated

* someone was left behind

==

examples/contrib/run_swag.py
examples/distillation/run_squad_w_distillation.py
examples/hans/test_hans.py
examples/mm-imdb/run_mmimdb.py
examples/ner/run_ner.py
examples/run_bertology.py
examples/run_generation.py
examples/run_glue.py
examples/run_language_modeling.py
examples/run_multiple_choice.py
examples/run_squad.py
examples/run_xnli.py
templates/adding_a_new_example_script/run_xxx.py
==================
298bed16a;Julien Chaumond;2020-03-01 14:08:01 -0500;make style

==

examples/distillation/run_squad_w_distillation.py
==================
852e032ca;VictorSanh;2020-03-01 01:56:50 +0000;include roberta in run_squad_w_distillation - cc @graviraja

==

examples/distillation/run_squad_w_distillation.py
==================
b5509abb3;VictorSanh;2020-03-01 01:39:24 +0000;--do_lower_case will always trick me...

==

examples/distillation/README.md
==================
d6ef587a1;Julien Chaumond;2020-02-28 23:19:17 -0500;[ci] Fixup e36bd94345af6045108a391f9ac7f4dc557548de

==

.circleci/config.yml
==================
e36bd9434;Julien Chaumond;2020-02-28 21:11:08 -0500;[ci] Run all tests on (self-hosted) GPU (#3020)
* Create self-hosted.yml

* Update self-hosted.yml

* Update self-hosted.yml

* Update self-hosted.yml

* Update self-hosted.yml

* Update self-hosted.yml

* do not run slow tests, for now

* [ci] For comparison with circleci, let's also run CPU-tests

* [ci] reorganize

* clearer filenames

* [ci] Final tweaks before merging

* rm slow tests on circle ci

* Trigger CI

* On GPU this concurrency was way too high

==

.circleci/config.yml
.github/workflows/github-push.yml
.github/workflows/self-push.yml
.github/workflows/self-scheduled.yml
==================
908fa43b5;srush;2020-02-27 16:45:33 -0500;Changes to NER examples for PLT and TPU (#3053)
* changes to allow for tpu training

* black

* tpu

* tpu

==

examples/ner/run_pl.sh
examples/ner/run_pl_ner.py
examples/ner/transformer_base.py
==================
8bcb37bfb;Lysandre Debut;2020-02-27 10:22:55 -0500;NER support for Albert in run_ner.py and NerPipeline (#2983)
* * Added support for Albert when fine-tuning for NER

* Added support for Albert in NER pipeline

* Added command-line options to examples/ner/run_ner.py to better control tokenization

* Added class AlbertForTokenClassification

* Changed output for NerPipeline to use .convert_ids_to_tokens(...) instead of .decode(...) to better reflect tokens

* Added ,

* Now passes style guide enforcement

* Changes from reviews.

* Code now passes style enforcement

* Added test for AlbertForTokenClassification

* Added test for AlbertForTokenClassification

==
==================
6a3758804;Sam Shleifer;2020-02-27 10:22:35 -0500;spelling: strictly (#3042)

==

src/transformers/modeling_utils.py
==================
f4ff44a6d;Cola;2020-02-27 23:56:47 +0900;Fix batch_encode_plus (#3041)

==

src/transformers/tokenization_utils.py
==================
f71157529;Martin Malmsten;2020-02-27 12:24:20 +0100;Added test for AlbertForTokenClassification

==

tests/test_modeling_albert.py
==================
aceb6a090;Martin Malmsten;2020-02-27 11:52:46 +0100;Added test for AlbertForTokenClassification

==

tests/test_modeling_albert.py
==================
d762d4289;Martin Malmsten;2020-02-26 23:50:40 +0100;Code now passes style enforcement

==

examples/ner/run_ner.py
==================
9495d38b0;Martin Malmsten;2020-02-26 23:36:39 +0100;Changes from reviews.

==

examples/ner/run_ner.py
src/transformers/modeling_albert.py
==================
b370cc7e9;Julien Chaumond;2020-02-26 21:48:49 +0000;[gpu] Fixup fdd61b19928e87a5354c36923182e801bfedb31b

==

tests/test_modeling_gpt2.py
==================
f5516805c;Julien Chaumond;2020-02-26 20:47:49 +0000;Fix bart slow test

==

tests/test_modeling_bart.py
==================
5bc99e7f3;Andrew Walker;2020-02-26 11:39:54 -0600;fix several typos in Distil* readme (#3034)

==

examples/distillation/README.md
==================
fdd61b199;Patrick von Platen;2020-02-26 18:04:37 +0100;Fix attn mask gpt2 when using past (#3033)
* fix issue and add some tests

* fix issue and add some tests

* updated doc string gpt2

==

src/transformers/modeling_gpt2.py
tests/test_modeling_common.py!
tests/test_modeling_gpt2.py
==================
9cda3620b;Julien Chaumond;2020-02-26 11:59:25 -0500;Fix (non-slow) tests on GPU (torch) (#3024)
* Fix tests on GPU (torch)

* Fix bart slow tests

Co-authored-by: Sam Shleifer <sshleifer@gmail.com>

==

src/transformers/modeling_bart.py
tests/test_modeling_bart.py
tests/test_modeling_common.py
tests/test_modeling_t5.py
==================
9df74b8bc;Sam Shleifer;2020-02-26 11:36:27 -0500;Delete all mentions of Model2Model (#3019)

==

docs/source/quickstart.md
src/transformers/__init__.py
src/transformers/modeling_encoder_decoder.py
tests/test_modeling_encoder_decoder.py
==================
bb7c46852;Lysandre Debut;2020-02-25 18:43:36 -0500;Documentation (#2989)
* All Tokenizers

BertTokenizer + few fixes
RobertaTokenizer
OpenAIGPTTokenizer + Fixes
GPT2Tokenizer + fixes
TransfoXLTokenizer
Correct rst for TransformerXL
XLMTokenizer + fixes
XLNet Tokenizer + Style
DistilBERT + Fix XLNet RST
CTRLTokenizer
CamemBERT Tokenizer
FlaubertTokenizer
XLMRobertaTokenizer
cleanup

* cleanup

==

docs/source/model_doc/albert.rst
docs/source/model_doc/bert.rst
docs/source/model_doc/camembert.rst
docs/source/model_doc/ctrl.rst
docs/source/model_doc/gpt.rst
docs/source/model_doc/gpt2.rst
docs/source/model_doc/roberta.rst
docs/source/model_doc/transformerxl.rst
docs/source/model_doc/xlm.rst
docs/source/model_doc/xlmroberta.rst
docs/source/model_doc/xlnet.rst
src/transformers/configuration_flaubert.py
src/transformers/configuration_gpt2.py
src/transformers/configuration_openai.py
src/transformers/configuration_xlm.py
src/transformers/modeling_bert.py
src/transformers/modeling_tf_bert.py
src/transformers/tokenization_albert.py
src/transformers/tokenization_bert.py
src/transformers/tokenization_camembert.py
src/transformers/tokenization_ctrl.py
src/transformers/tokenization_distilbert.py
src/transformers/tokenization_flaubert.py
src/transformers/tokenization_gpt2.py
src/transformers/tokenization_openai.py
src/transformers/tokenization_roberta.py
src/transformers/tokenization_transfo_xl.py
src/transformers/tokenization_xlm.py
src/transformers/tokenization_xlm_roberta.py
src/transformers/tokenization_xlnet.py
==================
c913eb9c3;Patrick von Platen;2020-02-25 22:51:25 +0100;Add integration tests for xlm roberta modelling and xlm roberta tokenzier (#3014)
* add first files

* add xlm roberta integration tests

* make style

* flake 8 issues solved

==

tests/test_modeling_xlm_roberta.py
tests/test_tokenization_xlm_roberta.py
==================
e8ce63ff2;srush;2020-02-25 14:47:43 -0500;Change masking to direct labeling for TPU support. (#2982)
* change masking to direct labelings

* fix black

* switch to ignore index

* .

* fix black

==

src/transformers/modeling_bert.py
src/transformers/modeling_distilbert.py
src/transformers/modeling_roberta.py
src/transformers/modeling_xlnet.py
==================
7a7ee28cb;Jhuo IH;2020-02-25 11:06:57 -0800;missing ner link (#2967)

==

examples/README.md
==================
65e7c90a7;Lysandre Debut;2020-02-25 13:48:24 -0500;Adding usage examples for common tasks (#2850)
* Usage: Sequence Classification & Question Answering

* Pipeline example

* Language modeling

* TensorFlow code for Sequence classification

* Custom TF/PT toggler in docs

* QA + LM for TensorFlow

* Finish Usage for both PyTorch and TensorFlow

* Addressing Julien's comments

* More assertive

* cleanup

* Favicon
- added favicon option in conf.py along with the favicon image
- udpated ü§ó logo. slightly smaller and should appear more consistent across editing programs (no more tongue on the outside of the mouth)

Co-authored-by: joshchagani <joshua@joshuachagani.com>

==

docs/source/_static/css/huggingface.css
docs/source/_static/js/custom.js
docs/source/_static/js/huggingface_logo.svg
docs/source/conf.py
docs/source/favicon.ico
docs/source/index.rst
docs/source/usage.rst
==================
f5b50c6b8;Patrick von Platen;2020-02-25 16:41:54 +0100;make style

==

tests/test_modeling_gpt2.py
tests/test_modeling_transfo_xl.py
tests/test_modeling_xlm.py
tests/test_modeling_xlnet.py
==================
ec16142ee;Patrick von Platen;2020-02-25 16:37:59 +0100;add special tokens to pretrain configs of respective lm head models

==

src/transformers/configuration_gpt2.py
src/transformers/configuration_transfo_xl.py
src/transformers/configuration_xlm.py
src/transformers/configuration_xlnet.py
src/transformers/modeling_utils.py
==================
e645dcbb7;Patrick von Platen;2020-02-25 16:37:56 +0100;add special tokens to pretrain configs of respective lm head models

==

tests/test_modeling_gpt2.py
tests/test_modeling_transfo_xl.py
tests/test_modeling_xlm.py
tests/test_modeling_xlnet.py
==================
e693cd1e8;Julien Chaumond;2020-02-24 19:54:47 -0500;[ci] Run slow tests every day

==

.circleci/config.yml
==================
4fc63151a;Julien Chaumond;2020-02-24 19:51:34 -0500;[ci] Attempt to fix #2844

==

.circleci/config.yml
==================
b90745c59;Lysandre Debut;2020-02-24 18:45:53 -0500;Test correct tokenizers after default switch (#3003)

==

tests/test_tokenization_auto.py
==================
3716c3d8a;Lysandre Debut;2020-02-24 18:30:57 -0500;False by default (#3002)

==

src/transformers/tokenization_auto.py
==================
f9ec5ca90;Lysandre;2020-02-24 18:22:54 -0500;Release: v2.5.1

==

docs/source/conf.py
setup.py
src/transformers/__init__.py
==================
4cd9c0971;Funtowicz Morgan;2020-02-25 00:20:42 +0100;Fix for fast tokenizers save_pretrained compatibility with Python. (#2933)
* Renamed file generate by tokenizers when calling save_pretrained to match python.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Added save_vocabulary tests.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Remove python quick and dirty fix for clean Rust impl.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Bump tokenizers dependency to 0.5.1

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* TransfoXLTokenizerFast uses a json vocabulary file + warning about incompatibility between Python and Rust

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Added some save_pretrained / from_pretrained unittests.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Update tokenizers to 0.5.2

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Quality and format.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* flake8

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Making sure there is really a bug in unittest

* Fix TransfoXL constructor vocab_file / pretrained_vocab_file mixin.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

==

setup.py
src/transformers/tokenization_transfo_xl.py
src/transformers/tokenization_utils.py
tests/test_tokenization_fast.py
==================
ee60840ee;Sandro Cavallari;2020-02-25 06:50:24 +0800;fix _update_memory fn call in transformer-xl (#2971)

==

src/transformers/modeling_transfo_xl.py
==================
6a50d501e;Patrick von Platen;2020-02-24 21:42:38 +0100;add explaining example to XLNet LM modeling (#2997)
* add explaining example to XLNet LM modeling

* improve docstring for xlnet

==

src/transformers/modeling_xlnet.py
==================
65d74c496;Patrick von Platen;2020-02-24 21:11:10 +0100;Add preprocessing step for transfo-xl tokenization to avoid tokenizing words followed by punction to <unk> (#2987)
* add preprocessing to add space before punctuation for transfo_xl

* improve warning messages

* make style

* compile regex at instantination of tokenizer object

==

examples/run_generation.py
src/transformers/tokenization_transfo_xl.py
==================
a143d9479;Bram Vanroy;2020-02-24 20:58:15 +0100;Add local_files_only parameter to pretrained items (#2930)
* Add disable_outgoing to pretrained items

Setting disable_outgoing=True disables outgonig traffic:
- etags are not looked up
- models are not downloaded

* parameter name change

* Remove forgotten print

==

src/transformers/configuration_utils.py
src/transformers/file_utils.py
src/transformers/modeling_utils.py
src/transformers/tokenization_utils.py
==================
286d1ec74;Manuel Romero;2020-02-24 17:49:29 +0100;Create README.md

==

model_cards/mrm8488/bert-spanish-cased-finetuned-pos/README.md
==================
7984a70ee;Lysandre Debut;2020-02-24 14:19:39 -0500;kwargs are passed to both model and configuration in AutoModels (#2998)

==

src/transformers/modeling_auto.py
==================
21d8b6a33;Lysandre Debut;2020-02-24 12:09:46 -0500;Testing that batch_encode_plus is the same as encode_plus (#2973)
* Testing that encode_plus and batch_encode_plus behave the same way

Spoiler alert: they don't

* Testing rest of arguments in batch_encode_plus

* Test tensor return in batch_encode_plus

* Addressing Sam's comments

* flake8

* Simplified with `num_added_tokens`

==

src/transformers/tokenization_t5.py
src/transformers/tokenization_utils.py
tests/test_tokenization_common.py
==================
17c45c39e;Patrick von Platen;2020-02-24 17:51:57 +0100;Add slow generate tests for pretrained lm models (#2909)
* add slow generate lm_model tests

* fix conflicts

* merge conflicts

* fix conflicts

* add slow generate lm_model tests

* make style

* delete unused variable

* fix conflicts

* fix conflicts

* fix conflicts

* delete unused variable

* fix conflicts

* finished hard coded tests

==

src/transformers/modeling_xlm.py
tests/test_modeling_common.py
tests/test_modeling_ctrl.py
tests/test_modeling_gpt2.py
tests/test_modeling_openai.py
tests/test_modeling_transfo_xl.py
tests/test_modeling_xlm.py
tests/test_modeling_xlnet.py
==================
8194df8e0;Lysandre Debut;2020-02-24 08:42:54 -0500;Warning on `add_special_tokens` (#2966)
Warning on `add_special_tokens` when passed to `encode`, `encode_plus` and `batch_encode_plus`
==

src/transformers/tokenization_utils.py
==================
38f5fe9e0;Patrick von Platen;2020-02-23 22:55:32 +0100;add_ctags_to_git_ignore (#2984)

==

.gitignore
==================
105dcb416;Martin Malmsten;2020-02-23 21:47:59 +0100;Now passes style guide enforcement

==

examples/ner/run_ner.py
==================
33eb8a165;Martin Malmsten;2020-02-23 21:43:31 +0100;Added ,

==

examples/ner/run_ner.py
==================
869b66f6b;Martin Malmsten;2020-02-23 21:05:22 +0100;* Added support for Albert when fine-tuning for NER
* Added support for Albert in NER pipeline

* Added command-line options to examples/ner/run_ner.py to better control tokenization

* Added class AlbertForTokenClassification

* Changed output for NerPipeline to use .convert_ids_to_tokens(...) instead of .decode(...) to better reflect tokens

==

examples/ner/run_ner.py
src/transformers/__init__.py
src/transformers/modeling_albert.py
src/transformers/modeling_auto.py
src/transformers/pipelines.py
==================
129f0604a;Sam Shleifer;2020-02-23 11:28:48 -0500;Delete untested, broken Model2LSTM (#2968)

==

src/transformers/modeling_encoder_decoder.py
==================
0e84559d6;Lysandre Debut;2020-02-23 09:50:39 -0500;Correct `special_tokens_mask` when `add_special_tokens=False` (#2965)
Don't know of a use case where that would be useful, but this is more consistent
==

src/transformers/tokenization_utils.py
==================
92487a1dc;Sam Shleifer;2020-02-22 16:25:04 -0500;Bart: fix layerdrop and cached decoder_input_ids for generation (#2969)

==

src/transformers/modeling_bart.py
tests/test_modeling_bart.py
==================
c36416e53;Joe Davison;2020-02-22 12:09:01 -0500;Add standardized get_vocab method to tokenizers

==
==================
cafc4dfc7;saippuakauppias;2020-02-22 15:37:51 +0300;fix hardcoded path in examples readme

==

examples/README.md
==================
34b4b5a9e;Malte Pietsch;2020-02-21 16:45:27 +0100;Update modelcard of bert-base-german-cased
Add image
==

model_cards/bert-base-german-cased-README.md
==================
7df12d7bf;Manuel Romero;2020-02-20 18:30:54 +0100;Update README.md
- I added an example using the model with pipelines to show that we have set```{"use_fast": False}``` in the tokenizer.
- I added a Colab to play with the model and pipelines
- I added a Colab to discover Huggingface pipelines at the end of the document
==

model_cards/mrm8488/distill-bert-base-spanish-wwm-cased-finetuned-spa-squad2-es/README.md
==================
cc6775cdf;Funtowicz Morgan;2020-02-22 15:27:47 +0100;Fix max_length not taken into account when using pad_to_max_length on fast tokenizers (#2961)
* enable_padding should pad up to max_length if set.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Added more testing on padding.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

==

src/transformers/tokenization_utils.py
tests/test_tokenization_fast.py
==================
94ff2d6ee;Lysandre Debut;2020-02-21 17:10:18 -0500;Remove double bias (#2958)

==

src/transformers/modeling_albert.py
src/transformers/modeling_bert.py
src/transformers/modeling_roberta.py
==================
b5b3445c4;Sam Shleifer;2020-02-21 16:10:21 -0500;Only use F.gelu for torch >=1.4.0 (#2955)
* Only use F.gelu for torch >=1.4.0

* Use F.gelu for newer torch

==

src/transformers/activations.py
==================
fc38d4c86;Patrick von Platen;2020-02-21 18:10:00 +0100;Improve special_token_id logic in run_generation.py and add tests (#2885)
* improving generation

* finalized special token behaviour for no_beam_search generation

* solved modeling_utils merge conflict

* solve merge conflicts in modeling_utils.py

* add run_generation improvements from PR #2749

* adapted language generation to not use hardcoded -1 if no padding token is available

* remove the -1 removal as hard coded -1`s are not necessary anymore

* add lightweight language generation testing for randomely initialized models - just checking whether no errors are thrown

* add slow language generation tests for pretrained models using hardcoded output with pytorch seed

* delete ipdb

* check that all generated tokens are valid

* renaming

* renaming Generation -> Generate

* make style

* updated so that generate_beam_search has same token behavior than generate_no_beam_search

* consistent return format for run_generation.py

* deleted pretrain lm generate tests -> will be added in another PR

* cleaning of unused if statements and renaming

* run_generate will always return an iterable

* make style

* consistent renaming

* improve naming, make sure generate function always returns the same tensor, add docstring

* add slow tests for all lmhead models

* make style and improve example comments modeling_utils

* better naming and refactoring in modeling_utils

* improving generation

* finalized special token behaviour for no_beam_search generation

* solved modeling_utils merge conflict

* solve merge conflicts in modeling_utils.py

* add run_generation improvements from PR #2749

* adapted language generation to not use hardcoded -1 if no padding token is available

* remove the -1 removal as hard coded -1`s are not necessary anymore

* add lightweight language generation testing for randomely initialized models - just checking whether no errors are thrown

* add slow language generation tests for pretrained models using hardcoded output with pytorch seed

* delete ipdb

* check that all generated tokens are valid

* renaming

* renaming Generation -> Generate

* make style

* updated so that generate_beam_search has same token behavior than generate_no_beam_search

* consistent return format for run_generation.py

* deleted pretrain lm generate tests -> will be added in another PR

* cleaning of unused if statements and renaming

* run_generate will always return an iterable

* make style

* consistent renaming

* improve naming, make sure generate function always returns the same tensor, add docstring

* add slow tests for all lmhead models

* make style and improve example comments modeling_utils

* better naming and refactoring in modeling_utils

* changed fast random lm generation testing design to more general one

* delete in old testing design in gpt2

* correct old variable name

* temporary fix for encoder_decoder lm generation tests - has to be updated when t5 is fixed

* adapted all fast random generate tests to new design

* better warning description in modeling_utils

* better comment

* better comment and error message

Co-authored-by: Thomas Wolf <thomwolf@users.noreply.github.com>

==

examples/run_generation.py
examples/test_examples.py
src/transformers/configuration_utils.py
src/transformers/modeling_utils.py
tests/test_modeling_common.py
tests/test_modeling_ctrl.py
tests/test_modeling_gpt2.py
tests/test_modeling_openai.py
tests/test_modeling_transfo_xl.py
tests/test_modeling_xlm.py
tests/test_modeling_xlnet.py
==================
c749a543f;maximeilluin;2020-02-21 18:01:02 +0100;Added CamembertForQuestionAnswering (#2746)
* Added CamembertForQuestionAnswering

* fixed camembert tokenizer case

==

examples/run_squad.py
src/transformers/__init__.py
src/transformers/data/processors/squad.py
src/transformers/modeling_camembert.py
==================
5211d333b;Bram Vanroy;2020-02-21 17:28:32 +0100;Update modeling_tf_utils.py (#2924)
Tensorflow does not use .eval() vs .train().

closes https://github.com/huggingface/transformers/issues/2906
==

src/transformers/modeling_tf_utils.py
==================
3e98f27e4;ahotrod;2020-02-21 05:54:41 -0800;Create README.md for xlnet_large_squad (#2942)

==

model_cards/ahotrod/xlnet_large_squad2_512/README.md
==================
4452b44b9;Martin Malmsten;2020-02-21 14:53:05 +0100;Labels are now added to model config under id2label and label2id (#2945)

==

examples/ner/run_ner.py
==================
53ce3854a;Sam Shleifer;2020-02-20 18:11:13 -0500;New BartModel (#2745)
* Results same as fairseq
* Wrote a ton of tests
* Struggled with api signatures
* added some docs



==

docs/source/index.rst
docs/source/model_doc/bart.rst
docs/source/pretrained_models.rst
examples/summarization/modeling_bertabs.py
src/transformers/__init__.py
src/transformers/configuration_auto.py
src/transformers/configuration_bart.py
src/transformers/convert_bart_original_pytorch_checkpoint_to_pytorch.py
src/transformers/modeling_auto.py
src/transformers/modeling_bart.py
src/transformers/modeling_encoder_decoder.py
src/transformers/modeling_roberta.py
src/transformers/modeling_utils.py
src/transformers/tokenization_auto.py
src/transformers/tokenization_bart.py
src/transformers/utils_encoder_decoder.py
tests/test_modeling_bart.py
tests/test_modeling_common.py
tests/test_modeling_roberta.py
tests/test_modeling_t5.py
==================
564fd75d6;guillaume-be;2020-02-20 22:08:21 +0100;Removed unused fields in DistilBert TransformerBlock (#2710)
* Removed unused fields in DistilBert TransformerBlock
==

src/transformers/modeling_distilbert.py
==================
889d3bfdb;srush;2020-02-20 15:31:17 -0500;default arg fix (#2937)

==

examples/ner/transformer_base.py
==================
197d74f98;Joe Davison;2020-02-20 15:25:46 -0500;Add get_vocab method to PretrainedTokenizer

==

src/transformers/tokenization_albert.py
src/transformers/tokenization_bert.py
src/transformers/tokenization_ctrl.py
src/transformers/tokenization_gpt2.py
src/transformers/tokenization_openai.py
src/transformers/tokenization_t5.py
src/transformers/tokenization_transfo_xl.py
src/transformers/tokenization_utils.py
src/transformers/tokenization_xlm.py
src/transformers/tokenization_xlm_roberta.py
src/transformers/tokenization_xlnet.py
tests/test_tokenization_common.py
==================
ea8eba35e;Scott Gigante;2020-02-20 15:25:15 -0500;Fix InputExample docstring (#2891)

==

src/transformers/data/processors/utils.py
==================
e2a6445eb;Funtowicz Morgan;2020-02-20 17:55:03 +0100;Tokenizer fast warnings (#2922)
* Remove warning when pad_to_max_length is not set.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Move RoberTa warning to RoberTa and not GPT2 base tokenizer.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

==

src/transformers/tokenization_gpt2.py
src/transformers/tokenization_roberta.py
src/transformers/tokenization_utils.py
==================
9b3093311;Funtowicz Morgan;2020-02-20 17:53:32 +0100;Expose all constructor parameter for BertTokenizerFast (#2921)
Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>
==

src/transformers/tokenization_bert.py
==================
b662f0e62;srush;2020-02-20 11:50:05 -0500;Support for torch-lightning in NER examples (#2890)
* initial pytorch lightning commit

* tested multigpu

* Fix learning rate schedule

* black formatting

* fix flake8

* isort

* isort

* .

Co-authored-by: Check your git settings! <chris@chris-laptop>

==

examples/README.md
examples/ner/README.md
examples/ner/run.sh
examples/ner/run_ner.py
examples/ner/run_pl.sh
examples/ner/run_pl_ner.py
examples/ner/run_tf_ner.py
examples/ner/transformer_base.py
examples/ner/utils_ner.py
setup.cfg
==================
ab1238393;Ilias Chalkidis;2020-02-19 13:26:16 +0200;Update to include example of LM
The model files have been updated in order to include the classification layers, based on https://github.com/huggingface/transformers/issues/2901, and now can be also used as a LM.
==

model_cards/nlpaueb/bert-base-greek-uncased-v1/README.md
==================
976e9afec;Santiago Castro;2020-02-19 16:49:31 -0500;Add syntax highlighting to the BibTeX in README

==

README.md
==================
cbc570554;Cong;2020-02-20 22:52:01 +0900;Fix spell: EsperBERTo, not EspertBERTo

==

model_cards/julien-c/EsperBERTo-small/README.md
==================
d490b5d50;Funtowicz Morgan;2020-02-20 00:58:04 +0100;Fast Tokenizers save pretrained should return the list of generated file paths. (#2918)
* Correctly return the tuple of generated file(s) when calling save_pretrained

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Quality and format.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

==

src/transformers/tokenization_utils.py
tests/test_tokenization_fast.py
==================
2708b44ee;Lysandre;2020-02-19 18:46:25 -0500;Patch ALBERT with heads in TensorFlow

==

src/transformers/modeling_tf_albert.py
==================
1abd53b1a;Lysandre;2020-02-19 18:24:11 -0500;Patch ALBERT with heads in TensorFlow

==

src/transformers/modeling_tf_albert.py
==================
e67676424;Funtowicz Morgan;2020-02-19 22:09:51 +0100;Override build_inputs_with_special_tokens for fast tokenizers (#2912)
* Override build_inputs_with_special_tokens for fast impl + unittest.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Quality + format.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

==

src/transformers/tokenization_bert.py
src/transformers/tokenization_roberta.py
src/transformers/tokenization_utils.py
tests/test_tokenization_fast.py
==================
59c23ad9c;Lysandre;2020-02-19 11:57:17 -0500;README link + better instructions for release

==

README.md
setup.py
==================
22b2b5790;Lysandre;2020-02-19 11:53:30 -0500;Documentation v2.5.0

==

.circleci/deploy.sh
==================
fb560dcb0;Lysandre;2020-02-19 11:46:19 -0500;Release: v2.5.0
Welcome Rust Tokenizers
==

docs/source/conf.py
setup.py
src/transformers/__init__.py
==================
3f3fa7f7d;Funtowicz Morgan;2020-02-19 17:35:40 +0100;Integrate fast tokenizers library inside transformers (#2674)
* Implemented fast version of tokenizers

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Bumped tokenizers version requirements to latest 0.2.1

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Added matching tests

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Matching OpenAI GPT tokenization !

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Matching GPT2 on tokenizers

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Expose add_prefix_space as constructor parameter for GPT2

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Matching Roberta tokenization !

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Removed fast implementation of CTRL.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Binding TransformerXL tokenizers to Rust.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Updating tests accordingly.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Added tokenizers as top-level modules.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Black & isort.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Rename LookupTable to WordLevel to match Rust side.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Black.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Use "fast" suffix instead of "ru" for rust tokenizers implementations.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Introduce tokenize() method on fast tokenizers.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* encode_plus dispatchs to batch_encode_plus

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* batch_encode_plus now dispatchs to encode if there is only one input element.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Bind all the encode_plus parameter to the forwarded batch_encode_plus call.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Bump tokenizers dependency to 0.3.0

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Formatting.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Fix tokenization_auto with support for new (python, fast) mapping schema.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Give correct fixtures path in test_tokenization_fast.py for the CLI.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Expose max_len_ properties on BertTokenizerFast

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Move max_len_ properties to PreTrainedTokenizerFast and override in specific subclasses.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* _convert_encoding should keep the batch axis tensor if only one sample in the batch.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Add warning message for RobertaTokenizerFast if used for MLM.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Added use_fast (bool) parameter on AutoTokenizer.from_pretrained().

This allows to easily enable/disable Rust-based tokenizer instantiation.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Let's tokenizers handle all the truncation and padding stuff.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Allow to provide tokenizer arguments during pipeline creation.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Update test_fill_mask pipeline to not use fast tokenizers.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Fix too much parameters for convert_encoding.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* When enabling padding, max_length should be set to None.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Avoid returning nested tensors of length 1 when calling encode_plus

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Ensure output is padded when return_tensor is not None.

Tensor creation requires the inital list input to be of the exact same size.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Disable transfoxl unittest if pytorch is not available (required to load the model)

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* encode_plus should not remove the leading batch axis if return_tensor is set

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Temporary disable fast tokenizers on QA pipelines.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Fix formatting issues.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Update tokenizers to 0.4.0

* Update style

* Enable truncation + stride unit test on fast tokenizers.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Add unittest ensuring special_tokens set match between Python and Rust.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Ensure special_tokens are correctly set during construction.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Give more warning feedback to the user in case of padding without pad_token.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* quality & format.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Added possibility to add a single token as str

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Added unittest for add_tokens and add_special_tokens on fast tokenizers.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Fix rebase mismatch on pipelines qa default model.

QA requires cased input while the tokenizers would be uncased.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Addressing review comment: Using offset mapping relative to the original string + unittest.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Addressing review comment: save_vocabulary requires folder and file name

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Addressing review comment: Simplify import for Bert.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Addressing review comment: truncate_and_pad disables padding according to the same heuristic than the one enabling padding.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Addressing review comment: Remove private member access in tokenize()

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Addressing review comment: Bump tokenizers dependency to 0.4.2

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* format & quality.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Addressing review comment: Use named arguments when applicable.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Addressing review comment: Add Github link to Roberta/GPT2 space issue on masked input.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Addressing review comment: Move max_len_single_sentence / max_len_sentences_pair to PreTrainedTokenizerFast + tests.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Addressing review comment: Relax type checking to include tuple and list object.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Addressing review comment: Document the truncate_and_pad manager behavior.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Raise an exception if return_offsets_mapping is not available with the current tokenizer.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Ensure padding is set on the tokenizers before setting any padding strategy + unittest.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* On pytorch we need to stack tensor to get proper new axis.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Generalize tests to different framework removing hard written return_tensors="..."

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Bump tokenizer dependency for num_special_tokens_to_add

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Overflowing tokens in batch_encode_plus are now stacked over the batch axis.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Improved error message for padding strategy without pad token.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Bumping tokenizers dependency to 0.5.0 for release.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Optimizing convert_encoding around 4x improvement. :rocket:

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* expose pad_to_max_length in encode_plus to avoid duplicating the parameters in kwargs

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Generate a proper overflow_to_sampling_mapping when return_overflowing_tokens is True.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Fix unittests for overflow_to_sampling_mapping not being returned as tensor.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Format & quality.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Remove perfect alignment constraint for Roberta (allowing 1% difference max)

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

* Triggering final CI

Co-authored-by: MOI Anthony <xn1t0x@gmail.com>

==

setup.py
src/transformers/__init__.py
src/transformers/pipelines.py
src/transformers/tokenization_auto.py
src/transformers/tokenization_bert.py
src/transformers/tokenization_distilbert.py
src/transformers/tokenization_gpt2.py
src/transformers/tokenization_openai.py
src/transformers/tokenization_roberta.py
src/transformers/tokenization_transfo_xl.py
src/transformers/tokenization_utils.py
tests/test_pipelines.py
tests/test_tokenization_auto.py
tests/test_tokenization_fast.py
==================
ffb93ec0c;Bin Wang;2020-02-18 10:30:21 -0800;Create README.md

==

model_cards/binwang/xlnet-base-cased/README.md
==================
20fc18fbd;Sam Shleifer;2020-02-18 16:14:50 -0500;Skip flaky test_tf_question_answering (#2845)
* Skip flaky test

* Style

==

tests/test_pipelines.py
==================
2ae98336d;VictorSanh;2020-02-18 16:17:35 +0000;fix vocab size in binarized_data (distil): int16 vs int32

==

examples/distillation/scripts/binarized_data.py
==================
0dbddba6d;VictorSanh;2020-02-17 20:19:57 +0000;fix typo in hans example call

==

examples/README.md
==================
29ab4b7f4;Manuel Romero;2020-02-17 15:23:11 +0100;Create README.md

==

model_cards/mrm8488/bert-spanish-cased-finetuned-ner/README.md
==================
c88ed74cc;Stefan Schweter;2020-02-17 01:13:57 +0100;[model_cards] üáπüá∑ Add new (cased) BERTurk model

==

model_cards/dbmdz/bert-base-turkish-cased/README.md
==================
5b2d4f265;Thomas Wolf;2020-02-17 14:36:49 +0100;Merge pull request #2881 from patrickvonplaten/add_vim_swp_to_gitignore
update .gitignore to ignore .swp files created when using vim
==
==================
fb4d8d083;Patrick von Platen;2020-02-17 14:26:28 +0100;update .gitignore to ignore .swp files created when using vim

==

.gitignore
==================
6083c1566;Manuel Romero;2020-02-16 13:10:53 +0100;Update README.md
I trained the model for more epochs so I improved the results. This commit will update the results of the model and add a gif using it with **transformers/pipelines**
==

model_cards/mrm8488/distill-bert-base-spanish-wwm-cased-finetuned-spa-squad2-es/README.md
==================
73028c5df;Julien Chaumond;2020-02-14 15:16:33 -0500;[model_cards] EsperBERTo

==

model_cards/julien-c/EsperBERTo-small-pos/README.md
model_cards/julien-c/EsperBERTo-small/README.md
==================
81fb8d325;Timo Moeller;2020-02-14 19:39:23 +0100;Update model card: new performance chart (#2864)
* Update model performance for correct German conll03 dataset

* Adjust text

* Adjust line spacing

==

model_cards/bert-base-german-cased-README.md
==================
4e69104a1;Julien Chaumond;2020-02-14 10:27:11 -0500;[model_cards] Also use the thumbnail as meta
Co-Authored-By: Ilias Chalkidis <ihalk@di.uoa.gr>

==

model_cards/nlpaueb/bert-base-greek-uncased-v1/README.md
==================
73d79d42b;Julien Chaumond;2020-02-14 09:51:11 -0500;[model_cards] nlptown/bert-base-multilingual-uncased-sentiment
cc @yvespeirsman

Co-Authored-By: Yves Peirsman <yvespeirsman@users.noreply.github.com>

==

model_cards/nlptown/bert-base-multilingual-uncased-sentiment/README.md
==================
47b735f99;Yves Peirsman;2020-02-14 15:31:15 +0100;Added model card for bert-base-multilingual-uncased-sentiment (#2859)
* Created model card for nlptown/bert-base-multilingual-sentiment

* Delete model card

* Created model card for bert-base-multilingual-uncased-sentiment as README

==

model_cards/nlptown/bert-base-multilingual-uncased-sentiment/README.md
==================
7d22fefd3;Julien Chaumond;2020-02-13 20:14:43 -0500;[pipeline] Alias NerPipeline as TokenClassificationPipeline

==

src/transformers/__init__.py
src/transformers/pipelines.py
==================
61a2b7dc9;Manuel Romero;2020-02-14 05:53:43 +0100;Fix typo

==

model_cards/mrm8488/distill-bert-base-spanish-wwm-cased-finetuned-spa-squad2-es/README.md
==================
6e261d3a2;Ilias Chalkidis;2020-02-14 11:40:17 +0200;Fix typos

==

model_cards/nlpaueb/bert-base-greek-uncased-v1/README.md
==================
4e597c8e4;Manuel Romero;2020-02-14 09:45:49 +0100;Fix typo

==

examples/run_xnli.py
==================
925a13ced;Julien Chaumond;2020-02-13 23:07:29 -0500;[model_cards] mv README.md

==

model_cards/mrm8488/distill-bert-base-spanish-wwm-cased-finetuned-spa-squad2-es/README.md
==================
575a3b7aa;Manuel Romero;2020-02-14 04:58:25 +0100;Create distill-bert-base-spanish-wwm-cased-finetuned-spa-squad2-es.md

==

model_cards/mrm8488/distill-bert-base-spanish-wwm-cased-finetuned-spa-squad2-es.md
==================
4d36472b9;Julien Chaumond;2020-02-14 03:25:29 +0000;[run_ner] Don't crash if fine-tuning local model that doesn't end with digit

==

examples/run_ner.py
==================
851401830;Ilias Chalkidis;2020-02-14 02:50:25 +0200;Update with additional information
Added a "Pre-training details" section
==

model_cards/nlpaueb/bert-base-greek-uncased-v1/README.md
==================
1eec69a90;Ilias Chalkidis;2020-02-14 02:16:46 +0200;Create README.md

==

model_cards/nlpaueb/bert-base-greek-uncased-v1/README.md
==================
8744402f1;Felix MIKAELIAN;2020-02-13 23:19:13 +0100;add model_card flaubert-base-uncased-squad (#2833)
* add model_card

* Add tag

cc @fmikaelian

Co-authored-by: Julien Chaumond <chaumond@gmail.com>

==

model_cards/fmikaelian/flaubert-base-uncased-squad/README.md
==================
7f98edd7e;Severin Simmler;2020-02-13 21:43:44 +0100;Model card: Literary German BERT (#2843)
* feat: create model card

* chore: add description

* feat: stats plot

* Delete prosa-jahre.svg

* feat: years plot (again)

* chore: add more details

* fix: typos

* feat: kfold plot

* feat: kfold plot

* Rename model_cards/severinsimmler/literary-german-bert.md to model_cards/severinsimmler/literary-german-bert/README.md

* Support for linked images + add tags

cc @severinsimmler

Co-authored-by: Julien Chaumond <chaumond@gmail.com>

==

model_cards/severinsimmler/literary-german-bert/README.md
model_cards/severinsimmler/literary-german-bert/kfold.png
model_cards/severinsimmler/literary-german-bert/prosa-jahre.png
==================
f1e8a51f0;Joe Davison;2020-02-13 13:29:43 -0500;Preserve spaces in GPT-2 tokenizers (#2778)
* Preserve spaces in GPT-2 tokenizers

Preserves spaces after special tokens in GPT-2 and inhereted (RoBERTa)
tokenizers, enabling correct BPE encoding. Automatically inserts a space
in front of first token in encode function when adding special tokens.

* Add tokenization preprocessing method

* Add framework argument to pipeline factory

Also fixes pipeline test issue. Each test input now treated as a
distinct sequence.

==

src/transformers/pipelines.py
src/transformers/tokenization_gpt2.py
src/transformers/tokenization_roberta.py
src/transformers/tokenization_utils.py
tests/test_pipelines.py
tests/test_tokenization_common.py
tests/test_tokenization_roberta.py
==================
0ed630f13;Sam Shleifer;2020-02-13 09:11:03 -0500;Attempt to increase timeout for circleci slow tests (#2844)

==

.circleci/config.yml
==================
ef74b0f07;Sam Shleifer;2020-02-13 08:28:33 -0500;get_activation('relu') provides a simple mapping from strings i‚Ä¶ (#2807)
* activations.py contains a mapping from string to activation function
* resolves some `gelu` vs `gelu_new` ambiguity

==

src/transformers/activations.py
src/transformers/modeling_bert.py
src/transformers/modeling_distilbert.py
src/transformers/modeling_gpt2.py
src/transformers/modeling_openai.py
src/transformers/modeling_utils.py
src/transformers/modeling_xlm.py
src/transformers/modeling_xlnet.py
tests/test_activations.py
==================
f54a5bd37;Lysandre;2020-02-10 17:59:38 -0500;Raise error when using an mlm flag for a clm model + correct TextDataset

==

examples/run_language_modeling.py
==================
569897ce2;Lysandre;2020-02-10 16:42:49 -0500;Fix a few issues regarding the language modeling script

==

examples/run_language_modeling.py
==================
21da89501;Julien Chaumond;2020-02-11 20:30:08 -0500;[model_cards] Better image for social sharing

==

model_cards/mrm8488/bert-base-spanish-wwm-cased-finetuned-spa-squad2-es/README.md
==================
9a70910d4;Julien Chaumond;2020-02-11 20:20:39 -0500;[model_cards] Tweak @mrm8488's model card

==

model_cards/mrm8488/bert-base-spanish-wwm-cased-finetuned-spa-squad2-es/README.md
==================
9274734a0;Julien Chaumond;2020-02-11 20:13:57 -0500;[model_cards] mv to correct location + tweak tag

==

model_cards/mrm8488/bert-base-spanish-wwm-cased-finetuned-spa-squad2-es/README.md
==================
69f948461;Manuel Romero;2020-02-12 01:45:45 +0100;Create bert-base-spanish-wwm-cased-finetuned-spa-squad2-es.md

==

model_cards/mrm8488/bert-base-spanish-wwm-cased-finetuned-spa-squad2-es.md
==================
e0b6247cf;Julien Chaumond;2020-02-11 18:25:21 -0500;[model_cards] Change formatting slightly as we updated our markdown engine
cc @tholor @loretoparisi @simonefrancia

==

model_cards/Musixmatch/umberto-commoncrawl-cased-v1/README.md
model_cards/Musixmatch/umberto-wikipedia-uncased-v1/README.md
model_cards/bert-base-german-cased-README.md
model_cards/deepset/roberta-base-squad2/README.md
==================
5f2dd71d1;sshleifer;2020-02-11 14:46:08 -0500;Smaller diff

==

CONTRIBUTING.md
==================
31158af57;sshleifer;2020-02-11 14:45:12 -0500;formatting

==

CONTRIBUTING.md
==================
5dd61fb9a;sshleifer;2020-02-11 14:41:29 -0500;Add more specific testing advice to Contributing.md

==

CONTRIBUTING.md
==================
ee5de0ba4;Oleksiy Syvokon;2020-02-06 11:43:31 +0000;BERT decoder: Fix causal mask dtype.
PyTorch < 1.3 requires multiplication operands to be of the same type.
This was violated when using default attention mask (i.e.,
attention_mask=None in arguments) given BERT in the decoder mode.

In particular, this was breaking Model2Model and made tutorial
from the quickstart failing.

==

src/transformers/modeling_bert.py
tests/test_modeling_bert.py
==================
bed38d3af;jiyeon;2020-02-11 15:55:33 +0900;Fix typo in src/transformers/data/processors/squad.py

==

src/transformers/data/processors/squad.py
==================
498d06e91;Stefan Schweter;2020-02-11 16:49:39 +0100;[model_cards] Add new German Europeana BERT models (#2805)
* [model_cards] New German Europeana BERT models from dbmdz

* [model_cards] Update German Europeana BERT models from dbmdz

==

model_cards/dbmdz/bert-base-german-europeana-cased/README.md
model_cards/dbmdz/bert-base-german-europeana-uncased/README.md
==================
3e3a9e2c0;Funtowicz Morgan;2020-02-11 10:48:42 +0000;Merge pull request #2793 from huggingface/tensorflow-210-circleci-fix
Fix circleci cuInit error on Tensorflow >= 2.1.0.
==
==================
1f5db9a13;Julien Chaumond;2020-02-10 17:45:13 -0500;[model_cards] Rm extraneous tag

==

model_cards/julien-c/dummy-unknown/README.md
==================
95bac8dab;Julien Chaumond;2020-02-10 17:42:42 -0500;[model_cards] Add language metadata to existing model cards
This will enable filtering on language (amongst other tags) on the website

cc @loretoparisi, @stefan-it, @HenrykBorzymowski, @marma

==

model_cards/KB/albert-base-swedish-cased-alpha/README.md
model_cards/KB/bert-base-swedish-cased-ner/README.md
model_cards/KB/bert-base-swedish-cased/README.md
model_cards/Musixmatch/umberto-commoncrawl-cased-v1/README.md
model_cards/Musixmatch/umberto-wikipedia-uncased-v1/README.md
model_cards/canwenxu/BERT-of-Theseus-MNLI/README.md
model_cards/dbmdz/bert-base-german-cased/README.md
model_cards/dbmdz/bert-base-german-uncased/README.md
model_cards/dbmdz/bert-base-italian-cased/README.md
model_cards/dbmdz/bert-base-italian-uncased/README.md
model_cards/dbmdz/bert-base-italian-xxl-cased/README.md
model_cards/dbmdz/bert-base-italian-xxl-uncased/README.md
model_cards/henryk/bert-base-multilingual-cased-finetuned-dutch-squad2/README.md
==================
ba498eac3;ahotrod;2020-02-10 14:27:59 -0800;Create README.md (#2785)
* Create README.md

* Update README.md

* Update README.md

* Update README.md

* [model_cards] Use code fences for consistency

Co-authored-by: Julien Chaumond <chaumond@gmail.com>

==

model_cards/ahotrod/albert_xxlargev1_squad2_512/README.md
==================
68ccc04ee;Malte Pietsch;2020-02-10 21:21:48 +0100;Add model readme for deepset/roberta-base-squad2 (#2797)
* Add readme for deepset/roberta-base-squad2

* update model readme

==

model_cards/deepset/roberta-base-squad2/README.md
==================
539f601be;Lysandre;2020-02-10 13:45:57 -0500;intermediate_size > hidden_dim in distilbert config docstrings

==

src/transformers/configuration_distilbert.py
==================
cfb7d108b;Lysandre;2020-02-10 12:19:31 -0500;FlauBERT lang embeddings only when n_langs > 1

==

src/transformers/modeling_flaubert.py
==================
b4691a438;Julien Chaumond;2020-02-10 11:26:14 -0500;[model_cards] BERT-of-Theseus: use the visual as thumbnail
cc @jetrunner

Co-Authored-By: Kevin Canwen Xu <canwenxu@outlook.com>

==

model_cards/canwenxu/BERT-of-Theseus-MNLI/README.md
==================
fc325e97c;Julien Chaumond;2020-02-10 11:25:35 -0500;[model_cards] Showcase model tag syntax

==

model_cards/julien-c/dummy-unknown/README.md
==================
fd639e5be;Lysandre;2020-02-10 11:25:56 -0500;Correct quickstart example when using the past

==

docs/source/quickstart.md
==================
63a5399bc;Julien Chaumond;2020-02-10 11:20:05 -0500;[model_cards] Specify language meta + thumbnail
cc @tholor

see #2799

==

model_cards/bert-base-german-cased-README.md
==================
125a75a12;Lysandre;2020-02-10 10:47:42 -0500;Correctly compute tokens when padding on the left

==

src/transformers/data/processors/squad.py
==================
9c64d1da3;Malte Pietsch;2020-02-10 16:27:29 +0100;Add model readme for bert-base-german-cased (#2799)
* add readme for bert-base-german-cased

* update readme

==

model_cards/bert-base-german-cased-README.md
==================
bf99014c4;Kevin Canwen Xu;2020-02-10 09:37:53 +0800;Create BERT-of-Theseus model card

==

model_cards/canwenxu/BERT-of-Theseus-MNLI/README.md
==================
92e974196;Thomas Wolf;2020-02-10 14:05:16 +0100;Merge pull request #2765 from huggingface/extract-cached-archives
Add option to `cached_path` to automatically extract archives
==
==================
6aa7973ae;Morgan Funtowicz;2020-02-10 13:24:37 +0100;Fix circleci cuInit error on Tensorflow >= 2.1.0.
Tensorflow 2.1.0 introduce a new dependency model where pip install tensorflow would install tf with GPU support.
Before it would just install with CPU support, thus CircleCI is looking for NVidia driver version at initialization of the
tensorflow related tests but fails as their is no NVidia Driver running.

Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

==

.circleci/config.yml
setup.py
==================
520e7f211;Lysandre;2020-02-07 16:37:52 -0500;Correct docstring for xlnet

==

src/transformers/modeling_xlnet.py
==================
dd2883032;Lysandre;2020-02-07 12:17:51 -0500;Update RoBERTa tips

==

docs/source/model_doc/roberta.rst
==================
db9793012;Lysandre;2020-02-07 12:12:25 -0500;Update XLM-R tips

==

docs/source/model_doc/xlmroberta.rst
==================
7046de299;Lysandre;2020-02-07 15:17:40 -0500;E231

==

src/transformers/pipelines.py
==================
0d3aa3c04;VictorSanh;2020-02-07 20:06:51 +0000;styling

==

src/transformers/pipelines.py
==================
d8b43600f;VictorSanh;2020-02-07 19:33:09 +0000;omission

==

tests/test_pipelines.py
==================
ee5a6856c;VictorSanh;2020-02-07 19:19:35 +0000;distilbert-base-cased weights + Readmes + omissions

==

README.md
docs/source/pretrained_models.rst
examples/distillation/README.md
examples/distillation/training_configs/distilbert-base-cased.json
src/transformers/configuration_distilbert.py
src/transformers/convert_pytorch_checkpoint_to_tf2.py
src/transformers/modeling_distilbert.py
src/transformers/modeling_tf_distilbert.py
src/transformers/pipelines.py
src/transformers/tokenization_distilbert.py
tests/test_pipelines.py
==================
73368963b;monologg;2020-01-27 23:39:44 +0900;Fix importing unofficial TF models with extra optimizer weights

==

src/transformers/modeling_albert.py
src/transformers/modeling_bert.py
src/transformers/modeling_t5.py
templates/adding_a_new_model/modeling_xxx.py
==================
d7dabfeff;Ari;2020-02-06 17:00:34 -0800;Fix documentation in ProjectedAdaptiveLogSoftmax

==

src/transformers/modeling_transfo_xl_utilities.py
==================
42f08e596;Julien Chaumond;2020-02-06 15:09:29 -0500;[examples] rename run_lm_finetuning to run_language_modeling

==

docs/source/main_classes/processors.rst
examples/README.md
examples/distillation/README.md
examples/run_language_modeling.py
==================
4f7bdb095;Julien Chaumond;2020-02-06 14:59:53 -0500;[examples] Fix broken markdown

==

examples/README.md
==================
c6c5c3fd4;thomwolf;2020-02-07 08:58:06 +0100;style and quality

==

src/transformers/modeling_tf_utils.py
src/transformers/modeling_utils.py
==================
961c69776;thomwolf;2020-02-07 08:53:17 +0100;@julien-c proposal for TF/PT compat in hf_buckets

==

src/transformers/modeling_tf_utils.py
src/transformers/modeling_utils.py
==================
d311f87bc;thomwolf;2020-02-07 00:05:28 +0100;cleanup

==

src/transformers/file_utils.py
==================
7d99e05f7;thomwolf;2020-02-07 00:03:12 +0100;file_cache has options to extract archives

==

src/transformers/file_utils.py
==================
2c12464a2;dchurchwell;2020-02-06 02:02:48 -0700;Changed vocabulary save function. Variable name was inconsistent, causing an error to be thrown when passing a file name instead of a directory.

==

src/transformers/tokenization_transfo_xl.py
==================
6fc3d34ab;Peter Izsak;2020-02-06 09:12:54 +0200;Fix multi-gpu evaluation in run_glue.py

==

examples/run_glue.py
==================
7748cbbe7;Julien Chaumond;2020-02-06 13:50:13 -0500;Oopsie

==

docs/source/_static/css/huggingface.css
==================
432c12521;Julien Chaumond;2020-02-06 13:49:35 -0500;[docs] Add menu w/ links to other pages on hf.co

==

docs/source/_static/css/huggingface.css
docs/source/_static/js/custom.js
==================
c069932f5;Clement;2020-02-06 14:17:36 -0500;Add contributors snapshot
powered by https://github.com/sourcerer-io/hall-of-fame
==

README.md
==================
33d3072e1;Lysandre Debut;2020-02-05 15:26:28 -0500;Arxiv README (#2747)
* Arxiv README

* ArXiv-NLP readme

==

model_cards/lysandre/arxiv-nlp/README.md
model_cards/lysandre/arxiv/README.md
==================
eae8ee038;Julien Chaumond;2020-02-05 14:20:03 -0500;[doc] model sharing: mention README.md + tweaks
cc @lysandrejik @thomwolf

==

README.md
docs/source/model_sharing.md
==================
6bb6a0176;James Betker;2020-02-04 15:10:11 -0700;Fix GPT2 config set to trainable
This prevents the model from being saved, and who knows
what else.

==

src/transformers/modeling_tf_gpt2.py
==================
ada24def2;Julien Chaumond;2020-02-05 12:49:18 -0500;[run_lm_finetuning] Tweak fix for non-long tensor, close #2728
see 1ebfeb79469d544a2bd817aa32c77e0514485ff9 and #2728

Co-Authored-By: Lysandre Debut <lysandre.debut@reseau.eseo.fr>

==

examples/run_lm_finetuning.py
==================
2184f8700;Lysandre;2020-02-03 18:39:54 -0500;RoBERTa TensorFlow Tests

==

tests/test_modeling_tf_roberta.py
==================
e615269cb;Lysandre;2020-02-03 18:33:27 -0500;Correct slow test

==

tests/test_modeling_flaubert.py
==================
5f96ebc0b;Lysandre;2020-02-03 18:28:32 -0500;Style

==

tests/test_modeling_flaubert.py
tests/test_modeling_roberta.py
==================
950c6a4f0;Lysandre;2020-02-03 18:28:17 -0500;Flaubert PyTorch tests

==

tests/test_modeling_flaubert.py
==================
d28b81dc2;Lysandre;2020-02-03 18:22:48 -0500;RoBERTa Pytorch tests

==

tests/test_modeling_roberta.py
==================
d1ab1fab1;Yuval Pinter;2020-02-04 17:12:42 -0500;pass langs parameter to certain XLM models (#2734)
* pass langs parameter to certain XLM models

Adding an argument that specifies the language the SQuAD dataset is in so language-sensitive XLMs (e.g. `xlm-mlm-tlm-xnli15-1024`) don't default to language `0`.
Allows resolution of issue #1799 .

* fixing from `make style`

* fixing style (again)

==

examples/run_squad.py
==================
9e5b549b4;sshleifer;2020-02-04 14:09:45 -0500;fix default getattr

==

tests/test_modeling_common.py
==================
25848a609;sshleifer;2020-02-04 13:58:05 -0500;double quotes

==

tests/test_modeling_common.py
==================
cbcb83f21;sshleifer;2020-02-03 17:03:16 -0500;minor cleanup of test_attention_outputs

==

tests/test_modeling_common.py
==================
3bf541725;Lysandre;2020-02-04 16:31:07 -0500;Revert erroneous fix

==

examples/contrib/run_openai_gpt.py
examples/distillation/distiller.py
examples/run_lm_finetuning.py
src/transformers/modeling_albert.py
src/transformers/modeling_bert.py
src/transformers/modeling_ctrl.py
src/transformers/modeling_distilbert.py
src/transformers/modeling_gpt2.py
src/transformers/modeling_openai.py
src/transformers/modeling_roberta.py
src/transformers/modeling_t5.py
src/transformers/modeling_transfo_xl.py
src/transformers/modeling_xlm.py
src/transformers/modeling_xlnet.py
==================
1ebfeb794;Lysandre;2020-02-04 15:56:16 -0500;Cast to long when masking tokens

==

examples/run_lm_finetuning.py
==================
9c67196b8;Lysandre;2020-02-04 11:11:37 -0500;Update quickstart

==

docs/source/quickstart.md
==================
90ab15cb7;Lysandre;2020-01-08 13:52:46 +0100;Remove redundant hidden states

==

src/transformers/modeling_encoder_decoder.py
==================
9a50828b5;Julien Chaumond;2020-02-03 17:53:39 -0500;Pipelines: fix crash when modelcard is None
cc @mfuntowicz does this seem correct?

==

src/transformers/pipelines.py
==================
6c1b23554;Lysandre;2020-02-03 17:23:53 -0500;Sample instead of greedy decoding by default in generate

==

src/transformers/modeling_utils.py
==================
239dd23f6;Lysandre;2020-02-03 16:08:05 -0500;[Follow up 213]
Masked indices should have -1 and not -100. Updating documentation + scripts that were forgotten
==

examples/contrib/run_openai_gpt.py
examples/distillation/distiller.py
examples/run_lm_finetuning.py
src/transformers/modeling_albert.py
src/transformers/modeling_bert.py
src/transformers/modeling_ctrl.py
src/transformers/modeling_distilbert.py
src/transformers/modeling_gpt2.py
src/transformers/modeling_openai.py
src/transformers/modeling_roberta.py
src/transformers/modeling_t5.py
src/transformers/modeling_transfo_xl.py
src/transformers/modeling_xlm.py
src/transformers/modeling_xlnet.py
==================
522c5b553;Martin Malmsten;2020-02-02 23:08:53 +0100;Added README.md to Swedish BERT models from National Library of Sweden

==

model_cards/KB/albert-base-swedish-cased-alpha/README.md
model_cards/KB/bert-base-swedish-cased-ner/README.md
model_cards/KB/bert-base-swedish-cased/README.md
==================
9329e5970;Julien Plu;2020-02-03 13:24:27 +0100;Add READMEs to Tensorflow versions of CamemBERT and XLM-RoBERTa

==

model_cards/jplu/tf-camembert-base/README.md
model_cards/jplu/tf-xlm-roberta-base/README.md
model_cards/jplu/tf-xlm-roberta-large/README.md
==================
2ba147ecf;Antonio Carlos Falc√£o Petri;2020-02-01 11:35:41 -0300;Fix typo in examples/utils_ner.py
"%s-%d".format() -> "{}-{}".format()
==

examples/utils_ner.py
==================
9773e5e0d;Bram Vanroy;2020-02-01 16:38:14 +0100;CLI script to gather environment info (#2699)
* add "info" command to CLI

As a convenience, add the info directive to CLI. Running `python transformers-cli info` will return a string containing the transformers version, platform, python version, PT/TF version and GPU support

* Swap f-strings for .format

Still supporting 3.5 so can't use f-strings (sad face)

* Add reference in issue to CLI

* Add the expected fields to issue template

This way, people can still add the information manually if they want. (Though I fear they'll just ignore it.)

* Remove heading from output

* black-ify

* order of imports

Should ensure isort test passes

* use is_X_available over import..pass

* style

* fix copy-paste bug

* Rename command info -> env

Also adds the command to CONTRIBUTING.md in "Did you find a bug" section

==

.github/ISSUE_TEMPLATE/bug-report.md
.github/ISSUE_TEMPLATE/migration.md
CONTRIBUTING.md
src/transformers/commands/env.py
transformers-cli
==================
ddb6f9476;Julien Chaumond;2020-01-31 17:12:39 -0500;[model_cards] dbmdz models
Co-Authored-By: Stefan Schweter <stefan-it@users.noreply.github.com>

==

model_cards/dbmdz/bert-base-german-cased/README.md
model_cards/dbmdz/bert-base-german-uncased/README.md
model_cards/dbmdz/bert-base-italian-cased/README.md
model_cards/dbmdz/bert-base-italian-uncased/README.md
model_cards/dbmdz/bert-base-italian-xxl-cased/README.md
model_cards/dbmdz/bert-base-italian-xxl-uncased/README.md
==================
6636826f0;Julien Chaumond;2020-01-31 17:12:03 -0500;[model_cards] Multilingual + Dutch SQuAD2.0
Co-Authored-By: HenrykBorzymowski <henrykborzymowski@users.noreply.github.com>

==

model_cards/henryk/bert-base-multilingual-cased-finetuned-dutch-squad2/README.md
==================
98dadc98e;Julien Chaumond;2020-01-31 17:10:51 -0500;[model_cards] UmBERTo
Co-Authored-By: Loreto Parisi <loretoparisi@gmail.com>
Co-Authored-By: Simone Francia <francia.simone1@gmail.com>

==

model_cards/Musixmatch/umberto-commoncrawl-cased-v1/README.md
model_cards/Musixmatch/umberto-wikipedia-uncased-v1/README.md
==================
d6fc34b45;Julien Chaumond;2020-01-31 17:10:04 -0500;[model_cards] add mine

==

model_cards/julien-c/bert-xsmall-dummy/README.md
model_cards/julien-c/dummy-unknown/README.md
==================
d426b58b9;Lysandre;2020-01-31 14:55:33 -0500;Patch: v2.4.1

==

docs/source/conf.py
setup.py
src/transformers/__init__.py
==================
1e82cd845;Lysandre;2020-01-31 14:16:34 -0500;Flaubert auto tokenizer + tests
cc @julien-c
==

src/transformers/configuration_flaubert.py
src/transformers/tokenization_auto.py
tests/test_modeling_auto.py
tests/test_tokenization_auto.py
==================
d18d47be6;Lysandre;2020-01-31 12:05:48 -0500;run_generation style

==

examples/run_generation.py
==================
ff6f1492e;Lysandre;2020-01-31 12:05:15 -0500;FlauBERT load in AutoModel
The FlauBERT configuration file inherits from XLMConfig, and is recognized as such when loading from AutoModels as the XLMConfig is checked before the FlaubertConfig.

Changing the order solves this problem, but a test should be added.
==

src/transformers/modeling_auto.py
==================
7365f01d4;Lysandre;2020-01-31 11:49:32 -0500;do_sample should be set to True in run_generation.py

==

examples/run_generation.py
==================
3a21d6da6;Arnaud;2020-01-31 16:38:57 +0100;Typo on markdown link in README.md

==

README.md
==================
0aa40e956;Lysandre;2020-01-31 09:55:34 -0500;v2.4.0 documentation

==

.circleci/deploy.sh
README.md
==================
8036ceb7c;Lysandre;2020-01-31 09:48:15 -0500;Update commands for pypi test

==

setup.py
==================
6664ea943;Lysandre;2020-01-31 09:40:32 -0500;Release: v2.4.0

==

docs/source/conf.py
setup.py
src/transformers/__init__.py
==================
5a6b138b0;Julien Chaumond;2020-01-30 21:05:53 -0500;[Umberto] model shortcuts (#2661)
* [Umberto] model shortcuts

cc @loretoparisi @simonefrancia

see #2485

* Ensure that tokenizers will be correctly configured

==

src/transformers/configuration_camembert.py
src/transformers/modeling_camembert.py
src/transformers/tokenization_camembert.py
==================
7fe294bf0;Julien Chaumond;2020-01-30 20:05:04 -0500;Hotfix: same handling of non-existent files as for config

==

src/transformers/modelcard.py
==================
b85c59f99;Julien Chaumond;2020-01-30 16:45:52 -0500;config.architectures

==

src/transformers/configuration_utils.py
src/transformers/modeling_utils.py
==================
f9bc3f577;Julien Chaumond;2020-01-30 16:45:38 -0500;style tweak

==

src/transformers/configuration_auto.py
src/transformers/tokenization_auto.py
==================
0b13fb822;Julien Chaumond;2020-01-30 16:01:22 -0500;No need for a model_type here
cc @lysandrejik

==

src/transformers/modeling_flaubert.py
==================
71a382319;Jared Nielsen;2020-01-30 13:51:54 -0800;Correct documentation

==

examples/README.md
==================
01a14ebd8;Lysandre;2020-01-30 18:40:22 -0500;Add FlauBERT to automodels

==

src/transformers/modeling_auto.py
==================
9fa836a73;Julien Chaumond;2020-01-30 18:15:42 -0500;fill_mask helper (#2576)
* fill_mask helper

* [poc] FillMaskPipeline

* Revert "[poc] FillMaskPipeline"

This reverts commit 67eeea55b0f97b46c2b828de0f4ee97d87338335.

* Revert "fill_mask helper"

This reverts commit cacc17b884e14bb6b07989110ffe884ad9e36eaa.

* README: clarify that Pipelines can also do text-classification

cf. question at the AI&ML meetup last week, @mfuntowicz

* Fix test: test feature-extraction pipeline

* Test tweaks

* Slight refactor of existing pipeline (in preparation of new FillMaskPipeline)

* Extraneous doc

* More robust way of doing this

@mfuntowicz as we don't rely on the model name anymore (see AutoConfig)

* Also add RobertaConfig as a quickfix for wrong token_type_ids

* cs

* [BIG] FillMaskPipeline

==

README.md
src/transformers/__init__.py
src/transformers/pipelines.py
tests/test_pipelines.py
==================
b43cb09aa;Hang Le;2020-01-30 11:40:18 +0100;Add layerdrop

==

docs/source/index.rst
src/transformers/configuration_flaubert.py
src/transformers/modeling_flaubert.py
==================
df27648bd;Lysandre;2020-01-30 10:07:22 -0500;Rename test_examples to test_doc_samples

==

tests/test_doc_samples.py
==================
93dccf527;Lysandre;2020-01-30 09:49:11 -0500;Pretrained models

==

docs/source/pretrained_models.rst
==================
90787fed8;Lysandre;2020-01-29 15:19:33 -0500;Style

==

src/transformers/modeling_flaubert.py
==================
73306d028;Lysandre;2020-01-29 15:16:22 -0500;FlauBERT documentation

==

docs/source/index.rst
docs/source/model_doc/flaubert.rst
src/transformers/configuration_flaubert.py
src/transformers/modeling_flaubert.py
==================
ce2f4227a;Lysandre;2020-01-29 14:17:12 -0500;Fix failing FlauBERT test

==

src/transformers/configuration_auto.py
==================
f0a4fc6cd;Hang Le;2020-01-15 17:22:25 +0100;Add Flaubert

==

README.md
examples/run_glue.py
src/transformers/__init__.py
src/transformers/configuration_auto.py
src/transformers/configuration_flaubert.py
src/transformers/modeling_flaubert.py
src/transformers/tokenization_flaubert.py
==================
a5381495e;Peter Izsak;2020-01-29 15:25:10 +0200;Added classifier dropout rate in ALBERT

==

src/transformers/configuration_albert.py
src/transformers/modeling_albert.py
==================
83446a88d;Bram Vanroy;2020-01-29 09:34:37 +0100;Use _pad_token  of pad_token_id
Requesting pad_token_id would cause an error message when it is None. Use private _pad_token instead.

==

src/transformers/tokenization_utils.py
==================
9fde13a3a;BramVanroy;2020-01-28 11:25:37 +0100;Add check to verify existence of pad_token_id
In batch_encode_plus we have to ensure that the tokenizer has a pad_token_id so that, when padding, no None values are added as padding. That would happen with gpt2, openai, transfoxl.

closes https://github.com/huggingface/transformers/issues/2640

==

src/transformers/tokenization_utils.py
==================
e63a81dd2;Lysandre;2020-01-29 16:29:20 -0500;Style

==

tests/test_tokenization_common.py
==================
217349016;Lysandre;2020-01-29 16:15:39 -0500;Copy object instead of passing the reference

==

src/transformers/tokenization_utils.py
tests/test_tokenization_common.py
==================
adb8c9313;Jared Nielsen;2020-01-28 17:24:42 -0800;Remove lines causing a KeyError

==

examples/run_tf_glue.py
==================
c69b08260;Lysandre;2020-01-29 12:05:03 -0500;Update documentation

==

docs/source/model_doc/camembert.rst
src/transformers/convert_pytorch_checkpoint_to_tf2.py
src/transformers/modeling_tf_camembert.py
==================
ca1d66734;Julien Plu;2020-01-07 15:51:39 +0100;Apply quality and style requirements once again

==

src/transformers/__init__.py
src/transformers/modeling_tf_camembert.py
templates/adding_a_new_example_script/run_xxx.py
templates/adding_a_new_example_script/utils_xxx.py
==================
5e3c72842;Julien Plu;2020-01-07 15:43:39 +0100;bugfix on model name

==

src/transformers/convert_pytorch_checkpoint_to_tf2.py
src/transformers/modeling_tf_camembert.py
==================
0731fa158;Julien Plu;2020-01-07 15:35:45 +0100;Apply quality and style requirements

==

src/transformers/__init__.py
src/transformers/convert_pytorch_checkpoint_to_tf2.py
src/transformers/modeling_tf_camembert.py
templates/adding_a_new_example_script/run_xxx.py
templates/adding_a_new_example_script/utils_xxx.py
==================
a3998e76a;Julien Plu;2020-01-07 14:57:57 +0100;Add TF2 CamemBERT model

==

src/transformers/__init__.py
src/transformers/convert_pytorch_checkpoint_to_tf2.py
src/transformers/modeling_tf_camembert.py
==================
b5625f131;Lysandre;2020-01-29 11:46:39 -0500;Style

==

src/transformers/modeling_tf_xlm_roberta.py
==================
44a5b4bbe;Lysandre;2020-01-29 11:45:38 -0500;Update documentation

==

docs/source/model_doc/xlmroberta.rst
src/transformers/modeling_tf_xlm_roberta.py
==================
7fc628d98;Julien Plu;2020-01-08 15:11:54 +0100;Apply style

==

src/transformers/__init__.py
src/transformers/convert_pytorch_checkpoint_to_tf2.py
templates/adding_a_new_example_script/run_xxx.py
templates/adding_a_new_example_script/utils_xxx.py
==================
64ca85561;Julien Plu;2020-01-08 14:22:56 +0100;Add TF2 XLM-RoBERTa model

==

src/transformers/__init__.py
src/transformers/convert_pytorch_checkpoint_to_tf2.py
src/transformers/modeling_tf_xlm_roberta.py
templates/adding_a_new_example_script/run_xxx.py
templates/adding_a_new_example_script/utils_xxx.py
==================
9d87eafd1;BramVanroy;2020-01-28 13:16:31 +0100;Streamlining
- mostly stylistic streamlining
- removed 'additional context' sections. They seem to be rarely used and might cause confusion. If more details are needed, users can add them to the 'details' section

==

.github/ISSUE_TEMPLATE/---new-benchmark.md
.github/ISSUE_TEMPLATE/--new-model-addition.md
.github/ISSUE_TEMPLATE/bug-report.md
.github/ISSUE_TEMPLATE/feature-request.md
.github/ISSUE_TEMPLATE/migration.md
.github/ISSUE_TEMPLATE/question-help.md
==================
a3b3638f6;BramVanroy;2020-01-28 13:09:13 +0100;phrasing

==

.github/ISSUE_TEMPLATE/question-help.md
==================
c96ca70f2;BramVanroy;2020-01-28 13:06:46 +0100;Update ---new-benchmark.md

==

.github/ISSUE_TEMPLATE/---new-benchmark.md
==================
7b5eda32b;BramVanroy;2020-01-28 13:05:26 +0100;Update --new-model-addition.md
Motivate users to @-tag authors of models to increase visibility and expand the community

==

.github/ISSUE_TEMPLATE/--new-model-addition.md
==================
c63d91dd1;BramVanroy;2020-01-28 13:03:50 +0100;Update bug-report.md
- change references to pytorch-transformers to transformers
- link to code formatting guidelines

==

.github/ISSUE_TEMPLATE/bug-report.md
==================
b2907cd06;BramVanroy;2020-01-28 13:00:38 +0100;Update feature-request.md
- add 'your contribution' section
- add code formatting link to 'additional context'

==

.github/ISSUE_TEMPLATE/feature-request.md
==================
2fec88ee0;BramVanroy;2020-01-28 12:57:43 +0100;Update question-help.md
Prefer that general questions are asked on Stack Overflow

==

.github/ISSUE_TEMPLATE/question-help.md
==================
7e03d2bd7;BramVanroy;2020-01-28 12:03:30 +0100;update migration guide
Streamlines usages of pytorch-transformers and pytorch-pretrained-bert. Add link to the README for the migration guide.

==

.github/ISSUE_TEMPLATE/migration.md
==================
335dd5e68;Lysandre;2020-01-28 09:42:11 -0500;Default save steps 50 to 500 in all scripts

==

examples/run_glue.py
examples/run_lm_finetuning.py
examples/run_multiple_choice.py
examples/run_ner.py
examples/run_squad.py
examples/run_xnli.py
==================
ea2600bd5;Lysandre;2020-01-27 21:57:23 -0500;Absolute definitive HeisenDistilBug solve
cc @julien-c @thomwolf
==

tests/test_modeling_tf_common.py
==================
5c3d441ee;Wietse de Vries;2019-12-23 13:54:14 +0100;Fix formatting

==

src/transformers/modeling_tf_bert.py
src/transformers/tokenization_bert.py
==================
f5a236c3c;Wietse de Vries;2019-12-19 13:28:53 +0100;Add Dutch pre-trained BERT model

==

docs/source/pretrained_models.rst
src/transformers/configuration_bert.py
src/transformers/modeling_bert.py
src/transformers/modeling_tf_bert.py
src/transformers/tokenization_bert.py
==================
6b4c3ee23;Julien Chaumond;2020-01-27 20:14:02 -0500;[run_lm_finetuning] GPT2 tokenizer doesn't have a pad_token
ping @lysandrejik

==

examples/run_lm_finetuning.py
==================
79815bf66;Julien Chaumond;2020-01-27 19:58:25 -0500;[serving] Fix typo

==

src/transformers/commands/serving.py
==================
5004d5af4;Julien Chaumond;2020-01-27 19:58:00 -0500;[serving] Update dependencies

==

setup.py
src/transformers/commands/serving.py
==================
9ca21c838;Lysandre;2020-01-27 14:49:12 -0500;Style

==

src/transformers/commands/serving.py
==================
e0849a66a;thomwolf;2020-01-24 17:54:32 -0500;adding in the doc

==

docs/source/model_doc/auto.rst
==================
6b081f04e;thomwolf;2020-01-24 17:50:01 -0500;style and quality

==

src/transformers/modeling_auto.py
src/transformers/modeling_tf_auto.py
==================
0e31e06a7;thomwolf;2020-01-24 17:49:02 -0500;Add AutoModelForPreTraining

==

src/transformers/__init__.py
src/transformers/modeling_auto.py
src/transformers/modeling_tf_auto.py
tests/test_modeling_auto.py
tests/test_modeling_tf_auto.py
==================
ea56d305b;Julien Chaumond;2020-01-27 12:13:32 -0500;make style

==

src/transformers/commands/serving.py
src/transformers/modeling_auto.py
==================
d440e21f5;Malte Pietsch;2020-01-27 17:08:44 +0100;add mapping of roberta for QA

==

src/transformers/modeling_auto.py
==================
875c4ae48;Lysandre;2020-01-27 12:09:58 -0500;Definitive HeisenDistilBug fix
cc @julien-c @@thomwolf
==

tests/test_modeling_tf_common.py
tests/test_modeling_tf_distilbert.py
==================
f09f42d4d;Lysandre;2020-01-27 11:46:00 -0500;Input Embeddings should be assigned
cc @julien-c
==

src/transformers/modeling_xlnet.py
==================
bac51fba3;Maksym Del;2020-01-27 17:04:16 +0200;Fix token_type_ids for XLM-R

==

src/transformers/tokenization_xlm_roberta.py
==================
babd41e7f;Lysandre;2020-01-24 17:06:55 -0500;Code quality

==

src/transformers/configuration_utils.py
==================
974d083c7;Lysandre;2020-01-24 16:46:03 -0500;Accurate model for configuration

==

src/transformers/modeling_auto.py
==================
983fef469;Lysandre;2020-01-24 16:36:11 -0500;AutoModels doc

==

docs/source/model_doc/auto.rst
src/transformers/modeling_auto.py
==================
009fcb0ec;Lysandre;2020-01-24 15:26:18 -0500;Configuration utils

==

src/transformers/configuration_utils.py
==================
11b13e94a;Julien Chaumond;2020-01-24 14:00:57 -0500;Add type to help my IDE out

==

src/transformers/modeling_utils.py
==================
1ce3fb5cc;VictorSanh;2020-01-24 11:45:22 -0500;update correct eval metrics (distilbert & co)

==

examples/README.md
examples/distillation/README.md
==================
62f580460;Nicholas Lourie;2020-01-21 22:01:00 -0800;Update the doc string for T5WithLMHeadModel
T5WithLMHeadModel's doc string claims that indices of -1 are
ignored while computing the cross-entropy loss in the forward
pass; however, indices of -1 throw an error while indices of -100
are ignored. This commit updates the doc string to be consistent
with the class's behavior.

==

src/transformers/modeling_t5.py
==================
908230d26;Lysandre;2020-01-24 10:08:59 -0500;Pickle CamemBERT tokenizer

==

src/transformers/tokenization_camembert.py
==================
24d5ad1dc;Lysandre;2020-01-22 19:24:02 -0500;Run the examples in slow

==

docs/source/model_doc/gpt.rst
src/transformers/file_utils.py
src/transformers/modeling_albert.py
src/transformers/modeling_bert.py
src/transformers/modeling_ctrl.py
src/transformers/modeling_distilbert.py
src/transformers/modeling_encoder_decoder.py
src/transformers/modeling_gpt2.py
src/transformers/modeling_mmbt.py
src/transformers/modeling_openai.py
src/transformers/modeling_roberta.py
src/transformers/modeling_tf_bert.py
src/transformers/modeling_tf_ctrl.py
src/transformers/modeling_tf_distilbert.py
src/transformers/modeling_tf_gpt2.py
src/transformers/modeling_tf_openai.py
src/transformers/modeling_tf_roberta.py
src/transformers/modeling_tf_utils.py
src/transformers/modeling_tf_xlnet.py
src/transformers/modeling_transfo_xl.py
src/transformers/modeling_utils.py
src/transformers/modeling_xlm.py
src/transformers/modeling_xlnet.py
tests/test_examples.py
==================
9ddf60b69;Lysandre;2020-01-21 15:58:25 -0500;Tips + whitespaces

==

docs/source/model_doc/camembert.rst
docs/source/model_doc/ctrl.rst
docs/source/model_doc/distilbert.rst
docs/source/model_doc/gpt.rst
docs/source/model_doc/gpt2.rst
docs/source/model_doc/roberta.rst
docs/source/model_doc/transformerxl.rst
docs/source/model_doc/xlm.rst
docs/source/model_doc/xlmroberta.rst
docs/source/model_doc/xlnet.rst
src/transformers/configuration_camembert.py
src/transformers/file_utils.py
src/transformers/modeling_albert.py
src/transformers/modeling_bert.py
src/transformers/modeling_camembert.py
src/transformers/modeling_ctrl.py
src/transformers/modeling_distilbert.py
src/transformers/modeling_gpt2.py
src/transformers/modeling_openai.py
src/transformers/modeling_roberta.py
src/transformers/modeling_tf_albert.py
src/transformers/modeling_tf_bert.py
src/transformers/modeling_tf_ctrl.py
src/transformers/modeling_tf_distilbert.py
src/transformers/modeling_tf_gpt2.py
src/transformers/modeling_tf_openai.py
src/transformers/modeling_tf_roberta.py
src/transformers/modeling_tf_transfo_xl.py
src/transformers/modeling_tf_xlm.py
src/transformers/modeling_tf_xlnet.py
src/transformers/modeling_transfo_xl.py
src/transformers/modeling_xlm.py
src/transformers/modeling_xlm_roberta.py
src/transformers/modeling_xlnet.py
==================
0e9899f45;Lysandre;2020-01-20 16:57:54 -0500;Fixes

==

docs/source/model_doc/camembert.rst
docs/source/model_doc/ctrl.rst
src/transformers/modeling_albert.py
src/transformers/modeling_bert.py
src/transformers/modeling_ctrl.py
src/transformers/modeling_distilbert.py
src/transformers/modeling_gpt2.py
src/transformers/modeling_openai.py
src/transformers/modeling_roberta.py
src/transformers/modeling_tf_albert.py
src/transformers/modeling_tf_bert.py
src/transformers/modeling_tf_ctrl.py
src/transformers/modeling_tf_distilbert.py
src/transformers/modeling_tf_gpt2.py
src/transformers/modeling_tf_openai.py
src/transformers/modeling_tf_roberta.py
src/transformers/modeling_tf_transfo_xl.py
src/transformers/modeling_tf_xlm.py
src/transformers/modeling_tf_xlnet.py
src/transformers/modeling_transfo_xl.py
src/transformers/modeling_utils.py
src/transformers/modeling_xlm.py
src/transformers/modeling_xlnet.py
==================
48ac24020;Lysandre;2020-01-20 16:23:18 -0500;TF CTRL

==

src/transformers/modeling_ctrl.py
src/transformers/modeling_tf_ctrl.py
==================
7511f3dd8;Lysandre;2020-01-20 16:17:54 -0500;PyTorch CTRL + Style

==

docs/source/model_doc/ctrl.rst
src/transformers/configuration_xlm_roberta.py
src/transformers/modeling_albert.py
src/transformers/modeling_bert.py
src/transformers/modeling_camembert.py
src/transformers/modeling_ctrl.py
src/transformers/modeling_distilbert.py
src/transformers/modeling_gpt2.py
src/transformers/modeling_openai.py
src/transformers/modeling_roberta.py
src/transformers/modeling_tf_albert.py
src/transformers/modeling_tf_bert.py
src/transformers/modeling_tf_distilbert.py
src/transformers/modeling_tf_gpt2.py
src/transformers/modeling_tf_openai.py
src/transformers/modeling_tf_roberta.py
src/transformers/modeling_tf_transfo_xl.py
src/transformers/modeling_tf_xlm.py
src/transformers/modeling_tf_xlnet.py
src/transformers/modeling_transfo_xl.py
src/transformers/modeling_xlm.py
src/transformers/modeling_xlm_roberta.py
src/transformers/modeling_xlnet.py
==================
980211a63;Lysandre;2020-01-20 16:09:09 -0500;XLM-RoBERTa

==

docs/source/index.rst
docs/source/model_doc/xlmroberta.rst
src/transformers/configuration_xlm_roberta.py
src/transformers/modeling_bert.py
src/transformers/modeling_xlm_roberta.py
==================
6bc966793;Lysandre;2020-01-20 15:42:25 -0500;TF DistilBERT

==

src/transformers/modeling_tf_distilbert.py
==================
db1a7f27a;Lysandre;2020-01-20 15:01:36 -0500;PyTorch DistilBERT

==

docs/source/model_doc/distilbert.rst
src/transformers/modeling_distilbert.py
==================
b28020f59;Lysandre;2020-01-20 13:31:04 -0500;TF RoBERTa

==

docs/source/model_doc/roberta.rst
src/transformers/modeling_tf_roberta.py
==================
3e1bc27e1;Lysandre;2020-01-20 13:24:04 -0500;Pytorch RoBERTa

==

docs/source/model_doc/roberta.rst
src/transformers/modeling_bert.py
src/transformers/modeling_roberta.py
==================
f44ff574d;Lysandre;2020-01-17 18:13:14 -0500;Camembert

==

docs/source/model_doc/camembert.rst
docs/source/model_doc/roberta.rst
src/transformers/modeling_camembert.py
==================
264eb2391;Lysandre;2020-01-17 17:25:40 -0500;TF XLM

==

src/transformers/modeling_tf_xlm.py
src/transformers/modeling_tf_xlnet.py
==================
ccebcae75;Lysandre;2020-01-17 16:59:52 -0500;PyTorch XLM

==

docs/source/model_doc/xlm.rst
src/transformers/modeling_xlm.py
==================
92b3cb786;Lysandre;2020-01-17 15:40:07 -0500;TF XLNet

==

src/transformers/modeling_tf_xlnet.py
src/transformers/modeling_xlnet.py
==================
cd656fb21;Lysandre;2020-01-17 14:59:31 -0500;PyTorch XLNet

==

docs/source/model_doc/xlnet.rst
src/transformers/modeling_xlnet.py
==================
83fa8d9fb;Lysandre;2020-01-17 13:34:56 -0500;TF Transformer-XL

==

src/transformers/modeling_tf_transfo_xl.py
==================
98edad418;Lysandre;2020-01-17 12:02:48 -0500;PyTorch Transformer-XL

==

docs/source/model_doc/transformerxl.rst
src/transformers/modeling_transfo_xl.py
==================
96d21ad06;Lysandre;2020-01-17 11:22:20 -0500;TF OpenAI GPT

==

src/transformers/modeling_tf_bert.py
src/transformers/modeling_tf_gpt2.py
src/transformers/modeling_tf_openai.py
==================
850795c48;Lysandre;2020-01-17 10:50:25 -0500;Pytorch GPT

==

docs/source/model_doc/gpt.rst
src/transformers/modeling_gpt2.py
src/transformers/modeling_openai.py
==================
1487b840d;Lysandre;2020-01-17 10:28:12 -0500;TF GPT2

==

docs/source/model_doc/gpt2.rst
src/transformers/modeling_tf_bert.py
src/transformers/modeling_tf_gpt2.py
==================
bd0d3fd76;Lysandre;2020-01-16 17:16:26 -0500;GPT-2 PyTorch models + better tips for BERT

==

docs/source/model_doc/bert.rst
docs/source/model_doc/gpt2.rst
src/transformers/modeling_gpt2.py
==================
dbeb7fb4e;Lysandre;2020-01-16 15:51:54 -0500;BERT TensorFlow

==

src/transformers/modeling_tf_albert.py
src/transformers/modeling_tf_bert.py
==================
cd77c750c;Lysandre;2020-01-16 14:45:02 -0500;BERT PyTorch models

==

docs/source/model_doc/bert.rst
src/transformers/modeling_albert.py
src/transformers/modeling_bert.py
==================
3922a2497;Lysandre;2020-01-15 15:50:30 -0500;TF ALBERT + TF Utilities + Fix warnings

==

docs/source/main_classes/optimizer_schedules.rst
docs/source/model_doc/albert.rst
src/transformers/file_utils.py
src/transformers/modeling_albert.py
src/transformers/modeling_tf_albert.py
src/transformers/modeling_tf_utils.py
==================
00df3d4de;Lysandre;2020-01-15 14:20:17 -0500;ALBERT Modeling + required changes to utilities

==

docs/source/model_doc/albert.rst
src/transformers/file_utils.py
src/transformers/modeling_albert.py
src/transformers/modeling_utils.py
==================
f81b6c95f;Lysandre;2020-01-15 09:21:38 -0500;Flake8 violation

==

tests/test_examples.py
==================
632675ea8;Lysandre;2020-01-14 17:30:31 -0500;Can test examples spread over multiple blocks

==

docs/source/glossary.rst
tests/test_examples.py
==================
eaa6b9afc;Lysandre;2020-01-14 16:31:39 -0500;Require Torch when testing examples

==

tests/test_examples.py
==================
9bab9b83d;Lysandre;2020-01-14 16:31:29 -0500;Glossary

==

docs/source/glossary.rst
docs/source/index.rst
==================
64abd3e0a;Lysandre;2020-01-14 13:29:27 -0500;Multi-line examples can be tested + ALBERT patch for CircleCI
All tests should now work fine.
==

src/transformers/configuration_albert.py
tests/test_examples.py
==================
837577256;Lysandre;2020-01-14 12:28:24 -0500;Automatic testing of examples
The CircleCI test should fail.
==

tests/test_examples.py
==================
90b7df444;Julien Chaumond;2020-01-22 22:41:21 -0500;Upload CLI: on win32, use slashes, not os.sep

==

src/transformers/commands/user.py
==================
119dc50e2;Julien Chaumond;2020-01-22 22:40:38 -0500;Doc tweak on model sharing

==

README.md
docs/source/model_sharing.md
==================
34a3c25a3;Julien Chaumond;2020-01-22 17:50:24 -0500;Fix for XLMRobertaConfig inherits from RobertaConfig
hat/tip @stefan-it

==

src/transformers/modeling_auto.py
src/transformers/tokenization_auto.py
==================
1a8e87be4;Julien Chaumond;2020-01-18 03:52:49 +0000;Line-by-line text dataset (including padding)

==

examples/run_lm_finetuning.py
==================
b94cf7faa;Julien Chaumond;2020-01-18 03:19:06 +0000;change order

==

examples/run_lm_finetuning.py
==================
2eaa8b6e5;Julien Chaumond;2020-01-18 01:23:56 +0000;Easier to not support this, as it could be confusing
cc @lysandrejik

==

examples/run_lm_finetuning.py
==================
801aaa550;Julien Chaumond;2020-01-17 23:13:51 +0000;make style

==

examples/run_lm_finetuning.py
==================
56d4ba8dd;Julien Chaumond;2020-01-17 23:05:56 +0000;[run_lm_finetuning] Train from scratch

==

examples/run_lm_finetuning.py
==================
c7f79815e;Lysandre;2020-01-21 11:40:24 -0500;Cleanup unused variables

==

src/transformers/tokenization_xlnet.py
==================
15579e2d5;Lysandre;2020-01-21 11:36:46 -0500;[SQuAD v2] Code quality

==

src/transformers/data/processors/squad.py
==================
088fa7b75;Lysandre;2020-01-21 11:33:45 -0500;Correct segment ID for XLNet single sequence

==

src/transformers/tokenization_xlnet.py
==================
073219b43;Lysandre;2020-01-21 11:15:22 -0500;Manage impossible examples SQuAD v2

==

src/transformers/data/processors/squad.py
==================
983c484fa;Branden Chan;2020-01-06 12:54:39 +0100;add __getstate__ and __setstate__ to XLMRobertaTokenizer

==

src/transformers/tokenization_xlm_roberta.py
==================
cefd51c50;James Betker;2020-01-17 10:58:10 -0700;Fix glue processor failing on tf datasets

==

src/transformers/data/processors/glue.py
==================
ca6ce3040;Lysandre;2020-01-20 10:48:26 -0500;Fix style

==

src/transformers/commands/serving.py
src/transformers/file_utils.py
==================
908cd5ea2;Morgan Funtowicz;2020-01-10 20:59:04 +0100;Make forward asynchrone to avoid long computation timing out.
Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

==

src/transformers/commands/serving.py
src/transformers/file_utils.py
==================
6e6c8c52e;Morgan Funtowicz;2020-01-10 14:20:10 +0100;Fix bad handling of env variable USE_TF / USE_TORCH leading to invalid framework being used.
Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

==

src/transformers/file_utils.py
==================
23c6998bf;Brendan Roof;2020-01-17 13:49:28 -0800;Add lower bound to tqdm for tqdm.auto
- It appears that `tqdm` only introduced `tqdm.auto` in 4.27.
- See https://github.com/tqdm/tqdm/releases/tag/v4.27.0.
- Without the lower bound I received the following stack trace in an environment where I already had tqdm installed:
```
  File "/home/brendanr/anaconda3/envs/allennlp/lib/python3.6/site-packages/transformers/__init__.py", line 20, in <module>
    from .file_utils import (TRANSFORMERS_CACHE, PYTORCH_TRANSFORMERS_CACHE, PYTORCH_PRETRAINED_BERT_CACHE,
  File "/home/brendanr/anaconda3/envs/allennlp/lib/python3.6/site-packages/transformers/file_utils.py", line 24, in <module>
    from tqdm.auto import tqdm
ModuleNotFoundError: No module named 'tqdm.auto'
```
==

setup.py
==================
65a89a897;Mark Neumann;2020-01-17 11:57:56 -0800;Fix BasicTokenizer to respect `never_split` parameters (#2557)
* add failing test

* fix call to _run_split_on_punc

* format with black

==

src/transformers/tokenization_bert.py
tests/test_tokenization_bert.py
==================
6d5049a24;jiyeon_baek;2020-01-17 20:43:32 +0900;Fix typo in examples/run_squad.py
Rul -> Run
==

examples/run_squad.py
==================
23a2cea8c;Julien Chaumond;2020-01-15 21:07:26 +0000;Tokenizer.from_pretrained: fetch all possible files remotely

==

src/transformers/configuration_utils.py
src/transformers/file_utils.py
src/transformers/tokenization_utils.py
tests/test_tokenization_auto.py
==================
99f9243de;Julien Chaumond;2020-01-15 17:43:27 +0000;same here, try to not serialize too much if unneeded

==

src/transformers/tokenization_utils.py
==================
9d8fd2d40;Julien Chaumond;2020-01-15 17:36:52 +0000;tokenizer.save_pretrained: only save file if non-empty

==

src/transformers/configuration_auto.py
src/transformers/tokenization_utils.py
tests/test_tokenization_auto.py
==================
6e2c28a14;Lysandre;2020-01-16 13:59:26 -0500;Run SQuAD warning when the doc stride may be too high

==

examples/run_squad.py
==================
b8f43cb27;Thomas Wolf;2020-01-16 13:28:25 +0100;Merge pull request #2239 from ns-moosavi/HANS-evaluation-example
HANS evaluation
==
==================
258ed2eaa;thomwolf;2020-01-16 13:19:56 +0100;adding details in readme

==

examples/README.md
==================
50ee59578;thomwolf;2020-01-06 23:03:18 +0100;update formating - make flake8 happy

==

examples/hans/test_hans.py
examples/hans/utils_hans.py
==================
1c9333584;thomwolf;2020-01-06 22:55:51 +0100;formating

==

examples/hans/hans_processors.py
examples/hans/test_hans.py
examples/hans/utils_hans.py
==================
e25b6fe35;thomwolf;2020-01-06 14:22:07 +0100;updating readme

==

examples/README.md
==================
27c7b9901;thomwolf;2020-01-06 14:21:58 +0100;adding details in readme - moving file

==

examples/README.md
examples/hans/hans_processors.py
examples/hans/test_hans.py
examples/hans/utils_hans.py
==================
99d451557;Nafise Sadat Moosavi;2019-12-20 14:32:07 +0100;HANS evaluation

==

examples/hans_processors.py
examples/test_hans.py
examples/utils_hans.py
src/transformers/data/metrics/__init__.py
==================
dc17f2a11;Thomas Wolf;2020-01-16 13:17:15 +0100;Merge pull request #2538 from huggingface/py3_super
:lipstick: super
==
==================
880854846;Thomas Wolf;2020-01-16 13:16:59 +0100;Merge pull request #2540 from huggingface/torch14_fix
[PyTorch 1.4] Fix failing torchscript test for xlnet
==
==================
d9fa1bad7;Julien Chaumond;2020-01-15 20:22:21 -0500;Fix failing torchscript test for xlnet
model.parameters() order is apparently not stable (only for xlnet, for some reason)

==

tests/test_modeling_common.py
==================
a98b2ca8c;Julien Chaumond;2020-01-15 19:05:51 -0500;Style + fixup BertJapaneseTokenizer

==

src/transformers/modeling_tf_roberta.py
src/transformers/tokenization_bert_japanese.py
src/transformers/tokenization_gpt2.py
==================
83a41d39b;Julien Chaumond;2020-01-15 18:33:50 -0500;:lipstick: super

==

examples/mm-imdb/utils_mmimdb.py
examples/pplm/pplm_classification_head.py
examples/pplm/run_pplm_discrim_train.py
examples/summarization/configuration_bertabs.py
examples/summarization/modeling_bertabs.py
src/transformers/configuration_albert.py
src/transformers/configuration_bert.py
src/transformers/configuration_ctrl.py
src/transformers/configuration_distilbert.py
src/transformers/configuration_gpt2.py
src/transformers/configuration_openai.py
src/transformers/configuration_t5.py
src/transformers/configuration_transfo_xl.py
src/transformers/configuration_xlm.py
src/transformers/configuration_xlnet.py
src/transformers/modeling_albert.py
src/transformers/modeling_bert.py
src/transformers/modeling_ctrl.py
src/transformers/modeling_distilbert.py
src/transformers/modeling_encoder_decoder.py
src/transformers/modeling_gpt2.py
src/transformers/modeling_mmbt.py
src/transformers/modeling_openai.py
src/transformers/modeling_roberta.py
src/transformers/modeling_t5.py
src/transformers/modeling_tf_albert.py
src/transformers/modeling_tf_bert.py
src/transformers/modeling_tf_ctrl.py
src/transformers/modeling_tf_distilbert.py
src/transformers/modeling_tf_gpt2.py
src/transformers/modeling_tf_openai.py
src/transformers/modeling_tf_roberta.py
src/transformers/modeling_tf_t5.py
src/transformers/modeling_tf_transfo_xl.py
src/transformers/modeling_tf_transfo_xl_utilities.py
src/transformers/modeling_tf_utils.py
src/transformers/modeling_tf_xlm.py
src/transformers/modeling_tf_xlnet.py
src/transformers/modeling_transfo_xl.py
src/transformers/modeling_transfo_xl_utilities.py
src/transformers/modeling_utils.py
src/transformers/modeling_xlm.py
src/transformers/modeling_xlnet.py
src/transformers/optimization.py
src/transformers/optimization_tf.py
src/transformers/tokenization_albert.py
src/transformers/tokenization_bert.py
src/transformers/tokenization_bert_japanese.py
src/transformers/tokenization_camembert.py
src/transformers/tokenization_ctrl.py
src/transformers/tokenization_gpt2.py
src/transformers/tokenization_openai.py
src/transformers/tokenization_roberta.py
src/transformers/tokenization_t5.py
src/transformers/tokenization_transfo_xl.py
src/transformers/tokenization_utils.py
src/transformers/tokenization_xlm.py
src/transformers/tokenization_xlm_roberta.py
src/transformers/tokenization_xlnet.py
templates/adding_a_new_model/configuration_xxx.py
templates/adding_a_new_model/modeling_tf_xxx.py
templates/adding_a_new_model/modeling_xxx.py
templates/adding_a_new_model/tests/test_tokenization_xxx.py
templates/adding_a_new_model/tokenization_xxx.py
tests/test_tokenization_albert.py
tests/test_tokenization_bert.py
tests/test_tokenization_bert_japanese.py
tests/test_tokenization_ctrl.py
tests/test_tokenization_gpt2.py
tests/test_tokenization_openai.py
tests/test_tokenization_roberta.py
tests/test_tokenization_t5.py
tests/test_tokenization_transfo_xl.py
tests/test_tokenization_xlm.py
tests/test_tokenization_xlnet.py
==================
cd51893d3;Julien Chaumond;2020-01-15 18:25:15 -0500;Merge branch 'Rexhaif-patch-1'

==
==================
248aeaa84;Julien Chaumond;2020-01-15 18:22:01 -0500;Merge branch 'patch-1' of https://github.com/Rexhaif/transformers into Rexhaif-patch-1

==
==================
c76c3cebe;Aditya Bhargava;2020-01-08 07:25:39 -0500;Add check for token_type_ids before tensorizing
Fix an issue where `prepare_for_model()` gives a `KeyError` when
`return_token_type_ids` is set to `False` and `return_tensors` is
enabled.

==

src/transformers/tokenization_utils.py
==================
eb59e9f70;Julien Chaumond;2020-01-15 16:28:50 +0000;Graduate sst-2 to a canonical one

==

src/transformers/configuration_distilbert.py
src/transformers/modeling_distilbert.py
src/transformers/modeling_tf_distilbert.py
src/transformers/pipelines.py
tests/test_pipelines.py
==================
e184ad13c;Julien Chaumond;2020-01-15 15:43:44 +0000;Close #2392

==

src/transformers/pipelines.py
tests/test_pipelines.py
==================
dfe012ad9;Lysandre;2020-01-07 13:44:56 +0100;Fix misleading RoBERTa token type ids

==

src/transformers/tokenization_roberta.py
==================
c024ab98d;Lysandre;2020-01-14 17:44:23 -0500;Improve padding side documentation

==

src/transformers/tokenization_utils.py
==================
9aeb0b9b8;Lysandre;2020-01-14 17:43:00 -0500;Improve padding side documentation

==

src/transformers/tokenization_utils.py
==================
715fa638a;Julien Chaumond;2020-01-14 18:58:21 +0000;Merge branch 'master' into from_scratch_training

==
==================
100e3b6f2;Lysandre;2020-01-14 10:13:31 -0500;Bias should be resized with the weights
Created a link between the linear layer bias and the model attribute bias. This does not change anything for the user nor for the conversion scripts, but allows the `resize_token_embeddings` method to resize the bias as well as the weights of the decoder.

Added a test.
==

src/transformers/modeling_albert.py
src/transformers/modeling_bert.py
src/transformers/modeling_roberta.py
tests/test_modeling_common.py
==================
6c32d8bb9;Lysandre;2020-01-14 08:07:57 -0500;Size > Dimensionality + Remove final TODOs

==

src/transformers/configuration_albert.py
src/transformers/configuration_bert.py
src/transformers/configuration_ctrl.py
src/transformers/configuration_distilbert.py
src/transformers/configuration_gpt2.py
src/transformers/configuration_openai.py
src/transformers/configuration_xlm.py
src/transformers/configuration_xlnet.py
==================
760164d63;Lysandre;2020-01-13 13:36:42 +0100;RoBERTa example

==

src/transformers/configuration_roberta.py
==================
387217bd3;Lysandre;2020-01-13 13:27:34 +0100;Added example usage

==

docs/source/model_doc/xlnet.rst
src/transformers/configuration_bert.py
src/transformers/configuration_camembert.py
src/transformers/configuration_ctrl.py
src/transformers/configuration_distilbert.py
src/transformers/configuration_gpt2.py
src/transformers/configuration_openai.py
src/transformers/configuration_transfo_xl.py
src/transformers/configuration_xlm.py
src/transformers/configuration_xlnet.py
==================
7d1bb7f25;Lysandre;2020-01-13 13:13:11 +0100;Add missing XLNet and XLM models

==

docs/source/model_doc/xlm.rst
docs/source/model_doc/xlnet.rst
==================
a1cb10046;Lysandre;2020-01-13 13:11:37 +0100;Wrap up configurations

==

src/transformers/configuration_gpt2.py
src/transformers/configuration_openai.py
src/transformers/configuration_xlm.py
src/transformers/configuration_xlnet.py
==================
c11b6fd39;Lysandre;2020-01-13 12:43:55 +0100;Update links in all configurations

==

src/transformers/configuration_albert.py
src/transformers/configuration_bert.py
src/transformers/configuration_camembert.py
src/transformers/configuration_ctrl.py
src/transformers/configuration_distilbert.py
src/transformers/configuration_gpt2.py
src/transformers/configuration_openai.py
src/transformers/configuration_roberta.py
src/transformers/configuration_transfo_xl.py
==================
632682726;Lysandre Debut;2020-01-12 21:53:19 +0100;Updated Configurations

==

docs/source/model_doc/albert.rst
src/transformers/configuration_albert.py
src/transformers/configuration_auto.py
src/transformers/configuration_bert.py
src/transformers/configuration_camembert.py
src/transformers/configuration_ctrl.py
src/transformers/configuration_distilbert.py
src/transformers/configuration_gpt2.py
src/transformers/configuration_mmbt.py
src/transformers/configuration_openai.py
src/transformers/configuration_roberta.py
src/transformers/configuration_transfo_xl.py
src/transformers/configuration_xlm.py
src/transformers/configuration_xlnet.py
==================
2b566c182;Thomas Wolf;2020-01-14 13:19:01 +0100;Merge pull request #2384 from dimagalat/master
Releasing file lock
==
==================
764f836d5;Julien Chaumond;2020-01-13 22:50:34 -0500;Update test_tokenization_auto.py

==

tests/test_tokenization_auto.py
==================
d5831acb0;Julien Chaumond;2020-01-13 22:47:33 -0500;Update test_tokenization_auto.py

==

tests/test_tokenization_auto.py
==================
ed6cd597c;Julien Chaumond;2020-01-13 22:46:35 -0500;Update test_tokenization_auto.py

==

tests/test_tokenization_auto.py
==================
5cb463a71;Julien Chaumond;2020-01-13 22:38:29 -0500;Update test_tokenization_auto.py

==

tests/test_tokenization_auto.py
==================
afc24ea5d;Julien Chaumond;2020-01-13 23:44:08 +0000;In a parallel setup this could fail

==

src/transformers/file_utils.py
==================
894812c65;Julien Chaumond;2020-01-13 23:34:19 +0000;Fixup mapping

==

src/transformers/tokenization_auto.py
==================
b20f11d4c;Julien Chaumond;2020-01-13 23:20:44 +0000;üî´ Python35

==

src/transformers/modeling_auto.py
src/transformers/modeling_tf_auto.py
src/transformers/tokenization_auto.py
==================
030462859;Julien Chaumond;2020-01-13 23:11:44 +0000;Map configs to models and tokenizers

==

src/transformers/configuration_auto.py
src/transformers/configuration_utils.py
src/transformers/modeling_auto.py
src/transformers/modeling_tf_auto.py
src/transformers/tokenization_auto.py
tests/test_configuration_auto.py
==================
1fc855e45;Julien Chaumond;2020-01-13 21:52:55 +0000;[tests] Safety checks on CONFIG_MAPPING

==

tests/test_configuration_auto.py
==================
3c86b6f3c;Julien Chaumond;2020-01-13 20:44:33 +0000;Py35 doesn't like inline variable types

==

setup.cfg
src/transformers/configuration_utils.py
==================
b803b067b;Julien Chaumond;2020-01-13 20:05:20 +0000;Config to Model mapping

==

examples/summarization/configuration_bertabs.py
src/transformers/configuration_albert.py
src/transformers/configuration_auto.py
src/transformers/configuration_bert.py
src/transformers/configuration_camembert.py
src/transformers/configuration_ctrl.py
src/transformers/configuration_distilbert.py
src/transformers/configuration_gpt2.py
src/transformers/configuration_openai.py
src/transformers/configuration_t5.py
src/transformers/configuration_transfo_xl.py
src/transformers/configuration_utils.py
src/transformers/configuration_xlm.py
src/transformers/configuration_xlm_roberta.py
src/transformers/configuration_xlnet.py
src/transformers/modeling_auto.py
templates/adding_a_new_model/configuration_xxx.py
==================
896a0eb1f;Thomas Wolf;2020-01-13 16:02:54 +0100;Merge pull request #2459 from Perseus14/patch-4
Update pipelines.py
==
==================
0d6c17fc1;Morgan Funtowicz;2020-01-13 11:18:27 +0100;black formatting

==

src/transformers/pipelines.py
==================
a3085020e;IWillPull;2020-01-11 06:00:07 +0200;Added repetition penalty to PPLM example (#2436)
* Added repetition penalty

* Default PPLM repetition_penalty to neutral

* Minor modifications to comply with reviewer's suggestions. (j -> token_idx)

* Formatted code with `make style`

==

examples/pplm/run_pplm.py
==================
cf8a70bf6;Julien Chaumond;2020-01-11 03:43:57 +0000;More AutoConfig tests

==

tests/test_configuration_auto.py
tests/test_modeling_auto.py
tests/test_modeling_tf_auto.py
tests/test_tokenization_auto.py
tests/utils.py
==================
6bb3edc30;Julien Chaumond;2020-01-11 03:18:56 +0000;Serialize model_type if exists

==

src/transformers/configuration_roberta.py
src/transformers/configuration_utils.py
==================
c6f682c1e;Julien Chaumond;2020-01-11 03:18:31 +0000;flake

==

tests/test_modeling_common.py
tests/test_tokenization_auto.py
==================
4d1c98c01;Julien Chaumond;2020-01-11 02:46:17 +0000;AutoConfig + other Auto classes honor model_type

==

src/transformers/configuration_auto.py
src/transformers/configuration_utils.py
src/transformers/modeling_auto.py
src/transformers/modeling_tf_auto.py
src/transformers/tokenization_auto.py
tests/fixtures/dummy-config.json
tests/test_configuration_auto.py
tests/test_modeling_auto.py
tests/test_tokenization_auto.py
==================
2f32dfd33;Julien Chaumond;2020-01-11 01:24:29 +0000;Convention: name mixins mixins

==

src/transformers/modeling_tf_utils.py
src/transformers/modeling_utils.py
tests/test_modeling_common.py
==================
e83d9f1c1;VictorSanh;2020-01-10 19:34:25 -0500;cleaning - change ' to " (black requirements)

==

examples/distillation/lm_seqs_dataset.py
==================
ebba9e929;VictorSanh;2020-01-10 19:14:58 -0500;minor spring cleaning - missing configs + processing

==

examples/distillation/lm_seqs_dataset.py
examples/distillation/training_configs/distilbert-base-multilingual-cased.json
examples/distillation/training_configs/distilroberta-base.json
==================
055e80cfa;Julien Chaumond;2020-01-10 21:36:18 +0000;rm old ConfigTester

==

tests/test_modeling_common.py
==================
b1e1a9f9b;Thomas Wolf;2020-01-10 22:18:54 +0100;Merge pull request #2495 from mschrimpf/patch-1
T5: move rp_bucket to relative_attention_bias' device
==
==================
fd8423321;Julien Chaumond;2020-01-10 20:36:46 +0000;keep list sorted

==

setup.cfg
==================
0cd81fb99;Julien Chaumond;2020-01-10 20:35:45 +0000;[isort] declare more third-parties in case no tf install

==

setup.cfg
==================
90d3b787f;Martin Schrimpf;2020-01-10 15:09:10 -0500;move rp_bucket to relative_attention_bias' device
otherwise, `rp_bucket` will always be on cpu and fail if `self.relative_attention_bias` is on cuda
==

src/transformers/modeling_t5.py
==================
84c0aa186;Julien Chaumond;2020-01-10 17:40:02 +0000;num_parameters helper

==

src/transformers/modeling_tf_utils.py
src/transformers/modeling_utils.py
tests/test_modeling_auto.py
tests/test_modeling_tf_auto.py
==================
331065e62;Victor SANH;2020-01-08 18:03:11 -0500;missing import

==

examples/distillation/run_squad_w_distillation.py
==================
414e9e712;Victor SANH;2020-01-08 17:56:42 -0500;indents test

==

examples/distillation/run_squad_w_distillation.py
==================
3cdb38a7c;Victor SANH;2020-01-08 17:19:05 -0500;indents

==

examples/distillation/run_squad_w_distillation.py
==================
ebd45980a;Victor SANH;2020-01-08 17:16:19 -0500;Align with `run_squad` + fix some errors

==

examples/distillation/run_squad_w_distillation.py
==================
45634f87f;Victor SANH;2020-01-08 15:50:42 -0500;fix Sampler in distributed training - evaluation

==

examples/distillation/run_squad_w_distillation.py
==================
af1ee9e64;Victor SANH;2020-01-08 15:47:53 -0500;Move `torch.nn.utils.clip_grad_norm_`

==

examples/distillation/run_squad_w_distillation.py
==================
164c794eb;Lysandre;2020-01-08 16:33:23 +0100;New SQuAD API for distillation script

==

examples/distillation/run_squad_w_distillation.py
==================
801f2ac8c;Lysandre;2020-01-09 12:14:30 +0100;Add PRETRAINED_INIT_CONFIGURATION to DistilBERT tokenizer

==

src/transformers/tokenization_distilbert.py
==================
bfec203d4;Yohei Tamura;2020-01-09 14:00:32 +0900;modified:   src/transformers/tokenization_utils.py

==

src/transformers/tokenization_utils.py
==================
f599623a9;Julien Chaumond;2020-01-08 15:46:37 -0500;PreTrainedTokenizerFast: hotfix _convert_encoding
cc @n1t0

==

src/transformers/tokenization_utils.py
==================
f26a35305;Rishabh Manoj;2020-01-08 21:12:34 +0530;Update pipelines.py
Modified QA pipeline to consider all features for each example before generating topk answers. 
Current pipeline only takes one SquadExample, one SquadFeature, one start logit list, one end logit list to retrieve the answer, this is not correct as one SquadExample can produce multiple SquadFeatures.
==

src/transformers/pipelines.py
==================
16ce15ed4;Lysandre;2020-01-08 13:18:30 +0100;DistilBERT token type ids removed from inputs in run_squad

==

examples/run_squad.py
==================
f24232cd1;Lysandre Debut;2020-01-08 11:39:00 +0100;Fix error with global step in run_squad.py

==

examples/run_squad.py
==================
1b59b57b5;thomwolf;2020-01-08 09:52:10 +0100;ignore_index equal -100 in T5 model

==

src/transformers/modeling_t5.py
==================
569da80ce;Romain Keramitas;2020-01-07 10:09:56 +0100;Make doc regarding masked indices more clear.
Signed-off-by: Romain Keramitas <r.keramitas@gmail.com>

==

src/transformers/modeling_albert.py
src/transformers/modeling_bert.py
src/transformers/modeling_camembert.py
src/transformers/modeling_ctrl.py
src/transformers/modeling_distilbert.py
src/transformers/modeling_gpt2.py
src/transformers/modeling_openai.py
src/transformers/modeling_roberta.py
==================
43114b89b;Oren Amsalem;2020-01-07 18:25:25 +0200;spelling correction (#2434)

==

examples/run_lm_finetuning.py
==================
d6a677b14;Genta Indra Winata;2020-01-08 00:21:23 +0800;Fix typograpical errors (#2438)

==

src/transformers/modeling_albert.py
src/transformers/modeling_bert.py
src/transformers/modeling_ctrl.py
src/transformers/modeling_gpt2.py
src/transformers/modeling_openai.py
src/transformers/modeling_t5.py
src/transformers/modeling_tf_albert.py
src/transformers/modeling_tf_bert.py
src/transformers/modeling_tf_ctrl.py
src/transformers/modeling_tf_gpt2.py
src/transformers/modeling_tf_openai.py
src/transformers/modeling_tf_roberta.py
src/transformers/modeling_tf_t5.py
src/transformers/modeling_tf_transfo_xl.py
src/transformers/modeling_tf_xlm.py
src/transformers/modeling_tf_xlnet.py
src/transformers/modeling_transfo_xl.py
src/transformers/modeling_xlm.py
src/transformers/modeling_xlnet.py
src/transformers/tokenization_utils.py
templates/adding_a_new_model/modeling_tf_xxx.py
templates/adding_a_new_model/modeling_xxx.py
==================
27c1b656c;Lysandre Debut;2020-01-07 16:16:12 +0100;Fix error with global step in run_lm_finetuning.py

==

examples/run_lm_finetuning.py
==================
24df44d9c;Lysandre;2020-01-07 15:53:42 +0100;Black version python 3.5

==

src/transformers/tokenization_xlm.py
==================
73be60c47;Lysandre Debut;2020-01-07 15:34:23 +0100;Quotes

==

src/transformers/tokenization_xlm.py
==================
6806f8204;Lysandre;2020-01-07 15:20:45 +0100;fix #2410

==

src/transformers/tokenization_xlm.py
==================
176d3b307;Simone Primarosa;2020-01-07 14:55:55 +0100;Add support for Albert and XLMRoberta for the Glue example (#2403)
* Add support for Albert and XLMRoberta for the Glue example

==

examples/run_glue.py
==================
9261c7f77;Morgan Funtowicz;2020-01-07 11:46:44 +0100;Remove f-string device creation on PyTorch GPU pipelines.
Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

==

src/transformers/pipelines.py
==================
91d33c798;Morgan Funtowicz;2020-01-06 15:29:52 +0100;Fix issue on pipelines where pytorch's tensors are not copied on the user-specified GPU device.
Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

==

src/transformers/pipelines.py
==================
2926852f1;Dima Galat;2020-01-07 11:56:03 +1100;fixed formatting

==

src/transformers/file_utils.py
==================
e2810edc8;Dima Galat;2020-01-07 11:47:25 +1100;removing redundant .flush

==

src/transformers/file_utils.py
==================
c301faa92;Julien Chaumond;2020-01-06 18:41:08 -0500;Distributed or parallel setup

==

.github/ISSUE_TEMPLATE/bug-report.md
.github/ISSUE_TEMPLATE/migration.md
==================
81d6841b4;alberduris;2019-12-31 13:24:54 +0100;GPU text generation: mMoved the encoded_prompt to correct device

==

.circleci/config.yml
.coveragerc
.github/ISSUE_TEMPLATE/---new-benchmark.md
.github/ISSUE_TEMPLATE/--new-model-addition.md
.github/ISSUE_TEMPLATE/bug-report.md
.github/ISSUE_TEMPLATE/feature-request.md
.github/ISSUE_TEMPLATE/migration.md
.github/ISSUE_TEMPLATE/question-help.md
.github/stale.yml
.gitignore
CONTRIBUTING.md
LICENSE
MANIFEST.in
Makefile
README.md
deploy_multi_version_doc.sh
docker/Dockerfile
docs/Makefile
docs/README.md
docs/source/_static/css/Calibre-Light.ttf
docs/source/_static/css/Calibre-Medium.otf
docs/source/_static/css/Calibre-Regular.otf
docs/source/_static/css/Calibre-Thin.otf
docs/source/_static/css/code-snippets.css
docs/source/_static/css/huggingface.css
docs/source/_static/js/custom.js
docs/source/_static/js/huggingface_logo.svg
docs/source/benchmarks.md
docs/source/bertology.rst
docs/source/conf.py
docs/source/converting_tensorflow_models.rst
docs/source/imgs/transformers_logo_name.png
docs/source/imgs/warmup_constant_schedule.png
docs/source/imgs/warmup_cosine_hard_restarts_schedule.png
docs/source/imgs/warmup_cosine_schedule.png
docs/source/imgs/warmup_cosine_warm_restarts_schedule.png
docs/source/imgs/warmup_linear_schedule.png
docs/source/index.rst
docs/source/installation.md
docs/source/main_classes/configuration.rst
docs/source/main_classes/model.rst
docs/source/main_classes/optimizer_schedules.rst
docs/source/main_classes/processors.rst
docs/source/main_classes/tokenizer.rst
docs/source/migration.md
docs/source/model_doc/albert.rst
docs/source/model_doc/auto.rst
docs/source/model_doc/bert.rst
docs/source/model_doc/camembert.rst
docs/source/model_doc/ctrl.rst
docs/source/model_doc/distilbert.rst
docs/source/model_doc/gpt.rst
docs/source/model_doc/gpt2.rst
docs/source/model_doc/roberta.rst
docs/source/model_doc/transformerxl.rst
docs/source/model_doc/xlm.rst
docs/source/model_doc/xlnet.rst
docs/source/model_sharing.md
docs/source/multilingual.rst
docs/source/notebooks.rst
docs/source/pretrained_models.rst
docs/source/quickstart.md
docs/source/serialization.rst
docs/source/torchscript.rst
examples/README.md
examples/benchmarks.py
examples/contrib/README.md
examples/contrib/run_camembert.py
examples/contrib/run_openai_gpt.py
examples/contrib/run_swag.py
examples/contrib/run_transfo_xl.py
examples/distillation/README.md
examples/distillation/distiller.py
examples/distillation/grouped_batch_sampler.py
examples/distillation/lm_seqs_dataset.py
examples/distillation/requirements.txt
examples/distillation/run_squad_w_distillation.py
examples/distillation/scripts/binarized_data.py
examples/distillation/scripts/extract.py
examples/distillation/scripts/extract_distilbert.py
examples/distillation/scripts/token_counts.py
examples/distillation/train.py
examples/distillation/training_configs/distilbert-base-uncased.json
examples/distillation/training_configs/distilgpt2.json
examples/distillation/utils.py
examples/mm-imdb/run_mmimdb.py
examples/mm-imdb/utils_mmimdb.py
examples/pplm/README.md
examples/pplm/imgs/headfigure.png
examples/pplm/imgs/wooly.png
examples/pplm/pplm_classification_head.py
examples/pplm/run_pplm.py
examples/pplm/run_pplm_discrim_train.py
examples/requirements.txt
examples/run_bertology.py
examples/run_generation.py
examples/run_glue.py
examples/run_lm_finetuning.py
examples/run_multiple_choice.py
examples/run_ner.py
examples/run_squad.py
examples/run_tf_glue.py
examples/run_tf_ner.py
examples/run_xnli.py
examples/summarization/README.md
examples/summarization/configuration_bertabs.py
examples/summarization/convert_bertabs_original_pytorch_checkpoint.py
examples/summarization/modeling_bertabs.py
examples/summarization/requirements.txt
examples/summarization/run_summarization.py
examples/summarization/test_utils_summarization.py
examples/summarization/utils_summarization.py
examples/test_examples.py
examples/tests_samples/.gitignore
examples/tests_samples/MRPC/dev.tsv
examples/tests_samples/MRPC/train.tsv
examples/tests_samples/SQUAD/dev-v2.0.json
examples/tests_samples/SQUAD/train-v2.0.json
examples/utils_multiple_choice.py
examples/utils_ner.py
hubconf.py
notebooks/Comparing-PT-and-TF-models.ipynb
notebooks/Comparing-TF-and-PT-models-MLM-NSP.ipynb
notebooks/Comparing-TF-and-PT-models-SQuAD.ipynb
notebooks/Comparing-TF-and-PT-models.ipynb
setup.cfg
setup.py
src/transformers/commands/__init__.py
src/transformers/commands/convert.py
src/transformers/commands/download.py
src/transformers/commands/run.py
src/transformers/commands/serving.py
src/transformers/commands/train.py
src/transformers/commands/user.py
src/transformers/configuration_albert.py
src/transformers/configuration_auto.py
src/transformers/configuration_bert.py
src/transformers/configuration_camembert.py
src/transformers/configuration_ctrl.py
src/transformers/configuration_distilbert.py
src/transformers/configuration_gpt2.py
src/transformers/configuration_mmbt.py
src/transformers/configuration_openai.py
src/transformers/configuration_roberta.py
src/transformers/configuration_t5.py
src/transformers/configuration_transfo_xl.py
src/transformers/configuration_utils.py
src/transformers/configuration_xlm.py
src/transformers/configuration_xlm_roberta.py
src/transformers/configuration_xlnet.py
src/transformers/convert_albert_original_tf_checkpoint_to_pytorch.py
src/transformers/convert_bert_pytorch_checkpoint_to_original_tf.py
src/transformers/convert_pytorch_checkpoint_to_tf2.py
src/transformers/convert_roberta_original_pytorch_checkpoint_to_pytorch.py
src/transformers/data/__init__.py
src/transformers/data/metrics/__init__.py
src/transformers/data/metrics/squad_metrics.py
src/transformers/data/processors/__init__.py
src/transformers/data/processors/glue.py
src/transformers/data/processors/squad.py
src/transformers/data/processors/utils.py
src/transformers/data/processors/xnli.py
src/transformers/file_utils.py
src/transformers/hf_api.py
src/transformers/modelcard.py
src/transformers/modeling_albert.py
src/transformers/modeling_auto.py
src/transformers/modeling_bert.py
src/transformers/modeling_camembert.py
src/transformers/modeling_ctrl.py
src/transformers/modeling_distilbert.py
src/transformers/modeling_encoder_decoder.py
src/transformers/modeling_gpt2.py
src/transformers/modeling_mmbt.py
src/transformers/modeling_openai.py
src/transformers/modeling_roberta.py
src/transformers/modeling_t5.py
src/transformers/modeling_tf_albert.py
src/transformers/modeling_tf_auto.py
src/transformers/modeling_tf_bert.py
src/transformers/modeling_tf_ctrl.py
src/transformers/modeling_tf_distilbert.py
src/transformers/modeling_tf_gpt2.py
src/transformers/modeling_tf_openai.py
src/transformers/modeling_tf_pytorch_utils.py
src/transformers/modeling_tf_roberta.py
src/transformers/modeling_tf_t5.py
src/transformers/modeling_tf_transfo_xl.py
src/transformers/modeling_tf_transfo_xl_utilities.py
src/transformers/modeling_tf_utils.py
src/transformers/modeling_tf_xlm.py
src/transformers/modeling_tf_xlnet.py
src/transformers/modeling_transfo_xl.py
src/transformers/modeling_transfo_xl_utilities.py
src/transformers/modeling_utils.py
src/transformers/modeling_xlm.py
src/transformers/modeling_xlm_roberta.py
src/transformers/modeling_xlnet.py
src/transformers/optimization.py
src/transformers/optimization_tf.py
src/transformers/tokenization_albert.py
src/transformers/tokenization_auto.py
src/transformers/tokenization_bert.py
src/transformers/tokenization_bert_japanese.py
src/transformers/tokenization_camembert.py
src/transformers/tokenization_ctrl.py
src/transformers/tokenization_distilbert.py
src/transformers/tokenization_gpt2.py
src/transformers/tokenization_openai.py
src/transformers/tokenization_roberta.py
src/transformers/tokenization_t5.py
src/transformers/tokenization_transfo_xl.py
src/transformers/tokenization_utils.py
src/transformers/tokenization_xlm.py
src/transformers/tokenization_xlm_roberta.py
src/transformers/tokenization_xlnet.py
templates/adding_a_new_example_script/README.md
templates/adding_a_new_example_script/run_xxx.py
templates/adding_a_new_example_script/utils_xxx.py
templates/adding_a_new_model/README.md
templates/adding_a_new_model/configuration_xxx.py
templates/adding_a_new_model/modeling_tf_xxx.py
templates/adding_a_new_model/modeling_xxx.py
templates/adding_a_new_model/tests/test_modeling_tf_xxx.py
templates/adding_a_new_model/tests/test_modeling_xxx.py
templates/adding_a_new_model/tests/test_tokenization_xxx.py
templates/adding_a_new_model/tokenization_xxx.py
tests/__init__.py
tests/fixtures/empty.txt
tests/fixtures/input.txt
tests/fixtures/sample_text.txt
tests/fixtures/spiece.model
tests/fixtures/test_sentencepiece.model
tests/test_configuration_common.py
tests/test_hf_api.py
tests/test_model_card.py
tests/test_modeling_albert.py
tests/test_modeling_auto.py
tests/test_modeling_bert.py
tests/test_modeling_common.py
tests/test_modeling_ctrl.py
tests/test_modeling_distilbert.py
tests/test_modeling_encoder_decoder.py
tests/test_modeling_gpt2.py
tests/test_modeling_openai.py
tests/test_modeling_roberta.py
tests/test_modeling_t5.py
tests/test_modeling_tf_albert.py
tests/test_modeling_tf_auto.py
tests/test_modeling_tf_bert.py
tests/test_modeling_tf_common.py
tests/test_modeling_tf_ctrl.py
tests/test_modeling_tf_distilbert.py
tests/test_modeling_tf_gpt2.py
tests/test_modeling_tf_openai_gpt.py
tests/test_modeling_tf_roberta.py
tests/test_modeling_tf_t5.py
tests/test_modeling_tf_transfo_xl.py
tests/test_modeling_tf_xlm.py
tests/test_modeling_tf_xlnet.py
tests/test_modeling_transfo_xl.py
tests/test_modeling_xlm.py
tests/test_modeling_xlnet.py
tests/test_optimization.py
tests/test_optimization_tf.py
tests/test_pipelines.py
tests/test_tokenization_albert.py
tests/test_tokenization_auto.py
tests/test_tokenization_bert.py
tests/test_tokenization_bert_japanese.py
tests/test_tokenization_common.py
tests/test_tokenization_ctrl.py
tests/test_tokenization_distilbert.py
tests/test_tokenization_gpt2.py
tests/test_tokenization_openai.py
tests/test_tokenization_roberta.py
tests/test_tokenization_t5.py
tests/test_tokenization_transfo_xl.py
tests/test_tokenization_utils.py
tests/test_tokenization_xlm.py
tests/test_tokenization_xlnet.py
tests/utils.py
utils/download_glue_data.py
utils/link_tester.py
valohai.yaml
==================
dd4df80f0;alberduris;2019-12-31 13:11:41 +0100;Moved the encoded_prompts to correct device

==

.circleci/config.yml
.coveragerc
.github/ISSUE_TEMPLATE/---new-benchmark.md
.github/ISSUE_TEMPLATE/--new-model-addition.md
.github/ISSUE_TEMPLATE/bug-report.md
.github/ISSUE_TEMPLATE/feature-request.md
.github/ISSUE_TEMPLATE/migration.md
.github/ISSUE_TEMPLATE/question-help.md
.github/stale.yml
.gitignore
CONTRIBUTING.md
LICENSE
MANIFEST.in
Makefile
README.md
deploy_multi_version_doc.sh
docker/Dockerfile
docs/Makefile
docs/README.md
docs/source/_static/css/Calibre-Light.ttf
docs/source/_static/css/Calibre-Medium.otf
docs/source/_static/css/Calibre-Regular.otf
docs/source/_static/css/Calibre-Thin.otf
docs/source/_static/css/code-snippets.css
docs/source/_static/css/huggingface.css
docs/source/_static/js/custom.js
docs/source/_static/js/huggingface_logo.svg
docs/source/benchmarks.md
docs/source/bertology.rst
docs/source/conf.py
docs/source/converting_tensorflow_models.rst
docs/source/imgs/transformers_logo_name.png
docs/source/imgs/warmup_constant_schedule.png
docs/source/imgs/warmup_cosine_hard_restarts_schedule.png
docs/source/imgs/warmup_cosine_schedule.png
docs/source/imgs/warmup_cosine_warm_restarts_schedule.png
docs/source/imgs/warmup_linear_schedule.png
docs/source/index.rst
docs/source/installation.md
docs/source/main_classes/configuration.rst
docs/source/main_classes/model.rst
docs/source/main_classes/optimizer_schedules.rst
docs/source/main_classes/processors.rst
docs/source/main_classes/tokenizer.rst
docs/source/migration.md
docs/source/model_doc/albert.rst
docs/source/model_doc/auto.rst
docs/source/model_doc/bert.rst
docs/source/model_doc/camembert.rst
docs/source/model_doc/ctrl.rst
docs/source/model_doc/distilbert.rst
docs/source/model_doc/gpt.rst
docs/source/model_doc/gpt2.rst
docs/source/model_doc/roberta.rst
docs/source/model_doc/transformerxl.rst
docs/source/model_doc/xlm.rst
docs/source/model_doc/xlnet.rst
docs/source/model_sharing.md
docs/source/multilingual.rst
docs/source/notebooks.rst
docs/source/pretrained_models.rst
docs/source/quickstart.md
docs/source/serialization.rst
docs/source/torchscript.rst
examples/README.md
examples/benchmarks.py
examples/contrib/README.md
examples/contrib/run_camembert.py
examples/contrib/run_openai_gpt.py
examples/contrib/run_swag.py
examples/contrib/run_transfo_xl.py
examples/distillation/README.md
examples/distillation/distiller.py
examples/distillation/grouped_batch_sampler.py
examples/distillation/lm_seqs_dataset.py
examples/distillation/requirements.txt
examples/distillation/run_squad_w_distillation.py
examples/distillation/scripts/binarized_data.py
examples/distillation/scripts/extract.py
examples/distillation/scripts/extract_distilbert.py
examples/distillation/scripts/token_counts.py
examples/distillation/train.py
examples/distillation/training_configs/distilbert-base-uncased.json
examples/distillation/training_configs/distilgpt2.json
examples/distillation/utils.py
examples/mm-imdb/run_mmimdb.py
examples/mm-imdb/utils_mmimdb.py
examples/pplm/README.md
examples/pplm/imgs/headfigure.png
examples/pplm/imgs/wooly.png
examples/pplm/pplm_classification_head.py
examples/pplm/run_pplm.py
examples/pplm/run_pplm_discrim_train.py
examples/requirements.txt
examples/run_bertology.py
examples/run_generation.py
examples/run_glue.py
examples/run_lm_finetuning.py
examples/run_multiple_choice.py
examples/run_ner.py
examples/run_squad.py
examples/run_tf_glue.py
examples/run_tf_ner.py
examples/run_xnli.py
examples/summarization/README.md
examples/summarization/configuration_bertabs.py
examples/summarization/convert_bertabs_original_pytorch_checkpoint.py
examples/summarization/modeling_bertabs.py
examples/summarization/requirements.txt
examples/summarization/run_summarization.py
examples/summarization/test_utils_summarization.py
examples/summarization/utils_summarization.py
examples/test_examples.py
examples/tests_samples/.gitignore
examples/tests_samples/MRPC/dev.tsv
examples/tests_samples/MRPC/train.tsv
examples/tests_samples/SQUAD/dev-v2.0.json
examples/tests_samples/SQUAD/train-v2.0.json
examples/utils_multiple_choice.py
examples/utils_ner.py
hubconf.py
notebooks/Comparing-PT-and-TF-models.ipynb
notebooks/Comparing-TF-and-PT-models-MLM-NSP.ipynb
notebooks/Comparing-TF-and-PT-models-SQuAD.ipynb
notebooks/Comparing-TF-and-PT-models.ipynb
setup.cfg
setup.py
src/transformers/commands/__init__.py
src/transformers/commands/convert.py
src/transformers/commands/download.py
src/transformers/commands/run.py
src/transformers/commands/serving.py
src/transformers/commands/train.py
src/transformers/commands/user.py
src/transformers/configuration_albert.py
src/transformers/configuration_auto.py
src/transformers/configuration_bert.py
src/transformers/configuration_camembert.py
src/transformers/configuration_ctrl.py
src/transformers/configuration_distilbert.py
src/transformers/configuration_gpt2.py
src/transformers/configuration_mmbt.py
src/transformers/configuration_openai.py
src/transformers/configuration_roberta.py
src/transformers/configuration_t5.py
src/transformers/configuration_transfo_xl.py
src/transformers/configuration_utils.py
src/transformers/configuration_xlm.py
src/transformers/configuration_xlm_roberta.py
src/transformers/configuration_xlnet.py
src/transformers/convert_albert_original_tf_checkpoint_to_pytorch.py
src/transformers/convert_bert_pytorch_checkpoint_to_original_tf.py
src/transformers/convert_pytorch_checkpoint_to_tf2.py
src/transformers/convert_roberta_original_pytorch_checkpoint_to_pytorch.py
src/transformers/data/__init__.py
src/transformers/data/metrics/__init__.py
src/transformers/data/metrics/squad_metrics.py
src/transformers/data/processors/__init__.py
src/transformers/data/processors/glue.py
src/transformers/data/processors/squad.py
src/transformers/data/processors/utils.py
src/transformers/data/processors/xnli.py
src/transformers/file_utils.py
src/transformers/hf_api.py
src/transformers/modelcard.py
src/transformers/modeling_albert.py
src/transformers/modeling_auto.py
src/transformers/modeling_bert.py
src/transformers/modeling_camembert.py
src/transformers/modeling_ctrl.py
src/transformers/modeling_distilbert.py
src/transformers/modeling_encoder_decoder.py
src/transformers/modeling_gpt2.py
src/transformers/modeling_mmbt.py
src/transformers/modeling_openai.py
src/transformers/modeling_roberta.py
src/transformers/modeling_t5.py
src/transformers/modeling_tf_albert.py
src/transformers/modeling_tf_auto.py
src/transformers/modeling_tf_bert.py
src/transformers/modeling_tf_ctrl.py
src/transformers/modeling_tf_distilbert.py
src/transformers/modeling_tf_gpt2.py
src/transformers/modeling_tf_openai.py
src/transformers/modeling_tf_pytorch_utils.py
src/transformers/modeling_tf_roberta.py
src/transformers/modeling_tf_t5.py
src/transformers/modeling_tf_transfo_xl.py
src/transformers/modeling_tf_transfo_xl_utilities.py
src/transformers/modeling_tf_utils.py
src/transformers/modeling_tf_xlm.py
src/transformers/modeling_tf_xlnet.py
src/transformers/modeling_transfo_xl.py
src/transformers/modeling_transfo_xl_utilities.py
src/transformers/modeling_utils.py
src/transformers/modeling_xlm.py
src/transformers/modeling_xlm_roberta.py
src/transformers/modeling_xlnet.py
src/transformers/optimization.py
src/transformers/optimization_tf.py
src/transformers/tokenization_albert.py
src/transformers/tokenization_auto.py
src/transformers/tokenization_bert.py
src/transformers/tokenization_bert_japanese.py
src/transformers/tokenization_camembert.py
src/transformers/tokenization_ctrl.py
src/transformers/tokenization_distilbert.py
src/transformers/tokenization_gpt2.py
src/transformers/tokenization_openai.py
src/transformers/tokenization_roberta.py
src/transformers/tokenization_t5.py
src/transformers/tokenization_transfo_xl.py
src/transformers/tokenization_utils.py
src/transformers/tokenization_xlm.py
src/transformers/tokenization_xlm_roberta.py
src/transformers/tokenization_xlnet.py
templates/adding_a_new_example_script/README.md
templates/adding_a_new_example_script/run_xxx.py
templates/adding_a_new_example_script/utils_xxx.py
templates/adding_a_new_model/README.md
templates/adding_a_new_model/configuration_xxx.py
templates/adding_a_new_model/modeling_tf_xxx.py
templates/adding_a_new_model/modeling_xxx.py
templates/adding_a_new_model/tests/test_modeling_tf_xxx.py
templates/adding_a_new_model/tests/test_modeling_xxx.py
templates/adding_a_new_model/tests/test_tokenization_xxx.py
templates/adding_a_new_model/tokenization_xxx.py
tests/__init__.py
tests/fixtures/empty.txt
tests/fixtures/input.txt
tests/fixtures/sample_text.txt
tests/fixtures/spiece.model
tests/fixtures/test_sentencepiece.model
tests/test_configuration_common.py
tests/test_hf_api.py
tests/test_model_card.py
tests/test_modeling_albert.py
tests/test_modeling_auto.py
tests/test_modeling_bert.py
tests/test_modeling_common.py
tests/test_modeling_ctrl.py
tests/test_modeling_distilbert.py
tests/test_modeling_encoder_decoder.py
tests/test_modeling_gpt2.py
tests/test_modeling_openai.py
tests/test_modeling_roberta.py
tests/test_modeling_t5.py
tests/test_modeling_tf_albert.py
tests/test_modeling_tf_auto.py
tests/test_modeling_tf_bert.py
tests/test_modeling_tf_common.py
tests/test_modeling_tf_ctrl.py
tests/test_modeling_tf_distilbert.py
tests/test_modeling_tf_gpt2.py
tests/test_modeling_tf_openai_gpt.py
tests/test_modeling_tf_roberta.py
tests/test_modeling_tf_t5.py
tests/test_modeling_tf_transfo_xl.py
tests/test_modeling_tf_xlm.py
tests/test_modeling_tf_xlnet.py
tests/test_modeling_transfo_xl.py
tests/test_modeling_xlm.py
tests/test_modeling_xlnet.py
tests/test_optimization.py
tests/test_optimization_tf.py
tests/test_pipelines.py
tests/test_tokenization_albert.py
tests/test_tokenization_auto.py
tests/test_tokenization_bert.py
tests/test_tokenization_bert_japanese.py
tests/test_tokenization_common.py
tests/test_tokenization_ctrl.py
tests/test_tokenization_distilbert.py
tests/test_tokenization_gpt2.py
tests/test_tokenization_openai.py
tests/test_tokenization_roberta.py
tests/test_tokenization_t5.py
tests/test_tokenization_transfo_xl.py
tests/test_tokenization_utils.py
tests/test_tokenization_xlm.py
tests/test_tokenization_xlnet.py
tests/utils.py
utils/download_glue_data.py
utils/link_tester.py
valohai.yaml
==================
1efc208ff;Lysandre Debut;2020-01-06 15:02:25 +0100;Complete DataProcessor class

==

src/transformers/data/processors/utils.py
==================
c45d0cf60;Simone Primarosa;2019-12-29 13:45:45 +0100;Improve logging message in the single sentence classification processor

==

src/transformers/data/processors/utils.py
==================
bf89be77b;Simone Primarosa;2019-12-29 13:41:46 +0100;Improve logging message in the single sentence classification processor

==

src/transformers/data/processors/utils.py
==================
bf8d4bc67;Simone Primarosa;2019-12-29 13:32:47 +0100;Improve logging message in glue feature conversion

==

src/transformers/data/processors/glue.py
==================
74755c89b;Lysandre;2020-01-06 14:41:53 +0100;Example snippet for BertForQuestionAnswering

==

src/transformers/modeling_bert.py
==================
0ffc8eaf5;Aymeric Augustin;2019-12-27 22:47:59 +0100;Enforce target version for black.
This should stabilize formatting.

==

.circleci/config.yml
Makefile
src/transformers/modeling_encoder_decoder.py
src/transformers/modeling_tf_utils.py
src/transformers/modeling_utils.py
src/transformers/pipelines.py
src/transformers/tokenization_albert.py
src/transformers/tokenization_bert.py
src/transformers/tokenization_bert_japanese.py
src/transformers/tokenization_camembert.py
src/transformers/tokenization_roberta.py
src/transformers/tokenization_t5.py
src/transformers/tokenization_utils.py
src/transformers/tokenization_xlm.py
src/transformers/tokenization_xlm_roberta.py
src/transformers/tokenization_xlnet.py
templates/adding_a_new_model/tokenization_xxx.py
tests/test_tokenization_bert.py
tests/test_tokenization_gpt2.py
==================
f01b3e668;karajan1001;2020-01-06 01:50:20 +0800;fix #2399 an ImportError in official example (#2400)
* fix #2399 an ImportError in official example

* style

Co-authored-by: Julien Chaumond <chaumond@gmail.com>

==

examples/run_tf_ner.py
==================
78528742f;Julien Chaumond;2020-01-05 12:43:39 -0500;Fix syntax + link to community page

==

README.md
==================
12e0aa436;Clement;2020-01-01 14:44:03 -0500;Proposition to include community models in readme

==

README.md
==================
80faf22b4;Morgan Funtowicz;2020-01-02 12:07:39 +0100;Updating documentation for converting tensorflow model to reflect the new cli convert format.
Signed-off-by: Morgan Funtowicz <morgan@huggingface.co>

==

docs/source/converting_tensorflow_models.rst
==================
d0e594f9d;Dima;2020-01-02 09:45:48 +1100;Releasing file lock

==

src/transformers/file_utils.py
==================
629b22adc;Julien Chaumond;2020-01-01 12:55:10 -0500;[run_lm_finetuning] mask_tokens: document types

==

examples/run_lm_finetuning.py
==================
594ca6dea;Julien Chaumond;2019-12-28 00:00:45 -0500;[debug] Debug Heisenbug, the old school way.

==

tests/test_modeling_tf_common.py
==================
0df4e62da;Julien Chaumond;2019-12-29 10:06:50 -0500;[http] Tweak http user-agent (#2353)

==

src/transformers/file_utils.py
==================
f75bf05ce;Thomas Wolf;2019-12-28 15:40:00 +0100;Merge pull request #2352 from huggingface/cli_tweaks
Cli tweaks
==
==================
0d467fd6d;Julien Chaumond;2019-12-27 23:06:48 -0500;Typo

==

src/transformers/commands/user.py
==================
d8293e84f;Julien Chaumond;2019-12-27 23:02:53 -0500;[cli] upload: max number of files at the same time

==

src/transformers/commands/user.py
==================
4d6c93e92;Julien Chaumond;2019-12-27 22:55:22 -0500;Kill __main__

==

src/transformers/__main__.py
templates/adding_a_new_model/README.md
==================
9b2badf3c;Julien Chaumond;2019-12-27 22:54:29 -0500;[cli] Update doc

==

README.md
docs/source/model_sharing.md
==================
f78ebc22a;Julien Chaumond;2019-12-27 22:53:49 -0500;[cli] Add ability to delete remote object

==

src/transformers/commands/convert.py
src/transformers/commands/user.py
src/transformers/hf_api.py
tests/test_hf_api.py
==================
bfe870be6;Anthony MOI;2019-12-27 11:05:52 -0500;Hotfix tokenizers version for sdist installs

==

setup.py
==================
74ea43284;Thomas Wolf;2019-12-27 10:50:47 +0100;Merge pull request #2286 from adelevie/patch-2
Typo in tokenization_utils.py
==
==================
492bea9aa;Thomas Wolf;2019-12-27 10:33:27 +0100;Merge pull request #2292 from patrickvonplaten/add_cached_past_for_language_generation
Add cached past for language generation
==
==================
e213900fa;Thomas Wolf;2019-12-27 10:29:06 +0100;Merge pull request #2290 from patrickvonplaten/fix_typo_in_doc_for_language_generation
duplicated line for repeating_words_penalty_for_language_generation
==
==================
9f5f64644;Thomas Wolf;2019-12-27 10:24:29 +0100;Merge pull request #2211 from huggingface/fast-tokenizers
Fast tokenizers
==
==================
9024b1999;Aymeric Augustin;2019-12-27 10:13:52 +0100;Auto-format (fixes previous commit).

==

src/transformers/commands/serving.py
==================
3233b58ad;Aymeric Augustin;2019-12-27 08:50:25 +0100;Quote square brackets in shell commands.
This ensures compatibility with zsh.

Fix #2316.

==

CONTRIBUTING.md
README.md
docs/README.md
src/transformers/commands/serving.py
==================
e6ec24fa8;Anthony MOI;2019-12-26 16:49:48 -0500;Better added_tokens handling

==

src/transformers/tokenization_utils.py
==================
599db139f;Anthony MOI;2019-12-26 15:13:30 -0500;Code style update

==

src/transformers/tokenization_bert.py
src/transformers/tokenization_gpt2.py
==================
835b76a46;Anthony MOI;2019-12-26 14:42:55 -0500;Handle unk_token
As we discussed, this is handled here directly 
cc @thomwolf

==

src/transformers/tokenization_utils.py
==================
7ead04ce1;Anthony MOI;2019-12-26 14:39:39 -0500;FastPreTrainedTokenizer => PreTrainedTokenizerFast

==

src/transformers/tokenization_bert.py
src/transformers/tokenization_gpt2.py
src/transformers/tokenization_utils.py
==================
1f82a5d91;Anthony MOI;2019-12-26 14:37:55 -0500;Update for changes in tokenizers API

==

setup.py
src/transformers/tokenization_bert.py
src/transformers/tokenization_gpt2.py
src/transformers/tokenization_utils.py
==================
8c67b529f;Thomas Wolf;2019-12-26 12:38:06 +0100;Merge pull request #2324 from kashif/patch-1
Typo in serving.py
==
==================
7211541ad;Kashif Rasul;2019-12-26 12:21:40 +0100;Typo in serving.py

==

src/transformers/commands/serving.py
==================
0f6017bee;patrickvonplaten;2019-12-26 00:35:11 +0100;improve comments for examples

==

src/transformers/modeling_utils.py
==================
87c8fca9b;patrickvonplaten;2019-12-25 02:27:25 +0100;add example for ctrl text generation in docs

==

src/transformers/modeling_utils.py
==================
88def24c4;patrickvonplaten;2019-12-26 00:27:16 +0100;merge conflicts - renamed to previous_token singular

==

src/transformers/modeling_utils.py
==================
822f725a0;patrickvonplaten;2019-12-23 21:49:32 +0100;duplicated line for repeating_words_penalty_for_language_generation

==

src/transformers/modeling_utils.py
==================
fc84bd525;patrickvonplaten;2019-12-25 23:32:44 +0100;adapt style to predefined style layout

==

src/transformers/modeling_ctrl.py
src/transformers/modeling_gpt2.py
src/transformers/modeling_transfo_xl.py
src/transformers/modeling_utils.py
src/transformers/modeling_xlnet.py
==================
deff792bb;patrickvonplaten;2019-12-25 18:50:39 +0100;add prepare inputs for transfo_xl and xlnet

==

src/transformers/modeling_transfo_xl.py
src/transformers/modeling_utils.py
src/transformers/modeling_xlnet.py
==================
9398058e1;patrickvonplaten;2019-12-25 16:34:28 +0100;add easy tensor shape match test

==

src/transformers/modeling_utils.py
==================
90cda45e9;patrickvonplaten;2019-12-25 16:29:20 +0100;add past re-ordering for beam search

==

src/transformers/modeling_utils.py
==================
6bca56fdb;patrickvonplaten;2019-12-24 01:02:58 +0100;check for self.config.mem_len instead of self.mem_len in _do_output_past

==

src/transformers/modeling_utils.py
==================
365ccd0af;patrickvonplaten;2019-12-23 23:55:05 +0100;make if statements cleaner for prepare_inputs_for_generation

==

src/transformers/modeling_ctrl.py
src/transformers/modeling_gpt2.py
src/transformers/modeling_utils.py
==================
d039c679d;patrickvonplaten;2019-12-23 23:39:16 +0100;better naming for if statement

==

src/transformers/modeling_utils.py
==================
7e0c5c731;patrickvonplaten;2019-12-23 22:33:45 +0100;changed do_output_past function to check for self.config.output_past instead of self.output_past

==

src/transformers/modeling_utils.py
==================
eeaa402cd;patrickvonplaten;2019-12-23 22:15:06 +0100;rename comments

==

src/transformers/modeling_utils.py
==================
7bb427129;patrickvonplaten;2019-12-23 22:10:35 +0100;remove ipdb debugging statements

==

src/transformers/modeling_utils.py
==================
267587c25;patrickvonplaten;2019-12-23 22:04:42 +0100;add and improve comments

==

src/transformers/modeling_ctrl.py
src/transformers/modeling_gpt2.py
src/transformers/modeling_utils.py
==================
d891fd0ae;patrickvonplaten;2019-12-23 21:19:27 +0100;add past hidden key states for more efficient language generation & add prepare_inputs for gpt2 and ctrl model

==

src/transformers/modeling_ctrl.py
src/transformers/modeling_gpt2.py
src/transformers/modeling_utils.py
==================
aeef4823a;Thomas Wolf;2019-12-25 22:39:20 +0100;Merge pull request #2303 from patrickvonplaten/fix_error_with_repetition_penalty
fix repetition penalty error in modeling_utils.py
==
==================
0412f3d92;Thomas Wolf;2019-12-25 22:37:42 +0100;Merge pull request #2291 from aaugustin/fix-flake8-F841
Fix F841 flake8 warning
==
==================
8742c9546;Thomas Wolf;2019-12-25 22:30:46 +0100;Merge pull request #2289 from patrickvonplaten/fix_effective_batch_size_lang_gen_xlm
fix bug in prepare inputs for language generation for xlm for effective batch_size > 1
==
==================
1240be3ed;Thomas Wolf;2019-12-25 20:52:30 +0100;Merge pull request #2312 from vitaliyradchenko/fix_special_and_add_tokens_loading
Correct tokenization for special and added tokens
==
==================
b262577d1;vitaliyradchenko;2019-12-25 18:31:35 +0200;add special tokens to unique_added_tokens_encoder

==

src/transformers/tokenization_utils.py
==================
83a234795;vitaliyradchenko;2019-12-25 16:42:26 +0200;fixed lack of added and special tokens

==

src/transformers/tokenization_utils.py
==================
cea04a244;Thomas Wolf;2019-12-25 12:43:22 +0100;Merge pull request #2310 from ShnitzelKiller/scatter-unfix
revert erroneous fix #2276
==
==================
e1844d9a4;James Noeckel;2019-12-25 01:34:02 -0800;use positional arguments due to inconsistent API

==

src/transformers/modeling_utils.py
==================
9fb7addd4;James Noeckel;2019-12-24 22:26:09 -0800;revert erroneous fix

==

src/transformers/modeling_utils.py
==================
734d29b03;Anthony MOI;2019-12-24 13:32:41 -0500;tokenizers is now a real dependency

==

setup.py
src/transformers/tokenization_bert.py
src/transformers/tokenization_gpt2.py
==================
2818e5056;Anthony MOI;2019-12-24 13:29:01 -0500;Add tests for fast tokenizers

==

tests/test_tokenization_bert.py
tests/test_tokenization_common.py
tests/test_tokenization_gpt2.py
==================
31c56f2e0;Anthony MOI;2019-12-24 12:43:27 -0500;Fix style

==

src/transformers/tokenization_bert.py
src/transformers/tokenization_gpt2.py
src/transformers/tokenization_utils.py
==================
951ae99be;Anthony MOI;2019-12-24 12:24:24 -0500;BertTokenizerFast

==

src/transformers/__init__.py
src/transformers/tokenization_bert.py
==================
041eac2d6;Anthony MOI;2019-12-24 12:24:14 -0500;GPT2TokenizerFast

==

src/transformers/__init__.py
src/transformers/tokenization_gpt2.py
==================
3471ff0d3;Anthony MOI;2019-12-24 12:23:30 -0500;FastPreTrainedTokenizer

==

src/transformers/tokenization_utils.py
==================
18e5bdbec;patrickvonplaten;2019-12-24 17:18:05 +0100;fix repetition penalty error in modeling_utils.py

==

src/transformers/modeling_utils.py
==================
f18ac4c28;patrickvonplaten;2019-12-24 16:43:24 +0100;fix sequence length for prepare_inputs for xlnet

==

src/transformers/modeling_xlnet.py
==================
359dc4383;patrickvonplaten;2019-12-24 16:33:20 +0100;fix effective batch_size error in prepare_inputs also for xlnet

==

src/transformers/modeling_xlnet.py
==================
d98a384cb;patrickvonplaten;2019-12-23 21:45:26 +0100;fix bug in prepare inputs for language generation for xlm for effective batch_size > 1

==

src/transformers/modeling_xlm.py
==================
3e0cf4951;thomwolf;2019-12-24 11:30:56 +0100;adding back last dropout in TF 2.0 T5

==

src/transformers/modeling_tf_t5.py
==================
35d32308d;thomwolf;2019-12-24 11:29:49 +0100;adding back final dropout in T5

==

src/transformers/modeling_t5.py
==================
81db12c3b;Thomas Wolf;2019-12-24 11:21:20 +0100;Merge pull request #2271 from aaugustin/improve-setup-and-requirements
Improve setup and requirements
==
==================
10724a812;Aymeric Augustin;2019-12-24 09:09:13 +0100;Run the slow tests every Monday morning.

==

.circleci/config.yml
==================
a8d34e534;Aymeric Augustin;2019-12-24 08:46:08 +0100;Remove [--editable] in install instructions.
Use -e only in docs targeted at contributors.

If a user copy-pastes  command line with [--editable], they will hit
an error. If they don't know the --editable option, we're giving them
a choice to make before they can move forwards, but this isn't a choice
they need to make right now.

==

CONTRIBUTING.md
README.md
docs/README.md
docs/source/installation.md
examples/README.md
examples/pplm/README.md
examples/summarization/README.md
==================
e74c73a85;Aymeric Augustin;2019-12-23 22:38:23 +0100;Enable F841 warning in flake8.

==

setup.cfg
==================
e6c0019c8;Aymeric Augustin;2019-12-23 22:31:39 +0100;Remove unused variables in tests.

==

tests/test_modeling_common.py
tests/test_modeling_tf_common.py
tests/test_modeling_tf_xlm.py
tests/test_tokenization_common.py
==================
495580dad;Aymeric Augustin;2019-12-23 22:36:21 +0100;Remove unused variables in templates.

==

templates/adding_a_new_example_script/utils_xxx.py
==================
71f94a8a1;Aymeric Augustin;2019-12-23 22:28:34 +0100;Remove unused variables in src.

==

src/transformers/data/metrics/__init__.py
src/transformers/modeling_albert.py
src/transformers/modeling_t5.py
src/transformers/modeling_tf_pytorch_utils.py
src/transformers/modeling_tf_t5.py
src/transformers/modeling_tf_transfo_xl_utilities.py
src/transformers/modeling_tf_utils.py
==================
81422c4e6;Aymeric Augustin;2019-12-23 22:23:44 +0100;Remove unused variables in examples.

==

examples/contrib/run_openai_gpt.py
examples/contrib/run_transfo_xl.py
examples/run_multiple_choice.py
examples/summarization/modeling_bertabs.py
==================
072750f4d;Aymeric Augustin;2019-12-23 22:28:47 +0100;Merge pull request #2288 from aaugustin/better-handle-optional-imports
Improve handling of optional imports
==
==================
4621ad6f9;Aymeric Augustin;2019-12-23 21:30:04 +0100;Use the same pattern as everywhere else.
This is really just for consistency.

==

src/transformers/tokenization_transfo_xl.py
==================
a31d4a297;Aymeric Augustin;2019-12-23 21:27:42 +0100;Reraise ImportError when sentencepiece isn't installed.
Else, the next line fails with a confusion exception because the spm
variable isn't defined.

==

src/transformers/tokenization_albert.py
src/transformers/tokenization_t5.py
src/transformers/tokenization_xlnet.py
==================
c8b0c1e55;Aymeric Augustin;2019-12-23 21:23:08 +0100;Improve exception type.
ImportError isn't really appropriate when there's no import involved.

==

src/transformers/commands/serving.py
src/transformers/commands/train.py
src/transformers/data/processors/squad.py
src/transformers/data/processors/utils.py
src/transformers/pipelines.py
==================
4c09a9609;Aymeric Augustin;2019-12-23 21:20:54 +0100;Simplify re-raising exceptions.
Most module use the simpler `raise` version. Normalize those that don't.

==

src/transformers/modeling_tf_pytorch_utils.py
src/transformers/modeling_utils.py
src/transformers/tokenization_xlm.py
==================
5565dcdd3;Aymeric Augustin;2019-12-23 21:16:26 +0100;Remove warning when scikit-learn isn't available.
Most users don't need it.

==

src/transformers/data/metrics/__init__.py
==================
8a6881822;Aymeric Augustin;2019-12-22 21:35:09 +0100;Run some tests on Python 3.7.
This will improve version coverage.

==

.circleci/config.yml
==================
7a865821d;Aymeric Augustin;2019-12-23 20:06:39 +0100;Remove stray egg-info directory automatically.
If a user or contributor ran `pip install -e .` on transformers < 3.0,
pip created a transformers.egg-info directory next to the transformers
directory at the root of the repository.

In transformers 3.0, the source is in a `src` subdirectory.
`pip install -e .` creates a transformers.egg-info directory there.
However, pip will still pick transformers.egg-info from the previous
location. This is a bug: https://github.com/pypa/pip/issues/5466

Users and contributors are likely to hit this problem because the
documentation for transformers 3.0 relies heavily on extra_requires
which didn't exist in earlier versions, so aren't defined in a stale
transformers.egg-info directory.

If such a directory exists, remove it. It's autogenerated, gitignored
and not supposed to contain anything of value.

==

setup.py
==================
70373a5f7;Aymeric Augustin;2019-12-22 21:31:12 +0100;Update contribution instructions.
Also provide shortcuts in a Makefile.

==

CONTRIBUTING.md
Makefile
README.md
docs/source/installation.md
==================
c3783399d;Aymeric Augustin;2019-12-22 20:37:50 +0100;Remove redundant requirements with transformers.

==

examples/distillation/requirements.txt
examples/summarization/requirements.txt
==================
d79e9c9a9;Aymeric Augustin;2019-12-23 19:17:07 +0100;Remove docs/requirements.txt.
It's superseded by the "docs" extras.

==

docs/README.md
docs/requirements.txt
==================
d73eb552e;Aymeric Augustin;2019-12-22 20:33:08 +0100;Remove requirements.txt.
It's redundant with setup.py and, also, incomplete (e.g. numpy).

==

requirements.txt
setup.py
==================
9fcc532df;Aymeric Augustin;2019-12-22 20:28:48 +0100;Remove requirements-dev.txt.
It was generated once, likely in a non-reproducible way (pip freeze
in a contributor's local environment), and never updated.

==

requirements-dev.txt
==================
76a1417f2;Aymeric Augustin;2019-12-22 20:28:26 +0100;Include all optional dependencies in extras.
Take advantage of this to simplify the Circle CI configuration.

Don't bother with tensorboardX: it's a fallback for PyTorch < 1.1.0.

==

.circleci/config.yml
CONTRIBUTING.md
setup.cfg
setup.py
==================
9fc8dcb2a;Aymeric Augustin;2019-12-22 20:17:04 +0100;Standardize import.
Every other file uses this pattern.

==

examples/run_ner.py
==================
f2522869e;Aymeric Augustin;2019-12-22 19:14:07 +0100;Review and update setup.py.

==

setup.py
==================
7cef764ec;Alan deLevie;2019-12-23 12:14:50 -0500;Typo in tokenization_utils.py
avoir -> avoid
==

src/transformers/tokenization_utils.py
==================
23dad8447;Aymeric Augustin;2019-12-23 17:06:32 +0100;Install deps from setup.py for building docs.
requirements.txt isn't up to date.

==

.circleci/config.yml
==================
d8e33dbd6;Aymeric Augustin;2019-12-23 16:49:35 +0100;Fix path to source code in docs config.
This should fix API docs, which went AWOL with yesterday's changes.

==

docs/source/conf.py
==================
59b123bc5;thomwolf;2019-12-23 16:47:24 +0100;fix tqdm logging level

==

src/transformers/file_utils.py
==================
ba2378ced;Thomas Wolf;2019-12-23 12:31:00 +0100;Merge pull request #2264 from upura/fix-doclink
Fix doc link in README
==
==================
e4e2a666c;Thomas Wolf;2019-12-23 12:19:48 +0100;Merge pull request #2276 from ShnitzelKiller/scatterfix
fix error due to wrong argument name to Tensor.scatter()
==
==================
398bb03f9;James Noeckel;2019-12-22 23:18:41 -0800;fix out-of-place call to scatter, whose named argument name is source, not src

==

src/transformers/modeling_utils.py
==================
ce50305e5;Aymeric Augustin;2019-12-22 23:04:37 +0100;Merge pull request #2270 from aaugustin/remove-python-2
Remove support for Python 2
==
==================
1a948d702;Aymeric Augustin;2019-12-22 18:33:51 +0100;Switch from comments to annotations for types.

==

src/transformers/hf_api.py
==================
1c62e87b3;Aymeric Augustin;2019-12-22 18:26:42 +0100;Use built-in open().
On Python 3, `open is io.open`.

==

examples/utils_multiple_choice.py
examples/utils_ner.py
setup.py
src/transformers/configuration_utils.py
src/transformers/convert_gpt2_original_tf_checkpoint_to_pytorch.py
src/transformers/convert_openai_original_tf_checkpoint_to_pytorch.py
src/transformers/convert_transfo_xl_original_tf_checkpoint_to_pytorch.py
src/transformers/convert_xlm_original_pytorch_checkpoint_to_pytorch.py
src/transformers/data/metrics/squad_metrics.py
src/transformers/file_utils.py
src/transformers/modelcard.py
src/transformers/modeling_openai.py
src/transformers/tokenization_bert.py
src/transformers/tokenization_ctrl.py
src/transformers/tokenization_gpt2.py
src/transformers/tokenization_openai.py
src/transformers/tokenization_transfo_xl.py
src/transformers/tokenization_utils.py
src/transformers/tokenization_xlm.py
templates/adding_a_new_example_script/utils_xxx.py
templates/adding_a_new_model/tests/test_tokenization_xxx.py
templates/adding_a_new_model/tokenization_xxx.py
tests/test_tokenization_bert.py
tests/test_tokenization_bert_japanese.py
tests/test_tokenization_common.py
tests/test_tokenization_ctrl.py
tests/test_tokenization_gpt2.py
tests/test_tokenization_roberta.py
tests/test_tokenization_transfo_xl.py
==================
d6eaf4e6d;Aymeric Augustin;2019-12-22 18:22:29 +0100;Update comments mentioning Python 2.

==

examples/contrib/run_swag.py
examples/distillation/run_squad_w_distillation.py
examples/mm-imdb/run_mmimdb.py
examples/run_glue.py
examples/run_lm_finetuning.py
examples/run_multiple_choice.py
examples/run_ner.py
examples/run_squad.py
examples/run_xnli.py
setup.py
templates/adding_a_new_example_script/run_xxx.py
==================
45841eaf7;Aymeric Augustin;2019-12-22 18:22:03 +0100;Remove references to Python 2 in documentation.

==

README.md
docs/source/installation.md
==================
0dddc1494;Aymeric Augustin;2019-12-22 18:21:35 +0100;Remove py3 marker.

==

.circleci/config.yml
==================
75a23d24a;Aymeric Augustin;2019-12-22 18:19:07 +0100;Remove import fallbacks.

==

examples/test_examples.py
src/transformers/file_utils.py
src/transformers/tokenization_gpt2.py
src/transformers/tokenization_roberta.py
==================
798b3b389;Aymeric Augustin;2019-12-22 18:12:11 +0100;Remove sys.version_info[0] == 2 or 3.

==

examples/contrib/run_swag.py
examples/utils_multiple_choice.py
src/transformers/convert_transfo_xl_original_tf_checkpoint_to_pytorch.py
src/transformers/data/processors/utils.py
src/transformers/file_utils.py
src/transformers/modeling_bert.py
src/transformers/modeling_tf_albert.py
src/transformers/modeling_tf_bert.py
src/transformers/modeling_tf_xlnet.py
src/transformers/modeling_xlnet.py
src/transformers/tokenization_gpt2.py
src/transformers/tokenization_transfo_xl.py
tests/test_configuration_common.py
tests/test_model_card.py
tests/test_modeling_common.py
tests/test_modeling_tf_common.py
tests/test_optimization.py
tests/test_tokenization_common.py
==================
8af25b166;Aymeric Augustin;2019-12-22 17:56:09 +0100;Remove six.

==

src/transformers/file_utils.py
src/transformers/hf_api.py
src/transformers/pipelines.py
src/transformers/tokenization_albert.py
src/transformers/tokenization_bert.py
src/transformers/tokenization_bert_japanese.py
src/transformers/tokenization_camembert.py
src/transformers/tokenization_ctrl.py
src/transformers/tokenization_gpt2.py
src/transformers/tokenization_openai.py
src/transformers/tokenization_t5.py
src/transformers/tokenization_transfo_xl.py
src/transformers/tokenization_utils.py
src/transformers/tokenization_xlm.py
src/transformers/tokenization_xlm_roberta.py
src/transformers/tokenization_xlnet.py
templates/adding_a_new_model/tokenization_xxx.py
tests/test_hf_api.py
tests/test_tokenization_utils.py
==================
6b2200fc8;Aymeric Augustin;2019-12-22 16:21:27 +0100;Remove u-prefixes.

==

examples/summarization/utils_summarization.py
src/transformers/commands/user.py
==================
c824d15aa;Aymeric Augustin;2019-12-22 16:20:32 +0100;Remove __future__ imports.

==

examples/contrib/run_swag.py
examples/contrib/run_transfo_xl.py
examples/distillation/run_squad_w_distillation.py
examples/mm-imdb/run_mmimdb.py
examples/run_generation.py
examples/run_glue.py
examples/run_lm_finetuning.py
examples/run_multiple_choice.py
examples/run_ner.py
examples/run_squad.py
examples/run_xnli.py
examples/test_examples.py
examples/utils_multiple_choice.py
examples/utils_ner.py
src/transformers/configuration_auto.py
src/transformers/configuration_bert.py
src/transformers/configuration_camembert.py
src/transformers/configuration_ctrl.py
src/transformers/configuration_distilbert.py
src/transformers/configuration_gpt2.py
src/transformers/configuration_mmbt.py
src/transformers/configuration_openai.py
src/transformers/configuration_roberta.py
src/transformers/configuration_t5.py
src/transformers/configuration_transfo_xl.py
src/transformers/configuration_utils.py
src/transformers/configuration_xlm.py
src/transformers/configuration_xlm_roberta.py
src/transformers/configuration_xlnet.py
src/transformers/convert_albert_original_tf_checkpoint_to_pytorch.py
src/transformers/convert_bert_original_tf_checkpoint_to_pytorch.py
src/transformers/convert_gpt2_original_tf_checkpoint_to_pytorch.py
src/transformers/convert_openai_original_tf_checkpoint_to_pytorch.py
src/transformers/convert_pytorch_checkpoint_to_tf2.py
src/transformers/convert_roberta_original_pytorch_checkpoint_to_pytorch.py
src/transformers/convert_t5_original_tf_checkpoint_to_pytorch.py
src/transformers/convert_transfo_xl_original_tf_checkpoint_to_pytorch.py
src/transformers/convert_xlm_original_pytorch_checkpoint_to_pytorch.py
src/transformers/convert_xlnet_original_tf_checkpoint_to_pytorch.py
src/transformers/data/processors/xnli.py
src/transformers/file_utils.py
src/transformers/hf_api.py
src/transformers/modelcard.py
src/transformers/modeling_auto.py
src/transformers/modeling_bert.py
src/transformers/modeling_camembert.py
src/transformers/modeling_ctrl.py
src/transformers/modeling_distilbert.py
src/transformers/modeling_encoder_decoder.py
src/transformers/modeling_gpt2.py
src/transformers/modeling_mmbt.py
src/transformers/modeling_openai.py
src/transformers/modeling_roberta.py
src/transformers/modeling_t5.py
src/transformers/modeling_tf_albert.py
src/transformers/modeling_tf_auto.py
src/transformers/modeling_tf_bert.py
src/transformers/modeling_tf_ctrl.py
src/transformers/modeling_tf_distilbert.py
src/transformers/modeling_tf_gpt2.py
src/transformers/modeling_tf_openai.py
src/transformers/modeling_tf_pytorch_utils.py
src/transformers/modeling_tf_roberta.py
src/transformers/modeling_tf_t5.py
src/transformers/modeling_tf_transfo_xl.py
src/transformers/modeling_tf_utils.py
src/transformers/modeling_tf_xlm.py
src/transformers/modeling_tf_xlnet.py
src/transformers/modeling_transfo_xl.py
src/transformers/modeling_utils.py
src/transformers/modeling_xlm.py
src/transformers/modeling_xlm_roberta.py
src/transformers/modeling_xlnet.py
src/transformers/optimization_tf.py
src/transformers/pipelines.py
src/transformers/tokenization_albert.py
src/transformers/tokenization_auto.py
src/transformers/tokenization_bert.py
src/transformers/tokenization_bert_japanese.py
src/transformers/tokenization_camembert.py
src/transformers/tokenization_ctrl.py
src/transformers/tokenization_distilbert.py
src/transformers/tokenization_gpt2.py
src/transformers/tokenization_openai.py
src/transformers/tokenization_roberta.py
src/transformers/tokenization_t5.py
src/transformers/tokenization_transfo_xl.py
src/transformers/tokenization_utils.py
src/transformers/tokenization_xlm.py
src/transformers/tokenization_xlm_roberta.py
src/transformers/tokenization_xlnet.py
templates/adding_a_new_example_script/run_xxx.py
templates/adding_a_new_example_script/utils_xxx.py
templates/adding_a_new_model/configuration_xxx.py
templates/adding_a_new_model/convert_xxx_original_tf_checkpoint_to_pytorch.py
templates/adding_a_new_model/modeling_tf_xxx.py
templates/adding_a_new_model/modeling_xxx.py
templates/adding_a_new_model/tests/test_modeling_tf_xxx.py
templates/adding_a_new_model/tests/test_modeling_xxx.py
templates/adding_a_new_model/tests/test_tokenization_xxx.py
templates/adding_a_new_model/tokenization_xxx.py
tests/test_configuration_common.py
tests/test_hf_api.py
tests/test_model_card.py
tests/test_modeling_albert.py
tests/test_modeling_auto.py
tests/test_modeling_bert.py
tests/test_modeling_common.py
tests/test_modeling_ctrl.py
tests/test_modeling_distilbert.py
tests/test_modeling_gpt2.py
tests/test_modeling_openai.py
tests/test_modeling_roberta.py
tests/test_modeling_t5.py
tests/test_modeling_tf_albert.py
tests/test_modeling_tf_auto.py
tests/test_modeling_tf_bert.py
tests/test_modeling_tf_common.py
tests/test_modeling_tf_ctrl.py
tests/test_modeling_tf_distilbert.py
tests/test_modeling_tf_gpt2.py
tests/test_modeling_tf_openai_gpt.py
tests/test_modeling_tf_roberta.py
tests/test_modeling_tf_t5.py
tests/test_modeling_tf_transfo_xl.py
tests/test_modeling_tf_xlm.py
tests/test_modeling_tf_xlnet.py
tests/test_modeling_transfo_xl.py
tests/test_modeling_xlm.py
tests/test_modeling_xlnet.py
tests/test_optimization.py
tests/test_optimization_tf.py
tests/test_tokenization_albert.py
tests/test_tokenization_auto.py
tests/test_tokenization_bert.py
tests/test_tokenization_bert_japanese.py
tests/test_tokenization_common.py
tests/test_tokenization_ctrl.py
tests/test_tokenization_distilbert.py
tests/test_tokenization_gpt2.py
tests/test_tokenization_openai.py
tests/test_tokenization_roberta.py
tests/test_tokenization_t5.py
tests/test_tokenization_transfo_xl.py
tests/test_tokenization_utils.py
tests/test_tokenization_xlm.py
tests/test_tokenization_xlnet.py
==================
b6ea0f43a;Aymeric Augustin;2019-12-22 17:47:27 +0100;Remove duplicate -v flag.

==

README.md
==================
5daca95dd;Thomas Wolf;2019-12-22 16:41:53 +0100;Merge pull request #2268 from aaugustin/improve-repository-structure
Improve repository structure
==
==================
54abc67ae;Thomas Wolf;2019-12-22 16:31:11 +0100;Merge pull request #2255 from aaugustin/implement-best-practices
Implement some Python best practices
==
==================
00204f2b4;Aymeric Augustin;2019-12-22 15:34:15 +0100;Replace CommonTestCases for tokenizers with a mixin.
This is the same change as for (TF)CommonTestCases for modeling.

==

templates/adding_a_new_model/tests/test_tokenization_xxx.py
tests/test_configuration_common.py
tests/test_model_card.py
tests/test_optimization.py
tests/test_tokenization_albert.py
tests/test_tokenization_bert.py
tests/test_tokenization_bert_japanese.py
tests/test_tokenization_common.py
tests/test_tokenization_ctrl.py
tests/test_tokenization_gpt2.py
tests/test_tokenization_openai.py
tests/test_tokenization_roberta.py
tests/test_tokenization_t5.py
tests/test_tokenization_transfo_xl.py
tests/test_tokenization_xlm.py
tests/test_tokenization_xlnet.py
==================
a3c5883f2;Aymeric Augustin;2019-12-22 15:29:45 +0100;Rename file for consistency.

==

templates/adding_a_new_model/tests/test_tokenization_xxx.py
tests/test_configuration_common.py
tests/test_model_card.py
tests/test_optimization.py
tests/test_tokenization_albert.py
tests/test_tokenization_bert.py
tests/test_tokenization_bert_japanese.py
tests/test_tokenization_common.py
tests/test_tokenization_ctrl.py
tests/test_tokenization_gpt2.py
tests/test_tokenization_openai.py
tests/test_tokenization_roberta.py
tests/test_tokenization_t5.py
tests/test_tokenization_transfo_xl.py
tests/test_tokenization_xlm.py
tests/test_tokenization_xlnet.py
==================
daf8bebcd;Aymeric Augustin;2019-12-22 15:27:41 +0100;Remove unused GPTModelTester.
It isn't imported anywhere.

==

tests/test_modeling_common.py
==================
345c23a60;Aymeric Augustin;2019-12-22 14:57:20 +0100;Replace (TF)CommonTestCases for modeling with a mixin.
I suspect the wrapper classes were created in order to prevent the
abstract base class (TF)CommonModelTester from being included in test
discovery and running, because that would fail.

I solved this by replacing the abstract base class with a mixin.

Code changes are just de-indenting and automatic reformattings
performed by black to use the extra line space.

==

templates/adding_a_new_model/tests/test_modeling_tf_xxx.py
templates/adding_a_new_model/tests/test_modeling_xxx.py
tests/test_modeling_albert.py
tests/test_modeling_bert.py
tests/test_modeling_common.py
tests/test_modeling_ctrl.py
tests/test_modeling_distilbert.py
tests/test_modeling_gpt2.py
tests/test_modeling_openai.py
tests/test_modeling_roberta.py
tests/test_modeling_t5.py
tests/test_modeling_tf_albert.py
tests/test_modeling_tf_bert.py
tests/test_modeling_tf_common.py
tests/test_modeling_tf_ctrl.py
tests/test_modeling_tf_distilbert.py
tests/test_modeling_tf_gpt2.py
tests/test_modeling_tf_openai_gpt.py
tests/test_modeling_tf_roberta.py
tests/test_modeling_tf_t5.py
tests/test_modeling_tf_transfo_xl.py
tests/test_modeling_tf_xlm.py
tests/test_modeling_tf_xlnet.py
tests/test_modeling_transfo_xl.py
tests/test_modeling_xlm.py
tests/test_modeling_xlnet.py
==================
7e98e211f;Aymeric Augustin;2019-12-22 14:11:34 +0100;Remove unittest.main() in test modules.
This construct isn't used anymore these days.

Running python tests/test_foo.py puts the tests/ directory on
PYTHONPATH, which isn't representative of how we run tests.

Use python -m unittest tests/test_foo.py instead.

==

examples/summarization/test_utils_summarization.py
examples/test_examples.py
templates/adding_a_new_model/tests/test_modeling_tf_xxx.py
templates/adding_a_new_model/tests/test_modeling_xxx.py
templates/adding_a_new_model/tests/test_tokenization_xxx.py
tests/test_configuration_common.py
tests/test_hf_api.py
tests/test_model_card.py
tests/test_modeling_albert.py
tests/test_modeling_auto.py
tests/test_modeling_bert.py
tests/test_modeling_common.py
tests/test_modeling_ctrl.py
tests/test_modeling_distilbert.py
tests/test_modeling_encoder_decoder.py
tests/test_modeling_gpt2.py
tests/test_modeling_openai.py
tests/test_modeling_roberta.py
tests/test_modeling_t5.py
tests/test_modeling_tf_albert.py
tests/test_modeling_tf_auto.py
tests/test_modeling_tf_bert.py
tests/test_modeling_tf_common.py
tests/test_modeling_tf_ctrl.py
tests/test_modeling_tf_distilbert.py
tests/test_modeling_tf_gpt2.py
tests/test_modeling_tf_openai_gpt.py
tests/test_modeling_tf_roberta.py
tests/test_modeling_tf_t5.py
tests/test_modeling_tf_transfo_xl.py
tests/test_modeling_tf_xlm.py
tests/test_modeling_tf_xlnet.py
tests/test_modeling_transfo_xl.py
tests/test_modeling_xlm.py
tests/test_modeling_xlnet.py
tests/test_optimization.py
tests/test_optimization_tf.py
tests/test_pipelines.py
tests/test_tokenization_albert.py
tests/test_tokenization_auto.py
tests/test_tokenization_bert.py
tests/test_tokenization_ctrl.py
tests/test_tokenization_distilbert.py
tests/test_tokenization_gpt2.py
tests/test_tokenization_openai.py
tests/test_tokenization_roberta.py
tests/test_tokenization_t5.py
tests/test_tokenization_transfo_xl.py
tests/test_tokenization_utils.py
tests/test_tokenization_xlm.py
tests/test_tokenization_xlnet.py
==================
6be7cdda6;Aymeric Augustin;2019-12-22 13:54:22 +0100;Move source code inside a src subdirectory.
This prevents transformers from being importable simply because the CWD
is the root of the git repository, while not being importable from other
directories. That led to inconsistent behavior, especially in examples.

Once you fetch this commit, in your dev environment, you must run:

    $ pip uninstall transformers
    $ pip install -e .

==

.circleci/config.yml
Makefile
setup.py
src/transformers/__init__.py
src/transformers/__main__.py
src/transformers/commands/__init__.py
src/transformers/commands/convert.py
src/transformers/commands/download.py
src/transformers/commands/run.py
src/transformers/commands/serving.py
src/transformers/commands/train.py
src/transformers/commands/user.py
src/transformers/configuration_albert.py
src/transformers/configuration_auto.py
src/transformers/configuration_bert.py
src/transformers/configuration_camembert.py
src/transformers/configuration_ctrl.py
src/transformers/configuration_distilbert.py
src/transformers/configuration_gpt2.py
src/transformers/configuration_mmbt.py
src/transformers/configuration_openai.py
src/transformers/configuration_roberta.py
src/transformers/configuration_t5.py
src/transformers/configuration_transfo_xl.py
src/transformers/configuration_utils.py
src/transformers/configuration_xlm.py
src/transformers/configuration_xlm_roberta.py
src/transformers/configuration_xlnet.py
src/transformers/convert_albert_original_tf_checkpoint_to_pytorch.py
src/transformers/convert_bert_original_tf_checkpoint_to_pytorch.py
src/transformers/convert_bert_pytorch_checkpoint_to_original_tf.py
src/transformers/convert_gpt2_original_tf_checkpoint_to_pytorch.py
src/transformers/convert_openai_original_tf_checkpoint_to_pytorch.py
src/transformers/convert_pytorch_checkpoint_to_tf2.py
src/transformers/convert_roberta_original_pytorch_checkpoint_to_pytorch.py
src/transformers/convert_t5_original_tf_checkpoint_to_pytorch.py
src/transformers/convert_transfo_xl_original_tf_checkpoint_to_pytorch.py
src/transformers/convert_xlm_original_pytorch_checkpoint_to_pytorch.py
src/transformers/convert_xlnet_original_tf_checkpoint_to_pytorch.py
src/transformers/data/__init__.py
src/transformers/data/metrics/__init__.py
src/transformers/data/metrics/squad_metrics.py
src/transformers/data/processors/__init__.py
src/transformers/data/processors/glue.py
src/transformers/data/processors/squad.py
src/transformers/data/processors/utils.py
src/transformers/data/processors/xnli.py
src/transformers/file_utils.py
src/transformers/hf_api.py
src/transformers/modelcard.py
src/transformers/modeling_albert.py
src/transformers/modeling_auto.py
src/transformers/modeling_bert.py
src/transformers/modeling_camembert.py
src/transformers/modeling_ctrl.py
src/transformers/modeling_distilbert.py
src/transformers/modeling_encoder_decoder.py
src/transformers/modeling_gpt2.py
src/transformers/modeling_mmbt.py
src/transformers/modeling_openai.py
src/transformers/modeling_roberta.py
src/transformers/modeling_t5.py
src/transformers/modeling_tf_albert.py
src/transformers/modeling_tf_auto.py
src/transformers/modeling_tf_bert.py
src/transformers/modeling_tf_ctrl.py
src/transformers/modeling_tf_distilbert.py
src/transformers/modeling_tf_gpt2.py
src/transformers/modeling_tf_openai.py
src/transformers/modeling_tf_pytorch_utils.py
src/transformers/modeling_tf_roberta.py
src/transformers/modeling_tf_t5.py
src/transformers/modeling_tf_transfo_xl.py
src/transformers/modeling_tf_transfo_xl_utilities.py
src/transformers/modeling_tf_utils.py
src/transformers/modeling_tf_xlm.py
src/transformers/modeling_tf_xlnet.py
src/transformers/modeling_transfo_xl.py
src/transformers/modeling_transfo_xl_utilities.py
src/transformers/modeling_utils.py
src/transformers/modeling_xlm.py
src/transformers/modeling_xlm_roberta.py
src/transformers/modeling_xlnet.py
src/transformers/optimization.py
src/transformers/optimization_tf.py
src/transformers/pipelines.py
src/transformers/tokenization_albert.py
src/transformers/tokenization_auto.py
src/transformers/tokenization_bert.py
src/transformers/tokenization_bert_japanese.py
src/transformers/tokenization_camembert.py
src/transformers/tokenization_ctrl.py
src/transformers/tokenization_distilbert.py
src/transformers/tokenization_gpt2.py
src/transformers/tokenization_openai.py
src/transformers/tokenization_roberta.py
src/transformers/tokenization_t5.py
src/transformers/tokenization_transfo_xl.py
src/transformers/tokenization_utils.py
src/transformers/tokenization_xlm.py
src/transformers/tokenization_xlm_roberta.py
src/transformers/tokenization_xlnet.py
==================
ced0a9420;Aymeric Augustin;2019-12-22 13:44:13 +0100;Switch test files to the standard test_*.py scheme.

==

.circleci/config.yml
README.md
docs/source/installation.md
examples/summarization/test_utils_summarization.py
templates/adding_a_new_model/tests/test_modeling_tf_xxx.py
templates/adding_a_new_model/tests/test_modeling_xxx.py
templates/adding_a_new_model/tests/test_tokenization_xxx.py
tests/test_configuration_common.py
tests/test_hf_api.py
tests/test_model_card.py
tests/test_modeling_albert.py
tests/test_modeling_auto.py
tests/test_modeling_bert.py
tests/test_modeling_common.py
tests/test_modeling_ctrl.py
tests/test_modeling_distilbert.py
tests/test_modeling_encoder_decoder.py
tests/test_modeling_gpt2.py
tests/test_modeling_openai.py
tests/test_modeling_roberta.py
tests/test_modeling_t5.py
tests/test_modeling_tf_albert.py
tests/test_modeling_tf_auto.py
tests/test_modeling_tf_bert.py
tests/test_modeling_tf_common.py
tests/test_modeling_tf_ctrl.py
tests/test_modeling_tf_distilbert.py
tests/test_modeling_tf_gpt2.py
tests/test_modeling_tf_openai_gpt.py
tests/test_modeling_tf_roberta.py
tests/test_modeling_tf_t5.py
tests/test_modeling_tf_transfo_xl.py
tests/test_modeling_tf_xlm.py
tests/test_modeling_tf_xlnet.py
tests/test_modeling_transfo_xl.py
tests/test_modeling_xlm.py
tests/test_modeling_xlnet.py
tests/test_optimization.py
tests/test_optimization_tf.py
tests/test_pipelines.py
tests/test_tokenization_albert.py
tests/test_tokenization_auto.py
tests/test_tokenization_bert.py
tests/test_tokenization_bert_japanese.py
tests/test_tokenization_ctrl.py
tests/test_tokenization_distilbert.py
tests/test_tokenization_gpt2.py
tests/test_tokenization_openai.py
tests/test_tokenization_roberta.py
tests/test_tokenization_t5.py
tests/test_tokenization_transfo_xl.py
tests/test_tokenization_utils.py
tests/test_tokenization_xlm.py
tests/test_tokenization_xlnet.py
==================
067395d5c;Aymeric Augustin;2019-12-22 13:32:34 +0100;Move tests outside of library.

==

.circleci/config.yml
Makefile
README.md
docs/source/installation.md
tests/__init__.py
tests/configuration_common_test.py
tests/fixtures/empty.txt
tests/fixtures/input.txt
tests/fixtures/sample_text.txt
tests/fixtures/spiece.model
tests/fixtures/test_sentencepiece.model
tests/hf_api_test.py
tests/model_card_test.py
tests/modeling_albert_test.py
tests/modeling_auto_test.py
tests/modeling_bert_test.py
tests/modeling_common_test.py
tests/modeling_ctrl_test.py
tests/modeling_distilbert_test.py
tests/modeling_encoder_decoder_test.py
tests/modeling_gpt2_test.py
tests/modeling_openai_test.py
tests/modeling_roberta_test.py
tests/modeling_t5_test.py
tests/modeling_tf_albert_test.py
tests/modeling_tf_auto_test.py
tests/modeling_tf_bert_test.py
tests/modeling_tf_common_test.py
tests/modeling_tf_ctrl_test.py
tests/modeling_tf_distilbert_test.py
tests/modeling_tf_gpt2_test.py
tests/modeling_tf_openai_gpt_test.py
tests/modeling_tf_roberta_test.py
tests/modeling_tf_t5_test.py
tests/modeling_tf_transfo_xl_test.py
tests/modeling_tf_xlm_test.py
tests/modeling_tf_xlnet_test.py
tests/modeling_transfo_xl_test.py
tests/modeling_xlm_test.py
tests/modeling_xlnet_test.py
tests/optimization_test.py
tests/optimization_tf_test.py
tests/pipelines_test.py
tests/tokenization_albert_test.py
tests/tokenization_auto_test.py
tests/tokenization_bert_japanese_test.py
tests/tokenization_bert_test.py
tests/tokenization_ctrl_test.py
tests/tokenization_distilbert_test.py
tests/tokenization_gpt2_test.py
tests/tokenization_openai_test.py
tests/tokenization_roberta_test.py
tests/tokenization_t5_test.py
tests/tokenization_tests_commons.py
tests/tokenization_transfo_xl_test.py
tests/tokenization_utils_test.py
tests/tokenization_xlm_test.py
tests/tokenization_xlnet_test.py
tests/utils.py
==================
698f9e3d7;Aymeric Augustin;2019-12-22 13:29:58 +0100;Remove trailing whitespace in README.

==

README.md
==================
c11b3e292;Aymeric Augustin;2019-12-22 11:17:48 +0100;Sort imports for optional third-party libraries.
These libraries aren't always installed in the virtual environment where
isort is running. Declaring them properly avoids mixing these
third-party imports with local imports.

==

.circleci/config.yml
examples/distillation/distiller.py
examples/distillation/utils.py
examples/mm-imdb/utils_mmimdb.py
examples/pplm/run_pplm_discrim_train.py
examples/run_ner.py
examples/run_tf_glue.py
examples/run_tf_ner.py
setup.cfg
transformers/convert_roberta_original_pytorch_checkpoint_to_pytorch.py
==================
2a34d5b71;Aymeric Augustin;2019-12-22 11:07:31 +0100;Stabilize import order for packaging.
I don't want to consider it a dependency of transformers, but it's
usually there in local development and usually not there in CI.

==

setup.cfg
==================
c9270086e;Aymeric Augustin;2019-12-21 22:32:10 +0100;Disable flake8 F841 in CI to get a passing run.
I'll fix it later.

==

setup.cfg
==================
577a03664;Aymeric Augustin;2019-12-21 22:31:44 +0100;Enforce flake8 in CI.

==

.circleci/config.yml
==================
7c6812645;Aymeric Augustin;2019-12-21 22:13:14 +0100;Restore proper import for HTTPError.

==

transformers/commands/user.py
transformers/tests/hf_api_test.py
==================
939148b05;Aymeric Augustin;2019-12-21 22:04:51 +0100;Fix F401 flake8 warning (x28).
Do manually what autoflake couldn't manage.

==

examples/run_bertology.py
templates/adding_a_new_model/tests/modeling_tf_xxx_test.py
templates/adding_a_new_model/tests/modeling_xxx_test.py
transformers/convert_pytorch_checkpoint_to_tf2.py
transformers/convert_roberta_original_pytorch_checkpoint_to_pytorch.py
transformers/modeling_auto.py
transformers/modeling_tf_utils.py
transformers/modeling_tf_xlm.py
transformers/modeling_xlnet.py
transformers/tests/modeling_common_test.py
transformers/tests/modeling_tf_bert_test.py
==================
783a61699;Aymeric Augustin;2019-12-21 21:54:07 +0100;Fix F401 flake8 warning (x88 / 116).
This change is mostly autogenerated with:

    $ python -m autoflake --in-place --recursive --remove-all-unused-imports --ignore-init-module-imports examples templates transformers utils hubconf.py setup.py

I made minor changes in the generated diff.

==

examples/distillation/distiller.py
examples/distillation/scripts/extract.py
examples/distillation/scripts/extract_distilbert.py
examples/run_bertology.py
examples/run_squad.py
examples/run_tf_ner.py
templates/adding_a_new_model/configuration_xxx.py
templates/adding_a_new_model/modeling_tf_xxx.py
templates/adding_a_new_model/modeling_xxx.py
templates/adding_a_new_model/tests/modeling_tf_xxx_test.py
templates/adding_a_new_model/tokenization_xxx.py
transformers/commands/convert.py
transformers/configuration_t5.py
transformers/convert_pytorch_checkpoint_to_tf2.py
transformers/convert_roberta_original_pytorch_checkpoint_to_pytorch.py
transformers/data/metrics/squad_metrics.py
transformers/data/processors/squad.py
transformers/hf_api.py
transformers/modeling_auto.py
transformers/modeling_ctrl.py
transformers/modeling_distilbert.py
transformers/modeling_encoder_decoder.py
transformers/modeling_gpt2.py
transformers/modeling_openai.py
transformers/modeling_t5.py
transformers/modeling_tf_auto.py
transformers/modeling_tf_ctrl.py
transformers/modeling_tf_distilbert.py
transformers/modeling_tf_pytorch_utils.py
transformers/modeling_tf_roberta.py
transformers/modeling_tf_transfo_xl.py
transformers/modeling_tf_transfo_xl_utilities.py
transformers/modeling_transfo_xl.py
transformers/modeling_transfo_xl_utilities.py
transformers/modeling_utils.py
transformers/pipelines.py
transformers/tests/modeling_auto_test.py
transformers/tests/modeling_distilbert_test.py
transformers/tests/modeling_t5_test.py
transformers/tests/modeling_tf_albert_test.py
transformers/tests/modeling_tf_auto_test.py
transformers/tests/modeling_tf_common_test.py
transformers/tests/modeling_tf_ctrl_test.py
transformers/tests/modeling_tf_distilbert_test.py
transformers/tests/modeling_tf_t5_test.py
transformers/tests/tokenization_albert_test.py
transformers/tests/tokenization_distilbert_test.py
transformers/tests/tokenization_transfo_xl_test.py
transformers/tokenization_bert_japanese.py
transformers/tokenization_distilbert.py
transformers/tokenization_roberta.py
transformers/tokenization_xlm.py
==================
80327a13e;Aymeric Augustin;2019-12-21 18:35:41 +0100;Fix F401 flake8 warning (x152 / 268).
This change is mostly autogenerated with:

    $ python -m autoflake --in-place --recursive examples templates transformers utils hubconf.py setup.py

I made minor changes in the generated diff.

==

examples/contrib/run_camembert.py
examples/run_bertology.py
examples/summarization/configuration_bertabs.py
templates/adding_a_new_model/configuration_xxx.py
templates/adding_a_new_model/modeling_tf_xxx.py
templates/adding_a_new_model/modeling_xxx.py
templates/adding_a_new_model/tests/modeling_tf_xxx_test.py
transformers/configuration_bert.py
transformers/configuration_ctrl.py
transformers/configuration_distilbert.py
transformers/configuration_gpt2.py
transformers/configuration_openai.py
transformers/configuration_t5.py
transformers/configuration_transfo_xl.py
transformers/configuration_xlm.py
transformers/configuration_xlnet.py
transformers/data/metrics/__init__.py
transformers/data/processors/squad.py
transformers/modeling_ctrl.py
transformers/modeling_distilbert.py
transformers/modeling_encoder_decoder.py
transformers/modeling_gpt2.py
transformers/modeling_openai.py
transformers/modeling_t5.py
transformers/modeling_tf_bert.py
transformers/modeling_tf_ctrl.py
transformers/modeling_tf_distilbert.py
transformers/modeling_tf_gpt2.py
transformers/modeling_tf_openai.py
transformers/modeling_tf_transfo_xl.py
transformers/modeling_tf_transfo_xl_utilities.py
transformers/modeling_tf_xlm.py
transformers/modeling_tf_xlnet.py
transformers/modeling_transfo_xl.py
transformers/modeling_transfo_xl_utilities.py
transformers/modeling_utils.py
transformers/modeling_xlnet.py
transformers/tests/configuration_common_test.py
transformers/tests/modeling_auto_test.py
transformers/tests/modeling_common_test.py
transformers/tests/modeling_ctrl_test.py
transformers/tests/modeling_tf_albert_test.py
transformers/tests/modeling_tf_auto_test.py
transformers/tests/modeling_tf_bert_test.py
transformers/tests/modeling_tf_common_test.py
transformers/tests/modeling_tf_ctrl_test.py
transformers/tests/modeling_tf_gpt2_test.py
transformers/tests/modeling_tf_openai_gpt_test.py
transformers/tests/modeling_tf_t5_test.py
transformers/tests/modeling_tf_xlnet_test.py
transformers/tests/modeling_xlnet_test.py
transformers/tests/tokenization_auto_test.py
transformers/tests/tokenization_bert_japanese_test.py
transformers/tests/tokenization_distilbert_test.py
transformers/tokenization_bert_japanese.py
transformers/tokenization_distilbert.py
transformers/tokenization_roberta.py
utils/download_glue_data.py
==================
654e051e2;Aymeric Augustin;2019-12-21 21:31:37 +0100;Ignore F401 flake8 warning (x326 / 594).

==

transformers/__init__.py
transformers/data/__init__.py
transformers/data/processors/__init__.py
==================
fa2ccbc08;Aymeric Augustin;2019-12-21 21:22:55 +0100;Fix E266 flake8 warning (x90).

==

examples/contrib/run_swag.py
examples/distillation/run_squad_w_distillation.py
examples/distillation/scripts/extract.py
examples/distillation/train.py
examples/mm-imdb/run_mmimdb.py
examples/run_bertology.py
examples/run_glue.py
examples/run_lm_finetuning.py
examples/run_multiple_choice.py
examples/run_ner.py
examples/run_xnli.py
templates/adding_a_new_example_script/run_xxx.py
templates/adding_a_new_model/convert_xxx_original_tf_checkpoint_to_pytorch.py
transformers/convert_albert_original_tf_checkpoint_to_pytorch.py
transformers/convert_bert_original_tf_checkpoint_to_pytorch.py
transformers/convert_gpt2_original_tf_checkpoint_to_pytorch.py
transformers/convert_openai_original_tf_checkpoint_to_pytorch.py
transformers/convert_pytorch_checkpoint_to_tf2.py
transformers/convert_roberta_original_pytorch_checkpoint_to_pytorch.py
transformers/convert_t5_original_tf_checkpoint_to_pytorch.py
transformers/convert_xlm_original_pytorch_checkpoint_to_pytorch.py
transformers/convert_xlnet_original_tf_checkpoint_to_pytorch.py
transformers/modeling_distilbert.py
transformers/modeling_tf_distilbert.py
transformers/modeling_tf_pytorch_utils.py
transformers/modeling_tf_transfo_xl.py
transformers/modeling_tf_xlnet.py
transformers/modeling_transfo_xl.py
transformers/modeling_xlnet.py
transformers/optimization_tf.py
==================
2ab78325f;Aymeric Augustin;2019-12-21 20:53:17 +0100;Fix F821 flake8 warning (x47).
Ignore warnings related to Python 2, because it's going away soon.

==

examples/contrib/run_swag.py
examples/run_generation.py
examples/utils_multiple_choice.py
templates/adding_a_new_model/modeling_tf_xxx.py
templates/adding_a_new_model/modeling_xxx.py
transformers/commands/user.py
transformers/data/processors/utils.py
transformers/file_utils.py
transformers/hf_api.py
transformers/modeling_bert.py
transformers/modeling_tf_albert.py
transformers/modeling_tf_auto.py
transformers/modeling_tf_bert.py
transformers/modeling_tf_xlnet.py
transformers/modeling_xlnet.py
transformers/tests/tokenization_utils_test.py
transformers/tokenization_albert.py
transformers/tokenization_gpt2.py
transformers/tokenization_transfo_xl.py
transformers/tokenization_utils.py
transformers/tokenization_xlnet.py
==================
631be2707;Aymeric Augustin;2019-12-21 20:22:05 +0100;Fix E722 flake8 warnings (x26).

==

examples/contrib/run_swag.py
examples/distillation/distiller.py
examples/distillation/run_squad_w_distillation.py
examples/mm-imdb/run_mmimdb.py
examples/pplm/run_pplm.py
examples/pplm/run_pplm_discrim_train.py
examples/run_glue.py
examples/run_lm_finetuning.py
examples/run_multiple_choice.py
examples/run_squad.py
examples/run_xnli.py
templates/adding_a_new_example_script/run_xxx.py
transformers/__init__.py
transformers/hf_api.py
transformers/modeling_utils.py
transformers/tests/modeling_tf_common_test.py
transformers/tokenization_ctrl.py
transformers/tokenization_gpt2.py
transformers/tokenization_openai.py
transformers/tokenization_xlm.py
==================
b0f7db73c;Aymeric Augustin;2019-12-21 18:25:59 +0100;Fix E741 flake8 warning (x14).

==

examples/contrib/run_swag.py
templates/adding_a_new_model/modeling_xxx.py
transformers/modeling_albert.py
transformers/modeling_bert.py
transformers/modeling_gpt2.py
transformers/modeling_openai.py
transformers/modeling_t5.py
transformers/tokenization_t5.py
==================
ea89bec18;Aymeric Augustin;2019-12-21 18:13:15 +0100;Fix E231 flake8 warning (x9).

==

transformers/hf_api.py
transformers/optimization_tf.py
transformers/pipelines.py
transformers/tests/model_card_test.py
transformers/tokenization_ctrl.py
transformers/tokenization_openai.py
==================
fd2f17a7a;Aymeric Augustin;2019-12-21 18:07:03 +0100;Fix E714 flake8 warning (x8).

==

examples/summarization/modeling_bertabs.py
templates/adding_a_new_model/modeling_tf_xxx.py
transformers/modeling_tf_albert.py
transformers/modeling_tf_bert.py
transformers/modeling_tf_gpt2.py
transformers/modeling_tf_openai.py
transformers/modeling_tf_t5.py
transformers/modeling_tf_transfo_xl.py
==================
5eab3cf6b;Aymeric Augustin;2019-12-21 18:03:57 +0100;Fix W605 flake8 warning (x5).

==

examples/contrib/run_openai_gpt.py
transformers/tokenization_xlm.py
==================
7dce8dc7a;Aymeric Augustin;2019-12-21 18:01:54 +0100;Fix E731 flake8 warning (x3).

==

examples/summarization/run_summarization.py
transformers/commands/serving.py
transformers/modeling_utils.py
==================
eed46f38b;Aymeric Augustin;2019-12-21 18:01:03 +0100;Fix E302 flake8 warning (x3).

==

templates/adding_a_new_model/modeling_tf_xxx.py
templates/adding_a_new_model/modeling_xxx.py
transformers/modeling_t5.py
==================
b1de7ae08;Aymeric Augustin;2019-12-21 17:59:13 +0100;Fix F811 flake8 warning (x1).

==

transformers/tests/modeling_xlnet_test.py
==================
357db7098;Aymeric Augustin;2019-12-21 17:58:17 +0100;Fix E712 flake8 warning (x1).

==

examples/run_multiple_choice.py
==================
f9c5317db;Aymeric Augustin;2019-12-21 17:57:20 +0100;Fix E265 flake8 warning (x1).

==

examples/contrib/run_swag.py
==================
28e608a2c;Aymeric Augustin;2019-12-21 17:02:36 +0100;Remove trailing whitespace from all Python files.
Fixes flake8 warning W291 (x224).

==

examples/benchmarks.py
hubconf.py
templates/adding_a_new_model/modeling_tf_xxx.py
templates/adding_a_new_model/modeling_xxx.py
transformers/commands/user.py
transformers/data/processors/squad.py
transformers/modeling_albert.py
transformers/modeling_auto.py
transformers/modeling_camembert.py
transformers/modeling_ctrl.py
transformers/modeling_distilbert.py
transformers/modeling_gpt2.py
transformers/modeling_mmbt.py
transformers/modeling_roberta.py
transformers/modeling_t5.py
transformers/modeling_tf_albert.py
transformers/modeling_tf_ctrl.py
transformers/modeling_tf_distilbert.py
transformers/modeling_tf_gpt2.py
transformers/modeling_tf_openai.py
transformers/modeling_tf_roberta.py
transformers/modeling_tf_t5.py
transformers/modeling_tf_utils.py
transformers/modeling_xlm.py
transformers/modeling_xlm_roberta.py
transformers/tokenization_utils.py
utils/download_glue_data.py
==================
1efa0a755;Aymeric Augustin;2019-12-21 17:06:41 +0100;Add black-compatible flake8 configuration.

==

setup.cfg
==================
d0c9fe277;Aymeric Augustin;2019-12-21 16:29:21 +0100;Fix circular import in transformers.pipelines.
Submodules shouldn't import from their parent in general.

==

transformers/pipelines.py
==================
5ca054757;Aymeric Augustin;2019-12-21 16:00:11 +0100;Update "make style" to sort imports with isort.

==

Makefile
==================
9e80fc7b2;Aymeric Augustin;2019-12-21 15:58:40 +0100;Enforce isort in CI.
We need https://github.com/timothycrosley/isort/pull/1000 but there's no
release with this fix yet, so we'll install from GitHub.

==

.circleci/config.yml
==================
158e82e06;Aymeric Augustin;2019-12-21 15:57:32 +0100;Sort imports with isort.
This is the result of:

    $ isort --recursive examples templates transformers utils hubconf.py setup.py

==

examples/benchmarks.py
examples/contrib/run_camembert.py
examples/contrib/run_openai_gpt.py
examples/contrib/run_swag.py
examples/contrib/run_transfo_xl.py
examples/distillation/distiller.py
examples/distillation/grouped_batch_sampler.py
examples/distillation/lm_seqs_dataset.py
examples/distillation/run_squad_w_distillation.py
examples/distillation/scripts/binarized_data.py
examples/distillation/scripts/extract.py
examples/distillation/scripts/extract_distilbert.py
examples/distillation/scripts/token_counts.py
examples/distillation/train.py
examples/distillation/utils.py
examples/mm-imdb/run_mmimdb.py
examples/mm-imdb/utils_mmimdb.py
examples/pplm/run_pplm.py
examples/pplm/run_pplm_discrim_train.py
examples/run_bertology.py
examples/run_generation.py
examples/run_glue.py
examples/run_lm_finetuning.py
examples/run_multiple_choice.py
examples/run_ner.py
examples/run_squad.py
examples/run_tf_glue.py
examples/run_tf_ner.py
examples/run_xnli.py
examples/summarization/convert_bertabs_original_pytorch_checkpoint.py
examples/summarization/modeling_bertabs.py
examples/summarization/run_summarization.py
examples/summarization/utils_summarization.py
examples/summarization/utils_summarization_test.py
examples/test_examples.py
examples/utils_multiple_choice.py
examples/utils_ner.py
hubconf.py
setup.py
templates/adding_a_new_example_script/run_xxx.py
templates/adding_a_new_example_script/utils_xxx.py
templates/adding_a_new_model/configuration_xxx.py
templates/adding_a_new_model/convert_xxx_original_tf_checkpoint_to_pytorch.py
templates/adding_a_new_model/modeling_tf_xxx.py
templates/adding_a_new_model/modeling_xxx.py
templates/adding_a_new_model/tests/modeling_tf_xxx_test.py
templates/adding_a_new_model/tests/modeling_xxx_test.py
templates/adding_a_new_model/tests/tokenization_xxx_test.py
templates/adding_a_new_model/tokenization_xxx.py
transformers/__init__.py
transformers/commands/convert.py
transformers/commands/run.py
transformers/commands/serving.py
transformers/commands/train.py
transformers/commands/user.py
transformers/configuration_albert.py
transformers/configuration_auto.py
transformers/configuration_bert.py
transformers/configuration_camembert.py
transformers/configuration_ctrl.py
transformers/configuration_distilbert.py
transformers/configuration_gpt2.py
transformers/configuration_mmbt.py
transformers/configuration_openai.py
transformers/configuration_roberta.py
transformers/configuration_t5.py
transformers/configuration_transfo_xl.py
transformers/configuration_utils.py
transformers/configuration_xlm.py
transformers/configuration_xlm_roberta.py
transformers/configuration_xlnet.py
transformers/convert_albert_original_tf_checkpoint_to_pytorch.py
transformers/convert_bert_original_tf_checkpoint_to_pytorch.py
transformers/convert_bert_pytorch_checkpoint_to_original_tf.py
transformers/convert_gpt2_original_tf_checkpoint_to_pytorch.py
transformers/convert_openai_original_tf_checkpoint_to_pytorch.py
transformers/convert_pytorch_checkpoint_to_tf2.py
transformers/convert_roberta_original_pytorch_checkpoint_to_pytorch.py
transformers/convert_t5_original_tf_checkpoint_to_pytorch.py
transformers/convert_transfo_xl_original_tf_checkpoint_to_pytorch.py
transformers/convert_xlm_original_pytorch_checkpoint_to_pytorch.py
transformers/convert_xlnet_original_tf_checkpoint_to_pytorch.py
transformers/data/__init__.py
transformers/data/metrics/__init__.py
transformers/data/metrics/squad_metrics.py
transformers/data/processors/__init__.py
transformers/data/processors/glue.py
transformers/data/processors/squad.py
transformers/data/processors/utils.py
transformers/data/processors/xnli.py
transformers/file_utils.py
transformers/hf_api.py
transformers/modelcard.py
transformers/modeling_albert.py
transformers/modeling_auto.py
transformers/modeling_bert.py
transformers/modeling_camembert.py
transformers/modeling_ctrl.py
transformers/modeling_distilbert.py
transformers/modeling_encoder_decoder.py
transformers/modeling_gpt2.py
transformers/modeling_mmbt.py
transformers/modeling_openai.py
transformers/modeling_roberta.py
transformers/modeling_t5.py
transformers/modeling_tf_albert.py
transformers/modeling_tf_auto.py
transformers/modeling_tf_bert.py
transformers/modeling_tf_ctrl.py
transformers/modeling_tf_distilbert.py
transformers/modeling_tf_gpt2.py
transformers/modeling_tf_openai.py
transformers/modeling_tf_pytorch_utils.py
transformers/modeling_tf_roberta.py
transformers/modeling_tf_t5.py
transformers/modeling_tf_transfo_xl.py
transformers/modeling_tf_transfo_xl_utilities.py
transformers/modeling_tf_utils.py
transformers/modeling_tf_xlm.py
transformers/modeling_tf_xlnet.py
transformers/modeling_transfo_xl.py
transformers/modeling_transfo_xl_utilities.py
transformers/modeling_utils.py
transformers/modeling_xlm.py
transformers/modeling_xlm_roberta.py
transformers/modeling_xlnet.py
transformers/optimization.py
transformers/optimization_tf.py
transformers/pipelines.py
transformers/tests/configuration_common_test.py
transformers/tests/hf_api_test.py
transformers/tests/model_card_test.py
transformers/tests/modeling_albert_test.py
transformers/tests/modeling_auto_test.py
transformers/tests/modeling_bert_test.py
transformers/tests/modeling_common_test.py
transformers/tests/modeling_ctrl_test.py
transformers/tests/modeling_distilbert_test.py
transformers/tests/modeling_encoder_decoder_test.py
transformers/tests/modeling_gpt2_test.py
transformers/tests/modeling_openai_test.py
transformers/tests/modeling_roberta_test.py
transformers/tests/modeling_t5_test.py
transformers/tests/modeling_tf_albert_test.py
transformers/tests/modeling_tf_auto_test.py
transformers/tests/modeling_tf_bert_test.py
transformers/tests/modeling_tf_common_test.py
transformers/tests/modeling_tf_ctrl_test.py
transformers/tests/modeling_tf_distilbert_test.py
transformers/tests/modeling_tf_gpt2_test.py
transformers/tests/modeling_tf_openai_gpt_test.py
transformers/tests/modeling_tf_roberta_test.py
transformers/tests/modeling_tf_t5_test.py
transformers/tests/modeling_tf_transfo_xl_test.py
transformers/tests/modeling_tf_xlm_test.py
transformers/tests/modeling_tf_xlnet_test.py
transformers/tests/modeling_transfo_xl_test.py
transformers/tests/modeling_xlm_test.py
transformers/tests/modeling_xlnet_test.py
transformers/tests/optimization_test.py
transformers/tests/optimization_tf_test.py
transformers/tests/pipelines_test.py
transformers/tests/tokenization_albert_test.py
transformers/tests/tokenization_auto_test.py
transformers/tests/tokenization_bert_japanese_test.py
transformers/tests/tokenization_bert_test.py
transformers/tests/tokenization_ctrl_test.py
transformers/tests/tokenization_distilbert_test.py
transformers/tests/tokenization_gpt2_test.py
transformers/tests/tokenization_openai_test.py
transformers/tests/tokenization_roberta_test.py
transformers/tests/tokenization_t5_test.py
transformers/tests/tokenization_tests_commons.py
transformers/tests/tokenization_transfo_xl_test.py
transformers/tests/tokenization_utils_test.py
transformers/tests/tokenization_xlm_test.py
transformers/tests/tokenization_xlnet_test.py
transformers/tests/utils.py
transformers/tokenization_albert.py
transformers/tokenization_auto.py
transformers/tokenization_bert.py
transformers/tokenization_bert_japanese.py
transformers/tokenization_camembert.py
transformers/tokenization_ctrl.py
transformers/tokenization_distilbert.py
transformers/tokenization_gpt2.py
transformers/tokenization_openai.py
transformers/tokenization_roberta.py
transformers/tokenization_t5.py
transformers/tokenization_transfo_xl.py
transformers/tokenization_utils.py
transformers/tokenization_xlm.py
transformers/tokenization_xlm_roberta.py
transformers/tokenization_xlnet.py
utils/download_glue_data.py
==================
9d00f78f1;upura;2019-12-22 16:07:05 +0900;fix doc link

==

.circleci/deploy.sh
deploy_multi_version_doc.sh
==================
b668a740c;Daniil Larionov;2019-12-22 00:01:14 +0300;Fixing incorrect link in model docstring
The docstring contains a link to Salesforce/CTRL repo, while the model itself is Facebookresearch/mmbt. It may be the wrong copy\paste.
==

transformers/modeling_mmbt.py
==================
bc1715c1e;Aymeric Augustin;2019-12-21 15:56:44 +0100;Add black-compatible isort configuration.
lines_after_imports = 2 is a matter of taste; I like it.

==

setup.cfg
==================
36883c119;Aymeric Augustin;2019-12-21 15:50:39 +0100;Add "make style" to format code with black.

==

Makefile
==================
6e5291a91;Aymeric Augustin;2019-12-21 15:49:11 +0100;Enforce black in CI.

==

.circleci/config.yml
==================
fa84ae26d;Aymeric Augustin;2019-12-21 15:46:46 +0100;Reformat source code with black.
This is the result of:

    $ black --line-length 119 examples templates transformers utils hubconf.py setup.py

There's a lot of fairly long lines in the project. As a consequence, I'm
picking the longest widely accepted line length, 119 characters.

This is also Thomas' preference, because it allows for explicit variable
names, to make the code easier to understand.

==

examples/benchmarks.py
examples/contrib/run_camembert.py
examples/contrib/run_openai_gpt.py
examples/contrib/run_swag.py
examples/contrib/run_transfo_xl.py
examples/distillation/distiller.py
examples/distillation/grouped_batch_sampler.py
examples/distillation/lm_seqs_dataset.py
examples/distillation/run_squad_w_distillation.py
examples/distillation/scripts/binarized_data.py
examples/distillation/scripts/extract.py
examples/distillation/scripts/extract_distilbert.py
examples/distillation/scripts/token_counts.py
examples/distillation/train.py
examples/distillation/utils.py
examples/mm-imdb/run_mmimdb.py
examples/mm-imdb/utils_mmimdb.py
examples/pplm/pplm_classification_head.py
examples/pplm/run_pplm.py
examples/pplm/run_pplm_discrim_train.py
examples/run_bertology.py
examples/run_generation.py
examples/run_glue.py
examples/run_lm_finetuning.py
examples/run_multiple_choice.py
examples/run_ner.py
examples/run_squad.py
examples/run_tf_glue.py
examples/run_tf_ner.py
examples/run_xnli.py
examples/summarization/convert_bertabs_original_pytorch_checkpoint.py
examples/summarization/modeling_bertabs.py
examples/summarization/run_summarization.py
examples/summarization/utils_summarization.py
examples/summarization/utils_summarization_test.py
examples/test_examples.py
examples/utils_multiple_choice.py
examples/utils_ner.py
hubconf.py
setup.py
templates/adding_a_new_example_script/run_xxx.py
templates/adding_a_new_example_script/utils_xxx.py
templates/adding_a_new_model/configuration_xxx.py
templates/adding_a_new_model/convert_xxx_original_tf_checkpoint_to_pytorch.py
templates/adding_a_new_model/modeling_tf_xxx.py
templates/adding_a_new_model/modeling_xxx.py
templates/adding_a_new_model/tests/modeling_tf_xxx_test.py
templates/adding_a_new_model/tests/modeling_xxx_test.py
templates/adding_a_new_model/tests/tokenization_xxx_test.py
templates/adding_a_new_model/tokenization_xxx.py
transformers/__init__.py
transformers/__main__.py
transformers/commands/__init__.py
transformers/commands/convert.py
transformers/commands/download.py
transformers/commands/run.py
transformers/commands/serving.py
transformers/commands/train.py
transformers/commands/user.py
transformers/configuration_albert.py
transformers/configuration_auto.py
transformers/configuration_bert.py
transformers/configuration_camembert.py
transformers/configuration_ctrl.py
transformers/configuration_distilbert.py
transformers/configuration_gpt2.py
transformers/configuration_mmbt.py
transformers/configuration_openai.py
transformers/configuration_roberta.py
transformers/configuration_t5.py
transformers/configuration_transfo_xl.py
transformers/configuration_utils.py
transformers/configuration_xlm.py
transformers/configuration_xlm_roberta.py
transformers/configuration_xlnet.py
transformers/convert_albert_original_tf_checkpoint_to_pytorch.py
transformers/convert_bert_original_tf_checkpoint_to_pytorch.py
transformers/convert_bert_pytorch_checkpoint_to_original_tf.py
transformers/convert_gpt2_original_tf_checkpoint_to_pytorch.py
transformers/convert_openai_original_tf_checkpoint_to_pytorch.py
transformers/convert_pytorch_checkpoint_to_tf2.py
transformers/convert_roberta_original_pytorch_checkpoint_to_pytorch.py
transformers/convert_t5_original_tf_checkpoint_to_pytorch.py
transformers/convert_transfo_xl_original_tf_checkpoint_to_pytorch.py
transformers/convert_xlm_original_pytorch_checkpoint_to_pytorch.py
transformers/convert_xlnet_original_tf_checkpoint_to_pytorch.py
transformers/data/__init__.py
transformers/data/metrics/__init__.py
transformers/data/metrics/squad_metrics.py
transformers/data/processors/__init__.py
transformers/data/processors/glue.py
transformers/data/processors/squad.py
transformers/data/processors/utils.py
transformers/data/processors/xnli.py
transformers/file_utils.py
transformers/hf_api.py
transformers/modelcard.py
transformers/modeling_albert.py
transformers/modeling_auto.py
transformers/modeling_bert.py
transformers/modeling_camembert.py
transformers/modeling_ctrl.py
transformers/modeling_distilbert.py
transformers/modeling_encoder_decoder.py
transformers/modeling_gpt2.py
transformers/modeling_mmbt.py
transformers/modeling_openai.py
transformers/modeling_roberta.py
transformers/modeling_t5.py
transformers/modeling_tf_albert.py
transformers/modeling_tf_auto.py
transformers/modeling_tf_bert.py
transformers/modeling_tf_ctrl.py
transformers/modeling_tf_distilbert.py
transformers/modeling_tf_gpt2.py
transformers/modeling_tf_openai.py
transformers/modeling_tf_pytorch_utils.py
transformers/modeling_tf_roberta.py
transformers/modeling_tf_t5.py
transformers/modeling_tf_transfo_xl.py
transformers/modeling_tf_transfo_xl_utilities.py
transformers/modeling_tf_utils.py
transformers/modeling_tf_xlm.py
transformers/modeling_tf_xlnet.py
transformers/modeling_transfo_xl.py
transformers/modeling_transfo_xl_utilities.py
transformers/modeling_utils.py
transformers/modeling_xlm.py
transformers/modeling_xlm_roberta.py
transformers/modeling_xlnet.py
transformers/optimization.py
transformers/optimization_tf.py
transformers/pipelines.py
transformers/tests/configuration_common_test.py
transformers/tests/hf_api_test.py
transformers/tests/model_card_test.py
transformers/tests/modeling_albert_test.py
transformers/tests/modeling_auto_test.py
transformers/tests/modeling_bert_test.py
transformers/tests/modeling_common_test.py
transformers/tests/modeling_ctrl_test.py
transformers/tests/modeling_distilbert_test.py
transformers/tests/modeling_encoder_decoder_test.py
transformers/tests/modeling_gpt2_test.py
transformers/tests/modeling_openai_test.py
transformers/tests/modeling_roberta_test.py
transformers/tests/modeling_t5_test.py
transformers/tests/modeling_tf_albert_test.py
transformers/tests/modeling_tf_auto_test.py
transformers/tests/modeling_tf_bert_test.py
transformers/tests/modeling_tf_common_test.py
transformers/tests/modeling_tf_ctrl_test.py
transformers/tests/modeling_tf_distilbert_test.py
transformers/tests/modeling_tf_gpt2_test.py
transformers/tests/modeling_tf_openai_gpt_test.py
transformers/tests/modeling_tf_roberta_test.py
transformers/tests/modeling_tf_t5_test.py
transformers/tests/modeling_tf_transfo_xl_test.py
transformers/tests/modeling_tf_xlm_test.py
transformers/tests/modeling_tf_xlnet_test.py
transformers/tests/modeling_transfo_xl_test.py
transformers/tests/modeling_xlm_test.py
transformers/tests/modeling_xlnet_test.py
transformers/tests/optimization_test.py
transformers/tests/optimization_tf_test.py
transformers/tests/pipelines_test.py
transformers/tests/tokenization_albert_test.py
transformers/tests/tokenization_auto_test.py
transformers/tests/tokenization_bert_japanese_test.py
transformers/tests/tokenization_bert_test.py
transformers/tests/tokenization_ctrl_test.py
transformers/tests/tokenization_distilbert_test.py
transformers/tests/tokenization_gpt2_test.py
transformers/tests/tokenization_openai_test.py
transformers/tests/tokenization_roberta_test.py
transformers/tests/tokenization_t5_test.py
transformers/tests/tokenization_tests_commons.py
transformers/tests/tokenization_transfo_xl_test.py
transformers/tests/tokenization_utils_test.py
transformers/tests/tokenization_xlm_test.py
transformers/tests/tokenization_xlnet_test.py
transformers/tests/utils.py
transformers/tokenization_albert.py
transformers/tokenization_auto.py
transformers/tokenization_bert.py
transformers/tokenization_bert_japanese.py
transformers/tokenization_camembert.py
transformers/tokenization_ctrl.py
transformers/tokenization_distilbert.py
transformers/tokenization_gpt2.py
transformers/tokenization_openai.py
transformers/tokenization_roberta.py
transformers/tokenization_t5.py
transformers/tokenization_transfo_xl.py
transformers/tokenization_utils.py
transformers/tokenization_xlm.py
transformers/tokenization_xlm_roberta.py
transformers/tokenization_xlnet.py
utils/download_glue_data.py
utils/link_tester.py
==================
63e3827c6;Aymeric Augustin;2019-12-21 15:38:08 +0100;Remove empty file.
Likely it was added by accident.

==

try.py
==================
645713e2c;Thomas Wolf;2019-12-21 15:33:22 +0100;Merge pull request #2254 from huggingface/fix-tfroberta
adding positional embeds masking to TFRoBERTa
==
==================
73f6e9817;Thomas Wolf;2019-12-21 15:26:08 +0100;Merge pull request #2115 from suvrat96/add_mmbt_model
[WIP] Add MMBT Model to Transformers Repo
==
==================
77676c27d;thomwolf;2019-12-21 15:24:48 +0100;adding positional embeds masking to TFRoBERTa

==

transformers/modeling_tf_roberta.py
==================
344126fe5;thomwolf;2019-12-21 15:06:52 +0100;move example to mm-imdb folder

==

examples/README.md
examples/mm-imdb/run_mmimdb.py
examples/mm-imdb/utils_mmimdb.py
==================
5b7fb6a4a;Thomas Wolf;2019-12-21 15:03:53 +0100;Merge pull request #2134 from bkkaggle/saving-and-resuming
closes #1960 Add saving and resuming functionality for remaining examples
==
==================
6f68d559a;Thomas Wolf;2019-12-21 14:55:40 +0100;Merge pull request #2130 from huggingface/ignored-index-coherence
[BREAKING CHANGE] Setting all ignored index to the PyTorch standard
==
==================
1ab25c49d;thomwolf;2019-12-21 14:54:30 +0100;Merge branch 'master' into pr/2115

==
==================
b03872aae;thomwolf;2019-12-21 14:49:54 +0100;fix merge

==

examples/run_squad.py
==================
518ba748e;Thomas Wolf;2019-12-21 14:41:39 +0100;Merge branch 'master' into saving-and-resuming

==
==================
18601c3b6;Thomas Wolf;2019-12-21 14:33:16 +0100;Merge pull request #2173 from erenup/master
run_squad with roberta
==
==================
6e7102cfb;Thomas Wolf;2019-12-21 14:31:44 +0100;Merge pull request #2203 from gthb/patch-1
fix: wrong architecture count in README
==
==================
deceb0016;Thomas Wolf;2019-12-21 14:31:20 +0100;Merge pull request #2177 from mandubian/issue-2106
:zip: #2106 tokenizer.tokenize speed improvement (3-8x) by caching added_tokens in a Set
==
==================
eeb70cdd7;Thomas Wolf;2019-12-21 14:29:59 +0100;Merge branch 'master' into saving-and-resuming

==
==================
ed9b84816;Thomas Wolf;2019-12-21 14:27:35 +0100;Merge pull request #1840 from huggingface/generation_sampler
[WIP] Sampling sequence generator for transformers
==
==================
f86ed2318;thomwolf;2019-12-21 14:13:06 +0100;update doc

==

transformers/modeling_utils.py
==================
cfa038051;thomwolf;2019-12-21 14:12:52 +0100;Merge branch 'master' into generation_sampler

==
==================
300ec3003;thomwolf;2019-12-21 14:02:19 +0100;fixing run_generation example - using torch.no_grad

==

examples/run_generation.py
transformers/configuration_xlm.py
transformers/modeling_utils.py
transformers/modeling_xlm.py
==================
1c3774689;thomwolf;2019-12-21 13:52:49 +0100;fixing run_generation

==

examples/run_generation.py
transformers/configuration_utils.py
transformers/modeling_utils.py
==================
7e17f09fb;Thomas Wolf;2019-12-21 13:38:48 +0100;Merge pull request #1803 from importpandas/fix-xlnet-squad2.0
fix run_squad.py during fine-tuning xlnet on squad2.0
==
==================
8a2be93b4;thomwolf;2019-12-21 13:31:28 +0100;fix merge

==

examples/run_squad.py
==================
562f86403;Thomas Wolf;2019-12-21 12:48:10 +0100;Merge branch 'master' into fix-xlnet-squad2.0

==
==================
8618bf15d;Thomas Wolf;2019-12-21 12:42:05 +0100;Merge pull request #1736 from huggingface/fix-tf-xlnet
Fix TFXLNet
==
==================
2fa8737c4;Thomas Wolf;2019-12-21 12:36:11 +0100;Merge pull request #1586 from enzoampil/include_special_tokens_in_bert_examples
Add special tokens to documentation for bert examples to resolve issue: #1561
==
==================
f15f08714;Thomas Wolf;2019-12-21 12:13:27 +0100;Merge pull request #1764 from DomHudson/bug-fix-1761
Bug-fix: Roberta Embeddings Not Masked
==
==================
fae4d1c26;Thomas Wolf;2019-12-21 11:54:23 +0100;Merge pull request #2217 from aaugustin/test-parallelization
Support running tests in parallel
==
==================
b8e924e10;Aymeric Augustin;2019-12-21 08:50:15 +0100;Restore test.
This looks like debug code accidentally committed in b18509c2.

Refs #2250.

==

transformers/tests/modeling_tf_albert_test.py
==================
767bc3ca6;Aymeric Augustin;2019-12-21 08:46:26 +0100;Fix typo in model name.
This looks like a copy/paste mistake. Probably this test was never run.

Refs #2250.

==

transformers/tests/modeling_tf_xlm_test.py
==================
343c094f2;Aymeric Augustin;2019-12-20 20:56:59 +0100;Run examples separately from tests.
This optimizes the total run time of the Circle CI test suite.

==

.circleci/config.yml
==================
80caf79d0;Aymeric Augustin;2019-12-20 20:56:59 +0100;Prevent excessive parallelism in PyTorch.
We're already using as many processes in parallel as we have CPU cores.
Furthermore, the number of core may be incorrectly calculated as 36
(we've seen this in pytest-xdist) which make compound the problem.

PyTorch performance craters without this.

==

.circleci/config.yml
==================
bb3bfa2d2;Aymeric Augustin;2019-12-20 20:56:59 +0100;Distribute tests from the same file to the same worker.
This should prevent two issues:

- hitting API rate limits for tests that hit the HF API
- multiplying the cost of expensive test setups

==

.circleci/config.yml
==================
29cbab98f;Aymeric Augustin;2019-12-20 20:56:59 +0100;Parallelize tests on Circle CI.
Set the number of CPUs manually based on the Circle CI resource class,
or else we're getting 36 CPUs, which is far too much (perhaps that's
the underlying hardware and not what Circle CI allocates to us).

Don't parallelize the custom tokenizers tests because they take less
than one second to run and parallelization actually makes them slower.

==

.circleci/config.yml
==================
a4c9338b8;Aymeric Augustin;2019-12-20 20:56:59 +0100;Prevent parallel downloads of the same file with a lock.
Since the file is written to the filesystem, a filesystem lock is the
way to go here. Add a dependency on the third-party filelock library to
get cross-platform functionality.

==

setup.py
transformers/file_utils.py
==================
b670c2668;Aymeric Augustin;2019-12-20 20:56:58 +0100;Take advantage of the cache when running tests.
Caching models across test cases and across runs of the test suite makes
slow tests somewhat more bearable.

Use gettempdir() instead of /tmp in tests. This makes it easier to
change the location of the cache with semi-standard TMPDIR/TEMP/TMP
environment variables.

Fix #2222.

==

templates/adding_a_new_model/tests/modeling_tf_xxx_test.py
templates/adding_a_new_model/tests/modeling_xxx_test.py
transformers/tests/modeling_albert_test.py
transformers/tests/modeling_bert_test.py
transformers/tests/modeling_common_test.py
transformers/tests/modeling_ctrl_test.py
transformers/tests/modeling_distilbert_test.py
transformers/tests/modeling_gpt2_test.py
transformers/tests/modeling_openai_test.py
transformers/tests/modeling_roberta_test.py
transformers/tests/modeling_t5_test.py
transformers/tests/modeling_tf_albert_test.py
transformers/tests/modeling_tf_auto_test.py
transformers/tests/modeling_tf_bert_test.py
transformers/tests/modeling_tf_ctrl_test.py
transformers/tests/modeling_tf_distilbert_test.py
transformers/tests/modeling_tf_gpt2_test.py
transformers/tests/modeling_tf_openai_gpt_test.py
transformers/tests/modeling_tf_roberta_test.py
transformers/tests/modeling_tf_t5_test.py
transformers/tests/modeling_tf_transfo_xl_test.py
transformers/tests/modeling_tf_xlm_test.py
transformers/tests/modeling_tf_xlnet_test.py
transformers/tests/modeling_transfo_xl_test.py
transformers/tests/modeling_xlm_test.py
transformers/tests/modeling_xlnet_test.py
transformers/tests/utils.py
==================
b67fa1a8d;Aymeric Augustin;2019-12-20 20:56:58 +0100;Download models directly to cache_dir.
This allows moving the file instead of copying it, which is more
reliable. Also it avoids writing large amounts of data to /tmp,
which may not be large enough to accomodate it.

Refs #2222.

==

transformers/file_utils.py
==================
286d5bb6b;Aymeric Augustin;2019-12-20 20:56:58 +0100;Use a random temp dir for writing pruned models in tests.

==

transformers/tests/modeling_common_test.py
==================
478e456e8;Aymeric Augustin;2019-12-20 20:56:58 +0100;Use a random temp dir for writing file in tests.

==

transformers/tests/modeling_common_test.py
==================
12726f855;Aymeric Augustin;2019-12-20 20:56:58 +0100;Remove redundant torch.jit.trace in tests.
This looks like it could be expensive, so don't run it twice.

==

transformers/tests/modeling_common_test.py
==================
ac1b449cc;Julien Chaumond;2019-12-21 00:09:01 -0500;[doc] move distilroberta to more appropriate place
cc @lysandrejik

==

docs/source/pretrained_models.rst
==================
3e52915fa;Julien Chaumond;2019-12-20 19:01:27 -0500;[RoBERTa] Embeddings: fix dimensionality bug

==

transformers/modeling_roberta.py
transformers/tests/modeling_roberta_test.py
==================
228f52867;Dom Hudson;2019-11-07 19:58:17 +0000;Bug fix: 1764

==

transformers/modeling_roberta.py
transformers/tests/modeling_roberta_test.py
==================
a80778f40;Francesco;2019-12-18 16:05:28 +0100;small refactoring (only esthetic, not functional)

==

transformers/modeling_encoder_decoder.py
==================
3df1d2d14;Francesco;2019-12-17 10:19:54 +0100;- Create the output directory (whose name is passed by the user in the "save_directory" parameter) where it will be saved encoder and decoder, if not exists. - Empty the output directory, if it contains any files or subdirectories. - Create the "encoder" directory inside "save_directory", if not exists. - Create the "decoder" directory inside "save_directory", if not exists. - Save the encoder and the decoder in the previous two directories, respectively.

==

transformers/modeling_encoder_decoder.py
==================
a436574bf;Lysandre;2019-12-20 16:22:20 -0500;Release: v2.3.0

==

README.md
docs/source/conf.py
setup.py
transformers/__init__.py
try.py
==================
d0f8b9a97;Thomas Wolf;2019-12-20 22:10:39 +0100;Merge pull request #2244 from huggingface/fix-tok-pipe
Fix Camembert and XLM-R `decode` method- Fix NER pipeline alignement
==
==================
a557836a7;Thomas Wolf;2019-12-20 22:08:08 +0100;Merge pull request #2191 from huggingface/fix_sp_np
Numpy compatibility for sentence piece
==
==================
655fd0685;thomwolf;2019-12-20 21:57:49 +0100;clean up

==

transformers/commands/run.py
==================
e5812462f;thomwolf;2019-12-20 21:51:48 +0100;clean up debug and less verbose tqdm

==

transformers/file_utils.py
transformers/pipelines.py
==================
4775ec354;thomwolf;2019-12-20 21:47:15 +0100;add overwrite - fix ner decoding

==

transformers/commands/run.py
transformers/pipelines.py
==================
cb6d54bfd;Lysandre;2019-12-20 15:06:28 -0500;Numpy compatibility for sentence piece
convert to int earlier

==

transformers/tokenization_utils.py
==================
f79a7dc66;thomwolf;2019-12-20 20:57:45 +0100;fix NER pipeline

==

transformers/pipelines.py
==================
a24101105;thomwolf;2019-12-20 20:43:48 +0100;fix pipeline NER

==

transformers/pipelines.py
==================
e37ca8e11;thomwolf;2019-12-20 20:43:42 +0100;fix camembert and XLM-R tokenizer

==

transformers/tokenization_camembert.py
transformers/tokenization_xlm_roberta.py
==================
ceae85ad6;thomwolf;2019-12-20 19:52:24 +0100;fix mc loading

==

transformers/pipelines.py
==================
71883b6dd;thomwolf;2019-12-20 19:40:23 +0100;update link in readme

==

README.md
==================
8d5a47c79;Thomas Wolf;2019-12-20 19:34:08 +0100;Merge pull request #2243 from huggingface/fix-xlm-roberta
fixing xlm-roberta tokenizer max_length and automodels
==
==================
79e4a6a25;thomwolf;2019-12-20 19:33:12 +0100;update serving API

==

transformers/commands/serving.py
==================
bbaaec046;thomwolf;2019-12-20 19:19:20 +0100;fixing CLI pipeline

==

transformers/commands/run.py
transformers/pipelines.py
==================
1c12ee0e5;thomwolf;2019-12-20 18:28:27 +0100;fixing xlm-roberta tokenizer max_length and automodels

==

transformers/modeling_auto.py
transformers/modeling_utils.py
transformers/pipelines.py
transformers/tokenization_utils.py
transformers/tokenization_xlm_roberta.py
==================
65c75fc58;Lysandre;2019-12-20 11:34:16 -0500;Clean special tokens test

==

transformers/tests/tokenization_tests_commons.py
==================
fb393ad99;Lysandre;2019-12-20 11:29:58 -0500;Added test for all special tokens

==

transformers/tests/tokenization_tests_commons.py
==================
90debb9ff;Dirk Groeneveld;2019-12-19 17:03:01 -0800;Keep even the first of the special tokens intact while lowercasing.

==

transformers/tokenization_utils.py
==================
b98ff8854;Morgan Funtowicz;2019-12-20 15:52:50 +0100;Added pipelines quick tour in README

==

README.md
==================
3a2c4e6f6;Thomas Wolf;2019-12-20 15:28:29 +0100;Merge pull request #1548 from huggingface/cli
[2.2] - Command-line interface - Pipeline class
==
==================
4e3f745ba;R√©mi Louf;2019-12-20 11:13:46 +0100;add example for Model2Model in quickstart

==

docs/source/quickstart.md
==================
db0795b5d;thomwolf;2019-12-20 15:07:00 +0100;defaults models for tf and pt - update tests

==

transformers/pipelines.py
transformers/tests/pipelines_test.py
==================
7f7408452;Morgan Funtowicz;2019-12-20 14:47:04 +0100;Fix leading axis added when saving through the command run

==

transformers/commands/run.py
==================
c37815f13;thomwolf;2019-12-20 14:35:40 +0100;clean up PT <=> TF 2.0 conversion and config loading

==

transformers/convert_pytorch_checkpoint_to_tf2.py
transformers/modeling_tf_utils.py
transformers/modeling_utils.py
==================
73fcebf7e;thomwolf;2019-12-20 13:47:35 +0100;update serving command

==

setup.py
transformers-cli
transformers/commands/serving.py
==================
59941c5d1;Thomas Wolf;2019-12-20 13:26:38 +0100;Merge pull request #2189 from stefan-it/xlmr
Add support for XLM-RoBERTa
==
==================
15dda5ea3;thomwolf;2019-12-20 13:20:41 +0100;remove python 2 tests for circle-ci cc @aaugustin @julien-c @LysandreJik

==

.circleci/config.yml
==================
01ffc65e9;thomwolf;2019-12-20 13:16:23 +0100;update tests to remove unittest.patch

==

transformers/pipelines.py
transformers/tests/pipelines_test.py
==================
825697cad;thomwolf;2019-12-20 12:51:10 +0100;fix tests

==

transformers/pipelines.py
==================
1fa93ca1e;thomwolf;2019-12-20 12:34:19 +0100;Clean up framework handling

==

transformers/pipelines.py
==================
ca6bdb28f;thomwolf;2019-12-20 12:10:40 +0100;fix pipelines and rename model_card => modelcard

==

transformers/__init__.py
transformers/file_utils.py
transformers/modelcard.py
transformers/pipelines.py
transformers/tests/model_card_test.py
==================
61d9ee45e;Morgan Funtowicz;2019-12-20 11:47:56 +0100;All tests are green.

==

transformers/pipelines.py
transformers/tests/pipelines_test.py
==================
ff36e6d8d;Thomas Wolf;2019-12-20 10:28:10 +0100;Merge pull request #2231 from huggingface/requests_user_agent
[http] customizable requests user-agent
==
==================
e516a34a1;Morgan Funtowicz;2019-12-20 09:38:08 +0100;Use BasicTokenizer to split over whitespaces.

==

transformers/pipelines.py
==================
9d0d1cd33;Morgan Funtowicz;2019-12-20 09:30:37 +0100;Filter out entity for NER task.

==

transformers/pipelines.py
==================
15d897ff4;Julien Chaumond;2019-12-19 18:29:22 -0500;[http] customizable requests user-agent

==

transformers/file_utils.py
==================
f25e9b6f7;Julien Chaumond;2019-12-19 18:28:17 -0500;[hf_bucket_url] support for cloudfront urls

==

transformers/file_utils.py
==================
a5a06a851;Julien Chaumond;2019-12-19 16:24:20 -0500;[doc] Param name consistency

==

examples/run_lm_finetuning.py
==================
1718fb9e7;Aidan Kierans;2019-12-19 16:23:18 -0500;Minor/basic text fixes (#2229)
* Small clarification

Matches line 431 to line 435 for additional clarity and consistency.

* Fixed minor typo

The letter "s" was previously omitted from the word "docstrings".

==

CONTRIBUTING.md
examples/run_lm_finetuning.py
==================
9a399ead2;Julien Chaumond;2019-12-19 15:45:48 -0500;Revert incorrect #1778

==

transformers/modeling_tf_pytorch_utils.py
transformers/modeling_utils.py
==================
3376adc05;Stefan Schweter;2019-12-19 21:30:23 +0100;configuration/modeling/tokenization: add various fine-tuned XLM-RoBERTa models for English, German, Spanish and Dutch (CoNLL datasets)

==

transformers/configuration_xlm_roberta.py
transformers/modeling_xlm_roberta.py
transformers/tokenization_xlm_roberta.py
==================
e4baa68dd;thomwolf;2019-12-19 20:37:26 +0100;tick-tock cc @julien-c

==

.circleci/config.yml
==================
149dc376a;thomwolf;2019-12-19 20:34:28 +0100;fix tests

==

transformers/configuration_utils.py
==================
407093b3f;thomwolf;2019-12-19 20:26:51 +0100;Merge branch 'cli' of https://github.com/huggingface/transformers into cli

==
==================
c7be096c3;thomwolf;2019-12-19 20:26:08 +0100;Merge branch 'master' into cli

==
==================
a305067f2;Morgan Funtowicz;2019-12-19 19:41:48 +0100;Removed __main__

==

setup.py
transformers/__main__.py
==================
3492a6ec1;Morgan Funtowicz;2019-12-19 19:06:44 +0100;Addressing Thom's comments.

==

transformers/pipelines.py
==================
33adab2b9;Lysandre;2019-12-19 12:40:43 -0500;Fix albert example

==

transformers/modeling_tf_albert.py
transformers/modeling_utils.py
==================
a1f1dce0a;Lysandre;2019-12-19 12:25:55 -0500;Correct max position for SQUAD and TFDS

==

transformers/data/processors/squad.py
==================
62c1fc3c1;Francesco;2019-12-19 14:43:10 +0100;Removed duplicate XLMConfig, XLMForQuestionAnswering and XLMTokenizer from import statement of run_squad.py script

==

examples/run_squad.py
==================
284572efc;Ejar;2019-12-18 17:47:47 +0100;Updated typo on the link
Updated documentation due to typo
==

examples/README.md
==================
ed6ba9391;patrickvonplaten;2019-12-19 01:26:01 +0100;corrected typo in example for t5 model input argument

==

transformers/modeling_t5.py
transformers/modeling_tf_t5.py
==================
81a911cce;Morgan Funtowicz;2019-12-19 15:12:06 +0100;Doc, doc, ... doc.

==

transformers/pipelines.py
==================
faef6f619;Morgan Funtowicz;2019-12-19 12:28:17 +0100;Fix logic order for USE_TF/USE_TORCH

==

transformers/file_utils.py
==================
5664327c2;Morgan Funtowicz;2019-12-19 12:27:54 +0100;Hide train command for now.

==

transformers-cli
==================
3b29322d4;Morgan Funtowicz;2019-12-19 12:24:17 +0100;Expose all the pipeline argument on serve command.

==

transformers/commands/serving.py
==================
fc624716a;Morgan Funtowicz;2019-12-19 11:49:06 +0100;Renaming framework env variables flags from NO_ to USE_

==

transformers/file_utils.py
==================
f516cf395;Morgan Funtowicz;2019-12-19 11:42:33 +0100;Allow pipeline to write output in binary format

==

transformers/commands/run.py
transformers/pipelines.py
==================
d72fa2a0f;Morgan Funtowicz;2019-12-19 10:54:10 +0100;Fix inputs_for_model call in QuestionAnsweringPipeline accessing __dict__ on list.

==

transformers/pipelines.py
==================
bcc99fd92;Morgan Funtowicz;2019-12-19 10:32:21 +0100;Fix wrong automatic config allocation through AutoConfig

==

transformers/pipelines.py
==================
a26ce4dee;Stefan Schweter;2019-12-19 02:23:01 +0100;examples: add XLM-RoBERTa to glue script

==

examples/run_glue.py
==================
ec5d6c6a7;Morgan Funtowicz;2019-12-19 00:12:10 +0100;Adressing issue with NER task omitting first and last word.

==

transformers/pipelines.py
==================
fe9aab105;Stefan Schweter;2019-12-18 23:47:48 +0100;tokenization: use S3 location for XLM-RoBERTa model

==

transformers/tokenization_xlm_roberta.py
==================
5c5f67a25;Stefan Schweter;2019-12-18 23:47:00 +0100;modeling: use S3 location for XLM-RoBERTa model

==

transformers/modeling_xlm_roberta.py
==================
db90e1211;Stefan Schweter;2019-12-18 23:46:33 +0100;configuration: use S3 location for XLM-RoBERTa model

==

transformers/configuration_xlm_roberta.py
==================
d0724d079;Morgan Funtowicz;2019-12-18 23:27:26 +0100;Add PipedPipelineDataFormat

==

transformers/__init__.py
transformers/commands/run.py
transformers/pipelines.py
==================
7711403bb;Morgan Funtowicz;2019-12-18 22:59:51 +0100;Expose config through the cli arguments

==

transformers/commands/run.py
==================
8bb166db5;Morgan Funtowicz;2019-12-18 22:53:19 +0100;Expose more information in the output of TextClassificationPipeline

==

transformers/pipelines.py
==================
f09d99964;Stefan Schweter;2019-12-18 19:49:33 +0100;docs: fix numbering üòÖ

==

docs/source/index.rst
==================
dd7a958fd;Stefan Schweter;2019-12-18 19:45:46 +0100;docs: add XLM-RoBERTa to pretrained model list (incl. all parameters)

==

docs/source/pretrained_models.rst
==================
d35405b7a;Stefan Schweter;2019-12-18 19:45:10 +0100;docs: add XLM-RoBERTa to index page

==

docs/source/index.rst
==================
3e89fca54;Stefan Schweter;2019-12-18 19:44:23 +0100;readme: add XLM-RoBERTa to model architecture list

==

README.md
==================
128cfdee9;Stefan Schweter;2019-12-18 19:28:16 +0100;tokenization add XLM-RoBERTa base model

==

transformers/tokenization_xlm_roberta.py
==================
e778dd854;Stefan Schweter;2019-12-18 19:27:34 +0100;modeling: add XLM-RoBERTa base model

==

transformers/modeling_xlm_roberta.py
==================
04b602f96;Morgan Funtowicz;2019-12-18 18:28:39 +0100;Put module import on top of the module.

==

transformers/pipelines.py
==================
64a971a91;Stefan Schweter;2019-12-18 18:24:32 +0100;auto: add XLM-RoBERTa to auto tokenization

==

transformers/tokenization_auto.py
==================
036831e27;Stefan Schweter;2019-12-18 18:23:42 +0100;auto: add XLM-RoBERTa to audo modeling

==

transformers/modeling_auto.py
==================
41a13a637;Stefan Schweter;2019-12-18 18:20:27 +0100;auto: add XLMRoBERTa to auto configuration

==

transformers/configuration_auto.py
==================
0c88c856d;Morgan Funtowicz;2019-12-18 18:18:16 +0100;Unnest QuestionAnsweringArgumentHandler

==

transformers/pipelines.py
==================
8efc6dd54;Lysandre;2019-12-18 10:47:59 -0500;fix #2214

==

transformers/configuration_xlm.py
==================
a2978465a;Gunnlaugur Thor Briem;2019-12-18 14:54:46 +0000;Merge branch 'master' into patch-1

==
==================
01b68be34;Stefan Schweter;2019-12-18 12:24:46 +0100;converter: remove XLM-RoBERTa specific script (can be done with the script for RoBERTa now)

==

transformers/convert_xlm_roberta_original_pytorch_checkpoint_to_pytorch.py
==================
3d2096f51;thomwolf;2019-12-18 11:50:54 +0100;further cleanup

==

examples/run_generation.py
transformers/configuration_xlm.py
transformers/modeling_utils.py
transformers/modeling_xlm.py
transformers/modeling_xlnet.py
transformers/tokenization_utils.py
==================
ca31abc6d;Stefan Schweter;2019-12-18 11:36:54 +0100;tokenization: *align* fairseq and spm vocab to fix some tokenization errors

==

transformers/tokenization_xlm_roberta.py
==================
8e5587fb7;thomwolf;2019-12-18 11:32:37 +0100;few fixes on sampling

==

transformers/modeling_utils.py
==================
cce3089b6;Stefan Schweter;2019-12-18 11:05:16 +0100;Merge remote-tracking branch 'upstream/master' into xlmr

==
==================
641a8decd;thomwolf;2019-12-18 10:43:48 +0100;clean up code and add arbitrary number of return sequences

==

transformers/configuration_utils.py
transformers/modeling_encoder_decoder.py
transformers/modeling_utils.py
transformers/tests/sampling_test.py
==================
e347725d8;Morgan Funtowicz;2019-12-17 23:32:52 +0100;More fine-grained control over pipeline creation with config argument.

==

transformers/pipelines.py
==================
94c99db34;Julien Chaumond;2019-12-17 20:34:22 -0500;[FinBERT] fix incorrect url

==

transformers/tokenization_bert.py
==================
7ffa81739;Julien Chaumond;2019-12-16 18:55:14 -0500;[s3] mv files and update links

==

transformers/configuration_bert.py
transformers/modeling_bert.py
transformers/modeling_tf_bert.py
transformers/tokenization_bert.py
==================
c5f35e61d;Antti Virtanen;2019-12-16 21:06:14 +0200;Uploaded files to AWS.

==

transformers/configuration_bert.py
transformers/modeling_bert.py
transformers/modeling_tf_bert.py
transformers/tokenization_bert.py
==================
abc43ffbf;Antti Virtanen;2019-12-16 18:08:00 +0200;Add pretrained model documentation for FinBERT.

==

docs/source/pretrained_models.rst
==================
8ac840ff8;Antti Virtanen;2019-12-16 17:08:25 +0200;Adding Finnish BERT.

==

transformers/configuration_bert.py
transformers/modeling_bert.py
transformers/modeling_tf_bert.py
transformers/tokenization_bert.py
==================
a0d386455;Julien Chaumond;2019-12-17 20:07:39 -0500;Fix outdated tokenizer doc

==

templates/adding_a_new_model/tokenization_xxx.py
transformers/tokenization_bert.py
transformers/tokenization_distilbert.py
==================
ea636440d;Julien Chaumond;2019-12-17 18:06:42 -0500;[roberta.conversion] Do not hardcode vocab size
and support for fairseq 0.9+

==

transformers/convert_roberta_original_pytorch_checkpoint_to_pytorch.py
==================
a4df2e011;Arman Cohan;2019-11-26 16:03:07 -0800;update roberta conversion
- update to fix conversion for the updated fairseq model
- create save directory if not exist

==

transformers/convert_roberta_original_pytorch_checkpoint_to_pytorch.py
==================
77d397202;thomwolf;2019-12-17 23:28:46 +0100;clean up dead code

==

transformers/modeling_utils.py
==================
bbc0c86f9;thomwolf;2019-12-17 23:27:02 +0100;beam search + single beam decoding

==

transformers/modeling_utils.py
==================
5e289f69b;Lysandre;2019-12-17 14:17:11 -0500;regex 2019.12.17 install fails with Python 2

==

requirements.txt
setup.py
==================
2cff4bd8f;Lysandre;2019-12-17 14:01:04 -0500;Fix segmentation fault

==

transformers/file_utils.py
==================
55397dfb9;Julien Chaumond;2019-12-17 13:10:51 -0500;CsvPipelineDataFormat: Fix for single-column

==

transformers/pipelines.py
==================
b6938916a;thomwolf;2019-12-17 17:23:36 +0100;adding beam search

==

transformers/configuration_utils.py
transformers/modeling_utils.py
==================
d303f84e7;Gunnlaugur Thor Briem;2019-12-17 16:18:00 +0000;fix: wrong architecture count in README
Just say ‚Äúthe following‚Äù so that this intro doesn't so easily fall out of date :) )
==

README.md
==================
2fde5a248;Morgan Funtowicz;2019-12-17 12:16:07 +0100;Initial bunch of documentation.

==

transformers/pipelines.py
==================
2f1c745cd;thomwolf;2019-12-17 11:47:54 +0100;update conversion script

==

transformers/convert_xlm_roberta_original_pytorch_checkpoint_to_pytorch.py
==================
83bc5235c;thomwolf;2019-12-17 11:47:32 +0100;Merge branch 'master' into pr/2189

==
==================
d7c62661a;Morgan Funtowicz;2019-12-17 11:23:39 +0100;Provide serving dependencies for tensorflow and pytorch (serving-tf, serving-torch)

==

setup.py
==================
f349826a5;Stefan Schweter;2019-12-17 10:36:04 +0100;model: fix cls and sep token for XLM-RoBERTa documentation

==

transformers/modeling_xlm_roberta.py
==================
f06160627;Thomas Wolf;2019-12-17 09:10:16 +0100;Merge pull request #2164 from huggingface/cleanup-configs
[SMALL BREAKING CHANGE] Cleaning up configuration classes - Adding Model Cards
==
==================
805c21aeb;erenup;2019-12-17 11:36:00 +0800;tried to fix the failed checks

==

transformers/modeling_roberta.py
==================
d000195ee;erenup;2019-12-17 11:28:34 +0800;add comment for example_index and unique_id in single process

==

transformers/data/processors/squad.py
transformers/modeling_roberta.py
==================
3c6efd0ca;erenup;2019-12-17 11:18:12 +0800;updated usage example in modeling_roberta for question and answering

==

transformers/modeling_roberta.py
==================
3f5ccb183;Julien Chaumond;2019-12-16 18:20:23 -0500;[doc] Clarify uploads
cf https://github.com/huggingface/transformers/commit/855ff0e91d8b3bd75a3b1c1316e2efd814373764#commitcomment-36452545

==

README.md
docs/source/model_sharing.md
==================
3cb51299c;thomwolf;2019-12-16 22:32:05 +0100;Fix #2109

==

transformers/modeling_tf_pytorch_utils.py
transformers/modeling_tf_utils.py
==================
18a879f47;Lysandre;2019-12-16 16:44:29 -0500;fix #2180

==

examples/run_generation.py
==================
d80340921;Lysandre;2019-12-16 16:31:38 -0500;Fix run squad evaluate during training

==

examples/run_squad.py
==================
a468870fd;thomwolf;2019-12-16 22:22:30 +0100;refactoring generation

==

transformers/configuration_utils.py
transformers/modeling_utils.py
==================
855ff0e91;Julien Chaumond;2019-12-16 12:42:22 -0500;[doc] Model upload and sharing
ping @lysandrejik @thomwolf

Is this clear enough? Anything we should add?

==

README.md
docs/source/index.rst
docs/source/model_sharing.md
==================
d064009b7;Stefan Schweter;2019-12-16 17:23:25 +0100;converter: fix vocab size

==

transformers/convert_xlm_roberta_original_pytorch_checkpoint_to_pytorch.py
==================
a701a0cee;Stefan Schweter;2019-12-16 17:17:56 +0100;configuration: fix model name for large XLM-RoBERTa model

==

transformers/configuration_xlm_roberta.py
==================
59a1aefb1;Stefan Schweter;2019-12-16 17:00:55 +0100;tokenization: add support for new XLM-RoBERTa model. Add wrapper around fairseq tokenization logic

==

transformers/tokenization_xlm_roberta.py
==================
69f4f058f;Stefan Schweter;2019-12-16 17:00:12 +0100;model: add support for new XLM-RoBERTa model

==

transformers/modeling_xlm_roberta.py
==================
a648ff738;Stefan Schweter;2019-12-16 16:47:39 +0100;configuration: add support for XLM-RoBERTa model

==

transformers/configuration_xlm_roberta.py
==================
9ed09cb4a;Stefan Schweter;2019-12-16 16:46:58 +0100;converter: add conversion script for original XLM-RoBERTa weights to Transformers-compatible weights

==

transformers/convert_xlm_roberta_original_pytorch_checkpoint_to_pytorch.py
==================
d3549b66a;Stefan Schweter;2019-12-16 16:38:39 +0100;module: add support for XLM-RoBERTa (__init__)

==

transformers/__init__.py
==================
a096e2a88;Morgan Funtowicz;2019-12-16 16:38:02 +0100;WIP serving through HTTP internally using pipelines.

==

transformers/commands/serving.py
==================
71b475051;Stefan Schweter;2019-12-16 16:37:27 +0100;examples: add support for XLM-RoBERTa to run_ner script

==

examples/run_ner.py
==================
43a4e1bbe;Morgan Funtowicz;2019-12-16 16:00:41 +0100;Adressing issue in varargs handling for question answering.

==

transformers/pipelines.py
==================
46ccbb42f;Morgan Funtowicz;2019-12-16 15:49:41 +0100;Make CLI run command use integer mapping for device argument.

==

transformers/commands/run.py
==================
bbc707cf3;Morgan Funtowicz;2019-12-16 15:49:09 +0100;Fix non-keyworded varargs handling in DefaultArgumentHandler for pipeline.

==

transformers/pipelines.py
==================
9c391277c;Morgan Funtowicz;2019-12-16 15:19:13 +0100;Allow tensors placement on specific device through CLI and pipeline.

==

transformers/commands/run.py
transformers/pipelines.py
==================
1bbdbacd5;thomwolf;2019-12-16 14:38:20 +0100;update __init__ and saving

==

transformers/__init__.py
transformers/model_card.py
==================
955d7ecb5;Morgan Funtowicz;2019-12-16 14:34:54 +0100;Refactored Pipeline with dedicated argument handler.

==

transformers/pipelines.py
==================
031ad4eb3;thomwolf;2019-12-16 14:20:57 +0100;improving JSON error messages (for model card and configurations)

==

transformers/configuration_utils.py
transformers/model_card.py
==================
db0a9ee6e;thomwolf;2019-12-16 14:08:08 +0100;adding albert to TF auto models cc @LysandreJik

==

transformers/modeling_tf_auto.py
==================
a4d07b983;thomwolf;2019-12-16 14:00:32 +0100;dict of all config and model files cc @LysandreJik

==

transformers/__init__.py
transformers/configuration_auto.py
transformers/model_card.py
transformers/modeling_auto.py
transformers/modeling_tf_auto.py
==================
d3418a94f;thomwolf;2019-12-16 13:52:41 +0100;update tests

==

transformers/tests/configuration_common_test.py
transformers/tests/model_card_test.py
==================
56e98ba81;thomwolf;2019-12-16 11:07:27 +0100;add model cards cc @mfuntowicz

==

transformers/__init__.py
transformers/file_utils.py
transformers/model_card.py
transformers/tests/model_card_test.py
==================
8669598ab;thomwolf;2019-12-16 09:59:36 +0100;update t5 tf

==

transformers/tests/modeling_tf_t5_test.py
==================
1b8613acb;thomwolf;2019-12-16 09:51:42 +0100;updating t5 config class

==

transformers/configuration_t5.py
transformers/tests/modeling_t5_test.py
==================
8e3b1c860;Morgan Funtowicz;2019-12-15 01:37:52 +0100;Added FeatureExtraction pipeline.

==

transformers/pipelines.py
==================
f1971bf30;Morgan Funtowicz;2019-12-15 01:37:16 +0100;Binding pipelines to the cli.

==

transformers-cli
transformers/commands/run.py
transformers/pipelines.py
==================
cc0135134;Pascal Voitot;2019-12-14 15:21:56 +0100;:zip: #2106 basic tokenizer.tokenize global speed improvement (3-8x) by simply caching added_tokens in a Set

==

transformers/tokenization_utils.py
==================
dc667ce1a;thomwolf;2019-12-14 09:56:27 +0100;double check cc @LysandreJik

==

examples/contrib/run_openai_gpt.py
examples/distillation/distiller.py
==================
7140363e0;thomwolf;2019-12-14 09:44:53 +0100;update bertabs

==

examples/summarization/configuration_bertabs.py
==================
a52d56c8d;Thomas Wolf;2019-12-14 09:43:07 +0100;Merge branch 'master' into cleanup-configs

==
==================
e92bcb7eb;Thomas Wolf;2019-12-14 09:40:43 +0100;Merge pull request #1739 from huggingface/t5
[WIP] Adding Google T5 model
==
==================
cbb368ca0;thomwolf;2019-12-14 09:31:18 +0100;distilbert tests

==

transformers/tests/modeling_common_test.py
==================
b6d4284b2;Julien Chaumond;2019-12-13 22:43:15 -0500;[cli] Uploads: fix + test edge case

==

transformers/hf_api.py
transformers/tests/fixtures/empty.txt
transformers/tests/hf_api_test.py
==================
a1faaf996;erenup;2019-12-14 08:57:13 +0800;deleted useless file

==

srl_label.txt
==================
c7780700f;erenup;2019-12-14 08:53:59 +0800;Merge branch 'refs/heads/squad_roberta'
# Conflicts:
#	transformers/data/processors/squad.py

==
==================
76f0d99f0;erenup;2019-12-14 08:45:17 +0800;Merge remote-tracking branch 'refs/remotes/huggingface/master'

==
==================
8e9526b4b;erenup;2019-12-14 08:43:58 +0800;add multiple processing

==

examples/run_squad.py
transformers/data/processors/squad.py
==================
7bd11dda6;Lysandre;2019-12-13 16:45:30 -0500;Release: v2.2.2

==

README.md
docs/source/conf.py
setup.py
transformers/__init__.py
==================
c3248cf12;LysandreJik;2019-12-11 12:36:37 -0500;Tests for all tokenizers

==

transformers/tests/tokenization_bert_test.py
transformers/tests/tokenization_gpt2_test.py
transformers/tests/tokenization_tests_commons.py
==================
f2ac50cb5;Pascal Voitot;2019-12-10 09:58:06 +0100;better for python2.x

==

transformers/tests/tokenization_bert_test.py
==================
4cbdc7d91;Pascal Voitot;2019-12-10 09:37:15 +0100;missed space

==

transformers/tests/tokenization_bert_test.py
transformers/tests/tokenization_gpt2_test.py
==================
dd2add9f6;Pascal Voitot;2019-12-10 00:29:44 +0100;more tests

==

transformers/tests/tokenization_bert_test.py
transformers/tests/tokenization_gpt2_test.py
==================
df160af73;Pascal Voitot;2019-12-10 00:03:38 +0100;:bug: #2096 in tokenizer.decode, space is not joined between all subtexts instead of before added tokens

==

transformers/tests/tokenization_bert_test.py
transformers/tokenization_utils.py
==================
5b7b78e08;Pascal Voitot;2019-12-08 23:22:02 +0100;:bug: #2096 in tokenizer.decode, adds a space after special tokens to return right formatted string

==

transformers/tokenization_utils.py
==================
866d73ca2;Julien Chaumond;2019-12-13 16:09:23 -0500;[cli] Upload is now compatible with folders

==

transformers/commands/user.py
==================
d46147294;Lysandre;2019-12-13 15:31:52 -0500;return for SQuAD [BLACKED]

==

transformers/data/processors/glue.py
transformers/data/processors/squad.py
==================
f24a228a9;Lysandre;2019-12-13 14:50:35 -0500;Speed up tokenization process

==

transformers/data/processors/squad.py
transformers/tokenization_utils.py
==================
c8ed1c82c;Lysandre;2019-12-13 12:13:48 -0500;[SQUAD] Load checkpoint when evaluating without training

==

examples/run_squad.py
==================
5c00e344c;thomwolf;2019-12-13 16:33:29 +0100;update model doc - swith 3B/11B to 3b/11b

==

docs/source/pretrained_models.rst
transformers/configuration_t5.py
transformers/modeling_t5.py
transformers/modeling_tf_t5.py
transformers/tokenization_t5.py
==================
0b51532ce;Morgan Funtowicz;2019-12-13 16:22:50 +0100;Reintroducing the batch_encode_plus method

==

transformers/tokenization_utils.py
==================
110394b2b;Thomas Wolf;2019-12-13 16:03:32 +0100;Merge branch 'master' into t5

==
==================
5a5c4349e;Pierric Cistac;2019-12-13 10:02:33 -0500;Fix summarization `to_cpu` doc

==

examples/summarization/README.md
==================
8ade20409;thomwolf;2019-12-13 14:48:47 +0100;fix tf

==

transformers/modeling_openai.py
transformers/modeling_tf_openai.py
==================
47f0e3cfb;thomwolf;2019-12-13 14:33:24 +0100;cleaning up configuration classes

==

examples/summarization/configuration_bertabs.py
templates/adding_a_new_model/configuration_xxx.py
templates/adding_a_new_model/tests/modeling_tf_xxx_test.py
templates/adding_a_new_model/tests/modeling_xxx_test.py
transformers/configuration_albert.py
transformers/configuration_bert.py
transformers/configuration_ctrl.py
transformers/configuration_distilbert.py
transformers/configuration_gpt2.py
transformers/configuration_openai.py
transformers/configuration_transfo_xl.py
transformers/configuration_utils.py
transformers/configuration_xlm.py
transformers/configuration_xlnet.py
transformers/convert_roberta_original_pytorch_checkpoint_to_pytorch.py
transformers/modeling_gpt2.py
transformers/modeling_tf_gpt2.py
transformers/modeling_tf_transfo_xl.py
transformers/modeling_tf_transfo_xl_utilities.py
transformers/modeling_tf_xlnet.py
transformers/modeling_transfo_xl.py
transformers/modeling_xlnet.py
transformers/tests/modeling_albert_test.py
transformers/tests/modeling_bert_test.py
transformers/tests/modeling_common_test.py
transformers/tests/modeling_ctrl_test.py
transformers/tests/modeling_distilbert_test.py
transformers/tests/modeling_gpt2_test.py
transformers/tests/modeling_openai_test.py
transformers/tests/modeling_roberta_test.py
transformers/tests/modeling_tf_albert_test.py
transformers/tests/modeling_tf_bert_test.py
transformers/tests/modeling_tf_ctrl_test.py
transformers/tests/modeling_tf_distilbert_test.py
transformers/tests/modeling_tf_gpt2_test.py
transformers/tests/modeling_tf_openai_gpt_test.py
transformers/tests/modeling_tf_roberta_test.py
transformers/tests/modeling_tf_transfo_xl_test.py
transformers/tests/modeling_tf_xlm_test.py
transformers/tests/modeling_tf_xlnet_test.py
transformers/tests/modeling_transfo_xl_test.py
transformers/tests/modeling_xlm_test.py
transformers/tests/modeling_xlnet_test.py
==================
8938b546b;Morgan Funtowicz;2019-12-13 14:27:04 +0100;Removed from_config

==

transformers/pipelines.py
==================
1ca52567a;Morgan Funtowicz;2019-12-13 14:13:14 +0100;Allow model conversion in the pipeline allocator.

==

transformers/pipelines.py
==================
28e64ad5a;Morgan Funtowicz;2019-12-13 14:12:54 +0100;Raise an exception if the pipeline allocator can't determine the tokenizer from the model.

==

transformers/pipelines.py
==================
be5bf7b81;Morgan Funtowicz;2019-12-13 14:12:17 +0100;Added NER pipeline.

==

transformers/pipelines.py
==================
80eacb8f1;Morgan Funtowicz;2019-12-13 14:10:22 +0100;Adding labels mapping for classification models in their respective config.

==

transformers/configuration_utils.py
==================
33e72b08d;thomwolf;2019-12-13 11:33:05 +0100;fix inner dimensions for 3B/11B models

==

transformers/modeling_t5.py
transformers/modeling_tf_t5.py
==================
9b312f9d4;erenup;2019-12-13 14:51:40 +0800;initial version for roberta squad

==

examples/run_squad.py
transformers/__init__.py
transformers/data/metrics/squad_metrics.py
transformers/data/processors/squad.py
transformers/modeling_roberta.py
==================
40ed71723;erenup;2019-12-13 09:10:17 +0800;Merge remote-tracking branch 'refs/remotes/huggingface/master'

==
==================
7296f1010;LysandreJik;2019-12-12 13:01:04 -0500;Cleanup squad and add allow train_file and predict_file usage

==

examples/run_squad.py
transformers/data/processors/squad.py
==================
5d67aa21a;Julien Chaumond;2019-12-12 12:39:41 -0500;[doc] Replicate doc from #2144

==

transformers/configuration_auto.py
transformers/configuration_utils.py
transformers/modeling_auto.py
transformers/modeling_encoder_decoder.py
transformers/modeling_tf_auto.py
transformers/modeling_tf_utils.py
transformers/modeling_utils.py
transformers/tokenization_auto.py
transformers/tokenization_utils.py
==================
3fd71c443;LysandreJik;2019-12-12 12:08:54 -0500;Update example scripts

==

examples/distillation/distiller.py
examples/utils_ner.py
==================
fe92755b9;LysandreJik;2019-12-12 11:37:19 -0500;Fix special tokens mask in encode

==

transformers/tokenization_utils.py
==================
fbf5455a8;Alan deLevie;2019-12-11 10:14:48 -0500;Fix typo in examples/run_glue.py args declaration.
deay -> decay
==

examples/run_glue.py
==================
f19dad61c;thomwolf;2019-12-12 14:46:30 +0100;fixing XLM conversion tests with dummy input

==

transformers/modeling_tf_pytorch_utils.py
transformers/modeling_tf_xlm.py
transformers/modeling_xlm.py
==================
f69dbecc3;Morgan Funtowicz;2019-12-12 10:25:36 +0100;Expose classification labels mapping (and reverse) in model config.

==

transformers/configuration_utils.py
==================
90df44f0a;Thomas Wolf;2019-12-12 08:21:46 +0100;Merge pull request #2063 from guillaume-be/special_tokens_mask_value_not_used
special_tokens_mask value was unused and calculated twice
==
==================
707f9e924;Thomas Wolf;2019-12-12 08:20:43 +0100;Merge pull request #2081 from pglock/patch-1
handle string with only whitespaces as empty
==
==================
137e20a84;Thomas Wolf;2019-12-12 08:09:12 +0100;Merge pull request #2075 from huggingface/check-link-validity
Check link validity
==
==================
d5712f7ca;Thomas Wolf;2019-12-12 08:00:51 +0100;Merge branch 'master' into check-link-validity

==
==================
9c58b236e;Thomas Wolf;2019-12-12 07:43:40 +0100;Merge pull request #2144 from huggingface/from-pretrained-from-url
Allowing from_pretrained to load from url directly
==
==================
413f41921;thomwolf;2019-12-12 07:34:42 +0100;fix merge

==

transformers/tests/utils.py
==================
386a93f0f;Thomas Wolf;2019-12-12 07:31:05 +0100;Merge branch 'master' into from-pretrained-from-url

==
==================
2d103546e;Thomas Wolf;2019-12-12 07:24:47 +0100;Merge pull request #2148 from huggingface/fix_encode_plus
Fix encode plus
==
==================
1748fdf65;Julien Chaumond;2019-12-11 23:31:23 +0000;[doc] Fix rst table

==

docs/source/pretrained_models.rst
==================
36fc52a3b;Julien Chaumond;2019-12-10 22:03:35 +0000;Update links to weights

==

transformers/configuration_bert.py
transformers/modeling_bert.py
transformers/modeling_tf_bert.py
transformers/tokenization_bert_japanese.py
==================
371c5ddfa;Julien Chaumond;2019-12-10 21:55:43 +0000;Py2 tests for Lysandre

==

.circleci/config.yml
==================
5505cf701;Julien Chaumond;2019-12-10 21:53:44 +0000;Run tests on Py2 too, for Lysandre

==

.circleci/config.yml
==================
9cb97c0c0;Julien Chaumond;2019-12-10 21:48:56 +0000;Actually run the tests

==

.circleci/config.yml
==================
95854c4a2;Julien Chaumond;2019-12-10 21:46:00 +0000;Actually run the tests

==

.circleci/config.yml
==================
d2100428d;Julien Chaumond;2019-12-10 21:43:49 +0000;Update to new test infra and only run conditionally

==

.circleci/config.yml
transformers/tests/tokenization_bert_japanese_test.py
transformers/tests/utils.py
==================
597ba7feb;Masatoshi Suzuki;2019-12-05 11:30:40 +0900;Support testing Japanese BERT tokenizers

==

.circleci/config.yml
==================
6a43dc9d7;Masatoshi Suzuki;2019-12-05 11:19:02 +0900;Support Python 2

==

transformers/tokenization_bert_japanese.py
==================
a09da4eeb;Masatoshi Suzuki;2019-11-29 19:24:43 +0900;Add a test for Japanese BERT tokenizers

==

transformers/tests/tokenization_bert_japanese_test.py
==================
57b5cb3ea;Masatoshi Suzuki;2019-11-20 09:02:10 +0900;Fix loading BertJapaneseTokenizer

==

transformers/tokenization_auto.py
==================
c03c0dfd2;Masatoshi Suzuki;2019-11-15 17:24:56 +0900;Add support for Japanese BERT models by cl-tohoku

==

docs/source/pretrained_models.rst
transformers/__init__.py
transformers/configuration_bert.py
transformers/modeling_bert.py
transformers/modeling_tf_bert.py
transformers/tokenization_auto.py
transformers/tokenization_bert_japanese.py
==================
4f15e5a26;Julien Chaumond;2019-12-11 17:41:51 -0500;Add tests.
Maybe not the best possible place for the tests, lmk.

==

transformers/tests/modeling_auto_test.py
transformers/tests/modeling_tf_auto_test.py
transformers/tests/tokenization_auto_test.py
transformers/tests/utils.py
==================
18e1f751f;Julien Chaumond;2019-12-11 17:07:46 -0500;TF support

==

transformers/modeling_tf_utils.py
transformers/modeling_utils.py
==================
31e5b5ff2;Julien Chaumond;2019-12-11 15:22:02 -0500;Fix tests + first example of doc

==

transformers/tokenization_utils.py
==================
3d57c5111;LysandreJik;2019-12-11 15:10:17 -0500;Fix encode plus

==

transformers/tokenization_utils.py
==================
c999a3e50;Julien Chaumond;2019-12-11 12:29:58 -0500;Allow from_pretrained to take a remote identifier

==

transformers/configuration_utils.py
transformers/file_utils.py
transformers/modeling_utils.py
transformers/tokenization_utils.py
==================
030faccb8;Stefan Schweter;2019-12-11 17:44:21 +0100;doc: fix pretrained models table

==

docs/source/pretrained_models.rst
==================
6709739a0;thomwolf;2019-12-11 17:19:18 +0100;allowing from_pretrained to load from url directly

==

transformers/modeling_tf_utils.py
transformers/modeling_utils.py
==================
29570db25;thomwolf;2019-12-11 17:19:18 +0100;allowing from_pretrained to load from url directly

==

transformers/modeling_tf_utils.py
transformers/modeling_utils.py
==================
2e2f9fed5;Julien Chaumond;2019-12-11 11:11:56 -0500;rm duplicate imports

==

transformers/modeling_auto.py
==================
c28273793;Morgan Funtowicz;2019-12-11 14:52:01 +0100;Add missing DistilBert and Roberta to AutoModelForTokenClassification

==

transformers/modeling_auto.py
transformers/modeling_tf_auto.py
==================
4c12860f7;LysandreJik;2019-12-11 09:22:37 -0500;Remove misleading documentation

==

transformers/tokenization_utils.py
==================
b040bff6d;Morgan Funtowicz;2019-12-11 14:13:58 +0100;Added supported model to AutoModelTokenClassification

==

transformers/__init__.py
transformers/modeling_auto.py
transformers/modeling_tf_auto.py
==================
fafd4c86e;thomwolf;2019-12-11 13:47:27 +0100;fix TF 2.0 version of T5 - update conversion script

==

transformers/convert_pytorch_checkpoint_to_tf2.py
transformers/file_utils.py
transformers/modeling_t5.py
transformers/modeling_tf_t5.py
transformers/modeling_tf_utils.py
transformers/modeling_utils.py
==================
6aa919469;Bilal Khan;2019-12-09 17:20:12 -0600;Update run_xnli to save optimizer and scheduler states, then resume training from a checkpoint

==

examples/run_xnli.py
==================
89896fe04;Bilal Khan;2019-12-09 10:07:52 -0600;Update run_ner to save optimizer and scheduler states, then resume training from a checkpoint

==

examples/run_ner.py
==================
fdc05cd68;Bilal Khan;2019-12-09 10:04:55 -0600;Update run_squad to save optimizer and scheduler states, then resume training from a checkpoint

==

examples/run_squad.py
==================
854ec5784;Bilal Khan;2019-12-09 10:00:36 -0600;Update run_glue to save optimizer and scheduler states, then resume training from a checkpoint

==

examples/run_glue.py
==================
9a24e0cf7;Morgan Funtowicz;2019-12-11 00:33:25 +0100;Refactored qa pipeline argument handling + unittests

==

transformers/pipelines.py
transformers/tests/pipelines_test.py
==================
b72f9d340;LysandreJik;2019-12-10 18:33:17 -0500;Correct index in script

==

examples/run_lm_finetuning.py
==================
51ae20329;Thomas Wolf;2019-12-10 22:18:55 +0100;Merge pull request #2129 from leopd/master
Progress indicator improvements when downloading pre-trained models.
==
==================
ec6fb25c2;LysandreJik;2019-12-10 15:49:20 -0500;Patch documentation

==

transformers/modeling_bert.py
==================
418589244;LysandreJik;2019-12-10 15:26:19 -0500;Uniforming the ignored indices

==

templates/adding_a_new_model/modeling_xxx.py
transformers/modeling_albert.py
transformers/modeling_bert.py
transformers/modeling_camembert.py
transformers/modeling_ctrl.py
transformers/modeling_distilbert.py
transformers/modeling_gpt2.py
transformers/modeling_openai.py
transformers/modeling_roberta.py
transformers/modeling_tf_roberta.py
transformers/modeling_transfo_xl.py
transformers/modeling_xlm.py
transformers/modeling_xlnet.py
==================
58d75aa31;Leo Dirac;2019-12-10 11:36:56 -0800;Progress indicator improvements when downloading pre-trained models.

==

transformers/file_utils.py
==================
6a7338270;LysandreJik;2019-12-10 14:33:24 -0500;Complete warning + cleanup

==

examples/run_squad.py
transformers/tokenization_utils.py
==================
dc4e9e5cb;Lysandre;2019-12-10 19:21:20 +0000;DataParallel for SQuAD + fix XLM

==

examples/run_squad.py
transformers/data/metrics/squad_metrics.py
transformers/tokenization_xlm.py
==================
67a8be8e9;thomwolf;2019-12-10 17:50:32 +0100;fix backward in tests

==

transformers/tests/modeling_common_test.py
==================
07bc8efbc;R√©mi Louf;2019-11-15 10:51:38 +0100;add greedy decoding and sampling

==

examples/run_generation.py
transformers/modeling_encoder_decoder.py
transformers/modeling_transfo_xl.py
transformers/modeling_utils.py
transformers/modeling_xlm.py
transformers/modeling_xlnet.py
transformers/tests/sampling_test.py
==================
63e36007e;Morgan Funtowicz;2019-12-10 16:47:35 +0100;Make sure padding, cls and another non-context tokens cannot appear in the answer.

==

transformers/pipelines.py
==================
f2538c127;thomwolf;2019-12-10 16:33:11 +0100;all tests in torch no grad

==

transformers/tests/modeling_common_test.py
==================
a5df980c5;thomwolf;2019-12-10 16:01:15 +0100;updating distilbert test

==

transformers/tests/modeling_common_test.py
transformers/tests/modeling_tf_common_test.py
==================
40a39ab65;Morgan Funtowicz;2019-12-10 15:59:38 +0100;Reuse recent SQuAD refactored data structure inside QA pipelines.

==

transformers/data/processors/__init__.py
transformers/modeling_auto.py
transformers/pipelines.py
==================
7c3a15ace;thomwolf;2019-12-10 15:36:54 +0100;Merge branch 'master' into t5

==
==================
981a5c8c1;thomwolf;2019-12-10 15:36:19 +0100;updating models urls

==

transformers/configuration_t5.py
transformers/convert_pytorch_checkpoint_to_tf2.py
transformers/modeling_t5.py
transformers/modeling_tf_t5.py
transformers/tokenization_t5.py
==================
e6cff60b4;Thomas Wolf;2019-12-10 15:34:08 +0100;Merge pull request #2069 from huggingface/cleaner-pt-tf-conversion
clean up PT <=> TF conversion
==
==================
4b82c485d;R√©mi Louf;2019-12-10 14:49:53 +0100;remove misplaced summarization documentation

==

examples/README.md
==================
8ae1044f8;thomwolf;2019-12-10 15:11:07 +0100;updating tests and TF 2.0 model

==

transformers/modeling_t5.py
transformers/modeling_tf_t5.py
transformers/tests/modeling_common_test.py
transformers/tests/modeling_t5_test.py
transformers/tests/modeling_tf_common_test.py
transformers/tests/modeling_tf_t5_test.py
transformers/tests/tokenization_t5_test.py
==================
aae74065d;Morgan Funtowicz;2019-12-09 18:35:26 +0100;Added QuestionAnsweringPipeline unit tests.

==

transformers/tests/pipelines_test.py
==================
a7d3794a2;Morgan Funtowicz;2019-12-09 18:34:58 +0100;Remove token_type_ids for compatibility with DistilBert

==

transformers/pipelines.py
==================
fe0f552e0;Morgan Funtowicz;2019-12-09 14:13:17 +0100;Use attention_mask everywhere.

==

transformers/pipelines.py
==================
348e19aa2;Morgan Funtowicz;2019-12-09 12:10:26 +0100;Expose attention_masks and input_lengths arguments to batch_encode_plus

==

transformers/pipelines.py
==================
c2407fdd8;Morgan Funtowicz;2019-12-09 11:47:52 +0100;Enable the Tensorflow backend.

==

transformers/pipelines.py
==================
f116cf599;Morgan Funtowicz;2019-12-09 11:32:49 +0100;Allow hidding frameworks through environment variables (NO_TF, NO_TORCH).

==

transformers/file_utils.py
==================
6e61e0605;Morgan Funtowicz;2019-12-09 11:13:27 +0100;batch_encode_plus generates the encoder_attention_mask to avoid attending over padded values.

==

transformers/pipelines.py
==================
02110485b;Morgan Funtowicz;2019-12-06 18:11:27 +0100;Added batching, topk, chars index and scores.

==

transformers/pipelines.py
==================
e1d89cb24;Morgan Funtowicz;2019-12-06 00:52:04 +0100;Added QuestionAnsweringPipeline with batch support.

==

transformers/__init__.py
transformers/pipeline.py
transformers/pipelines.py
==================
0558c9cb9;thomwolf;2019-12-10 12:58:48 +0100;Merge branch 'master' into t5

==
==================
81babb227;Morgan Funtowicz;2019-12-03 14:56:57 +0100;Added download command through the cli.
It allows to predownload models and tokenizers.

==

transformers-cli
transformers/commands/download.py
==================
31a3a73ee;thomwolf;2019-10-17 15:10:11 +0200;updating CLI

==

transformers-cli
transformers/__init__.py
transformers/commands/train.py
transformers/data/processors/utils.py
transformers/modeling_tf_utils.py
transformers/pipeline.py
==================
7c1697562;thomwolf;2019-10-17 13:17:05 +0200;compatibility with sklearn and keras

==

transformers/commands/train.py
transformers/data/processors/utils.py
transformers/pipeline.py
==================
b81ab431f;thomwolf;2019-10-17 12:06:27 +0200;updating AutoModels and AutoConfiguration - adding pipelines

==

transformers/configuration_auto.py
transformers/modeling_auto.py
transformers/modeling_tf_auto.py
transformers/pipeline.py
==================
2d8559731;thomwolf;2019-10-16 23:19:45 +0200;add pipeline - train

==

transformers/commands/train.py
transformers/data/processors/utils.py
transformers/pipeline.py
==================
72c36b9ea;thomwolf;2019-10-16 14:17:58 +0200;[WIP] - CLI

==

setup.py
transformers-cli
transformers/__init__.py
transformers/__main__.py
transformers/commands/convert.py
transformers/commands/serving.py
transformers/commands/train.py
transformers/data/__init__.py
transformers/data/processors/__init__.py
transformers/data/processors/utils.py
==================
e57d00ee1;Thomas Wolf;2019-12-10 11:07:26 +0100;Merge pull request #1984 from huggingface/squad-refactor
[WIP] Squad refactor
==
==================
ecabbf6d2;Thomas Wolf;2019-12-10 10:07:56 +0100;Merge pull request #2107 from huggingface/encoder-mask-shape
create encoder attention mask from shape of hidden states
==
==================
608a8f5b5;thomwolf;2019-12-10 10:01:01 +0100;updating tf 2.0 layer_norm to T5 layer norm

==

transformers/modeling_tf_t5.py
==================
df3961121;Suvrat Bhooshan;2019-12-09 18:25:28 -0800;Add MMBT Model to Transformers Repo

==

README.md
examples/README.md
examples/run_mmimdb.py
examples/utils_mmimdb.py
transformers/__init__.py
transformers/configuration_mmbt.py
transformers/modeling_mmbt.py
==================
1d1893046;Julien Chaumond;2019-12-10 01:32:42 +0000;Harmonize `no_cuda` flag with other scripts

==

examples/summarization/run_summarization.py
requirements.txt
==================
f7eba0900;R√©mi Louf;2019-12-06 22:01:48 +0100;clean for release

==

examples/convert_bertextabs_original_pytorch_checkpoint_to_pytorch.py
examples/summarization/modeling_bertabs.py
examples/summarization/requirements.txt
examples/summarization/run_summarization.py
requirements.txt
transformers/convert_bertextabs_original_pytorch_checkpoint_to_pytorch.py
transformers/generate/__init__.py
transformers/modeling_encoder_decoder.py
==================
2a64107e4;R√©mi Louf;2019-12-06 15:45:09 +0100;improve device usage

==

examples/summarization/README.md
examples/summarization/convert_bertabs_original_pytorch_checkpoint.py
examples/summarization/modeling_bertabs.py
examples/summarization/run_summarization.py
==================
c0707a85d;R√©mi Louf;2019-12-06 11:49:27 +0100;add README

==

examples/README.md
examples/summarization/README.md
==================
ade3cdf5a;R√©mi Louf;2019-12-06 11:36:44 +0100;integrate ROUGE

==

examples/summarization/modeling_bertabs.py
examples/summarization/run_summarization.py
requirements.txt
==================
076602bdc;R√©mi Louf;2019-12-06 10:11:44 +0100;prevent BERT weights from being downloaded twice

==

examples/summarization/modeling_bertabs.py
==================
5909f7102;R√©mi Louf;2019-12-05 21:07:49 +0100;add py-rouge dependency

==

requirements.txt
==================
a1994a71e;R√©mi Louf;2019-12-05 21:05:06 +0100;simplified model and configuration

==

examples/summarization/configuration_bertabs.py
examples/summarization/modeling_bertabs.py
examples/summarization/run_summarization.py
==================
3a9a9f786;R√©mi Louf;2019-12-05 19:09:47 +0100;default output dir to documents dir

==

examples/summarization/run_summarization.py
examples/summarization/utils_summarization.py
==================
693606a75;R√©mi Louf;2019-12-05 18:55:15 +0100;update the docs

==

examples/README.md
==================
c0443df59;R√©mi Louf;2019-12-05 18:13:41 +0100;remove beam search

==

transformers/generate/beam_search.py
transformers/tests/beam_search_tests.py
==================
2403a6659;R√©mi Louf;2019-11-23 00:18:44 +0100;give transformers API to BertAbs

==

examples/convert_bertextabs_original_pytorch_checkpoint_to_pytorch.py
examples/summarization/configuration_bertabs.py
examples/summarization/convert_bertabs_original_pytorch_checkpoint.py
examples/summarization/modeling_bertabs.py
examples/summarization/run_summarization.py
examples/summarization/utils_summarization.py
examples/summarization/utils_summarization_test.py
transformers/convert_bertextabs_original_pytorch_checkpoint_to_pytorch.py
transformers/generate/beam_search.py
==================
4d1819990;R√©mi Louf;2019-11-12 17:59:34 +0100;cast bool tensor to long for pytorch < 1.3

==

transformers/modeling_bert.py
==================
9f75565ea;R√©mi Louf;2019-11-08 15:48:31 +0100;setup training

==

requirements.txt
transformers/generate/beam_search.py
==================
4735c2af0;R√©mi Louf;2019-11-08 11:16:26 +0100;tweaks to the BeamSearch API

==

transformers/generate/beam_search.py
transformers/tests/beam_search_tests.py
==================
ba089c780;R√©mi Louf;2019-11-06 13:55:24 +0100;share pretrained embeddings

==

examples/utils_summarization.py
requirements.txt
transformers/generate/beam_search.py
==================
9660ba1cb;R√©mi Louf;2019-10-31 17:59:16 +0100;Add beam search

==

examples/run_summarization_finetuning.py
examples/utils_summarization.py
transformers/generate/__init__.py
transformers/generate/beam_search.py
transformers/modeling_beam_search.py
transformers/tests/beam_search_tests.py
==================
1c71ecc88;R√©mi Louf;2019-10-31 10:16:08 +0100;load the pretrained weights for encoder-decoder
We currently save the pretrained_weights of the encoder and decoder in
two separate directories `encoder` and `decoder`. However, for the
`from_pretrained` function to operate with automodels we need to
specify the type of model in the path to the weights.

The path to the encoder/decoder weights is handled by the
`PreTrainedEncoderDecoder` class in the `save_pretrained` function. Sice
there is no easy way to infer the type of model that was initialized for
the encoder and decoder we add a parameter `model_type` to the function.
This is not an ideal solution as it is error prone, and the model type
should be carried by the Model classes somehow.

This is a temporary fix that should be changed before merging.

==

examples/run_summarization_finetuning.py
transformers/modeling_encoder_decoder.py
==================
07f4cd73f;R√©mi Louf;2019-10-31 09:48:27 +0100;update function to add special tokens
Since I started my PR the `add_special_token_single_sequence` function
has been deprecated for another; I replaced it with the new function.

==

examples/utils_summarization.py
==================
5c877fe94;Pierric Cistac;2019-12-09 18:53:00 -0500;fix albert links

==

README.md
docs/source/index.rst
docs/source/pretrained_models.rst
==================
79526f82f;Bilal Khan;2019-11-28 19:20:29 -0600;Remove unnecessary epoch variable

==

examples/run_lm_finetuning.py
==================
9626e0458;Bilal Khan;2019-11-27 20:00:16 -0600;Add functionality to continue training from last saved global_step

==

examples/run_lm_finetuning.py
==================
2d73591a1;Bilal Khan;2019-11-27 19:13:10 -0600;Stop saving current epoch

==

examples/run_lm_finetuning.py
==================
0eb973b0d;Bilal Khan;2019-11-27 19:10:24 -0600;Use saved optimizer and scheduler states if available

==

examples/run_lm_finetuning.py
==================
a03fcf570;Bilal Khan;2019-11-27 18:42:07 -0600;Save tokenizer after each epoch to be able to resume training from a checkpoint

==

examples/run_lm_finetuning.py
==================
f71b1bb05;Bilal Khan;2019-11-27 08:39:00 -0600;Save optimizer state, scheduler state and current epoch

==

examples/run_lm_finetuning.py
==================
8e651f56b;thomwolf;2019-12-09 22:13:57 +0100;fix tf tests

==

transformers/tests/modeling_tf_common_test.py
==================
808bb8da7;thomwolf;2019-12-09 21:48:34 +0100;fix transfo xl tests

==

transformers/tests/modeling_common_test.py
transformers/tests/modeling_tf_transfo_xl_test.py
transformers/tests/modeling_transfo_xl_test.py
==================
b016dd16c;thomwolf;2019-12-09 21:38:07 +0100;fix tests on python 3.5

==

transformers/modeling_t5.py
transformers/tests/modeling_common_test.py
transformers/tokenization_t5.py
==================
2a4ef098d;LysandreJik;2019-12-09 10:46:47 -0500;Add ALBERT and XLM to SQuAD script

==

examples/run_squad.py
==================
00c4e3958;Lysandre Debut;2019-12-09 10:41:15 -0500;Merge branch 'master' into squad-refactor

==
==================
169fea685;thomwolf;2019-12-09 16:25:33 +0100;updating T5

==

transformers/modeling_t5.py
==================
3520be782;R√©mi Louf;2019-12-09 11:13:09 +0100;create encoder attention mask from shape of hidden states
We currently create encoder attention masks (when they're not provided)
based on the shape of the inputs to the encoder. This is obviously
wrong; sequences can be of different lengths. We now create the encoder
attention mask based on the batch_size and sequence_length of the
encoder hidden states.

==

transformers/modeling_bert.py
==================
0cb163865;Aymeric Augustin;2019-12-07 13:46:14 +0100;Remove pytest dependency. (#2093)

==

transformers/tests/optimization_tf_test.py
==================
2670b0d68;Michael Watkins;2019-12-04 17:53:25 +0200;Fix bug which lowercases special tokens

==

transformers/tests/tokenization_tests_commons.py
transformers/tokenization_utils.py
==================
35401fe50;Aymeric Augustin;2019-12-06 19:57:38 +0100;Remove dependency on pytest for running tests (#2055)
* Switch to plain unittest for skipping slow tests.

Add a RUN_SLOW environment variable for running them.

* Switch to plain unittest for PyTorch dependency.

* Switch to plain unittest for TensorFlow dependency.

* Avoid leaking open files in the test suite.

This prevents spurious warnings when running tests.

* Fix unicode warning on Python 2 when running tests.

The warning was:

    UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal

* Support running PyTorch tests on a GPU.

Reverts 27e015bd.

* Tests no longer require pytest.

* Make tests pass on cuda

==

README.md
docs/source/installation.md
setup.py
templates/adding_a_new_model/tests/modeling_tf_xxx_test.py
templates/adding_a_new_model/tests/modeling_xxx_test.py
transformers/modeling_openai.py
transformers/tests/conftest.py
transformers/tests/modeling_albert_test.py
transformers/tests/modeling_auto_test.py
transformers/tests/modeling_bert_test.py
transformers/tests/modeling_common_test.py
transformers/tests/modeling_ctrl_test.py
transformers/tests/modeling_distilbert_test.py
transformers/tests/modeling_encoder_decoder_test.py
transformers/tests/modeling_gpt2_test.py
transformers/tests/modeling_openai_test.py
transformers/tests/modeling_roberta_test.py
transformers/tests/modeling_tf_albert_test.py
transformers/tests/modeling_tf_auto_test.py
transformers/tests/modeling_tf_bert_test.py
transformers/tests/modeling_tf_common_test.py
transformers/tests/modeling_tf_ctrl_test.py
transformers/tests/modeling_tf_distilbert_test.py
transformers/tests/modeling_tf_gpt2_test.py
transformers/tests/modeling_tf_openai_gpt_test.py
transformers/tests/modeling_tf_roberta_test.py
transformers/tests/modeling_tf_transfo_xl_test.py
transformers/tests/modeling_tf_xlm_test.py
transformers/tests/modeling_tf_xlnet_test.py
transformers/tests/modeling_transfo_xl_test.py
transformers/tests/modeling_xlm_test.py
transformers/tests/modeling_xlnet_test.py
transformers/tests/optimization_test.py
transformers/tests/tokenization_auto_test.py
transformers/tests/tokenization_bert_test.py
transformers/tests/tokenization_distilbert_test.py
transformers/tests/tokenization_roberta_test.py
transformers/tests/tokenization_tests_commons.py
transformers/tests/tokenization_transfo_xl_test.py
transformers/tests/tokenization_utils_test.py
transformers/tests/tokenization_xlm_test.py
transformers/tests/tokenization_xlnet_test.py
transformers/tests/utils.py
transformers/tokenization_albert.py
transformers/tokenization_ctrl.py
transformers/tokenization_gpt2.py
transformers/tokenization_openai.py
transformers/tokenization_utils.py
transformers/tokenization_xlm.py
transformers/tokenization_xlnet.py
==================
e4679cddc;Julien Chaumond;2019-12-06 11:56:23 -0500;[cli] Uploads: add progress bar (#2078)
* [cli] Uploads: add progress bar

see https://github.com/huggingface/transformers/pull/2044#discussion_r354057827 for context

* rename + documentation

* Add auto-referential comment

==

transformers/hf_api.py
==================
1d87b37d1;thomwolf;2019-12-06 15:30:09 +0100;updating

==

transformers/convert_pytorch_checkpoint_to_tf2.py
==================
4cb9b6055;Thomas Wolf;2019-12-06 12:14:48 +0100;Merge pull request #2077 from patrickvonplaten/change_documentation_for_past_output_shape
corrected documentation for past tensor shape for ctrl and gpt2 model
==
==================
5482822a2;Thomas Wolf;2019-12-06 12:12:22 +0100;Merge pull request #2046 from jplu/tf2-ner-example
Add NER TF2 example.
==
==================
fc1bb1f86;Thomas Wolf;2019-12-06 12:06:42 +0100;Merge pull request #2068 from huggingface/fix-2042
Nicer error message when Bert's input is missing batch size
==
==================
21451ec6b;Philipp Glock;2019-12-06 10:32:43 +0100;handle string with only whitespaces as empty

==

transformers/tokenization_utils.py
==================
f230d91b4;R√©mi Louf;2019-12-05 21:24:57 +0100;check the validity of links
We add a script and a CI workflow to check that all download links
present in the source code are valid.

==

.circleci/config.yml
utils/link_tester.py
==================
d0383e4da;patrickvonplaten;2019-12-06 01:24:22 +0100;corrected documentation for past tensor shape for ctrl and gpt2 model

==

transformers/modeling_ctrl.py
transformers/modeling_gpt2.py
transformers/modeling_tf_ctrl.py
transformers/modeling_tf_gpt2.py
==================
e9217da5f;LysandreJik;2019-12-05 16:01:51 -0500;Cleanup
Improve global visibility on the run_squad script, remove unused files and fixes related to XLNet.
==

examples/run_squad.py
examples/utils_squad.py
examples/utils_squad_evaluate.py
transformers/data/metrics/squad_metrics.py
transformers/data/processors/squad.py
==================
9ecd83dac;LysandreJik;2019-12-05 14:44:57 -0500;Patch evaluation for impossible values + cleanup

==

docs/source/main_classes/processors.rst
examples/run_squad.py
transformers/data/processors/squad.py
transformers/tokenization_utils.py
==================
35ff345fc;VictorSanh;2019-12-05 12:07:04 -0500;update requirements

==

examples/distillation/distiller.py
examples/distillation/requirements.txt
==================
552c44a9b;VictorSanh;2019-12-05 10:14:58 -0500;release distilm-bert

==

README.md
docs/source/pretrained_models.rst
examples/distillation/README.md
==================
ee53de7aa;Rosanne Liu;2019-12-05 06:20:07 -0800;Pr for pplm (#2060)
* license

* changes

* ok

* Update paper link and commands to run

* pointer to uber repo

==

examples/pplm/README.md
examples/pplm/run_pplm.py
examples/pplm/run_pplm_discrim_train.py
==================
f8fb4335c;thomwolf;2019-12-05 15:19:32 +0100;clean up a little bit PT <=> TF conversion

==

transformers/convert_pytorch_checkpoint_to_tf2.py
transformers/modeling_utils.py
==================
bebaa1403;Thomas Wolf;2019-12-05 14:41:56 +0100;Merge pull request #2045 from aaugustin/remove-dead-code
Remove dead code in tests.
==
==================
18fb93530;thomwolf;2019-12-05 14:36:34 +0100;fixing #2042 - Nicer error message

==

transformers/modeling_bert.py
==================
2d5d86e03;thomwolf;2019-12-05 14:06:29 +0100;fix #2031

==

transformers/tokenization_albert.py
==================
af077b15e;Thomas Wolf;2019-12-05 13:45:44 +0100;Merge pull request #2065 from huggingface/fixing-camembert
Fixing camembert tokenization
==
==================
3268ebd22;thomwolf;2019-12-05 13:35:29 +0100;fix xlnet test

==

transformers/tests/modeling_xlnet_test.py
==================
6c5297a42;thomwolf;2019-12-05 13:27:58 +0100;Fixing camembert tokenization

==

transformers/tokenization_camembert.py
==================
9200a759d;Julien Plu;2019-12-05 12:56:43 +0100;Add few tests on the TF optimization file with some info in the documentation. Complete the README.

==

docs/source/main_classes/optimizer_schedules.rst
examples/README.md
examples/run_tf_ner.py
transformers/tests/optimization_tf_test.py
==================
1f179f095;Thomas Wolf;2019-12-05 12:39:04 +0100;Merge pull request #2011 from AdityaSoni19031997/patch-1
typo fix on the docs as per Pytorch v1.1+
==
==================
1eaf44e71;Thomas Wolf;2019-12-05 12:32:39 +0100;Merge pull request #2007 from roskoN/xlnet_attention_fix
fixed XLNet attention output for both attention streams whenever target_mapping is provided
==
==================
71e4693f0;thomwolf;2019-12-05 12:14:24 +0100;fix #1968

==

transformers/__init__.py
==================
f9f395b21;Thomas Wolf;2019-12-05 11:56:48 +0100;Merge pull request #1735 from ondewo/tf-do-not-use-gpu-on-import
Do not use GPU when importing transformers
==
==================
75a97af6b;thomwolf;2019-12-05 11:26:55 +0100;fix #1450 - add doc

==

examples/README.md
==================
8b388827b;thomwolf;2019-12-05 11:18:43 +0100;fix #1920

==

transformers/tokenization_ctrl.py
==================
d425a4d60;Thomas Wolf;2019-12-05 09:54:09 +0100;Merge pull request #1870 from alexzubiaga/xlnet-for-token-classification
XLNet for Token classification
==
==================
1eb89ddf7;Thomas Wolf;2019-12-05 09:44:07 +0100;Merge pull request #2044 from huggingface/cli_upload
CLI for authenticated file sharing
==
==================
7f998b1b8;Guillaume B;2019-12-05 08:57:49 +0100;special_tokens_mask value was unused and calculated twice

==

transformers/tokenization_utils.py
==================
fb0d2f1da;VictorSanh;2019-12-05 03:00:16 -0500;preparing release distil-mBERT

==

transformers/modeling_tf_distilbert.py
==================
3ba417e1a;Julien Chaumond;2019-12-04 18:40:52 -0500;[cli] ls: Tabular formatting

==

transformers/commands/user.py
==================
ce158a076;LysandreJik;2019-12-04 17:55:52 -0500;Return dataset (pytorch)

==

transformers/data/processors/squad.py
==================
7a0351997;LysandreJik;2019-12-04 17:24:35 -0500;Documentation

==

docs/source/main_classes/processors.rst
transformers/data/processors/squad.py
==================
96fa9a8a7;Julien Chaumond;2019-12-04 17:22:50 -0500;Python 2 + Post mime-type to S3

==

transformers/hf_api.py
transformers/tests/hf_api_test.py
==================
33508ae31;LysandreJik;2019-12-04 16:26:45 -0500;Remove `only_first`

==

transformers/data/processors/squad.py
==================
f7e4a7cdf;LysandreJik;2019-12-04 16:24:15 -0500;Cleanup

==

examples/run_squad.py
examples/test_examples.py
examples/tests_samples/SQUAD/dev-v2.0.json
examples/tests_samples/SQUAD/train-v2.0.json
transformers/data/metrics/squad_metrics.py
transformers/data/processors/squad.py
==================
a7ca6d738;LysandreJik;2019-12-04 15:43:34 -0500;Padding side is  tokenizer-dependant

==

transformers/data/processors/squad.py
transformers/tests/tokenization_tests_commons.py
transformers/tokenization_utils.py
transformers/tokenization_xlnet.py
==================
cca75e788;LysandreJik;2019-12-04 15:42:29 -0500;Kill the demon spawn

==

examples/run_squad.py
transformers/data/processors/squad.py
==================
bf119c056;LysandreJik;2019-12-04 11:34:59 -0500;TFDS dataset can now be evaluated

==

transformers/data/processors/squad.py
==================
ff98b041d;Julien Plu;2019-12-04 16:53:06 +0100;Fix whitespace issue

==

transformers/__init__.py
==================
9ddc3f1a1;LysandreJik;2019-12-04 10:37:00 -0500;Naming update + XLNet/XLM evaluation

==

examples/run_squad.py
transformers/data/metrics/squad_metrics.py
==================
5bfcd0485;thomwolf;2019-12-04 14:53:11 +0100;fix #1991

==

examples/run_lm_finetuning.py
==================
cae641ff2;Thomas Wolf;2019-12-04 13:28:39 +0100;Merge pull request #1846 from tamuhey/patch/iss1845
fix summary_type value of SequenceSummary
==
==================
254ebb979;Julien Plu;2019-12-04 10:00:25 +0100;Bugfix on init file. Missing comma.

==

transformers/__init__.py
==================
ecb923da9;Julien Plu;2019-12-04 09:43:15 +0100;Create a NER example similar to the Pytorch one. It takes the same options, and can be run the same way.

==

examples/run_tf_ner.py
transformers/__init__.py
transformers/modeling_tf_distilbert.py
transformers/optimization_tf.py
==================
40255ab00;Aymeric Augustin;2019-12-04 08:21:02 +0100;Remove dead code in tests.

==

transformers/tests/modeling_tf_common_test.py
==================
e4fbf3e2c;Julien Chaumond;2019-12-04 00:52:23 -0500;CLI for authenticated file sharing

==

setup.py
transformers-cli
transformers/commands/__init__.py
transformers/commands/user.py
transformers/hf_api.py
transformers/tests/hf_api_test.py
==================
de276de1c;LysandreJik;2019-12-03 17:15:51 -0500;Working evaluation

==

examples/run_squad.py
transformers/data/metrics/squad_metrics.py
transformers/data/processors/squad.py
==================
7edb51f3a;Julien Chaumond;2019-12-03 22:07:25 +0000;[pplm] split classif head into its own file

==

examples/pplm/pplm_classification_head.py
examples/pplm/run_pplm.py
examples/pplm/run_pplm_discrim_train.py
==================
c835bc85c;LysandreJik;2019-12-03 15:28:16 -0500;Compute predictions

==

transformers/data/metrics/squad_metrics.py
==================
285b1241e;LysandreJik;2019-12-03 15:00:49 -0500;Added SquadResult

==

transformers/data/processors/squad.py
==================
8101924a6;LysandreJik;2019-12-03 11:20:26 -0500;Patch: v2.2.1

==

README.md
docs/source/conf.py
setup.py
transformers/__init__.py
==================
48cbf267c;VictorSanh;2019-12-03 11:01:37 -0500;Use full dataset for eval (SequentialSampler in Distributed setting)

==

examples/run_glue.py
examples/run_lm_finetuning.py
examples/run_multiple_choice.py
examples/run_xnli.py
==================
f434bfc62;Julien Chaumond;2019-12-03 10:53:02 -0500;[pplm] Update S3 links
Co-Authored-By: Piero Molino <w4nderlust@gmail.com>

==

examples/pplm/run_pplm.py
==================
96e83506d;Ethan Perez;2019-11-29 18:22:34 -0600;Always use SequentialSampler during evaluation
When evaluating, shouldn't we always use the SequentialSampler instead of DistributedSampler? Evaluation only runs on 1 GPU no matter what, so if you use the DistributedSampler with N GPUs, I think you'll only evaluate on 1/N of the evaluation set. That's at least what I'm finding when I run an older/modified version of this repo.
==

examples/run_squad.py
==================
3b48806f7;Julien Chaumond;2019-12-03 10:11:47 -0500;[pplm] README: add setup + tweaks

==

examples/pplm/README.md
==================
0cb2c9089;Julien Chaumond;2019-12-02 18:17:28 -0500;readme
Co-Authored-By: Rosanne Liu <mimosavvy@gmail.com>

==

examples/pplm/README.md
examples/pplm/imgs/headfigure.png
examples/pplm/imgs/wooly.png
==================
1efb2ae7f;Julien Chaumond;2019-12-02 16:01:57 -0500;[pplm] move scripts under examples/pplm/

==

examples/pplm/run_pplm.py
examples/pplm/run_pplm_discrim_train.py
==================
a59fdd162;Piero Molino;2019-12-01 15:48:33 -0800;generate_text_pplm now works with batch_size > 1

==

examples/run_pplm.py
==================
893d0d64f;w4nderlust;2019-11-29 20:19:33 -0800;Changed order of some parameters to be more consistent. Identical results.

==

examples/run_pplm.py
==================
f42816e7f;w4nderlust;2019-11-29 20:00:43 -0800;Added additional check for url and path in discriminator model params

==

examples/run_pplm.py
==================
f10b92501;w4nderlust;2019-11-29 19:59:02 -0800;Imrpovements: model_path renamed pretrained_model, tokenizer loaded from pretrained_model, pretrained_model set to discriminator's when discrim is specified, sample = False by default but cli parameter introduced. To obtain identical samples call the cli with --sample

==

examples/run_pplm.py
==================
75904dae6;w4nderlust;2019-11-29 18:51:27 -0800;Removed global variable device

==

examples/run_pplm_discrim_train.py
==================
7fd54b55a;piero;2019-11-27 21:45:19 -0800;Added support for generic discriminators

==

examples/run_pplm.py
==================
b0eaff36e;piero;2019-11-27 21:43:43 -0800;Added a +1 to epoch when saving weights

==

examples/run_pplm_discrim_train.py
==================
611961ade;piero;2019-11-27 21:34:49 -0800;Added tqdm to preprocessing

==

examples/run_pplm_discrim_train.py
==================
afc7dcd94;piero;2019-11-27 20:08:53 -0800;Now run_pplm works on cpu. Identical output as before (when using gpu).

==

examples/run_pplm.py
==================
61399e5af;piero;2019-11-27 19:51:42 -0800;Cleaned perturb_past. Identical output as before.

==

examples/run_pplm.py
==================
ffc293540;piero;2019-11-27 18:30:42 -0800;Fix for making unditioned generation work. Identical output as before.

==

examples/run_pplm.py
==================
9f693a0c4;piero;2019-11-27 18:16:30 -0800;Cleaned generate_text_pplm. Identical output as before.

==

examples/run_pplm.py
==================
61a12f790;piero;2019-11-27 17:54:49 -0800;Renamed SmallConst to SMALL_CONST and introduced BIG_CONST. Identical output as before.

==

examples/run_pplm.py
==================
ef47b2c03;piero;2019-11-27 17:50:21 -0800;Removed commented code. Identical output as before.

==

examples/run_pplm.py
==================
7ea12db3f;piero;2019-11-27 17:49:39 -0800;Removed commented code. Identical output as before.

==

examples/run_pplm.py
==================
08c6e456a;piero;2019-11-27 17:48:46 -0800;Cleaned full_text_generation. Identical output as before.

==

examples/run_pplm.py
==================
6c9c13178;piero;2019-11-27 17:27:39 -0800;More cleanup for run_model. Identical output as before.

==

examples/run_pplm.py
==================
7ffe47c88;piero;2019-11-27 16:39:49 -0800;Improved device specification

==

examples/run_pplm_discrim_train.py
==================
4f2164e40;piero;2019-11-27 16:32:45 -0800;First cleanup step, changing function names and passing parameters all the way through without using args. Identical output as before.

==

examples/run_pplm.py
==================
821de121e;piero;2019-11-27 15:27:49 -0800;Minor changes

==

examples/run_pplm_discrim_train.py
==================
7469d03b1;w4nderlust;2019-11-26 21:30:57 -0800;Fixed minor bug when running training on cuda

==

examples/run_pplm_discrim_train.py
==================
0b51fba20;piero;2019-11-26 13:15:56 -0800;Added script for training a discriminator for pplm to use

==

examples/run_pplm.py
examples/run_pplm_discrim_train.py
==================
34a83faab;Piero Molino;2019-11-25 19:15:25 -0800;Let's make PPLM great again

==

examples/run_pplm.py
==================
d5faa74cd;Julien Chaumond;2019-11-05 15:48:00 +0000;tokenizer white space: revert to previous behavior

==

examples/run_pplm.py
==================
0b77d66a6;Julien Chaumond;2019-11-05 15:35:51 +0000;rm extraneous import

==

examples/run_pplm.py
==================
83b1e6ac9;Rosanne Liu;2019-11-03 04:51:57 +0000;fix the loss backward issue
(cherry picked from commit 566468cc984c6ec7e10dfc62b5b4191781a99cd2)

==

examples/run_pplm.py
==================
572c24cfa;Julien Chaumond;2019-11-05 15:34:37 +0000;PPLM (squashed)
Co-authored-by: piero <piero@uber.com>
Co-authored-by: Rosanne Liu <mimosavvy@gmail.com>

==

examples/run_pplm.py
==================
f19a78a63;Thomas Wolf;2019-12-03 16:13:01 +0100;Merge pull request #1903 from valohai/master
Valohai integration
==
==================
d100ad99c;Thomas Wolf;2019-12-03 16:03:48 +0100;Merge pull request #2014 from aaugustin/mark-tf-auto-model-test-as-slow
Mark tests in TFAutoModelTest as slow.
==
==================
66fc8d25a;Juha Kiili;2019-12-03 10:49:50 +0200;Change ref to original GLUE downloader script

==

utils/download_glue_data.py
==================
fbaf05bd9;LysandreJik;2019-12-02 18:23:00 -0500;Remove annoying tokenization message

==

transformers/tokenization_utils.py
==================
e85855f2c;Lysandre;2019-12-02 18:00:19 -0500;Fix ALBERT exports with pretraining + sp classifier; Fix naming for ALBERT TF models

==

transformers/modeling_albert.py
transformers/modeling_tf_albert.py
==================
b3d834ae1;Lysandre;2019-12-02 15:01:52 -0500;Reorganize ALBERT conversion script

==

transformers/modeling_albert.py
==================
f3776df0f;thomwolf;2019-12-02 15:47:00 +0100;WIP debugging

==

transformers/modeling_t5.py
==================
5ab93083e;Aymeric Augustin;2019-12-01 18:25:15 +0100;Mark tests in TFAutoModelTest as slow.
Each test forces downloading the same 536MB file, which is slow
even with a decent internet connection.

==

transformers/tests/modeling_tf_auto_test.py
==================
c356290c8;Aditya Soni;2019-12-01 14:08:14 +0530;typo fix as per Pytorch v1.1+

==

docs/source/migration.md
==================
76c0bc06d;Rostislav Nedelchev;2019-11-30 21:01:04 +0100;[XLNet] Changed post-processing of attention w.r.t to target_mapping
Whenever target_mapping is provided to the input, XLNet outputs two different attention streams.
Based on that the attention output would be on of the two:
- a list of tensors (usual case for most transformers)
- a list of 2-tuples of tensors, one tesor for each of attention streams
Docs and unit-tests have been updated

==

transformers/modeling_xlnet.py
transformers/tests/modeling_xlnet_test.py
==================
b90791e95;Rostislav Nedelchev;2019-11-30 15:57:51 +0100;fixed XLNet attenttion output for both attention streams

==

transformers/modeling_xlnet.py
==================
b0ee7c7df;maxvidal;2019-11-29 12:32:37 +0100;Added Camembert to available models

==

examples/run_lm_finetuning.py
==================
ecf15ebf3;Elad Segal;2019-11-29 12:48:41 +0200;Add ALBERT to AutoClasses

==

transformers/configuration_auto.py
transformers/modeling_auto.py
transformers/tokenization_auto.py
==================
4a666885b;thomwolf;2019-11-28 15:56:53 +0100;reducing my level of enthousiasm

==

transformers/modeling_tf_utils.py
==================
adb5c79ff;thomwolf;2019-11-28 15:51:43 +0100;update all tf.shape and tensor.shape to shape_list

==

templates/adding_a_new_model/modeling_tf_xxx.py
transformers/__init__.py
transformers/modeling_tf_albert.py
transformers/modeling_tf_bert.py
transformers/modeling_tf_ctrl.py
transformers/modeling_tf_distilbert.py
transformers/modeling_tf_gpt2.py
transformers/modeling_tf_openai.py
transformers/modeling_tf_roberta.py
transformers/modeling_tf_transfo_xl.py
transformers/modeling_tf_transfo_xl_utilities.py
transformers/modeling_tf_utils.py
transformers/modeling_tf_xlnet.py
==================
2421e54f8;Juha Kiili;2019-11-29 15:39:28 +0200;Add link to original source and license to download_glue.data.py

==

utils/download_glue_data.py
==================
41aa0e800;Juha Kiili;2019-11-29 15:33:25 +0200;Refactor logs and fix loss bug

==

examples/run_glue.py
==================
1ab8dc44b;Thomas Wolf;2019-11-29 09:26:33 +0100;Merge pull request #1876 from huggingface/mean-fix
Mean does not exist in TF2
==
==================
f0d22b636;Thomas Wolf;2019-11-29 09:25:47 +0100;Merge pull request #1873 from stefan-it/distilbert-german
German DistilBERT
==
==================
1e9ac5a7c;Lysandre;2019-11-28 17:43:47 -0500;New -> normal

==

transformers/data/processors/squad.py
==================
0b84b9fd8;Lysandre;2019-11-28 17:38:52 -0500;Add processors to __init__

==

transformers/__init__.py
transformers/data/__init__.py
transformers/data/processors/__init__.py
==================
f671997ef;Lysandre;2019-11-28 17:17:20 -0500;Interface with TFDS

==

transformers/data/processors/squad.py
==================
bd41e8292;Lysandre;2019-11-28 16:03:56 -0500;Cleanup & Evaluation now works

==

examples/run_squad.py
transformers/data/processors/squad.py
==================
d49c43ff7;Thomas Wolf;2019-11-28 16:08:37 +0100;Merge pull request #1778 from eukaryote31/patch-2
from_pretrained: convert DialoGPT format
==
==================
91caf2462;Thomas Wolf;2019-11-28 16:06:55 +0100;Merge pull request #1770 from huggingface/initi-encoder-mask
Only init encoder_attention_mask if stack is decoder
==
==================
49a69d5b7;Thomas Wolf;2019-11-28 15:24:08 +0100;Merge pull request #1753 from digantamisra98/patch-1
Added Mish Activation Function
==
==================
96e7ee723;Thomas Wolf;2019-11-27 23:28:30 +0100;Merge pull request #1740 from huggingface/fix-ctrl-past
Fix CTRL past
==
==================
8da47b078;thomwolf;2019-11-27 23:11:37 +0100;fix merge tests

==

transformers/modeling_ctrl.py
==================
8c276b9c9;Stefan Schweter;2019-11-27 18:11:49 +0100;Merge branch 'master' into distilbert-german

==
==================
3c28a2daa;Yao Lu;2019-11-27 11:45:22 -0500;add add_special_tokens=True for input examples

==

transformers/modeling_bert.py
==================
a36f981d1;Thomas Wolf;2019-11-27 17:25:46 +0100;Merge branch 'master' into fix-ctrl-past

==
==================
5afca00b4;Thomas Wolf;2019-11-27 17:14:49 +0100;Merge pull request #1724 from huggingface/fix_encode_plus
Fix encode_plus
==
==================
49108288b;Thomas Wolf;2019-11-27 17:11:07 +0100;Merge pull request #1624 from Huawei-MRC-OSI/resumable_http
Add support for resumable downloads for HTTP protocol.
==
==================
5340d1f21;Thomas Wolf;2019-11-27 17:10:36 +0100;Merge branch 'master' into resumable_http

==
==================
10bd1ddb3;VictorSanh;2019-11-25 19:41:00 +0000;soft launch distilbert multilingual

==

transformers/configuration_distilbert.py
transformers/modeling_distilbert.py
transformers/tokenization_distilbert.py
==================
d5478b939;VictorSanh;2019-11-25 19:40:48 +0000;add distilbert + update run_xnli wrt run_glue

==

examples/run_xnli.py
==================
07ab8d7af;VictorSanh;2019-11-05 17:33:14 -0500;fix bug

==

transformers/data/processors/xnli.py
==================
d47402263;VictorSanh;2019-11-05 12:56:03 -0500;cleaning simple_accuracy since not used anymore

==

transformers/data/processors/xnli.py
==================
bcd8dc6b4;VictorSanh;2019-11-05 12:53:08 -0500;move xnli_compute_metrics to data/metrics

==

transformers/__init__.py
transformers/data/__init__.py
transformers/data/metrics/__init__.py
transformers/data/processors/xnli.py
==================
73fe2e738;VictorSanh;2019-11-05 12:51:43 -0500;remove fstrings

==

examples/run_xnli.py
transformers/data/processors/xnli.py
==================
3e7656f7a;VictorSanh;2019-11-05 11:58:53 -0500;update readme

==

examples/README.md
==================
abd397e95;VictorSanh;2019-11-05 10:57:24 -0500;uniformize w/ the cache_dir update

==

examples/run_xnli.py
==================
d75d49a51;VictorSanh;2019-11-05 10:32:20 -0500;add XnliProcessor to doc

==

docs/source/main_classes/processors.rst
==================
d5910b312;VictorSanh;2019-11-05 10:21:25 -0500;move xnli processor (and utils) to transformers/data/processors

==

examples/run_xnli.py
transformers/__init__.py
transformers/data/__init__.py
transformers/data/processors/__init__.py
transformers/data/processors/xnli.py
==================
289cf4d2b;VictorSanh;2019-11-05 10:10:02 -0500;change default for XNLI: dev --> test

==

examples/run_xnli.py
examples/utils_xnli.py
==================
cb7b77a8a;VictorSanh;2019-10-30 18:13:52 -0400;fix some typos

==

transformers/tokenization_xlm.py
==================
84a0b522c;VictorSanh;2019-10-29 18:53:45 -0400;mbert reproducibility results

==

examples/README.md
==================
c4336ecbb;VictorSanh;2019-10-29 12:04:20 -0400;xnli - output_mode consistency

==

examples/run_xnli.py
==================
d52e98ff9;VictorSanh;2019-10-29 11:51:15 -0400;add xnli examples/README.md

==

examples/README.md
==================
71f71ddb3;VictorSanh;2019-10-29 11:50:42 -0400;run_xnli + utils_xnli

==

examples/run_xnli.py
examples/utils_xnli.py
==================
b5d884d25;Julien Chaumond;2019-11-27 11:05:18 -0500;Uniformize #1952

==

README.md
examples/README.md
==================
7fd1d42a0;Thomas Wolf;2019-11-27 17:05:18 +0100;Merge pull request #1592 from watkinsm/do_lower_case
Consider do_lower_case in PreTrainedTokenizer
==
==================
21637d492;Thomas Wolf;2019-11-27 17:04:39 +0100;Merge branch 'master' into do_lower_case

==
==================
de2696f68;R√©mi Louf;2019-11-26 16:30:27 +0100;suggest to track repo w/ https rather than ssh

==

CONTRIBUTING.md
==================
88b317739;root;2019-11-27 09:58:38 +0000;Fix issue: #1962, input's shape seem to cause error in 2.2.0 version tf_albert_model

==

transformers/modeling_tf_albert.py
==================
45d767297;Lysandre;2019-11-27 10:12:20 -0500;Updated v2.2.0 doc

==

.circleci/deploy.sh
==================
361620954;Lysandre;2019-11-27 10:11:37 -0500;Remove TFBertForPreTraining from ALBERT doc

==

docs/source/model_doc/albert.rst
==================
cc7968227;Lysandre;2019-11-26 15:52:25 -0500;Updated v2.2.0 doc

==

.circleci/deploy.sh
==================
ce02550d5;Lysandre;2019-11-26 15:47:02 -0500;Fix pretrained models table

==

docs/source/pretrained_models.rst
==================
cf26a0c85;Lysandre;2019-11-26 15:40:03 -0500;Fix pretrained models table

==

README.md
docs/source/pretrained_models.rst
==================
44b82c777;Lysandre;2019-11-26 15:15:11 -0500;Updated v2.2.0 doc

==

.circleci/deploy.sh
==================
ee4647bd5;Lysandre;2019-11-26 15:10:51 -0500;CamemBERT & ALBERT doc

==

docs/source/index.rst
docs/source/model_doc/albert.rst
docs/source/model_doc/camembert.rst
==================
7c6000e41;Lysandre;2019-11-26 14:55:29 -0500;Updated v2.2.0 doc

==

.circleci/deploy.sh
==================
668aac45d;Lysandre;2019-11-26 14:52:42 -0500;Pretrained models

==

docs/source/pretrained_models.rst
==================
8742baa53;Julien Chaumond;2019-11-26 14:39:47 -0500;Improve test protocol for inputs_embeds in TF

==

transformers/tests/modeling_tf_common_test.py
==================
cf62bdc96;Julien Chaumond;2019-11-26 14:37:32 -0500;Improve test protocol for inputs_embeds in TF
cc @lysandrejik

==

transformers/tests/modeling_tf_common_test.py
==================
b63214527;Lysandre Debut;2019-11-26 14:27:15 -0500;Update master documentation link in README

==

README.md
==================
ae98d4599;Lysandre;2019-11-26 14:12:44 -0500;Release: v2.2.0

==

.circleci/deploy.sh
README.md
deploy_multi_version_doc.sh
docs/source/conf.py
setup.py
transformers/__init__.py
==================
f2f329408;Lysandre;2019-11-26 12:59:28 -0500;Fix input embeddings

==

transformers/tests/modeling_albert_test.py
transformers/tests/modeling_tf_albert_test.py
transformers/tests/modeling_tf_common_test.py
==================
bdfe21ab2;Julien Chaumond;2019-11-26 12:54:36 -0500;Change param order for consistency

==

transformers/modeling_albert.py
==================
c536c2a48;LysandreJik;2019-11-26 11:22:52 -0500;ALBERT Input Embeds

==

transformers/modeling_albert.py
transformers/modeling_tf_albert.py
==================
f873b55e4;LysandreJik;2019-11-26 10:28:41 -0500;Warning for ALBERT-v2 models

==

transformers/modeling_utils.py
==================
c9cb7f8a0;Lysandre;2019-11-11 15:12:54 -0500;Torch 1.1.0 compatibility + FP16 O1 + TF checkpoints
Co-authored-by: wassname

==

transformers/modeling_albert.py
transformers/modeling_tf_albert.py
==================
b18509c20;Lysandre;2019-11-08 00:12:21 +0000;Tests for ALBERT in TF2 + fixes

==

transformers/__init__.py
transformers/modeling_tf_albert.py
transformers/tests/modeling_tf_albert_test.py
==================
7bddbf596;Lysandre;2019-11-07 23:50:05 +0000;TFAlbertForSequenceClassification

==

transformers/modeling_tf_albert.py
==================
f6f382532;Lysandre;2019-11-07 23:40:45 +0000;ALBERT in TF2

==

transformers/__init__.py
transformers/convert_pytorch_checkpoint_to_tf2.py
transformers/modeling_tf_albert.py
==================
d9daad98c;Lysandre;2019-11-07 19:55:43 +0000;Re-ordering of group_idx/layer_idx + Python 2 tests

==

transformers/modeling_albert.py
==================
9d5c49546;Lysandre;2019-11-07 17:32:52 +0000;Tests for AlbertForQuestionAnswering AlbertForSequenceClassification

==

transformers/tests/modeling_albert_test.py
==================
16263f968;Lysandre;2019-11-07 17:29:29 +0000;Headmasking

==

transformers/modeling_albert.py
transformers/tests/modeling_albert_test.py
==================
abb23a78b;Lysandre;2019-11-07 17:09:16 +0000;Head pruning for ALBERT

==

transformers/modeling_albert.py
transformers/tests/modeling_albert_test.py
==================
4374eaea7;Lysandre;2019-11-06 20:47:42 +0000;ALBERT for SQuAD

==

examples/run_squad.py
transformers/__init__.py
transformers/modeling_albert.py
==================
70d99980d;Lysandre;2019-11-04 11:34:30 -0500;ALBERT-V2

==

transformers/configuration_albert.py
transformers/convert_albert_original_tf_checkpoint_to_pytorch.py
transformers/modeling_albert.py
transformers/tokenization_albert.py
==================
c110c41fd;Lysandre;2019-11-01 21:59:40 +0000;Run GLUE and remove LAMB

==

examples/run_glue.py
transformers/optimization.py
==================
6637a77f8;Lysandre;2019-11-01 15:17:31 +0000;AlbertForSequenceClassification

==

transformers/__init__.py
transformers/modeling_albert.py
==================
0d07a23c0;Lysandre;2019-11-01 15:07:01 +0000;LAMB implementation

==

transformers/optimization.py
==================
c98754559;Lysandre;2019-10-31 18:48:02 +0000;Converting script

==

transformers/__init__.py
transformers/convert_albert_original_tf_checkpoint_to_pytorch.py
==================
4f3a54bfc;Lysandre;2019-10-31 16:37:34 +0000;ALBERT can load pre-trained models. Doesn't inherit from BERT anymore.

==

transformers/__init__.py
transformers/configuration_albert.py
transformers/modeling_albert.py
transformers/tokenization_albert.py
==================
c4403006b;Lysandre;2019-10-31 15:30:11 +0000;External MLM head

==

transformers/configuration_albert.py
transformers/modeling_albert.py
==================
b21402fc8;Lysandre;2019-10-31 14:19:31 +0000;Python 2 tests + licence

==

transformers/modeling_albert.py
transformers/tokenization_albert.py
==================
c14a22272;Lysandre;2019-10-31 14:04:10 +0000;ALBERT passes all tests

==

transformers/configuration_albert.py
transformers/modeling_albert.py
transformers/tests/tokenization_albert_test.py
==================
870320a24;Lysandre;2019-10-30 22:30:21 +0000;Early tests

==

transformers/modeling_albert.py
transformers/tests/modeling_albert_test.py
==================
25a31953e;Lysandre;2019-10-30 21:18:06 +0000;Output Attentions + output hidden states

==

transformers/modeling_albert.py
==================
ce9eade29;Lysandre;2019-10-30 20:50:44 +0000;Initializer range using BertPreTrainedModel

==

transformers/modeling_albert.py
==================
5680a1106;Lysandre;2019-10-30 20:42:49 +0000;Activation function managed from the config file

==

transformers/configuration_albert.py
transformers/modeling_albert.py
==================
1e5b31c38;Lysandre;2019-10-30 20:25:32 +0000;Several fixes and improvements

==

transformers/modeling_albert.py
transformers/tests/fixtures/spiece.model
transformers/tokenization_albert.py
==================
ee20201d3;Lysandre;2019-10-30 16:19:49 +0000;Tokenization tests + fixes + init

==

transformers/__init__.py
transformers/tests/fixtures/30k-clean.model
transformers/tests/tokenization_albert_test.py
transformers/tokenization_albert.py
transformers/tokenization_xlnet.py
==================
e3ea5d1d8;Lysandre;2019-10-30 15:03:30 +0000;Docstrings

==

transformers/modeling_albert.py
==================
fedac786d;Lysandre;2019-10-30 14:39:16 +0000;Tokenization + small fixes

==

transformers/modeling_albert.py
transformers/tokenization_albert.py
==================
67b422662;Lysandre;2019-10-29 23:33:53 +0000;Documentation + improved AlbertForMaskedLM

==

transformers/modeling_albert.py
==================
1b9256433;Lysandre;2019-10-29 23:23:18 +0000;Reorganize and cleanup

==

transformers/modeling_albert.py
==================
12290c0d5;Lysandre;2019-10-29 23:19:02 +0000;Handles multi layer and multi groups

==

transformers/modeling_albert.py
==================
139affaa8;Lysandre;2019-10-29 22:45:24 +0000;Albert layer/layer groups

==

transformers/modeling_albert.py
==================
91ccbae78;Lysandre;2019-10-29 21:21:57 +0000;Accepts multiple sizes

==

transformers/modeling_albert.py
==================
c0c208833;Lysandre;2019-10-29 19:57:38 +0000;ALBERT model

==

transformers/configuration_albert.py
transformers/convert_albert_original_tf_checkpoint_to_pytorch.py
transformers/modeling_albert.py
==================
8e5d84fcc;v_sboliu;2019-11-26 12:04:31 +0800;Fixed typo

==

transformers/modeling_bert.py
==================
0669c1fcd;Lysandre;2019-11-25 19:22:21 -0500;SQuAD v2 BERT + XLNet

==

transformers/__init__.py
transformers/data/__init__.py
transformers/data/processors/__init__.py
transformers/data/processors/squad.py
==================
5d3b8daad;manansanghi;2019-11-22 10:31:54 -0800;Minor bug fixes on run_ner.py

==

examples/run_ner.py
==================
aa92a184d;ƒ∞brahim Ethem Demirci;2019-11-16 16:49:37 +0300;resize model when special tokenizer present

==

examples/run_lm_finetuning.py
==================
07bf43074;Bilal Khan;2019-11-21 20:52:06 -0600;Fix GPT2 docstring

==

transformers/tokenization_gpt2.py
==================
fa963ecc5;Evpok Padding;2019-11-21 11:03:12 +0100;if‚Üíelif

==

transformers/modeling_auto.py
==================
c8eb8157b;Evpok Padding;2019-11-21 11:01:20 +0100;fix docstrings

==

transformers/modeling_auto.py
==================
99f750d64;Evpok Padding;2019-11-21 10:35:07 +0100;add Camembert models to modeling_auto

==

transformers/modeling_auto.py
==================
7485caefb;Lysandre;2019-11-25 09:33:39 -0500;fix #1894

==

examples/run_lm_finetuning.py
==================
afaa33585;Julien Chaumond;2019-11-23 11:34:45 -0500;[doc] Fix assets urls

==

docs/source/_static/js/custom.js
==================
176cd1ce1;Julien Chaumond;2019-11-23 11:18:54 -0500;[doc] homogenize instructions slightly

==

README.md
examples/README.md
==================
041a901f3;Nikolay Korolev;2019-11-22 22:56:43 +0300;Fix typo in documentation. toto -> to

==

transformers/tokenization_gpt2.py
==================
e0e55bc55;Lysandre;2019-11-22 16:18:18 -0500;Manage training example & refactor the refactor

==

transformers/data/processors/squad.py
==================
c3ba64523;Lysandre;2019-11-22 14:36:49 -0500;Works for XLNet

==

examples/run_squad.py
transformers/data/processors/squad.py
==================
a5a8a6175;LysandreJik;2019-11-21 19:18:20 -0500;Works for BERT

==

transformers/data/processors/squad.py
==================
a7dafe2f4;LysandreJik;2019-11-21 11:30:40 -0500;Padding strategy (left and right) rather than boolean flag

==

transformers/tests/tokenization_tests_commons.py
transformers/tokenization_utils.py
==================
9f374c825;LysandreJik;2019-11-22 16:27:15 -0500;`encode` and `encode_plus` handle attention masks and padding

==

transformers/tests/tokenization_tests_commons.py
transformers/tokenization_utils.py
transformers/tokenization_xlnet.py
==================
72e506b22;Lysandre;2019-11-19 09:49:55 -0500;wip

==

examples/run_squad.py
transformers/__init__.py
transformers/data/__init__.py
transformers/data/processors/__init__.py
transformers/data/processors/squad.py
transformers/tokenization_utils.py
==================
ea52f8245;Lysandre;2019-11-18 14:42:59 -0500;Moved some SQuAD logic to /data

==

transformers/__init__.py
transformers/data/__init__.py
transformers/data/processors/__init__.py
transformers/data/processors/squad.py
==================
26db31e0c;R√©mi Louf;2019-11-20 18:13:38 +0100;update the documentation

==

examples/README.md
==================
6f70bb8c6;R√©mi Louf;2019-11-20 18:01:03 +0100;add instructions to run the examples

==

README.md
==================
05d4232f6;Juha Kiili;2019-11-21 12:38:17 +0200;Add valohai.yaml

==

valohai.yaml
==================
aac355140;Aarni Koskela;2019-11-21 12:37:39 +0200;Add download_glue_data.py from kamalkraj/ALBERT-TF2.0
Original source: https://github.com/kamalkraj/ALBERT-TF2.0/blob/fa90194e5fe729dbb19f32ac29c8d6d6372c0f93/download_glue_data.py
Original license: https://github.com/kamalkraj/ALBERT-TF2.0/blob/fa90194e5fe729dbb19f32ac29c8d6d6372c0f93/LICENSE (Apache-2.0)

==

utils/download_glue_data.py
==================
2cf3447e0;Juha Kiili;2019-11-21 12:35:25 +0200;Glue: log in Valohai-compatible JSON format too

==

examples/run_glue.py
==================
0cdfcca24;Thomas Wolf;2019-11-21 10:56:07 +0100;Merge pull request #1860 from stefan-it/camembert-for-token-classification
[WIP] Add support for CamembertForTokenClassification
==
==================
e70cdf083;Jin Young Sohn;2019-11-20 22:33:26 +0000;Cleanup TPU bits from run_glue.py
TPU runner is currently implemented in:
https://github.com/pytorch-tpu/transformers/blob/tpu/examples/run_glue_tpu.py.

We plan to upstream this directly into `huggingface/transformers`
(either `master` or `tpu`) branch once it's been more thoroughly tested.

==

examples/run_glue.py
==================
454455c69;Lysandre;2019-11-20 09:42:48 -0500;fix #1879

==

examples/utils_squad.py
==================
3de31f8d2;Lysandre;2019-11-19 18:14:14 -0500;mean does not exist in TF2

==

transformers/modeling_tf_utils.py
==================
da06afafc;Stefan Schweter;2019-11-19 21:57:00 +0100;tree-wide: add trailing comma in configuration maps

==

transformers/configuration_distilbert.py
transformers/modeling_distilbert.py
transformers/tokenization_distilbert.py
==================
2e2c0375c;Stefan Schweter;2019-11-19 20:41:18 +0100;distilbert: add German distilbert model to positional embedding sizes map

==

transformers/tokenization_distilbert.py
==================
e7cf2ccd1;Stefan Schweter;2019-11-19 19:55:19 +0100;distillation: add German distilbert model

==

examples/distillation/README.md
==================
e631383d4;Stefan Schweter;2019-11-19 19:52:40 +0100;docs: add new German distilbert model to pretrained models

==

docs/source/pretrained_models.rst
==================
f21dfe36b;Stefan Schweter;2019-11-19 19:51:31 +0100;distilbert: add vocab for new German distilbert model

==

transformers/tokenization_distilbert.py
==================
22333945f;Stefan Schweter;2019-11-19 19:51:01 +0100;distilbert: add pytorch model for new German distilbert model

==

transformers/modeling_distilbert.py
==================
337802783;Stefan Schweter;2019-11-19 19:50:32 +0100;distilbert: add configuration for new German distilbert model

==

transformers/configuration_distilbert.py
==================
4193aa9f8;alexzubiaga;2019-11-19 10:32:08 +0100;add TFXLNetForTokenClassification implementation and unit test
add XLNetForTokenClassification implementation and unit tests

==

transformers/__init__.py
transformers/modeling_tf_xlnet.py
transformers/modeling_xlnet.py
transformers/tests/modeling_tf_xlnet_test.py
transformers/tests/modeling_xlnet_test.py
==================
f3386d938;Kazutoshi Shinoda;2019-11-17 18:08:51 +0900;typo "deay" -> "decay"

==

examples/run_squad.py
==================
56c84863a;Stefan Schweter;2019-11-18 15:50:16 +0100;camembert: add support for CamemBERT in run_ner example

==

examples/run_ner.py
==================
0b3d45eb6;Stefan Schweter;2019-11-18 15:49:44 +0100;camembert: add implementation for save_vocabulary method

==

transformers/tokenization_camembert.py
==================
3916b334a;Julien Chaumond;2019-11-18 09:29:11 -0500;[camembert] Acknowledge the full author list

==

README.md
==================
44455eb5b;Sebastian Stabinger;2019-11-18 10:07:05 +0100;Adds CamemBERT to Model architectures list

==

README.md
==================
33753d913;Stefan Schweter;2019-11-18 14:14:54 +0100;module: import CamembertForTokenClassification

==

transformers/__init__.py
==================
d32ce2c8d;Stefan Schweter;2019-11-18 14:14:19 +0100;camembert: add wrapper for CamembertForTokenClassification

==

transformers/modeling_camembert.py
==================
d08a338c3;Yohei Tamura;2019-11-16 18:47:37 +0900;modified:   transformers/modeling_utils.py

==

transformers/modeling_utils.py
==================
0477b307c;Julien Chaumond;2019-11-15 23:54:11 -0500;[camembert] tokenizer: use additional_special_tokens

==

transformers/tokenization_camembert.py
==================
f9abf73e3;Julien Chaumond;2019-11-15 23:40:20 -0500;[camembert] realign w/ recent changes

==

transformers/modeling_camembert.py
transformers/tokenization_camembert.py
==================
26858f27c;Julien Chaumond;2019-11-15 23:23:31 -0500;[camembert] Upload to s3 + rename script

==

examples/contrib/run_camembert.py
transformers/configuration_camembert.py
transformers/modeling_camembert.py
transformers/tokenization_camembert.py
==================
035fea531;Louis MARTIN;2019-11-12 17:41:41 -0800;Add CamemBERT to auto files and docs

==

docs/source/pretrained_models.rst
transformers/configuration_auto.py
transformers/tokenization_auto.py
==================
694d4fcbb;Louis MARTIN;2019-11-12 17:28:37 -0800;Add CamemBERT classes to __init__.py

==

transformers/__init__.py
==================
3e20c2e87;Louis MARTIN;2019-11-12 17:16:24 -0800;Update demo_camembert.py with new classes

==

examples/contrib/demo_camembert.py
==================
f12e4d8da;Louis MARTIN;2019-11-12 17:14:50 -0800;Move demo_camembert.py to examples/contrib

==

examples/contrib/demo_camembert.py
==================
fb6c70a91;Louis MARTIN;2019-11-12 17:11:49 -0800;Update tokenization_camembert.py with urls

==

transformers/tokenization_camembert.py
==================
e44b939e7;Louis MARTIN;2019-11-12 17:11:00 -0800;Add configuration_camembert.py and modeling_camembert.py

==

transformers/configuration_camembert.py
transformers/modeling_camembert.py
==================
6e72fd094;Louis MARTIN;2019-11-08 17:09:48 -0800;Add demo_camembert.py

==

examples/demo_camembert.py
==================
14b3aa3b3;Louis MARTIN;2019-11-08 16:47:21 -0800;Add tokenization_camembert.py

==

transformers/tokenization_camembert.py
==================
ca99a2d50;Xu Hongshen;2019-11-15 14:55:26 +0800;Update example readme

==

examples/README.md
==================
7da3ef24c;Xu Hongshen;2019-11-12 16:11:24 +0800;add is_impossible tensor to model inputs during fine-tuning xlnet on squad2.0

==

examples/run_squad.py
==================
74ce8de7d;Thomas Wolf;2019-11-14 22:47:53 +0100;Merge pull request #1792 from stefan-it/distilbert-for-token-classification
DistilBERT for token classification
==
==================
05db5bc1a;Thomas Wolf;2019-11-14 22:40:22 +0100;added small comparison between BERT, RoBERTa and DistilBERT

==

examples/README.md
==================
9629e2c67;Thomas Wolf;2019-11-14 22:24:05 +0100;Merge pull request #1804 from ronakice/master
fix multi-gpu eval in torch examples
==
==================
5b322a36d;Thomas Wolf;2019-11-14 22:17:24 +0100;Merge pull request #1811 from huggingface/special-tokens
Fix special tokens addition in decoder #1807
==
==================
1a237d7f4;Thomas Wolf;2019-11-14 22:11:54 +0100;Merge pull request #1831 from iedmrc/gpt2-tokenization-sum-func-replacement
sum() is replaced by itertools.chain.from_iterable()
==
==================
df99f8c5a;Thomas Wolf;2019-11-14 22:10:31 +0100;Merge pull request #1832 from huggingface/memory-leak-schedulers
replace LambdaLR scheduler wrappers by function
==
==================
0be9ae7b3;Thomas Wolf;2019-11-14 22:04:49 +0100;Merge pull request #1833 from huggingface/max-length-warning
Token indices sequence length is longer than the specified maximum sequence length for this model
==
==================
be7f2aacc;Lysandre;2019-11-14 14:54:44 -0500;[CI][DOC] Don't rebuild if folder exists - Correct directory.

==

.circleci/deploy.sh
==================
8f8d69716;Lysandre;2019-11-14 14:48:21 -0500;[CI][DOC] Don't rebuild if folder exists.

==

.circleci/deploy.sh
==================
2276bf69b;R√©mi Louf;2019-11-14 18:00:14 +0100;update the examples, docs and template

==

README.md
docs/source/main_classes/optimizer_schedules.rst
docs/source/migration.md
examples/contrib/run_openai_gpt.py
examples/contrib/run_swag.py
examples/distillation/distiller.py
examples/distillation/run_squad_w_distillation.py
examples/run_glue.py
examples/run_lm_finetuning.py
examples/run_multiple_choice.py
examples/run_ner.py
examples/run_squad.py
templates/adding_a_new_example_script/run_xxx.py
==================
d7929899d;Lysandre;2019-11-14 10:49:00 -0500;Specify checkpoint in saved file for run_lm_finetuning.py

==

examples/run_lm_finetuning.py
==================
a67e74788;Lysandre;2019-11-14 10:30:22 -0500;Reorganized max_len warning

==

transformers/tokenization_utils.py
==================
e18f786cd;Lysandre;2019-11-14 10:06:00 -0500;Quickstart example showcasing past

==

docs/source/quickstart.md
==================
022525b00;R√©mi Louf;2019-11-12 11:08:47 +0100;replace LambdaLR scheduler wrappers by function
Custom schedulers are currently initiated by wrapping Pytorch's LambdaLR
class and passing a method of the wrapping class to the __init__
function of LambdaLR. This approach is not appropriate for several
reasons:

1. one does not need to define a class when it only defines a
__init__() method;
2. instantiating the parent class by passing a method of the child class
creates a cyclical reference which leads to memory leaks. See issues #1742 and #1134.

In this commit we replace the wrapper classes with functions that
instantiate `LambdaLR` with a custom learning rate function. We use a
closure to specify the parameter of the latter. We also do a bit of
renaming within the function to explicit the behaviour and removed
docstrings that were subsequently not necessary.

==

transformers/__init__.py
transformers/optimization.py
transformers/tests/optimization_test.py
==================
7627dde1f;ƒ∞brahim Ethem Demirci;2019-11-14 17:06:15 +0300;sum() is the leanest method to flatten a string list, so it's been replaced by itertools.chain.from_iterable()

==

transformers/tokenization_utils.py
==================
74d0bcb6f;Lysandre;2019-11-12 15:27:57 -0500;Fix special tokens addition in decoder

==

transformers/tests/tokenization_tests_commons.py
transformers/tokenization_utils.py
==================
155c782a2;Julien Chaumond;2019-11-11 22:19:14 -0500;[inputs_embeds] All TF models + tests

==

transformers/modeling_tf_bert.py
transformers/modeling_tf_ctrl.py
transformers/modeling_tf_distilbert.py
transformers/modeling_tf_gpt2.py
transformers/modeling_tf_openai.py
transformers/modeling_tf_roberta.py
transformers/modeling_tf_transfo_xl.py
transformers/modeling_tf_xlm.py
transformers/modeling_tf_xlnet.py
transformers/tests/modeling_tf_bert_test.py
transformers/tests/modeling_tf_common_test.py
==================
2aef2f0bb;Julien Chaumond;2019-11-11 20:03:19 -0500;[common attributes] Fix previous commit for transfo-xl

==

transformers/__init__.py
transformers/tests/modeling_common_test.py
==================
2f1746426;Julien Chaumond;2019-11-11 19:56:45 -0500;[common attributes] Slightly sharper test coverage

==

transformers/tests/modeling_common_test.py
==================
9d2398fd9;Julien Chaumond;2019-11-11 16:37:08 -0500;Ooopsie

==

transformers/tests/modeling_tf_common_test.py
==================
70d97ddd6;Julien Chaumond;2019-11-11 16:30:22 -0500;[TF models] Common attributes as per #1721

==

transformers/modeling_tf_bert.py
transformers/modeling_tf_ctrl.py
transformers/modeling_tf_distilbert.py
transformers/modeling_tf_gpt2.py
transformers/modeling_tf_openai.py
transformers/modeling_tf_roberta.py
transformers/modeling_tf_transfo_xl.py
transformers/modeling_tf_utils.py
transformers/modeling_tf_xlm.py
transformers/modeling_tf_xlnet.py
transformers/tests/modeling_tf_common_test.py
==================
872403be1;Julien Chaumond;2019-11-11 15:22:54 -0500;This is not a @property after all

==

templates/adding_a_new_model/modeling_xxx.py
==================
dd6b2e05e;Julien Chaumond;2019-11-11 15:18:09 -0500;whitespace

==

.gitignore
transformers/modeling_tf_utils.py
==================
d409aca32;Lysandre;2019-11-12 10:59:37 -0500;Clarify the use of past in GPT2 and CTRL

==

transformers/modeling_ctrl.py
transformers/modeling_gpt2.py
==================
7246d3c2f;Michael Watkins;2019-11-06 13:18:16 +0200;Consider do_lower_case in PreTrainedTokenizer
As pointed out in #1545, when using an uncased model, and adding
a new uncased token, the tokenizer does not correctly identify this
in the case that the input text contains the token in a cased format.

For instance, if we load bert-base-uncased into BertTokenizer, and
then use .add_tokens() to add "cool-token", we get the expected
result for .tokenize('this is a cool-token'). However, we get a
possibly unexpected result for .tokenize('this is a cOOl-Token'),
which in fact mirrors the result for the former from before the new
token was added.

This commit adds
- functionality to PreTrainedTokenizer to handle this
situation in case a tokenizer (currently Bert, DistilBert,
and XLNet) has the do_lower_case=True kwarg by:
    1) lowercasing tokens added with .add_tokens()
    2) lowercasing text at the beginning of .tokenize()
- new common test case for tokenizers

https://github.com/huggingface/transformers/issues/1545

==

transformers/tests/tokenization_tests_commons.py
transformers/tokenization_utils.py
==================
2e3117655;ronakice;2019-11-12 05:55:11 -0500;fix multi-gpu eval

==

examples/run_glue.py
examples/run_lm_finetuning.py
examples/run_multiple_choice.py
examples/run_ner.py
examples/run_squad.py
examples/run_summarization_finetuning.py
==================
8aba81a0b;thomwolf;2019-11-12 08:52:43 +0100;fix #1789

==

README.md
==================
94e55253a;Stefan Schweter;2019-11-11 16:20:15 +0100;tests: add test case for DistilBertForTokenClassification implementation

==

transformers/tests/modeling_distilbert_test.py
==================
2b07b9e5e;Stefan Schweter;2019-11-11 16:19:34 +0100;examples: add DistilBert support for NER fine-tuning

==

examples/run_ner.py
==================
1806eabf5;Stefan Schweter;2019-11-11 16:18:48 +0100;module: add DistilBertForTokenClassification import

==

transformers/__init__.py
==================
1c7253cc5;Stefan Schweter;2019-11-11 16:18:16 +0100;modeling: add DistilBertForTokenClassification implementation

==

transformers/modeling_distilbert.py
==================
b5d330d11;Lysandre;2019-11-11 10:15:14 -0500;Fix #1784

==

templates/adding_a_new_model/tokenization_xxx.py
transformers/tokenization_bert.py
transformers/tokenization_roberta.py
transformers/tokenization_utils.py
transformers/tokenization_xlm.py
transformers/tokenization_xlnet.py
==================
90f6e73a3;eukaryote;2019-11-09 16:46:19 +0000;Add DialoGPT support for Pytorch->TF

==

transformers/modeling_tf_pytorch_utils.py
==================
ef9985296;eukaryote;2019-11-09 16:32:40 +0000;from_pretrained: convert DialoGPT format
DialoGPT checkpoints have "lm_head.decoder.weight" instead of "lm_head.weight". 

(see: https://www.reddit.com/r/MachineLearning/comments/dt5woy/p_dialogpt_state_of_the_art_conversational_model/f6vmwuy?utm_source=share&utm_medium=web2x)
==

transformers/modeling_utils.py
==================
7a9aae104;Adrian Bauer;2019-11-07 17:08:39 -0500;Fix run_bertology.py
Make imports and args.overwrite_cache match run_glue.py

==

examples/run_bertology.py
==================
268d4f209;thomwolf;2019-11-08 16:41:55 +0100;fix position biases + better tests

==

transformers/modeling_t5.py
transformers/tests/modeling_t5_test.py
==================
b4fcd59a5;thomwolf;2019-11-08 14:38:53 +0100;add sentinels in tokenizer

==

transformers/tokenization_t5.py
==================
15e53c4e8;thomwolf;2019-11-08 12:43:21 +0100;maybe fix tests

==

transformers/tests/modeling_tf_common_test.py
==================
f03c0c142;thomwolf;2019-11-08 11:49:46 +0100;adding models in readme and auto classes

==

README.md
docs/source/pretrained_models.rst
transformers/__main__.py
transformers/convert_pytorch_checkpoint_to_tf2.py
transformers/modeling_auto.py
transformers/modeling_tf_auto.py
transformers/tokenization_auto.py
==================
4321c5412;thomwolf;2019-11-08 11:49:32 +0100;fix tests

==

transformers/tests/modeling_tf_common_test.py
==================
727a79b30;thomwolf;2019-11-08 11:35:03 +0100;added TF2 model and tests - updated templates

==

templates/adding_a_new_model/modeling_tf_xxx.py
templates/adding_a_new_model/modeling_xxx.py
transformers/__init__.py
transformers/configuration_auto.py
transformers/configuration_t5.py
transformers/modeling_t5.py
transformers/modeling_tf_pytorch_utils.py
transformers/modeling_tf_t5.py
transformers/modeling_utils.py
transformers/tests/modeling_tf_common_test.py
transformers/tests/modeling_tf_t5_test.py
==================
cd286c214;R√©mi Louf;2019-11-08 11:31:16 +0100;add condition around mask transformation

==

transformers/modeling_bert.py
==================
28d0ba35d;R√©mi Louf;2019-11-08 11:22:19 +0100;only init encoder_attention_mask if stack is decoder
We currently initialize `encoder_attention_mask` when it is `None`,
whether the stack is that of an encoder or a decoder. Since this
may lead to bugs that are difficult to tracks down, I added a condition
that assesses whether the current stack is a decoder.

==

transformers/modeling_bert.py
==================
8fda532c3;thomwolf;2019-11-07 17:09:50 +0100;fix python 2 sentencepiece tokenization

==

transformers/tests/tokenization_t5_test.py
transformers/tokenization_t5.py
==================
ba10065c4;thomwolf;2019-11-07 15:55:36 +0100;update model, conversion script, tests and template

==

templates/adding_a_new_model/convert_xxx_original_tf_checkpoint_to_pytorch.py
transformers/__init__.py
transformers/configuration_t5.py
transformers/convert_t5_original_tf_checkpoint_to_pytorch.py
transformers/modeling_t5.py
transformers/tests/modeling_common_test.py
transformers/tests/modeling_t5_test.py
transformers/tokenization_t5.py
==================
070dcf1c0;Diganta Misra;2019-11-07 03:45:43 +0530;Added Mish Activation Function
Mish is a new activation function proposed here - https://arxiv.org/abs/1908.08681
It has seen some recent success and has been adopted in SpaCy, Thic, TensorFlow Addons and FastAI-dev. 
All benchmarks recorded till now (including against ReLU, Swish and GELU) is present in the repository - https://github.com/digantamisra98/Mish
Might be a good addition to experiment with especially in the Bert Model.
==

transformers/modeling_bert.py
==================
1c542df7e;Julien Chaumond;2019-11-06 16:26:31 -0500;Add RoBERTa-based GPT-2 Output Detector from OpenAI
converted from https://github.com/openai/gpt-2-output-dataset/tree/master/detector

Co-Authored-By: Lysandre Debut <lysandre.debut@reseau.eseo.fr>
Co-Authored-By: Jong Wook Kim <jongwook@nyu.edu>
Co-Authored-By: Jeff Wu <wuthefwasthat@gmail.com>

==

docs/source/pretrained_models.rst
transformers/configuration_roberta.py
transformers/modeling_roberta.py
transformers/tokenization_roberta.py
==================
2f3a42101;Julien Chaumond;2019-11-06 16:43:09 +0000;Fix other PyTorch models

==

templates/adding_a_new_model/modeling_xxx.py
transformers/modeling_distilbert.py
==================
d5319793c;Julien Chaumond;2019-11-06 16:21:00 +0000;Fix BERT

==

transformers/modeling_bert.py
==================
27e015bd5;Julien Chaumond;2019-11-06 15:56:12 +0000;[tests] Flag to test on cuda

==

transformers/tests/conftest.py
transformers/tests/modeling_bert_test.py
==================
13d9135fa;Julien Chaumond;2019-11-06 14:44:00 +0000;[tests] get rid of warning
cf. https://docs.pytest.org/en/latest/example/simple.html

==

transformers/tests/conftest.py
==================
076a20793;thomwolf;2019-11-06 11:52:50 +0100;adding tests and updating model

==

transformers/__init__.py
transformers/configuration_t5.py
transformers/modeling_t5.py
transformers/tests/modeling_common_test.py
transformers/tests/modeling_t5_test.py
transformers/tests/modeling_tf_t5_test.py
transformers/tests/tokenization_t5_test.py
==================
73f2c342f;thomwolf;2019-11-06 11:52:39 +0100;fixing template

==

templates/adding_a_new_model/configuration_xxx.py
templates/adding_a_new_model/modeling_xxx.py
==================
3835e1e65;thomwolf;2019-11-06 11:52:29 +0100;adding tokenizer

==

transformers/tokenization_t5.py
==================
f88c104d8;Julien Chaumond;2019-11-05 19:56:43 -0500;[run_tf_glue] Add comment for context

==

examples/run_tf_glue.py
==================
30968d70a;Julien Chaumond;2019-11-05 19:06:12 -0500;misc doc

==

docs/source/serialization.rst
examples/contrib/run_openai_gpt.py
templates/adding_a_new_model/README.md
templates/adding_a_new_model/modeling_tf_xxx.py
templates/adding_a_new_model/modeling_xxx.py
transformers/modeling_utils.py
==================
de890ae67;Dom Hudson;2019-11-05 11:04:59 +0000;Updating docblocks in optimizers.py

==

transformers/optimization.py
==================
d7d36181f;Lysandre;2019-11-05 17:49:01 +0000;GPT-2 XL

==

docs/source/pretrained_models.rst
transformers/configuration_gpt2.py
transformers/modeling_gpt2.py
transformers/tokenization_gpt2.py
==================
151e4ab4e;LysandreJik;2019-11-05 16:26:51 +0000;Fix CTRL past

==

transformers/modeling_ctrl.py
==================
88e5bef58;thomwolf;2019-11-05 17:02:52 +0100;share position biases

==

transformers/modeling_t5.py
==================
568c0ffb7;thomwolf;2019-11-05 16:40:29 +0100;adding T5 model

==

transformers/modeling_encoder_decoder.py
transformers/modeling_t5.py
==================
7daacf00d;Julien Chaumond;2019-11-05 09:55:28 -0500;Merge pull request #1695 from huggingface/models_inputs_embeds
model forwards can take an inputs_embeds param
==
==================
a44f112fb;Clement;2019-11-05 08:48:26 -0500;add authors for models

==

.github/ISSUE_TEMPLATE/--new-model-addition.md
==================
60a5babd5;thomwolf;2019-11-05 12:01:23 +0100;adding files

==

transformers/configuration_t5.py
transformers/convert_t5_original_tf_checkpoint_to_pytorch.py
transformers/modeling_t5.py
transformers/modeling_tf_t5.py
transformers/tokenization_t5.py
==================
124409d07;Filip Povolny;2019-11-05 11:48:45 +0100;Make dummy inputs a property of TFPreTrainedModel.

==

transformers/modeling_tf_utils.py
==================
e99071f10;Thomas Wolf;2019-11-05 11:34:20 +0100;Merge pull request #1734 from orena1/patch-1
add progress bar to convert_examples_to_features
==
==================
dfb61caf7;thomwolf;2019-11-05 11:25:13 +0100;fix #1692

==

transformers/modeling_tf_xlnet.py
==================
ba973342e;Thomas Wolf;2019-11-05 11:13:12 +0100;Merge pull request #1553 from WilliamTambellini/timeSquadInference
Add speed log to examples/run_squad.py
==
==================
8df7dfd2a;Filip Povolny;2019-11-05 11:09:16 +0100;Make dummy inputs a local variable in TFPreTrainedModel.

==

transformers/modeling_tf_utils.py
==================
237fad339;Thomas Wolf;2019-11-05 10:55:33 +0100;Merge pull request #1709 from oneraghavan/master
Fixing mode in evaluate during training
==
==================
f1e4db2aa;thomwolf;2019-11-05 09:38:00 +0100;Fix #1686

==

transformers/modeling_openai.py
==================
d7906165a;Oren Amsalem;2019-11-05 10:34:27 +0200;add progress bar for convert_examples_to_features
It takes considerate amount of time (~10 min) to parse the examples to features, it is good to have a progress-bar to track this
==

examples/utils_squad.py
==================
d2e2577dd;Thomas Wolf;2019-11-05 08:36:30 +0100;Merge pull request #1723 from huggingface/fix-1623
Fix #1623
==
==================
00337e968;Julien Chaumond;2019-11-05 00:39:18 +0000;[inputs_embeds] All PyTorch models

==

templates/adding_a_new_model/modeling_tf_xxx.py
templates/adding_a_new_model/modeling_xxx.py
transformers/modeling_bert.py
transformers/modeling_ctrl.py
transformers/modeling_distilbert.py
transformers/modeling_gpt2.py
transformers/modeling_openai.py
transformers/modeling_roberta.py
transformers/modeling_tf_bert.py
transformers/modeling_tf_ctrl.py
transformers/modeling_tf_distilbert.py
transformers/modeling_tf_gpt2.py
transformers/modeling_tf_openai.py
transformers/modeling_tf_roberta.py
transformers/modeling_tf_transfo_xl.py
transformers/modeling_tf_xlm.py
transformers/modeling_tf_xlnet.py
transformers/modeling_transfo_xl.py
transformers/modeling_xlm.py
transformers/modeling_xlnet.py
transformers/tests/modeling_common_test.py
==================
9eddf44b7;Julien Chaumond;2019-11-04 17:19:15 +0000;docstring + check

==

transformers/modeling_gpt2.py
transformers/modeling_tf_utils.py
transformers/modeling_utils.py
==================
8e11de0e8;Julien Chaumond;2019-11-01 16:28:32 +0000;model forwards can take an inputs_embeds param

==

transformers/modeling_gpt2.py
==================
68f7064a3;Lysandre;2019-11-04 11:52:35 -0500;Add `model.train()` line to ReadMe training example
Co-Authored-By: Santosh-Gupta <San.Gupta.ML@gmail.com>

==

README.md
==================
8d6b9d717;thomwolf;2019-11-04 17:07:51 +0100;fix #1532 and encode_plus

==

transformers/tests/tokenization_tests_commons.py
transformers/tokenization_utils.py
==================
c8f271219;Thomas Wolf;2019-11-04 16:21:52 +0100;Merge pull request #1721 from huggingface/common_attributes
Add common getter and setter for input_embeddings & output_embeddings
==
==================
89d627289;thomwolf;2019-11-04 16:21:12 +0100;Fix #1623

==

examples/distillation/run_squad_w_distillation.py
examples/run_bertology.py
examples/run_glue.py
examples/run_lm_finetuning.py
examples/run_multiple_choice.py
examples/run_ner.py
examples/run_squad.py
templates/adding_a_new_example_script/run_xxx.py
==================
b340a910e;thomwolf;2019-11-04 16:03:36 +0100;fix tests - flagged as slow all the tests downloading from AWS

==

transformers/modeling_roberta.py
transformers/tests/modeling_auto_test.py
transformers/tests/modeling_common_test.py
transformers/tests/modeling_encoder_decoder_test.py
transformers/tests/tokenization_auto_test.py
transformers/tests/tokenization_bert_test.py
transformers/tests/tokenization_distilbert_test.py
transformers/tests/tokenization_roberta_test.py
transformers/tests/tokenization_utils_test.py
transformers/tests/tokenization_xlm_test.py
transformers/tests/tokenization_xlnet_test.py
==================
f02805da6;thomwolf;2019-11-04 15:42:23 +0100;fix tests

==

transformers/tests/modeling_common_test.py
==================
1d4d07025;Thomas Wolf;2019-11-04 15:37:15 +0100;Merge pull request #1549 from hlums/master
Fix token order in xlnet preprocessing for SQuAD
==
==================
1724cee8c;thomwolf;2019-11-04 15:34:10 +0100;switch from properties to methods

==

templates/adding_a_new_model/modeling_xxx.py
transformers/modeling_bert.py
transformers/modeling_ctrl.py
transformers/modeling_distilbert.py
transformers/modeling_gpt2.py
transformers/modeling_openai.py
transformers/modeling_roberta.py
transformers/modeling_transfo_xl.py
transformers/modeling_utils.py
transformers/modeling_xlm.py
transformers/modeling_xlnet.py
transformers/tests/modeling_common_test.py
==================
9b45d0f87;thomwolf;2019-11-04 12:28:56 +0100;Add common properties input_embeddings and output_embeddings

==

templates/adding_a_new_model/modeling_xxx.py
transformers/modeling_bert.py
transformers/modeling_ctrl.py
transformers/modeling_distilbert.py
transformers/modeling_gpt2.py
transformers/modeling_openai.py
transformers/modeling_roberta.py
transformers/modeling_transfo_xl.py
transformers/modeling_utils.py
transformers/modeling_xlm.py
transformers/modeling_xlnet.py
transformers/tests/modeling_common_test.py
==================
9a3b173cd;Thomas Wolf;2019-11-04 11:41:26 +0100;Merge branch 'master' into master

==
==================
ad9086862;thomwolf;2019-11-04 11:27:22 +0100;Update example readme

==

examples/README.md
==================
e5b1048ba;Raghavan;2019-11-03 16:14:46 +0530;Fixing mode in evaluate during training

==

examples/run_ner.py
==================
8a6283557;Thomas Wolf;2019-11-01 22:02:24 +0100;Merge pull request #1679 from cregouby/master
Fix https://github.com/huggingface/transformers/issues/1673
==
==================
93d2fff07;Julien Chaumond;2019-11-01 09:47:38 -0400;Close #1654

==

docs/source/model_doc/ctrl.rst
==================
1a2b40cb5;Lysandre;2019-10-31 18:00:51 -0400;run_tf_glue MRPC evaluation only for MRPC

==

examples/run_tf_glue.py
==================
be36cf92f;Timothy Liu;2019-10-30 00:44:23 +0000;Added mixed precision support to benchmarks.py

==

examples/benchmarks.py
==================
2a5663c28;Julien Chaumond;2019-10-31 18:28:34 +0000;Merge branch 'mataney-fix_top_k_top_p_filtering'

==
==================
f96ce1c24;Julien Chaumond;2019-10-31 18:27:11 +0000;[run_generation] Fix generation with batch_size>1

==

examples/run_generation.py
==================
3c1b6f594;Julien Chaumond;2019-10-31 13:53:51 -0400;Merge branch 'master' into fix_top_k_top_p_filtering

==
==================
0e4cc050d;Sergey Mironov;2019-10-24 18:15:55 +0300;Add support for resumable downloads for HTTP protocol.

==

transformers/configuration_auto.py
transformers/configuration_utils.py
transformers/file_utils.py
transformers/modeling_auto.py
transformers/modeling_tf_auto.py
transformers/modeling_tf_utils.py
transformers/modeling_utils.py
transformers/tokenization_auto.py
transformers/tokenization_utils.py
==================
ac29353ab;cregouby;2019-10-31 10:04:40 +0100;Fix https://github.com/huggingface/transformers/issues/1673

==

transformers/modeling_utils.py
==================
fa735208c;Victor SANH;2019-10-30 14:27:28 -0400;update readme - fix example command distil*

==

examples/distillation/README.md
==================
c7058d822;Thomas Wolf;2019-10-30 17:14:07 +0100;Merge pull request #1608 from focox/master
Error raised by "tmp_eval_loss += tmp_eval_loss.item()" when using multi-gpu
==
==================
22838f19f;Thomas Wolf;2019-10-30 17:08:00 +0100;Merge pull request #1668 from tlkh/fix-tf-xlm
Fixed training for TF XLM
==
==================
7f84fc571;Thomas Wolf;2019-10-30 17:05:58 +0100;Merge pull request #1670 from huggingface/templates
Templates and explanation for adding a new model and example script
==
==================
04c69db39;Thomas Wolf;2019-10-30 17:04:03 +0100;Merge pull request #1628 from huggingface/tfglue
run_tf_glue works with all tasks
==
==================
5c6a19a94;Thomas Wolf;2019-10-30 17:03:14 +0100;Merge pull request #1604 from huggingface/deploy_doc
Versioning in documentation
==
==================
3df436724;Thomas Wolf;2019-10-30 17:00:40 +0100;Merge pull request #1601 from huggingface/clean-roberta
Clean roberta model & all tokenizers now add special tokens by default (breaking change)
==
==================
6d73c92ca;Thomas Wolf;2019-10-30 16:54:18 +0100;Merge pull request #1455 from huggingface/conditional-generation
[WIP] Sequence generation using pretrained BERT
==
==================
36174696c;Thomas Wolf;2019-10-30 16:51:06 +0100;Merge branch 'master' into clean-roberta

==
==================
228cdd6a6;Thomas Wolf;2019-10-30 16:40:35 +0100;Merge branch 'master' into conditional-generation

==
==================
3cf2020c6;R√©mi Louf;2019-10-30 16:27:51 +0100;change kwargs processing

==

transformers/modeling_encoder_decoder.py
==================
a88a0e441;R√©mi Louf;2019-10-30 16:06:29 +0100;add tests to encoder-decoder model

==

transformers/tests/modeling_common_test.py
transformers/tests/modeling_encoder_decoder_test.py
==================
3f07cd419;R√©mi Louf;2019-10-30 15:09:53 +0100;update test on Bert to include decoder mode

==

transformers/tests/modeling_bert_test.py
==================
55fbfea36;Thomas Wolf;2019-10-30 12:25:40 +0100;Update CONTRIBUTING.md
Co-Authored-By: Stefan Schweter <stefan.schweter@bsb-muenchen.de>
==

CONTRIBUTING.md
==================
cef2a8f90;Thomas Wolf;2019-10-30 12:25:31 +0100;Update CONTRIBUTING.md
Co-Authored-By: Stefan Schweter <stefan.schweter@bsb-muenchen.de>
==

CONTRIBUTING.md
==================
328a86d2a;thomwolf;2019-10-30 11:37:55 +0100;adding links to the templates in readme and contributing

==

CONTRIBUTING.md
README.md
==================
7f4226f9e;thomwolf;2019-10-30 11:31:56 +0100;adding templates

==

templates/adding_a_new_example_script/README.md
templates/adding_a_new_example_script/run_xxx.py
templates/adding_a_new_example_script/utils_xxx.py
templates/adding_a_new_model/README.md
templates/adding_a_new_model/configuration_xxx.py
templates/adding_a_new_model/convert_xxx_original_tf_checkpoint_to_pytorch.py
templates/adding_a_new_model/modeling_tf_xxx.py
templates/adding_a_new_model/modeling_xxx.py
templates/adding_a_new_model/tests/modeling_tf_xxx_test.py
templates/adding_a_new_model/tests/modeling_xxx_test.py
templates/adding_a_new_model/tests/tokenization_xxx_test.py
templates/adding_a_new_model/tokenization_xxx.py
==================
070507df1;R√©mi Louf;2019-10-30 11:24:12 +0100;format utils for summarization

==

examples/utils_summarization.py
examples/utils_summarization_test.py
==================
da10de846;R√©mi Louf;2019-10-30 11:19:58 +0100;fix bug with padding mask + add corresponding test

==

examples/utils_summarization.py
examples/utils_summarization_test.py
==================
3b0d2fa30;R√©mi Louf;2019-10-30 10:54:46 +0100;rename seq2seq to encoder_decoder

==

examples/README.md
examples/run_summarization_finetuning.py
transformers/__init__.py
transformers/modeling_encoder_decoder.py
==================
9c1bdb5b6;R√©mi Louf;2019-10-30 10:43:13 +0100;revert renaming of lm_labels to ltr_lm_labels

==

examples/run_summarization_finetuning.py
transformers/modeling_bert.py
transformers/modeling_seq2seq.py
==================
842f3bf04;Timothy Liu;2019-10-30 01:32:15 +0000;Fixed training for TF XLM

==

transformers/modeling_tf_xlm.py
==================
098a89f31;R√©mi Louf;2019-10-29 20:08:03 +0100;update docstrings; rename lm_labels to more explicit ltr_lm_labels

==

examples/run_summarization_finetuning.py
transformers/modeling_bert.py
==================
dfce40969;R√©mi Louf;2019-10-29 17:10:20 +0100;resolve PR comments

==

examples/run_summarization_finetuning.py
examples/run_summarization_finetuning_test.py
examples/utils_summarization.py
examples/utils_summarization_test.py
transformers/modeling_beam_search.py
transformers/modeling_bert.py
transformers/modeling_seq2seq.py
==================
079bfb32f;altsoph;2019-10-25 13:26:37 +0300;Evaluation fixed.

==

examples/run_lm_finetuning.py
==================
438f2730a;altsoph;2019-10-25 13:22:58 +0300;Evaluation code fixed.

==

examples/run_lm_finetuning.py
==================
4c3ac4a7d;R√©mi Louf;2019-10-18 12:29:30 +0200;here's one big commit

==

examples/README.md
examples/run_seq2seq_finetuning.py
examples/run_summarization_finetuning.py
examples/run_summarization_finetuning_test.py
transformers/__init__.py
transformers/modeling_beam_search.py
transformers/modeling_bert.py
transformers/modeling_seq2seq.py
==================
932543f77;R√©mi Louf;2019-10-17 21:07:55 +0200;fix test of truncation function

==

examples/run_seq2seq_finetuning_test.py
==================
a67413ccc;R√©mi Louf;2019-10-17 18:08:09 +0200;extend works in-place

==

examples/run_seq2seq_finetuning.py
==================
cb26b035c;R√©mi Louf;2019-10-17 17:52:32 +0200;remove potential UndefinedError

==

transformers/modeling_xlm.py
==================
b915ba9df;R√©mi Louf;2019-10-17 17:44:20 +0200;pad sequence with 0, mask with -1

==

examples/run_seq2seq_finetuning.py
==================
dc580dd4c;R√©mi Louf;2019-10-17 16:56:36 +0200;add lm_labels for the LM cross-entropy

==

transformers/modeling_bert.py
==================
f873a3edb;R√©mi Louf;2019-10-17 15:21:46 +0200;the decoder attends to the output of the encoder stack (last layer)

==

transformers/modeling_bert.py
transformers/modeling_seq2seq.py
==================
d36680df5;Lorenzo Ampil;2019-10-27 14:51:36 +0800;Rever changes to TF distilbert due to failed test: TFDistilBertModelTest.test_pt_tf_model_equivalence

==

transformers/modeling_tf_distilbert.py
==================
ec276d6ab;Lorenzo Ampil;2019-10-27 14:00:40 +0800;Add special tokens to documentation for the tensorflow model examples #1561

==

transformers/modeling_tf_bert.py
transformers/modeling_tf_ctrl.py
transformers/modeling_tf_distilbert.py
transformers/modeling_tf_gpt2.py
transformers/modeling_tf_openai.py
transformers/modeling_tf_roberta.py
transformers/modeling_tf_transfo_xl.py
transformers/modeling_tf_xlm.py
transformers/modeling_tf_xlnet.py
==================
6e011690a;Lorenzo Ampil;2019-10-27 13:59:14 +0800;Add special tokens to documentation for the rest of pytorch model examples #1561

==

transformers/modeling_ctrl.py
transformers/modeling_distilbert.py
transformers/modeling_gpt2.py
transformers/modeling_openai.py
transformers/modeling_roberta.py
transformers/modeling_transfo_xl.py
transformers/modeling_xlm.py
transformers/modeling_xlnet.py
==================
beaf66b1f;Lysandre;2019-10-24 21:43:28 +0000;Remove break

==

transformers/data/processors/glue.py
==================
bab6ad01a;Lysandre;2019-10-24 21:41:45 +0000;run_tf_glue works with all tasks

==

examples/run_tf_glue.py
transformers/data/processors/glue.py
transformers/data/processors/utils.py
==================
ae1d03fc5;Matt Maybeno;2019-10-24 10:43:57 -0700;Add roberta to doc

==

examples/run_ner.py
==================
4e5f88b74;Matt Maybeno;2019-10-23 22:50:03 -0700;Add Roberta to run_ner.py

==

examples/run_ner.py
==================
b92d68421;Matt Maybeno;2019-10-23 21:31:28 -0700;Use roberta model and update doc strings

==

transformers/modeling_roberta.py
transformers/modeling_tf_roberta.py
==================
66085a132;Matt Maybeno;2019-10-23 21:05:13 -0700;RoBERTa token classification
[WIP] copy paste bert token classification for roberta

==

transformers/__init__.py
transformers/modeling_roberta.py
transformers/modeling_tf_roberta.py
transformers/tests/modeling_roberta_test.py
transformers/tests/modeling_tf_roberta_test.py
==================
b82bfbd0c;Lysandre;2019-10-24 15:55:31 +0000;Updated README to show all available documentation

==

README.md
==================
5b6cafb11;VictorSanh;2019-10-23 10:35:16 -0400;[release] fix table weirdness

==

examples/distillation/README.md
==================
8ad5c591c;VictorSanh;2019-10-23 10:29:47 -0400;[RELEASE] DistilRoBERTa

==

docs/source/pretrained_models.rst
examples/distillation/README.md
==================
bd847ce7d;focox@qq.com;2019-10-23 20:27:13 +0800;fixed the bug raised by "tmp_eval_loss += tmp_eval_loss.item()" when parallelly using multi-gpu.

==

examples/run_ner.py
==================
6e85bccaf;Lysandre Debut;2019-10-22 18:07:01 -0400;Fixed typo

==

.circleci/deploy.sh
==================
fbcc5ff9f;Lysandre;2019-10-22 18:01:10 -0400;Change branch to master

==

.circleci/config.yml
==================
69eba0ab1;Lysandre;2019-10-22 17:53:52 -0400;Edit script path

==

.circleci/config.yml
==================
bc3e57d55;Lysandre;2019-10-22 17:51:30 -0400;Multi version doc deployment

==

.circleci/config.yml
.circleci/deploy.sh
==================
ef1b8b2ae;Julien Chaumond;2019-10-22 21:27:20 +0000;[CTRL] warn if generation prompt does not start with a control code
see also https://github.com/salesforce/ctrl/pull/50

==

README.md
examples/README.md
examples/run_generation.py
transformers/tokenization_ctrl.py
==================
e16d46843;Juli√°n Peller (dataista);2019-10-22 16:11:02 -0300;Fix architectures count

==

README.md
==================
7d709e55e;Lysandre;2019-10-22 14:12:33 -0400;Remove

==

examples/benchmarks.py
examples/distillation/scripts/binarized_data.py
examples/run_generation.py
transformers/tests/tokenization_bert_test.py
transformers/tests/tokenization_distilbert_test.py
transformers/tests/tokenization_roberta_test.py
transformers/tests/tokenization_tests_commons.py
transformers/tests/tokenization_xlm_test.py
transformers/tests/tokenization_xlnet_test.py
transformers/tokenization_utils.py
==================
44286b94d;Lysandre;2019-10-22 13:46:48 -0400;RoBERTa doesn't print a warning when no special tokens are passed.

==

transformers/modeling_roberta.py
transformers/modeling_tf_roberta.py
==================
1cfd97486;Lysandre;2019-10-22 13:32:23 -0400;Option to benchmark only one of the two libraries

==

examples/benchmarks.py
==================
777faa8ae;Lysandre;2019-10-22 11:26:42 -0400;Fix #1597

==

transformers/tokenization_ctrl.py
==================
b8c9ea001;Thomas Wolf;2019-10-22 13:59:20 +0200;Merge pull request #1580 from pminervini/master
Gradient norm clipping should be done right before calling the optimiser
==
==================
abd7110e2;Pasquale Minervini;2019-10-21 19:56:52 +0100;gradient norm clipping should be done right before calling the optimiser - fixing run_glue and run_ner as well

==

examples/run_glue.py
examples/run_ner.py
==================
4d456542e;thomwolf;2019-10-21 16:34:14 +0200;Fix citation

==

README.md
==================
0e64fec1a;Thomas Wolf;2019-10-21 14:31:57 +0200;Merge pull request #1568 from daemon/patch-1
Fix hanging when loading pretrained models
==
==================
3a52b6579;Lorenzo Ampil;2019-10-21 12:55:51 +0800;Add special tokens to documentation for bert examples to resolve issue: #1561

==

transformers/modeling_bert.py
==================
86a630702;erenup;2019-10-21 12:06:09 +0800;Merge branch 'huggingface/master'

==
==================
3775550c4;Pasquale Minervini;2019-10-20 22:33:56 +0100;gradient norm clipping should be done right before calling the optimiser

==

examples/run_squad.py
==================
bf2c36a92;Pasquale Minervini;2019-10-20 23:30:45 +0200;Merge pull request #1 from huggingface/master
update
==
==================
a2c8c8ef0;Ralph Tang;2019-10-19 16:19:20 -0400;Fix hanging when loading pretrained models
- Fix hanging when loading pretrained models from the cache without having internet access. This is a widespread issue on supercomputers whose internal compute nodes are firewalled.
==

transformers/file_utils.py
==================
82f6abd98;LysandreJik;2019-10-18 17:27:10 -0400;Benchmark section added to the documentation

==

docs/source/benchmarks.md
docs/source/index.rst
==================
7dd29ed2f;LysandreJik;2019-10-18 10:53:04 -0400;Benchmarks example script

==

examples/benchmarks.py
==================
8efc0ec91;Lysandre Debut;2019-10-18 10:45:44 -0400;Add Benchmarks to issue templates

==

.github/ISSUE_TEMPLATE/---new-benchmark.md
==================
0919389d9;William Tambellini;2019-10-17 14:41:04 -0700;Add speed log to examples/run_squad.py
Add a speed estimate log (time per example)
for evaluation to examples/run_squad.py

==

examples/run_squad.py
==================
fd97761c5;VictorSanh;2019-10-17 15:28:58 -0400;soft launch distilroberta

==

transformers/configuration_roberta.py
transformers/modeling_roberta.py
transformers/modeling_tf_roberta.py
transformers/tokenization_roberta.py
==================
ecd15667f;leo-du;2019-10-17 11:04:34 -0700;fix repetition penalty

==

examples/run_generation.py
==================
56e2ee4ea;thomwolf;2019-10-17 16:33:31 +0200;fix model2model

==

transformers/modeling_seq2seq.py
==================
8cd56e303;thomwolf;2019-10-17 16:33:26 +0200;fix data processing in script

==

examples/run_seq2seq_finetuning.py
==================
578d23e06;R√©mi Louf;2019-10-17 14:02:27 +0200;add training pipeline (formatting temporary)

==

examples/run_seq2seq_finetuning.py
==================
47a06d88a;R√©mi Louf;2019-10-17 13:04:26 +0200;use two different tokenizers for storyand summary

==

examples/run_seq2seq_finetuning.py
==================
bfb9b540d;R√©mi Louf;2019-10-17 12:59:51 +0200;add Model2Model to __init__

==

examples/run_seq2seq_finetuning.py
transformers/__init__.py
==================
c1bc709c3;R√©mi Louf;2019-10-17 10:41:53 +0200;correct the truncation and padding of dataset

==

examples/run_seq2seq_finetuning.py
==================
87d60b6e1;R√©mi Louf;2019-10-17 10:18:19 +0200;reword explanation of encoder_attention_mask

==

transformers/modeling_bert.py
==================
638fe7f5a;R√©mi Louf;2019-10-17 10:13:07 +0200;correct composition of padding and causal masks

==

transformers/modeling_bert.py
==================
4e0f24348;R√©mi Louf;2019-10-17 09:41:53 +0200;document the MLM modification + raise exception on MLM training with encoder-decoder

==

transformers/modeling_bert.py
==================
624a5644c;R√©mi Louf;2019-10-17 09:27:56 +0200;revert black formatting to conform with lib style

==

transformers/modeling_seq2seq.py
==================
9b71fc9a1;R√©mi Louf;2019-10-16 21:31:38 +0200;tying weights is going to be a clusterfuck

==

transformers/modeling_seq2seq.py
==================
95ec1d08b;R√©mi Louf;2019-10-16 20:55:42 +0200;separate inputs into encoder & decoder inputs

==

transformers/modeling_seq2seq.py
==================
e4e0ee14b;R√©mi Louf;2019-10-16 20:05:32 +0200;add separator between data import and train

==

examples/run_seq2seq_finetuning.py
==================
a424892fa;R√©mi Louf;2019-10-16 18:24:32 +0200;correct syntax error: dim() and not dims()

==

transformers/modeling_bert.py
==================
33c01368b;R√©mi Louf;2019-10-16 18:13:05 +0200;remove Bert2Rnd test

==

transformers/tests/modeling_bert_test.py
==================
c54419461;Lysandre Debut;2019-10-16 11:05:13 -0400;Remove `special_tokens_mask` from inputs in README
Co-authored-by: Thomas Wolf @thomwolf
==

README.md
==================
075206961;R√©mi Louf;2019-10-16 16:12:22 +0200;adapt attention masks for the decoder case
The introduction of a decoder introduces 2 changes:
- We need to be able to specify a separate mask in the cross
attention to mask the positions corresponding to padding tokens in the
encoder state.
- The self-attention in the decoder needs to be causal on top of not
attending to padding tokens.

==

transformers/modeling_bert.py
==================
c5a94a610;R√©mi Louf;2019-10-16 12:50:36 +0200;fix function that defines masks in XLM
the definition of `get_masks` would blow with the proper combination of
arguments. It was just a matter of moving a definition outside of a
control structure.

==

transformers/modeling_xlm.py
==================
488a66415;R√©mi Louf;2019-10-15 21:03:32 +0200;add `is_decoder` attribute to `PretrainedConfig`
We currenctly instantiate encoders and decoders for the seq2seq by
passing the `is_decoder` keyword argument to the `from_pretrained`
classmethod. On the other hand, the model class looks for the value
of the `is_decoder` attribute in its config.

In order for the value to propagate from the kwarg to the configuration
we simply need to define `is_decoder` as an attribute to the base
`PretrainedConfig`, with a default at `False`.

==

transformers/configuration_utils.py
==================
4c81960b9;R√©mi Louf;2019-10-15 17:53:38 +0200;comment the seq2seq functions

==

transformers/modeling_seq2seq.py
==================
6d6c32673;R√©mi Louf;2019-10-15 16:07:07 +0200;take path to pretrained for encoder and decoder for init

==

transformers/modeling_seq2seq.py
==================
0d81fc853;R√©mi Louf;2019-10-15 15:26:33 +0200;specify in readme that both datasets are required

==

examples/README.md
==================
19e996478;R√©mi Louf;2019-10-15 15:20:28 +0200;remove Bert2Bert from module declaration

==

transformers/__init__.py
==================
1aec94058;R√©mi Louf;2019-10-15 15:18:07 +0200;test the full story processing

==

examples/run_seq2seq_finetuning.py
examples/run_seq2seq_finetuning_test.py
==================
22e1af685;R√©mi Louf;2019-10-15 14:39:56 +0200;truncation function is fully tested

==

examples/run_seq2seq_finetuning.py
examples/run_seq2seq_finetuning_test.py
==================
260ac7d9a;R√©mi Louf;2019-10-15 12:24:35 +0200;wip commit, switching computers

==

examples/run_seq2seq_finetuning.py
examples/run_seq2seq_finetuning_test.py
==================
be916cb3f;thomwolf;2019-10-15 10:37:13 +0200;Merge branch 'master' of https://github.com/huggingface/transformers

==
==================
5875aaf76;thomwolf;2019-10-15 10:36:46 +0200;install tensorboard

==

examples/requirements.txt
==================
40f14ff54;Thomas Wolf;2019-10-15 10:25:00 +0200;Merge pull request #1513 from slayton58/amp_fp16_einsum
Force einsum to run in fp16
==
==================
e703e4dfe;Thomas Wolf;2019-10-15 10:24:13 +0200;Merge pull request #1509 from julian-pani/patch-3
remove leftover usage of DUMMY_INPUTS
==
==================
898ce064f;thomwolf;2019-10-15 10:04:19 +0200;add tests on TF2.0 & PT checkpoint => model convertion functions

==

transformers/tests/modeling_tf_common_test.py
==================
d147671c6;Thomas Wolf;2019-10-15 09:57:18 +0200;Merge pull request #1508 from tlkh/master
Added performance enhancements (XLA, AMP) to examples
==
==================
2c1d5564a;thomwolf;2019-10-15 09:56:52 +0200;add readme information

==

examples/README.md
==================
08bd8f9f3;Thomas Wolf;2019-10-15 09:50:36 +0200;Merge pull request #1505 from e-budur/master
Fixed the sample code in the title 'Quick tour'.
==
==================
8aa3b753b;Thomas Wolf;2019-10-15 09:44:19 +0200;Merge pull request #1434 from bryant1410/patch-1
Remove unnecessary use of FusedLayerNorm in XLNet
==
==================
621e7a252;Thomas Wolf;2019-10-15 09:35:24 +0200;Merge pull request #1275 from stecklin/ner-fine-tuning
Implement fine-tuning BERT on CoNLL-2003 named entity recognition task
==
==================
c55badcee;thomwolf;2019-10-15 09:33:52 +0200;Add NER finetuning details by @stefan-it in example readme

==

examples/README.md
==================
788e63262;Julien Chaumond;2019-10-11 18:04:29 -0400;[ner] Honor args.overwrite_cache

==

examples/run_ner.py
==================
0f9ebb0b4;thomwolf;2019-10-03 16:54:52 -0400;add seqeval as requirement for examples

==

examples/requirements.txt
==================
66adb7173;thomwolf;2019-10-03 16:54:40 -0400;update to transformers

==

examples/run_ner.py
==================
5ff9cd158;Marianne Stecklina;2019-09-23 10:51:54 +0200;Add option to predict on test set

==

examples/run_ner.py
examples/utils_ner.py
==================
7f5367e0b;Marianne Stecklina;2019-09-19 11:29:20 +0200;Add cli argument for configuring labels

==

examples/run_ner.py
examples/utils_ner.py
==================
e1d4179b6;Marianne Stecklina;2019-09-19 09:28:00 +0200;Make file reading more robust

==

examples/utils_ner.py
==================
383ef9674;Marianne Stecklina;2019-09-17 15:18:57 +0200;Implement fine-tuning BERT on CoNLL-2003 named entity recognition task

==

examples/run_ner.py
examples/utils_ner.py
==================
5adb39e75;Marianne Stecklina;2019-09-23 10:51:54 +0200;Add option to predict on test set

==

examples/run_ner.py
examples/utils_ner.py
==================
99b189df6;Marianne Stecklina;2019-09-19 11:29:20 +0200;Add cli argument for configuring labels

==

examples/run_ner.py
examples/utils_ner.py
==================
3e9420add;Marianne Stecklina;2019-09-19 09:28:00 +0200;Make file reading more robust

==

examples/utils_ner.py
==================
cde42c435;Marianne Stecklina;2019-09-17 15:18:57 +0200;Implement fine-tuning BERT on CoNLL-2003 named entity recognition task

==

examples/run_ner.py
examples/utils_ner.py
==================
74c503580;hlums;2019-10-14 21:27:11 +0000;Fix token order in xlnet preprocessing.

==

examples/run_squad.py
examples/utils_squad.py
==================
fe25eefc1;R√©mi Louf;2019-10-14 20:45:39 +0200;add instructions to fetch the dataset

==

examples/README.md
==================
412793275;R√©mi Louf;2019-10-14 20:45:16 +0200;delegate the padding with special tokens to the tokenizer

==

examples/run_seq2seq_finetuning.py
==================
447fffb21;R√©mi Louf;2019-10-14 18:12:20 +0200;process the raw CNN/Daily Mail dataset
the data provided by Li Dong et al. were already tokenized, which means
that they are not compatible with  all the models in the library. We
thus process the raw data directly and tokenize them using the models'
tokenizers.

==

examples/run_seq2seq_finetuning.py
==================
80889a022;Thomas Wolf;2019-10-14 17:40:32 +0200;Merge pull request #1512 from louismartin/fix-roberta-convert
Fix import error in script to convert faisreq roberta checkpoints
==
==================
4e6a55751;Simon Layton;2019-09-13 15:21:40 -0400;Force einsum to fp16

==

examples/run_squad.py
==================
f62f992cf;Thomas Wolf;2019-10-14 16:14:52 +0200;Merge pull request #1502 from jeffxtang/master
the working example code to use BertForQuestionAnswering 
==
==================
67d10960a;R√©mi Louf;2019-10-14 14:09:21 +0200;load and prepare CNN/Daily Mail data
We write a function to load an preprocess the CNN/Daily Mail dataset as
provided by Li Dong et al. The issue is that this dataset has already
been tokenized by the authors, so we actually need to find the original,
plain-text dataset if we want to apply it to all models.

==

examples/run_seq2seq_finetuning.py
==================
d9d387afc;thomwolf;2019-10-14 12:14:40 +0200;clean up

==

transformers/modeling_seq2seq.py
==================
b7141a1bc;thomwolf;2019-10-14 12:14:08 +0200;maxi simplication

==

transformers/modeling_seq2seq.py
==================
bfbe68f03;thomwolf;2019-10-14 12:04:23 +0200;update forward pass

==

transformers/modeling_seq2seq.py
==================
0ef9bc923;thomwolf;2019-10-14 11:58:13 +0200;Cleaning up seq2seq [WIP]

==

transformers/modeling_bert.py
transformers/modeling_seq2seq.py
==================
49cba6e54;Louis MARTIN;2019-10-14 01:38:57 -0700;Fix import error in script to convert faisreq roberta checkpoints

==

transformers/convert_roberta_original_pytorch_checkpoint_to_pytorch.py
==================
099358675;JulianPani;2019-10-14 02:09:53 +0300;remove usage of DUMMY_INPUTS
Hey @thomwolf  
This change https://github.com/huggingface/transformers/commit/da26bae61b8c1e741fdc6735d46c61b43f649561#diff-8ddce309e88e8eb5b4d02228fd8881daL28-L29 removed the constant, but one usage of that constant remains in the code.
==

transformers/modeling_tf_pytorch_utils.py
==================
376e65a67;Timothy Liu;2019-10-13 11:04:49 +0000;Added automatic mixed precision and XLA options to run_tf_glue.py

==

examples/run_tf_glue.py
==================
86f23a194;Timothy Liu;2019-10-13 10:21:35 +0000;Minor enhancements to run_tf_glue.py

==

examples/run_tf_glue.py
==================
5a8c6e771;Emrah Budur;2019-10-12 14:17:17 +0300;Fixed the sample code in the title 'Quick tour'.

==

README.md
==================
e76d71521;jeffxtang;2019-10-11 17:04:02 -0700;the working example code to use BertForQuestionAnswering and get an answer from a text and a question

==

transformers/modeling_bert.py
==================
d844db400;VictorSanh;2019-10-11 16:55:42 -0400;Add citation bibtex

==

examples/distillation/README.md
==================
a701c9b32;Lysandre;2019-10-11 16:05:30 -0400;CTRL to tf automodels

==

transformers/modeling_tf_auto.py
==================
b3261e7ac;R√©mi Louf;2019-10-11 18:40:38 +0200;read parameters from CLI, load model & tokenizer

==

examples/run_seq2seq_finetuning.py
examples/run_summarization.py
==================
d889e0b71;R√©mi Louf;2019-10-11 17:36:12 +0200;add base for seq2seq finetuning

==

examples/run_seq2seq_finetuning.py
==================
f8e98d677;R√©mi Louf;2019-10-11 16:48:11 +0200;load pretrained embeddings in Bert decoder
In Rothe et al.'s "Leveraging Pre-trained Checkpoints for Sequence
Generation Tasks", Bert2Bert is initialized with pre-trained weights for
the encoder, and only pre-trained embeddings for the decoder. The
current version of the code completely randomizes the weights of the
decoder.

We write a custom function to initiliaze the weights of the decoder; we
first initialize the decoder with the weights and then randomize
everything but the embeddings.

==

transformers/modeling_bert.py
==================
3ddce1d74;Lysandre;2019-10-11 06:37:49 -0400;Release: 2.1.1

==

docs/source/conf.py
setup.py
transformers/__init__.py
==================
4428aefc6;Thomas Wolf;2019-10-11 16:33:00 +0200;Merge pull request #1488 from huggingface/pytorch-tpu
GLUE on TPU
==
==================
3b43b0187;Thomas Wolf;2019-10-11 16:25:43 +0200;Merge pull request #1482 from huggingface/tf2_integration_tests
Integration of TF 2.0 models with other Keras modules
==
==================
4b8f3e8f3;thomwolf;2019-10-11 16:18:16 +0200;adding citation

==

README.md
==================
18a3cef7d;thomwolf;2019-10-11 16:09:42 +0200;no nans

==

transformers/tests/modeling_common_test.py
transformers/tests/modeling_tf_common_test.py
==================
1f5d9513d;thomwolf;2019-10-11 15:55:01 +0200;fix test

==

transformers/tests/modeling_tf_xlnet_test.py
transformers/tests/modeling_xlnet_test.py
==================
0f9fc4fbd;thomwolf;2019-10-11 15:47:08 +0200;adding option to desactivate past/memory outputs

==

transformers/configuration_utils.py
transformers/modeling_ctrl.py
transformers/modeling_gpt2.py
transformers/modeling_tf_ctrl.py
transformers/modeling_tf_xlnet.py
transformers/modeling_xlnet.py
transformers/tests/modeling_tf_xlnet_test.py
transformers/tests/modeling_xlnet_test.py
==================
700331b5e;Thomas Wolf;2019-10-11 13:01:52 +0200;Merge pull request #1492 from stefan-it/bert-german-dbmdz-models
Add new BERT models for German (cased and uncased)
==
==================
573dde9b4;Thomas Wolf;2019-10-11 12:10:58 +0200;Merge pull request #1405 from slayton58/xlnet_layer_reorder
Re-order XLNet attention head outputs for better perf
==
==================
5f25a5f36;Stefan Schweter;2019-10-11 10:20:33 +0200;model: add support for new German BERT models (cased and uncased) from @dbmdz

==

docs/source/pretrained_models.rst
docs/source/serialization.rst
transformers/configuration_bert.py
transformers/modeling_bert.py
transformers/tokenization_bert.py
==================
f382a8dec;Luran He;2019-10-10 16:27:53 -0400;convert int to str before adding to a str

==

examples/run_lm_finetuning.py
==================
639f4b719;Lysandre;2019-10-10 19:17:25 +0000;Don't save/load when on TPU

==

examples/run_glue.py
==================
d4e7934ac;Lysandre;2019-10-10 19:03:06 +0000;GLUE on TPU

==

examples/run_glue.py
==================
1e68c2867;R√©mi Louf;2019-10-10 18:07:11 +0200;add test for initialization of Bert2Rnd

==

examples/run_summarization.py
transformers/tests/modeling_bert_test.py
==================
2a4fef837;thomwolf;2019-10-10 15:57:35 +0200;move Circle-CI from TF2-rc0 to official TF2

==

.circleci/config.yml
==================
751e24608;thomwolf;2019-10-10 15:47:20 +0200;using tf.print in  roberta

==

transformers/modeling_tf_roberta.py
==================
fa218e648;R√©mi Louf;2019-10-10 15:16:07 +0200;fix syntax errors

==

transformers/modeling_bert.py
==================
c9e8c5194;thomwolf;2019-10-10 15:16:05 +0200;fixing SequenceSummary head in TF 2.0

==

transformers/modeling_tf_utils.py
==================
da26bae61;thomwolf;2019-10-10 14:30:48 +0200;adding more tests on TF and pytorch serialization - updating configuration for better serialization

==

transformers/__init__.py
transformers/configuration_utils.py
transformers/convert_pytorch_checkpoint_to_tf2.py
transformers/modeling_tf_bert.py
transformers/modeling_tf_ctrl.py
transformers/modeling_tf_distilbert.py
transformers/modeling_tf_gpt2.py
transformers/modeling_tf_openai.py
transformers/modeling_tf_pytorch_utils.py
transformers/modeling_tf_roberta.py
transformers/modeling_tf_transfo_xl.py
transformers/modeling_tf_utils.py
transformers/modeling_tf_xlm.py
transformers/modeling_tf_xlnet.py
transformers/tests/modeling_common_test.py
==================
3e1cd8241;R√©mi Louf;2019-10-10 14:18:20 +0200;fix stupid (re)naming issue

==

transformers/modeling_bert.py
==================
81ee29ee8;R√©mi Louf;2019-10-10 14:13:37 +0200;remove the staticmethod used to load the config

==

transformers/modeling_bert.py
==================
bb04edb45;thomwolf;2019-10-10 13:08:24 +0200;Add tests that TF 2.0 model can be integrated with other Keras modules

==

transformers/tests/modeling_tf_common_test.py
==================
d7092d592;R√©mi Louf;2019-10-10 12:51:14 +0200;rename the attributes in the Bert Layer
Since the preloading of weights relies on the name of the class's
attributes changing the namespace breaks loading pretrained weights on
Bert and all related models. I reverted `self_attention` to `attention`
and us `crossattention` for the decoder instead.

==

transformers/modeling_bert.py
==================
51261167b;R√©mi Louf;2019-10-10 12:17:22 +0200;prune both attention and self-attention heads

==

transformers/modeling_bert.py
==================
17177e737;R√©mi Louf;2019-10-10 12:03:58 +0200;add is_decoder as an attribute to Config class

==

transformers/modeling_bert.py
==================
6596e3d56;Thomas Wolf;2019-10-10 11:56:55 +0200;Merge pull request #1454 from bkkaggle/pytorch-built-in-tensorboard
Change tensorboard imports to use built-in tensorboard if available
==
==================
4bc460119;Thomas Wolf;2019-10-10 11:56:20 +0200;Merge pull request #1480 from huggingface/fix_ctrl_tokenizer
Fixing CTRL tokenizer - Update error messages - XLM-MLM in run_generation
==
==================
177a72120;thomwolf;2019-10-10 11:45:47 +0200;move back to simple space spliting

==

examples/run_generation.py
transformers/tokenization_ctrl.py
==================
df85a0ff0;R√©mi Louf;2019-10-10 11:38:26 +0200;replace double quotes with simple quotes

==

transformers/modeling_bert.py
==================
9ca788b2e;R√©mi Louf;2019-10-10 11:33:28 +0200;merge the two Bert layers classes

==

transformers/modeling_bert.py
==================
a5997dd81;thomwolf;2019-10-10 11:31:01 +0200;better error messages

==

examples/run_generation.py
transformers/configuration_utils.py
transformers/modeling_utils.py
transformers/tokenization_utils.py
==================
edfc8f822;R√©mi Louf;2019-10-10 10:17:27 +0200;Remove  and do the branching in

==

transformers/modeling_bert.py
==================
09cfd1223;R√©mi Louf;2019-10-10 10:15:27 +0200;remove  and do the branching in

==

transformers/modeling_bert.py
==================
43a237f15;thomwolf;2019-10-10 10:11:16 +0200;switching to moses tokenizer

==

transformers/tokenization_ctrl.py
==================
877ef2c6c;R√©mi Louf;2019-10-10 10:02:18 +0200;override `from_pretrained` in Bert2Rnd
In the seq2seq model we need to both load pretrained weights in the
encoder and initialize the decoder randomly. Because the
`from_pretrained` method defined in the base class relies on module
names to assign weights, it would also initialize the decoder with
pretrained weights. To avoid this we override the method to only
initialize the encoder with pretrained weights.

==

transformers/modeling_bert.py
==================
851ef592c;R√©mi Louf;2019-10-10 10:02:03 +0200;add comment on recursive weights loading

==

transformers/modeling_utils.py
==================
036483fae;LysandreJik;2019-10-09 16:33:15 -0400;Temporary CTRL tokenizer fix

==

transformers/tokenization_ctrl.py
==================
9c2e0a4ac;LysandreJik;2019-10-09 12:14:03 -0400;Release: 2.1.0

==

docs/source/conf.py
setup.py
transformers/__init__.py
==================
7fe98d8c1;LysandreJik;2019-10-09 12:12:36 -0400;Update CTRL documentation

==

docs/source/index.rst
transformers/modeling_ctrl.py
transformers/modeling_tf_ctrl.py
==================
89f86f966;LysandreJik;2019-10-09 12:04:06 -0400;CTRL added to the documentation

==

docs/source/model_doc/ctrl.rst
==================
e17ea08e2;LysandreJik;2019-10-09 11:32:21 -0400;Pycharm folder added to gitignore

==

.gitignore
==================
2431fea98;Lysandre Debut;2019-10-09 11:31:05 -0400;Merge pull request #1383 from keskarnitish/master
Adding CTRL
==
==================
d9e60f4f0;thomwolf;2019-10-09 17:25:08 +0200;Merge branch 'master' into pr/1383

==
==================
e84470ef8;Lysandre Debut;2019-10-09 11:18:24 -0400;Merge pull request #1384 from huggingface/encoding-qol
Quality of life enhancements in encoding + patch MLM masking
==
==================
07d055f84;thomwolf;2019-10-09 17:10:04 +0200;higher tolerance

==

transformers/tests/modeling_tf_common_test.py
==================
48b438ff2;thomwolf;2019-10-09 17:06:30 +0200;doc and conversion

==

docs/source/pretrained_models.rst
transformers/__init__.py
transformers/convert_pytorch_checkpoint_to_tf2.py
==================
69629c4f0;jinoobaek-qz;2019-10-07 15:48:39 -0700;Improve naming and only do regex when necessary

==

examples/run_lm_finetuning.py
==================
bf34a252b;jinoobaek-qz;2019-10-07 15:26:57 -0700;Golden path

==

examples/run_lm_finetuning.py
==================
528d3f327;jinoobaek-qz;2019-10-07 15:25:09 -0700;Improve readability and improve make less assumptions about checkpoint format

==

examples/run_lm_finetuning.py
==================
56301bd9e;jinoobaek-qz;2019-10-05 23:48:54 -0700;Extract method

==

examples/run_lm_finetuning.py
==================
d6c546971;jinoobaek-qz;2019-10-05 22:41:38 -0700;Delete older checkpoint after saving new checkpoint

==

examples/run_lm_finetuning.py
==================
54a31f50f;jinoobaek-qz;2019-10-05 22:34:36 -0700;Add save_total_limit

==

examples/run_lm_finetuning.py
==================
c19b8e4ae;thomwolf;2019-10-09 13:51:05 +0200;fixing CTRL tests and OpenAI GPT tests

==

transformers/modeling_ctrl.py
transformers/modeling_openai.py
transformers/modeling_tf_ctrl.py
transformers/tests/modeling_tf_common_test.py
==================
6dce6dda1;thomwolf;2019-10-09 11:57:55 +0200;fixing TF 2.0 model - adding more severe test on pt/tf equivalence

==

transformers/tests/modeling_tf_common_test.py
==================
c56d921dd;thomwolf;2019-10-09 11:07:43 +0200;adding TF 2.0 model

==

transformers/configuration_ctrl.py
transformers/modeling_ctrl.py
transformers/modeling_roberta.py
transformers/modeling_tf_ctrl.py
transformers/tests/modeling_tf_ctrl_test.py
transformers/tests/modeling_tf_gpt2_test.py
==================
1c5079952;thomwolf;2019-10-09 04:26:20 +0200;simpler distilbert mask - fix tf tests

==

transformers/modeling_distilbert.py
transformers/modeling_tf_distilbert.py
==================
58b302caf;Thomas Wolf;2019-10-09 03:52:42 +0200;Merge pull request #1398 from dveselov/patch-1
Fixed typo in docs README
==
==================
439fac723;Thomas Wolf;2019-10-09 03:14:34 +0200;Merge pull request #1409 from brian41005/master
Evaluation result.txt path changing #1286
==
==================
23b7138ab;thomwolf;2019-10-09 01:54:44 +0200;fix #1378 and #1453

==

transformers/modeling_tf_distilbert.py
==================
5ce8d29ab;Bilal Khan;2019-10-08 08:14:08 -0500;Change tensorboard imports to use built-in tensorboard if available

==

examples/contrib/run_swag.py
examples/distillation/distiller.py
examples/distillation/run_squad_w_distillation.py
examples/run_glue.py
examples/run_lm_finetuning.py
examples/run_multiple_choice.py
examples/run_squad.py
==================
d688af19e;Julien Chaumond;2019-10-08 16:37:52 -0400;Update link to swift-coreml-transformers
cc @lysandrejik

==

README.md
docs/source/installation.md
docs/source/installation.rst
==================
45dc04f33;thomwolf;2019-10-08 17:37:17 +0200;tf model [WIP]

==

transformers/modeling_ctrl.py
transformers/modeling_tf_ctrl.py
==================
770b15b58;R√©mi Louf;2019-10-08 17:32:28 +0200;rename class in __init__

==

transformers/modeling_bert.py
==================
248314772;thomwolf;2019-10-08 17:19:28 +0200;fix tokenization

==

transformers/tests/tokenization_ctrl_test.py
transformers/tokenization_ctrl.py
==================
03c2c762a;thomwolf;2019-10-08 17:12:03 +0200;update tokenizer

==

transformers/tokenization_ctrl.py
==================
3edfa1d6a;thomwolf;2019-10-08 17:11:58 +0200;update model to use past

==

transformers/modeling_ctrl.py
transformers/tests/modeling_ctrl_test.py
==================
f4d41fe33;R√©mi Louf;2019-10-08 16:55:34 +0200;Merge pull request #1448 from huggingface/contributing
add contribution guidelines
==
==================
61ed88900;R√©mi Louf;2019-10-08 16:30:07 +0200;remove old seq2seq file

==

transformers/modeling_seq2seq.py
transformers/tests/modeling_bert_test.py
transformers/tests/modeling_seq2seq_test.py
==================
8abfee9ec;R√©mi Louf;2019-10-08 16:07:25 +0200;rename Bert2Bert -> Bert2Rnd

==

transformers/__init__.py
transformers/modeling_bert.py
transformers/tests/modeling_bert_test.py
==================
82628b0fc;R√©mi Louf;2019-10-08 15:57:25 +0200;add a placeholder test

==

transformers/__init__.py
transformers/tests/modeling_bert_test.py
==================
070098309;R√©mi Louf;2019-10-08 15:39:47 +0200;Add BertDecoderModel and Bert2Bert classes
I am not sure what happens when the class is initialized with the
pretrained weights.

==

transformers/modeling_bert.py
==================
75feacf17;R√©mi Louf;2019-10-08 11:39:04 +0200;add general structure for Bert2Bert class

==

transformers/modeling_bert.py
==================
15a2fc88a;R√©mi Louf;2019-10-08 11:10:35 +0200;add General attention classes
The modifications that I introduced in a previous commit did break
Bert's internal API. I reverted these changes and added more general
classes to handle the encoder-decoder attention case.

There may be a more elegant way to deal with retro-compatibility (I am
not comfortable with the current state of the code), but I cannot see it
right now.

==

transformers/modeling_bert.py
==================
cd6a59d5c;R√©mi Louf;2019-10-08 10:11:02 +0200;add a decoder layer for Bert

==

transformers/modeling_bert.py
==================
45de313a9;R√©mi Louf;2019-10-08 11:54:10 +0200;add bullet point on modifying an existing PR

==

CONTRIBUTING.md
==================
ade05b6ce;R√©mi Louf;2019-10-07 23:20:25 +0200;add code contribution

==

CONTRIBUTING.md
==================
e9c09052a;R√©mi Louf;2019-10-07 22:16:12 +0200;add issues and requests guidelines

==

CONTRIBUTING.md
==================
8fcc6507c;LysandreJik;2019-10-07 15:02:42 -0400;Multilingual

==

docs/source/index.rst
docs/source/multilingual.rst
==================
6e3e1c959;R√©mi Louf;2019-10-07 18:49:26 +0200;Merge pull request #1447 from huggingface/dev-requirements
Provide requirements.txt for development dependencies
==
==================
7ce83b493;VictorSanh;2019-10-07 12:30:27 -0400;update weights for distilgpt2

==

examples/distillation/README.md
==================
9f81f1cba;VictorSanh;2019-10-07 12:30:19 -0400;fix convert pt_to_tf2 for custom weights

==

transformers/convert_pytorch_checkpoint_to_tf2.py
==================
7afd00a66;R√©mi Louf;2019-10-07 17:58:13 +0200;freeze dev requirements

==

requirements-dev.txt
==================
a0dcefa38;R√©mi Louf;2019-10-07 17:53:58 +0200;generalize BertSelfAttention to take separate query, key, value
There is currently no way to specify the quey, key and value separately
in the Attention module. However, the decoder's "encoder-decoder
attention" layers take the decoder's last output as a query, the
encoder's states as key and value. We thus modify the existing code so
query, key and value can be added separately.

This obviously poses some naming conventions; `BertSelfAttention` is not
a self-attention module anymore. The way the residual is forwarded is
now awkard, etc. We will need to do some refacto once the decoder is
fully implemented.

==

transformers/modeling_bert.py
==================
31adbb247;R√©mi Louf;2019-10-07 16:43:21 +0200;add class wireframes for Bert decoder

==

transformers/modeling_bert.py
==================
dda1adad6;R√©mi Louf;2019-10-07 16:31:46 +0200;rename BertLayer to BertEncoderLayer

==

transformers/modeling_bert.py
==================
0053c0e05;R√©mi Louf;2019-10-07 16:29:15 +0200;do some (light) housekeeping
Several packages were imported but never used, indentation and line
spaces did not follow PEP8.

==

transformers/modeling_bert.py
==================
bd5363cc8;thomwolf;2019-10-07 15:37:30 +0200;update CTRL configuration

==

transformers/configuration_ctrl.py
==================
dc8944116;thomwolf;2019-10-07 15:37:25 +0200;update CTRL pytorch model

==

transformers/modeling_ctrl.py
==================
320b7a7e0;thomwolf;2019-10-07 14:26:59 +0200;fix #1416

==

transformers/file_utils.py
==================
386e86e22;R√©mi Louf;2019-10-07 13:00:06 +0200;raise exception when class initialized with __init__

==

transformers/modeling_seq2seq.py
==================
4446c02b8;R√©mi Louf;2019-10-07 12:04:05 +0200;add wireframe for seq2seq model

==

transformers/modeling_seq2seq.py
transformers/tests/modeling_seq2seq_test.py
==================
1615360c7;Thomas Wolf;2019-10-07 05:02:23 -0400;Merge pull request #1438 from SeanBE/master
fix pytorch-transformers migration description in README
==
==================
6dc6c716c;seanBE;2019-10-07 09:59:54 +0100;fix pytorch-transformers migration description in README

==

README.md
==================
904158ac4;Christopher Goh;2019-10-07 11:03:49 +0800;Rephrase forward method to reduce ambiguity

==

README.md
==================
0f65d8cbb;Christopher Goh;2019-10-07 01:14:34 +0800;Fix some typos in README

==

README.md
==================
1dea291a0;Santiago Castro;2019-10-06 13:35:01 -0400;Remove unnecessary use of FusedLayerNorm in XLNet

==

transformers/modeling_xlnet.py
==================
f3e0218fb;LysandreJik;2019-10-05 21:05:16 -0400;Correct device assignment in run_generation

==

examples/run_generation.py
==================
78ef1a993;thomwolf;2019-10-04 17:59:44 -0400;fixes

==

examples/utils_multiple_choice.py
transformers/data/processors/glue.py
transformers/tests/tokenization_tests_commons.py
transformers/tokenization_utils.py
==================
6c1d0bc06;thomwolf;2019-10-04 17:38:38 -0400;update encode_plus - add truncation strategies

==

examples/run_lm_finetuning.py
transformers/tests/tokenization_bert_test.py
transformers/tests/tokenization_distilbert_test.py
transformers/tests/tokenization_roberta_test.py
transformers/tests/tokenization_tests_commons.py
transformers/tests/tokenization_xlm_test.py
transformers/tests/tokenization_xlnet_test.py
transformers/tokenization_bert.py
transformers/tokenization_roberta.py
transformers/tokenization_utils.py
transformers/tokenization_xlm.py
transformers/tokenization_xlnet.py
==================
0820bb055;VictorSanh;2019-10-04 17:16:10 -0400;unecessary carriage return

==

examples/run_squad.py
==================
f5891c382;VictorSanh;2019-10-04 17:15:04 -0400;run_squad --> run_squad_w_distillation

==

examples/distillation/run_squad_w_distillation.py
examples/run_squad.py
==================
764a7923e;VictorSanh;2019-09-27 17:44:32 -0400;add distillation+finetuning option in run_squad

==

examples/run_squad.py
==================
bb464289c;Lysandre Debut;2019-10-04 16:41:26 -0400;New model addition issue template

==

.github/ISSUE_TEMPLATE/--new-model-addition.md
.github/ISSUE_TEMPLATE/bug-report.md
.github/ISSUE_TEMPLATE/feature-request.md
.github/ISSUE_TEMPLATE/migration.md
.github/ISSUE_TEMPLATE/question-help.md
==================
92c0f2fb9;thomwolf;2019-10-04 15:48:06 -0400;Merge remote-tracking branch 'origin/julien_multiple-choice' into encoding-qol

==
==================
9e136ff57;Julien Chaumond;2019-10-04 15:00:56 -0400;Honor args.overwrite_cache (h/t @erenup)

==

examples/run_glue.py
examples/run_multiple_choice.py
==================
7bddb45a6;LysandreJik;2019-10-04 14:27:38 -0400;Decode documentaton

==

transformers/tokenization_utils.py
==================
dbed1c5d9;keskarnitish;2019-09-30 09:48:41 -0700;Adding CTRL (squashed commit)
adding conversion script

adding first draft of modeling & tokenization

adding placeholder for test files

bunch of changes

registering the tokenizer/model/etc

tests

change link; something is very VERY wrong here

weird end-of-word thingy going on

i think the tokenization works now ; wrote the unit tests

overall structure works;load w next

the monster is alive!

works after some cleanup as well

adding emacs autosave to gitignore

currently only supporting the 48 layer one; seems to infer fine on my macbook

cleanup

fixing some documentation

fixing some documentation

tests passing?

now works on CUDA also

adding greedy?

adding greedy sampling

works well

==

.gitignore
README.md
examples/run_generation.py
transformers/__init__.py
transformers/configuration_auto.py
transformers/configuration_ctrl.py
transformers/modeling_auto.py
transformers/modeling_ctrl.py
transformers/tests/modeling_ctrl_test.py
transformers/tests/tokenization_ctrl_test.py
transformers/tokenization_auto.py
transformers/tokenization_ctrl.py
==================
b3cfd9794;Thomas Wolf;2019-10-03 19:04:02 -0400;Merge pull request #1373 from TimYagan/fix-css
Fixed critical css font-family issues
==
==================
81a1e1246;Lysandre Debut;2019-10-03 22:43:57 +0000;Merge pull request #1313 from enzoampil/master
Add option to use a 'stop token'
==
==================
d3f24dfad;Lysandre Debut;2019-10-03 22:43:09 +0000;Merge branch 'master' into master

==
==================
ecc4f1bdf;LysandreJik;2019-10-03 17:42:16 -0400;XLM use_lang_embedding flag in run_generation

==

examples/run_generation.py
==================
c2c2ca0fd;LysandreJik;2019-10-03 17:18:48 -0400;Added XLM to run_generation, with prompt language selection.

==

examples/run_generation.py
==================
1569610f2;Thomas Wolf;2019-10-03 17:06:17 -0400;Merge pull request #1296 from danai-antoniou/add-duplicate-tokens-error
Added ValueError for duplicates in list of added tokens
==
==================
e1b2949ae;drc10723;2019-10-03 21:22:36 +0530;DistillBert Documentation Code Example fixes

==

transformers/modeling_distilbert.py
transformers/modeling_tf_distilbert.py
==================
899883644;Simon Layton;2019-10-03 12:05:15 -0400;Fix test fails and warnings
Attention output was in bnij ordering instead of ijbn which everything
else will expect. This was an oversight on my part, and keeps the
attention inputs/outputs identical to the original code.

Also moved back from tensor slicing to index_select in rel_shift_bnij to
make the tracer happy.

==

transformers/modeling_xlnet.py
==================
e2ae9c0b7;VictorSanh;2019-10-03 11:42:21 -0400;fix links in doc index

==

docs/source/index.rst
==================
aebd83230;LysandreJik;2019-10-02 18:04:38 -0400;Update naming + remove f string in run_lm_finetuning example

==

examples/run_lm_finetuning.py
transformers/tests/tokenization_tests_commons.py
transformers/tokenization_bert.py
transformers/tokenization_roberta.py
transformers/tokenization_utils.py
transformers/tokenization_xlm.py
transformers/tokenization_xlnet.py
==================
651bfb7ad;LysandreJik;2019-09-30 17:27:40 -0400;always_truncate by default

==

transformers/tests/tokenization_tests_commons.py
transformers/tokenization_utils.py
==================
5ed50a93f;LysandreJik;2019-09-30 14:14:27 -0400;LM finetuning won't mask special tokens anymore

==

examples/run_lm_finetuning.py
==================
cc412edd4;LysandreJik;2019-09-30 14:11:41 -0400;Supports already existing special tokens

==

transformers/tests/tokenization_tests_commons.py
transformers/tokenization_bert.py
transformers/tokenization_roberta.py
transformers/tokenization_utils.py
transformers/tokenization_xlm.py
transformers/tokenization_xlnet.py
==================
2f259b228;LysandreJik;2019-09-30 11:48:18 -0400;Sequence IDS

==

transformers/tests/tokenization_tests_commons.py
transformers/tokenization_bert.py
transformers/tokenization_roberta.py
transformers/tokenization_utils.py
transformers/tokenization_xlm.py
transformers/tokenization_xlnet.py
==================
7c789c337;LysandreJik;2019-09-30 10:20:14 -0400;Always truncate argument in the encode method

==

transformers/tests/tokenization_tests_commons.py
transformers/tokenization_utils.py
==================
7af077791;Brian Ma;2019-10-03 16:29:43 +0800;Update run_glue.py
add DistilBert model shortcut into ALL_MODELS
==

examples/run_glue.py
==================
c1689ac30;VictorSanh;2019-10-03 10:56:39 -0400;fix name

==

docs/source/pretrained_models.rst
==================
4a790c40b;VictorSanh;2019-10-03 10:54:02 -0400;update doc for distil*

==

docs/source/pretrained_models.rst
==================
6be46a6e6;VictorSanh;2019-10-03 10:07:18 -0400;update links to new weights

==

transformers/configuration_gpt2.py
transformers/modeling_gpt2.py
transformers/modeling_tf_gpt2.py
transformers/tokenization_gpt2.py
==================
5f07d8f11;VictorSanh;2019-10-03 09:59:32 -0400;prepare release

==

examples/distillation/README.md
==================
35071007c;VictorSanh;2019-10-02 23:01:36 -0400;incoming release üî• update links to arxiv preprint

==

README.md
examples/distillation/README.md
==================
f1f23ad17;VictorSanh;2019-10-02 19:03:32 -0400;fix buf in convert_pt_chkpt_to_tf2

==

transformers/convert_pytorch_checkpoint_to_tf2.py
==================
2a91f6071;VictorSanh;2019-10-02 15:02:42 -0400;upddate README - TODO updadte link to paper

==

examples/distillation/README.md
==================
c51e533a5;VictorSanh;2019-10-02 11:02:53 -0400;update train.py

==

examples/distillation/train.py
==================
a76c3f9cb;VictorSanh;2019-10-02 11:02:43 -0400;update requirements

==

examples/distillation/requirements.txt
==================
bb9c5ead5;VictorSanh;2019-10-02 11:02:30 -0400;update distiller

==

examples/distillation/distiller.py
==================
a12ab0a8d;VictorSanh;2019-10-02 11:01:55 -0400;update binarized_data

==

examples/distillation/scripts/binarized_data.py
==================
4d6dfbd37;VictorSanh;2019-10-02 11:01:41 -0400;update extract

==

examples/distillation/scripts/extract.py
==================
23edebc07;VictorSanh;2019-10-02 11:01:33 -0400;update extract_distilbert

==

examples/distillation/scripts/extract_distilbert.py
==================
cbfcfce20;VictorSanh;2019-10-02 11:01:20 -0400;update token_counts

==

examples/distillation/scripts/token_counts.py
==================
19e4ebbe3;VictorSanh;2019-10-02 11:01:07 -0400;grouped_batch_sampler

==

examples/distillation/grouped_batch_sampler.py
==================
594202a93;VictorSanh;2019-10-02 11:00:57 -0400;lm_seqs_dataset

==

examples/distillation/lm_seqs_dataset.py
==================
38084507c;VictorSanh;2019-10-02 11:00:46 -0400;add distillation_configs

==

examples/distillation/training_configs/distilbert-base-uncased.json
examples/distillation/training_configs/distilgpt2.json
==================
9ffda216e;Simon Layton;2019-10-03 09:23:16 -0400;Fix missed head transpose

==

transformers/modeling_xlnet.py
==================
b5d73976a;erenup;2019-10-03 20:48:17 +0800;Revert "fixing for roberta tokenizer decoding"
This reverts commit 22e7c4edaf007d92912df54c336d10078ac7d565.

==

examples/run_squad.py
examples/utils_squad.py
==================
22e7c4eda;erenup;2019-10-03 18:33:53 +0800;fixing for roberta tokenizer decoding

==

examples/run_squad.py
examples/utils_squad.py
==================
2195c0d5f;Brian Ma;2019-10-03 12:49:12 +0800;Evaluation result.txt path changing #1286

==

examples/run_glue.py
examples/run_lm_finetuning.py
examples/run_multiple_choice.py
==================
ebb32261b;LysandreJik;2019-10-02 17:52:56 -0400;fix #1401

==

docs/source/pretrained_models.rst
==================
d51b58940;Simon Layton;2019-09-18 12:52:32 -0400;Re-order attention head outputs for better perf
Significant performance boost over the original orderings
on an already somewhat optimised branch this gave me > 2x end-to-end
throughput on a squad xlnet fine-tuning task (batch 8, seq-length 612,
fp16)

==

transformers/modeling_xlnet.py
==================
63ed224b7;Santiago Castro;2019-10-02 11:02:08 -0400;initialy -> initially

==

transformers/modeling_bert.py
transformers/modeling_tf_bert.py
transformers/modeling_tf_distilbert.py
transformers/modeling_tf_xlm.py
==================
a95158518;danai-antoniou;2019-10-02 07:44:15 +0100;Moved duplicate token check

==

transformers/tokenization_utils.py
==================
d73957899;danai-antoniou;2019-10-02 07:38:50 +0100;Merge branch 'master' of https://github.com/danai-antoniou/pytorch-transformers into add-duplicate-tokens-error

==
==================
cd69bc9c8;Dima Veselov;2019-10-02 03:21:55 +0300;Fixed typo in docs README

==

docs/README.md
==================
391db836a;thomwolf;2019-10-01 19:09:13 -0400;fix #1260 - remove special logic for decoding pairs of sequence

==

transformers/tokenization_utils.py
==================
963529e29;Thomas Wolf;2019-10-01 18:46:07 -0400;Merge pull request #1288 from echan00/master
Typo with LM Fine tuning script
==
==================
f7978f70e;thomwolf;2019-10-01 18:45:38 -0400;use format instead of f-strings

==

examples/run_lm_finetuning.py
==================
1e4a19136;Thomas Wolf;2019-10-01 18:40:22 -0400;Merge pull request #1284 from slayton58/pooler_end_logits_fp16_fix
Fix fp16 masking in PoolerEndLogits
==
==================
c50783e38;thomwolf;2019-10-01 18:17:48 -0400;Merge branch 'pooler_end_logits_fp16_fix' of https://github.com/slayton58/pytorch-transformers into pr/1284

==
==================
6971556ab;DenysNahurnyi;2019-10-01 21:57:18 +0300;Fix syntax typo in README.md

==

README.md
==================
b35066295;Julien Chaumond;2019-09-30 16:37:09 -0400;overflowing_tokens do not really make sense here, let's just return a number
Co-Authored-By: Lysandre Debut <lysandre.debut@reseau.eseo.fr>

==

examples/utils_multiple_choice.py
transformers/tokenization_utils.py
==================
f5bcde0b2;Julien Chaumond;2019-09-30 16:04:55 -0400;[multiple-choice] Simplify and use tokenizer.encode_plus

==

examples/README.md
examples/run_multiple_choice.py
examples/utils_multiple_choice.py
transformers/tokenization_utils.py
==================
5c3b32d44;Santosh Gupta;2019-09-28 16:35:06 -0700;Update README.md
Lines 183 - 200, fixed indentation. Line 198, replaced `tokenizer_class` with `BertTokenizer`, since `tokenizer_class` is not defined in the loop it belongs to.
==

README.md
==================
2dc8cb873;VictorSanh;2019-09-29 19:51:01 -0400;fix unknown imports (*ForMultipleChoice) in run_multiple_choice

==

transformers/__init__.py
==================
0a4ed7192;Tim Yagan;2019-09-29 13:51:01 +0200;Fixed critical css font-family issues
Fixed critical css font-family issues to ensure compatibility with multiple webbrowsers
==

docs/source/_static/css/huggingface.css
==================
ae50ad91e;Thomas Wolf;2019-09-28 10:26:42 +0200;Merge pull request #1362 from FeiWang96/doc
fix link
==
==================
60f791631;wangfei;2019-09-28 16:20:17 +0800;Fix link in readme

==

README.md
==================
a6a6d9e63;Ikuya Yamada;2019-09-12 12:13:37 -1000;fix padding_idx of RoBERTa model

==

transformers/modeling_roberta.py
==================
d8b641c83;Julien Chaumond;2019-09-27 17:22:01 -0400;6 -> 8 models

==

docs/source/quickstart.md
transformers/tokenization_utils.py
==================
c6acbdd50;Julien Chaumond;2019-09-27 17:02:29 -0400;Close #1304

==

transformers/tokenization_roberta.py
==================
df7cd9e4e;Thomas Wolf;2019-09-27 23:00:34 +0200;Merge pull request #1353 from wendingp/patch-1
Fix some typos
==
==================
6a17b3c51;Thomas Wolf;2019-09-27 22:59:54 +0200;Merge pull request #1355 from agrinh/master
Fix tensorflow_dataset glue support
==
==================
04e9a6f51;Thomas Wolf;2019-09-27 22:58:19 +0200;Merge pull request #1359 from dennymarcels/patch-1
Update run_lm_finetuning.py
==
==================
947859063;Denny;2019-09-27 15:18:42 -0300;Update run_lm_finetuning.py
The previous method, just as phrased, did not exist in the class.
==

examples/run_lm_finetuning.py
==================
795b3e76f;Agrin Hilmkil;2019-09-27 17:32:28 +0200;Add docstring for processor method

==

transformers/data/processors/glue.py
transformers/data/processors/utils.py
==================
e31a47280;Agrin Hilmkil;2019-09-27 16:51:17 +0200;Fix tensorflow_dataset glue support
`glue_convert_examples_to_features` assumed that tensorflow_dataset
examples contains the features `'sentence1'` and `'sentence2'`. This
commit encapsulates the choice of features in the glue processor and
uses that to parse examples.

==

transformers/data/processors/glue.py
==================
4f2b6579b;pj;2019-09-27 22:55:43 +0800;Fix some typos

==

docs/source/quickstart.md
==================
ca559826c;Thomas Wolf;2019-09-27 13:08:00 +0200;Merge pull request #1349 from ogabrielluiz/master
Just some typos
==
==================
d2de5b9d8;Gabriel Luiz Freitas Almeida;2019-09-27 07:08:36 -0300;Just some typos

==

docs/source/quickstart.md
==================
d83d29576;Thomas Wolf;2019-09-27 10:35:12 +0200;Merge pull request #1337 from mgrankin/fastdataset
faster dataset building
==
==================
f6de00030;Thomas Wolf;2019-09-27 10:30:07 +0200;Merge pull request #1346 from BramVanroy/documentation
Add small  note about the output of hidden states (closes #1332)
==
==================
15749bfc1;BramVanroy;2019-09-27 10:01:36 +0200;Add small  note about the output of hidden states

==

README.md
==================
da2e47ad1;thomwolf;2019-09-27 09:41:15 +0200;clean up a little run_tf_glue

==

examples/run_tf_glue.py
==================
528c288fa;thomwolf;2019-09-27 09:40:29 +0200;clean up run_tf_glue

==

examples/run_tf_glue.py
==================
702f58984;VictorSanh;2019-09-27 00:20:14 -0400;fix input in run_glue for distilbert

==

examples/run_glue.py
==================
22d2fded2;Julien Chaumond;2019-09-26 18:22:45 -0400;[docs] Fix doc auto-deploy
Co-Authored-By: Lysandre Debut <lysandre.debut@reseau.eseo.fr>

==

.circleci/config.yml
==================
fc9faa8a4;Julien Chaumond;2019-09-26 18:19:51 -0400;[docs] Doc tweaks
Co-Authored-By: Lysandre Debut <lysandre.debut@reseau.eseo.fr>

==

docs/README.md
docs/requirements.txt
docs/source/_static/js/custom.js
docs/source/examples.md
==================
ecfddc603;LysandreJik;2019-09-26 16:49:03 -0400;Update RoBERTa and GPT-2 Tokenizer documentation (fix #1343)

==

transformers/tokenization_gpt2.py
transformers/tokenization_roberta.py
==================
93f0c5fc7;LysandreJik;2019-09-26 11:45:00 -0400;Repository link in the documentation

==

docs/source/index.rst
==================
6c3b13151;thomwolf;2019-09-26 16:23:28 +0200;typo in readme/doc

==

README.md
docs/source/model_doc/xlm.rst
==================
f83b35b77;thomwolf;2019-09-26 16:14:23 +0200;Merge branch 'master' of https://github.com/huggingface/pytorch-transformers

==
==================
4e63c9072;thomwolf;2019-09-26 16:14:21 +0200;update installation instructions in readme

==

README.md
==================
7e957237e;LysandreJik;2019-09-26 10:08:56 -0400;[Doc] XLM + Torch in documentation

==

docs/requirements.txt
docs/source/model_doc/xlm.rst
==================
302a4813a;LysandreJik;2019-09-26 09:57:30 -0400;Doc building requirements [TF2]

==

docs/requirements.txt
==================
f71a4577b;mgrankin;2019-09-26 16:53:13 +0300;faster dataset building

==

examples/run_lm_finetuning.py
==================
a3e0dbba9;LysandreJik;2019-09-26 09:51:14 -0400;Doc building requirements [TF]

==

docs/requirements.txt
==================
0f92f76ca;Lysandre Debut;2019-09-26 08:59:52 -0400;CircleCI reference in README

==

README.md
==================
4094958df;LysandreJik;2019-09-26 08:50:55 -0400;Doc building requirements

==

docs/requirements.txt
==================
7d8b395af;LysandreJik;2019-09-26 08:49:31 -0400;Doc building requirements

==

docs/requirements.txt
==================
927904bc9;LysandreJik;2019-09-26 08:47:15 -0400;[doc] pytorch_transformers -> transformers

==

docs/source/main_classes/model.rst
docs/source/main_classes/processors.rst
docs/source/model_doc/bert.rst
docs/source/model_doc/distilbert.rst
docs/source/model_doc/gpt.rst
docs/source/model_doc/gpt2.rst
docs/source/model_doc/roberta.rst
docs/source/model_doc/transformerxl.rst
docs/source/model_doc/xlnet.rst
==================
294edfd83;LysandreJik;2019-09-26 08:16:12 -0400;Release version in documentation

==

docs/source/conf.py
==================
de5e4864c;LysandreJik;2019-09-26 08:04:54 -0400;Documentation

==

docs/source/index.rst
==================
e4e35296f;thomwolf;2019-09-26 13:52:24 +0200;update setup.py metadata

==

setup.py
==================
1d646badb;thomwolf;2019-09-26 13:48:00 +0200;Merge branch 'master' of https://github.com/huggingface/pytorch-transformers

==
==================
9676d1a2a;thomwolf;2019-09-26 13:47:58 +0200;update readme and setup.py

==

README.md
setup.py
==================
8349d7577;LysandreJik;2019-09-25 09:05:43 -0400;Various small doc fixes

==

docs/source/main_classes/processors.rst
docs/source/model_doc/xlm.rst
docs/source/model_doc/xlnet.rst
==================
fb056494e;LysandreJik;2019-09-25 08:48:53 -0400;Example usage

==

docs/source/main_classes/processors.rst
==================
36f592cc8;LysandreJik;2019-09-25 08:39:33 -0400;Updated doc for `InputExample` and `InputFeatures`

==

docs/source/main_classes/processors.rst
transformers/data/processors/utils.py
==================
ad4a393e2;LysandreJik;2019-09-25 08:30:07 -0400;Changed processor documentation architecture. Added documentation for GLUE

==

docs/source/main_classes/processors.rst
transformers/data/processors/glue.py
==================
c4ac7a76d;LysandreJik;2019-09-25 07:09:30 -0400;GLUE processors

==

docs/source/index.rst
docs/source/main_classes/processors.rst
==================
4acd87ff4;LysandreJik;2019-09-25 06:31:05 -0400;TF models added to documentation

==

docs/source/main_classes/model.rst
docs/source/model_doc/bert.rst
docs/source/model_doc/distilbert.rst
docs/source/model_doc/gpt.rst
docs/source/model_doc/gpt2.rst
docs/source/model_doc/roberta.rst
docs/source/model_doc/transformerxl.rst
docs/source/model_doc/xlm.rst
docs/source/model_doc/xlnet.rst
==================
cf5c5c9e1;LysandreJik;2019-09-26 07:43:13 -0400;Documentation

==

docs/source/index.rst
docs/source/pretrained_models.rst
==================
4dde31cb7;thomwolf;2019-09-26 12:18:26 +0200;update readme

==

README.md
==================
17ea43cf9;Thomas Wolf;2019-09-26 12:11:03 +0200;Merge pull request #1203 from huggingface/tf2
[2.0] TF 2.0 support
==
==================
80bf868a2;thomwolf;2019-09-26 12:04:47 +0200;Merge branch 'master' into tf2

==
==================
481d9c4fb;thomwolf;2019-09-26 12:02:54 +0200;Merge branch 'master' into tf2

==
==================
4ddc31ff4;thomwolf;2019-09-26 12:00:38 +0200;update readme with migration change

==

README.md
==================
f47f7f461;thomwolf;2019-09-26 11:28:44 +0200;add logo

==

README.md
docs/source/imgs/transformers_logo_name.png
==================
9fabc0b6a;thomwolf;2019-09-26 11:21:34 +0200;wip readme

==

README.md
==================
31c23bd5e;thomwolf;2019-09-26 10:15:53 +0200;[BIG] pytorch-transformers => transformers

==

.circleci/config.yml
.coveragerc
.github/ISSUE_TEMPLATE/migration.md
README.md
docker/Dockerfile
docs/source/_static/js/custom.js
docs/source/bertology.rst
docs/source/conf.py
docs/source/converting_tensorflow_models.rst
docs/source/index.rst
docs/source/installation.rst
docs/source/main_classes/configuration.rst
docs/source/main_classes/model.rst
docs/source/main_classes/optimizer_schedules.rst
docs/source/main_classes/tokenizer.rst
docs/source/migration.md
docs/source/model_doc/auto.rst
docs/source/model_doc/bert.rst
docs/source/model_doc/distilbert.rst
docs/source/model_doc/gpt.rst
docs/source/model_doc/gpt2.rst
docs/source/model_doc/roberta.rst
docs/source/model_doc/transformerxl.rst
docs/source/model_doc/xlm.rst
docs/source/model_doc/xlnet.rst
docs/source/notebooks.rst
docs/source/pretrained_models.rst
docs/source/quickstart.md
docs/source/serialization.rst
docs/source/torchscript.rst
examples/README.md
examples/contrib/run_openai_gpt.py
examples/contrib/run_swag.py
examples/contrib/run_transfo_xl.py
examples/distillation/README.md
examples/distillation/distiller.py
examples/distillation/scripts/binarized_data.py
examples/distillation/scripts/extract_for_distil.py
examples/distillation/train.py
examples/run_bertology.py
examples/run_generation.py
examples/run_glue.py
examples/run_lm_finetuning.py
examples/run_multiple_choice.py
examples/run_squad.py
examples/run_tf_glue.py
examples/utils_squad.py
hubconf.py
setup.py
transformers/__init__.py
transformers/__main__.py
transformers/configuration_auto.py
transformers/configuration_bert.py
transformers/configuration_distilbert.py
transformers/configuration_gpt2.py
transformers/configuration_openai.py
transformers/configuration_roberta.py
transformers/configuration_transfo_xl.py
transformers/configuration_utils.py
transformers/configuration_xlm.py
transformers/configuration_xlnet.py
transformers/convert_bert_original_tf_checkpoint_to_pytorch.py
transformers/convert_bert_pytorch_checkpoint_to_original_tf.py
transformers/convert_gpt2_original_tf_checkpoint_to_pytorch.py
transformers/convert_openai_original_tf_checkpoint_to_pytorch.py
transformers/convert_pytorch_checkpoint_to_tf2.py
transformers/convert_roberta_original_pytorch_checkpoint_to_pytorch.py
transformers/convert_transfo_xl_original_tf_checkpoint_to_pytorch.py
transformers/convert_xlm_original_pytorch_checkpoint_to_pytorch.py
transformers/convert_xlnet_original_tf_checkpoint_to_pytorch.py
transformers/data/__init__.py
transformers/data/metrics/__init__.py
transformers/data/processors/__init__.py
transformers/data/processors/glue.py
transformers/data/processors/utils.py
transformers/file_utils.py
transformers/modeling_auto.py
transformers/modeling_bert.py
transformers/modeling_distilbert.py
transformers/modeling_gpt2.py
transformers/modeling_openai.py
transformers/modeling_roberta.py
transformers/modeling_tf_auto.py
transformers/modeling_tf_bert.py
transformers/modeling_tf_distilbert.py
transformers/modeling_tf_gpt2.py
transformers/modeling_tf_openai.py
transformers/modeling_tf_pytorch_utils.py
transformers/modeling_tf_roberta.py
transformers/modeling_tf_transfo_xl.py
transformers/modeling_tf_transfo_xl_utilities.py
transformers/modeling_tf_utils.py
transformers/modeling_tf_xlm.py
transformers/modeling_tf_xlnet.py
transformers/modeling_transfo_xl.py
transformers/modeling_transfo_xl_utilities.py
transformers/modeling_utils.py
transformers/modeling_xlm.py
transformers/modeling_xlnet.py
transformers/optimization.py
transformers/tests/__init__.py
transformers/tests/configuration_common_test.py
transformers/tests/conftest.py
transformers/tests/fixtures/input.txt
transformers/tests/fixtures/sample_text.txt
transformers/tests/fixtures/test_sentencepiece.model
transformers/tests/modeling_auto_test.py
transformers/tests/modeling_bert_test.py
transformers/tests/modeling_common_test.py
transformers/tests/modeling_distilbert_test.py
transformers/tests/modeling_gpt2_test.py
transformers/tests/modeling_openai_test.py
transformers/tests/modeling_roberta_test.py
transformers/tests/modeling_tf_auto_test.py
transformers/tests/modeling_tf_bert_test.py
transformers/tests/modeling_tf_common_test.py
transformers/tests/modeling_tf_distilbert_test.py
transformers/tests/modeling_tf_gpt2_test.py
transformers/tests/modeling_tf_openai_gpt_test.py
transformers/tests/modeling_tf_roberta_test.py
transformers/tests/modeling_tf_transfo_xl_test.py
transformers/tests/modeling_tf_xlm_test.py
transformers/tests/modeling_tf_xlnet_test.py
transformers/tests/modeling_transfo_xl_test.py
transformers/tests/modeling_xlm_test.py
transformers/tests/modeling_xlnet_test.py
transformers/tests/optimization_test.py
transformers/tests/tokenization_auto_test.py
transformers/tests/tokenization_bert_test.py
transformers/tests/tokenization_distilbert_test.py
transformers/tests/tokenization_gpt2_test.py
transformers/tests/tokenization_openai_test.py
transformers/tests/tokenization_roberta_test.py
transformers/tests/tokenization_tests_commons.py
transformers/tests/tokenization_transfo_xl_test.py
transformers/tests/tokenization_utils_test.py
transformers/tests/tokenization_xlm_test.py
transformers/tests/tokenization_xlnet_test.py
transformers/tokenization_auto.py
transformers/tokenization_bert.py
transformers/tokenization_distilbert.py
transformers/tokenization_gpt2.py
transformers/tokenization_openai.py
transformers/tokenization_roberta.py
transformers/tokenization_transfo_xl.py
transformers/tokenization_utils.py
transformers/tokenization_xlm.py
transformers/tokenization_xlnet.py
==================
2f071fcb0;thomwolf;2019-09-26 10:09:45 +0200;clean up TFConv1D API

==

pytorch_transformers/modeling_tf_utils.py
==================
570533344;thomwolf;2019-09-26 10:06:20 +0200;add initialization for everybody

==

examples/run_tf_glue.py
pytorch_transformers/modeling_tf_distilbert.py
pytorch_transformers/modeling_tf_gpt2.py
pytorch_transformers/modeling_tf_openai.py
pytorch_transformers/modeling_tf_roberta.py
pytorch_transformers/modeling_tf_transfo_xl.py
pytorch_transformers/modeling_tf_utils.py
pytorch_transformers/modeling_tf_xlm.py
pytorch_transformers/modeling_tf_xlnet.py
==================
f2a337b3e;thomwolf;2019-09-26 09:02:43 +0200;fix tokenization tests for gpt2 roberta

==

pytorch_transformers/tests/tokenization_gpt2_test.py
pytorch_transformers/tests/tokenization_roberta_test.py
==================
4a233e5b2;Thomas Wolf;2019-09-26 08:50:02 +0200;Merge pull request #1315 from bryant1410/patch-1
Remove unnecessary use of FusedLayerNorm
==
==================
7a99e4b19;thomwolf;2019-09-26 08:41:02 +0200;fix #1196 and fix #1285

==

pytorch_transformers/tokenization_gpt2.py
==================
7c9f8f93f;thomwolf;2019-09-26 01:59:53 +0200;fix tests

==

examples/run_tf_glue.py
pytorch_transformers/modeling_tf_bert.py
==================
d6dde438e;thomwolf;2019-09-26 01:45:55 +0200;add batch dimension in encode

==

examples/run_tf_glue.py
pytorch_transformers/tokenization_utils.py
==================
4a21c4d88;thomwolf;2019-09-26 01:30:06 +0200;add warning if neither pt nor tf are found

==

examples/run_tf_glue.py
pytorch_transformers/__init__.py
==================
2967de06f;thomwolf;2019-09-25 22:08:38 +0200;adding intialization to bert

==

pytorch_transformers/modeling_tf_bert.py
pytorch_transformers/modeling_tf_utils.py
==================
a6bcfb801;thomwolf;2019-09-25 21:14:12 +0200;fix tests

==

pytorch_transformers/__init__.py
pytorch_transformers/file_utils.py
pytorch_transformers/tokenization_utils.py
==================
78863f6b3;thomwolf;2019-09-25 21:09:46 +0200;fix tokenizer to tensors

==

pytorch_transformers/tokenization_utils.py
==================
8a618e0af;thomwolf;2019-09-25 21:04:52 +0200;clean up __init__

==

pytorch_transformers/__init__.py
pytorch_transformers/tokenization_utils.py
==================
3b7fb48c3;thomwolf;2019-09-25 17:46:16 +0200;fix loading from tf/pt

==

examples/run_tf_glue.py
pytorch_transformers/modeling_tf_utils.py
pytorch_transformers/modeling_utils.py
==================
a049c8043;thomwolf;2019-09-25 17:33:16 +0200;push fix to training

==

examples/run_glue.py
examples/run_tf_glue.py
pytorch_transformers/configuration_utils.py
pytorch_transformers/data/processors/utils.py
pytorch_transformers/modeling_tf_utils.py
pytorch_transformers/modeling_utils.py
==================
a9f24a16b;mataney;2019-09-25 15:53:29 +0300;[FIX] fix run_generation.py to work with batch_size > 1

==

examples/run_generation.py
==================
5def3302f;thomwolf;2019-09-25 12:38:08 +0200;update run_glue

==

examples/run_glue.py
==================
f71758f7a;thomwolf;2019-09-25 12:00:50 +0200;update internal glue processors

==

examples/run_glue.py
pytorch_transformers/data/processors/glue.py
pytorch_transformers/data/processors/utils.py
==================
0f091062d;thomwolf;2019-09-25 10:21:52 +0200;Merge branch 'glue-example' into tf2

==
==================
c4acc3a8e;thomwolf;2019-09-25 10:19:14 +0200;let encode accept tensor inputs

==

pytorch_transformers/__init__.py
pytorch_transformers/file_utils.py
pytorch_transformers/tokenization_utils.py
==================
e8e956dbb;Thomas Wolf;2019-09-24 22:49:57 +0200;Merge pull request #1327 from huggingface/tf2-determinism
Pytorch/TF2 determinism
==
==================
e4022d96f;Thomas Wolf;2019-09-24 21:40:10 +0200;Merge pull request #1325 from huggingface/glue-included
[Proposal] GLUE processors included in library
==
==================
1761d2091;LysandreJik;2019-09-24 14:59:10 -0400;Check to see if the models have the same results when in eval mode (pt) or when training=False (tf)

==

pytorch_transformers/tests/modeling_common_test.py
pytorch_transformers/tests/modeling_tf_common_test.py
==================
789ea7203;thomwolf;2019-09-24 17:32:01 +0200;fix output_token_type in glue

==

pytorch_transformers/data/processors/glue.py
==================
1cbd566c6;thomwolf;2019-09-24 17:24:52 +0200;Merge branch 'glue-example' into glue-included

==
==================
743e383d4;thomwolf;2019-09-24 17:21:54 +0200;py2 fix

==

pytorch_transformers/data/metrics/__init__.py
==================
99a90e43d;thomwolf;2019-09-24 17:16:46 +0200;update data processors __init__

==

pytorch_transformers/__init__.py
pytorch_transformers/data/__init__.py
pytorch_transformers/data/processors/__init__.py
pytorch_transformers/data/processors/glue.py
==================
b5ec526f8;thomwolf;2019-09-24 17:10:50 +0200;updated data processor and metrics

==

.gitignore
examples/run_glue.py
pytorch_transformers/__init__.py
pytorch_transformers/data/__init__.py
pytorch_transformers/data/metrics/__init__.py
pytorch_transformers/data/processors/__init__.py
pytorch_transformers/data/processors/glue.py
pytorch_transformers/data/processors/utils.py
pytorch_transformers/preprocessing/__init__.py
==================
a6981076e;thomwolf;2019-09-24 16:46:26 +0200;various updates

==

examples/utils_glue.py
pytorch_transformers/tests/tokenization_tests_commons.py
pytorch_transformers/tokenization_bert.py
pytorch_transformers/tokenization_roberta.py
pytorch_transformers/tokenization_utils.py
pytorch_transformers/tokenization_xlm.py
pytorch_transformers/tokenization_xlnet.py
==================
0b82e3d0d;LysandreJik;2019-09-24 09:52:25 -0400;Relative imports

==

pytorch_transformers/preprocessing/__init__.py
pytorch_transformers/preprocessing/glue.py
==================
f09e5ecef;LysandreJik;2019-09-24 09:47:34 -0400;[Proposal] GLUE processors included in library

==

examples/run_glue.py
pytorch_transformers/preprocessing/__init__.py
pytorch_transformers/preprocessing/glue.py
pytorch_transformers/preprocessing/utils.py
==================
128bdd4c3;thomwolf;2019-09-24 15:43:39 +0200;fix tests pt/tf

==

pytorch_transformers/modeling_tf_pytorch_utils.py
==================
72402d1ac;LysandreJik;2019-09-24 09:41:14 -0400;Fixed DistilBERT tokenizer

==

pytorch_transformers/tests/tokenization_distilbert_test.py
pytorch_transformers/tokenization_distilbert.py
==================
28a30af6d;thomwolf;2019-09-24 15:33:39 +0200;fix auto models

==

pytorch_transformers/modeling_tf_auto.py
==================
de203853c;thomwolf;2019-09-24 15:30:55 +0200;docstring for xlnet

==

pytorch_transformers/modeling_tf_utils.py
pytorch_transformers/modeling_tf_xlm.py
pytorch_transformers/modeling_tf_xlnet.py
==================
559790f9e;thomwolf;2019-09-24 15:26:57 +0200;docstring for xlm

==

pytorch_transformers/modeling_tf_bert.py
==================
b3087ddde;thomwolf;2019-09-24 15:21:51 +0200;docstring t-xl

==

pytorch_transformers/modeling_tf_transfo_xl.py
==================
4761a3978;thomwolf;2019-09-24 15:19:09 +0200;doctring roberta

==

pytorch_transformers/modeling_tf_gpt2.py
pytorch_transformers/modeling_tf_roberta.py
==================
45a6f2edd;thomwolf;2019-09-24 15:15:47 +0200;docstring for GPT

==

pytorch_transformers/modeling_tf_openai.py
==================
e7ba5bc85;thomwolf;2019-09-24 15:12:36 +0200;docstring for GPT2

==

pytorch_transformers/modeling_tf_gpt2.py
==================
d340e2329;LysandreJik;2019-09-24 09:09:28 -0400;create_mask_from_sequences -> create_token_type_ids_from_sequences

==

pytorch_transformers/tokenization_bert.py
pytorch_transformers/tokenization_distilbert.py
pytorch_transformers/tokenization_roberta.py
pytorch_transformers/tokenization_utils.py
pytorch_transformers/tokenization_xlm.py
pytorch_transformers/tokenization_xlnet.py
==================
b94f73bab;thomwolf;2019-09-24 15:06:51 +0200;distilbert docstring

==

pytorch_transformers/modeling_tf_bert.py
pytorch_transformers/modeling_tf_distilbert.py
==================
9678c4941;thomwolf;2019-09-24 14:57:05 +0200;docstrings for bert

==

pytorch_transformers/modeling_tf_bert.py
==================
f3d1511b5;thomwolf;2019-09-24 14:42:09 +0200;fix imports

==

pytorch_transformers/modeling_tf_auto.py
==================
dd2d90f34;thomwolf;2019-09-24 14:39:41 +0200;update automodels

==

pytorch_transformers/modeling_tf_auto.py
==================
ee261439a;thomwolf;2019-09-24 14:30:28 +0200;add save_pretrained

==

pytorch_transformers/modeling_tf_utils.py
==================
29bb3e4eb;thomwolf;2019-09-24 14:23:46 +0200;double loading ok

==

pytorch_transformers/modeling_tf_pytorch_utils.py
==================
f5397ffc3;thomwolf;2019-09-24 14:03:58 +0200;update loading logics

==

pytorch_transformers/__init__.py
pytorch_transformers/file_utils.py
pytorch_transformers/modeling_tf_utils.py
pytorch_transformers/modeling_utils.py
==================
271f21362;thomwolf;2019-09-24 13:51:28 +0200;updating to load tf model in pt - fixing headmasking test

==

pytorch_transformers/modeling_tf_pytorch_utils.py
pytorch_transformers/modeling_tf_utils.py
pytorch_transformers/modeling_utils.py
pytorch_transformers/tests/modeling_common_test.py
==================
cf9c1cbb6;thomwolf;2019-09-24 13:32:47 +0200;fix tests chen only using tf

==

pytorch_transformers/tests/modeling_tf_common_test.py
==================
2167e366b;thomwolf;2019-09-24 13:27:45 +0200;update circleCi

==

.circleci/config.yml
==================
e9a103c17;thomwolf;2019-09-24 13:25:50 +0200;bidirectional conversion TF <=> PT - extended tests

==

.circleci/config.yml
pytorch_transformers/__init__.py
pytorch_transformers/modeling_tf_pytorch_utils.py
pytorch_transformers/modeling_xlm.py
pytorch_transformers/tests/modeling_tf_common_test.py
pytorch_transformers/tests/modeling_tf_xlm_test.py
pytorch_transformers/tests/modeling_xlm_test.py
==================
c832f43a4;LysandreJik;2019-09-24 07:21:38 -0400;`output_token_type` -> `token_type_ids`

==

examples/utils_glue.py
pytorch_transformers/tests/tokenization_tests_commons.py
pytorch_transformers/tokenization_utils.py
==================
3927d7756;LysandreJik;2019-09-24 07:15:11 -0400;Updated the GLUE pre-processing method

==

examples/utils_glue.py
==================
0ea82b246;LysandreJik;2019-09-24 07:10:09 -0400;Updated tests

==

pytorch_transformers/tests/tokenization_tests_commons.py
pytorch_transformers/tokenization_utils.py
==================
9d44236f7;LysandreJik;2019-09-24 07:03:24 -0400;Updated DistilBERT

==

examples/utils_glue.py
pytorch_transformers/tests/tokenization_tests_commons.py
pytorch_transformers/tokenization_utils.py
==================
a7e01a248;thomwolf;2019-09-24 10:58:52 +0200;converting distilled/fine-tuned models

==

pytorch_transformers/__init__.py
pytorch_transformers/convert_pytorch_checkpoint_to_tf2.py
pytorch_transformers/modeling_tf_distilbert.py
pytorch_transformers/modeling_tf_pytorch_utils.py
==================
8ba44ced9;thomwolf;2019-09-24 09:48:23 +0200;fix roberta conversion script

==

pytorch_transformers/__init__.py
pytorch_transformers/convert_pytorch_checkpoint_to_tf2.py
==================
2b11fa517;thomwolf;2019-09-23 22:35:45 +0200;update __init__ and conversion script

==

pytorch_transformers/__init__.py
pytorch_transformers/convert_pytorch_checkpoint_to_tf2.py
==================
6448396d5;thomwolf;2019-09-23 22:27:13 +0200;fix roberta test

==

pytorch_transformers/modeling_tf_roberta.py
==================
1e47dee24;thomwolf;2019-09-23 22:08:10 +0200;Merge branch 'tf2' of https://github.com/huggingface/pytorch-transformers into tf2

==
==================
c9591f6fa;thomwolf;2019-09-23 22:08:08 +0200;updated models input format + tests

==

pytorch_transformers/modeling_tf_bert.py
pytorch_transformers/modeling_tf_distilbert.py
pytorch_transformers/modeling_tf_gpt2.py
pytorch_transformers/modeling_tf_openai.py
pytorch_transformers/modeling_tf_roberta.py
pytorch_transformers/modeling_tf_transfo_xl.py
pytorch_transformers/modeling_tf_xlm.py
pytorch_transformers/modeling_tf_xlnet.py
pytorch_transformers/tests/modeling_tf_bert_test.py
pytorch_transformers/tests/modeling_tf_common_test.py
==================
798da627e;Julien Chaumond;2019-09-23 12:06:10 -0400;Fix TFBert tests in Python 3.5

==

pytorch_transformers/tests/modeling_tf_auto_test.py
pytorch_transformers/tests/modeling_tf_bert_test.py
==================
c014d1f0c;thomwolf;2019-09-23 16:39:57 +0200;fix the skipping

==

pytorch_transformers/tests/modeling_tf_auto_test.py
pytorch_transformers/tests/modeling_tf_bert_test.py
==================
0b22e47a4;thomwolf;2019-09-23 16:38:03 +0200;skipping pretrained TF model tests for now

==

pytorch_transformers/tests/modeling_tf_bert_test.py
==================
830d212be;thomwolf;2019-09-23 16:26:06 +0200;test circleCI h5py version

==

pytorch_transformers/tests/modeling_tf_auto_test.py
==================
7c0f2d0a6;Thomas Wolf;2019-09-23 14:54:55 +0100;Merge pull request #1294 from sshleifer/delete-n-special-doc
Delete n_special reference in docstring
==
==================
a31e591d2;thomwolf;2019-09-23 15:54:10 +0200;fix XLM tests

==

pytorch_transformers/modeling_xlm.py
pytorch_transformers/tests/modeling_xlm_test.py
==================
447de34dd;thomwolf;2019-09-23 15:38:29 +0200;tests for distilbert and roberta

==

pytorch_transformers/configuration_distilbert.py
pytorch_transformers/modeling_tf_distilbert.py
pytorch_transformers/tests/modeling_tf_distilbert_test.py
==================
98dd19b96;Santiago Castro;2019-09-22 20:31:36 -0400;Remove unnecessary use of FusedLayerNorm

==

pytorch_transformers/modeling_bert.py
==================
4b543c300;Lorenzo Ampil;2019-09-22 21:38:38 +0800;Add option to use a 'stop token' which will be used to truncate the output text to everything till right before the 'stop token'

==

examples/run_generation.py
==================
68a3e0223;thomwolf;2019-09-20 23:14:51 +0200;roberta and distilbert

==

pytorch_transformers/modeling_tf_bert.py
pytorch_transformers/modeling_tf_distilbert.py
pytorch_transformers/modeling_tf_roberta.py
pytorch_transformers/tests/modeling_tf_roberta_test.py
==================
a2d4950f5;Maxpa1n;2019-09-20 10:08:39 +0800;fix annotation

==

pytorch_transformers/tokenization_xlnet.py
==================
9f995b99d;VictorSanh;2019-09-19 21:36:06 +0000;minor fixes

==

examples/distillation/distiller.py
examples/distillation/train.py
==================
3fe5c8e8a;VictorSanh;2019-09-19 19:34:22 +0000;update bert-base-uncased rslts

==

examples/README.md
==================
354944e60;VictorSanh;2019-09-19 19:25:21 +0000;[distillation] big update w/ new weights

==

examples/distillation/README.md
examples/distillation/dataset.py
examples/distillation/distiller.py
examples/distillation/requirements.txt
examples/distillation/scripts/binarized_data.py
examples/distillation/scripts/extract_for_distil.py
examples/distillation/train.py
==================
2e6797cc7;danai-antoniou;2019-09-19 15:40:42 +0100;Added valuerror for duplicate added tokens

==

pytorch_transformers/tokenization_utils.py
==================
ab984a8b7;LysandreJik;2019-09-19 15:01:33 +0200;Python 2 compatibility

==

pytorch_transformers/tokenization_utils.py
==================
3df208c93;LysandreJik;2019-09-19 14:47:52 +0200;Tokenizer accepts token list as well as string

==

pytorch_transformers/tests/tokenization_tests_commons.py
pytorch_transformers/tokenization_utils.py
==================
66ea76b8a;LysandreJik;2019-09-19 13:50:51 +0200;prepare_for_model and prepare_pair_for_model methods. Added an option to select which sequence will be truncated.

==

pytorch_transformers/tests/tokenization_tests_commons.py
pytorch_transformers/tokenization_utils.py
==================
60414f31a;LysandreJik;2019-09-19 10:53:45 +0200;GLUE updated with new methods

==

examples/utils_glue.py
==================
baa74326a;LysandreJik;2019-09-19 10:42:32 +0200;Stride + tests + small fixes

==

pytorch_transformers/tests/tokenization_tests_commons.py
pytorch_transformers/tokenization_distilbert.py
pytorch_transformers/tokenization_utils.py
==================
c10c7d59e;LysandreJik;2019-09-19 10:13:10 +0200;Mask computing in standalone method. Tests.

==

pytorch_transformers/tests/tokenization_tests_commons.py
pytorch_transformers/tokenization_bert.py
pytorch_transformers/tokenization_distilbert.py
pytorch_transformers/tokenization_roberta.py
pytorch_transformers/tokenization_utils.py
pytorch_transformers/tokenization_xlm.py
pytorch_transformers/tokenization_xlnet.py
==================
bf503158c;LysandreJik;2019-09-19 09:55:36 +0200;Sentence -> Sequence. Removed output_mask from the special token addition methods.

==

examples/run_lm_finetuning.py
pytorch_transformers/tests/tokenization_bert_test.py
pytorch_transformers/tests/tokenization_distilbert_test.py
pytorch_transformers/tests/tokenization_roberta_test.py
pytorch_transformers/tests/tokenization_tests_commons.py
pytorch_transformers/tests/tokenization_xlm_test.py
pytorch_transformers/tests/tokenization_xlnet_test.py
pytorch_transformers/tokenization_bert.py
pytorch_transformers/tokenization_distilbert.py
pytorch_transformers/tokenization_roberta.py
pytorch_transformers/tokenization_utils.py
pytorch_transformers/tokenization_xlm.py
pytorch_transformers/tokenization_xlnet.py
==================
8cba05726;LysandreJik;2019-09-19 09:42:13 +0200;Doc + remove artefacts

==

pytorch_transformers/tokenization_utils.py
==================
6393261e4;LysandreJik;2019-09-19 09:30:09 +0200;encode + encode_plus tests modified

==

pytorch_transformers/tests/tokenization_tests_commons.py
==================
dcc9bb325;LysandreJik;2019-09-19 09:29:48 +0200;Modified encode to return only lists. Added a more complete encode_plus method

==

pytorch_transformers/tokenization_utils.py
==================
af23b626c;LysandreJik;2019-09-11 18:20:24 +0200;Max encoding length + corresponding tests

==

pytorch_transformers/tests/tokenization_tests_commons.py
pytorch_transformers/tokenization_utils.py
==================
c4d4f3ec8;LysandreJik;2019-09-11 11:22:04 +0200;Updated DistilBERT test to reflect the sequence encoding

==

pytorch_transformers/tests/tokenization_distilbert_test.py
==================
d572d7027;LysandreJik;2019-09-11 11:20:07 +0200;Number of added tokens calculator

==

pytorch_transformers/tests/tokenization_tests_commons.py
pytorch_transformers/tokenization_utils.py
==================
de8e14b6c;LysandreJik;2019-09-11 10:21:18 +0200;Added DistilBERT to run_squad script

==

examples/run_squad.py
==================
88368c2a1;LysandreJik;2019-09-06 18:05:56 -0400;Added DistilBERT to `run_lm_finetuning`

==

examples/run_lm_finetuning.py
==================
2d8ec5a68;LysandreJik;2019-09-06 17:51:55 -0400;Changed warning to be more explicit
Co-authored by: julien_c <chaumond@gmail.com>

==

pytorch_transformers/tokenization_utils.py
==================
75635072e;LysandreJik;2019-09-06 17:23:47 -0400;Updated GLUE script to add DistilBERT. Cleaned up unused args in the utils file.

==

examples/run_glue.py
examples/utils_glue.py
==================
92a9976e9;LysandreJik;2019-09-06 17:19:31 -0400;Distilbert sequence builder w/ mask

==

pytorch_transformers/tokenization_distilbert.py
==================
59057abe5;LysandreJik;2019-09-02 22:03:39 -0400;typo

==

examples/utils_glue.py
==================
bac332fec;LysandreJik;2019-09-02 21:46:11 -0400;Updated the GLUE data processor. Corrections to RoBERTa and XLNet.

==

examples/utils_glue.py
pytorch_transformers/tokenization_roberta.py
pytorch_transformers/tokenization_xlnet.py
==================
c3df2136e;LysandreJik;2019-09-02 17:47:16 -0400;Added binary masking tests

==

pytorch_transformers/tests/tokenization_tests_commons.py
pytorch_transformers/tokenization_utils.py
==================
e391d4735;LysandreJik;2019-09-02 16:42:32 -0400;Tokenizers' encode function can output binary masks

==

pytorch_transformers/tokenization_bert.py
pytorch_transformers/tokenization_roberta.py
pytorch_transformers/tokenization_utils.py
pytorch_transformers/tokenization_xlm.py
pytorch_transformers/tokenization_xlnet.py
==================
119610b5c;sshleifer;2019-09-19 01:35:01 -0700;Merge branch 'master' into delete-n-special-doc

==
==================
08e4ad5ee;sshleifer;2019-09-18 16:35:01 -0700;Remove documentation for unused kwarg

==

pytorch_transformers/configuration_openai.py
==================
f0340eccf;Erik Chan;2019-09-18 13:42:11 -0700;Typo
Typo
==

examples/run_lm_finetuning.py
==================
0d1dad6d5;Thomas Wolf;2019-09-18 21:42:51 +0200;Merge pull request #1004 from erenup/master
Refactoring old run_swag.py
==
==================
8960988f3;erenup;2019-09-19 01:10:05 +0800;fixed to find best dev acc

==

examples/run_multiple_choice.py
==================
b57bfb5fa;erenup;2019-09-18 21:45:04 +0800;Merge pull request #3 from erenup/run_multiple_choice_merge
Run multiple choice merge
==
==================
46ffc2832;erenup;2019-09-18 21:43:46 +0800;Merge branch 'master' into run_multiple_choice_merge # Please enter a commit message to explain why this merge is necessary, # especially if it merges an updated upstream into a topic branch. # # Lines starting with '#' will be ignored, and an empty message aborts # the commit.

==
==================
ec94f4e0f;Simon Layton;2019-09-18 09:30:58 -0400;Fix fp16 masking in PoolerEndLogits
Necessary to run xlnet (at least in squad) with `--fp16 --fp16_opt_level="O2"`, otherwise loss is immediately `NaN` and fine-tuning cannot proceed.
==

pytorch_transformers/modeling_utils.py
==================
15143fbad;erenup;2019-09-18 21:18:46 +0800;move run_multiple_choice.py and utils_multiple_choice.py to examples

==

examples/run_multiple_choice.py
examples/utils_multiple_choice.py
==================
3cd628975;erenup;2019-09-18 21:16:59 +0800;Merge remote-tracking branch 'huggingface/master' into run_multiple_choice_merge
# Conflicts:
#	examples/contrib/run_swag.py

==
==================
36362cf08;erenup;2019-09-18 21:13:40 +0800;move schedule.step after optimizer.step

==

examples/single_model_scripts/run_multiple_choice.py
==================
3a527fa82;thomwolf;2019-09-18 14:15:48 +0200;OpenAI GPT tests ok

==

pytorch_transformers/configuration_xlm.py
pytorch_transformers/configuration_xlnet.py
pytorch_transformers/modeling_tf_gpt2.py
pytorch_transformers/modeling_tf_openai.py
pytorch_transformers/tests/modeling_tf_openai_gpt_test.py
==================
556442afb;thomwolf;2019-09-18 14:12:41 +0200;hot fix

==

pytorch_transformers/modeling_tf_xlm.py
==================
160b5d608;thomwolf;2019-09-18 14:10:20 +0200;fix xlm lang_embeddings loading

==

pytorch_transformers/modeling_tf_xlm.py
==================
26497d119;thomwolf;2019-09-18 12:17:21 +0200;fix tests

==

pytorch_transformers/tests/modeling_tf_common_test.py
==================
6a083fd44;thomwolf;2019-09-18 12:11:32 +0200;update pt-tf conversion script

==

pytorch_transformers/convert_pytorch_checkpoint_to_tf2.py
pytorch_transformers/modeling_tf_pytorch_utils.py
==================
f6969cc12;thomwolf;2019-09-18 11:12:02 +0200;upgrade max model difference to 2e-2 (for transfo-xl adaptive softmax + inputs)

==

pytorch_transformers/convert_pytorch_checkpoint_to_tf2.py
==================
e768f2322;thomwolf;2019-09-18 10:07:47 +0200;update run_openai_gpt to fix #1264

==

examples/contrib/run_openai_gpt.py
==================
833499391;thomwolf;2019-09-18 10:01:27 +0200;clean up examples - updated to new keyword inputs - #1246

==

examples/contrib/README.md
examples/contrib/run_openai_gpt.py
examples/contrib/run_swag.py
examples/contrib/run_transfo_xl.py
examples/lm_finetuning/README.md
examples/lm_finetuning/finetune_on_pregenerated.py
examples/lm_finetuning/pregenerate_training_data.py
examples/lm_finetuning/simple_lm_finetuning.py
==================
62760baf4;Julien Chaumond;2019-09-17 18:29:15 -0400;tiny fixes

==

README.md
pytorch_transformers/modeling_bert.py
pytorch_transformers/modeling_distilbert.py
pytorch_transformers/modeling_gpt2.py
pytorch_transformers/modeling_openai.py
pytorch_transformers/modeling_roberta.py
pytorch_transformers/modeling_transfo_xl.py
pytorch_transformers/modeling_xlm.py
pytorch_transformers/modeling_xlnet.py
==================
45de034bf;thomwolf;2019-09-17 10:25:06 +0200;fix #1223

==

pytorch_transformers/modeling_xlnet.py
==================
5a81e79e2;erenup;2019-09-16 22:39:54 +0800;Merge pull request #2 from erenup/run_multiple_choice_add_doc
Run multiple choice add doc
==
==================
5882c442e;erenup;2019-09-16 22:38:08 +0800;add example usage

==

examples/README.md
==================
a9debaca3;erenup;2019-09-16 19:55:24 +0800;fixed init_weight

==

pytorch_transformers/modeling_roberta.py
pytorch_transformers/modeling_xlnet.py
==================
c88f05163;thomwolf;2019-09-16 13:42:20 +0200;fix typo in XLM models

==

docs/source/pretrained_models.rst
==================
982f181aa;erenup;2019-09-16 19:12:00 +0800;Merge remote-tracking branch 'origin/master' into run_multiple_choice_add_doc

==
==================
84b9d1c42;erenup;2019-09-16 19:06:12 +0800;Merge remote-tracking branch 'huggingface/master'
# Conflicts:
#	pytorch_transformers/__init__.py

==
==================
603b470a3;erenup;2019-09-16 18:53:37 +0800;add warnning info

==

examples/single_model_scripts/utils_multiple_choice.py
==================
4812a5a76;erenup;2019-09-16 11:50:18 +0800;add doc string

==

examples/single_model_scripts/run_multiple_choice.py
examples/single_model_scripts/utils_multiple_choice.py
pytorch_transformers/modeling_roberta.py
pytorch_transformers/modeling_xlnet.py
==================
4b956b2a6;thomwolf;2019-09-13 17:09:20 +0200;add layer_norm_epsilon configuration for transformer xl

==

pytorch_transformers/configuration_transfo_xl.py
pytorch_transformers/convert_pytorch_checkpoint_to_tf2.py
pytorch_transformers/modeling_tf_pytorch_utils.py
pytorch_transformers/modeling_tf_transfo_xl.py
pytorch_transformers/modeling_transfo_xl.py
==================
b97af8cce;thomwolf;2019-09-13 16:43:49 +0200;skip finetuned checkpoints

==

pytorch_transformers/__init__.py
pytorch_transformers/convert_pytorch_checkpoint_to_tf2.py
pytorch_transformers/modeling_tf_transfo_xl_utilities.py
==================
65c49bb27;thomwolf;2019-09-13 15:50:51 +0200;adding TF 2.0 adaptive softmax with logits + loss outputs

==

pytorch_transformers/modeling_tf_bert.py
pytorch_transformers/modeling_tf_transfo_xl.py
pytorch_transformers/modeling_tf_transfo_xl_utilities.py
pytorch_transformers/modeling_tf_xlm.py
pytorch_transformers/modeling_transfo_xl.py
pytorch_transformers/tests/modeling_tf_bert_test.py
pytorch_transformers/tests/modeling_tf_transfo_xl_test.py
==================
39c38b2ea;thomwolf;2019-09-12 16:47:11 +0200;fix

==

pytorch_transformers/modeling_tf_pytorch_utils.py
==================
dcddf498c;thomwolf;2019-09-12 16:46:32 +0200;fix bert layernorm

==

pytorch_transformers/modeling_tf_pytorch_utils.py
==================
d3a3a0353;thomwolf;2019-09-12 16:42:52 +0200;clean up cache after conversion

==

pytorch_transformers/convert_pytorch_checkpoint_to_tf2.py
==================
a84adddd1;thomwolf;2019-09-12 13:14:07 +0200;convert all models

==

pytorch_transformers/convert_pytorch_checkpoint_to_tf2.py
pytorch_transformers/modeling_tf_transfo_xl.py
==================
32e1332ac;VictorSanh;2019-09-11 14:19:07 +0000;[distil] fix once for all general logger for scripts

==

examples/distillation/scripts/binarized_data.py
examples/distillation/scripts/token_counts.py
==================
b62abe87c;Thomas Wolf;2019-09-11 15:53:28 +0200;Merge pull request #1249 from ziliwang/master
fixed: hard coding for max and min number will out of range in fp16, which will cause nan.
==
==================
969d3ae95;thomwolf;2019-09-11 15:47:33 +0200;XLMWithLMHead fixed - standardize conversion

==

pytorch_transformers/modeling_tf_bert.py
pytorch_transformers/modeling_tf_gpt2.py
pytorch_transformers/modeling_tf_pytorch_utils.py
pytorch_transformers/modeling_tf_xlm.py
pytorch_transformers/modeling_tf_xlnet.py
pytorch_transformers/modeling_xlm.py
==================
646711e1e;thomwolf;2019-09-11 15:34:17 +0200;standardize scopes names - add conversion methods

==

pytorch_transformers/convert_pytorch_checkpoint_to_tf2.py
pytorch_transformers/convert_xlm_original_pytorch_checkpoint_to_pytorch.py
pytorch_transformers/modeling_tf_bert.py
pytorch_transformers/modeling_tf_gpt2.py
pytorch_transformers/modeling_tf_pytorch_utils.py
pytorch_transformers/modeling_tf_xlm.py
pytorch_transformers/modeling_tf_xlnet.py
==================
4356f791a;thomwolf;2019-09-11 11:49:54 +0200;XLM passing tests

==

pytorch_transformers/__init__.py
pytorch_transformers/modeling_tf_xlm.py
pytorch_transformers/modeling_tf_xlnet.py
pytorch_transformers/modeling_xlm.py
pytorch_transformers/tests/modeling_tf_xlm_test.py
pytorch_transformers/tests/modeling_xlm_test.py
==================
11ac4b955;LysandreJik;2019-09-11 10:13:44 +0200;[CI] Symbolic link for documentation

==

.circleci/config.yml
==================
8bdee1cb7;Zili Wang;2019-09-11 15:41:53 +0800;fixed: hard coding for max and min number will out of range in fp16, which will cause nan.

==

pytorch_transformers/modeling_transfo_xl.py
pytorch_transformers/modeling_utils.py
==================
7424b2848;ziliwang;2019-09-11 11:02:23 +0800;Merge pull request #1 from huggingface/master
merege from original repo
==
==================
364920e21;VictorSanh;2019-09-10 21:45:01 +0000;fix small bug/typo

==

examples/distillation/dataset.py
==================
23c23f539;Thomas Wolf;2019-09-10 22:16:45 +0200;Merge pull request #1229 from SKRohit/master
changes in evaluate function in run_lm_finetuning.py
==
==================
99a54ac51;Thomas Wolf;2019-09-10 22:15:47 +0200;Merge pull request #1233 from searchivarius/master
Fix to prevent crashing on assert len(tokens_b)>=1
==
==================
439b37b47;Thomas Wolf;2019-09-10 22:14:18 +0200;Merge pull request #1241 from mattolson93/patch-1
Fixing typo in gpt2 for doc site's class link
==
==================
f2cf6ce4a;mattolson93;2019-09-10 09:12:01 -0700;Fixing typo in gpt2 for doc site's class link

==

pytorch_transformers/modeling_gpt2.py
==================
465870c33;thomwolf;2019-09-10 16:44:41 +0200;Xlnet working - also added simple question answering model for XLNet

==

pytorch_transformers/__init__.py
pytorch_transformers/configuration_utils.py
pytorch_transformers/configuration_xlnet.py
pytorch_transformers/convert_pytorch_checkpoint_to_tf2.py
pytorch_transformers/modeling_tf_bert.py
pytorch_transformers/modeling_tf_gpt2.py
pytorch_transformers/modeling_tf_utils.py
pytorch_transformers/modeling_tf_xlnet.py
pytorch_transformers/modeling_xlnet.py
pytorch_transformers/tests/modeling_tf_xlnet_test.py
==================
16b636179;thomwolf;2019-09-10 12:39:27 +0200;xlnet paassing first test

==

pytorch_transformers/modeling_tf_bert.py
pytorch_transformers/modeling_tf_gpt2.py
pytorch_transformers/modeling_tf_xlnet.py
pytorch_transformers/tests/modeling_tf_xlnet_test.py
==================
32aabe8c3;thomwolf;2019-09-10 12:17:18 +0200;WIP XLNet

==

pytorch_transformers/__init__.py
pytorch_transformers/configuration_utils.py
pytorch_transformers/modeling_tf_gpt2.py
pytorch_transformers/modeling_tf_utils.py
pytorch_transformers/modeling_tf_xlnet.py
pytorch_transformers/tests/modeling_tf_common_test.py
pytorch_transformers/tests/modeling_tf_xlnet_test.py
==================
2c177a87e;Thomas Wolf;2019-09-10 11:55:27 +0200;Merge pull request #1228 from huggingface/head-masking-test
Trying to fix the head masking test
==
==================
f851fb55c;thomwolf;2019-09-10 09:24:08 +0200;fixing error message

==

pytorch_transformers/modeling_tf_bert.py
==================
eab980fd6;searchivarius;2019-09-09 19:58:08 -0400;Fix to prevent crashing on assert len(tokens_b)>=1

==

examples/lm_finetuning/pregenerate_training_data.py
==================
a95ced626;VictorSanh;2019-09-09 19:53:35 +0000;[Distillation] save last chkpt as `pytorch_model.bin`

==

examples/distillation/distiller.py
==================
50c6bc419;thomwolf;2019-09-09 17:46:01 +0200;fix tf bert model

==

pytorch_transformers/configuration_bert.py
pytorch_transformers/convert_pytorch_checkpoint_to_tf2.py
pytorch_transformers/modeling_bert.py
pytorch_transformers/modeling_tf_bert.py
pytorch_transformers/modeling_tf_gpt2.py
pytorch_transformers/modeling_utils.py
==================
4b082bd4d;Rohit Kumar Singh;2019-09-09 19:59:27 +0530;Merge pull request #1 from SKRohit/SKRohit-patch-1
changes in return statement of evaluate function
==
==================
e5df36397;Rohit Kumar Singh;2019-09-09 19:55:57 +0530;changes in return statement of evaluate function
changed `results` to `result` and removed `results` dict defined previously
==

examples/run_lm_finetuning.py
==================
0537139b2;thomwolf;2019-09-09 14:47:31 +0200;removing tf.function

==

pytorch_transformers/modeling_tf_bert.py
pytorch_transformers/modeling_tf_gpt2.py
pytorch_transformers/modeling_tf_utils.py
==================
84d346b68;Thomas Wolf;2019-09-09 15:42:51 +0300;Merge pull request #1195 from huggingface/reorder_arguments
[2.0] Reodering arguments for torch jit #1010 and future TF2.0 compatibility
==
==================
3f05de6dd;Thomas Wolf;2019-09-09 15:42:25 +0300;Merge branch 'master' into reorder_arguments

==
==================
33cb00f41;thomwolf;2019-09-09 14:29:24 +0200;add GPT2 to init - fix weights loading - remove tf.function

==

pytorch_transformers/__init__.py
pytorch_transformers/convert_pytorch_checkpoint_to_tf2.py
pytorch_transformers/modeling_tf_bert.py
pytorch_transformers/modeling_tf_gpt2.py
==================
78b2a53f1;thomwolf;2019-09-09 13:38:10 +0200;debug file download in tests error

==

pytorch_transformers/convert_pytorch_checkpoint_to_tf2.py
pytorch_transformers/modeling_tf_utils.py
==================
6b3438df2;thomwolf;2019-09-09 12:48:36 +0200;fixing GPT2 double head model and updating the torch version tests

==

pytorch_transformers/modeling_gpt2.py
pytorch_transformers/modeling_tf_gpt2.py
pytorch_transformers/modeling_tf_utils.py
pytorch_transformers/modeling_utils.py
pytorch_transformers/tests/modeling_gpt2_test.py
pytorch_transformers/tests/modeling_tf_gpt2_test.py
==================
e36003723;thomwolf;2019-09-09 11:08:49 +0200;Merge branch 'tf2' of https://github.com/huggingface/pytorch-transformers into tf2

==
==================
b7175a270;thomwolf;2019-09-09 11:04:03 +0200;fixed imports in tests and gpt2 config test

==

pytorch_transformers/__init__.py
pytorch_transformers/modeling_tf_gpt2.py
pytorch_transformers/modeling_tf_utils.py
pytorch_transformers/modeling_xlnet.py
pytorch_transformers/tests/modeling_auto_test.py
pytorch_transformers/tests/modeling_bert_test.py
pytorch_transformers/tests/modeling_common_test.py
pytorch_transformers/tests/modeling_distilbert_test.py
pytorch_transformers/tests/modeling_gpt2_test.py
pytorch_transformers/tests/modeling_openai_test.py
pytorch_transformers/tests/modeling_roberta_test.py
pytorch_transformers/tests/modeling_tf_auto_test.py
pytorch_transformers/tests/modeling_tf_bert_test.py
pytorch_transformers/tests/modeling_tf_common_test.py
pytorch_transformers/tests/modeling_tf_gpt2_test.py
pytorch_transformers/tests/modeling_transfo_xl_test.py
pytorch_transformers/tests/modeling_xlm_test.py
pytorch_transformers/tests/modeling_xlnet_test.py
pytorch_transformers/tests/optimization_test.py
pytorch_transformers/tests/tokenization_transfo_xl_test.py
==================
995e38b7a;Thomas Wolf;2019-09-09 10:26:36 +0300;Merge pull request #1214 from huggingface/new-examples
Better examples
==
==================
3401980fc;thomwolf;2019-09-09 10:22:12 +0300;fix #1208

==

pytorch_transformers/modeling_xlnet.py
==================
728637356;thomwolf;2019-09-09 10:18:55 +0300;WIP GPT2

==

pytorch_transformers/modeling_tf_bert.py
pytorch_transformers/modeling_tf_gpt2.py
pytorch_transformers/modeling_tf_utils.py
pytorch_transformers/tests/modeling_gpt2_test.py
pytorch_transformers/tests/modeling_tf_gpt2_test.py
==================
34f28b2a1;thomwolf;2019-09-08 15:01:12 +0300;WIP GPT2

==

pytorch_transformers/modeling_tf_bert.py
pytorch_transformers/modeling_tf_gpt2.py
pytorch_transformers/modeling_tf_utils.py
==================
ad88563bd;thomwolf;2019-09-06 23:38:51 +0300;WIP GPT-2

==

pytorch_transformers/modeling_tf_gpt2.py
pytorch_transformers/modeling_tf_utils.py
==================
64d83c7ae;thomwolf;2019-09-05 21:21:49 +0200;WIP

==

pytorch_transformers/modeling_tf_gpt2.py
pytorch_transformers/modeling_tf_utils.py
==================
01597e5b9;thomwolf;2019-09-05 12:21:08 +0200;add tf auto models + tests

==

pytorch_transformers/__init__.py
pytorch_transformers/modeling_tf_auto.py
pytorch_transformers/modeling_tf_utils.py
pytorch_transformers/tests/modeling_tf_auto_test.py
==================
f5c698b21;thomwolf;2019-09-05 12:02:14 +0200;add weights tying, attention and hidden states output tests

==

pytorch_transformers/modeling_tf_bert.py
pytorch_transformers/modeling_tf_utils.py
pytorch_transformers/tests/modeling_tf_common_test.py
==================
6dc4b6f34;thomwolf;2019-09-05 11:22:13 +0200;skip transfo-xl tokenizer tests with tf for now

==

pytorch_transformers/tests/tokenization_transfo_xl_test.py
==================
e30579f76;thomwolf;2019-09-05 11:20:56 +0200;no pytest version checking

==

.circleci/config.yml
==================
518307dfc;thomwolf;2019-09-05 11:18:55 +0200;test suite independent of framework

==

.circleci/config.yml
pytorch_transformers/__init__.py
pytorch_transformers/convert_pytorch_checkpoint_to_tf.py
pytorch_transformers/modeling_tf_bert.py
pytorch_transformers/tests/modeling_auto_test.py
pytorch_transformers/tests/modeling_bert_test.py
pytorch_transformers/tests/modeling_common_test.py
pytorch_transformers/tests/modeling_distilbert_test.py
pytorch_transformers/tests/modeling_gpt2_test.py
pytorch_transformers/tests/modeling_openai_test.py
pytorch_transformers/tests/modeling_roberta_test.py
pytorch_transformers/tests/modeling_tf_bert_test.py
pytorch_transformers/tests/modeling_tf_common_test.py
pytorch_transformers/tests/modeling_transfo_xl_test.py
pytorch_transformers/tests/modeling_xlm_test.py
pytorch_transformers/tests/modeling_xlnet_test.py
pytorch_transformers/tests/optimization_test.py
pytorch_transformers/tests/tokenization_auto_test.py
pytorch_transformers/tests/tokenization_transfo_xl_test.py
pytorch_transformers/tokenization_transfo_xl.py
==================
9d0a11a68;thomwolf;2019-09-05 10:23:04 +0200;update dependencies and circle-ci

==

.circleci/config.yml
requirements.txt
setup.py
==================
24a20483f;thomwolf;2019-09-05 03:13:26 +0200;update conversion script names

==

pytorch_transformers/__main__.py
pytorch_transformers/convert_gpt2_original_tf_checkpoint_to_pytorch.py
pytorch_transformers/convert_openai_original_tf_checkpoint_to_pytorch.py
pytorch_transformers/convert_roberta_original_pytorch_checkpoint_to_pytorch.py
pytorch_transformers/convert_transfo_xl_original_tf_checkpoint_to_pytorch.py
pytorch_transformers/convert_xlm_original_pytorch_checkpoint_to_pytorch.py
pytorch_transformers/convert_xlnet_original_tf_checkpoint_to_pytorch.py
==================
6f152572c;thomwolf;2019-09-05 03:10:11 +0200;add conversion script, rename conversion scripts

==

pytorch_transformers/convert_bert_original_tf_checkpoint_to_pytorch.py
pytorch_transformers/convert_bert_pytorch_checkpoint_to_original_tf.py
pytorch_transformers/convert_bert_pytorch_checkpoint_to_tf.py
==================
a4704b126;thomwolf;2019-09-05 03:06:09 +0200;skipping tf tests if tf is not installed

==

pytorch_transformers/tests/modeling_tf_bert_test.py
pytorch_transformers/tests/modeling_tf_common_test.py
==================
ad0ab9afe;thomwolf;2019-09-05 02:53:52 +0200;fix test when tf is not here

==

.circleci/config.yml
pytorch_transformers/__init__.py
pytorch_transformers/tests/modeling_tf_bert_test.py
pytorch_transformers/tests/modeling_tf_common_test.py
==================
59fe641b8;thomwolf;2019-09-05 02:34:09 +0200;also gathering file names in file_utils

==

pytorch_transformers/__init__.py
pytorch_transformers/configuration_utils.py
pytorch_transformers/file_utils.py
pytorch_transformers/modeling_utils.py
==================
d68a8fe46;thomwolf;2019-09-05 02:27:39 +0200;add tf bert files

==

pytorch_transformers/file_utils.py
pytorch_transformers/modeling_tf_bert.py
pytorch_transformers/modeling_tf_utils.py
pytorch_transformers/tests/modeling_tf_common_test.py
pytorch_transformers/tests/modeling_tf_test.py
==================
7ae642b72;thomwolf;2019-09-05 02:17:50 +0200;update conversion scripts

==

pytorch_transformers/__init__.py
pytorch_transformers/convert_gpt2_checkpoint_to_pytorch.py
pytorch_transformers/convert_openai_checkpoint_to_pytorch.py
pytorch_transformers/convert_pytorch_checkpoint_to_tf.py
pytorch_transformers/convert_roberta_checkpoint_to_pytorch.py
pytorch_transformers/convert_tf_checkpoint_to_pytorch.py
pytorch_transformers/convert_transfo_xl_checkpoint_to_pytorch.py
pytorch_transformers/convert_xlm_checkpoint_to_pytorch.py
pytorch_transformers/convert_xlnet_checkpoint_to_pytorch.py
==================
69bff8993;thomwolf;2019-09-05 00:41:24 +0200;clean ups

==

pytorch_transformers/tests/modeling_distilbert_test.py
pytorch_transformers/tests/modeling_transfo_xl_test.py
==================
1efb1f166;thomwolf;2019-09-05 00:26:57 +0200;split configuration and modeling files

==

pytorch_transformers/__init__.py
pytorch_transformers/configuration_auto.py
pytorch_transformers/configuration_bert.py
pytorch_transformers/configuration_distilbert.py
pytorch_transformers/configuration_gpt2.py
pytorch_transformers/configuration_openai.py
pytorch_transformers/configuration_roberta.py
pytorch_transformers/configuration_transfo_xl.py
pytorch_transformers/configuration_utils.py
pytorch_transformers/configuration_xlm.py
pytorch_transformers/configuration_xlnet.py
pytorch_transformers/file_utils.py
pytorch_transformers/modeling_auto.py
pytorch_transformers/modeling_bert.py
pytorch_transformers/modeling_distilbert.py
pytorch_transformers/modeling_gpt2.py
pytorch_transformers/modeling_openai.py
pytorch_transformers/modeling_roberta.py
pytorch_transformers/modeling_transfo_xl.py
pytorch_transformers/modeling_utils.py
pytorch_transformers/modeling_xlm.py
pytorch_transformers/modeling_xlnet.py
pytorch_transformers/tests/configuration_common_test.py
pytorch_transformers/tests/modeling_auto_test.py
pytorch_transformers/tests/modeling_bert_test.py
pytorch_transformers/tests/modeling_common_test.py
pytorch_transformers/tests/modeling_distilbert_test.py
pytorch_transformers/tests/modeling_gpt2_test.py
pytorch_transformers/tests/modeling_openai_test.py
pytorch_transformers/tests/modeling_roberta_test.py
pytorch_transformers/tests/modeling_transfo_xl_test.py
pytorch_transformers/tests/modeling_xlm_test.py
pytorch_transformers/tests/modeling_xlnet_test.py
==================
1eb125fb9;thomwolf;2019-09-04 22:47:38 +0200;be sure we have uint8

==

pytorch_transformers/modeling_transfo_xl.py
==================
3f91338be;LysandreJik;2019-09-06 17:48:06 -0400;Patched a few outdated parameters

==

examples/README.md
==================
f47f9a587;LysandreJik;2019-09-06 17:10:33 -0400;Updated outdated examples

==

examples/README.md
==================
ee027c89f;thomwolf;2019-09-06 23:40:05 +0300;fix #1165

==

hubconf.py
==================
e52737d5a;LysandreJik;2019-09-06 12:13:31 -0400;Updated docs README to feature the examples symlink

==

docs/README.md
==================
5e151f5e7;LysandreJik;2019-09-06 12:08:36 -0400;Table of contents

==

examples/README.md
==================
593c07043;LysandreJik;2019-09-06 12:00:12 -0400;Better examples

==

docs/requirements.txt
docs/source/conf.py
docs/source/examples.rst
examples/README.md
examples/run_lm_finetuning.py
==================
5ac8b6226;Thomas Wolf;2019-09-05 21:44:16 +0200;Merge pull request #1205 from maru0kun/patch-2
Fix typo
==
==================
5c6cac102;thomwolf;2019-09-05 21:31:29 +0200;adding test for common properties and cleaning up a bit base class

==

pytorch_transformers/tests/tokenization_tests_commons.py
pytorch_transformers/tokenization_utils.py
==================
ed717635f;Thomas Wolf;2019-09-05 21:16:58 +0200;Merge pull request #1201 from huggingface/configuration_refactoring
[2.0] - Split configuration and modeling files
==
==================
04b50cabf;VictorSanh;2019-09-05 18:49:28 +0000;gitignore

==

.gitignore
==================
dddd6b992;VictorSanh;2019-09-05 18:26:14 +0000;Update DistilBERT training code

==

examples/distillation/README.md
examples/distillation/distiller.py
examples/distillation/requirements.txt
==================
f9453d15e;Julien Chaumond;2019-09-05 12:35:22 -0400;Fix broken link

==

README.md
==================
f7ee2e5d2;Julien Chaumond;2019-09-03 10:29:41 -0400;[README] link to Write With Transformer

==

README.md
==================
d73794772;maru0kun;2019-09-05 19:24:57 +0900;Fix typo

==

pytorch_transformers/tokenization_utils.py
==================
705237b4e;thomwolf;2019-09-05 12:21:08 +0200;add tf auto models + tests

==

pytorch_transformers/__init__.py
pytorch_transformers/modeling_tf_auto.py
pytorch_transformers/modeling_tf_utils.py
pytorch_transformers/tests/modeling_tf_auto_test.py
==================
600a42329;thomwolf;2019-09-05 12:02:14 +0200;add weights tying, attention and hidden states output tests

==

pytorch_transformers/modeling_tf_bert.py
pytorch_transformers/modeling_tf_utils.py
pytorch_transformers/tests/modeling_tf_common_test.py
==================
04d2006f2;thomwolf;2019-09-05 11:22:13 +0200;skip transfo-xl tokenizer tests with tf for now

==

pytorch_transformers/tests/tokenization_transfo_xl_test.py
==================
7f6a0c0d6;thomwolf;2019-09-05 11:20:56 +0200;no pytest version checking

==

.circleci/config.yml
==================
7c0baf952;thomwolf;2019-09-05 11:18:55 +0200;test suite independent of framework

==

.circleci/config.yml
pytorch_transformers/__init__.py
pytorch_transformers/convert_pytorch_checkpoint_to_tf.py
pytorch_transformers/modeling_tf_bert.py
pytorch_transformers/tests/modeling_auto_test.py
pytorch_transformers/tests/modeling_bert_test.py
pytorch_transformers/tests/modeling_common_test.py
pytorch_transformers/tests/modeling_distilbert_test.py
pytorch_transformers/tests/modeling_gpt2_test.py
pytorch_transformers/tests/modeling_openai_test.py
pytorch_transformers/tests/modeling_roberta_test.py
pytorch_transformers/tests/modeling_tf_bert_test.py
pytorch_transformers/tests/modeling_tf_common_test.py
pytorch_transformers/tests/modeling_transfo_xl_test.py
pytorch_transformers/tests/modeling_xlm_test.py
pytorch_transformers/tests/modeling_xlnet_test.py
pytorch_transformers/tests/optimization_test.py
pytorch_transformers/tests/tokenization_auto_test.py
pytorch_transformers/tests/tokenization_transfo_xl_test.py
pytorch_transformers/tokenization_transfo_xl.py
==================
7775a3d2e;thomwolf;2019-09-05 10:23:04 +0200;update dependencies and circle-ci

==

.circleci/config.yml
requirements.txt
setup.py
==================
33dd59e97;thomwolf;2019-09-05 03:13:26 +0200;update conversion script names

==

pytorch_transformers/__main__.py
pytorch_transformers/convert_gpt2_original_tf_checkpoint_to_pytorch.py
pytorch_transformers/convert_openai_original_tf_checkpoint_to_pytorch.py
pytorch_transformers/convert_roberta_original_pytorch_checkpoint_to_pytorch.py
pytorch_transformers/convert_transfo_xl_original_tf_checkpoint_to_pytorch.py
pytorch_transformers/convert_xlm_original_pytorch_checkpoint_to_pytorch.py
pytorch_transformers/convert_xlnet_original_tf_checkpoint_to_pytorch.py
==================
5951d8602;thomwolf;2019-09-05 03:10:11 +0200;add conversion script, rename conversion scripts

==

pytorch_transformers/convert_bert_original_tf_checkpoint_to_pytorch.py
pytorch_transformers/convert_bert_pytorch_checkpoint_to_original_tf.py
pytorch_transformers/convert_bert_pytorch_checkpoint_to_tf.py
==================
aa4c8804f;thomwolf;2019-09-05 03:06:09 +0200;skipping tf tests if tf is not installed

==

pytorch_transformers/tests/modeling_tf_bert_test.py
pytorch_transformers/tests/modeling_tf_common_test.py
==================
134847db8;thomwolf;2019-09-05 02:53:52 +0200;fix test when tf is not here

==

.circleci/config.yml
pytorch_transformers/__init__.py
pytorch_transformers/tests/modeling_tf_bert_test.py
pytorch_transformers/tests/modeling_tf_common_test.py
==================
981f7f525;thomwolf;2019-09-05 02:34:52 +0200;Merge branch 'tf2' of https://github.com/huggingface/pytorch-transformers into tf2

==
==================
bffd17a43;thomwolf;2019-09-05 02:27:39 +0200;add tf bert files

==

pytorch_transformers/file_utils.py
pytorch_transformers/modeling_tf_bert.py
pytorch_transformers/modeling_tf_utils.py
pytorch_transformers/tests/modeling_tf_common_test.py
pytorch_transformers/tests/modeling_tf_test.py
==================
85df4f7cc;thomwolf;2019-09-05 02:34:09 +0200;also gathering file names in file_utils

==

pytorch_transformers/__init__.py
pytorch_transformers/configuration_utils.py
pytorch_transformers/file_utils.py
pytorch_transformers/modeling_utils.py
==================
11fae9e63;thomwolf;2019-09-05 02:27:39 +0200;add tf bert files

==

pytorch_transformers/file_utils.py
pytorch_transformers/modeling_tf_bert.py
pytorch_transformers/modeling_tf_utils.py
pytorch_transformers/tests/modeling_tf_common_test.py
pytorch_transformers/tests/modeling_tf_test.py
==================
121f88cae;thomwolf;2019-09-05 02:17:50 +0200;update conversion scripts

==

pytorch_transformers/__init__.py
pytorch_transformers/convert_gpt2_checkpoint_to_pytorch.py
pytorch_transformers/convert_openai_checkpoint_to_pytorch.py
pytorch_transformers/convert_pytorch_checkpoint_to_tf.py
pytorch_transformers/convert_roberta_checkpoint_to_pytorch.py
pytorch_transformers/convert_tf_checkpoint_to_pytorch.py
pytorch_transformers/convert_transfo_xl_checkpoint_to_pytorch.py
pytorch_transformers/convert_xlm_checkpoint_to_pytorch.py
pytorch_transformers/convert_xlnet_checkpoint_to_pytorch.py
==================
d77abd4d0;thomwolf;2019-09-05 00:41:24 +0200;clean ups

==

pytorch_transformers/tests/modeling_distilbert_test.py
pytorch_transformers/tests/modeling_transfo_xl_test.py
==================
2a667b1eb;thomwolf;2019-09-05 00:26:57 +0200;split configuration and modeling files

==

pytorch_transformers/__init__.py
pytorch_transformers/configuration_auto.py
pytorch_transformers/configuration_bert.py
pytorch_transformers/configuration_distilbert.py
pytorch_transformers/configuration_gpt2.py
pytorch_transformers/configuration_openai.py
pytorch_transformers/configuration_roberta.py
pytorch_transformers/configuration_transfo_xl.py
pytorch_transformers/configuration_utils.py
pytorch_transformers/configuration_xlm.py
pytorch_transformers/configuration_xlnet.py
pytorch_transformers/file_utils.py
pytorch_transformers/modeling_auto.py
pytorch_transformers/modeling_bert.py
pytorch_transformers/modeling_distilbert.py
pytorch_transformers/modeling_gpt2.py
pytorch_transformers/modeling_openai.py
pytorch_transformers/modeling_roberta.py
pytorch_transformers/modeling_transfo_xl.py
pytorch_transformers/modeling_utils.py
pytorch_transformers/modeling_xlm.py
pytorch_transformers/modeling_xlnet.py
pytorch_transformers/tests/configuration_common_test.py
pytorch_transformers/tests/modeling_auto_test.py
pytorch_transformers/tests/modeling_bert_test.py
pytorch_transformers/tests/modeling_common_test.py
pytorch_transformers/tests/modeling_distilbert_test.py
pytorch_transformers/tests/modeling_gpt2_test.py
pytorch_transformers/tests/modeling_openai_test.py
pytorch_transformers/tests/modeling_roberta_test.py
pytorch_transformers/tests/modeling_transfo_xl_test.py
pytorch_transformers/tests/modeling_xlm_test.py
pytorch_transformers/tests/modeling_xlnet_test.py
==================
0be6a2a62;thomwolf;2019-09-04 22:47:38 +0200;be sure we have uint8

==

pytorch_transformers/modeling_transfo_xl.py
==================
7fba47b7d;thomwolf;2019-09-04 22:29:17 +0200;WIP reordering

==

pytorch_transformers/modeling_gpt2.py
pytorch_transformers/modeling_openai.py
pytorch_transformers/tests/modeling_roberta_test.py
==================
e25cba78c;thomwolf;2019-09-04 12:43:18 +0200;WIP reodering arguments for torchscript and TF

==

pytorch_transformers/modeling_bert.py
pytorch_transformers/modeling_distilbert.py
pytorch_transformers/modeling_gpt2.py
pytorch_transformers/modeling_openai.py
pytorch_transformers/modeling_roberta.py
pytorch_transformers/modeling_transfo_xl.py
pytorch_transformers/modeling_xlm.py
pytorch_transformers/modeling_xlnet.py
pytorch_transformers/tests/modeling_bert_test.py
pytorch_transformers/tests/modeling_distilbert_test.py
==================
38b79b5a6;thomwolf;2019-09-04 22:36:30 +0200;Fixing this TransformerXL bool issue

==

pytorch_transformers/modeling_transfo_xl.py
==================
0b52642d3;LysandreJik;2019-09-04 11:03:32 -0400;1.2.0 in docs

==

docs/source/conf.py
==================
89fd3450a;thomwolf;2019-09-04 13:32:18 +0200;Release: 1.2.0

==

pytorch_transformers/__init__.py
setup.py
==================
9fd6e7ab9;Thomas Wolf;2019-09-04 12:50:49 +0200;Merge pull request #1190 from shijie-wu/xlm-tokenization
Fix reference of import in XLM tokenization
==
==================
a15562e17;Shijie Wu;2019-09-03 18:27:29 -0700;Fix reference of import when called for the second time

==

pytorch_transformers/tokenization_xlm.py
==================
0287d264e;Thomas Wolf;2019-09-02 23:14:04 +0200;Merge pull request #1162 from huggingface/xlnet-bias
XLNet bias fix on resize embeddings (cf #1124)
==
==================
7f522437b;LysandreJik;2019-09-02 13:40:25 -0400;Updated documentation for LM finetuning script

==

docs/source/examples.rst
docs/source/model_doc/distilbert.rst
==================
3fbf301bb;LysandreJik;2019-09-02 12:35:14 -0400;[CI] Updated resource size for python 3 tests

==

.circleci/config.yml
==================
2dcc5a162;Julien Chaumond;2019-09-02 12:27:11 -0400;[doc] Add blurb about large-scale model downloads
cc @n1t0 @lysandrejik @thomwolf

==

docs/source/installation.rst
==================
7b0c99add;Thomas Wolf;2019-09-02 09:01:16 +0200;Merge pull request #1174 from huggingface/fix_byte_level_added_tokens
Fix byte-level BPE decoding error when using added tokens
==
==================
31d3373bc;LysandreJik;2019-09-01 21:07:00 -0400;Appends space before special token

==

pytorch_transformers/tokenization_utils.py
==================
fede4ef45;thomwolf;2019-09-02 02:27:39 +0200;fixing #1133

==

pytorch_transformers/tests/tokenization_tests_commons.py
pytorch_transformers/tokenization_utils.py
==================
b6cd856b0;Thomas Wolf;2019-09-02 02:00:07 +0200;Merge pull request #1164 from stefan-it/master
distillation: fix ModuleNotFoundError error in token counts script
==
==================
ff7368eb6;Thomas Wolf;2019-09-01 09:42:15 +0200;Merge pull request #1077 from huggingface/pruning-save-and-load
Pruning changes so that deleted heads are kept on save/load
==
==================
6ae0bb529;LysandreJik;2019-08-31 14:46:31 -0400;XLM 100 different URLs

==

pytorch_transformers/modeling_xlm.py
pytorch_transformers/tokenization_xlm.py
==================
819b468f7;LysandreJik;2019-08-31 14:40:51 -0400;Fixed XLM model url

==

pytorch_transformers/modeling_xlm.py
==================
58b59a0c3;LysandreJik;2019-08-31 13:17:08 -0400;Random seed is accessible anywhere within the common tests

==

pytorch_transformers/tests/modeling_common_test.py
==================
a1c34bd28;Stefan Schweter;2019-08-31 12:21:38 +0200;distillation: fix ModuleNotFoundError error in token counts script

==

examples/distillation/scripts/token_counts.py
==================
ea86bef54;LysandreJik;2019-08-31 00:56:22 -0400;Check for None

==

pytorch_transformers/modeling_utils.py
==================
e0f867a9b;LysandreJik;2019-08-31 00:50:59 -0400;XLNet bias fix on resize embeddings (cf #1124)

==

pytorch_transformers/modeling_utils.py
==================
11600edc6;LysandreJik;2019-08-31 00:37:41 -0400;Rebase on master + DistilBERT head pruning patch

==

pytorch_transformers/modeling_distilbert.py
==================
b6992b7b4;LysandreJik;2019-08-31 00:33:11 -0400;Applied patch to OpenAI GPT, RoBERTa, TransfoL, XLM and XLNet

==

pytorch_transformers/modeling_openai.py
pytorch_transformers/modeling_roberta.py
pytorch_transformers/modeling_transfo_xl.py
pytorch_transformers/modeling_xlm.py
pytorch_transformers/modeling_xlnet.py
==================
bdb4409ed;thomwolf;2019-08-31 01:59:07 +0200;updated pruning logic with sets - Bert and GPT-2

==

pytorch_transformers/modeling_bert.py
pytorch_transformers/modeling_gpt2.py
pytorch_transformers/modeling_utils.py
==================
0c8e823b0;LysandreJik;2019-08-29 17:20:11 -0400;Added patch to remaining models

==

pytorch_transformers/modeling_bert.py
pytorch_transformers/modeling_openai.py
pytorch_transformers/modeling_xlm.py
==================
0cd283522;LysandreJik;2019-08-27 15:56:59 -0400;Attempt to fix head index

==

pytorch_transformers/modeling_gpt2.py
==================
c85b5db61;LysandreJik;2019-08-21 21:37:30 -0400;Conditional append/init + fixed warning

==

pytorch_transformers/modeling_utils.py
==================
5c2b94c82;LysandreJik;2019-08-21 21:24:48 -0400;Changed string so that Circle CI accepts the warning

==

pytorch_transformers/modeling_utils.py
==================
87747518e;LysandreJik;2019-08-21 21:20:39 -0400;Blocks deletion from already deleted heads. Necessary integration test.
Now raises a warning when a head to be deleted already has been deleted. An integration test verifying the total pipeline (-> from config -> save model -> load model -> additional head pruning) has been added.
==

pytorch_transformers/modeling_bert.py
pytorch_transformers/modeling_gpt2.py
pytorch_transformers/modeling_openai.py
pytorch_transformers/modeling_utils.py
pytorch_transformers/modeling_xlm.py
pytorch_transformers/tests/modeling_common_test.py
==================
719cb3738;LysandreJik;2019-08-21 20:12:06 -0400;Pruning for GPT and GPT-2

==

pytorch_transformers/modeling_gpt2.py
pytorch_transformers/modeling_openai.py
pytorch_transformers/tests/modeling_common_test.py
==================
fc1fbae45;LysandreJik;2019-08-21 18:57:30 -0400;XLM can be pruned

==

pytorch_transformers/modeling_xlm.py
pytorch_transformers/tests/modeling_common_test.py
==================
42e00cf9e;Lysandre;2019-08-19 22:43:02 -0400;Pruning saved to configuration first try

==

pytorch_transformers/modeling_bert.py
pytorch_transformers/modeling_utils.py
pytorch_transformers/tests/modeling_common_test.py
==================
d7a4c3252;LysandreJik;2019-08-31 00:08:56 -0400;Fixed filename

==

pytorch_transformers/tests/modeling_distilbert_test.py
==================
7f006cdd8;LysandreJik;2019-08-30 23:58:49 -0400;Set seed for head_masking test

==

pytorch_transformers/tests/modeling_common_test.py
==================
0fd0b674e;Julien Chaumond;2019-08-30 20:36:26 -0400;[ci] legible output [skip ci]

==

.circleci/config.yml
==================
b65a994f5;Julien Chaumond;2019-08-30 20:33:16 -0400;[ci] decrease parallelism to increase success prob

==

.circleci/config.yml
==================
1d438f15b;Julien Chaumond;2019-08-30 20:20:15 -0400;[XLNet] Use pytorch's layernorm like in BERT
See #1089

cc @thomwolf @lysandrejik

Also @dhpollack

==

pytorch_transformers/modeling_xlnet.py
==================
574c5b3a7;Julien Chaumond;2019-08-30 20:09:24 -0400;[RoBERTa] LayerNorm's eps is not a nn.Parameter so there's no point setting it on the model
Instead we correctly store it on the config

(regenerating the hosted config files)

cc @lysandrejik

==

pytorch_transformers/convert_roberta_checkpoint_to_pytorch.py
==================
09363f2a8;LysandreJik;2019-08-30 19:48:32 -0400;Fix documentation index

==

docs/source/index.rst
==================
51e980ce3;Thomas Wolf;2019-08-30 23:29:11 +0200;Merge pull request #1155 from anhnt170489/apex_fp16
Update apex fp16 implementation
==
==================
206c35e9a;Thomas Wolf;2019-08-30 23:23:08 +0200;Merge pull request #1154 from ziliwang/master
fix: hard coding for max number
==
==================
f3d18c71e;Thomas Wolf;2019-08-30 23:21:59 +0200;Merge pull request #1152 from epwalsh/fix-special-tokens
fix adding special tokens
==
==================
d483cd8e4;Thomas Wolf;2019-08-30 23:18:58 +0200;Merge pull request #1074 from huggingface/improved_testing
Shortcut to special tokens' ids - fix GPT2 & RoBERTa tokenizers - improved testing for GPT/GPT-2
==
==================
d2f21f08f;Thomas Wolf;2019-08-30 23:15:40 +0200;Merge pull request #1092 from shijie-wu/xlm-tokenization
Added cleaned configuration properties for tokenizer with serialization - improve tokenization of XLM
==
==================
12b9cc9e2;Thomas Wolf;2019-08-30 23:08:57 +0200;Merge pull request #1110 from huggingface/automodels
Torch.hub now based on AutoModels - Updating AutoModels with AutoModelWithLMHead, Sequence Classification and Question Answering
==
==================
bfe93a5a2;thomwolf;2019-08-30 22:43:26 +0200;fix distilbert in auto tokenizer

==

pytorch_transformers/tokenization_auto.py
==================
256086bc6;thomwolf;2019-08-30 22:34:23 +0200;clean up and simplify hubconf

==

hubconf.py
hubconfs/automodels_hubconf.py
hubconfs/bert_hubconf.py
hubconfs/gpt2_hubconf.py
hubconfs/gpt_hubconf.py
hubconfs/transformer_xl_hubconf.py
hubconfs/xlm_hubconf.py
hubconfs/xlnet_hubconf.1.py
pytorch_transformers/modeling_auto.py
==================
80aa87d9a;thomwolf;2019-08-30 22:24:23 +0200;fix distilbert tokenizer

==

pytorch_transformers/tokenization_auto.py
==================
455a4c842;thomwolf;2019-08-30 22:20:51 +0200;add distilbert tokenizer

==

pytorch_transformers/tokenization_auto.py
==================
7a1f174a9;thomwolf;2019-08-30 22:20:44 +0200;update names of torch.hub to simpler names - update docstring

==

hubconf.py
hubconfs/automodels_hubconf.py
==================
c665e0fcf;thomwolf;2019-08-30 21:53:36 +0200;Merge branch 'automodels' of https://github.com/huggingface/pytorch-transformers into automodels

==
==================
9b6e3b34d;LysandreJik;2019-08-30 14:09:02 -0400;Docstrings

==

hubconfs/automodels_hubconf.py
==================
dec8f4d6f;LysandreJik;2019-08-30 13:52:18 -0400;Added DistilBERT models to all other AutoModels.

==

pytorch_transformers/modeling_auto.py
==================
bc29aa67a;LysandreJik;2019-08-30 11:13:51 -0400;HubConf configuration

==

hubconf.py
hubconfs/automodels_hubconf.py
==================
f35f61228;thomwolf;2019-08-27 14:42:03 +0200;updating docstring for AutoModel

==

pytorch_transformers/modeling_auto.py
pytorch_transformers/modeling_utils.py
pytorch_transformers/tokenization_auto.py
==================
7ca965385;LysandreJik;2019-08-26 16:08:43 -0400;Pytorch Hub & AutoModels

==

hubconf.py
==================
25e838943;LysandreJik;2019-08-26 16:02:23 -0400;Tests for added AutoModels

==

pytorch_transformers/tests/modeling_auto_test.py
==================
dc43215c0;LysandreJik;2019-08-26 15:44:30 -0400;Added multiple AutoModel classes: AutoModelWithLMHead, AutoModelForQuestionAnswering and AutoModelForSequenceClassification

==

pytorch_transformers/__init__.py
pytorch_transformers/modeling_auto.py
==================
282c276e0;VictorSanh;2019-08-30 12:02:29 -0400;typos + file name coherence in distillation README

==

examples/distillation/README.md
==================
803c1cc4e;VictorSanh;2019-08-30 12:01:27 -0400;fix relative import bug cf Issue #1140

==

examples/distillation/scripts/binarized_data.py
==================
7044ed6b0;thomwolf;2019-08-30 17:36:11 +0200;fix tokenizers serialization

==

pytorch_transformers/tests/tokenization_dilbert_test.py
pytorch_transformers/tests/tokenization_tests_commons.py
pytorch_transformers/tokenization_transfo_xl.py
pytorch_transformers/tokenization_xlnet.py
==================
cd65c41a8;Thomas Wolf;2019-08-30 17:15:16 +0200;Merge branch 'master' into xlm-tokenization

==
==================
69da972ac;thomwolf;2019-08-30 17:09:36 +0200;added test and debug tokenizer configuration serialization

==

pytorch_transformers/tests/tokenization_bert_test.py
pytorch_transformers/tests/tokenization_gpt2_test.py
pytorch_transformers/tests/tokenization_openai_test.py
pytorch_transformers/tests/tokenization_roberta_test.py
pytorch_transformers/tests/tokenization_tests_commons.py
pytorch_transformers/tests/tokenization_transfo_xl_test.py
pytorch_transformers/tests/tokenization_xlm_test.py
pytorch_transformers/tests/tokenization_xlnet_test.py
pytorch_transformers/tokenization_utils.py
==================
88111de07;thomwolf;2019-08-30 16:55:48 +0200;saving and reloading tokenizer configurations

==

pytorch_transformers/tokenization_utils.py
==================
b66e9b443;Thomas Wolf;2019-08-30 16:30:33 +0200;Merge pull request #1158 from rabeehk/master
regarding #1026 pull request 
==
==================
0a2fecdf9;Thomas Wolf;2019-08-30 16:30:08 +0200;Merge branch 'master' into master

==
==================
3871b8a10;thomwolf;2019-08-30 16:28:42 +0200;adding xlm 17 and 100 models and config on aws

==

pytorch_transformers/modeling_xlm.py
==================
8678ff8df;thomwolf;2019-08-30 16:26:04 +0200;adding 17 and 100 xlm models

==

pytorch_transformers/tokenization_xlm.py
==================
e0caab0cf;LysandreJik;2019-08-30 10:09:17 -0400;fix link

==

docs/source/index.rst
==================
a600b30cc;LysandreJik;2019-08-30 10:08:14 -0400;Fix index number in documentation

==

docs/source/index.rst
==================
20c06fa37;LysandreJik;2019-08-30 10:06:51 -0400;Added DistilBERT to documentation index

==

docs/source/index.rst
==================
39eb31e11;Rabeeh KARIMI;2019-08-30 15:44:41 +0200;remove reloading tokenizer in the training, adding it to the evaluation part

==

examples/run_glue.py
==================
350bb6bff;Rabeeh KARIMI;2019-08-30 15:34:28 +0200;updated tokenizer loading for addressing reproducibility issues

==

examples/run_glue.py
==================
82462c5cb;thomwolf;2019-08-30 15:30:41 +0200;Added option to setup pretrained tokenizer arguments

==

pytorch_transformers/tokenization_bert.py
pytorch_transformers/tokenization_utils.py
pytorch_transformers/tokenization_xlm.py
==================
41f35d0b3;Thomas Wolf;2019-08-30 14:49:08 +0200;Merge pull request #1089 from dhpollack/dhp/use_pytorch_layernorm
change layernorm code to pytorch's native layer norm
==
==================
01ad55f8c;Thomas Wolf;2019-08-30 14:15:36 +0200;Merge pull request #1026 from rabeehk/master
loads the tokenizer for each checkpoint, to solve the reproducability‚Ä¶
==
==================
50e615f43;Thomas Wolf;2019-08-30 13:40:35 +0200;Merge branch 'master' into improved_testing

==
==================
f8aace6bc;thomwolf;2019-08-30 13:39:52 +0200;update tokenizers to use self.XX_token_id instead of converting self.XX_token

==

pytorch_transformers/tokenization_bert.py
pytorch_transformers/tokenization_roberta.py
pytorch_transformers/tokenization_xlm.py
pytorch_transformers/tokenization_xlnet.py
==================
8faf2e086;thomwolf;2019-08-30 13:36:22 +0200;more doc on special tokens

==

pytorch_transformers/tokenization_utils.py
==================
f7978490b;Thomas Wolf;2019-08-30 13:28:16 +0200;Merge pull request #1148 from huggingface/circleci
Documentation auto-deploy
==
==================
ce5ef4b35;thomwolf;2019-08-30 13:22:43 +0200;python2 doesn't spark joy

==

pytorch_transformers/tests/tokenization_gpt2_test.py
pytorch_transformers/tests/tokenization_roberta_test.py
==================
5dd7b677a;thomwolf;2019-08-30 12:43:08 +0200;clean up all byte-level bpe tests

==

pytorch_transformers/tests/tokenization_gpt2_test.py
pytorch_transformers/tests/tokenization_roberta_test.py
pytorch_transformers/tokenization_gpt2.py
==================
ca1a00a30;thomwolf;2019-08-30 12:29:31 +0200;fix for python2

==

pytorch_transformers/tests/tokenization_gpt2_test.py
pytorch_transformers/tests/tokenization_roberta_test.py
==================
4e6a3172c;thomwolf;2019-08-30 12:23:37 +0200;update roberta docstring as well

==

pytorch_transformers/tokenization_roberta.py
==================
fd10d79b5;thomwolf;2019-08-30 12:23:12 +0200;update GPT2 docstring

==

pytorch_transformers/tokenization_gpt2.py
==================
abe734ca1;thomwolf;2019-08-30 12:20:18 +0200;fix GPT-2 and RoBERTa tests to be clean now

==

pytorch_transformers/tests/tokenization_gpt2_test.py
pytorch_transformers/tests/tokenization_roberta_test.py
pytorch_transformers/tests/tokenization_tests_commons.py
==================
0f5a79945;thomwolf;2019-08-30 11:49:23 +0200;fix GPT2DoubleHeadModel docstring

==

pytorch_transformers/modeling_gpt2.py
==================
d51f72d5d;thomwolf;2019-08-30 11:41:11 +0200;adding shortcut to the ids of all the special tokens

==

pytorch_transformers/modeling_gpt2.py
pytorch_transformers/tests/tokenization_tests_commons.py
pytorch_transformers/tokenization_utils.py
==================
306af132d;thomwolf;2019-08-30 11:30:51 +0200;update readme to mention add_special_tokens more clearly in example

==

README.md
==================
50e6daf83;thomwolf;2019-08-30 11:27:43 +0200;fix Roberta tokenizer __init__

==

pytorch_transformers/tokenization_roberta.py
==================
0517e7a1c;thomwolf;2019-08-30 11:23:49 +0200;Fix GPT2 and RoBERTa tokenizer to beging with a space - update Roberta tokenizer

==

pytorch_transformers/modeling_gpt2.py
pytorch_transformers/tokenization_gpt2.py
pytorch_transformers/tokenization_roberta.py
==================
6e1ac34e2;erenup;2019-08-30 15:50:11 +0800;Merge remote-tracking branch 'huggingface/master'

==
==================
2fb9a934b;jamin;2019-08-30 14:05:28 +0900;re-format

==

examples/lm_finetuning/finetune_on_pregenerated.py
==================
c8731b958;jamin;2019-08-30 13:54:00 +0900;update apex fp16 implementation

==

examples/lm_finetuning/finetune_on_pregenerated.py
==================
6060b2f89;ziliwang;2019-08-30 12:13:47 +0800;fix: hard coding for max number
fp16 max number is 65504, the original 1e30 will cause Nan in fp16
==

pytorch_transformers/modeling_xlnet.py
==================
07e21307b;epwalsh;2019-08-29 13:44:50 -0700;fix adding special tokens

==

pytorch_transformers/tokenization_utils.py
==================
caf1d116a;LysandreJik;2019-08-29 15:30:10 -0400;Closing bracket in DistilBERT's token count.

==

examples/distillation/scripts/token_counts.py
==================
e7fba4bef;LysandreJik;2019-08-29 12:14:29 -0400;Documentation auto-deploy

==

.circleci/config.yml
==================
fe8fb10b4;Luis;2019-08-29 09:54:45 +0200;Small modification of comment in the run_glue.py example
Add RoBERTa to the comment as it was not explicit that RoBERTa don't use token_type_ids.
==

examples/run_glue.py
==================
2a2832ce7;erenup;2019-08-29 16:27:44 +0800;Merge pull request #1 from erenup/run_multiple_choice
roberta, xlnet for multiple choice
==
==================
942d3f4b2;erenup;2019-08-29 10:21:17 +0800;modifiy code of arc label insurance

==

examples/single_model_scripts/utils_multiple_choice.py
==================
bf3dc778b;LysandreJik;2019-08-28 18:24:43 -0400;Changed learning rate for run_squad test

==

examples/test_examples.py
==================
0a74c88ac;thomwolf;2019-08-28 22:41:42 +0200;fix #1131

==

pytorch_transformers/modeling_xlnet.py
==================
5f297c7be;Thomas Wolf;2019-08-28 22:22:11 +0200;Merge pull request #1087 from huggingface/fix-warnings
Decode now calls private property instead of public method
==
==================
d9847678b;Thomas Wolf;2019-08-28 22:00:52 +0200;Merge pull request #1136 from adai183/update_SQuAD_script
swap order of optimizer.step() and scheduler.step()
==
==================
0f8ad8920;Thomas Wolf;2019-08-28 22:00:12 +0200;Merge pull request #1135 from stefan-it/master
distilbert: fix number of hidden_size
==
==================
9ce42dc54;LysandreJik;2019-08-28 13:56:28 -0400;Pretrained models table fix

==

docs/source/pretrained_models.rst
==================
1d15a7f27;Andreas Daiminger;2019-08-28 19:18:27 +0200;swap order of optimizer.step() and scheduler.step()

==

examples/run_squad.py
==================
ed2ab1c22;Stefan Schweter;2019-08-28 18:08:16 +0200;distilbert: fix number of hidden_size

==

pytorch_transformers/modeling_distilbert.py
==================
0ecfd17f4;Thomas Wolf;2019-08-28 16:51:50 +0200;Merge pull request #987 from huggingface/generative-finetuning
Generative finetuning
==
==================
50792dbdc;Thomas Wolf;2019-08-28 16:43:09 +0200;Merge pull request #1127 from huggingface/dilbert
DilBERT
==
==================
e7706f514;thomwolf;2019-08-28 16:37:22 +0200;update again

==

pytorch_transformers/tokenization_distilbert.py
==================
b5eb283aa;thomwolf;2019-08-28 16:36:55 +0200;update credits

==

examples/distillation/dataset.py
examples/distillation/distiller.py
examples/distillation/utils.py
pytorch_transformers/modeling_distilbert.py
==================
f753d4e32;LysandreJik;2019-08-28 10:15:02 -0400;Removed typings for Python 2

==

pytorch_transformers/modeling_distilbert.py
==================
75bc2a03c;LysandreJik;2019-08-28 10:05:15 -0400;Updated article link

==

README.md
docs/source/pretrained_models.rst
pytorch_transformers/modeling_distilbert.py
==================
1dc43e56c;LysandreJik;2019-08-28 09:37:27 -0400;Documentation additions

==

docs/source/index.rst
docs/source/model_doc/distilbert.rst
docs/source/pretrained_models.rst
pytorch_transformers/modeling_distilbert.py
==================
912a377e9;thomwolf;2019-08-28 13:59:42 +0200;dilbert -> distilbert

==

README.md
examples/distillation/README.md
examples/distillation/dataset.py
examples/distillation/distiller.py
examples/distillation/scripts/binarized_data.py
examples/distillation/scripts/extract_for_distil.py
examples/distillation/scripts/token_counts.py
examples/distillation/train.py
examples/distillation/utils.py
pytorch_transformers/__init__.py
pytorch_transformers/modeling_auto.py
pytorch_transformers/modeling_distilbert.py
pytorch_transformers/tests/modeling_dilbert_test.py
pytorch_transformers/tests/tokenization_dilbert_test.py
pytorch_transformers/tokenization_distilbert.py
==================
c9bce1811;thomwolf;2019-08-28 13:22:45 +0200;fixing model to add torchscript, embedding resizing, head pruning and masking + tests

==

pytorch_transformers/modeling_bert.py
pytorch_transformers/modeling_dilbert.py
pytorch_transformers/tests/modeling_dilbert_test.py
==================
62df4ba59;thomwolf;2019-08-28 12:22:56 +0200;add dilbert tokenizer and tests

==

pytorch_transformers/__init__.py
pytorch_transformers/tests/tokenization_bert_test.py
pytorch_transformers/tests/tokenization_dilbert_test.py
pytorch_transformers/tokenization_dilbert.py
==================
4ce5f36f7;thomwolf;2019-08-28 12:14:31 +0200;update readmes

==

README.md
examples/distillation/README.md
==================
ec4b1c659;erenup;2019-08-28 16:50:40 +0800;logging truth error

==

examples/single_model_scripts/utils_multiple_choice.py
==================
df52abe37;erenup;2019-08-28 16:36:21 +0800;add sep_toekn between question and choice

==

examples/single_model_scripts/utils_multiple_choice.py
==================
43c243254;erenup;2019-08-28 16:03:17 +0800;avoid invalid labels of truth

==

examples/single_model_scripts/utils_multiple_choice.py
==================
3c7e676f8;erenup;2019-08-28 15:57:29 +0800;add test related code: test the best dev acc model when model is training

==

examples/single_model_scripts/run_multiple_choice.py
==================
a5fe16687;VictorSanh;2019-08-28 07:22:54 +0000;fix typo

==

pytorch_transformers/modeling_auto.py
==================
497f73c96;VictorSanh;2019-08-28 07:16:30 +0000;add DilBERT to master REAME

==

README.md
==================
93e82ab42;VictorSanh;2019-08-28 06:26:09 +0000;Write README for DilBERT

==

examples/distillation/README.md
==================
19b7c9b0b;VictorSanh;2019-08-28 06:25:44 +0000;add DilBert model for squad

==

pytorch_transformers/modeling_dilbert.py
==================
fea921d38;VictorSanh;2019-08-28 04:45:39 +0000;add licensing

==

examples/distillation/dataset.py
examples/distillation/distiller.py
examples/distillation/scripts/binarized_data.py
examples/distillation/scripts/extract_for_distil.py
examples/distillation/scripts/token_counts.py
examples/distillation/train.py
examples/distillation/utils.py
==================
da1e4e53f;VictorSanh;2019-08-28 04:01:03 +0000;some fixes in `train.py` for loading previous checkpoint

==

examples/distillation/train.py
==================
0d8f8848d;VictorSanh;2019-08-28 04:00:19 +0000;add `scripts/extract_for_distil.py`

==

examples/distillation/scripts/extract_for_distil.py
==================
7f2c384c8;VictorSanh;2019-08-28 04:00:03 +0000;add `scripts/token_counts.py`

==

examples/distillation/scripts/token_counts.py
==================
4d16b279e;VictorSanh;2019-08-28 03:59:48 +0000;add `scripts/binarized_data.py`

==

examples/distillation/scripts/binarized_data.py
==================
c513415b1;LysandreJik;2019-08-27 23:59:00 -0400;Dilbert tests from CommonTests

==

pytorch_transformers/tests/modeling_common_test.py
pytorch_transformers/tests/modeling_dilbert_test.py
==================
778a263f0;LysandreJik;2019-08-27 22:28:42 -0400;GilBert added to AutoModels

==

pytorch_transformers/modeling_auto.py
==================
74d78beeb;VictorSanh;2019-08-28 03:13:11 +0000;fix: add qa_dropout and seq_classif_dropout

==

pytorch_transformers/modeling_dilbert.py
==================
7f5d85347;VictorSanh;2019-08-28 02:44:51 +0000;fix small typo

==

pytorch_transformers/modeling_bert.py
pytorch_transformers/modeling_transfo_xl.py
pytorch_transformers/modeling_xlm.py
pytorch_transformers/modeling_xlnet.py
==================
906581ae3;VictorSanh;2019-08-28 02:43:33 +0000;add s3 links for dilbert (+fix small typo)

==

pytorch_transformers/modeling_dilbert.py
==================
b247b0d88;VictorSanh;2019-08-28 02:12:47 +0000;add `train.py` for distillation

==

examples/distillation/train.py
==================
780f183e5;VictorSanh;2019-08-28 01:39:52 +0000;add requirements

==

examples/distillation/requirements.txt
==================
e424d2e45;VictorSanh;2019-08-28 01:10:10 +0000;add README

==

examples/distillation/README.md
==================
1ae81e4aa;VictorSanh;2019-08-28 01:10:05 +0000;add dataset. distiller, utils

==

examples/distillation/dataset.py
examples/distillation/distiller.py
examples/distillation/utils.py
==================
5d29f8e99;VictorSanh;2019-08-28 00:57:16 +0000;fix bugs

==

pytorch_transformers/modeling_dilbert.py
==================
a8ad83040;VictorSanh;2019-08-28 00:45:33 +0000;fix bugs

==

pytorch_transformers/modeling_dilbert.py
==================
ca4baf8ca;Shijie Wu;2019-08-27 20:03:18 -0400;Match order of casing in OSS XLM; Improve document; Clean up dependency

==

pytorch_transformers/tokenization_xlm.py
requirements.txt
setup.py
==================
60c984da6;VictorSanh;2019-08-27 22:25:55 +0000;fix bugs

==

pytorch_transformers/__init__.py
pytorch_transformers/modeling_dilbert.py
==================
42968138c;VictorSanh;2019-08-27 22:00:38 +0000;wip wouf

==

pytorch_transformers/__init__.py
pytorch_transformers/modeling_dilbert.py
==================
1d2324006;VictorSanh;2019-08-27 14:27:47 +0000;wip

==

pytorch_transformers/modeling_dilbert.py
==================
d06c5a2a0;Thomas Wolf;2019-08-27 15:01:01 +0200;Merge pull request #1120 from CrafterKolyan/patch-3
Change attention mask dtype to be bool. Fix #1119
==
==================
edc5222fc;Thomas Wolf;2019-08-27 14:58:50 +0200;Merge pull request #1118 from CrafterKolyan/patch-2
Documentation fix #1117
==
==================
9cf298dfc;Thomas Wolf;2019-08-27 14:56:43 +0200;Merge pull request #1116 from CrafterKolyan/patch-1
Delete nonexistent parameter from documentation fix #1115
==
==================
0d288727b;thomwolf;2019-08-27 14:50:22 +0200;fix #1106

==

docs/source/examples.rst
==================
447afe9cd;thomwolf;2019-08-27 14:42:03 +0200;updating docstring for AutoModel

==

pytorch_transformers/modeling_auto.py
pytorch_transformers/modeling_utils.py
pytorch_transformers/tokenization_auto.py
==================
a175a9dc0;thomwolf;2019-08-27 14:05:59 +0200;add kwargs to base encode function

==

pytorch_transformers/tokenization_utils.py
==================
53282b5bd;Nikolay Korolev;2019-08-27 14:19:03 +0300;Change attention mask dtype to be bool. Fix #1119

==

pytorch_transformers/modeling_transfo_xl.py
==================
26bda7722;Nikolay Korolev;2019-08-27 12:22:42 +0300;Fix documentation #1117
Rename parameter in documentation + Delete its second occurrence.
==

pytorch_transformers/modeling_gpt2.py
==================
c8933bb2d;Nikolay Korolev;2019-08-27 12:10:36 +0300;Delete nonexistent parameter from documentation
Changed documentation of GPT2Model, GPT2LMHeadModel and GPT2DoubleHeadsModel
==

pytorch_transformers/modeling_gpt2.py
==================
e08c01aa1;LysandreJik;2019-08-26 18:13:06 -0400;fix #1102

==

pytorch_transformers/modeling_roberta.py
pytorch_transformers/tokenization_roberta.py
==================
84a3a9689;LysandreJik;2019-08-26 16:08:43 -0400;Pytorch Hub & AutoModels

==

hubconf.py
==================
f68339639;LysandreJik;2019-08-26 16:02:23 -0400;Tests for added AutoModels

==

pytorch_transformers/tests/modeling_auto_test.py
==================
cb60ce59d;LysandreJik;2019-08-26 15:44:30 -0400;Added multiple AutoModel classes: AutoModelWithLMHead, AutoModelForQuestionAnswering and AutoModelForSequenceClassification

==

pytorch_transformers/__init__.py
pytorch_transformers/modeling_auto.py
==================
529a16dec;LysandreJik;2019-08-26 15:00:43 -0400;Generic encoding implementation.

==

pytorch_transformers/tokenization_utils.py
==================
f1b018740;Shijie Wu;2019-08-23 20:33:01 -0400;Add use_lang_emb to config

==

pytorch_transformers/modeling_xlm.py
==================
e85123d39;Shijie Wu;2019-08-23 20:27:52 -0400;Add custom tokenizer for zh and ja

==

pytorch_transformers/tokenization_xlm.py
requirements.txt
setup.py
==================
06510ccb5;thomwolf;2019-08-23 22:08:10 +0200;typo

==

examples/run_lm_finetuning.py
==================
3bcbebd44;thomwolf;2019-08-23 22:07:26 +0200;max_len_single_sentence & max_len_sentences_pair as attributes so they can be modified

==

pytorch_transformers/tokenization_bert.py
pytorch_transformers/tokenization_gpt2.py
pytorch_transformers/tokenization_openai.py
pytorch_transformers/tokenization_roberta.py
pytorch_transformers/tokenization_transfo_xl.py
pytorch_transformers/tokenization_utils.py
pytorch_transformers/tokenization_xlm.py
pytorch_transformers/tokenization_xlnet.py
==================
436ce0721;Shijie Wu;2019-08-23 14:40:17 -0400;Tokenization behave the same as original XLM proprocessing for most languages except zh, ja and th; Change API to allow specifying language in `tokenize`

==

pytorch_transformers/tokenization_xlm.py
requirements.txt
setup.py
==================
ab7bd5ef9;thomwolf;2019-08-23 17:31:21 +0200;fixing tokenization and training

==

examples/run_lm_finetuning.py
==================
47d685343;thomwolf;2019-08-23 17:31:11 +0200;adding max_lengths for single sentences and sentences pairs

==

pytorch_transformers/tokenization_bert.py
pytorch_transformers/tokenization_roberta.py
pytorch_transformers/tokenization_utils.py
pytorch_transformers/tokenization_xlm.py
pytorch_transformers/tokenization_xlnet.py
==================
df9d6effa;Thomas Wolf;2019-08-23 16:53:53 +0200;Merge pull request #1081 from huggingface/fix_distributed_barrier_hang
Fix distributed barrier hang
==
==================
3f20dd718;Thomas Wolf;2019-08-23 12:42:39 +0200;Merge pull request #1075 from abhishekraok/modeling_utils_config_None
reraise EnvironmentError in modeling_utils.py
==
==================
e13465fb8;David Pollack;2019-08-23 12:12:12 +0200;change layernorm code to pytorch's native layer norm

==

pytorch_transformers/modeling_bert.py
==================
c603d099a;Abhishek Rao;2019-08-22 15:25:40 -0700;reraise EnvironmentError in from_pretrained functions of Model and Tokenizer

==

pytorch_transformers/modeling_utils.py
pytorch_transformers/tokenization_utils.py
==================
2ba1a14fb;LysandreJik;2019-08-22 17:25:55 -0400;Decode now calls private property instead of public method

==

pytorch_transformers/tokenization_utils.py
==================
90dcd8c05;Thomas Wolf;2019-08-22 10:43:30 +0200;Merge branch 'master' into generative-finetuning

==
==================
57272d5dd;VictorSanh;2019-08-22 00:25:49 -0400;fix for glue

==

examples/run_glue.py
==================
b006a7a12;VictorSanh;2019-08-22 00:25:42 -0400;fix for squad

==

examples/run_squad.py
==================
14eef67eb;Abhishek Rao;2019-08-21 15:48:43 -0700;Fix at config rather than model

==

pytorch_transformers/modeling_utils.py
==================
296df2b18;Abhishek Rao;2019-08-21 15:29:30 -0700;reraise exception

==

pytorch_transformers/modeling_utils.py
==================
55f69a11b;Lysandre;2019-08-21 18:09:25 -0400;OpenAI GPT tests now extend CommonTests

==

pytorch_transformers/tests/modeling_gpt2_test.py
pytorch_transformers/tests/modeling_openai_test.py
==================
47267ba55;Lysandre;2019-08-21 17:50:16 -0400;OpenAI GPT-2 now depends on CommonTests.

==

pytorch_transformers/tests/modeling_common_test.py
pytorch_transformers/tests/modeling_gpt2_test.py
==================
034aa0c2d;Lysandre;2019-08-21 17:27:38 -0400;Fixed GPT2DoubleHeadsModel example and weight tying

==

pytorch_transformers/modeling_gpt2.py
==================
e00b4ff1d;thomwolf;2019-08-21 22:22:17 +0200;fix #1017

==

README.md
==================
814a3f4e0;Lysandre;2019-08-21 14:11:14 -0400;Removed `attention_mask` from GPT-2 and GPT documentation. Corrected `multiple_choice_labels` to actual name `mc_labels`

==

pytorch_transformers/modeling_gpt2.py
pytorch_transformers/modeling_openai.py
==================
2f9397139;Lysandre;2019-08-21 11:29:37 -0400;Added GPT-2 LARGE to Pre-trained Models documentation

==

docs/source/pretrained_models.rst
==================
d6bbcbc4c;Lysandre;2019-08-21 11:22:05 -0400;Added finetuning example to documentation

==

docs/source/examples.rst
==================
6f877d9da;VictorSanh;2019-08-21 03:43:29 +0000;Update dev results on GLUE (bert-base-uncased) w/ median on 5 runs

==

docs/source/examples.rst
==================
07681b6b5;Thomas Wolf;2019-08-21 03:05:56 +0200;Merge pull request #1064 from huggingface/gpt-2-large
Adding gpt-2 large (774M parameters) model
==
==================
fdc487d8b;thomwolf;2019-08-21 02:35:01 +0200;Add max length

==

pytorch_transformers/tokenization_gpt2.py
==================
aa05dc893;thomwolf;2019-08-21 02:29:34 +0200;adding gpt-2 large

==

pytorch_transformers/convert_gpt2_checkpoint_to_pytorch.py
pytorch_transformers/convert_openai_checkpoint_to_pytorch.py
pytorch_transformers/convert_transfo_xl_checkpoint_to_pytorch.py
pytorch_transformers/modeling_gpt2.py
pytorch_transformers/tokenization_gpt2.py
==================
e4515faf5;Thomas Wolf;2019-08-21 01:54:05 +0200;Merge pull request #1057 from huggingface/fixes
Add a few of typos corrections, bugs fixes and small improvements
==
==================
41789c6c3;Thomas Wolf;2019-08-21 01:53:48 +0200;Merge pull request #1059 from GuillemGSubies/master
Better use of spacy tokenizer in open ai and xlm tokenizers
==
==================
260c86082;Thomas Wolf;2019-08-21 01:46:03 +0200;Merge pull request #1027 from samvelyan/iterative_split_on_token
Re-implemented tokenize() iteratively in PreTrainedTokenizer.
==
==================
d30cbaf5d;Thomas Wolf;2019-08-21 01:33:02 +0200;Merge branch 'master' into iterative_split_on_token

==
==================
9beaa85b0;Thomas Wolf;2019-08-21 01:20:46 +0200;Merge pull request #1055 from qipeng/run_squad_fix
Fix #1015 (tokenizer defaults to use_lower_case=True when loading from trained models)
==
==================
e753f249e;Thomas Wolf;2019-08-21 01:14:40 +0200;Merge pull request #806 from wschin/fix-a-path
Fix a path so that a test can run on Windows
==
==================
2d042274a;Lysandre;2019-08-20 14:15:28 -0400;Sequence special token handling for BERT and RoBERTa

==

examples/run_lm_finetuning.py
==================
3bffd2e8e;Peng Qi;2019-08-20 10:59:28 -0700;more fixes

==

examples/run_glue.py
examples/run_squad.py
==================
c3619f553;Thomas Wolf;2019-08-20 17:39:06 +0200;Merge pull request #1060 from CrafterKolyan/patch-1
Fix typo. configuratoin -> configuration
==
==================
3b56427a1;Thomas Wolf;2019-08-20 17:13:44 +0200;Merge pull request #1040 from FeiWang96/multi_gpu
Fix bug of multi-gpu training in lm finetuning
==
==================
43489756a;thomwolf;2019-08-20 16:59:11 +0200;adding proxies options for the from_pretrained methods

==

.gitignore
pytorch_transformers/file_utils.py
pytorch_transformers/modeling_utils.py
pytorch_transformers/tokenization_utils.py
==================
a690edab1;thomwolf;2019-08-20 15:52:12 +0200;various fix and clean up on run_lm_finetuning

==

.gitignore
examples/run_lm_finetuning.py
examples/utils_lm.py
==================
ad6e62cd8;Nikolay Korolev;2019-08-20 15:43:06 +0300;Fix typo. configuratoin -> configuration

==

README.md
==================
388e3251f;Guillem Garc√≠a Subies;2019-08-20 14:19:39 +0200;Update tokenization_xlm.py

==

pytorch_transformers/tokenization_xlm.py
==================
f5e2ed0fd;Guillem Garc√≠a Subies;2019-08-20 14:19:25 +0200;Update tokenization_openai.py

==

pytorch_transformers/tokenization_openai.py
==================
562b99836;Guillem Garc√≠a Subies;2019-08-20 14:10:19 +0200;Update tokenization_openai.py

==

pytorch_transformers/tokenization_openai.py
==================
bb0444628;Guillem Garc√≠a Subies;2019-08-20 14:07:40 +0200;Update tokenization_openai.py

==

pytorch_transformers/tokenization_openai.py
==================
bfd75056b;Guillem Garc√≠a Subies;2019-08-20 14:06:17 +0200;Update tokenization_xlm.py

==

pytorch_transformers/tokenization_xlm.py
==================
fc7413259;erenup;2019-08-20 19:06:41 +0800;add best steps to train

==

examples/single_model_scripts/run_multiple_choice.py
==================
933841d90;Thomas Wolf;2019-08-20 12:42:24 +0200;Merge pull request #1056 from Morizeyao/master
Swap of optimizer.step and scheduler.step for lm finetuning examples
==
==================
6d0aa7398;thomwolf;2019-08-20 12:20:21 +0200;fix #1034

==

pytorch_transformers/modeling_xlm.py
==================
b0b9b8091;Julien Chaumond;2019-08-20 11:33:46 +0200;minor typo

==

pytorch_transformers/modeling_gpt2.py
pytorch_transformers/modeling_openai.py
==================
53c8f700f;thomwolf;2019-08-20 11:29:26 +0200;fix #808

==

pytorch_transformers/modeling_bert.py
pytorch_transformers/modeling_gpt2.py
pytorch_transformers/modeling_openai.py
pytorch_transformers/modeling_roberta.py
pytorch_transformers/modeling_transfo_xl.py
pytorch_transformers/modeling_xlm.py
pytorch_transformers/modeling_xlnet.py
==================
901dde0e4;thomwolf;2019-08-20 11:05:51 +0200;fix #1014

==

pytorch_transformers/tokenization_bert.py
==================
e239a4a20;thomwolf;2019-08-20 11:02:00 +0200;close #984

==

docs/source/pretrained_models.rst
==================
fecaed0ed;thomwolf;2019-08-20 10:56:12 +0200;add force_download option to from_pretrained methods

==

pytorch_transformers/file_utils.py
pytorch_transformers/modeling_utils.py
pytorch_transformers/tokenization_utils.py
==================
d86b49ac8;Duzeyao;2019-08-20 16:46:34 +0800;swap optimizer.step and scheduler.step

==

examples/lm_finetuning/finetune_on_pregenerated.py
==================
45ab8bf60;Duzeyao;2019-08-20 16:40:39 +0800;Revert "Update finetune_on_pregenerated.py"
This reverts commit a1359b970cb4bfa41008a45b44dd2a25e579bff3.

==

examples/lm_finetuning/finetune_on_pregenerated.py
==================
97c30b73d;erenup;2019-08-20 16:31:04 +0800;add test related code

==

examples/single_model_scripts/run_multiple_choice.py
==================
d5e60e5b7;erenup;2019-08-20 16:25:50 +0800;add test related code

==

examples/single_model_scripts/run_multiple_choice.py
examples/single_model_scripts/utils_multiple_choice.py
==================
a1359b970;Zeyao Du;2019-08-20 16:00:07 +0800;Update finetune_on_pregenerated.py

==

examples/lm_finetuning/finetune_on_pregenerated.py
==================
28f7ca1f8;Zeyao Du;2019-08-20 15:58:42 +0800;swap optimizer.step and scheduler.step

==

examples/lm_finetuning/simple_lm_finetuning.py
==================
a368b8779;Peng Qi;2019-08-19 13:07:00 -0700;Fix #1015

==

examples/run_squad.py
==================
f94f1c601;Lysandre;2019-08-19 14:58:50 -0400;Distributed training + tokenizer agnostic mask token

==

examples/run_generative_finetuning.py
examples/utils_lm.py
==================
c589862b7;Lysandre;2019-08-19 10:17:47 -0400;Doc: loading from config alone does not load the model weights

==

pytorch_transformers/modeling_bert.py
pytorch_transformers/modeling_gpt2.py
pytorch_transformers/modeling_openai.py
pytorch_transformers/modeling_roberta.py
pytorch_transformers/modeling_transfo_xl.py
pytorch_transformers/modeling_utils.py
pytorch_transformers/modeling_xlm.py
pytorch_transformers/modeling_xlnet.py
==================
5a49b793d;Thomas Wolf;2019-08-19 15:31:46 +0200;Merge pull request #1023 from tuvuumass/patch-1
fix issue #824
==
==================
4270d3da1;erenup;2019-08-19 16:38:52 +0800;fix a bug of evaluating

==

examples/single_model_scripts/run_multiple_choice.py
==================
b8fde4386;erenup;2019-08-19 16:36:43 +0800;a coding bug

==

pytorch_transformers/modeling_xlnet.py
==================
40acf6b52;Chi-Liang Liu;2019-07-30 18:37:37 +0800;don't save model without training

==

examples/run_squad.py
==================
47e9aea0f;erenup;2019-08-18 17:00:53 +0800;add args info to evaluate_result.txt

==

examples/single_model_scripts/run_multiple_choice.py
==================
5582bc4b2;erenup;2019-08-18 16:01:48 +0800;add multiple choice to robreta and xlnet, test on swag, roberta=0.82.28 , xlnet=0.80

==

examples/single_model_scripts/run_multiple_choice.py
examples/single_model_scripts/utils_multiple_choice.py
pytorch_transformers/__init__.py
pytorch_transformers/modeling_roberta.py
pytorch_transformers/modeling_xlnet.py
==================
856a63da4;wangfei;2019-08-18 11:03:47 +0800;Fix: save model/model.module

==

examples/lm_finetuning/finetune_on_pregenerated.py
examples/lm_finetuning/simple_lm_finetuning.py
==================
1ef41b833;wangfei;2019-08-18 11:03:12 +0800;Revert "Fix: save model/model.module"
This reverts commit 00e9c4cc9616cab1666cab0a331b5d7e68946928.

==

examples/lm_finetuning/finetune_on_pregenerated.py
examples/lm_finetuning/simple_lm_finetuning.py
==================
00e9c4cc9;wangfei;2019-08-18 11:02:02 +0800;Fix: save model/model.module

==

examples/lm_finetuning/finetune_on_pregenerated.py
examples/lm_finetuning/simple_lm_finetuning.py
==================
189ff9b66;Christophe Bourguignat;2019-08-17 18:46:50 +0200;Update README after RoBERTa addition

==

README.md
==================
e384ae2b9;erenup;2019-08-17 12:05:57 +0800;Merge remote-tracking branch 'huggingface/master' merge huggingface/master to update

==
==================
d8923270e;Jason Phang;2019-08-16 15:58:19 -0400;Correct truncation for RoBERTa in 2-input GLUE

==

examples/utils_glue.py
==================
5652f54ac;Lysandre;2019-08-16 13:49:56 -0400;Simplified data generator + better perplexity calculator
GPT-2 now obtains ~20 perplexity on WikiText-2
==

examples/run_generative_finetuning.py
examples/utils_lm.py
==================
7e7fc53da;LysandreJik;2019-08-16 11:02:10 -0400;Fixing run_glue example with RoBERTa

==

examples/run_glue.py
examples/utils_glue.py
==================
715534800;LysandreJik;2019-08-14 09:52:57 -0400;BERT + RoBERTa masking tokens handling + GPU device update.

==

examples/run_generative_finetuning.py
examples/utils_lm.py
==================
339e556fe;LysandreJik;2019-08-09 18:08:15 -0400;CLM for BERT, beginning of CLM fot RoBERTa; still needs a better masking token mechanism.

==

examples/run_generative_finetuning.py
==================
5c18825a1;LysandreJik;2019-08-06 14:57:07 -0400;Removed dataset limit

==

examples/utils_lm.py
==================
3e3e14549;LysandreJik;2019-08-06 12:14:18 -0400;Added GPT to the generative fine-tuning.

==

examples/run_generative_finetuning.py
examples/utils_lm.py
==================
47975ed53;LysandreJik;2019-08-06 11:21:48 -0400;Language Modeling fine-tuning using GPT-2.

==

examples/run_generative_finetuning.py
examples/utils_lm.py
==================
ab0528066;LysandreJik;2019-08-16 09:53:26 -0400;Order of strings in AutoModel/AutoTokenizer updated.

==

pytorch_transformers/modeling_auto.py
pytorch_transformers/tokenization_auto.py
==================
b8ff56896;wangfei;2019-08-16 12:11:05 +0800;Fix bug of multi-gpu training in lm finetuning

==

examples/lm_finetuning/finetune_on_pregenerated.py
examples/lm_finetuning/simple_lm_finetuning.py
==================
9d0029e21;LysandreJik;2019-08-15 17:17:35 -0400;Added RoBERTa example to README

==

README.md
==================
83dba0b67;LysandreJik;2019-08-15 17:07:07 -0400;Added RoBERTa tokenizer to AutoTokenizer

==

pytorch_transformers/modeling_auto.py
pytorch_transformers/tokenization_auto.py
==================
e24e19ce3;LysandreJik;2019-08-15 14:02:11 -0400;Added RoBERTa to AutoModel/AutoConfig

==

pytorch_transformers/modeling_auto.py
==================
fe02e45e4;LysandreJik;2019-08-15 11:15:08 -0400;Release: 1.1.0

==

pytorch_transformers/__init__.py
setup.py
==================
88efc65ba;Lysandre Debut;2019-08-15 11:11:10 -0400;Merge pull request #964 from huggingface/RoBERTa
RoBERTa: model conversion, inference, tests üî•
==
==================
830817015;LysandreJik;2019-08-15 10:29:04 -0400;Warning for RoBERTa sequences encoded without special tokens.

==

pytorch_transformers/modeling_roberta.py
==================
572dcfd1d;LysandreJik;2019-08-14 14:56:14 -0400;Doc

==

docs/source/index.rst
docs/source/model_doc/roberta.rst
docs/source/pretrained_models.rst
pytorch_transformers/modeling_roberta.py
pytorch_transformers/tokenization_roberta.py
pytorch_transformers/tokenization_utils.py
==================
c4ef10344;Julien Chaumond;2019-08-14 12:31:09 -0400;[RoBERTa] First 4 authors
cf. https://github.com/huggingface/pytorch-transformers/pull/964#discussion_r313574354

Co-Authored-By: Myle Ott <myleott@fb.com>

==

README.md
==================
3d47a7f8a;Rabeeh KARIMI;2019-08-14 10:58:26 +0200;loads the tokenizer for each checkpoint, to solve the reproducability issue

==

examples/run_glue.py
==================
9ce36e3e4;samvelyan;2019-08-14 08:57:09 +0000;Re-implemented tokenize() iteratively in PreTrainedTokenizer.

==

pytorch_transformers/tokenization_utils.py
==================
39f426be6;LysandreJik;2019-08-13 15:19:50 -0400;Added special tokens <pad> and <mask> to RoBERTa.

==

examples/run_glue.py
pytorch_transformers/tokenization_roberta.py
==================
baf08ca1d;Julien Chaumond;2019-08-13 12:51:15 -0400;[RoBERTa] run_glue: correct pad_token + reorder labels

==

examples/run_glue.py
==================
3d87991f6;LysandreJik;2019-08-13 12:00:24 -0400;Fixed error with encoding

==

pytorch_transformers/tests/tokenization_roberta_test.py
pytorch_transformers/tokenization_utils.py
==================
ba4bce258;tuvuumass;2019-08-13 11:26:27 -0400;fix issue #824

==

examples/run_bertology.py
==================
634a3172d;LysandreJik;2019-08-12 15:14:15 -0400;Added integration tests for sequence builders.

==

pytorch_transformers/tests/tokenization_bert_test.py
pytorch_transformers/tests/tokenization_roberta_test.py
pytorch_transformers/tests/tokenization_xlm_test.py
pytorch_transformers/tests/tokenization_xlnet_test.py
==================
22ac004a7;LysandreJik;2019-08-12 15:13:53 -0400;Added documentation and changed parameters for special_tokens_sentences_pair.

==

pytorch_transformers/tokenization_bert.py
pytorch_transformers/tokenization_roberta.py
pytorch_transformers/tokenization_utils.py
pytorch_transformers/tokenization_xlm.py
pytorch_transformers/tokenization_xlnet.py
==================
912fdff89;Julien Chaumond;2019-08-12 13:49:50 -0400;[RoBERTa] Update `run_glue` for RoBERTa

==

examples/run_glue.py
examples/utils_glue.py
==================
b3d83d68d;Julien Chaumond;2019-08-12 12:28:55 -0400;Fixup 9d0603148bc34255fad0cad73ce438ecd7306322

==

pytorch_transformers/convert_roberta_checkpoint_to_pytorch.py
==================
a7b4cfe91;carefree0910;2019-08-11 21:36:51 +0800;Update README.md
I assume that it should test the `re-load` functionality after testing the `save` functionality, however I'm also surprised that nobody points this out after such a long time, so maybe I've misunderstood the purpose. This PR is just in case :)
==

README.md
==================
b219029c4;erenup;2019-08-11 15:20:37 +0800;refactoring old run_swag. This script is mainly refatored from run_squad in pytorch_transformers

==

examples/single_model_scripts/run_swag.py
==================
aaedfc35a;thomwolf;2019-08-10 20:04:37 +0200;Merge branch 'master' of https://github.com/huggingface/pytorch-transformers

==
==================
c683c3d5a;thomwolf;2019-08-10 20:04:35 +0200;fix #993

==

pytorch_transformers/modeling_gpt2.py
pytorch_transformers/modeling_openai.py
==================
706076649;Kevin Trebing;2019-08-09 11:28:39 +0100;Corrected logger.error info
Signed-off-by: Kevin Trebing <Kevin.Trebing@gmx.net>

==

pytorch_transformers/modeling_bert.py
pytorch_transformers/modeling_gpt2.py
==================
75d5f98fd;LysandreJik;2019-08-09 15:02:13 -0400;Roberta tokenization + fixed tests (py3 + py2).

==

pytorch_transformers/tests/modeling_roberta_test.py
pytorch_transformers/tests/tokenization_roberta_test.py
pytorch_transformers/tokenization_roberta.py
==================
14e970c27;LysandreJik;2019-08-09 15:01:38 -0400;Tokenization encode/decode class-based sequence handling

==

pytorch_transformers/tests/tokenization_tests_commons.py
pytorch_transformers/tokenization_bert.py
pytorch_transformers/tokenization_utils.py
pytorch_transformers/tokenization_xlm.py
pytorch_transformers/tokenization_xlnet.py
==================
3566d2791;LysandreJik;2019-08-08 19:04:34 -0400;Clarified PreTrainedModel.from_pretrained warning messages in documentation.

==

pytorch_transformers/modeling_utils.py
==================
fbd746bd0;LysandreJik;2019-08-08 18:21:34 -0400;Updated test architecture

==

pytorch_transformers/tests/modeling_roberta_test.py
pytorch_transformers/tests/tokenization_roberta_test.py
pytorch_transformers/tests/tokenization_tests_commons.py
==================
6c41a8f5d;LysandreJik;2019-08-08 18:20:32 -0400;Encode and Decode are back in the superclass. They now handle sentence pairs special tokens.

==

pytorch_transformers/__init__.py
pytorch_transformers/modeling_roberta.py
pytorch_transformers/tokenization_roberta.py
pytorch_transformers/tokenization_utils.py
==================
e367ac469;Julien Chaumond;2019-08-08 11:26:11 -0400;[RoBERTa] Re-apply 39d72bcc7b2c99c04b6f483f0d8e7bdff547d37c
cc @lysandrejik

==

pytorch_transformers/convert_roberta_checkpoint_to_pytorch.py
==================
9d0603148;Julien Chaumond;2019-08-08 11:24:54 -0400;[RoBERTa] RobertaForSequenceClassification + conversion

==

pytorch_transformers/convert_roberta_checkpoint_to_pytorch.py
pytorch_transformers/modeling_roberta.py
pytorch_transformers/tests/modeling_roberta_test.py
==================
f2b300df6;LysandreJik;2019-08-08 10:38:57 -0400;fix #976

==

pytorch_transformers/modeling_bert.py
pytorch_transformers/modeling_xlm.py
pytorch_transformers/modeling_xlnet.py
==================
7df303f5a;LysandreJik;2019-08-08 10:36:26 -0400;fix #971

==

pytorch_transformers/modeling_bert.py
pytorch_transformers/modeling_gpt2.py
pytorch_transformers/modeling_openai.py
pytorch_transformers/modeling_xlm.py
==================
d2cc6b101;LysandreJik;2019-08-08 09:42:05 -0400;Merge branch 'master' into RoBERTa

==
==================
39d72bcc7;LysandreJik;2019-08-07 14:21:57 -0400;Fixed the RoBERTa checkpoint conversion script according to the LM head refactoring.

==

pytorch_transformers/convert_roberta_checkpoint_to_pytorch.py
==================
770043eea;LysandreJik;2019-08-07 12:53:19 -0400;Sentence-pair tasks handling. Using common tests on RoBERTa. Forced push to fix indentation.

==

pytorch_transformers/__init__.py
pytorch_transformers/modeling_roberta.py
pytorch_transformers/tests/modeling_roberta_test.py
pytorch_transformers/tests/tokenization_roberta_test.py
pytorch_transformers/tokenization_roberta.py
==================
7729ef738;Thomas Wolf;2019-08-07 10:11:25 +0200;Merge pull request #955 from FeiWang96/master
Fix comment typo
==
==================
5c6ecf37e;Thomas Wolf;2019-08-07 10:10:20 +0200;Merge pull request #958 from saket404/typo-fix
Fixed small typo
==
==================
b4f9464f9;Thomas Wolf;2019-08-07 10:09:55 +0200;Merge pull request #960 from ethanjperez/patch-1
Fixing unused weight_decay argument
==
==================
822d6768e;Thomas Wolf;2019-08-07 10:09:20 +0200;Merge pull request #962 from guotong1988/patch-1
Update modeling_xlnet.py
==
==================
7e6102ce7;Thomas Wolf;2019-08-07 10:09:04 +0200;Merge pull request #963 from guotong1988/patch-2
Update modeling_bert.py
==
==================
3773ba44f;Thomas Wolf;2019-08-07 10:08:45 +0200;Merge pull request #977 from chrisgzf/master
Fixed typo in migration guide
==
==================
a80aa03bd;Thomas Wolf;2019-08-07 10:08:22 +0200;Merge pull request #973 from FeiWang96/bert_config
 Fix examples of loading pretrained models in docstring
==
==================
a6f412da0;Christopher Goh;2019-08-07 02:19:14 +0800;Fixed typo in migration guide

==

README.md
==================
6ec1ee9ec;wangfei;2019-08-06 11:32:54 +0800;Fix examples in docstring

==

pytorch_transformers/modeling_bert.py
pytorch_transformers/modeling_openai.py
pytorch_transformers/modeling_xlm.py
pytorch_transformers/modeling_xlnet.py
==================
72622926e;wangfei;2019-08-06 11:32:41 +0800;Fix examples in docstring

==

pytorch_transformers/modeling_transfo_xl.py
==================
f889e77b9;wangfei;2019-08-06 11:30:35 +0800;Fix examples of loading pretrained models in docstring

==

pytorch_transformers/modeling_gpt2.py
==================
beb03ec6c;wangfei;2019-08-06 11:15:57 +0800;Fix examples of loading pretrained models in docstring

==

pytorch_transformers/modeling_bert.py
pytorch_transformers/modeling_gpt2.py
pytorch_transformers/modeling_openai.py
pytorch_transformers/modeling_transfo_xl.py
pytorch_transformers/modeling_xlm.py
pytorch_transformers/modeling_xlnet.py
==================
4fc9f9ef5;Thomas Wolf;2019-08-05 19:17:47 +0200;Merge pull request #910 from huggingface/auto_models
Adding AutoTokenizer and AutoModel classes that automatically detect architecture - Clean up tokenizers
==
==================
d43dc48b3;Thomas Wolf;2019-08-05 19:17:35 +0200;Merge branch 'master' into auto_models

==
==================
0b524b084;thomwolf;2019-08-05 19:08:19 +0200;remove derived classes for now

==

docs/source/model_doc/auto.rst
pytorch_transformers/__init__.py
pytorch_transformers/modeling_auto.py
pytorch_transformers/tests/modeling_auto_test.py
==================
13936a962;thomwolf;2019-08-05 18:48:16 +0200;update doc and tests

==

docs/source/index.rst
docs/source/model_doc/auto.rst
pytorch_transformers/__init__.py
pytorch_transformers/tests/tokenization_auto_test.py
==================
ed4e54226;thomwolf;2019-08-05 18:14:07 +0200;adding tests

==

pytorch_transformers/__init__.py
pytorch_transformers/modeling_auto.py
pytorch_transformers/modeling_utils.py
pytorch_transformers/tests/modeling_auto_test.py
==================
3a126e73d;thomwolf;2019-08-05 17:26:29 +0200;fix #950

==

pytorch_transformers/convert_transfo_xl_checkpoint_to_pytorch.py
==================
7223886dc;thomwolf;2019-08-05 17:16:56 +0200;fix #944

==

README.md
==================
70c10caa0;thomwolf;2019-08-05 17:09:37 +0200;add option mentioned in #940

==

examples/run_glue.py
examples/run_squad.py
==================
077ad693e;thomwolf;2019-08-05 16:46:29 +0200;tweak issue templates wordings

==

.github/ISSUE_TEMPLATE/bug-report.md
.github/ISSUE_TEMPLATE/migration.md
==================
02d4087cb;thomwolf;2019-08-05 16:26:01 +0200;Merge branch 'master' of https://github.com/huggingface/pytorch-pretrained-BERT

==
==================
7c524d631;thomwolf;2019-08-05 16:25:54 +0200;add issue templates

==

.github/ISSUE_TEMPLATE/bug-report.md
.github/ISSUE_TEMPLATE/feature-request.md
.github/ISSUE_TEMPLATE/migration.md
.github/ISSUE_TEMPLATE/question-help.md
==================
6f05ad72b;Lysandre Debut;2019-08-05 10:18:00 -0400;Merge pull request #791 from huggingface/doc
RestructuredText table for pretrained models.
==
==================
b90e29d52;thomwolf;2019-08-05 16:06:34 +0200;working on automodels

==

docs/source/model_doc/auto.rst
examples/run_squad.py
pytorch_transformers/modeling_auto.py
pytorch_transformers/modeling_gpt2.py
pytorch_transformers/modeling_openai.py
pytorch_transformers/modeling_utils.py
==================
58830807d;thomwolf;2019-08-05 14:38:59 +0200;inidicate we only support pytorch 1.0.0+ now

==

README.md
requirements.txt
setup.py
==================
328afb709;thomwolf;2019-08-05 14:08:56 +0200;cleaning up tokenizer tests structure (at last) - last remaining ppb refs

==

README.md
docs/source/migration.md
docs/source/serialization.rst
docs/source/torchscript.rst
pytorch_transformers/__init__.py
pytorch_transformers/convert_pytorch_checkpoint_to_tf.py
pytorch_transformers/file_utils.py
pytorch_transformers/tests/tokenization_bert_test.py
pytorch_transformers/tests/tokenization_gpt2_test.py
pytorch_transformers/tests/tokenization_openai_test.py
pytorch_transformers/tests/tokenization_tests_commons.py
pytorch_transformers/tests/tokenization_transfo_xl_test.py
pytorch_transformers/tests/tokenization_xlm_test.py
pytorch_transformers/tests/tokenization_xlnet_test.py
pytorch_transformers/tokenization_bert.py
pytorch_transformers/tokenization_utils.py
==================
0e918707d;Thomas Wolf;2019-08-05 12:55:04 +0200;Merge pull request #907 from dhpollack/fix_convert_to_tf
Fix convert to tf
==
==================
cb9db101c;Julien Chaumond;2019-08-04 22:04:15 -0400;Python 2 must DIE

==

pytorch_transformers/modeling_roberta.py
pytorch_transformers/tests/tokenization_roberta_test.py
pytorch_transformers/tokenization_roberta.py
==================
05c083520;Julien Chaumond;2019-08-04 21:39:21 -0400;[RoBERTa] model conversion, inference, tests üî•

==

README.md
pytorch_transformers/convert_roberta_checkpoint_to_pytorch.py
pytorch_transformers/modeling_roberta.py
pytorch_transformers/tests/modeling_roberta_test.py
pytorch_transformers/tests/tokenization_roberta_test.py
pytorch_transformers/tokenization_roberta.py
==================
d7fd10568;Èõ∑Êâì‰∏çÂä®ÔºÅ;2019-08-05 08:58:19 +0800;Update modeling_bert.py

==

pytorch_transformers/modeling_bert.py
==================
84eb69908;Èõ∑Êâì‰∏çÂä®ÔºÅ;2019-08-05 08:57:09 +0800;Update modeling_xlnet.py

==

pytorch_transformers/modeling_xlnet.py
==================
00132b7a7;thomwolf;2019-08-04 22:42:55 +0200;updating docs - adding few tests to tokenizers

==

docs/source/index.rst
docs/source/main_classes/configuration.rst
docs/source/main_classes/model.rst
docs/source/main_classes/optimizer_schedules.rst
docs/source/main_classes/tokenizer.rst
docs/source/model_doc/overview.rst
docs/source/quickstart.md
docs/source/serialization.rst
pytorch_transformers/modeling_utils.py
pytorch_transformers/tokenization_utils.py
==================
28ba345ec;Ethan Perez;2019-08-04 12:31:46 -0400;Fixing unused weight_decay argument
Currently the L2 regularization is hard-coded to "0.01", even though there is a --weight_decay flag implemented (that is unused). I'm making this flag control the weight decay used for fine-tuning in this script.
==

examples/single_model_scripts/run_openai_gpt.py
==================
009273dbd;thomwolf;2019-08-04 12:14:57 +0200;big doc update [WIP]

==

README.md
docs/source/converting_tensorflow_models.rst
docs/source/index.rst
docs/source/installation.rst
docs/source/main_classes/configuration.rst
docs/source/main_classes/model.rst
docs/source/main_classes/optimizer_schedules.rst
docs/source/main_classes/tokenizer.rst
docs/source/migration.md
docs/source/model_doc/bert.rst
docs/source/quickstart.md
docs/source/serialization.rst
pytorch_transformers/__init__.py
pytorch_transformers/modeling_utils.py
pytorch_transformers/tokenization_bert.py
pytorch_transformers/tokenization_gpt2.py
pytorch_transformers/tokenization_transfo_xl.py
pytorch_transformers/tokenization_utils.py
pytorch_transformers/tokenization_xlnet.py
==================
836e51369;Saket Khandelwal;2019-08-04 16:05:10 +1000;Fixed small typo

==

pytorch_transformers/convert_gpt2_checkpoint_to_pytorch.py
pytorch_transformers/convert_openai_checkpoint_to_pytorch.py
pytorch_transformers/convert_tf_checkpoint_to_pytorch.py
pytorch_transformers/convert_xlnet_checkpoint_to_pytorch.py
==================
a24f83060;wangfei;2019-08-03 12:17:06 +0800;Fix comment typo

==

pytorch_transformers/modeling_bert.py
==================
44dd941ef;Julien Chaumond;2019-07-31 21:09:04 -0400;link to `swift-coreml-transformers`

==

README.md
docs/source/installation.rst
==================
f2a3eb987;Anthony MOI;2019-07-31 11:05:06 -0400;Fix small typos

==

README.md
==================
97091acb8;Pierric Cistac;2019-07-31 10:37:56 -0400;Small spelling fix

==

README.md
==================
769bb643c;Gr√©gory Ch√¢tel;2019-07-31 16:17:15 +0200;Fixing a broken link.

==

README.md
==================
c90119e54;David Pollack;2019-07-29 16:56:02 +0200;spelling mistake

==

pytorch_transformers/convert_pytorch_checkpoint_to_tf.py
==================
bfbe52ec3;thomwolf;2019-07-27 20:25:39 +0200;cleaning up example docstrings

==

hubconfs/bert_hubconf.py
hubconfs/gpt2_hubconf.py
hubconfs/gpt_hubconf.py
hubconfs/transformer_xl_hubconf.py
hubconfs/xlm_hubconf.py
hubconfs/xlnet_hubconf.1.py
pytorch_transformers/modeling_auto.py
pytorch_transformers/modeling_bert.py
pytorch_transformers/modeling_gpt2.py
pytorch_transformers/modeling_openai.py
pytorch_transformers/modeling_transfo_xl.py
pytorch_transformers/modeling_utils.py
pytorch_transformers/modeling_xlm.py
pytorch_transformers/modeling_xlnet.py
pytorch_transformers/tokenization_auto.py
==================
4cc1bf81e;thomwolf;2019-07-27 12:08:21 +0200;typos

==

pytorch_transformers/modeling_auto.py
pytorch_transformers/modeling_utils.py
pytorch_transformers/tokenization_bert.py
==================
ac27548b2;thomwolf;2019-07-27 11:50:47 +0200;fix unk_token test

==

pytorch_transformers/tokenization_gpt2.py
==================
c717d3857;thomwolf;2019-07-26 23:30:48 +0200;dictionnary => dictionary

==

docs/source/model_doc/overview.rst
docs/source/serialization.rst
hubconfs/bert_hubconf.py
hubconfs/gpt_hubconf.py
hubconfs/transformer_xl_hubconf.py
pytorch_transformers/modeling_utils.py
pytorch_transformers/tokenization_utils.py
==================
6b763d04a;Thomas Wolf;2019-07-26 21:36:21 +0200;Merge pull request #911 from huggingface/small_fixes
Small fixes
==
==================
7b6e474c9;thomwolf;2019-07-26 21:26:44 +0200;fix #901

==

pytorch_transformers/tokenization_utils.py
==================
632d71141;thomwolf;2019-07-26 21:14:37 +0200;fix #908

==

pytorch_transformers/__init__.py
==================
c054b5ee6;Thomas Wolf;2019-07-26 19:31:02 +0200;Merge pull request #896 from zijunsun/master
fix multi-gpu training bug when using fp16
==
==================
27b0f86d3;thomwolf;2019-07-26 17:09:21 +0200;clean up pretrained

==

pytorch_transformers/tokenization_utils.py
==================
57e54ec07;thomwolf;2019-07-26 17:09:07 +0200;add unk_token to gpt2

==

pytorch_transformers/tokenization_gpt2.py
==================
ac42049c0;thomwolf;2019-07-26 17:08:59 +0200;add auto models and auto tokenizer

==

pytorch_transformers/modeling_auto.py
pytorch_transformers/tokenization_auto.py
==================
09ecf225e;David Pollack;2019-07-26 15:20:44 +0200;fixed the fix.  tf session madness.

==

pytorch_transformers/convert_pytorch_checkpoint_to_tf.py
==================
edfd965ac;David Pollack;2019-07-26 14:13:46 +0200;fix convert_to_tf

==

pytorch_transformers/convert_pytorch_checkpoint_to_tf.py
==================
f0aeb7a81;zijunsun;2019-07-26 15:23:29 +0800;multi-gpu training also should be after apex fp16ÔºàsquadÔºâ

==

examples/run_squad.py
==================
46cc9dd2b;Thomas Wolf;2019-07-25 15:03:21 +0200;Merge pull request #899 from sukuya/master
Fixed import to use torchscript flag.
==
==================
6219ad721;Thomas Wolf;2019-07-25 15:01:22 +0200;Merge pull request #888 from rococode/patch-1
Update docs for parameter rename
==
==================
0b6122e96;Thomas Wolf;2019-07-25 14:59:59 +0200;Merge pull request #882 from Liangtaiwan/squad_v1_bug
fix squad v1 error (na_prob_file should be None)
==
==================
c244562ca;Thomas Wolf;2019-07-25 14:58:48 +0200;Merge pull request #893 from joelgrus/patch-2
make save_pretrained do the right thing with added tokens
==
==================
e1e2ab348;Sukuya;2019-07-25 16:53:11 +0800;Merge pull request #1 from sukuya/sukuya-patch-1
Update torchscript.rst
==
==================
35c52f2f3;Sukuya;2019-07-25 16:51:11 +0800;Update torchscript.rst
Import fixed to pytorch_transformers else torchscript flag can't be used.
==

docs/source/torchscript.rst
==================
adb3ef636;zijunsun;2019-07-25 13:09:10 +0800;multi-gpu training also should be after apex fp16

==

examples/run_glue.py
==================
ae152cec0;Joel Grus;2019-07-24 16:54:48 -0700;make save_pretrained work with added tokens
right now it's dumping the *decoder* when it should be dumping the *encoder*. this fixes that.
==

pytorch_transformers/tokenization_utils.py
==================
66b15f73f;rococo // Ron;2019-07-24 11:27:08 -0700;Update docs for parameter rename
OpenAIGPTLMHeadModel now accepts `labels` instead of `lm_labels`
==

pytorch_transformers/modeling_openai.py
==================
a7fce6d91;Chi-Liang Liu;2019-07-24 16:11:36 +0800;fix squad v1 error (na_prob_file should be None)

==

examples/run_squad.py
==================
067923d32;Thomas Wolf;2019-07-23 18:16:35 +0200;Merge pull request #873 from huggingface/identity_replacement
Add nn.Identity replacement for old PyTorch
==
==================
368670ac3;Thomas Wolf;2019-07-23 18:05:30 +0200;Merge pull request #866 from xanlsh/master
Rework how PreTrainedModel.from_pretrained handles its arguments
==
==================
1383c7b87;thomwolf;2019-07-23 17:52:20 +0200;Fix #869

==

pytorch_transformers/modeling_utils.py
==================
6070b5544;thomwolf;2019-07-23 17:46:01 +0200;fix #868

==

examples/run_glue.py
examples/run_squad.py
==================
2c9a3115b;thomwolf;2019-07-23 16:45:55 +0200;fix #858

==

examples/run_glue.py
==================
4fb56c772;Anish Moorthy;2019-07-23 10:41:02 -0400;Remove unused *args parameter from PreTrainedConfig.from_pretrained

==

pytorch_transformers/modeling_utils.py
==================
e179c5549;Anish Moorthy;2019-07-23 10:39:51 -0400;Add docs for from_pretrained functions, rename return_unused_args

==

pytorch_transformers/modeling_utils.py
==================
fec76a481;Thomas Wolf;2019-07-23 16:05:29 +0200;Update readme

==

README.md
==================
859c44177;Thomas Wolf;2019-07-23 16:03:06 +0200;Merge pull request #872 from huggingface/saving_schedules
Updating schedules for state_dict saving/loading
==
==================
0740e63e4;thomwolf;2019-07-23 15:57:18 +0200;updating schedules for state_dict saving

==

pytorch_transformers/optimization.py
pytorch_transformers/tests/optimization_test.py
==================
268c6cc16;Thomas Wolf;2019-07-23 15:29:31 +0200;Merge pull request #845 from rabeehk/master
fixed version issues in run_openai_gpt
==
==================
1d7d01c08;Thomas Wolf;2019-07-23 15:28:31 +0200;Merge pull request #847 from lpq29743/master
typos
==
==================
c4bc66886;Thomas Wolf;2019-07-23 15:24:25 +0200;Merge pull request #860 from Yiqing-Zhou/patch-1
read().splitlines() -> readlines()
==
==================
ba52fe69d;thomwolf;2019-07-23 15:10:02 +0200;update breaking change section regarding from_pretrained keyword arguments

==

README.md
==================
b1019d2a8;Yiqing-Zhou;2019-07-23 20:41:26 +0800;token[-1] -> token.rstrip('\n')

==

pytorch_transformers/tokenization_bert.py
==================
0227b4a94;thomwolf;2019-07-23 14:06:43 +0200;fix #827

==

pytorch_transformers/convert_xlm_checkpoint_to_pytorch.py
pytorch_transformers/modeling_bert.py
pytorch_transformers/modeling_gpt2.py
pytorch_transformers/modeling_openai.py
pytorch_transformers/modeling_transfo_xl.py
pytorch_transformers/modeling_transfo_xl_utilities.py
pytorch_transformers/modeling_xlm.py
pytorch_transformers/modeling_xlnet.py
==================
490ebbdcf;Anish Moorthy;2019-07-22 15:51:51 -0400;Fix PretrainedModel.from_pretrained not passing cache_dir forward

==

pytorch_transformers/modeling_utils.py
==================
b8009cb0d;Anish Moorthy;2019-07-22 17:56:27 -0400;Make PreTrainedModel.from_pretrained pass unused arguments to model

==

pytorch_transformers/modeling_utils.py
==================
bef0c629c;Yiqing-Zhou;2019-07-22 22:30:49 +0800;fix
Remove '\n' before adding token into vocab
==

pytorch_transformers/tokenization_bert.py
==================
897d0841b;Yiqing-Zhou;2019-07-22 20:49:09 +0800;read().splitlines() -> readlines()
splitlines() does not work as what we expect here for bert-base-chinese because there is a '\u2028' (unicode line seperator) token in vocab file. Value of '\u2028'.splitlines() is ['', ''].
Perhaps we should use readlines() instead.
==

pytorch_transformers/tokenization_bert.py
==================
2f869dc66;rish-16;2019-07-20 16:49:42 +0800;Fixed typo

==

README.md
==================
76be189b0;Peiqin Lin;2019-07-21 20:39:42 +0800;typos

==

examples/run_glue.py
examples/run_squad.py
==================
f63ff536a;Rabeeh KARIMI;2019-07-20 12:43:07 +0200;fixed version issues in run_openai_gpt

==

examples/single_model_scripts/run_openai_gpt.py
==================
a61549907;Thomas Wolf;2019-07-18 23:32:33 +0200;Merge pull request #797 from yzy5630/fix-examples
fix some errors for distributed lm_finetuning
==
==================
dbecfcf32;Thomas Wolf;2019-07-18 18:30:32 +0200;Merge pull request #815 from praateekmahajan/update-readme-link
Update Readme link for Fine Tune/Usage section
==
==================
acc48a0cc;Peiqin Lin;2019-07-18 14:15:38 +0800;typos

==

README.md
==================
a1fe4ba9c;yzy5630;2019-07-18 15:45:23 +0800;use new API for save and load

==

examples/lm_finetuning/finetune_on_pregenerated.py
examples/lm_finetuning/simple_lm_finetuning.py
==================
0d46b1755;Praateek Mahajan;2019-07-17 22:50:10 -0700;Update Readme
Incorrect link for `Quick tour: Fine-tuning/usage scripts`
==

README.md
==================
a7ba27b1b;yzy5630;2019-07-18 08:52:51 +0800;add parser for adam

==

examples/lm_finetuning/finetune_on_pregenerated.py
examples/lm_finetuning/simple_lm_finetuning.py
==================
c4e961569;Wei-Sheng Chin;2019-07-17 09:08:40 -0700;Fix a path so that test can run on Windows

==

pytorch_transformers/tests/modeling_common_test.py
==================
9d381e7be;LysandreJik;2019-07-17 09:25:38 -0400;Fixed incorrect links in the PretrainedModel

==

docs/source/pretrained_models.rst
==================
d6522e287;yzy5630;2019-07-17 21:22:34 +0800;change loss and optimizer to new API

==

examples/lm_finetuning/finetune_on_pregenerated.py
examples/lm_finetuning/simple_lm_finetuning.py
==================
71d597dad;thomwolf;2019-07-17 13:51:09 +0200;fix #800

==

examples/run_squad.py
==================
4bcddf6fc;Thomas Wolf;2019-07-17 12:31:26 +0200;Merge pull request #801 from bzantium/master
import sys twice
==
==================
506ab34d0;Thomas Wolf;2019-07-17 12:26:34 +0200;Merge pull request #796 from stefan-it/minor-doc-updates
Minor documentation updates
==
==================
cd8980e1f;Minho Ryu;2019-07-17 18:12:01 +0900;import sys twice

==

pytorch_transformers/file_utils.py
==================
123da5a2f;yzy5630;2019-07-17 09:56:07 +0800;fix errors for lm_finetuning examples

==

examples/lm_finetuning/finetune_on_pregenerated.py
==================
60a1bdcda;yzy5630;2019-07-17 09:16:20 +0800;fix some errors for distributed lm_finetuning

==

examples/lm_finetuning/simple_lm_finetuning.py
==================
e6cc6d237;Stefan Schweter;2019-07-16 23:42:28 +0200;docs: fix link to various notebooks

==

docs/source/notebooks.rst
==================
5b78400e2;Stefan Schweter;2019-07-16 23:41:57 +0200;docs: fix link to modeling example source (bert)

==

docs/source/model_doc/overview.rst
==================
61cc3ee35;Stefan Schweter;2019-07-16 23:41:04 +0200;docs: fix link to tf checkpoint to pytorch script

==

docs/source/converting_tensorflow_models.rst
==================
dbbd94cb7;Stefan Schweter;2019-07-16 23:40:04 +0200;docs: fix link to bertology example and update dataset description

==

docs/source/bertology.rst
==================
5fe0b378d;thomwolf;2019-07-16 21:35:53 +0200;adding missing docstring fix #793

==

pytorch_transformers/modeling_bert.py
==================
e848b5473;thomwolf;2019-07-16 21:22:19 +0200;fix #792

==

examples/single_model_scripts/run_transfo_xl.py
==================
c5b3d86a9;thomwolf;2019-07-16 21:21:05 +0200;Merge branch 'master' of https://github.com/huggingface/pytorch-pretrained-BERT

==
==================
6b7076020;thomwolf;2019-07-16 21:21:03 +0200;typos

==

README.md
setup.py
==================
117ed9299;LysandreJik;2019-07-16 11:58:47 -0400;RestructuredText table for pretrained models.

==

docs/source/pretrained_models.rst
==================
b33a38509;Thomas Wolf;2019-07-16 16:18:37 +0200;update readme

==

README.md
==================
ed7549bb1;thomwolf;2019-07-16 16:10:58 +0200;release version 1.0

==

pytorch_transformers/__init__.py
setup.py
==================
6a72d9aa5;thomwolf;2019-07-16 16:09:29 +0200;updated examples in readme

==

README.md
==================
b59043bf8;thomwolf;2019-07-16 16:03:48 +0200;update readme

==

README.md
==================
edc79acb3;thomwolf;2019-07-16 16:02:32 +0200;simpler quick tour

==

README.md
==================
5c82d3488;thomwolf;2019-07-16 15:45:58 +0200;indicate default evaluation in breaking changes

==

README.md
docs/source/migration.md
==================
4acaa6506;thomwolf;2019-07-16 15:41:57 +0200;model in evaluation mode by default after from_pretrained

==

README.md
pytorch_transformers/modeling_utils.py
==================
f289e6cfe;thomwolf;2019-07-16 15:31:21 +0200;fix docstrings

==

pytorch_transformers/modeling_bert.py
pytorch_transformers/modeling_gpt2.py
pytorch_transformers/modeling_openai.py
pytorch_transformers/modeling_transfo_xl.py
pytorch_transformers/modeling_xlm.py
pytorch_transformers/modeling_xlnet.py
==================
9726b229c;thomwolf;2019-07-16 15:17:45 +0200;model name typo

==

pytorch_transformers/modeling_xlm.py
==================
1849aa7d3;thomwolf;2019-07-16 15:11:29 +0200;update readme and pretrained model weight files

==

README.md
examples/requirements.txt
pytorch_transformers/tests/tokenization_tests_commons.py
pytorch_transformers/tokenization_transfo_xl.py
pytorch_transformers/tokenization_utils.py
pytorch_transformers/tokenization_xlm.py
==================
43e0e8fa0;thomwolf;2019-07-16 13:56:47 +0200;updates to readme and doc

==

README.md
docs/source/converting_tensorflow_models.rst
docs/source/index.rst
docs/source/installation.rst
docs/source/migration.md
docs/source/philosophy.md
docs/source/pretrained_models.rst
docs/source/quickstart.md
docs/source/usage.rst
==================
f31154cb9;thomwolf;2019-07-16 11:51:13 +0200;Merge branch 'xlnet'

==
==================
1b35d05d4;thomwolf;2019-07-16 09:41:55 +0200;update conversion scripts and __main__

==

pytorch_transformers/__main__.py
pytorch_transformers/convert_gpt2_checkpoint_to_pytorch.py
pytorch_transformers/convert_openai_checkpoint_to_pytorch.py
pytorch_transformers/convert_tf_checkpoint_to_pytorch.py
pytorch_transformers/convert_transfo_xl_checkpoint_to_pytorch.py
pytorch_transformers/convert_xlm_checkpoint_to_pytorch.py
pytorch_transformers/convert_xlnet_checkpoint_to_pytorch.py
pytorch_transformers/modeling_xlnet.py
pytorch_transformers/tokenization_transfo_xl.py
pytorch_transformers/tokenization_utils.py
pytorch_transformers/tokenization_xlnet.py
==================
352e3ff99;thomwolf;2019-07-16 09:03:49 +0200;added migration guide to readme

==

README.md
pytorch_transformers/modeling_utils.py
==================
8ad7e5b4f;thomwolf;2019-07-16 00:29:15 +0200;indeed

==

README.md
==================
064d0a0b7;thomwolf;2019-07-16 00:21:33 +0200;update readme

==

README.md
==================
3b8b0e01b;thomwolf;2019-07-16 00:12:55 +0200;update readme

==

README.md
docs/source/serialization.rst
pytorch_transformers/modeling_utils.py
pytorch_transformers/modeling_xlnet.py
==================
76da9765b;thomwolf;2019-07-15 17:52:35 +0200;fix run_generation test

==

examples/test_examples.py
==================
e691fc096;thomwolf;2019-07-15 17:45:24 +0200;update QA models tests + run_generation

==

examples/run_generation.py
examples/test_examples.py
pytorch_transformers/tests/modeling_xlm_test.py
pytorch_transformers/tests/modeling_xlnet_test.py
==================
15d8b1266;thomwolf;2019-07-15 17:30:42 +0200;update tokenizer - update squad example for xlnet

==

examples/run_glue.py
examples/run_squad.py
examples/test_examples.py
examples/utils_squad.py
pytorch_transformers/modeling_utils.py
pytorch_transformers/modeling_xlnet.py
pytorch_transformers/tests/tokenization_bert_test.py
pytorch_transformers/tests/tokenization_gpt2_test.py
pytorch_transformers/tests/tokenization_openai_test.py
pytorch_transformers/tests/tokenization_tests_commons.py
pytorch_transformers/tests/tokenization_transfo_xl_test.py
pytorch_transformers/tests/tokenization_xlm_test.py
pytorch_transformers/tests/tokenization_xlnet_test.py
pytorch_transformers/tokenization_bert.py
pytorch_transformers/tokenization_gpt2.py
pytorch_transformers/tokenization_openai.py
pytorch_transformers/tokenization_transfo_xl.py
pytorch_transformers/tokenization_utils.py
pytorch_transformers/tokenization_xlm.py
pytorch_transformers/tokenization_xlnet.py
==================
3b469cb42;thomwolf;2019-07-15 15:28:37 +0200;updating squad for compatibility with XLNet

==

examples/run_squad.py
examples/utils_squad.py
examples/utils_squad_evaluate.py
pytorch_transformers/modeling_utils.py
pytorch_transformers/modeling_xlnet.py
==================
8ca767f13;thomwolf;2019-07-15 13:49:07 +0200;clean up optimization

==

pytorch_transformers/optimization.py
==================
74a24f0fe;thomwolf;2019-07-15 13:49:01 +0200;clean up file_utils

==

pytorch_transformers/file_utils.py
==================
ab49fafc0;thomwolf;2019-07-15 12:51:23 +0200;update tokenization docstrings for #328

==

pytorch_transformers/tokenization_bert.py
==================
a9ab15174;thomwolf;2019-07-15 12:42:12 +0200;fix #328

==

pytorch_transformers/tokenization_bert.py
==================
f7cd7392f;thomwolf;2019-07-15 12:32:19 +0200;fixed tests

==

pytorch_transformers/modeling_bert.py
pytorch_transformers/modeling_gpt2.py
pytorch_transformers/modeling_openai.py
pytorch_transformers/modeling_transfo_xl.py
pytorch_transformers/modeling_utils.py
pytorch_transformers/modeling_xlm.py
pytorch_transformers/modeling_xlnet.py
==================
e28d8bde0;thomwolf;2019-07-15 12:08:06 +0200;doc on base classes

==

pytorch_transformers/modeling_utils.py
==================
44c985fac;thomwolf;2019-07-15 11:36:50 +0200;update doc for XLM and XLNet

==

pytorch_transformers/modeling_bert.py
pytorch_transformers/modeling_gpt2.py
pytorch_transformers/modeling_openai.py
pytorch_transformers/modeling_transfo_xl.py
pytorch_transformers/modeling_utils.py
pytorch_transformers/modeling_xlm.py
pytorch_transformers/modeling_xlnet.py
==================
0201d8601;thomwolf;2019-07-15 10:11:09 +0200;added doc for transformer-xl

==

pytorch_transformers/modeling_gpt2.py
pytorch_transformers/modeling_openai.py
pytorch_transformers/modeling_transfo_xl.py
==================
4cb489457;thomwolf;2019-07-15 09:58:01 +0200;added doc for openai GPT

==

pytorch_transformers/modeling_bert.py
pytorch_transformers/modeling_openai.py
==================
62b8eb43c;thomwolf;2019-07-15 09:49:02 +0200;fix add_start_docstrings on python 2 (removed)

==

pytorch_transformers/modeling_bert.py
pytorch_transformers/modeling_gpt2.py
pytorch_transformers/modeling_utils.py
==================
5bc3d0cc5;thomwolf;2019-07-15 09:40:05 +0200;added gpt2 doc

==

pytorch_transformers/modeling_bert.py
pytorch_transformers/modeling_gpt2.py
==================
183fedfed;thomwolf;2019-07-15 09:00:09 +0200;fix doc on python2

==

pytorch_transformers/modeling_bert.py
==================
0e9825e25;thomwolf;2019-07-14 23:43:28 +0200;small fix to run_glue

==

examples/run_glue.py
==================
2397f958f;thomwolf;2019-07-14 23:20:10 +0200;updating examples and doc

==

README.md
docs/source/index.rst
docs/source/model_doc/bert.rst
docs/source/model_doc/gpt.rst
docs/source/model_doc/overview.rst
examples/lm_finetuning/finetune_on_pregenerated.py
examples/run_bertology.py
examples/run_generation.py
examples/run_glue.py
examples/run_squad.py
examples/single_model_scripts/run_openai_gpt.py
examples/single_model_scripts/run_swag.py
examples/single_model_scripts/run_transfo_xl.py
examples/test_examples.py
pytorch_transformers/modeling_bert.py
pytorch_transformers/modeling_utils.py
==================
c490f5ce8;thomwolf;2019-07-13 15:26:58 +0200;added generation examples in tests

==

examples/test_examples.py
==================
8bb02c27e;thomwolf;2019-07-13 15:25:06 +0200;Merge branch 'xlnet' of https://github.com/huggingface/pytorch-pretrained-BERT into xlnet

==
==================
7d4b200e4;thomwolf;2019-07-13 15:25:03 +0200;good quality generation example for GPT, GPT-2, Transfo-XL, XLNet

==

examples/run_generation.py
examples/test_examples.py
pytorch_transformers/modeling_gpt2.py
pytorch_transformers/modeling_openai.py
pytorch_transformers/modeling_transfo_xl.py
pytorch_transformers/modeling_xlnet.py
pytorch_transformers/tests/modeling_xlnet_test.py
pytorch_transformers/tokenization_transfo_xl.py
pytorch_transformers/tokenization_utils.py
pytorch_transformers/tokenization_xlnet.py
==================
69dc01093;Thomas Wolf;2019-07-13 12:08:57 +0200;Merge pull request #786 from huggingface/doc-sphinx
New documentation for pytorch-transformers
==
==================
7322c314a;thomwolf;2019-07-12 14:24:08 +0200;remove python2 testing for examples

==

.circleci/config.yml
examples/run_bert_swag.py
==================
936e813c8;thomwolf;2019-07-12 14:16:06 +0200;clean up examples - added squad example and test

==

examples/generation_xlnet.py
examples/run_bert_extract_features.py
examples/run_bert_squad.py
examples/run_bertology.py
examples/run_glue.py
examples/run_gpt2.py
examples/run_squad.py
examples/run_swag.py
examples/run_xlnet_squad.py
examples/test_examples.py
examples/tests_samples/.gitignore
examples/tests_samples/SQUAD/dev-v2.0-small.json
examples/utils_squad.py
examples/utils_squad_evaluate.py
==================
699bc7e86;thomwolf;2019-07-12 11:46:57 +0200;fix gpt-2 unk token test

==

docs/README.md
pytorch_transformers/tokenization_gpt2.py
==================
762ded9b1;thomwolf;2019-07-12 11:28:52 +0200;wip examples

==

examples/run_bert_squad.py
examples/run_glue.py
examples/run_squad.py
==================
744295636;thomwolf;2019-07-12 11:26:16 +0200;save config file

==

pytorch_transformers/modeling_utils.py
==================
292140b92;Thomas Wolf;2019-07-12 11:10:25 +0200;Merge pull request #781 from huggingface/embeddings
Clean up input embeddings resizing and weights tying
==
==================
c57e9d946;Thomas Wolf;2019-07-12 11:10:14 +0200;Merge branch 'xlnet' into embeddings

==
==================
2918b7d2a;thomwolf;2019-07-12 10:57:58 +0200;updating tests

==

pytorch_transformers/modeling_bert.py
pytorch_transformers/modeling_gpt2.py
pytorch_transformers/modeling_openai.py
pytorch_transformers/modeling_transfo_xl.py
pytorch_transformers/modeling_utils.py
pytorch_transformers/modeling_xlm.py
pytorch_transformers/modeling_xlnet.py
pytorch_transformers/tests/modeling_bert_test.py
pytorch_transformers/tests/modeling_common_test.py
pytorch_transformers/tests/modeling_gpt2_test.py
pytorch_transformers/tests/modeling_openai_test.py
pytorch_transformers/tests/modeling_transfo_xl_test.py
pytorch_transformers/tests/modeling_xlm_test.py
pytorch_transformers/tests/modeling_xlnet_test.py
==================
3fbceed8d;LysandreJik;2019-07-11 22:29:55 -0400;Fix layer reference loss + previous attempted fix

==

pytorch_transformers/modeling_bert.py
pytorch_transformers/modeling_gpt2.py
pytorch_transformers/modeling_openai.py
pytorch_transformers/tests/modeling_common_test.py
==================
6c2ee16c0;LysandreJik;2019-07-11 22:09:16 -0400;Test suite testing the tie_weights function as well as the resize_token_embeddings function. Patched an issue relating to the tied weights I had introduced with the TorchScript addition. Byte order mark management in TSV glue reading.

==

examples/utils_glue.py
pytorch_transformers/modeling_bert.py
pytorch_transformers/modeling_gpt2.py
pytorch_transformers/modeling_openai.py
pytorch_transformers/tests/modeling_common_test.py
==================
3821ecbf4;LysandreJik;2019-07-11 20:16:28 -0400;Byte order mark management in TSV glue reading.

==

examples/utils_glue.py
==================
e3fb4310d;LysandreJik;2019-07-11 18:44:29 -0400;From pretrained correct initialization. Unknown token handling for gpt2.

==

pytorch_transformers/modeling_gpt2.py
pytorch_transformers/modeling_openai.py
pytorch_transformers/tokenization_gpt2.py
==================
bd404735a;thomwolf;2019-07-12 00:02:49 +0200;embeddings resizing + tie_weights

==

pytorch_transformers/modeling_bert.py
pytorch_transformers/modeling_gpt2.py
pytorch_transformers/modeling_openai.py
pytorch_transformers/modeling_transfo_xl.py
pytorch_transformers/modeling_utils.py
pytorch_transformers/modeling_xlm.py
pytorch_transformers/modeling_xlnet.py
pytorch_transformers/tests/modeling_bert_test.py
pytorch_transformers/tests/modeling_common_test.py
pytorch_transformers/tests/modeling_gpt2_test.py
pytorch_transformers/tests/modeling_openai_test.py
pytorch_transformers/tests/modeling_transfo_xl_test.py
pytorch_transformers/tests/modeling_utils_test.py
pytorch_transformers/tests/modeling_xlm_test.py
pytorch_transformers/tests/modeling_xlnet_test.py
==================
50e62a4cb;LysandreJik;2019-07-11 16:50:21 -0400;fix gpt/gpt-2 from pretrained

==

pytorch_transformers/modeling_gpt2.py
==================
273617b86;thomwolf;2019-07-11 22:45:03 +0200;update config - fix gpt/gpt-2 from pretrained

==

.circleci/config.yml
pytorch_transformers/modeling_gpt2.py
pytorch_transformers/modeling_openai.py
==================
6b13f4cb3;thomwolf;2019-07-11 22:36:35 +0200;update circle-ci

==

.circleci/config.yml
==================
2b644785f;thomwolf;2019-07-11 22:31:50 +0200;add tests on examples and large circle ci config

==

.circleci/config.yml
==================
c6bf1a400;thomwolf;2019-07-11 22:29:08 +0200;fix test examples et model pretrained

==

examples/test_examples.py
pytorch_transformers/modeling_utils.py
pytorch_transformers/tests/modeling_utils_test.py
==================
92a782b10;thomwolf;2019-07-11 22:20:10 +0200;fix run_glue test

==

examples/run_glue.py
pytorch_transformers/optimization.py
==================
6491575fd;LysandreJik;2019-07-11 12:38:21 -0400;Added TorchScript disclaimer. CSS modifications.

==

docs/source/_static/css/huggingface.css
docs/source/conf.py
docs/source/examples.rst
docs/source/torchscript.rst
==================
ccb6947dc;thomwolf;2019-07-11 17:39:47 +0200;optimization tests

==

examples/run_glue.py
examples/test_examples.py
pytorch_transformers/optimization.py
pytorch_transformers/tests/optimization_test.py
==================
e4f9dca01;Thomas Wolf;2019-07-11 15:46:39 +0200;Merge pull request #773 from huggingface/doc-sphinx
Sphinx doc, XLM Checkpoints
==
==================
b87eb82b4;Thomas Wolf;2019-07-11 15:46:27 +0200;Merge branch 'xlnet' into doc-sphinx

==
==================
d216e798a;Thomas Wolf;2019-07-11 15:43:47 +0200;Merge pull request #777 from huggingface/examples
Working GLUE Example for XLNet (STS-B) 
==
==================
6135de2fa;thomwolf;2019-07-11 15:39:49 +0200;readme update

==

README.md
==================
b21d84b02;thomwolf;2019-07-11 15:37:34 +0200;update examples

==

examples/run_bert_classifier.py
examples/run_glue.py
examples/run_xlnet_classifier.py
examples/utils.py
pytorch_transformers/optimization.py
==================
ec07cf5a6;thomwolf;2019-07-11 14:48:22 +0200;rewamp optimization

==

examples/run_glue.py
pytorch_transformers/__init__.py
pytorch_transformers/modeling_utils.py
pytorch_transformers/modeling_xlnet.py
pytorch_transformers/optimization.py
pytorch_transformers/optimization_openai.py
pytorch_transformers/tests/optimization_test.py
==================
4fef5919a;thomwolf;2019-07-11 12:03:08 +0200;updating examples

==

examples/run_glue.py
examples/utils_glue.py
pytorch_transformers/modeling_bert.py
pytorch_transformers/modeling_gpt2.py
pytorch_transformers/modeling_openai.py
pytorch_transformers/modeling_transfo_xl.py
pytorch_transformers/modeling_transfo_xl_utilities.py
pytorch_transformers/modeling_utils.py
pytorch_transformers/modeling_xlnet.py
pytorch_transformers/tokenization_transfo_xl.py
==================
7fdbc4782;LysandreJik;2019-07-10 19:37:24 -0400;Added the two CLM XLM pretrained checkpoints. Fixed file extensions for config/vocab/merges of XLM models.

==

pytorch_transformers/modeling_xlm.py
pytorch_transformers/tokenization_xlm.py
==================
dee3e45b9;LysandreJik;2019-07-10 19:04:21 -0400;Fixed XLM weights conversion script. Added 5 new checkpoints for XLM.

==

pytorch_transformers/convert_xlm_checkpoint_to_pytorch.py
pytorch_transformers/modeling_xlm.py
pytorch_transformers/tokenization_xlm.py
==================
c82b74b99;LysandreJik;2019-07-10 15:30:19 -0400;Fixed Sphinx errors and warnings

==

docs/source/index.rst
docs/source/model_doc/overview.rst
docs/source/notebooks.rst
==================
5288913bd;LysandreJik;2019-07-10 15:16:40 -0400;All TODOs to be checked by Thom have been added.

==

pytorch_transformers/modeling_gpt2.py
pytorch_transformers/modeling_openai.py
pytorch_transformers/modeling_xlnet.py
==================
f773faa25;LysandreJik;2019-07-10 14:45:56 -0400;Fixed all links. Removed TPU. Changed CLI to Converting TF models. Many minor formatting adjustments. Added "TODO Lysandre filled" where necessary.

==

docs/source/_static/css/huggingface.css
docs/source/bertology.md
docs/source/bertology.rst
docs/source/converting_tensorflow_models.rst
docs/source/examples.rst
docs/source/index.rst
docs/source/model_doc/bert.rst
docs/source/model_doc/gpt.rst
docs/source/model_doc/gpt2.rst
docs/source/model_doc/overview.rst
docs/source/model_doc/transformerxl.rst
docs/source/model_doc/xlm.rst
docs/source/model_doc/xlnet.rst
docs/source/notebooks.rst
docs/source/tpu.rst
docs/source/usage.rst
pytorch_transformers/modeling_bert.py
pytorch_transformers/tokenization_xlm.py
pytorch_transformers/tokenization_xlnet.py
==================
50b7e52a7;thomwolf;2019-07-10 15:33:34 +0200;WIP examples

==

examples/run_glue.py
examples/run_squad.py
examples/utils.py
pytorch_transformers/modeling_xlnet.py
pytorch_transformers/optimization.py
==================
3f56ad5af;LysandreJik;2019-07-09 18:50:59 -0400;Updated CircleCI's config.yml to use a large resource class.

==

.circleci/config.yml
==================
c4bab2dc8;LysandreJik;2019-07-09 18:03:01 -0400;Added footer with social links.

==

docs/source/_static/css/huggingface.css
docs/source/_static/js/custom.js
==================
331db8cc0;LysandreJik;2019-07-09 17:01:56 -0400;Added viewcode plugin for source code visualization within the static website.

==

docs/source/_static/css/code-snippets.css
docs/source/_static/css/huggingface.css
docs/source/conf.py
==================
83fb311ef;LysandreJik;2019-07-09 16:38:30 -0400;Patched warnings + Refactored XLNet's Docstrings

==

docs/source/examples.rst
docs/source/index.rst
docs/source/model_doc/overview.rst
docs/source/model_doc/xlm.rst
docs/source/model_doc/xlnet.rst
pytorch_transformers/modeling_bert.py
pytorch_transformers/modeling_xlnet.py
==================
8fe2c9d98;LysandreJik;2019-07-09 15:55:31 -0400;Refactored Docstrings of BERT, GPT2, GPT, TransfoXL, XLM and XLNet.

==

docs/source/cli.rst
docs/source/model_doc/bert.rst
docs/source/model_doc/gpt.rst
docs/source/model_doc/gpt2.rst
docs/source/model_doc/transformerxl.rst
docs/source/model_doc/xlm.rst
docs/source/usage.rst
pytorch_transformers/modeling_bert.py
pytorch_transformers/modeling_gpt2.py
pytorch_transformers/modeling_openai.py
pytorch_transformers/modeling_transfo_xl.py
pytorch_transformers/modeling_xlm.py
pytorch_transformers/modeling_xlnet.py
==================
ed6c8d37f;thomwolf;2019-07-09 17:14:52 +0200;fix merge

==

examples/run_glue.py
pytorch_transformers/modeling_bert.py
==================
e468192e2;thomwolf;2019-07-09 17:05:37 +0200;Merge branch 'pytorch-transformers' into xlnet

==
==================
4ce237c88;thomwolf;2019-07-09 17:00:32 +0200;update run_glue

==

examples/run_glue.py
==================
9dd2c8603;Thomas Wolf;2019-07-09 16:56:34 +0200;Merge pull request #767 from huggingface/doc
Documentation
==
==================
e0e5c7faf;LysandreJik;2019-07-09 10:16:09 -0400;Added requirements.txt file.

==

docs/README.md
docs/requirements.txt
==================
3b7cb7bf4;thomwolf;2019-07-09 16:12:15 +0200;small update to run_glue

==

examples/run_glue.py
==================
269e73b60;LysandreJik;2019-07-09 10:11:29 -0400;Adding example detailing how to add a new file to the documentation + adding fonts.

==

docs/README.md
docs/source/_static/css/Calibre-Light.ttf
docs/source/_static/css/Calibre-Medium.otf
docs/source/_static/css/Calibre-Regular.otf
docs/source/_static/css/Calibre-Thin.otf
docs/source/bertology.md
docs/source/conf.py
docs/source/index.rst
docs/source/migration.md
docs/source/philosophy.md
docs/source/torchscript.rst
==================
d743f2f34;thomwolf;2019-07-09 15:58:58 +0200;updating test

==

pytorch_transformers/tests/tokenization_tests_commons.py
==================
d0efbd3cd;thomwolf;2019-07-09 15:46:43 +0200;update sequencesummary module

==

.coveragerc
examples/test_examples.py
pytorch_transformers/modeling_gpt2.py
pytorch_transformers/modeling_openai.py
pytorch_transformers/tests/modeling_tests_commons.py
==================
d5481cbe1;thomwolf;2019-07-09 15:29:42 +0200;adding tests to examples - updating summary module - coverage update

==

.coveragerc
.gitignore
examples/run_glue.py
examples/test_examples.py
examples/tests_samples/.gitignore
examples/tests_samples/MRPC/dev.tsv
examples/tests_samples/MRPC/train.tsv
pytorch_transformers/modeling_bert.py
pytorch_transformers/modeling_gpt2.py
pytorch_transformers/modeling_openai.py
pytorch_transformers/modeling_transfo_xl.py
pytorch_transformers/modeling_utils.py
pytorch_transformers/modeling_xlm.py
pytorch_transformers/modeling_xlnet.py
pytorch_transformers/tokenization_bert.py
pytorch_transformers/tokenization_utils.py
pytorch_transformers/tokenization_xlm.py
==================
c079d7ddf;thomwolf;2019-07-09 10:40:59 +0200;fix python 2 tests

==

pytorch_transformers/tests/tokenization_bert_test.py
pytorch_transformers/tests/tokenization_gpt2_test.py
pytorch_transformers/tests/tokenization_openai_test.py
pytorch_transformers/tests/tokenization_tests_commons.py
pytorch_transformers/tests/tokenization_transfo_xl_test.py
pytorch_transformers/tests/tokenization_xlm_test.py
pytorch_transformers/tests/tokenization_xlnet_test.py
pytorch_transformers/tokenization_utils.py
==================
b19786985;thomwolf;2019-07-09 10:25:18 +0200;unified tokenizer api and serialization + tests

==

examples/run_glue.py
examples/run_xlnet_classifier.py
examples/utils_glue.py
pytorch_transformers/__init__.py
pytorch_transformers/convert_transfo_xl_checkpoint_to_pytorch.py
pytorch_transformers/convert_xlm_checkpoint_to_pytorch.py
pytorch_transformers/modeling_bert.py
pytorch_transformers/modeling_gpt2.py
pytorch_transformers/modeling_openai.py
pytorch_transformers/modeling_transfo_xl.py
pytorch_transformers/modeling_utils.py
pytorch_transformers/modeling_xlm.py
pytorch_transformers/modeling_xlnet.py
pytorch_transformers/tests/modeling_bert_test.py
pytorch_transformers/tests/modeling_tests_commons.py
pytorch_transformers/tests/modeling_transfo_xl_test.py
pytorch_transformers/tests/modeling_utils_test.py
pytorch_transformers/tests/modeling_xlm_test.py
pytorch_transformers/tests/modeling_xlnet_test.py
pytorch_transformers/tests/tokenization_bert_test.py
pytorch_transformers/tests/tokenization_gpt2_test.py
pytorch_transformers/tests/tokenization_openai_test.py
pytorch_transformers/tests/tokenization_tests_commons.py
pytorch_transformers/tests/tokenization_transfo_xl_test.py
pytorch_transformers/tests/tokenization_utils_test.py
pytorch_transformers/tests/tokenization_xlm_test.py
pytorch_transformers/tests/tokenization_xlnet_test.py
pytorch_transformers/tokenization_bert.py
pytorch_transformers/tokenization_gpt2.py
pytorch_transformers/tokenization_openai.py
pytorch_transformers/tokenization_transfo_xl.py
pytorch_transformers/tokenization_utils.py
pytorch_transformers/tokenization_xlm.py
pytorch_transformers/tokenization_xlnet.py
==================
6847e30e1;LysandreJik;2019-07-08 17:34:24 -0400;New page detailing the use of TorchScript.

==

docs/source/index.rst
docs/source/torchscript.rst
==================
ab3065180;LysandreJik;2019-07-08 16:05:26 -0400;Hugging Face theme.

==

README.md
docs/source/_static/css/code-snippets.css
docs/source/_static/css/huggingface.css
docs/source/_static/js/custom.js
docs/source/_static/js/huggingface_logo.svg
docs/source/conf.py
docs/source/imgs/warmup_constant_schedule.png
docs/source/imgs/warmup_cosine_hard_restarts_schedule.png
docs/source/imgs/warmup_cosine_schedule.png
docs/source/imgs/warmup_cosine_warm_restarts_schedule.png
docs/source/imgs/warmup_linear_schedule.png
docs/source/index.rst
docs/source/model_doc/overview.rst
==================
a60ae1a50;LysandreJik;2019-07-08 11:50:32 -0400;Docstrings best practice shown in the BERT documentation.

==

pytorch_pretrained_bert/modeling_bert.py
pytorch_pretrained_bert/optimization.py
pytorch_pretrained_bert/tokenization_bert.py
==================
64fd98637;LysandreJik;2019-07-05 17:44:59 -0400;Tokenizers and Config classes are referenced.

==

docs/source/model_doc/bert.rst
docs/source/model_doc/gpt.rst
docs/source/model_doc/gpt2.rst
docs/source/model_doc/transformerxl.rst
docs/source/model_doc/xlm.rst
docs/source/model_doc/xlnet.rst
==================
df759114c;LysandreJik;2019-07-05 17:35:26 -0400;Single file documentation for each model, accompanied by the Documentation overview.

==

docs/index.rst
docs/source/index.rst
docs/source/model_doc/bert.rst
docs/source/model_doc/gpt.rst
docs/source/model_doc/gpt2.rst
docs/source/model_doc/overview.rst
docs/source/model_doc/transformerxl.rst
docs/source/model_doc/xlm.rst
docs/source/model_doc/xlnet.rst
==================
03de9686a;LysandreJik;2019-07-05 17:11:13 -0400;Initial folder structure for the documentation. A draft of documentation change has been made in the BertModel class.

==

docs/Makefile
docs/README.md
docs/source/cli.rst
docs/source/conf.py
docs/source/doc.rst
docs/source/examples.rst
docs/source/index.rst
docs/source/installation.rst
docs/source/notebooks.rst
docs/source/tpu.rst
docs/source/usage.rst
pytorch_pretrained_bert/modeling_bert.py
==================
3d5f29138;thomwolf;2019-07-05 17:22:15 +0200;updates to run_glue

==

examples/run_glue.py
==================
99b90edab;thomwolf;2019-07-05 17:09:35 +0200;cleaning up run_glue example

==

examples/run_glue.py
==================
1113f97f3;thomwolf;2019-07-05 16:31:13 +0200;clean up glue example

==

examples/run_bert_classifier.py
examples/run_glue.py
examples/utils_glue.py
pytorch_transformers/tokenization_utils.py
==================
162ba383b;thomwolf;2019-07-05 15:57:14 +0200;fix model loading

==

examples/run_bert_classifier.py
pytorch_transformers/modeling_utils.py
pytorch_transformers/tests/modeling_utils_test.py
==================
6dacc79d3;thomwolf;2019-07-05 15:11:59 +0200;fix python2 tests

==

pytorch_transformers/tests/tokenization_tests_commons.py
pytorch_transformers/tokenization_utils.py
==================
36bca545f;thomwolf;2019-07-05 15:02:59 +0200;tokenization abstract class - tests for examples

==

examples/run_squad.py
examples/test_examples.py
pytorch_transformers/__init__.py
pytorch_transformers/modeling_bert.py
pytorch_transformers/modeling_gpt2.py
pytorch_transformers/modeling_openai.py
pytorch_transformers/modeling_transfo_xl.py
pytorch_transformers/modeling_utils.py
pytorch_transformers/modeling_xlm.py
pytorch_transformers/modeling_xlnet.py
pytorch_transformers/tests/model_utils_test.py
pytorch_transformers/tests/modeling_bert_test.py
pytorch_transformers/tests/modeling_gpt2_test.py
pytorch_transformers/tests/modeling_openai_test.py
pytorch_transformers/tests/modeling_tests_commons.py
pytorch_transformers/tests/modeling_transfo_xl_test.py
pytorch_transformers/tests/modeling_utils_test.py
pytorch_transformers/tests/modeling_xlm_test.py
pytorch_transformers/tests/modeling_xlnet_test.py
pytorch_transformers/tests/tokenization_bert_test.py
pytorch_transformers/tests/tokenization_gpt2_test.py
pytorch_transformers/tests/tokenization_openai_test.py
pytorch_transformers/tests/tokenization_transfo_xl_test.py
pytorch_transformers/tests/tokenization_utils_test.py
pytorch_transformers/tests/tokenization_xlm_test.py
pytorch_transformers/tests/tokenization_xlnet_test.py
pytorch_transformers/tokenization_bert.py
pytorch_transformers/tokenization_gpt2.py
pytorch_transformers/tokenization_openai.py
pytorch_transformers/tokenization_transfo_xl.py
pytorch_transformers/tokenization_utils.py
pytorch_transformers/tokenization_xlm.py
pytorch_transformers/tokenization_xlnet.py
==================
a4f980547;thomwolf;2019-07-05 12:31:34 +0200;remove circle ci parallelism

==

.circleci/config.yml
==================
eb91f6437;thomwolf;2019-07-05 12:30:15 +0200;update readme and setup

==

README.md
setup.py
==================
78462aad6;Thomas Wolf;2019-07-05 12:04:30 +0200;Merge pull request #733 from ceremonious/parallel-generation
Added option to use multiple workers to create training data
==
==================
781124b0d;Thomas Wolf;2019-07-05 12:01:17 +0200;Merge pull request #620 from chrislarson1/convert-back-to-tf
Convert pytorch models back to tensorflow
==
==================
e5fe2bb5e;Thomas Wolf;2019-07-05 12:00:04 +0200;Merge pull request #745 from leimao/leimao
fix evaluation bug
==
==================
0231ba291;thomwolf;2019-07-05 11:59:04 +0200;circle-ci

==

README.md
==================
0bab55d5d;thomwolf;2019-07-05 11:55:36 +0200;[BIG] name change

==

.circleci/config.yml
.coveragerc
README.md
docker/Dockerfile
examples/bertology.py
examples/generation_xlnet.py
examples/lm_finetuning/finetune_on_pregenerated.py
examples/lm_finetuning/pregenerate_training_data.py
examples/lm_finetuning/simple_lm_finetuning.py
examples/run_bert_classifier.py
examples/run_bert_extract_features.py
examples/run_bert_squad.py
examples/run_bert_swag.py
examples/run_gpt2.py
examples/run_openai_gpt.py
examples/run_transfo_xl.py
examples/run_xlnet_classifier.py
examples/run_xlnet_squad.py
examples/tests/examples_tests.py
examples/utils_squad.py
hubconfs/bert_hubconf.py
hubconfs/gpt2_hubconf.py
hubconfs/gpt_hubconf.py
hubconfs/transformer_xl_hubconf.py
hubconfs/xlm_hubconf.py
hubconfs/xlnet_hubconf.1.py
notebooks/Comparing-TF-and-PT-models-MLM-NSP.ipynb
notebooks/Comparing-TF-and-PT-models.ipynb
pytorch_transformers/__init__.py
pytorch_transformers/__main__.py
pytorch_transformers/convert_gpt2_checkpoint_to_pytorch.py
pytorch_transformers/convert_openai_checkpoint_to_pytorch.py
pytorch_transformers/convert_tf_checkpoint_to_pytorch.py
pytorch_transformers/convert_transfo_xl_checkpoint_to_pytorch.py
pytorch_transformers/convert_xlm_checkpoint_to_pytorch.py
pytorch_transformers/convert_xlnet_checkpoint_to_pytorch.py
pytorch_transformers/file_utils.py
pytorch_transformers/model_utils.py
pytorch_transformers/modeling_bert.py
pytorch_transformers/modeling_gpt2.py
pytorch_transformers/modeling_openai.py
pytorch_transformers/modeling_transfo_xl.py
pytorch_transformers/modeling_transfo_xl_utilities.py
pytorch_transformers/modeling_xlm.py
pytorch_transformers/modeling_xlnet.py
pytorch_transformers/optimization.py
pytorch_transformers/optimization_openai.py
pytorch_transformers/tests/__init__.py
pytorch_transformers/tests/conftest.py
pytorch_transformers/tests/fixtures/input.txt
pytorch_transformers/tests/fixtures/sample_text.txt
pytorch_transformers/tests/fixtures/test_sentencepiece.model
pytorch_transformers/tests/model_tests_commons.py
pytorch_transformers/tests/model_utils_test.py
pytorch_transformers/tests/modeling_bert_test.py
pytorch_transformers/tests/modeling_gpt2_test.py
pytorch_transformers/tests/modeling_openai_test.py
pytorch_transformers/tests/modeling_transfo_xl_test.py
pytorch_transformers/tests/modeling_xlm_test.py
pytorch_transformers/tests/modeling_xlnet_test.py
pytorch_transformers/tests/optimization_test.py
pytorch_transformers/tests/tokenization_bert_test.py
pytorch_transformers/tests/tokenization_gpt2_test.py
pytorch_transformers/tests/tokenization_openai_test.py
pytorch_transformers/tests/tokenization_tests_commons.py
pytorch_transformers/tests/tokenization_transfo_xl_test.py
pytorch_transformers/tests/tokenization_xlm_test.py
pytorch_transformers/tests/tokenization_xlnet_test.py
pytorch_transformers/tokenization_bert.py
pytorch_transformers/tokenization_gpt2.py
pytorch_transformers/tokenization_openai.py
pytorch_transformers/tokenization_transfo_xl.py
pytorch_transformers/tokenization_xlm.py
pytorch_transformers/tokenization_xlnet.py
setup.py
==================
9113b50c9;thomwolf;2019-07-05 11:31:51 +0200;hubs [WIP]

==

hubconfs/xlm_hubconf.py
hubconfs/xlnet_hubconf.1.py
==================
175fce0a5;Thomas Wolf;2019-07-05 11:22:03 +0200;Merge pull request #758 from huggingface/doc
Release 0.7 - Add tokenizer API + tests
==
==================
e75c3f70a;thomwolf;2019-07-05 11:20:27 +0200;standardizing tokenizers API and adding tests

==

pytorch_pretrained_bert/model_utils.py
pytorch_pretrained_bert/modeling_bert.py
pytorch_pretrained_bert/tests/tokenization_bert_test.py
pytorch_pretrained_bert/tests/tokenization_gpt2_test.py
pytorch_pretrained_bert/tests/tokenization_openai_test.py
pytorch_pretrained_bert/tests/tokenization_tests_commons.py
pytorch_pretrained_bert/tests/tokenization_transfo_xl_test.py
pytorch_pretrained_bert/tests/tokenization_xlm_test.py
pytorch_pretrained_bert/tests/tokenization_xlnet_test.py
pytorch_pretrained_bert/tokenization_bert.py
pytorch_pretrained_bert/tokenization_gpt2.py
pytorch_pretrained_bert/tokenization_openai.py
pytorch_pretrained_bert/tokenization_transfo_xl.py
pytorch_pretrained_bert/tokenization_xlm.py
pytorch_pretrained_bert/tokenization_xlnet.py
==================
c0239e09e;thomwolf;2019-07-04 17:06:30 +0200;first commit

==

docs/index.rst
==================
cf86d23ef;thomwolf;2019-07-04 17:02:21 +0200;parallelism in circlci

==

.circleci/config.yml
==================
15b70338b;thomwolf;2019-07-04 16:50:42 +0200;adding squad model to xlnet and xlm

==

pytorch_pretrained_bert/model_utils.py
pytorch_pretrained_bert/modeling_xlm.py
pytorch_pretrained_bert/modeling_xlnet.py
pytorch_pretrained_bert/tests/modeling_openai_test.py
pytorch_pretrained_bert/tests/modeling_xlm_test.py
pytorch_pretrained_bert/tests/modeling_xlnet_test.py
==================
fbe04423b;thomwolf;2019-07-04 00:25:30 +0200;Common SequenceSummary class

==

pytorch_pretrained_bert/__init__.py
pytorch_pretrained_bert/model_utils.py
pytorch_pretrained_bert/modeling_gpt2.py
pytorch_pretrained_bert/modeling_openai.py
pytorch_pretrained_bert/modeling_xlnet.py
==================
c22545aa4;thomwolf;2019-07-03 23:03:57 +0200;fix xlm torchscript

==

pytorch_pretrained_bert/modeling_xlm.py
==================
3b23a846b;thomwolf;2019-07-03 22:54:58 +0200;Merge branch 'xlnet' of https://github.com/huggingface/pytorch-pretrained-BERT into xlnet

==
==================
8fa3a1f0d;thomwolf;2019-07-03 22:54:53 +0200;updating tests

==

pytorch_pretrained_bert/modeling_xlm.py
pytorch_pretrained_bert/tests/model_tests_commons.py
pytorch_pretrained_bert/tests/modeling_bert_test.py
pytorch_pretrained_bert/tests/modeling_xlm_test.py
pytorch_pretrained_bert/tests/modeling_xlnet_test.py
pytorch_pretrained_bert/tokenization_xlm.py
==================
c41f2bad6;thomwolf;2019-07-03 22:54:39 +0200;WIP XLM + refactoring

==

examples/lm_finetuning/finetune_on_pregenerated.py
examples/lm_finetuning/pregenerate_training_data.py
examples/lm_finetuning/simple_lm_finetuning.py
examples/run_bert_classifier.py
examples/run_bert_extract_features.py
examples/run_bert_squad.py
examples/run_bert_swag.py
examples/utils_squad.py
hubconfs/bert_hubconf.py
notebooks/Comparing-TF-and-PT-models-MLM-NSP.ipynb
notebooks/Comparing-TF-and-PT-models.ipynb
pytorch_pretrained_bert/__init__.py
pytorch_pretrained_bert/convert_tf_checkpoint_to_pytorch.py
pytorch_pretrained_bert/modeling_bert.py
pytorch_pretrained_bert/modeling_gpt2.py
pytorch_pretrained_bert/modeling_openai.py
pytorch_pretrained_bert/modeling_transfo_xl.py
pytorch_pretrained_bert/modeling_xlm.py
pytorch_pretrained_bert/tests/model_tests_commons.py
pytorch_pretrained_bert/tests/model_utils_test.py
pytorch_pretrained_bert/tests/modeling_bert_test.py
pytorch_pretrained_bert/tests/modeling_xlm_test.py
pytorch_pretrained_bert/tests/tokenization_bert_test.py
pytorch_pretrained_bert/tests/tokenization_xlm_test.py
pytorch_pretrained_bert/tokenization_bert.py
pytorch_pretrained_bert/tokenization_openai.py
pytorch_pretrained_bert/tokenization_xlm.py
==================
64ce4dbd8;Thomas Wolf;2019-07-03 22:52:03 +0200;Merge pull request #748 from huggingface/torchscript
Release 0.7 - Add Torchscript capabilities
==
==================
b43b130f3;LysandreJik;2019-07-03 16:21:17 -0400;TorchScript flag in config; Tied weights when not running TorchScript; tuple concatenation clean-up.

==

pytorch_pretrained_bert/model_utils.py
pytorch_pretrained_bert/modeling.py
pytorch_pretrained_bert/modeling_gpt2.py
pytorch_pretrained_bert/modeling_openai.py
pytorch_pretrained_bert/modeling_xlnet.py
pytorch_pretrained_bert/tests/model_tests_commons.py
==================
4703148f0;LysandreJik;2019-07-03 14:50:23 -0400;TransformerXL can't be exported to TorchScript because of control-flow. Exception added to tests.

==

pytorch_pretrained_bert/tests/model_tests_commons.py
pytorch_pretrained_bert/tests/modeling_transfo_xl_test.py
==================
971c24687;LysandreJik;2019-07-03 11:03:09 -0400;XLNET can be exported to TorchScript

==

pytorch_pretrained_bert/modeling_xlnet.py
==================
be54b1696;LysandreJik;2019-07-02 18:09:45 -0400;GPT can be exported to TorchScript

==

pytorch_pretrained_bert/modeling_openai.py
==================
d8e83de79;LysandreJik;2019-07-02 18:01:09 -0400;GPT2 can be exported to TorchScript

==

pytorch_pretrained_bert/modeling_gpt2.py
==================
288be7b7e;thomwolf;2019-07-02 23:42:31 +0200;xlm

==

pytorch_pretrained_bert/convert_xlm_checkpoint_to_pytorch.py
pytorch_pretrained_bert/modeling_xlm.py
pytorch_pretrained_bert/tokenization_xlm.py
==================
e891bb43d;LysandreJik;2019-07-02 17:23:18 -0400;BERT can be exported to TorchScript

==

pytorch_pretrained_bert/modeling.py
==================
6ce1ee04f;LysandreJik;2019-07-02 17:22:59 -0400;TorchScript testing with output_attentions and output_hidden_state

==

pytorch_pretrained_bert/tests/model_tests_commons.py
==================
7ed5bf706;thomwolf;2019-07-02 16:42:22 +0200;add tests

==

pytorch_pretrained_bert/tests/model_tests_commons.py
==================
708877958;thomwolf;2019-07-02 16:35:29 +0200;updating tests and models, adding weights initialization test

==

pytorch_pretrained_bert/file_utils.py
pytorch_pretrained_bert/model_utils.py
pytorch_pretrained_bert/modeling.py
pytorch_pretrained_bert/modeling_gpt2.py
pytorch_pretrained_bert/modeling_openai.py
pytorch_pretrained_bert/modeling_transfo_xl.py
pytorch_pretrained_bert/modeling_xlm.py
pytorch_pretrained_bert/modeling_xlnet.py
pytorch_pretrained_bert/tests/model_tests_commons.py
pytorch_pretrained_bert/tests/modeling_gpt2_test.py
pytorch_pretrained_bert/tests/modeling_openai_test.py
pytorch_pretrained_bert/tests/modeling_transfo_xl_test.py
pytorch_pretrained_bert/tests/modeling_xlnet_test.py
==================
99ae5ab88;thomwolf;2019-07-02 12:40:39 +0200;update config tests and circle-ci

==

.circleci/config.yml
pytorch_pretrained_bert/modeling_gpt2.py
pytorch_pretrained_bert/modeling_openai.py
pytorch_pretrained_bert/modeling_transfo_xl.py
pytorch_pretrained_bert/modeling_xlnet.py
pytorch_pretrained_bert/tests/model_tests_commons.py
==================
1484d67de;thomwolf;2019-07-02 12:13:17 +0200;[LARGE] updating all tests and API

==

pytorch_pretrained_bert/model_utils.py
pytorch_pretrained_bert/modeling.py
pytorch_pretrained_bert/modeling_gpt2.py
pytorch_pretrained_bert/modeling_openai.py
pytorch_pretrained_bert/modeling_transfo_xl.py
pytorch_pretrained_bert/modeling_xlm.py
pytorch_pretrained_bert/modeling_xlnet.py
pytorch_pretrained_bert/tests/__init__.py
pytorch_pretrained_bert/tests/conftest.py
pytorch_pretrained_bert/tests/fixtures/input.txt
pytorch_pretrained_bert/tests/fixtures/sample_text.txt
pytorch_pretrained_bert/tests/fixtures/test_sentencepiece.model
pytorch_pretrained_bert/tests/model_tests_commons.py
pytorch_pretrained_bert/tests/model_utils_test.py
pytorch_pretrained_bert/tests/modeling_gpt2_test.py
pytorch_pretrained_bert/tests/modeling_openai_test.py
pytorch_pretrained_bert/tests/modeling_test.py
pytorch_pretrained_bert/tests/modeling_transfo_xl_test.py
pytorch_pretrained_bert/tests/modeling_xlnet_test.py
pytorch_pretrained_bert/tests/optimization_test.py
pytorch_pretrained_bert/tests/tokenization_gpt2_test.py
pytorch_pretrained_bert/tests/tokenization_openai_test.py
pytorch_pretrained_bert/tests/tokenization_test.py
pytorch_pretrained_bert/tests/tokenization_transfo_xl_test.py
pytorch_pretrained_bert/tests/tokenization_xlnet_test.py
tests/modeling_gpt2_test.py
tests/modeling_openai_test.py
tests/modeling_test.py
xlnet
==================
64b2a828c;Lei Mao;2019-07-01 14:56:24 -0700;fix evaluation bug

==

examples/run_squad.py
==================
4f8b5f687;thomwolf;2019-06-29 23:35:21 +0200;add fix for serialization of tokenizer

==

pytorch_pretrained_bert/tokenization_xlnet.py
tests/tokenization_xlnet_test.py
==================
d9184620f;thomwolf;2019-06-29 23:10:40 +0200;fix tests and new API

==

pytorch_pretrained_bert/modeling.py
pytorch_pretrained_bert/modeling_xlnet.py
tests/modeling_test.py
tests/modeling_xlnet_test.py
==================
dad3c7a48;Thomas Wolf;2019-06-28 17:28:25 +0200;Merge pull request #723 from tonianelope/master
Update Adam optimizer to follow pytorch convention for betas parameter (#510)
==
==================
e296d5bef;Thomas Wolf;2019-06-28 17:10:58 +0200;Merge pull request #704 from deepset-ai/master
Adjust s3 german Bert file storage
==
==================
c68b4ecee;Thomas Wolf;2019-06-28 17:08:51 +0200;Merge pull request #718 from Rocketknight1/master
Incorrect docstring for BertForMaskedLM
==
==================
213981d8c;thomwolf;2019-06-28 16:45:24 +0200;updating bert API

==

pytorch_pretrained_bert/modeling.py
==================
2b56e9889;thomwolf;2019-06-28 16:35:09 +0200;standardizing API across models - XLNetForSeqClass working

==

examples/run_xlnet_classifier.py
pytorch_pretrained_bert/modeling.py
pytorch_pretrained_bert/modeling_xlm.py
pytorch_pretrained_bert/modeling_xlnet.py
==================
3a00674cb;thomwolf;2019-06-27 17:18:46 +0200;fix imports

==

examples/run_bert_classifier.py
examples/run_bert_squad.py
examples/run_xlnet_classifier.py
examples/run_xlnet_squad.py
==================
d939d6fd0;thomwolf;2019-06-27 09:39:44 +0200;fix hidden-state extraction

==

pytorch_pretrained_bert/modeling_xlnet.py
==================
0c2ff3481;thomwolf;2019-06-27 09:27:50 +0200;extracting double hidden-state from xlnet

==

pytorch_pretrained_bert/modeling_xlnet.py
==================
08ff056c4;Mayhul Arora;2019-06-26 16:16:12 -0700;Added option to use multiple workers to create training data for lm fine tuning

==

examples/lm_finetuning/pregenerate_training_data.py
==================
3deea56c0;thomwolf;2019-06-26 13:41:12 +0200;fixing loading fucntion

==

pytorch_pretrained_bert/modeling_xlm.py
pytorch_pretrained_bert/modeling_xlnet.py
==================
f56b8033f;thomwolf;2019-06-26 13:13:15 +0200;more versatile loading

==

pytorch_pretrained_bert/model_utils.py
pytorch_pretrained_bert/modeling_xlm.py
==================
4d47f4985;thomwolf;2019-06-26 12:52:44 +0200;slight refactoring, add abstract class for model loading

==

pytorch_pretrained_bert/__init__.py
pytorch_pretrained_bert/model_utils.py
pytorch_pretrained_bert/modeling.py
pytorch_pretrained_bert/modeling_gpt2.py
pytorch_pretrained_bert/modeling_openai.py
pytorch_pretrained_bert/modeling_transfo_xl.py
pytorch_pretrained_bert/modeling_xlm.py
pytorch_pretrained_bert/modeling_xlnet.py
==================
59cefd4f9;thomwolf;2019-06-26 11:28:27 +0200;fix #726 - get_lr in examples

==

examples/run_bert_squad.py
examples/run_xlnet_classifier.py
examples/run_xlnet_squad.py
==================
ddc2cc61a;thomwolf;2019-06-26 11:17:42 +0200;fix python2 tests

==

pytorch_pretrained_bert/model_utils.py
==================
7e3070ae4;thomwolf;2019-06-26 11:12:00 +0200;add from_pretrained method to all configuration classes

==

pytorch_pretrained_bert/__init__.py
pytorch_pretrained_bert/convert_xlnet_checkpoint_to_pytorch.py
pytorch_pretrained_bert/file_utils.py
pytorch_pretrained_bert/model_utils.py
pytorch_pretrained_bert/modeling.py
pytorch_pretrained_bert/modeling_gpt2.py
pytorch_pretrained_bert/modeling_openai.py
pytorch_pretrained_bert/modeling_transfo_xl.py
pytorch_pretrained_bert/modeling_xlm.py
pytorch_pretrained_bert/modeling_xlnet.py
tests/modeling_xlnet_test.py
==================
93e9971c5;thomwolf;2019-06-26 10:02:45 +0200;fix tests

==

README.md
pytorch_pretrained_bert/modeling_transfo_xl.py
pytorch_pretrained_bert/modeling_transfo_xl_utilities.py
pytorch_pretrained_bert/modeling_xlnet_utilities.py
tests/modeling_transfo_xl_test.py
tests/modeling_xlnet_test.py
==================
092dacfd6;thomwolf;2019-06-26 09:54:05 +0200;changing is_regression to unified API

==

examples/utils_glue.py
pytorch_pretrained_bert/convert_xlnet_checkpoint_to_pytorch.py
pytorch_pretrained_bert/modeling.py
pytorch_pretrained_bert/modeling_xlnet.py
==================
e55d4c4ed;thomwolf;2019-06-26 00:57:53 +0200;various updates to conversion, models and examples

==

README.md
examples/run_bert_swag.py
examples/run_xlnet_classifier.py
pytorch_pretrained_bert/__init__.py
pytorch_pretrained_bert/convert_xlnet_checkpoint_to_pytorch.py
pytorch_pretrained_bert/modeling_xlnet.py
==================
603c513b3;thomwolf;2019-06-25 10:45:07 +0200;update main conversion script and readme

==

README.md
pytorch_pretrained_bert/__main__.py
pytorch_pretrained_bert/convert_xlnet_checkpoint_to_pytorch.py
==================
7de174049;thomwolf;2019-06-25 10:27:58 +0200;add ability to restore fine-tuned TF mdoel

==

pytorch_pretrained_bert/convert_xlnet_checkpoint_to_pytorch.py
pytorch_pretrained_bert/modeling_xlnet.py
==================
c9885903a;tonianelope;2019-06-25 09:23:12 +0100;update betas to follow pytorch convention

==

README.md
pytorch_pretrained_bert/optimization.py
pytorch_pretrained_bert/optimization_openai.py
==================
7334bf6c2;thomwolf;2019-06-24 15:05:11 +0200;pad on left for xlnet

==

examples/run_xlnet_classifier.py
examples/utils_glue.py
==================
c888663f1;thomwolf;2019-06-24 14:38:24 +0200;overwrite output directories if needed

==

examples/run_bert_classifier.py
examples/run_bert_squad.py
examples/run_xlnet_classifier.py
examples/run_xlnet_squad.py
==================
62d78aa37;thomwolf;2019-06-24 14:36:11 +0200;updating GLUE utils for compatibility with XLNet

==

README.md
examples/run_xlnet_classifier.py
examples/utils_glue.py
notebooks/Comparing-TF-and-PT-models-SQuAD.ipynb
pytorch_pretrained_bert/modeling.py
pytorch_pretrained_bert/modeling_xlnet.py
pytorch_pretrained_bert/tokenization.py
pytorch_pretrained_bert/tokenization_xlnet.py
tests/modeling_xlnet_test.py
==================
24ed0b934;thomwolf;2019-06-24 12:00:09 +0200;updating run_xlnet_classifier

==

.gitignore
examples/run_xlnet_classifier.py
examples/run_xlnet_squad.py
pytorch_pretrained_bert/modeling.py
pytorch_pretrained_bert/modeling_xlnet.py
==================
f6081f225;thomwolf;2019-06-24 10:01:07 +0200;add xlnetforsequence classif and run_classifier example for xlnet

==

examples/bertology.py
examples/run_bert_classifier.py
examples/run_bert_extract_features.py
examples/run_bert_squad.py
examples/run_xlnet_classifier.py
examples/utils_glue.py
examples/utils_squad.py
hubconfs/xlnet_hubconf.py
pytorch_pretrained_bert/modeling_xlnet.py
==================
8d6a118ae;Rocketknight1;2019-06-23 18:47:05 +0100;Incorrect docstring for the head_mask argument to BertForMaskedLM

==

pytorch_pretrained_bert/modeling.py
==================
06716d753;Matt;2019-06-23 18:46:03 +0100;Merge pull request #3 from huggingface/master
Catch up with main repo
==
==================
c946bb51a;thomwolf;2019-06-22 22:28:49 +0200;fix xlnet tokenizer and python2

==

pytorch_pretrained_bert/tokenization_xlnet.py
tests/tokenization_xlnet_test.py
==================
98dc30b21;Thomas Wolf;2019-06-22 21:29:41 +0200;Merge pull request #714 from papower1/master
Correct a broken link on README
==
==================
eae5d3819;Thomas Wolf;2019-06-22 21:29:19 +0200;Merge pull request #715 from Rocketknight1/master
Include a reference for LM finetuning
==
==================
c7b2808ed;Rocketknight1;2019-06-22 15:04:01 +0100;Update LM finetuning README to include a literature reference

==

examples/lm_finetuning/README.md
==================
7c59e32d4;Matt;2019-06-22 14:59:47 +0100;Merge pull request #2 from huggingface/master
Updating my fork to the latest version
==
==================
ada0d8fec;Chang-Uk Shin;2019-06-22 20:34:45 +0900;Merge pull request #1 from papower1/papower1-patch-1
Correct a broken link and its context.
==
==================
fcc706343;Chang-Uk Shin;2019-06-22 20:33:48 +0900;Correct a broken link and its context.
Correct a broken link(run_lm_finetuning.py) and its context.
==

README.md
==================
181075635;thomwolf;2019-06-21 23:23:37 +0200;updating model loading and adding special tokens ids

==

examples/generation_xlnet.py
pytorch_pretrained_bert/modeling_xlnet.py
pytorch_pretrained_bert/tokenization_xlnet.py
xlnet
==================
ebd2cb8d7;thomwolf;2019-06-21 21:08:44 +0200;update from_pretrained to load XLNetModel as well

==

examples/generation_xlnet.py
pytorch_pretrained_bert/modeling_xlnet.py
pytorch_pretrained_bert/tokenization_xlnet.py
tests/modeling_xlnet_test.py
==================
483cbc36a;thomwolf;2019-06-21 16:38:01 +0200;test deviation with tf model: max ~1e-3 should be ok

==

hubconfs/xlnet_hubconf.py
pytorch_pretrained_bert/__init__.py
pytorch_pretrained_bert/modeling_xlnet.py
tests/modeling_xlnet_test.py
tests/tokenization_xlnet_test.py
==================
24d806898;thomwolf;2019-06-21 12:33:44 +0200;weights loading script ok

==

pytorch_pretrained_bert/convert_xlnet_checkpoint_to_pytorch.py
pytorch_pretrained_bert/modeling_xlnet.py
tests/modeling_xlnet_test.py
==================
32da75486;thomwolf;2019-06-21 11:09:51 +0200;add tokenizer and tests

==

pytorch_pretrained_bert/__init__.py
pytorch_pretrained_bert/modeling_xlnet.py
pytorch_pretrained_bert/modeling_xlnet_utilities.py
pytorch_pretrained_bert/tokenization_xlnet.py
requirements.txt
samples/test_sentencepiece.model
setup.py
tests/modeling_xlnet_test.py
tests/tokenization_test.py
tests/tokenization_transfo_xl_test.py
tests/tokenization_xlnet_test.py
==================
45709d753;thomwolf;2019-06-21 00:28:42 +0200;model running with simple inputs

==

pytorch_pretrained_bert/__init__.py
pytorch_pretrained_bert/convert_xlnet_checkpoint_to_pytorch.py
pytorch_pretrained_bert/modeling.py
pytorch_pretrained_bert/modeling_gpt2.py
pytorch_pretrained_bert/modeling_openai.py
pytorch_pretrained_bert/modeling_xlnet.py
tests/modeling_xlnet_test.py
==================
b407972e2;thomwolf;2019-06-20 13:52:56 +0200;update gitignore

==

.gitignore
==================
c2ea5aef7;thomwolf;2019-06-20 13:52:21 +0200;work in progress on xlnet

==

pytorch_pretrained_bert/modeling_xlnet.py
==================
de713fa9b;thomwolf;2019-06-20 10:54:19 +0200;starting

==

pytorch_pretrained_bert/convert_xlnet_checkpoint_to_pytorch.py
pytorch_pretrained_bert/modeling.py
pytorch_pretrained_bert/modeling_transfo_xl.py
pytorch_pretrained_bert/modeling_xlnet.py
pytorch_pretrained_bert/tokenization_xlnet.py
==================
c304593d8;thomwolf;2019-06-20 10:05:06 +0200;BERTology details in readme

==

README.md
==================
12e892e17;Thomas Wolf;2019-06-20 09:58:24 +0200;Merge pull request #697 from huggingface/updating_examples
Updating examples
==
==================
411981a08;thomwolf;2019-06-20 08:54:18 +0200;remove slow circle-ci

==

.circleci/config.yml
==================
716cc1c4d;chrislarson1;2019-06-19 23:18:57 -0400;added main() for programmatic call to convert pytorch->tf

==

pytorch_pretrained_bert/convert_pytorch_checkpoint_to_tf.py
==================
a8e071c69;chrislarson1;2019-06-19 23:08:08 -0400;added notebook to check correctness of the pytorch->tensorflow conversion

==

notebooks/Comparing-PT-and-TF-models.ipynb
==================
0a4fb0da5;chrislarson1;2019-06-19 22:56:20 -0400;Merge remote-tracking branch 'upstream/master' into convert-back-to-tf
merging in latest changes from upstream

==
==================
edfe91c36;thomwolf;2019-06-19 23:43:04 +0200;first version bertology ok

==

examples/bertology.py
==================
7766ce66d;thomwolf;2019-06-19 22:29:51 +0200;update bertology

==

examples/bertology.py
==================
7f00a36e2;thomwolf;2019-06-19 22:23:12 +0200;pruning should keep on device

==

pytorch_pretrained_bert/modeling.py
pytorch_pretrained_bert/modeling_gpt2.py
==================
e4b46d86c;thomwolf;2019-06-19 22:16:30 +0200;update head pruning

==

examples/bertology.py
==================
939cf2915;timoeller;2019-06-19 18:38:42 +0200;Adjust s3 german Bert file storage

==

pytorch_pretrained_bert/modeling.py
==================
0f40e8d6a;thomwolf;2019-06-19 15:38:46 +0200;debugger

==

examples/bertology.py
==================
0e1e8128b;thomwolf;2019-06-19 15:35:49 +0200;more logging

==

examples/bertology.py
==================
909d4f1af;thomwolf;2019-06-19 15:32:10 +0200;cuda again

==

examples/bertology.py
==================
14f0e8e55;thomwolf;2019-06-19 15:29:28 +0200;fix cuda

==

examples/bertology.py
==================
34d706a0e;thomwolf;2019-06-19 15:25:49 +0200;pruning in bertology

==

README.md
examples/bertology.py
examples/run_classifier.py
pytorch_pretrained_bert/modeling.py
pytorch_pretrained_bert/modeling_gpt2.py
pytorch_pretrained_bert/modeling_openai.py
tests/modeling_gpt2_test.py
tests/modeling_openai_test.py
tests/modeling_test.py
==================
dc8e0019b;thomwolf;2019-06-19 13:23:20 +0200;updating examples

==

README.md
examples/bertology.py
examples/run_classifier.py
pytorch_pretrained_bert/modeling.py
pytorch_pretrained_bert/tokenization.py
==================
68ab9599c;thomwolf;2019-06-19 09:38:38 +0200;small fix and updates to readme

==

README.md
examples/bertology.py
examples/run_classifier.py
examples/run_squad.py
pytorch_pretrained_bert/modeling.py
==================
f7e2ac01e;thomwolf;2019-06-18 22:43:35 +0200;update barrier

==

examples/run_classifier.py
examples/run_squad.py
==================
4d8c4337a;thomwolf;2019-06-18 22:41:28 +0200;test barrier in distrib training

==

README.md
examples/run_classifier.py
pytorch_pretrained_bert/modeling.py
==================
335995562;thomwolf;2019-06-18 22:23:10 +0200;updating run_classif

==

examples/run_classifier.py
==================
29b7b30ea;thomwolf;2019-06-18 22:20:21 +0200;updating evaluation on a single gpu

==

examples/run_classifier.py
==================
7d2001aa4;thomwolf;2019-06-18 22:13:30 +0200;overwrite_output_dir

==

examples/run_classifier.py
examples/run_squad.py
==================
16a1f338c;thomwolf;2019-06-18 17:06:31 +0200;fixing

==

examples/run_classifier.py
==================
92e0ad5ab;thomwolf;2019-06-18 17:00:52 +0200;no numpy

==

examples/run_classifier.py
==================
4e6edc327;thomwolf;2019-06-18 16:57:15 +0200;hop

==

examples/run_classifier.py
==================
f55b60b9e;thomwolf;2019-06-18 16:56:52 +0200;fixing again

==

examples/run_classifier.py
==================
8bd911829;thomwolf;2019-06-18 16:54:41 +0200;quick fix

==

examples/run_classifier.py
==================
3e847449a;thomwolf;2019-06-18 16:53:31 +0200;fix out_label_ids

==

examples/run_classifier.py
==================
aad3a54e9;thomwolf;2019-06-18 16:48:04 +0200;fix paths

==

examples/run_classifier.py
==================
40dbda687;thomwolf;2019-06-18 16:45:52 +0200;updating classification example

==

examples/run_classifier.py
==================
7388c83b6;thomwolf;2019-06-18 16:32:49 +0200;update run_classifier for distributed eval

==

examples/run_classifier.py
==================
972772324;thomwolf;2019-06-18 16:02:42 +0200;fix pickle

==

examples/run_classifier.py
==================
9710b68db;thomwolf;2019-06-18 16:01:15 +0200;fix pickles

==

examples/run_classifier.py
==================
15ebd67d4;thomwolf;2019-06-18 15:58:22 +0200;cache in run_classifier + various fixes to the examples

==

README.md
examples/bertology.py
examples/run_classifier.py
examples/run_classifier_dataset_utils.py
examples/run_squad.py
==================
e6e5f1925;thomwolf;2019-06-18 14:45:14 +0200;fix

==

examples/run_squad.py
==================
a432b3d46;thomwolf;2019-06-18 14:39:09 +0200;distributed traing t_total

==

examples/run_squad.py
==================
c5407f343;thomwolf;2019-06-18 14:29:03 +0200;split squad example in two

==

examples/run_squad.py
examples/run_squad_dataset_utils.py
==================
335f57baf;thomwolf;2019-06-18 14:03:46 +0200;only on main process

==

examples/run_squad.py
==================
326944d62;thomwolf;2019-06-18 14:02:42 +0200;add tensorboard to run_squad

==

examples/run_squad.py
==================
d82e5deeb;thomwolf;2019-06-18 12:13:14 +0200;set find_unused_parameters=True in DDP

==

README.md
examples/run_squad.py
==================
a59abedfb;thomwolf;2019-06-18 12:06:26 +0200;DDP update

==

examples/run_squad.py
==================
2ef5e0de8;thomwolf;2019-06-18 12:03:13 +0200;switch to pytorch DistributedDataParallel

==

examples/run_squad.py
==================
9ce37af99;thomwolf;2019-06-18 11:47:54 +0200;oups

==

examples/run_squad.py
==================
a40955f07;thomwolf;2019-06-18 11:46:14 +0200;no need to duplicate models anymore

==

examples/run_squad.py
==================
3763f8944;Thomas Wolf;2019-06-18 11:42:57 +0200;Merge pull request #696 from huggingface/split_config_weights
Split config weights
==
==================
f96475309;thomwolf;2019-06-18 11:36:28 +0200;explanation on the current location of the caching folder

==

README.md
==================
868de8d1d;thomwolf;2019-06-18 10:58:20 +0200;updating weights loading

==

pytorch_pretrained_bert/modeling.py
==================
64e0adda8;thomwolf;2019-06-18 10:51:31 +0200;better error message

==

pytorch_pretrained_bert/modeling.py
pytorch_pretrained_bert/modeling_gpt2.py
pytorch_pretrained_bert/modeling_openai.py
pytorch_pretrained_bert/modeling_transfo_xl.py
==================
382e2d1e5;thomwolf;2019-06-18 10:37:16 +0200;spliting config and weight files for bert also

==

README.md
examples/bertology.py
pytorch_pretrained_bert/modeling.py
pytorch_pretrained_bert/modeling_gpt2.py
pytorch_pretrained_bert/modeling_openai.py
pytorch_pretrained_bert/modeling_transfo_xl.py
==================
a6f251181;Thomas Wolf;2019-06-17 16:27:25 +0200;Merge pull request #694 from huggingface/release_0.6.3
Release 0.6.3
==
==================
4447f270b;thomwolf;2019-06-17 16:21:28 +0200;updating hub

==

README.md
hubconfs/bert_hubconf.py
hubconfs/gpt2_hubconf.py
hubconfs/gpt_hubconf.py
==================
33d3db5c4;thomwolf;2019-06-17 15:51:28 +0200;updating head masking, readme and docstrings

==

README.md
pytorch_pretrained_bert/modeling.py
pytorch_pretrained_bert/modeling_gpt2.py
pytorch_pretrained_bert/modeling_openai.py
tests/modeling_gpt2_test.py
tests/modeling_openai_test.py
tests/modeling_test.py
==================
965f172de;thomwolf;2019-06-17 14:34:12 +0200;output all hidden layers states in GPT/GPT-2

==

pytorch_pretrained_bert/modeling_gpt2.py
pytorch_pretrained_bert/modeling_openai.py
tests/modeling_gpt2_test.py
tests/modeling_openai_test.py
==================
f12007e42;thomwolf;2019-06-17 14:19:40 +0200;add head masking and pruning to openai GPT

==

pytorch_pretrained_bert/modeling_openai.py
tests/modeling_openai_test.py
==================
b860e47cf;thomwolf;2019-06-17 14:12:10 +0200;add head masking and pruning to gpt-2

==

pytorch_pretrained_bert/modeling_gpt2.py
tests/modeling_gpt2_test.py
==================
7220d47a1;thomwolf;2019-06-17 13:20:45 +0200;adding head pruning and tests

==

pytorch_pretrained_bert/modeling.py
tests/modeling_test.py
==================
8415a38b2;thomwolf;2019-06-17 13:03:48 +0200;better error messages

==

pytorch_pretrained_bert/modeling.py
pytorch_pretrained_bert/modeling_gpt2.py
pytorch_pretrained_bert/modeling_openai.py
pytorch_pretrained_bert/modeling_transfo_xl.py
pytorch_pretrained_bert/tokenization.py
pytorch_pretrained_bert/tokenization_gpt2.py
pytorch_pretrained_bert/tokenization_openai.py
pytorch_pretrained_bert/tokenization_transfo_xl.py
==================
96c4d3d98;thomwolf;2019-06-17 12:17:26 +0200;add head masking tests

==

pytorch_pretrained_bert/modeling.py
tests/modeling_test.py
==================
34858ae1d;thomwolf;2019-06-17 11:02:39 +0200;adding bert whole words, bertgerman and gpt-2 medium models, head masking

==

README.md
pytorch_pretrained_bert/modeling.py
pytorch_pretrained_bert/tokenization.py
==================
80684f6f8;Thomas Wolf;2019-06-15 23:14:10 +0200;Merge pull request #690 from shashwath94/projadpsftmax_fix
Transformer XL ProjectedAdaptiveLogSoftmax output fix
==
==================
9e363703d;Thomas Wolf;2019-06-15 23:13:41 +0200;Merge pull request #688 from deepset-ai/german_bert
Add German Bert model to code, update readme
==
==================
cc6cd430f;Thomas Wolf;2019-06-15 23:12:55 +0200;Merge pull request #691 from vanche/master
import class "GPT2MultipleChoiceHead"
==
==================
8289646d4;vanche;2019-06-15 22:19:30 +0900;import class "GPT2MultipleChoiceHead"

==

pytorch_pretrained_bert/__init__.py
==================
5076a5daa;Shashwath H A;2019-06-14 22:03:21 -0400;Fix proj adp softmax output return when n_clusters=0

==

pytorch_pretrained_bert/modeling_transfo_xl_utilities.py
==================
16af9ff7b;timoeller;2019-06-14 17:42:46 +0200;Add German Bert model to code, update readme

==

README.md
pytorch_pretrained_bert/modeling.py
pytorch_pretrained_bert/tokenization.py
==================
b3f9e9451;Thomas Wolf;2019-06-14 17:23:45 +0200;Merge pull request #687 from huggingface/tests_and_doc
Updating tests and doc
==
==================
44e9ddd7f;thomwolf;2019-06-14 17:17:43 +0200;fix num_special_tokens in GPT 2 test

==

pytorch_pretrained_bert/modeling_gpt2.py
==================
cad88e19d;Thomas Wolf;2019-06-14 17:02:47 +0200;Merge pull request #672 from oliverguhr/master
Add vocabulary and model config to the finetune output
==
==================
c6de62522;Thomas Wolf;2019-06-14 17:02:08 +0200;Merge pull request #655 from huggingface/finish_torchhub_interfaces
Finish torchhub interfaces
==
==================
ff276fc00;Thomas Wolf;2019-06-14 16:59:07 +0200;Merge branch 'master' into finish_torchhub_interfaces

==
==================
a64736dc2;Thomas Wolf;2019-06-14 16:57:45 +0200;Merge pull request #646 from Colanim/patch-1
Fix link in README
==
==================
460d9afd4;Thomas Wolf;2019-06-14 16:57:02 +0200;Merge pull request #640 from Barqawiz/master
Support latest multi language bert fine tune
==
==================
277c77f1c;Thomas Wolf;2019-06-14 16:56:26 +0200;Merge pull request #630 from tguens/master
Update run_squad.py
==
==================
659af2cbd;Thomas Wolf;2019-06-14 16:49:24 +0200;Merge pull request #604 from samuelbroscheit/master
Fixing issue "Training beyond specified 't_total' steps with schedule 'warmup_linear'" reported in #556
==
==================
2d6a53490;Thomas Wolf;2019-06-14 16:47:32 +0200;Merge pull request #597 from huggingface/attention
GPT-2 (medium size model, special_tokens, fine-tuning, attention) + repo code coverage metric 
==
==================
35e6baab3;Thomas Wolf;2019-06-14 16:41:56 +0200;Merge branch 'master' into attention

==
==================
5e1207b8a;thomwolf;2019-06-14 16:28:25 +0200;add attention to all bert models and add test

==

pytorch_pretrained_bert/modeling.py
tests/modeling_test.py
==================
bcc9e93e6;thomwolf;2019-06-14 15:38:20 +0200;fix test

==

tests/modeling_gpt2_test.py
==================
f9cde97b3;Thomas Wolf;2019-06-12 10:01:21 +0200;Merge pull request #675 from meetshah1995/patch-1
[hotfix] Fix frozen pooler parameters in SWAG example.
==
==================
e02ce4dc7;Meet Pragnesh Shah;2019-06-11 15:13:53 -0700;[hotfix] Fix frozen pooler parameters in SWAG example.

==

examples/run_swag.py
==================
5c08c8c27;Oliver Guhr;2019-06-11 13:46:33 +0200;adds the tokenizer + model config to the output

==

examples/lm_finetuning/finetune_on_pregenerated.py
examples/lm_finetuning/simple_lm_finetuning.py
==================
784c0ed89;Thomas Wolf;2019-06-11 11:29:10 +0200;Merge pull request #668 from jeonsworld/patch-2
apply Whole Word Masking technique
==
==================
a3a604cef;jeonsworld;2019-06-10 12:17:23 +0900;Update pregenerate_training_data.py
apply Whole Word Masking technique.
referred to [create_pretraining_data.py](https://github.com/google-research/bert/blob/master/create_pretraining_data.py)
==

examples/lm_finetuning/pregenerate_training_data.py
==================
ee0308f79;VictorSanh;2019-06-06 17:30:49 +0200;fix typo

==

hubconfs/bert_hubconf.py
==================
2d07f945a;VictorSanh;2019-06-06 17:10:24 +0200;fix error with torch.no_grad and loss computation

==

hubconfs/bert_hubconf.py
==================
6b8d22709;VictorSanh;2019-06-06 17:07:03 +0200;some cleaning

==

hubconfs/bert_hubconf.py
==================
122d5c52a;VictorSanh;2019-06-06 17:02:51 +0200;distinguish was is not trained

==

hubconfs/bert_hubconf.py
==================
2647ac329;VictorSanh;2019-06-06 16:57:40 +0200;forgot bertForPreTraining

==

hubconfs/bert_hubconf.py
==================
cf44d9839;VictorSanh;2019-06-06 16:36:02 +0200;Add more examples to BERT models for torchhub

==

hubconfs/bert_hubconf.py
==================
a3274ac40;thomwolf;2019-06-03 16:11:45 -0500;adding attention outputs in bert

==

pytorch_pretrained_bert/modeling.py
tests/modeling_gpt2_test.py
==================
826496580;VictorSanh;2019-06-03 17:10:25 -0400;Revert "add output_attentions for BertModel"
This reverts commit de5e5682a12463465a9eda4d2b13efad9c50d0dd.

==

pytorch_pretrained_bert/modeling.py
==================
de5e5682a;VictorSanh;2019-06-03 17:05:24 -0400;add output_attentions for BertModel

==

pytorch_pretrained_bert/modeling.py
==================
312fdd775;VictorSanh;2019-06-01 17:43:26 -0400;fix doc error

==

hubconfs/transformer_xl_hubconf.py
==================
cdf0f2fec;VictorSanh;2019-06-01 17:42:00 -0400;fix typo/presentation

==

hubconfs/gpt2_hubconf.py
hubconfs/transformer_xl_hubconf.py
==================
8f97f6c57;VictorSanh;2019-06-01 17:29:07 -0400;fix typo
cc @thomwolf

==

pytorch_pretrained_bert/modeling_gpt2.py
pytorch_pretrained_bert/modeling_openai.py
==================
466a96543;VictorSanh;2019-06-01 17:28:56 -0400;fix bug/typos

==

hubconfs/transformer_xl_hubconf.py
==================
c198ff5f1;VictorSanh;2019-06-01 16:28:42 -0400;fix typos/bugs

==

hubconfs/gpt2_hubconf.py
==================
592d1e3aa;VictorSanh;2019-06-01 16:19:32 -0400;fix typos

==

hubconfs/gpt2_hubconf.py
==================
f836130bf;VictorSanh;2019-06-01 16:08:29 -0400;update hubconf

==

hubconf.py
==================
c0c7ff575;VictorSanh;2019-06-01 16:08:24 -0400;add transformer xl compatibility for torchhub

==

hubconfs/transformer_xl_hubconf.py
pytorch_pretrained_bert/modeling_transfo_xl.py
==================
48a58646e;VictorSanh;2019-06-01 16:06:50 -0400;small fix in doc

==

hubconfs/gpt2_hubconf.py
==================
2576a5c6d;VictorSanh;2019-06-01 15:28:01 -0400;update hubconf for gpt2 torchhub compatibility

==

hubconf.py
==================
a92b6dc3c;VictorSanh;2019-06-01 15:27:43 -0400;add GPT2 torchhub compatibility

==

hubconfs/gpt2_hubconf.py
pytorch_pretrained_bert/modeling_gpt2.py
pytorch_pretrained_bert/tokenization_gpt2.py
==================
2a329c618;Thomas Wolf;2019-05-31 14:44:52 +0200;Merge pull request #651 from huggingface/gpt_torchhub
Add GPT* compatibility to torchhub
==
==================
45d21502f;VictorSanh;2019-05-31 01:04:16 -0400;update doc

==

hubconfs/gpt_hubconf.py
==================
98f5c7864;VictorSanh;2019-05-31 01:00:29 -0400;decorelate dependencies + fix bug

==

hubconf.py
hubconfs/gpt_hubconf.py
==================
c8bd026ef;VictorSanh;2019-05-31 00:36:58 -0400;move dependecies list to hubconf

==

hubconf.py
hubconfs/bert_hubconf.py
hubconfs/gpt_hubconf.py
==================
19ef2b0a6;VictorSanh;2019-05-31 00:33:33 -0400;Fix typo in hubconf

==

hubconf.py
==================
d0f591051;VictorSanh;2019-05-31 00:28:10 -0400;gpt_hubconf

==

hubconf.py
hubconfs/gpt_hubconf.py
==================
4a210c9fc;VictorSanh;2019-05-31 00:28:00 -0400;Move bert_hubconf to hubconfs

==

hubconfs/bert_hubconf.py
==================
0c5a4fe9c;VictorSanh;2019-05-31 00:27:18 -0400;modify from_pretrained for OpenAIGPT

==

pytorch_pretrained_bert/modeling_openai.py
==================
372a5c1ce;VictorSanh;2019-05-30 16:06:21 -0400;Hubconf doc - Specia case loading

==

hubconf.py
==================
96592b544;Victor SANH;2019-05-30 15:53:13 -0400;default in __init__s for classification BERT models (#650)

==

pytorch_pretrained_bert/modeling.py
==================
4cda86b08;VictorSanh;2019-05-30 18:38:00 +0000;Update hubconf for torchhub: paths+examples+doc

==

hubconf.py
==================
1eba8b9d9;Colanim;2019-05-30 14:01:46 +0900;Fix link in README

==

README.md
==================
314bc6bb4;Chris;2019-05-27 09:47:59 -0400;added transposes to attention.self.[query,key,value]

==

pytorch_pretrained_bert/convert_pytorch_checkpoint_to_tf.py
==================
c4fe56dcc;Ahmad Barqawi;2019-05-27 11:27:41 +0200;support latest multi language bert fine tune
fix issue of bert-base-multilingual and add support for uncased multilingual

==

examples/lm_finetuning/pregenerate_training_data.py
==================
8de1faea6;Chris;2019-05-22 20:38:16 -0400;update to hf->tf args

==

pytorch_pretrained_bert/convert_pytorch_checkpoint_to_tf.py
==================
d0adab2c3;Chris;2019-05-22 20:24:04 -0400;fn change; pytorch_model_dir required=False

==

pytorch_pretrained_bert/convert_pytorch_checkpoint_to_tf.py
==================
a309459b9;Chris;2019-05-22 20:17:27 -0400;fn change; pytorch_model_dir required=False

==

pytorch_pretrained_bert/convert_pytorch_checkpoint_to_tf.py
==================
9e7bc51b9;tguens;2019-05-22 17:27:59 +0800;Update run_squad.py
Indentation change so that the output "nbest_predictions.json" is not empty.
==

examples/run_squad.py
==================
69749f3fc;Chris;2019-05-18 17:16:01 -0400;update to hf->tf args

==

pytorch_pretrained_bert/convert_hf_checkpoint_to_tf.py
==================
f1433db4f;Chris;2019-05-18 17:09:08 -0400;update to hf->tf args

==

pytorch_pretrained_bert/convert_hf_checkpoint_to_tf.py
==================
077a5b0dc;Chris;2019-05-18 16:06:08 -0400;Merge remote-tracking branch 'upstream/master' into convert-back-to-tf
merging

==
==================
2bcda8d00;Chris;2019-05-18 15:55:11 -0400;update

==

pytorch_pretrained_bert/convert_hf_checkpoint_to_tf.py
==================
94247ad6c;samuelbroscheit;2019-05-13 12:38:22 +0200;Make num_train_optimization_steps int

==

examples/run_classifier.py
examples/run_squad.py
examples/run_swag.py
==================
49a77ac16;samuel.broscheit;2019-05-12 00:31:10 +0200;Clean up a little bit

==

examples/run_classifier.py
examples/run_squad.py
examples/run_swag.py
==================
3bf3f9596;samuel.broscheit;2019-05-12 00:13:45 +0200;Fixing the issues reported in https://github.com/huggingface/pytorch-pretrained-BERT/issues/556
Reason for issue was that optimzation steps where computed from example size, which is different from actual size of dataloader when an example is chunked into multiple instances.

Solution in this pull request is to compute num_optimization_steps directly from len(data_loader).

==

examples/run_classifier.py
examples/run_openai_gpt.py
examples/run_squad.py
examples/run_swag.py
==================
3fc63f126;Thomas Wolf;2019-05-10 13:48:12 +0200;Merge pull request #598 from burcturkoglu/master
Updating learning rate with special warm up in examples
==
==================
00c7fd2b7;burcturkoglu;2019-05-09 10:57:03 +0300;Division to num_train_optimizer of global_step in lr_this_step is removed.

==

examples/lm_finetuning/finetune_on_pregenerated.py
examples/lm_finetuning/simple_lm_finetuning.py
examples/run_classifier.py
examples/run_squad.py
examples/run_swag.py
==================
fa37b4da7;burcturkoglu;2019-05-09 10:55:24 +0300;Merge branch 'master' of https://github.com/huggingface/pytorch-pretrained-BERT

==
==================
5289b4b9e;burcturkoglu;2019-05-09 10:51:38 +0300;Division to num_train_optimizer of global_step in lr_this_step is removed.

==

examples/lm_finetuning/finetune_on_pregenerated.py
examples/lm_finetuning/simple_lm_finetuning.py
examples/run_classifier.py
examples/run_squad.py
examples/run_swag.py
==================
275179a00;thomwolf;2019-05-08 22:24:42 +0200;output attentions in GPT-2

==

pytorch_pretrained_bert/modeling_gpt2.py
==================
366a3b028;thomwolf;2019-05-08 21:43:51 +0200;clean up in tokenization

==

pytorch_pretrained_bert/modeling_gpt2.py
pytorch_pretrained_bert/tokenization_gpt2.py
pytorch_pretrained_bert/tokenization_openai.py
==================
701bd59b8;Thomas Wolf;2019-05-08 16:56:38 +0200;Merge pull request #585 from huntzhan/master
Make the epsilon of LayerNorm configurable.
==
==================
303b5e2b9;Thomas Wolf;2019-05-08 16:55:27 +0200;Merge pull request #545 from ailzhang/cache_dir
move pytroch_pretrained_bert cache folder under same path as torch
==
==================
0198399d8;Thomas Wolf;2019-05-08 16:07:50 +0200;Merge pull request #570 from MottoX/fix-1
Create optimizer only when args.do_train is True
==
==================
50fa92c02;Thomas Wolf;2019-05-08 16:06:13 +0200;Merge pull request #571 from MottoX/patch-1
Fix documentation typo
==
==================
0efc4ab63;thomwolf;2019-05-08 10:41:35 +0200;adding dropout to GPT-2 and embedding dropout to GPT

==

pytorch_pretrained_bert/modeling_gpt2.py
pytorch_pretrained_bert/modeling_openai.py
==================
ea9dbea9d;thomwolf;2019-05-07 23:27:18 +0200;update GPT2 loss computation for more flexbility

==

pytorch_pretrained_bert/modeling_gpt2.py
==================
ce8633654;thomwolf;2019-05-07 16:47:22 +0200;add predict_special_tokens option to GPT also

==

pytorch_pretrained_bert/modeling_openai.py
==================
d1b6979aa;thomwolf;2019-05-07 16:25:53 +0200;GPT-2 option to avoid predicting special tokens

==

pytorch_pretrained_bert/modeling_gpt2.py
pytorch_pretrained_bert/tokenization_gpt2.py
==================
101ab4dd8;huntzhan;2019-05-06 00:26:21 +0800;Make the epsilon of LayerNorm configurable.

==

pytorch_pretrained_bert/modeling.py
==================
41089bc7d;Chris;2019-05-02 13:26:22 -0400;added file to convert pytorch->tf

==

pytorch_pretrained_bert/convert_hf_checkpoint_to_tf.py
==================
0a8b4d65b;Chris;2019-05-02 13:20:59 -0400;added file to convert pytorch->tf

==

pytorch_pretrained_bert/convert_hf_checkpoint_to_tf.py
==================
968c1b44c;Chris;2019-05-02 13:19:56 -0400;added file to convert pytorch->tf

==

pytorch_pretrained_bert/convert_hf_checkpoint_to_tf.py
==================
96c2b77f0;Chris;2019-05-02 13:14:25 -0400;added file to convert pytorch->tf

==

pytorch_pretrained_bert/convert_hf_checkpoint_to_tf.py
==================
e211785ad;thomwolf;2019-05-02 18:31:26 +0200;extract attention weights from GPT

==

pytorch_pretrained_bert/modeling_openai.py
==================
18c8aef9d;MottoX;2019-05-02 19:23:36 +0800;Fix documentation typo

==

examples/run_classifier.py
==================
74dbba64b;MottoX;2019-05-02 19:09:29 +0800;Prepare optimizer only when args.do_train is True

==

examples/lm_finetuning/simple_lm_finetuning.py
examples/run_classifier.py
examples/run_openai_gpt.py
examples/run_squad.py
examples/run_swag.py
==================
db98a4a48;thomwolf;2019-05-01 11:40:48 +0200;gpt-2 tokenizer

==

pytorch_pretrained_bert/tokenization_gpt2.py
==================
3ae8c8be1;Thomas Wolf;2019-05-01 11:20:17 +0200;Merge pull request #562 from apappu97/roc_stories_lmlabels_fix
Small fix to remove shifting of lm labels during pre process of RocStories.
==
==================
e89520175;Thomas Wolf;2019-05-01 11:18:46 +0200;Merge pull request #564 from 8enmann/patch-2
Fix #537
==
==================
74f7906db;Ben Mann;2019-04-30 19:48:22 -0700;Fix #537

==

pytorch_pretrained_bert/tokenization_gpt2.py
==================
365fb34c6;Aneesh Pappu;2019-04-30 13:53:04 -0700;small fix to remove shifting of lm labels during pre process of roc stories, as this shifting happens interanlly in the model

==

examples/run_openai_gpt.py
==================
cd110835a;thomwolf;2019-04-30 11:35:40 +0200;coverage in circle-ci

==

.circleci/config.yml
==================
2dee86319;Thomas Wolf;2019-04-30 11:12:55 +0200;Merge pull request #527 from Mathieu-Prouveur/fix_value_training_loss
Update example files so that tr_loss is not affected by args.gradient‚Ä¶
==
==================
80f53f738;thomwolf;2019-04-30 11:10:22 +0200;gpt-2 from_pretrained can use special tokens

==

pytorch_pretrained_bert/modeling_gpt2.py
==================
e79ceb153;thomwolf;2019-04-30 11:05:54 +0200;gpt-2 special tokens

==

pytorch_pretrained_bert/modeling_gpt2.py
==================
1f5fc95b6;thomwolf;2019-04-30 11:05:26 +0200;add code coverage

==

.circleci/config.yml
.coveragerc
==================
c30139a01;thomwolf;2019-04-30 10:45:26 +0200;add special tokens to gpt-2

==

pytorch_pretrained_bert/modeling_gpt2.py
pytorch_pretrained_bert/modeling_openai.py
tests/modeling_gpt2_test.py
==================
87b9ec384;Mathieu Prouveur;2019-04-29 12:58:29 +0200;Fix tr_loss rescaling factor using global_step

==

examples/run_classifier.py
examples/run_swag.py
==================
3963d57c8;Ailing Zhang;2019-04-27 10:52:53 -0700;move pytroch_pretrained_bert cache folder under same path as torch

==

hubconf.py
pytorch_pretrained_bert/file_utils.py
==================
b832d5bb8;thomwolf;2019-04-25 21:37:47 +0200;Release: 0.6.2

==

pytorch_pretrained_bert/__init__.py
setup.py
==================
e6cf62d49;Thomas Wolf;2019-04-25 21:04:16 +0200;Merge pull request #488 from dhpollack/fix_multichoice
fixed BertForMultipleChoice model init and forward pass
==
==================
1cc1c3c34;Thomas Wolf;2019-04-25 21:02:35 +0200;Merge pull request #533 from lukovnikov/master
Docs for new learning rate code
==
==================
dee8af4e4;Thomas Wolf;2019-04-25 21:01:04 +0200;Merge pull request #518 from huggingface/schedules_in_examples
Fix training schedules in examples to match new API
==
==================
56a47ce2b;lukovnikov;2019-04-25 16:05:28 +0200;- replaced OpenAIGPTAdam with OpenAIAdam in docs

==

tests/optimization_test.py
==================
331a46ff0;lukovnikov;2019-04-25 16:04:37 +0200;- replaced OpenAIGPTAdam with OpenAIAdam in docs

==

README.md
==================
704037ad5;lukovnikov;2019-04-25 15:59:39 +0200;- updated docs for new LR API - added some images for illustration - updated comments in optimization

==

README.md
docs/imgs/warmup_constant_schedule.png
docs/imgs/warmup_cosine_hard_restarts_schedule.png
docs/imgs/warmup_cosine_schedule.png
docs/imgs/warmup_cosine_warm_restarts_schedule.png
docs/imgs/warmup_linear_schedule.png
pytorch_pretrained_bert/optimization.py
pytorch_pretrained_bert/optimization_openai.py
tests/optimization_test.py
==================
d76a57b0b;Thomas Wolf;2019-04-24 20:59:21 +0200;Merge pull request #506 from ailzhang/hubconf
Hubconf
==
==================
80f995a14;thomwolf;2019-04-24 16:51:54 +0200;revert BertForMultipleChoice linear classifier

==

pytorch_pretrained_bert/modeling.py
==================
ed8fad739;Mathieu Prouveur;2019-04-24 14:07:00 +0200;Update example files so that tr_loss is not affected by args.gradient_accumulation_step

==

examples/run_classifier.py
examples/run_swag.py
==================
d94c6b014;thomwolf;2019-04-23 11:17:06 +0200;fix training schedules in examples to match new API

==

examples/lm_finetuning/finetune_on_pregenerated.py
examples/lm_finetuning/simple_lm_finetuning.py
examples/run_classifier.py
examples/run_squad.py
examples/run_swag.py
==================
c36cca075;Thomas Wolf;2019-04-23 10:30:23 +0200;Merge pull request #515 from Rocketknight1/master
Fix --reduce_memory in finetune_on_pregenerated
==
==================
99e02c341;Thomas Wolf;2019-04-23 10:29:01 +0200;Merge pull request #512 from cynthia/master
Fix indentation weirdness in GPT-2 example.
==
==================
98cb7b2c5;Thomas Wolf;2019-04-23 10:27:38 +0200;Merge pull request #445 from lukovnikov/master
Learning rate schedules improvement + extension
==
==================
b8e2a9c58;Matthew Carrigan;2019-04-22 14:01:48 +0100;Made --reduce_memory actually do something in finetune_on_pregenerated

==

examples/lm_finetuning/finetune_on_pregenerated.py
==================
af8a0384f;Matt;2019-04-22 13:56:47 +0100;Merge pull request #1 from huggingface/master
Pulling commits from main repo
==
==================
14b1f719f;Sangwhan Moon;2019-04-22 02:20:22 +0900;Fix indentation weirdness in GPT-2 example.

==

examples/run_gpt2.py
==================
69850b401;lukovnikov;2019-04-21 14:02:38 +0200;python 2 compat

==

pytorch_pretrained_bert/optimization.py
==================
bb7557d3a;lukovnikov;2019-04-21 13:48:33 +0200;- removed __all__ in optimization - removed unused plotting code - using ABC for LRSchedule - added some schedule object init tests

==

pytorch_pretrained_bert/optimization.py
pytorch_pretrained_bert/optimization_openai.py
tests/optimization_test.py
==================
34ccc8ebf;lukovnikov;2019-04-21 13:16:15 +0200;Merge remote-tracking branch 'upstream/master'

==
==================
bfd6f6b25;Ailing Zhang;2019-04-17 13:39:46 -0700;fix from_pretrained positional args

==

hubconf.py
pytorch_pretrained_bert/modeling.py
==================
ae4c9fee7;Ailing Zhang;2019-04-13 15:00:48 -0700;add hubconf

==

hubconf.py
==================
68a889ee4;Thomas Wolf;2019-04-17 15:22:14 +0200;Merge pull request #500 from huggingface/network
Updating network handling
==
==================
34ae5bf83;thomwolf;2019-04-17 14:52:12 +0200;small clean up in tests

==

tests/tokenization_gpt2_test.py
==================
23d4554ec;thomwolf;2019-04-17 14:48:34 +0200;is python 2 happy now

==

pytorch_pretrained_bert/file_utils.py
==================
265550ec3;thomwolf;2019-04-17 14:22:35 +0200;relax network connection requirements

==

pytorch_pretrained_bert/file_utils.py
tests/tokenization_gpt2_test.py
==================
fa7652024;thomwolf;2019-04-17 13:32:22 +0200;fix file_utils on python 2

==

pytorch_pretrained_bert/file_utils.py
==================
bcde2c61c;thomwolf;2019-04-17 12:35:38 +0200;fix #497

==

pytorch_pretrained_bert/tokenization_gpt2.py
==================
929579f3b;thomwolf;2019-04-17 12:35:08 +0200;fix #497

==

pytorch_pretrained_bert/tokenization_gpt2.py
==================
31d387604;thomwolf;2019-04-17 11:58:27 +0200;adding s3 model tests with --runslow

==

.circleci/config.yml
tests/conftest.py
tests/modeling_gpt2_test.py
tests/modeling_openai_test.py
tests/modeling_test.py
tests/modeling_transfo_xl_test.py
tests/tokenization_gpt2_test.py
tests/tokenization_openai_test.py
tests/tokenization_test.py
tests/tokenization_transfo_xl_test.py
==================
8407429d7;Thomas Wolf;2019-04-17 11:11:36 +0200;Merge pull request #494 from SudoSharma/patch-1
Fix indentation for unconditional generation
==
==================
2e153930c;Thomas Wolf;2019-04-17 11:10:36 +0200;Merge pull request #495 from SudoSharma/patch-2
Fix gradient overflow issue during attention mask
==
==================
46078e1b4;Thomas Wolf;2019-04-17 11:08:54 +0200;Merge pull request #496 from 8enmann/patch-1
[run_gpt2.py] temperature should be a float, not int
==
==================
b8686130c;Thomas Wolf;2019-04-17 11:06:41 +0200;Merge pull request #498 from huggingface/GPT2_tokenization
Gpt2 tokenization
==
==================
5afa497cb;thomwolf;2019-04-17 11:04:41 +0200;fix GPT-2 tokenization to work also on python 3...

==

pytorch_pretrained_bert/tokenization_gpt2.py
==================
bc70779bf;thomwolf;2019-04-17 10:56:15 +0200;fixed GPT-2 tokenization on python 2

==

pytorch_pretrained_bert/file_utils.py
pytorch_pretrained_bert/tokenization_gpt2.py
tests/tokenization_gpt2_test.py
tests/tokenization_openai_test.py
==================
87677fcc4;Ben Mann;2019-04-16 15:23:21 -0700;[run_gpt2.py] temperature should be a float, not int

==

examples/run_gpt2.py
==================
9e666aaa2;Abhi Sharma;2019-04-16 11:42:34 -0700;Fix gradient overflow issue during attention mask
This fix is in reference to issue #382. GPT2 can now be trained in mixed precision, which I've confirmed with testing. I also tested unconditional generation on multiple seeds before and after changing 1e10 to 1e4 and there was no difference. Please let me know if there is anything else I can do to make this pull request better. Thanks for all your work!
==

pytorch_pretrained_bert/modeling_gpt2.py
==================
07154dadb;Abhi Sharma;2019-04-16 11:11:49 -0700;Fix indentation for unconditional generation

==

examples/run_gpt2.py
==================
bdaba1897;thomwolf;2019-04-16 17:44:06 +0200;updating GPT tokenization

==

pytorch_pretrained_bert/tokenization_openai.py
==================
18a8a15f7;thomwolf;2019-04-16 17:00:55 +0200;improving GPT2 tokenization and adding tests

==

README.md
pytorch_pretrained_bert/tokenization_gpt2.py
pytorch_pretrained_bert/tokenization_openai.py
tests/tokenization_gpt2_test.py
tests/tokenization_openai_test.py
==================
3d78e226e;Thomas Wolf;2019-04-16 08:49:54 +0200;Merge pull request #489 from huggingface/tokenization_serialization
Better serialization for Tokenizers and Configuration classes - Also fix #466
==
==================
3571187ef;thomwolf;2019-04-15 16:43:56 +0200;fix saving models in distributed setting examples

==

examples/run_classifier.py
examples/run_squad.py
==================
64b6ef4db;Thomas Wolf;2019-04-15 16:14:50 +0200;Merge pull request #490 from huggingface/better_finetuning_GPT_GPT-2
Clean up GPT and GPT-2 losses computation
==
==================
d61602245;thomwolf;2019-04-15 16:07:45 +0200;fix openai special tokens loading

==

pytorch_pretrained_bert/tokenization_openai.py
==================
df5d9c355;thomwolf;2019-04-15 15:43:01 +0200;load all models on cpu

==

pytorch_pretrained_bert/modeling.py
pytorch_pretrained_bert/modeling_gpt2.py
pytorch_pretrained_bert/modeling_openai.py
pytorch_pretrained_bert/modeling_transfo_xl.py
==================
2499b0a5f;thomwolf;2019-04-15 15:33:04 +0200;add ptvsd to run_squad

==

examples/run_squad.py
==================
7816f7921;thomwolf;2019-04-15 15:27:10 +0200;clean up distributed training logging in run_squad example

==

examples/run_squad.py
==================
1135f2384;thomwolf;2019-04-15 15:22:40 +0200;clean up logger in examples for distributed case

==

README.md
examples/run_classifier.py
examples/run_squad.py
==================
cc4330702;thomwolf;2019-04-15 15:06:10 +0200;update readme

==

README.md
==================
60ea6c59d;thomwolf;2019-04-15 15:00:33 +0200;added best practices for serialization in README and examples

==

README.md
examples/run_classifier.py
examples/run_openai_gpt.py
examples/run_squad.py
examples/run_swag.py
pytorch_pretrained_bert/__init__.py
pytorch_pretrained_bert/file_utils.py
pytorch_pretrained_bert/modeling.py
pytorch_pretrained_bert/modeling_gpt2.py
pytorch_pretrained_bert/modeling_openai.py
pytorch_pretrained_bert/modeling_transfo_xl.py
==================
179a2c2ff;thomwolf;2019-04-15 14:33:23 +0200;update example to work with new serialization semantic

==

examples/run_classifier.py
examples/run_openai_gpt.py
examples/run_squad.py
examples/run_swag.py
==================
b3c6ee0ac;thomwolf;2019-04-15 14:24:52 +0200;tokenization updates

==

pytorch_pretrained_bert/tokenization.py
pytorch_pretrained_bert/tokenization_transfo_xl.py
==================
20577d8a7;thomwolf;2019-04-15 14:21:41 +0200;add configuration serialization to readme

==

README.md
==================
9761aa484;thomwolf;2019-04-15 14:12:08 +0200;add to_json_file method to configuration classes

==

pytorch_pretrained_bert/modeling.py
pytorch_pretrained_bert/modeling_gpt2.py
pytorch_pretrained_bert/modeling_openai.py
pytorch_pretrained_bert/modeling_transfo_xl.py
tests/modeling_gpt2_test.py
tests/modeling_openai_test.py
tests/modeling_test.py
tests/modeling_transfo_xl_test.py
==================
b17963d82;thomwolf;2019-04-15 13:44:30 +0200;update readme

==

README.md
==================
e8568a3b1;thomwolf;2019-04-15 12:55:38 +0200;fixing tests

==

pytorch_pretrained_bert/tokenization_gpt2.py
pytorch_pretrained_bert/tokenization_openai.py
tests/tokenization_openai_test.py
tests/tokenization_transfo_xl_test.py
==================
870b734bf;thomwolf;2019-04-15 12:03:56 +0200;added tokenizers serialization tests

==

pytorch_pretrained_bert/tokenization.py
pytorch_pretrained_bert/tokenization_gpt2.py
pytorch_pretrained_bert/tokenization_openai.py
pytorch_pretrained_bert/tokenization_transfo_xl.py
tests/tokenization_openai_test.py
tests/tokenization_test.py
tests/tokenization_transfo_xl_test.py
==================
3e65f255d;thomwolf;2019-04-15 11:47:25 +0200;add serialization semantics to tokenizers - fix transfo-xl tokenizer

==

examples/run_transfo_xl.py
pytorch_pretrained_bert/tokenization.py
pytorch_pretrained_bert/tokenization_gpt2.py
pytorch_pretrained_bert/tokenization_openai.py
pytorch_pretrained_bert/tokenization_transfo_xl.py
==================
6b35cfd28;Thomas Wolf;2019-04-15 11:01:53 +0200;Merge pull request #423 from dhanajitb/master
making unconditional generation work
==
==================
aff44f0c0;Thomas Wolf;2019-04-15 10:58:34 +0200;Merge branch 'master' into master

==
==================
7e7e4753c;Thomas Wolf;2019-04-15 10:57:25 +0200;Merge pull request #480 from mboyanov/docs/cls_token_info
Extend the BertForSequenceClassification docs to mention the special CLS token.
==
==================
bb61b747d;Thomas Wolf;2019-04-15 10:56:48 +0200;Merge pull request #474 from jiesutd/master
Fix tsv read error in Windows
==
==================
7873d7646;Thomas Wolf;2019-04-15 10:55:57 +0200;Merge pull request #478 from Rocketknight1/master
Added a helpful error for users with single-document corpuses - fixes # 452
==
==================
38ba7b439;David Pollack;2019-04-15 10:38:01 +0200;fixed BertForMultipleChoice model init and forward pass

==

pytorch_pretrained_bert/modeling.py
==================
fe2756ff4;thomwolf;2019-04-15 10:04:05 +0200;update double head model

==

pytorch_pretrained_bert/modeling_openai.py
==================
34cf67fd6;Martin Boyanov;2019-04-12 21:30:28 +0300;Extend the BertForSequenceClassification docs to mention the special CLS token.

==

pytorch_pretrained_bert/modeling.py
==================
dbbd6c750;Matthew Carrigan;2019-04-12 15:07:58 +0100;Replaced some randints with cleaner randranges, and added a helpful error for users whose corpus is just one giant document.

==

examples/lm_finetuning/pregenerate_training_data.py
==================
b509bf765;thomwolf;2019-04-12 12:12:33 +0200;updating loss computation

==

pytorch_pretrained_bert/modeling_openai.py
==================
1d203a34c;thomwolf;2019-04-11 23:51:03 +0200;back to simple indexing

==

pytorch_pretrained_bert/modeling_openai.py
==================
616743330;Thomas Wolf;2019-04-11 21:54:46 +0200;Merge pull request #462 from 8enmann/master
fix run_gpt2.py
==
==================
2cdfb8b25;Thomas Wolf;2019-04-11 21:53:23 +0200;Merge pull request #467 from yaroslavvb/patch-2
Update README.md
==
==================
c49ce3c72;Jie Yang;2019-04-11 15:40:19 -0400;fix tsv read error in Windows

==

examples/run_classifier.py
==================
074c869bb;thomwolf;2019-04-11 20:53:50 +0200;fix OpenAIGPTMultipleChoiceHead

==

pytorch_pretrained_bert/modeling_openai.py
==================
724eb45ce;thomwolf;2019-04-11 17:12:00 +0200;add stale bot

==

.github/stale.yml
==================
4bc4c69af;thomwolf;2019-04-11 16:57:59 +0200;finetuning any BERT model - fixes #455

==

examples/lm_finetuning/finetune_on_pregenerated.py
==================
a05fad8dc;thomwolf;2019-04-11 13:16:17 +0200;fix typo

==

pytorch_pretrained_bert/modeling_openai.py
==================
4a82f4f85;thomwolf;2019-04-11 13:11:22 +0200;update special token addition

==

pytorch_pretrained_bert/modeling_openai.py
==================
991b8e65f;thomwolf;2019-04-11 11:43:15 +0200;Merge branch 'master' of https://github.com/huggingface/pytorch-pretrained-BERT

==
==================
e99b2014c;thomwolf;2019-04-11 11:43:13 +0200;fixes #471

==

pytorch_pretrained_bert/modeling_openai.py
==================
8fffba5f4;Yaroslav Bulatov;2019-04-09 14:45:47 -0700;Update README.md
Fix for

```> > > > 04/09/2019 21:39:38 - INFO - __main__ -   device: cuda n_gpu: 1, distributed training: False, 16-bits training: False
Traceback (most recent call last):
  File "/home/ubuntu/pytorch-pretrained-BERT/examples/lm_finetuning/simple_lm_finetuning.py", line 642, in <module>
    main()
  File "/home/ubuntu/pytorch-pretrained-BERT/examples/lm_finetuning/simple_lm_finetuning.py", line 502, in main
    raise ValueError("Training is currently the only implemented execution option. Please set `do_train`.")
ValueError: Training is currently the only implemented execution option. Please set `do_train`.
```
==

examples/lm_finetuning/README.md
==================
fd8a3556f;Benjamin Mann;2019-04-08 17:20:35 -0700;fix run_gpt2.py

==

examples/run_gpt2.py
==================
f4fc9c615;Dhanajit Brahma;2019-04-07 17:52:35 +0530;Merge branch 'master' of https://github.com/dhanajitb/pytorch-pretrained-BERT

==
==================
6c4c7be28;Dhanajit Brahma;2019-04-07 16:59:36 +0530;Merge remote-tracking branch 'upstream/master'

==
==================
4d3cf0d60;Dhanajit Brahma;2019-04-07 16:59:07 +0530;removing some redundant lines

==

examples/run_gpt2.py
==================
0d6a882f6;dhanajitb;2019-04-07 16:54:38 +0530;Cleaned some redundant lines
```while not args.unconditional:
   if not args.unconditional:
```
These lines have been updated
==

examples/run_gpt2.py
==================
fc7693adc;lukovnikov;2019-04-03 18:16:47 +0200;schedule fix

==

pytorch_pretrained_bert/optimization.py
==================
20686b78f;lukovnikov;2019-04-03 18:13:52 +0200;schedule fix

==

pytorch_pretrained_bert/optimization.py
tests/optimization_test.py
==================
1b4ce76c3;lukovnikov;2019-04-03 17:40:12 +0200;schedule fix

==

tests/optimization_test.py
==================
5fed5bb3d;lukovnikov;2019-04-03 17:20:29 +0200;schedule fix

==

pytorch_pretrained_bert/optimization.py
==================
23bd2eebf;lukovnikov;2019-04-03 17:10:34 +0200;schedule fix

==

tests/optimization_test.py
==================
91a073f80;lukovnikov;2019-04-03 17:10:08 +0200;schedule fix

==

pytorch_pretrained_bert/optimization.py
tests/optimization_test.py
==================
b64cc63a7;lukovnikov;2019-04-03 16:42:40 +0200;optimization schedule test update

==

tests/optimization_test.py
==================
d164867d9;lukovnikov;2019-04-03 16:13:51 +0200;- updated docs for optimization

==

tests/optimization_test.py
==================
1758c8fc7;lukovnikov;2019-04-03 16:08:34 +0200;- updated docs for optimization

==

pytorch_pretrained_bert/optimization.py
pytorch_pretrained_bert/optimization_openai.py
==================
725a56329;lukovnikov;2019-04-03 16:07:50 +0200;Merge remote-tracking branch 'upstream/master' into optim
# Conflicts:
#	pytorch_pretrained_bert/optimization.py

- updated docs for optimization

==
==================
94980b529;Thomas Wolf;2019-04-03 11:35:30 +0200;Merge pull request #404 from CatalinVoss/fix_lm_loss
Fix Language Modeling Loss
==
==================
9ca25ce82;Thomas Wolf;2019-04-03 11:26:58 +0200;Merge pull request #427 from jeonsworld/patch-1
fix sample_doc
==
==================
db4dccd1b;Thomas Wolf;2019-04-03 11:21:43 +0200;Merge pull request #389 from lukovnikov/master
Fix cosine schedule
==
==================
19666dcb3;thomwolf;2019-04-03 11:01:01 +0200;Should fix #438

==

pytorch_pretrained_bert/modeling.py
==================
1d8c23232;thomwolf;2019-04-03 10:51:03 +0200;Fix #436

==

pytorch_pretrained_bert/tokenization.py
==================
846b1fd6f;thomwolf;2019-04-03 10:50:38 +0200;Fix #419

==

examples/run_squad.py
==================
404adcdab;Thomas Wolf;2019-04-02 11:40:46 +0200;Merge pull request #437 from MottoX/fix-link
Fix links in README
==
==================
f26ce6992;Weixin Wang;2019-04-02 17:20:32 +0800;Fix links in README

==

README.md
==================
2f80dbbc0;Thomas Wolf;2019-04-02 10:41:56 +0200;Merge pull request #430 from MottoX/master
Fix typo in example code
==
==================
94adad6be;Thomas Wolf;2019-04-02 10:41:40 +0200;Merge pull request #435 from marpaia/training-fixes
Fixes to the TensorFlow conversion tool
==
==================
8b5c63e4d;Mike Arpaia;2019-04-01 12:53:51 -0600;Fixes to the TensorFlow conversion tool

==

examples/extract_features.py
pytorch_pretrained_bert/modeling.py
==================
d07db28f5;Weixin Wang;2019-03-31 01:20:18 +0800;Fix typo in example code
Modify 'unambigiously' to 'unambiguously'
==

examples/run_classifier.py
==================
60005f464;jeonsworld;2019-03-30 14:50:17 +0900;Update pregenerate_training_data.py
If the value of rand_end is returned from the randint function, the value of sampled_doc_index that matches current_idx is returned from searchsorted.

example:
cumsum_max = {int64} 30
doc_cumsum = {ndarray} [ 5  7 11 19 30]
doc_lengths = {list} <class 'list'>: [5, 2, 4, 8, 11]
if current_idx  = 1,
rand_start = 7
rand_end = 35
sentence_index = randint(7, 35) % cumsum_max
if randint return 35, sentence_index becomes 5.
if sentence_index is 5, np.searchsorted returns 1 equal to current_index.
==

examples/lm_finetuning/pregenerate_training_data.py
==================
4d3721f9b;Dhanajit Brahma;2019-03-29 21:56:47 +0530;Just updating
Merge remote-tracking branch 'upstream/master'

==
==================
ec5c1d613;Thomas Wolf;2019-03-29 09:14:11 +0100;Merge pull request #425 from Separius/patch-1
fix lm_finetuning's link
==
==================
b588ff362;Sepehr Sameni;2019-03-29 12:39:24 +0430;fix lm_finetuning's link

==

README.md
==================
f872eb98c;dhanajitb;2019-03-28 22:46:15 +0530;making unconditional generation work
The unconditional generation works now but if the seed is fixed, the sample is the same every time.
n_samples > 1 will give different samples though.
I am giving the start token as '<|endoftext|>' for the unconditional generation.
==

examples/run_gpt2.py
==================
694e2117f;Thomas Wolf;2019-03-28 09:06:53 +0100;Merge pull request #388 from ananyahjha93/master
Added remaining GLUE tasks to 'run_classifier.py'
==
==================
01520d541;Catalin Voss;2019-03-27 10:45:11 -0700;Remove my unhelpful comments :)

==

pytorch_pretrained_bert/modeling_gpt2.py
pytorch_pretrained_bert/modeling_openai.py
==================
f7c9dc8c9;Thomas Wolf;2019-03-27 12:30:03 +0100;Merge pull request #409 from ikuyamada/master
Remove padding_idx from position_embeddings and token_type_embeddings
==
==================
cc8c2d233;Thomas Wolf;2019-03-27 12:03:26 +0100;Merge pull request #396 from IndexFziQ/IndexFziQ
 add tqdm to the process of eval in examples/run_swag.py
==
==================
bbff03fbf;Thomas Wolf;2019-03-27 12:03:00 +0100;Merge pull request #394 from desireevl/master
Minor change in README
==
==================
2fb8ddeef;Thomas Wolf;2019-03-27 12:02:36 +0100;Merge pull request #392 from Rocketknight1/master
Add full language model fine-tuning
==
==================
34561e61a;thomwolf;2019-03-27 12:00:04 +0100;update main readme also

==

README.md
==================
361aff6de;thomwolf;2019-03-27 11:54:59 +0100;typos

==

examples/lm_finetuning/README.md
==================
cea8ba1d5;thomwolf;2019-03-27 11:53:44 +0100;adjusted formating and some wording in the readme

==

examples/lm_finetuning/README.md
==================
0401317b2;Ikuya Yamada;2019-03-26 21:56:35 +0900;Remove padding_idx from position_embeddings and token_type_embeddings

==

pytorch_pretrained_bert/modeling.py
==================
24e67fbf7;Matthew Carrigan;2019-03-25 12:33:30 +0000;Minor README update

==

examples/lm_finetuning/README.md
==================
8d1d1ffde;Matthew Carrigan;2019-03-25 12:15:19 +0000;Corrected the displayed loss when gradient_accumulation_steps > 1

==

examples/lm_finetuning/finetune_on_pregenerated.py
==================
fda2f6239;Catalin Voss;2019-03-24 14:37:13 -0700;Fix test failures due to old torch issue with non-contiguous view

==

pytorch_pretrained_bert/modeling_gpt2.py
pytorch_pretrained_bert/modeling_openai.py
==================
0dd796e35;Catalin Voss;2019-03-24 14:35:55 -0700;Also fix loss function issue with the double head models

==

pytorch_pretrained_bert/modeling_gpt2.py
pytorch_pretrained_bert/modeling_openai.py
==================
472857c47;Catalin Voss;2019-03-24 13:49:42 -0700;Fix typo syntax err (sorry, c/p from my repo)

==

pytorch_pretrained_bert/modeling_gpt2.py
pytorch_pretrained_bert/modeling_openai.py
==================
2e6f5ffb9;Catalin Voss;2019-03-24 13:36:46 -0700;Fix GPT language model loss here as well

==

pytorch_pretrained_bert/modeling_openai.py
==================
5938f31fa;Catalin Voss;2019-03-24 13:35:32 -0700;Fix c/p typo from my experiment code

==

pytorch_pretrained_bert/modeling_gpt2.py
==================
7797d21b8;Catalin Voss;2019-03-24 13:34:30 -0700;Fix GPT2 language modeling loss computation

==

pytorch_pretrained_bert/modeling_gpt2.py
==================
f47197916;Ananya Harsh Jha;2019-03-21 15:38:30 -0400;added GLUE dev set results and details on how to run GLUE tasks

==

README.md
==================
abb7d1ff6;Matthew Carrigan;2019-03-21 17:50:03 +0000;Added proper context management to ensure cleanup happens in the right order.

==

examples/lm_finetuning/pregenerate_training_data.py
==================
06a30cfdf;Matthew Carrigan;2019-03-21 17:04:12 +0000;Added a --reduce_memory option to the training script to keep training data on disc as a memmap rather than in memory

==

examples/lm_finetuning/README.md
==================
7d1ae644e;Matthew Carrigan;2019-03-21 17:02:18 +0000;Added a --reduce_memory option to the training script to keep training data on disc as a memmap rather than in memory

==

examples/lm_finetuning/finetune_on_pregenerated.py
examples/lm_finetuning/pregenerate_training_data.py
==================
2bba7f810;Matthew Carrigan;2019-03-21 16:50:16 +0000;Added a --reduce_memory option to shelve docs to disc instead of keeping them in memory.

==

examples/lm_finetuning/pregenerate_training_data.py
==================
8733ffcb5;Matthew Carrigan;2019-03-21 14:09:57 +0000;Removing a couple of other old unnecessary comments

==

examples/lm_finetuning/pregenerate_training_data.py
==================
8a861048d;Matthew Carrigan;2019-03-21 14:08:39 +0000;Fixed up the notes on a possible future low-memory path

==

examples/lm_finetuning/finetune_on_pregenerated.py
examples/lm_finetuning/pregenerate_training_data.py
==================
a8a577ba9;Matthew Carrigan;2019-03-21 14:05:52 +0000;Reduced memory usage for pregenerating the data a lot by writing it out on the fly without shuffling - the Sampler in the finetuning script will shuffle for us.

==

examples/lm_finetuning/finetune_on_pregenerated.py
==================
0ae59e662;Matthew Carrigan;2019-03-21 14:04:17 +0000;Reduced memory usage for pregenerating the data a lot by writing it out on the fly without shuffling - the Sampler in the finetuning script will shuffle for us.

==

examples/lm_finetuning/finetune_on_pregenerated.py
examples/lm_finetuning/pregenerate_training_data.py
==================
6a9038ba5;Matthew Carrigan;2019-03-21 13:36:41 +0000;Removed an old irrelevant comment

==

examples/lm_finetuning/pregenerate_training_data.py
==================
77944d1b3;Yuqiang Xie;2019-03-21 20:59:33 +0800;add tqdm to the process of eval
Maybe better.
==

examples/run_swag.py
==================
d52f914e2;Desiree Vogt-Lee;2019-03-21 15:02:59 +1000;weigths to weights

==

README.md
==================
29a392fbc;Matthew Carrigan;2019-03-20 17:35:17 +0000;Small README changes

==

examples/lm_finetuning/README.md
==================
832b2b005;Matthew Carrigan;2019-03-20 17:31:49 +0000;Adding README

==

examples/lm_finetuning/README.md
==================
934d3f4d2;Matthew Carrigan;2019-03-20 17:23:23 +0000;Syncing up argument names between the scripts

==

examples/lm_finetuning/pregenerate_training_data.py
examples/lm_finetuning/simple_lm_finetuning.py
==================
f19ba35b2;Matthew Carrigan;2019-03-20 16:47:06 +0000;Move old finetuning script into the new folder

==

examples/lm_finetuning/simple_lm_finetuning.py
==================
7de5c6aa5;Matthew Carrigan;2019-03-20 16:44:04 +0000;PEP8 and formatting cleanups

==

examples/lm_finetuning/finetune_on_pregenerated.py
examples/lm_finetuning/pregenerate_training_data.py
==================
1798e98e5;Matthew Carrigan;2019-03-20 16:42:37 +0000;Added final TODOs

==

examples/lm_finetuning/finetune_on_pregenerated.py
examples/lm_finetuning/pregenerate_training_data.py
==================
c64c2fc4c;Matthew Carrigan;2019-03-20 15:42:57 +0000;Fixed embarrassing indentation problem

==

examples/lm_finetuning/finetune_on_pregenerated.py
==================
0540d360f;Matthew Carrigan;2019-03-20 15:36:51 +0000;Fixed logging

==

examples/lm_finetuning/finetune_on_pregenerated.py
==================
976554a47;Matthew Carrigan;2019-03-20 14:23:51 +0000;First commit of the new LM finetuning

==

examples/lm_finetuning/finetune_on_pregenerated.py
examples/lm_finetuning/pregenerate_training_data.py
==================
262a9992d;lukovnikov;2019-03-18 18:29:12 +0100;class weights

==

pytorch_pretrained_bert/optimization.py
tests/optimization_test.py
==================
19cc2c084;lukovnikov;2019-03-18 15:13:35 +0100;same

==

pytorch_pretrained_bert/optimization_openai.py
==================
2283dcca5;lukovnikov;2019-03-18 13:40:12 +0100;import revert

==

pytorch_pretrained_bert/__init__.py
==================
b6c1cae67;lukovnikov;2019-03-18 13:32:04 +0100;branches, optim cosine fix

==

pytorch_pretrained_bert/optimization.py
==================
ef28b2c74;lukovnikov;2019-03-18 13:18:07 +0100;branches, optim cosine fix

==

pytorch_pretrained_bert/optimization.py
==================
90430ae7e;lukovnikov;2019-03-18 13:15:29 +0100;Merge remote-tracking branch 'origin/master'
# Conflicts:
#	pytorch_pretrained_bert/optimization.py

==
==================
bed6408dc;lukovnikov;2019-03-18 13:09:55 +0100;branches, optim cosine fix

==

pytorch_pretrained_bert/optimization.py
==================
e5b63fb54;Ananya Harsh Jha;2019-03-17 08:30:13 -0400;Merge branch 'master' of https://github.com/ananyahjha93/pytorch-pretrained-BERT
pull current master to local

==
==================
8a4e90ff4;Ananya Harsh Jha;2019-03-17 08:16:50 -0400;corrected folder creation error for MNLI-MM, verified GLUE results

==

examples/run_classifier.py
==================
e0bf01d9a;Ananya Harsh Jha;2019-03-16 14:10:48 -0400;added hack for mismatched MNLI

==

examples/run_classifier.py
==================
4c721c6b6;Ananya Harsh Jha;2019-03-15 23:21:24 -0400;added eval time metrics for GLUE tasks

==

examples/run_classifier.py
==================
f3e540488;Thomas Wolf;2019-03-15 12:54:40 +0100;Merge pull request #381 from tseretelitornike/master
Added missing imports.
==
==================
83857ffea;tseretelitornike;2019-03-15 12:45:48 +0100;Added missing imports.

==

examples/run_swag.py
==================
d5c037c3e;Thomas Wolf;2019-03-14 15:56:40 +0100;Merge pull request #380 from yongbowin/patch-3
typo in annotation
==
==================
d1e4fa98a;Yongbo Wang;2019-03-14 17:32:15 +0800;typo in annotation
modify `heruistic` to `heuristic` in line 660, `charcter` to `character` in line 661.
==

examples/run_squad.py
==================
59e2bdd08;Thomas Wolf;2019-03-14 10:17:18 +0100;Merge pull request #379 from yongbowin/patch-2
typo
==
==================
3d6452163;Yongbo Wang;2019-03-14 17:03:38 +0800;typo
modify `mull` to `null` in line 474 annotation.
==

examples/run_squad.py
==================
76906372b;Thomas Wolf;2019-03-14 10:00:47 +0100;Merge pull request #378 from huggingface/absolute_imports
Add absolute imports to GPT, GPT-2, Transfo-XL and and fix empty nbest_predictions.json
==
==================
a98dfe4ce;thomwolf;2019-03-14 09:57:06 +0100;fixing #377 (empty nbest_predictions.json)

==

examples/run_squad.py
==================
e5f2d9122;thomwolf;2019-03-14 09:55:01 +0100;adding absolute imports to gpt2, openai and transfo-xl

==

pytorch_pretrained_bert/modeling_gpt2.py
pytorch_pretrained_bert/modeling_openai.py
pytorch_pretrained_bert/modeling_transfo_xl.py
==================
043c8781e;Ananya Harsh Jha;2019-03-14 04:24:04 -0400;added code for all glue task processors

==

examples/run_classifier.py
==================
eecaaa734;Thomas Wolf;2019-03-14 09:03:32 +0100;Merge pull request #371 from yongbowin/patch-1
Simplify code, delete redundancy line
==
==================
20e652209;lukovnikov;2019-03-13 16:13:37 +0100;relation classification: replacing entity mention with mask token

==

pytorch_pretrained_bert/optimization.py
==================
22a465a91;Yongbo Wang;2019-03-13 09:42:06 +0800;Simplify code, delete redundancy line
delete redundancy line `if args.train`, simplify code.
==

examples/run_classifier.py
==================
eac039d21;lukovnikov;2019-03-12 13:45:12 +0100;changing docker

==

pytorch_pretrained_bert/optimization.py
==================
471daf1b6;lukovnikov;2019-03-12 13:32:42 +0100;changing docker

==

pytorch_pretrained_bert/optimization.py
==================
902461333;lukovnikov;2019-03-12 13:23:58 +0100;changing docker

==

pytorch_pretrained_bert/optimization.py
==================
baf66d141;lukovnikov;2019-03-12 13:22:23 +0100;restart cosine lr schedule

==

pytorch_pretrained_bert/optimization.py
==================
9b03d67b8;Thomas Wolf;2019-03-11 09:08:51 +0100;Merge pull request #362 from Bharat123rox/patch-1
Make the hyperlink of NVIDIA Apex clickable
==
==================
8435d78f0;Thomas Wolf;2019-03-11 09:08:27 +0100;Merge pull request #361 from junjieqian/jqian/updateReadme
Correct line number in README for classes
==
==================
80790705e;Thomas Wolf;2019-03-11 09:07:56 +0100;Merge pull request #359 from elonmuskceo/fix-typo
Update run_gpt2.py
==
==================
13aa13dbc;Thomas Wolf;2019-03-11 09:06:55 +0100;Merge pull request #358 from cdjhz/patch-1
add 'padding_idx=0' for BertEmbeddings
==
==================
c0660df5d;Thomas Wolf;2019-03-11 09:06:27 +0100;Merge pull request #357 from pglock/feature/354-use-dropout-layer-gpt
Use Dropout Layer in OpenAIGPTMultipleChoiceHead
==
==================
f91ce0b80;Bharat Raghunathan;2019-03-09 20:05:39 +0530;Make the hyperlink of NVIDIA Apex clickable

==

pytorch_pretrained_bert/modeling.py
==================
51efde54a;lukovnikov;2019-03-09 02:45:25 +0100;cos fix

==

pytorch_pretrained_bert/optimization.py
==================
f113a2dfd;lukovnikov;2019-03-09 02:29:57 +0100;readme de

==

pytorch_pretrained_bert/optimization.py
==================
90a41dbe1;lukovnikov;2019-03-09 02:23:20 +0100;BertAdam schedule objects

==

pytorch_pretrained_bert/__init__.py
pytorch_pretrained_bert/optimization.py
==================
d648a0220;Junjie Qian;2019-03-08 16:28:03 -0800;Correct line number in README for classes

==

README.md
==================
88874f6cf;lukovnikov;2019-03-08 19:08:30 +0100;BertAdam schedule objects

==

pytorch_pretrained_bert/optimization.py
==================
66d820680;Elon Musk;2019-03-08 11:59:08 -0500;Update run_gpt2.py

==

examples/run_gpt2.py
==================
72fa8d03a;Haozhe Ji;2019-03-07 20:02:55 +0800;add 'padding_idx=0' for BertEmbeddings

==

pytorch_pretrained_bert/modeling.py
==================
6190e8ce4;Philipp Glock;2019-03-07 10:12:45 +0100;Fix: use dropout layer

==

pytorch_pretrained_bert/modeling_openai.py
==================
7cc35c310;thomwolf;2019-03-06 11:43:21 +0100;fix openai gpt example and updating readme

==

README.md
examples/run_openai_gpt.py
==================
906b638ef;thomwolf;2019-03-06 10:24:19 +0100;updating readme

==

README.md
==================
994d86609;thomwolf;2019-03-06 10:21:24 +0100;fixing PYTORCH_PRETRAINED_BERT_CACHE use in examples

==

examples/run_classifier.py
examples/run_squad.py
examples/run_swag.py
==================
2dd8f524f;thomwolf;2019-03-06 10:10:41 +0100;removing test for long sequences error following #337

==

tests/tokenization_test.py
==================
5c85fc397;thomwolf;2019-03-06 10:05:21 +0100;fix typo - logger info

==

examples/extract_features.py
examples/run_classifier.py
examples/run_lm_finetuning.py
examples/run_openai_gpt.py
examples/run_squad.py
examples/run_swag.py
examples/run_transfo_xl.py
pytorch_pretrained_bert/convert_gpt2_checkpoint_to_pytorch.py
pytorch_pretrained_bert/convert_openai_checkpoint_to_pytorch.py
pytorch_pretrained_bert/convert_tf_checkpoint_to_pytorch.py
pytorch_pretrained_bert/convert_transfo_xl_checkpoint_to_pytorch.py
pytorch_pretrained_bert/modeling.py
pytorch_pretrained_bert/modeling_gpt2.py
pytorch_pretrained_bert/modeling_openai.py
pytorch_pretrained_bert/modeling_transfo_xl.py
pytorch_pretrained_bert/modeling_transfo_xl_utilities.py
pytorch_pretrained_bert/optimization.py
pytorch_pretrained_bert/optimization_openai.py
pytorch_pretrained_bert/tokenization.py
pytorch_pretrained_bert/tokenization_gpt2.py
pytorch_pretrained_bert/tokenization_openai.py
pytorch_pretrained_bert/tokenization_transfo_xl.py
==================
8e36da7ac;Thomas Wolf;2019-03-06 09:48:27 +0100;Merge pull request #347 from jplehmann/feature/sst2-processor
Processor for SST-2 task
==
==================
21c88a07b;Thomas Wolf;2019-03-06 09:48:01 +0100;Merge pull request #341 from potatochip/patch-1
catch exception if pathlib not install
==
==================
3c01dfb77;Thomas Wolf;2019-03-06 09:47:33 +0100;Merge pull request #338 from CatalinVoss/patch-3
Fix top k generation for k != 0
==
==================
477ec4b6c;Thomas Wolf;2019-03-06 09:45:49 +0100;Merge pull request #337 from CatalinVoss/patch-2
Allow tokenization of sequences > 512 for caching
==
==================
7b9e5a54b;Thomas Wolf;2019-03-06 09:44:56 +0100;Merge pull request #327 from lukovnikov/master
Issue#324: warmup linear fixes
==
==================
4784b04f4;Thomas Wolf;2019-03-06 09:37:11 +0100;Merge pull request #325 from john-hewitt/master
add BertTokenizer flag to skip basic tokenization
==
==================
4a49c2258;Catalin Voss;2019-03-05 12:31:45 -0800;Warn instead of raising in BERT and GPT-2 tokenizers as well, to allow for pre-caching of tokens

==

pytorch_pretrained_bert/tokenization.py
pytorch_pretrained_bert/tokenization_gpt2.py
==================
e99bc87e4;Catalin Voss;2019-03-05 12:24:18 -0800;Merge branch 'patch-1' into patch-2

==
==================
0f96d4b1f;John Lehmann;2019-03-05 13:38:28 -0600;Run classifier processor for SST-2.

==

examples/run_classifier.py
==================
0c970caa4;Aaron Mangum;2019-03-04 14:30:19 -0800;catch exception if pathlib not install

==

pytorch_pretrained_bert/file_utils.py
==================
4b4b07927;Catalin Voss;2019-03-02 21:54:44 -0800;Fix top k generation for k != 0

==

examples/run_gpt2.py
==================
9775b2eb2;Catalin Voss;2019-03-02 16:30:21 -0800;Allow tokenization of sequences > 512 for caching
For many applications requiring randomized data access, it's easier to cache the tokenized representations than the words. So why not turn this into a warning?
==

pytorch_pretrained_bert/tokenization_openai.py
==================
c0cf0a04d;Catalin Voss;2019-02-27 18:01:06 -0800;Fix typo

==

examples/run_openai_gpt.py
==================
4d1ad8323;John Hewitt;2019-02-27 14:50:41 -0800;update docstring of BERT tokenizer to reflect do_wordpiece_only

==

pytorch_pretrained_bert/tokenization.py
==================
35410da75;lukovnikov;2019-02-27 17:11:42 +0100;added warning

==

pytorch_pretrained_bert/optimization.py
pytorch_pretrained_bert/optimization_openai.py
==================
4d79e0d38;lukovnikov;2019-02-27 16:50:05 +0100;added warning

==

pytorch_pretrained_bert/optimization.py
pytorch_pretrained_bert/optimization_openai.py
==================
66a84b63b;lukovnikov;2019-02-27 16:38:00 +0100;added warning

==

pytorch_pretrained_bert/optimization.py
pytorch_pretrained_bert/optimization_openai.py
==================
070f3b21d;lukovnikov;2019-02-27 16:26:45 +0100;added warning

==

pytorch_pretrained_bert/optimization.py
pytorch_pretrained_bert/optimization_openai.py
==================
46ef64601;lukovnikov;2019-02-27 16:22:27 +0100;added warning

==

pytorch_pretrained_bert/optimization.py
pytorch_pretrained_bert/optimization_openai.py
==================
9bc3773c8;lukovnikov;2019-02-27 16:10:31 +0100;added warning

==

pytorch_pretrained_bert/optimization.py
==================
60a372387;lukovnikov;2019-02-27 15:54:09 +0100;added warning

==

pytorch_pretrained_bert/optimization.py
pytorch_pretrained_bert/optimization_openai.py
==================
e14c6b52e;John Hewitt;2019-02-26 20:11:24 -0800;add BertTokenizer flag to skip basic tokenization

==

README.md
pytorch_pretrained_bert/tokenization.py
==================
da2d8ca26;lukovnikov;2019-02-26 17:16:06 +0100;fix for negative learning rate with warmup_linear in BertAdam (happens when t_total is specified incorrectly) + copied BERT optimization warmup functions to OpenAI optimization file + added comments

==

pytorch_pretrained_bert/optimization.py
pytorch_pretrained_bert/optimization_openai.py
==================
e04bab59e;lukovnikov;2019-02-26 16:22:52 +0100;fix for negative learning rate with warmup_linear in BertAdam (happens when t_total is specified incorrectly) + copied BERT optimization warmup functions to OpenAI optimization file + added comments

==

pytorch_pretrained_bert/optimization.py
pytorch_pretrained_bert/optimization_openai.py
==================
2152bfeae;Thomas Wolf;2019-02-24 09:38:29 +0100;Merge pull request #316 from joelgrus/gpt2docs
update documentation for gpt-2
==
==================
8722e9eb3;Joel Grus;2019-02-23 06:31:59 -0800;finish updating docstrings

==

README.md
pytorch_pretrained_bert/modeling_gpt2.py
==================
33aa7a80c;Joel Grus;2019-02-22 15:37:59 -0800;update documentation

==

pytorch_pretrained_bert/modeling_gpt2.py
==================
a5b3a8954;Thomas Wolf;2019-02-21 10:23:27 +0100;Merge pull request #310 from spolu/spolu-nits_gpt2
Few small nits in GPT-2's README code examples
==
==================
ff22b3acc;Stanislas Polu;2019-02-21 09:15:27 +0000;Few small nits in GPT-2's code examples

==

README.md
==================
cbb7fad31;Thomas Wolf;2019-02-21 09:25:19 +0100;Merge pull request #307 from guotong1988/patch-1
Update README.md
==
==================
09efcece7;Tong Guo;2019-02-21 11:25:33 +0800;Update README.md

==

README.md
==================
97c815dae;Thomas Wolf;2019-02-20 21:24:06 +0100;Merge pull request #305 from bkj/patch-1
Update run_openai_gpt.py
==
==================
860723367;Ben Johnson;2019-02-20 13:58:54 -0500;Update run_openai_gpt.py

==

examples/run_openai_gpt.py
==================
f50b82af0;Thomas Wolf;2019-02-20 14:14:02 +0100;Merge pull request #302 from yongbowin/master
typo
==
==================
2fdab323d;Yongbo Wang;2019-02-20 21:11:06 +0800;typo

==

pytorch_pretrained_bert/tokenization_transfo_xl.py
==================
813e4d18b;Yongbo Wang;2019-02-20 21:10:07 +0800;typo

==

pytorch_pretrained_bert/tokenization.py
==================
833774075;Thomas Wolf;2019-02-19 14:00:28 +0100;Merge pull request #295 from tnlin/master
fix broken link in readme
==
==================
5b0e0b61f;Tony Lin;2019-02-19 20:34:18 +0800;fix typo in readme

==

README.md
==================
3ca35b99b;Thomas Wolf;2019-02-19 09:00:01 +0100;Merge pull request #293 from davidefiocco/patch-2
Minor README typos corrected
==
==================
0ae8eece5;Davide Fiocco;2019-02-18 21:28:28 +0100;MInor README typos corrected

==

README.md
==================
07ebe0fd0;Thomas Wolf;2019-02-18 21:07:39 +0100;Merge pull request #292 from sam-qordoba/patch-3
Fix typo in `GPT2Model` code sample
==
==================
1cb9c76ec;sam-qordoba;2019-02-18 09:27:26 -0800;Fix typo in `GPT2Model` code sample
Typo prevented code from running
==

README.md
==================
a25d056b7;Thomas Wolf;2019-02-18 15:30:11 +0100;update readme

==

README.md
==================
517d7c862;Thomas Wolf;2019-02-18 14:39:55 +0100;update readme

==

README.md
==================
ada22a1c9;Thomas Wolf;2019-02-18 14:37:41 +0100;more details in GPT-2 usage example

==

README.md
==================
522733f6c;Thomas Wolf;2019-02-18 14:32:10 +0100;readme typo fixes

==

README.md
==================
0202da027;thomwolf;2019-02-18 13:51:42 +0100;remove unnecessary example

==

examples/run_gpt2_generate_unconditional_samples.py
==================
8f46cd105;Thomas Wolf;2019-02-18 12:00:11 +0100;Merge pull request #288 from huggingface/gpt2
forgot to add regex to requirements.txt :(
==
==================
e0855e892;thomwolf;2019-02-18 11:54:51 +0100;forgot to add regex to requirements :(

==

pytorch_pretrained_bert/__init__.py
requirements.txt
setup.py
==================
0856a231c;Thomas Wolf;2019-02-18 11:38:05 +0100;Merge pull request #287 from huggingface/gpt2
Gpt2
==
==================
ab7f5d294;thomwolf;2019-02-18 11:33:54 +0100;simple

==

pytorch_pretrained_bert/tokenization_gpt2.py
==================
b450a7faf;thomwolf;2019-02-18 11:27:18 +0100;clean up tokenization - fix python 2 tests

==

pytorch_pretrained_bert/tokenization_gpt2.py
==================
d44db1145;thomwolf;2019-02-18 11:12:09 +0100;update readme

==

README.md
pytorch_pretrained_bert/tokenization_gpt2.py
==================
690a0dbf3;thomwolf;2019-02-18 10:50:30 +0100;fix example - masking

==

examples/run_gpt2.py
pytorch_pretrained_bert/modeling_gpt2.py
==================
fbb248a2e;thomwolf;2019-02-18 01:28:18 +0100;examples testing

==

examples/run_gpt2_generate_unconditional_samples.py
examples/run_gpt2_interactive_conditional_samples.py
pytorch_pretrained_bert/modeling_gpt2.py
==================
5ff0c6050;thomwolf;2019-02-18 00:55:47 +0100;language update

==

pytorch_pretrained_bert/modeling_gpt2.py
==================
210d40724;thomwolf;2019-02-18 00:55:39 +0100;updating init

==

pytorch_pretrained_bert/__init__.py
==================
b65f07d8c;thomwolf;2019-02-18 00:55:33 +0100;adding examples

==

examples/run_gpt2_generate_unconditional_samples.py
examples/run_gpt2_interactive_conditional_samples.py
==================
009ee86a1;thomwolf;2019-02-17 23:57:23 +0100;fix tests - bump up version

==

pytorch_pretrained_bert/__init__.py
pytorch_pretrained_bert/modeling_gpt2.py
pytorch_pretrained_bert/modeling_openai.py
setup.py
tests/modeling_gpt2_test.py
tests/tokenization_gpt2_test.py
==================
ffd623823;thomwolf;2019-02-17 23:38:51 +0100;adding gpt2

==

pytorch_pretrained_bert/__init__.py
pytorch_pretrained_bert/__main__.py
pytorch_pretrained_bert/convert_gpt2_checkpoint_to_pytorch.py
pytorch_pretrained_bert/modeling_gpt2.py
pytorch_pretrained_bert/tokenization_gpt2.py
tests/modeling_gpt2_test.py
tests/tokenization_gpt2_test.py
==================
3a2f97db6;Thomas Wolf;2019-02-17 15:30:46 +0100;Merge pull request #286 from hendrycks/patch-1
Update activation function docstring
==
==================
434d15da8;Dan Hendrycks;2019-02-16 12:17:52 -0800;Update activation function docstring

==

pytorch_pretrained_bert/modeling.py
==================
5faf38665;Thomas Wolf;2019-02-15 10:06:51 +0100;Merge pull request #282 from wlhgtc/master
Fix some bug about SQuAD code
==
==================
8efaf8f17;wlhgtc;2019-02-15 15:57:25 +0800;fix 'best_non_null_entry' is None error

==

examples/run_squad.py
==================
0e774e57a;Thomas Wolf;2019-02-14 08:39:58 +0100;Update readme
Adding details on how to extract a full list of hidden states for the Transformer-XL
==

README.md
==================
c35d9d48d;Thomas Wolf;2019-02-13 16:32:21 +0100;Merge pull request #275 from davidefiocco/patch-1
--do_lower_case is duplicated in parser args
==
==================
65df0d78e;Davide Fiocco;2019-02-13 15:30:05 +0100;--do_lower_case is duplicated in parser args
Deleting one repetition (please review!)
==

examples/run_lm_finetuning.py
==================
4e56da38d;Thomas Wolf;2019-02-13 10:19:25 +0100;Merge pull request #268 from wangxiaodiu/master
fixed a minor bug in README.md
==
==================
cdcb206e1;Thomas Wolf;2019-02-13 10:19:08 +0100;Merge pull request #273 from huggingface/update_to_fifth_release
Update to fifth release
==
==================
321d70a7a;thomwolf;2019-02-13 10:11:20 +0100;bump up to 0.5.1

==

pytorch_pretrained_bert/__init__.py
setup.py
==================
67376c02e;thomwolf;2019-02-13 10:11:11 +0100;update readme for tokenizers

==

README.md
==================
c6bea0844;thomwolf;2019-02-13 10:11:00 +0100;OpenAI GPT Tokenizer can fallback on using BERT BasicTokenizer

==

pytorch_pretrained_bert/tokenization_openai.py
==================
e7cfc46fc;thomwolf;2019-02-13 09:32:46 +0100;fix TransfoXLModel loading

==

pytorch_pretrained_bert/modeling_transfo_xl.py
==================
e1b3cfb50;Liang Niu;2019-02-12 15:54:23 +0400;fixed a minor bug in README.md

==

README.md
==================
3c33499f8;Thomas Wolf;2019-02-12 10:22:54 +0100;fix typo in readme

==

README.md
==================
03cdb2a39;Thomas Wolf;2019-02-11 14:19:26 +0100;Merge pull request #254 from huggingface/python_2
Adding OpenAI GPT and Transformer-XL models, compatibility with Python 2
==
==================
1e71f11de;thomwolf;2019-02-11 14:16:27 +0100;Release: 0.5.0

==

README.md
==================
d38caba16;thomwolf;2019-02-11 14:10:27 +0100;typo in run_squad

==

examples/run_squad.py
==================
af62cc5f2;thomwolf;2019-02-11 14:06:32 +0100;fix run_squad example

==

examples/run_squad.py
==================
eebc8abbe;thomwolf;2019-02-11 14:04:19 +0100;clarify and unify model saving logic in examples

==

README.md
examples/run_classifier.py
examples/run_squad.py
examples/run_swag.py
==================
81c7e3ec9;thomwolf;2019-02-11 13:37:12 +0100;fix typo in readme

==

README.md
==================
e8fe6b714;thomwolf;2019-02-11 13:30:04 +0100;adapting transfo tokenizer to transposed inputs

==

pytorch_pretrained_bert/tokenization_transfo_xl.py
==================
884ca81d8;thomwolf;2019-02-11 13:19:59 +0100;transposing the inputs of Transformer-XL to have a unified interface

==

README.md
pytorch_pretrained_bert/modeling_transfo_xl.py
pytorch_pretrained_bert/tokenization_transfo_xl.py
tests/modeling_transfo_xl_test.py
==================
32fea876b;thomwolf;2019-02-11 12:53:32 +0100;add distant debugging to run_transfo_xl

==

README.md
examples/run_transfo_xl.py
==================
b31ba2391;thomwolf;2019-02-11 12:15:43 +0100;cuda on in the examples by default

==

README.md
examples/run_transfo_xl.py
==================
0a9860daa;thomwolf;2019-02-11 10:47:52 +0100;tests pass on python 2 and 3

==

tests/tokenization_openai_test.py
==================
2071a9b86;thomwolf;2019-02-11 10:35:36 +0100;fix python 2.7 imports

==

README.md
pytorch_pretrained_bert/file_utils.py
tests/tokenization_openai_test.py
==================
8197eb9f1;thomwolf;2019-02-11 10:22:10 +0100;update Circle CI config

==

.circleci/config.yml
==================
525eba68a;thomwolf;2019-02-11 10:19:25 +0100;update Circle CI

==

.circleci/config.yml
==================
b514a60c3;thomwolf;2019-02-11 10:17:16 +0100;added tests for OpenAI GPT and Transformer-XL tokenizers

==

README.md
pytorch_pretrained_bert/tokenization_openai.py
pytorch_pretrained_bert/tokenization_transfo_xl.py
tests/tokenization_openai_test.py
tests/tokenization_transfo_xl_test.py
==================
9bdcba53f;thomwolf;2019-02-09 17:07:12 +0100;fix tests

==

tests/modeling_openai_test.py
==================
f0bf81e14;thomwolf;2019-02-09 17:05:23 +0100;back compatibility with Path inputs in fle_utils

==

pytorch_pretrained_bert/file_utils.py
==================
9f9909ea2;thomwolf;2019-02-09 16:59:21 +0100;update readme

==

README.md
==================
6cd769957;thomwolf;2019-02-09 16:59:17 +0100;update transfo xl example

==

examples/run_transfo_xl.py
pytorch_pretrained_bert/modeling_transfo_xl_utilities.py
==================
1320e4ec0;thomwolf;2019-02-09 16:58:53 +0100;mc_token_mask => mc_token_ids

==

examples/run_openai_gpt.py
pytorch_pretrained_bert/modeling_openai.py
tests/modeling_openai_test.py
==================
f4a07a392;thomwolf;2019-02-09 16:14:31 +0100;mems not splitted

==

examples/run_transfo_xl.py
==================
43b9af0ca;thomwolf;2019-02-09 16:12:19 +0100;mems initialized to None in run_transfo

==

examples/run_transfo_xl.py
==================
cfcb95417;thomwolf;2019-02-08 23:08:53 +0100;fix hasattr

==

pytorch_pretrained_bert/modeling_transfo_xl.py
==================
0c1a6f9b1;thomwolf;2019-02-08 22:32:25 +0100;update readme

==

README.md
==================
1756b5e95;thomwolf;2019-02-08 22:32:17 +0100;fix loading from Transfo-XL LM model

==

pytorch_pretrained_bert/modeling_transfo_xl.py
==================
dadd0c1b1;thomwolf;2019-02-08 22:31:57 +0100;updating __main__

==

pytorch_pretrained_bert/__main__.py
==================
102c6b238;thomwolf;2019-02-08 22:31:46 +0100;adding file cache to __init__

==

pytorch_pretrained_bert/__init__.py
==================
b80684b23;thomwolf;2019-02-08 22:31:32 +0100;fixing run openai gpt example

==

examples/run_openai_gpt.py
==================
80607874c;thomwolf;2019-02-08 21:49:05 +0100;fix layer norm epsilon in OpenAI GPT

==

pytorch_pretrained_bert/modeling_openai.py
==================
7b4b0cf96;thomwolf;2019-02-08 11:16:29 +0100;logging

==

examples/run_openai_gpt.py
==================
4bbb9f2d6;thomwolf;2019-02-08 11:14:29 +0100;log loss - helpers

==

examples/run_openai_gpt.py
==================
5d7e84571;thomwolf;2019-02-08 11:08:43 +0100;fix model on cuda

==

examples/run_openai_gpt.py
==================
eccb2f016;thomwolf;2019-02-08 11:05:20 +0100;hot fix

==

examples/run_openai_gpt.py
==================
5adc20723;thomwolf;2019-02-08 11:03:59 +0100;add distant debugging

==

examples/run_openai_gpt.py
==================
5ee4f1723;thomwolf;2019-02-08 10:37:40 +0100;adding option to load on cpu

==

pytorch_pretrained_bert/modeling.py
==================
2dfaf2f22;Thomas Wolf;2019-02-08 10:36:03 +0100;Merge pull request #261 from deepset-ai/rm_arg_lm_finetuning
removing unused argument eval_batch_size from LM finetuning #256
==
==================
777459b47;thomwolf;2019-02-08 10:33:14 +0100;run openai example running

==

examples/run_openai_gpt.py
pytorch_pretrained_bert/modeling_openai.py
pytorch_pretrained_bert/modeling_transfo_xl.py
==================
edcb56fd9;thomwolf;2019-02-08 09:54:49 +0100;more explicit variable name

==

pytorch_pretrained_bert/modeling.py
pytorch_pretrained_bert/modeling_openai.py
pytorch_pretrained_bert/tokenization.py
==================
6bc082da0;thomwolf;2019-02-08 00:02:26 +0100;updating examples

==

examples/run_openai_gpt.py
examples/run_transfo_xl.py
examples/train_openai_gpt.py
examples/train_transfo_xl.py
examples/transfo_xl_eval.py
==================
eb8fda51f;thomwolf;2019-02-07 23:15:20 +0100;update docstrings

==

pytorch_pretrained_bert/modeling_transfo_xl.py
==================
e77721e4f;thomwolf;2019-02-07 23:15:15 +0100;renamed examples

==

examples/train_openai_gpt.py
examples/train_transfo_xl.py
==================
009b58131;thomwolf;2019-02-07 23:15:05 +0100;updated readme

==

README.md
==================
f99f2fb66;thomwolf;2019-02-07 17:07:22 +0100;docstrings

==

README.md
pytorch_pretrained_bert/tokenization_openai.py
==================
438db43d4;thomwolf;2019-02-07 17:07:15 +0100;update adaptive softmax head

==

pytorch_pretrained_bert/modeling_transfo_xl_utilities.py
==================
c306869ea;thomwolf;2019-02-07 17:07:03 +0100;add two transformer xl models

==

pytorch_pretrained_bert/__init__.py
pytorch_pretrained_bert/convert_transfo_xl_checkpoint_to_pytorch.py
pytorch_pretrained_bert/modeling_transfo_xl.py
==================
d482e3d79;thomwolf;2019-02-07 17:06:41 +0100;adding examples for openai and transformer-xl

==

examples/openai_gpt_train.py
examples/transfo_xl_eval.py
examples/transfo_xl_train.py
==================
9c3c24800;thomwolf;2019-02-07 17:06:17 +0100;split saved model in config & weights

==

pytorch_pretrained_bert/modeling_openai.py
==================
2df41663f;thomwolf;2019-02-07 17:05:49 +0100;added test

==

tests/modeling_openai_test.py
tests/modeling_test.py
tests/modeling_transfo_xl_test.py
==================
9aebc711c;tholor;2019-02-07 11:49:38 +0100;adjust error message related to args.do_eval

==

examples/run_lm_finetuning.py
==================
4a450b25d;tholor;2019-02-07 10:06:38 +0100;removing unused argument eval_batch_size from LM finetuning #256

==

examples/run_lm_finetuning.py
==================
58f0a2745;Thomas Wolf;2019-02-06 20:33:18 +0100;Merge pull request #258 from BoeingX/master
Fix the undefined variable in squad example
==
==================
7ac3311e4;Baoyang Song;2019-02-06 19:36:08 +0100;Fix the undefined variable in squad example

==

examples/run_squad.py
==================
ed47cb6cb;thomwolf;2019-02-06 16:22:17 +0100;fixing transfo eval script

==

examples/eval_transfo_xl.py
pytorch_pretrained_bert/modeling_transfo_xl.py
==================
973926431;thomwolf;2019-02-06 15:42:29 +0100;fix differencies with tensorflow version (mem cells and adaptive sofmax clusters)

==

pytorch_pretrained_bert/modeling_transfo_xl.py
pytorch_pretrained_bert/modeling_transfo_xl_utilities.py
==================
ba9e4eb35;thomwolf;2019-02-06 00:28:00 +0100;fix unicode in tokenization tests

==

tests/tokenization_test.py
==================
34bdb7f9c;thomwolf;2019-02-06 00:25:12 +0100;update circle-ci for python 2.7 and 3.5

==

.circleci/config.yml
==================
848aae49e;Thomas Wolf;2019-02-06 00:13:20 +0100;Merge branch 'master' into python_2

==
==================
448937c00;thomwolf;2019-02-06 00:07:46 +0100;python 2 compatibility

==

examples/eval_transfo_xl.py
examples/run_classifier.py
examples/run_lm_finetuning.py
examples/run_squad.py
examples/run_swag.py
pytorch_pretrained_bert/__main__.py
pytorch_pretrained_bert/convert_openai_checkpoint_to_pytorch.py
pytorch_pretrained_bert/convert_transfo_xl_checkpoint_to_pytorch.py
pytorch_pretrained_bert/file_utils.py
pytorch_pretrained_bert/modeling.py
pytorch_pretrained_bert/modeling_openai.py
pytorch_pretrained_bert/modeling_transfo_xl.py
pytorch_pretrained_bert/tokenization.py
pytorch_pretrained_bert/tokenization_openai.py
pytorch_pretrained_bert/tokenization_transfo_xl.py
setup.py
tests/tokenization_test.py
==================
ba37ddc5c;thomwolf;2019-02-06 00:07:08 +0100;fix run_lm_modeling example command line

==

README.md
==================
822915142;thomwolf;2019-02-05 16:34:32 +0100;fix docstring

==

pytorch_pretrained_bert/modeling.py
==================
bd7463268;Thomas Wolf;2019-02-05 16:33:45 +0100;Merge pull request #251 from Iwontbecreative/active_loss_tok_classif
Only keep the active part mof the loss for token classification
==
==================
fd223374f;Thomas Wolf;2019-02-05 16:15:03 +0100;Merge pull request #208 from Liangtaiwan/mergesquad
Merge run_squad.py and run_squad2.py
==
==================
d609ba24c;thomwolf;2019-02-05 16:14:25 +0100;resolving merge conflicts

==

examples/run_squad.py
examples/run_squad2.py
==================
bde1eeebe;thomwolf;2019-02-05 16:11:22 +0100;rename

==

examples/run_squad.py
==================
3ea3b00e5;thomwolf;2019-02-05 16:10:27 +0100;merge squad example in single example

==

examples/run_both_squad.py
==================
d8e3bdbb4;thomwolf;2019-02-05 16:09:39 +0100;moved up to current master

==

examples/run_squad.py
==================
64ce90097;Thomas Wolf;2019-02-05 16:00:51 +0100;Merge pull request #248 from JoeDumoulin/squad1.1-fix
fix prediction on run-squad.py example
==
==================
0ad9b239a;thomwolf;2019-02-05 15:43:11 +0100;gitignore

==

.gitignore
==================
e9e77cd3c;Thomas Wolf;2019-02-05 15:40:44 +0100;Merge pull request #218 from matej-svejda/master
Fix learning rate problems in run_classifier.py
==
==================
1579c5363;thomwolf;2019-02-05 15:36:33 +0100;more explicit notation: num_train_step => num_train_optimization_steps

==

.gitignore
examples/run_classifier.py
examples/run_lm_finetuning.py
examples/run_squad.py
examples/run_squad2.py
examples/run_swag.py
==================
f3bda2352;Thibault Fevry;2019-02-04 11:46:36 -0500;Only keep the active part mof the loss for token classification

==

pytorch_pretrained_bert/modeling.py
==================
6179f537a;thomwolf;2019-02-04 17:41:22 +0100;clean up tokenization spaces

==

pytorch_pretrained_bert/tokenization_openai.py
==================
850da1cc3;thomwolf;2019-02-04 17:35:05 +0100;strip decoded outputs

==

pytorch_pretrained_bert/tokenization_openai.py
==================
01a3966bc;thomwolf;2019-02-04 17:26:25 +0100;more options on special tokens

==

pytorch_pretrained_bert/tokenization_openai.py
==================
05f961840;thomwolf;2019-02-04 13:06:19 +0100;logging

==

pytorch_pretrained_bert/tokenization_openai.py
==================
aa90e0c36;joe dumoulin;2019-02-01 10:15:44 -0800;fix prediction on run-squad.py example

==

examples/run_squad.py
==================
8f8bbd4a4;Thomas Wolf;2019-02-01 12:17:50 +0100;Merge pull request #244 from deepset-ai/prettify_lm_masking
Avoid confusion of inplace LM masking
==
==================
e2d53d95b;Thomas Wolf;2019-02-01 12:14:55 +0100;Merge pull request #242 from ksurya/argparse
Fix argparse type error
==
==================
7e0b415ab;Thomas Wolf;2019-02-01 12:14:05 +0100;Merge pull request #240 from girishponkiya/patch-1
Minor update in README
==
==================
ce75b169b;tholor;2019-01-31 11:42:06 +0100;avoid confusion of inplace masking of tokens_a / tokens_b

==

examples/run_lm_finetuning.py
==================
9bf528877;Surya Kasturi;2019-01-30 15:09:31 -0500;Update run_squad.py

==

examples/run_squad.py
==================
af2b78601;Surya Kasturi;2019-01-30 15:08:56 -0500;Update run_squad2.py

==

examples/run_squad2.py
==================
0dd2b750c;Girishkumar;2019-01-30 23:49:15 +0530;Minor update in README
Update links to classes in `modeling.py`
==

README.md
==================
516906999;Matej Svejda;2019-01-30 11:47:25 +0100;make examples consistent, revert error in num_train_steps calculation

==

examples/run_classifier.py
examples/run_lm_finetuning.py
examples/run_squad.py
examples/run_squad2.py
examples/run_swag.py
==================
3a848111e;thomwolf;2019-01-29 11:00:11 +0100;update config, docstrings and readme to switch to seperated tokens and position embeddings

==

README.md
pytorch_pretrained_bert/modeling_openai.py
==================
98c96fb1a;thomwolf;2019-01-29 10:31:42 +0100;splitting position and tokens embeddings in OpenAI GPT - updating tf imports - tests

==

pytorch_pretrained_bert/__main__.py
pytorch_pretrained_bert/convert_openai_checkpoint_to_pytorch.py
pytorch_pretrained_bert/convert_tf_checkpoint_to_pytorch.py
pytorch_pretrained_bert/modeling.py
pytorch_pretrained_bert/modeling_openai.py
pytorch_pretrained_bert/modeling_transfo_xl.py
tests/modeling_openai_test.py
==================
5456d8231;thomwolf;2019-01-29 09:54:18 +0100;more versatile model loading

==

pytorch_pretrained_bert/modeling.py
pytorch_pretrained_bert/modeling_openai.py
==================
9b2540b5a;thomwolf;2019-01-29 09:54:08 +0100;update __init__

==

pytorch_pretrained_bert/__init__.py
==================
bd3b3aee9;thomwolf;2019-01-28 17:47:29 +0100;update

==

pytorch_pretrained_bert/modeling.py
pytorch_pretrained_bert/modeling_openai.py
==================
a45a9cc0e;thomwolf;2019-01-28 17:16:02 +0100;update tests

==

tests/modeling_openai_test.py
==================
b12616fd8;thomwolf;2019-01-28 17:03:39 +0100;updating code organization to fix imports

==

pytorch_pretrained_bert/convert_openai_checkpoint_to_pytorch.py
pytorch_pretrained_bert/convert_tf_checkpoint_to_pytorch.py
pytorch_pretrained_bert/convert_transfo_xl_checkpoint_to_pytorch.py
pytorch_pretrained_bert/modeling.py
pytorch_pretrained_bert/modeling_openai.py
pytorch_pretrained_bert/modeling_transfo_xl.py
pytorch_pretrained_bert/modeling_transfo_xl_utilities.py
==================
d77dd62ff;thomwolf;2019-01-28 16:50:23 +0100;directly load from TF checkpoints + code cleanup

==

pytorch_pretrained_bert/__init__.py
pytorch_pretrained_bert/convert_openai_checkpoint_to_pytorch.py
pytorch_pretrained_bert/convert_tf_checkpoint_to_pytorch.py
pytorch_pretrained_bert/convert_transfo_xl_checkpoint_to_pytorch.py
pytorch_pretrained_bert/modeling.py
pytorch_pretrained_bert/modeling_openai.py
pytorch_pretrained_bert/modeling_transfo_xl.py
pytorch_pretrained_bert/tokenization_openai.py
==================
9c6a48c8c;Matej Svejda;2019-01-27 14:07:24 +0100;fix learning rate/fp16 and warmup problem for all examples

==

examples/run_classifier.py
examples/run_lm_finetuning.py
examples/run_squad.py
examples/run_squad2.py
examples/run_swag.py
==================
01ff4f82b;Matej Svejda;2019-01-22 23:40:06 +0100;learning rate problems in run_classifier.py

==

examples/run_classifier.py
==================
4eb2a49d4;liangtaiwan;2019-01-19 10:18:10 +0800;Merge run_squad.py and run_squad2.py

==

examples/run_squad.py
examples/run_squad2.py
==================
0a9d7c7ed;Thomas Wolf;2019-01-18 09:28:11 +0100;Merge pull request #201 from Liangtaiwan/squad2_save_bug
run_squad2 Don't save model if do not train
==
==================
be9fa192f;liangtaiwan;2019-01-18 00:41:55 +0800;don't save if do not train

==

examples/run_squad2.py
==================
9c35c132f;thomwolf;2019-01-17 09:19:19 +0100;apex LayerNorm

==

pytorch_pretrained_bert/modeling_transfo_xl.py
==================
b9c77b98d;thomwolf;2019-01-17 00:33:21 +0100;fix transposition in model conversion and memory initialization

==

pytorch_pretrained_bert/convert_transfo_xl_checkpoint_to_pytorch.py
pytorch_pretrained_bert/modeling_transfo_xl.py
==================
f040a43cb;Thomas Wolf;2019-01-16 23:51:52 +0100;Merge pull request #199 from davidefiocco/patch-1
(very) minor update to README
==
==================
35115eaf9;Davide Fiocco;2019-01-16 21:05:24 +0100;(very) minor update to README

==

README.md
==================
009101de1;thomwolf;2019-01-16 12:16:20 +0100;fix loading bug and check full conversion of model

==

pytorch_pretrained_bert/convert_transfo_xl_checkpoint_to_pytorch.py
pytorch_pretrained_bert/modeling_transfo_xl.py
==================
fea15cc9f;thomwolf;2019-01-16 11:54:54 +0100;update model conversion

==

pytorch_pretrained_bert/convert_transfo_xl_checkpoint_to_pytorch.py
pytorch_pretrained_bert/modeling_transfo_xl.py
==================
a28dfc865;thomwolf;2019-01-16 11:18:19 +0100;fix eval for wt103

==

examples/eval_transfo_xl.py
==================
c03c12687;thomwolf;2019-01-16 10:55:22 +0100;fix __main__ entry script

==

pytorch_pretrained_bert/__main__.py
==================
8831c6880;thomwolf;2019-01-16 10:31:16 +0100;fixing various parts of model conversion, loading and weights sharing

==

examples/eval_transfo_xl.py
pytorch_pretrained_bert/convert_transfo_xl_checkpoint_to_pytorch.py
pytorch_pretrained_bert/modeling_transfo_xl.py
pytorch_pretrained_bert/tokenization_transfo_xl.py
==================
bcd4aa8fe;thomwolf;2019-01-15 23:32:34 +0100;update evaluation example

==

examples/eval_transfo_xl.py
==================
a69ec2c72;thomwolf;2019-01-15 23:17:46 +0100;improved corpus and tokenization conversion - added evaluation script

==

examples/eval_transfo_xl.py
pytorch_pretrained_bert/__init__.py
pytorch_pretrained_bert/convert_transfo_xl_checkpoint_to_pytorch.py
pytorch_pretrained_bert/modeling_transfo_xl.py
pytorch_pretrained_bert/tokenization_transfo_xl.py
==================
7d03c5371;thomwolf;2019-01-15 16:07:25 +0100;conversion working

==

.gitignore
pytorch_pretrained_bert/__main__.py
pytorch_pretrained_bert/convert_openai_checkpoint_to_pytorch.py
pytorch_pretrained_bert/convert_transfo_xl_checkpoint_to_pytorch.py
pytorch_pretrained_bert/modeling_transfo_xl.py
==================
3a9c88377;thomwolf;2019-01-15 12:59:38 +0100;adding Transformer XL

==

pytorch_pretrained_bert/convert_transfo_xl_checkpoint_to_pytorch.py
pytorch_pretrained_bert/modeling_openai.py
pytorch_pretrained_bert/modeling_transfo_xl.py
pytorch_pretrained_bert/modeling_transfo_xl_utilities.py
pytorch_pretrained_bert/tokenization_transfo_xl.py
==================
647c98353;Thomas Wolf;2019-01-14 09:44:01 +0100;Merge pull request #193 from nhatchan/20190113_global_step
Fix importing unofficial TF models
==
==================
4e0cba105;Thomas Wolf;2019-01-14 09:40:07 +0100;Merge pull request #191 from nhatchan/20190113_py35_finetune
lm_finetuning compatibility with Python 3.5
==
==================
c94455651;Thomas Wolf;2019-01-14 09:39:03 +0100;Merge pull request #190 from nhatchan/20190113_finetune_doc
Fix documentation (missing backslashes)
==
==================
25eae7b0a;Thomas Wolf;2019-01-14 09:38:37 +0100;Merge pull request #189 from donglixp/patch-1
[bug fix] args.do_lower_case is always True
==
==================
cd30565ae;nhatchan;2019-01-14 13:35:40 +0900;Fix importing unofficial TF models
Importing unofficial TF models seems to be working well, at least for me.
This PR resolves #50.

==

pytorch_pretrained_bert/convert_tf_checkpoint_to_pytorch.py
==================
8edc898f6;nhatchan;2019-01-13 21:23:19 +0900;Fix documentation (missing backslashes)
This PR adds missing backslashes in LM Fine-tuning subsection in README.md.

==

README.md
==================
6c65cb249;nhatchan;2019-01-13 21:09:13 +0900;lm_finetuning compatibility with Python 3.5
dicts are not ordered in Python 3.5 or prior, which is a cause of #175.
This PR replaces one with a list, to keep its order.

==

examples/run_lm_finetuning.py
==================
a2da2b410;Li Dong;2019-01-13 19:51:11 +0800;[bug fix] args.do_lower_case is always True
The "default=True" makes args.do_lower_case always True.

```python
parser.add_argument("--do_lower_case",
                        default=True,
                        action='store_true')
```
==

examples/run_squad2.py
==================
35becc6d8;Thomas Wolf;2019-01-11 08:50:13 +0100;Merge pull request #182 from deepset-ai/fix_lowercase_and_saving
add do_lower_case arg and adjust model saving for lm finetuning.
==
==================
506e5bb0c;tholor;2019-01-11 08:31:37 +0100;add do_lower_case arg and adjust model saving for lm finetuning.

==

examples/run_lm_finetuning.py
==================
e485829a4;Thomas Wolf;2019-01-10 23:40:45 +0100;Merge pull request #174 from abeljim/master
Added Squad 2.0
==
==================
7e60205bd;Thomas Wolf;2019-01-10 23:39:10 +0100;Merge pull request #179 from likejazz/patch-2
Fix it to run properly even if without `--do_train` param.
==
==================
64326dccf;Sang-Kil Park;2019-01-10 21:51:39 +0900;Fix it to run properly even if without `--do_train` param.
It was modified similar to `run_classifier.py`, and Fixed to run properly even if without `--do_train` param.
==

examples/run_squad.py
==================
e5c78c668;thomwolf;2019-01-10 01:40:00 +0100;update readme and few typos

==

README.md
examples/extract_features.py
pytorch_pretrained_bert/modeling.py
==================
fa5222c29;thomwolf;2019-01-10 01:25:28 +0100;update readme

==

README.md
examples/run_openai_gpt.py
==================
0dd5f55ac;Thomas Wolf;2019-01-09 13:44:09 +0100;Merge pull request #172 from WrRan/never_split
Never split some texts.
==
==================
b3628f117;Unknown;2019-01-08 15:13:13 -0800;Added Squad 2.0

==

examples/run_squad2.py
==================
ab90d4cdd;thomwolf;2019-01-09 00:12:43 +0100;adding docs and example for OpenAI GPT

==

examples/run_openai_gpt.py
pytorch_pretrained_bert/modeling.py
pytorch_pretrained_bert/modeling_openai.py
==================
dc5df92fa;thomwolf;2019-01-08 17:18:47 +0100;added LM head for OpenAI

==

pytorch_pretrained_bert/__init__.py
pytorch_pretrained_bert/modeling_openai.py
tests/modeling_openai_test.py
==================
3cf12b235;thomwolf;2019-01-08 16:24:23 +0100;added tests + fixed losses

==

pytorch_pretrained_bert/modeling.py
pytorch_pretrained_bert/modeling_openai.py
pytorch_pretrained_bert/tokenization_openai.py
tests/modeling_openai_test.py
==================
eed51c5bd;thomwolf;2019-01-08 12:26:58 +0100;add OpenAI GPT

==

pytorch_pretrained_bert/__init__.py
pytorch_pretrained_bert/__main__.py
pytorch_pretrained_bert/convert_openai_checkpoint_to_pytorch.py
pytorch_pretrained_bert/modeling.py
pytorch_pretrained_bert/modeling_openai.py
pytorch_pretrained_bert/optimization_openai.py
pytorch_pretrained_bert/tokenization_openai.py
setup.py
==================
3f60a60ee;WrRan;2019-01-08 13:33:57 +0800;text in never_split should not lowercase

==

pytorch_pretrained_bert/tokenization.py
==================
751beb9e7;WrRan;2019-01-08 10:54:51 +0800;never split some text

==

pytorch_pretrained_bert/tokenization.py
==================
793dcd236;thomwolf;2019-01-07 13:37:55 +0100;Merge branch 'master' of https://github.com/huggingface/pytorch-pretrained-BERT into fifth-release

==
==================
2e4db64ca;thomwolf;2019-01-07 13:06:42 +0100;add do_lower_case tokenizer loading optino in run_squad and ine_tuning examples

==

examples/run_lm_finetuning.py
examples/run_squad.py
==================
c9fd35056;thomwolf;2019-01-07 13:01:54 +0100;remove default when action is store_true in arguments

==

examples/extract_features.py
examples/run_classifier.py
examples/run_lm_finetuning.py
examples/run_squad.py
examples/run_swag.py
==================
93f563b8a;thomwolf;2019-01-07 12:55:36 +0100;adding OpenAI GPT

==

pytorch_pretrained_bert/convert_openai_checkpoint_to_pytorch.py
pytorch_pretrained_bert/modeling.py
pytorch_pretrained_bert/modeling_openai.py
pytorch_pretrained_bert/optimization_openai.py
pytorch_pretrained_bert/tokenization_openai.py
==================
e048c7f1c;Thomas Wolf;2019-01-07 12:44:46 +0100;Merge pull request #171 from donglixp/patch-1
LayerNorm initialization
==
==================
d3d56f9a0;Thomas Wolf;2019-01-07 12:40:55 +0100;Merge pull request #166 from likejazz/patch-1
Fix error when `bert_model` param is path or url.
==
==================
766c6b2ce;Thomas Wolf;2019-01-07 12:31:06 +0100;Merge pull request #159 from jaderabbit/master
Allow do_eval to be used without do_train and to use the pretrained model in the output folder
==
==================
77966a43a;Thomas Wolf;2019-01-07 12:27:16 +0100;Merge pull request #156 from rodgzilla/cl_args_doc
Adding new pretrained model to the help of the `bert_model` argument.
==
==================
bcd607542;Thomas Wolf;2019-01-07 12:23:05 +0100;Merge pull request #145 from wlhgtc/master
Correct the  wrong note
==
==================
2e8c5c00e;Thomas Wolf;2019-01-07 12:21:13 +0100;Merge pull request #141 from SinghJasdeep/patch-1
loading saved model when n_classes != 2
==
==================
286037702;Thomas Wolf;2019-01-07 12:06:06 +0100;Merge pull request #134 from rodgzilla/update_doc_pretrained_models
Fixing various class documentations.
==
==================
c18bdb443;Thomas Wolf;2019-01-07 12:03:51 +0100;Merge pull request #124 from deepset-ai/master
Add example for fine tuning BERT language model
==
==================
d0d9b384f;Li Dong;2019-01-07 15:51:33 +0800;LayerNorm initialization
The LayerNorm gamma and beta should be initialized by .fill_(1.0) and .zero_().

reference links:

https://github.com/tensorflow/tensorflow/blob/989e78c412a7e0f5361d4d7dfdfb230c8136e749/tensorflow/contrib/layers/python/layers/layers.py#L2298

https://github.com/tensorflow/tensorflow/blob/989e78c412a7e0f5361d4d7dfdfb230c8136e749/tensorflow/contrib/layers/python/layers/layers.py#L2308
==

pytorch_pretrained_bert/modeling.py
==================
ca4e7aaa7;Sang-Kil Park;2019-01-05 11:42:54 +0900;Fix error when `bert_model` param is path or url.
Error occurs when `bert_model` param is path or url. Therefore, if it is path, specify the last path to prevent error.
==

examples/run_squad.py
==================
193e2df8b;Jade Abbott;2019-01-03 13:13:06 +0200;Remove rogue comment

==

examples/run_classifier.py
==================
c64de50ea;Jade Abbott;2019-01-03 12:34:57 +0200;nb_tr_steps is not initialized

==

examples/run_classifier.py
==================
b96149a19;Jade Abbott;2019-01-03 10:31:56 +0200;Training loss is not initialized if only do_eval is specified

==

examples/run_classifier.py
==================
be3b9bcf4;Jade Abbott;2019-01-03 09:02:33 +0200;Allow one to use the pretrained model in evaluation when do_train is not selected

==

examples/run_classifier.py
==================
186f75342;Gr√©gory Ch√¢tel;2019-01-02 14:00:59 +0100;Adding new pretrained model to the help of the `bert_model` argument.

==

examples/run_classifier.py
examples/run_squad.py
examples/run_swag.py
==================
e626eecc2;wlhgtc;2018-12-22 20:26:05 +0800;Update modeling.py

==

pytorch_pretrained_bert/modeling.py
==================
99709ee61;Jasdeep Singh;2018-12-20 13:55:47 -0800;loading saved model when n_classes != 2
Required to for: Assertion `t >= 0 && t < n_classes` failed,  if your default number of classes is not 2.
==

examples/run_classifier.py
==================
8da280ebb;Julien Chaumond;2018-12-20 16:33:39 -0500;Setup CI

==

.circleci/config.yml
README.md
==================
e5fc98c54;tholor;2018-12-20 18:30:52 +0100;add exemplary training data. update to nvidia apex. refactor 'item -> line in doc' mapping. add warning for unknown word.

==

README.md
examples/run_lm_finetuning.py
==================
717667484;Gr√©gory Ch√¢tel;2018-12-20 13:11:17 +0100;Fixing various class documentations.

==

pytorch_pretrained_bert/modeling.py
==================
7fb94ab93;Thomas Wolf;2018-12-19 10:29:17 +0100;Merge pull request #127 from patrick-s-h-lewis/tokenizer-error-on-long-seqs
raises value error for bert tokenizer for long sequences
==
==================
2feb29c0f;Thomas Wolf;2018-12-19 10:18:24 +0100;Merge pull request #130 from sodre/use-entry-points
Use entry-points instead of scripts
==
==================
2c9991496;Thomas Wolf;2018-12-19 10:15:53 +0100;Merge pull request #128 from sodre/add-license
Add license to source distribution
==
==================
17595ef2d;tholor;2018-12-19 09:22:53 +0100;Merge branch 'master' of https://github.com/deepset-ai/pytorch-pretrained-BERT

==
==================
67f4dd56a;tholor;2018-12-19 09:22:37 +0100;update readme for run_lm_finetuning

==

README.md
==================
ecf3ea197;Patrick Sodr√©;2018-12-19 01:58:53 +0000;Remove original script

==

bin/pytorch_pretrained_bert
==================
87c1244c7;Patrick Sodr√©;2018-12-19 01:57:04 +0000;Convert scripts into entry_points
The recommended approach to create launch scripts is to use entry_points
and console_scripts.

xref: https://packaging.python.org/guides/distributing-packages-using-setuptools/#scripts

==

pytorch_pretrained_bert/__main__.py
setup.py
==================
b3d86162b;Patrick Sodr√©;2018-12-19 01:41:18 +0000;Add license to source distribution

==

MANIFEST.in
==================
d57763f58;Julien Chaumond;2018-12-18 19:23:22 -0500;Fix typos

==

README.md
pytorch_pretrained_bert/tokenization.py
==================
78cf7b4ab;Patrick Lewis;2018-12-18 14:41:30 +0000;added code to raise value error for bert tokenizer for covert_tokens_to_indices

==

pytorch_pretrained_bert/tokenization.py
tests/tokenization_test.py
==================
a58361f19;deepset;2018-12-18 10:32:25 +0100;Add example for fine tuning BERT language model (#1)
Adds an example for loading a pre-trained BERT model and fine tune it as a language model (masked tokens & nextSentence) on your target corpus.
==

examples/run_lm_finetuning.py
==================
786cc4129;Thomas Wolf;2018-12-17 09:22:18 +0100;Typos in readme

==

README.md
==================
ecc0b54be;Thomas Wolf;2018-12-14 23:29:47 +0100;Merge pull request #119 from danyaljj/patch-1
Minor README fix 
==
==================
8b1b93947;Daniel Khashabi;2018-12-14 14:10:36 -0500;Minor fix.

==

README.md
==================
8809eb6c9;Thomas Wolf;2018-12-14 16:59:39 +0100;update readme with information on NVIDIA's apex

==

README.md
==================
e1bfad484;Thomas Wolf;2018-12-14 15:15:47 +0100;Merge pull request #112 from huggingface/fourth-release
Fourth release
==
==================
d82135888;thomwolf;2018-12-14 15:15:17 +0100;update readme

==

README.md
==================
37378898a;thomwolf;2018-12-14 15:02:32 +0100;adding DockerFile

==

docker/Dockerfile
==================
4a4b0e578;thomwolf;2018-12-14 14:46:25 +0100;remove logging. basicConfig from library code

==

pytorch_pretrained_bert/modeling.py
pytorch_pretrained_bert/tokenization.py
==================
ae88eb88a;thomwolf;2018-12-14 13:48:58 +0100;set encoding to 'utf-8' in calls to open

==

examples/extract_features.py
examples/run_classifier.py
examples/run_squad.py
examples/run_swag.py
pytorch_pretrained_bert/file_utils.py
pytorch_pretrained_bert/modeling.py
setup.py
==================
e1eab59aa;thomwolf;2018-12-13 14:54:02 +0100;no fp16 on evaluation

==

examples/run_classifier.py
examples/run_squad.py
examples/run_swag.py
==================
087798b7f;thomwolf;2018-12-13 14:48:12 +0100;fix reloading model for evaluation in examples

==

README.md
examples/run_classifier.py
examples/run_squad.py
examples/run_swag.py
==================
0f544625f;thomwolf;2018-12-13 13:35:59 +0100;fix swag example for work with apex

==

README.md
examples/run_swag.py
tests/optimization_test.py
==================
0cf88ff08;thomwolf;2018-12-13 13:28:00 +0100;make examples work without apex

==

examples/run_classifier.py
examples/run_squad.py
==================
52c53f39d;thomwolf;2018-12-13 13:02:17 +0100;clean up apex integration

==

pytorch_pretrained_bert/convert_tf_checkpoint_to_pytorch.py
pytorch_pretrained_bert/modeling.py
tests/optimization_test.py
==================
4946c2c50;thomwolf;2018-12-13 13:02:07 +0100;run_swag example in readme

==

README.md
==================
d23eed85b;thomwolf;2018-12-13 12:53:17 +0100;model loading apex modification

==
==================
1cbb32a54;thomwolf;2018-12-11 12:20:22 +0100;include version number + comment in setup.py

==

pytorch_pretrained_bert/__init__.py
setup.py
==================
ce5217763;thomwolf;2018-12-11 12:12:08 +0100;added version in __init__.py

==

pytorch_pretrained_bert/__init__.py
requirements.txt
setup.py
==================
d3fcec1a3;thomwolf;2018-12-11 11:58:07 +0100;add saving and loading model in examples

==

examples/run_classifier.py
examples/run_squad.py
==================
93f335ef8;thomwolf;2018-12-11 11:50:38 +0100;add pretrained loading from state_dict

==

pytorch_pretrained_bert/modeling.py
==================
b3caec5a5;thomwolf;2018-12-09 17:04:23 -0500;adding save checkpoint and loading in examples

==

examples/run_classifier.py
examples/run_squad.py
==================
85fff78c2;thomwolf;2018-12-09 16:57:51 -0500;compatibility PT 1.0 and 0.4.1

==

tests/optimization_test.py
==================
13bf0d465;thomwolf;2018-12-09 16:17:11 -0500;fixing Adam weights skip in TF convert script

==

pytorch_pretrained_bert/convert_tf_checkpoint_to_pytorch.py
==================
91aab2a6d;Thomas Wolf;2018-12-13 12:32:37 +0100;Merge pull request #116 from FDecaYed/deyuf/fp16_with_apex
Change to use apex for better fp16 and multi-gpu support
==
==================
32a227f50;Thomas Wolf;2018-12-13 12:15:15 +0100;Merge pull request #113 from hzhwcmhf/master
fix compatibility with python 3.5.2
==
==================
ffe9075f4;Thomas Wolf;2018-12-13 12:05:11 +0100;Merge pull request #96 from rodgzilla/multiple-choice-code
BertForMultipleChoice and Swag dataset example.
==
==================
3b0a14b76;Deyu Fu;2018-12-12 15:05:45 -0800;add fallback path for apex used in modeling.py

==

pytorch_pretrained_bert/modeling.py
==================
dcb50eaa4;Gr√©gory Ch√¢tel;2018-12-12 18:17:46 +0100;Swag example readme section update with gradient accumulation run.

==

README.md
==================
c8ea28604;Deyu Fu;2018-12-05 15:07:40 -0800;change to apex for better fp16 and multi-gpu support

==

README.md
examples/run_classifier.py
examples/run_squad.py
pytorch_pretrained_bert/modeling.py
pytorch_pretrained_bert/optimization.py
tests/optimization_test.py
==================
485adde74;hzhwcmhf;2018-12-11 22:49:19 +0800;add pathlib support for file_utils.py on python 3.5

==

pytorch_pretrained_bert/file_utils.py
==================
bc659f86a;hzhwcmhf;2018-12-11 20:18:56 +0800;fix compatibility with python 3.5.2; convert path to str

==

pytorch_pretrained_bert/file_utils.py
==================
1df6f2621;thomwolf;2018-12-11 12:20:31 +0100;Merge branch 'fourth-release' of https://github.com/huggingface/pytorch-pretrained-BERT into fourth-release

==
==================
770f805ae;thomwolf;2018-12-11 12:20:22 +0100;include version number + comment in setup.py

==

pytorch_pretrained_bert/__init__.py
setup.py
==================
ed3b62cd3;thomwolf;2018-12-11 12:12:08 +0100;added version in __init__.py

==

pytorch_pretrained_bert/__init__.py
requirements.txt
setup.py
==================
632f2d2df;Thomas Wolf;2018-12-11 06:00:53 -0500;Merge branch 'master' into fourth-release

==
==================
b13abfa9f;thomwolf;2018-12-11 11:58:07 +0100;add saving and loading model in examples

==

examples/run_classifier.py
examples/run_squad.py
==================
270fa2f20;thomwolf;2018-12-11 11:50:38 +0100;add pretrained loading from state_dict

==

pytorch_pretrained_bert/modeling.py
==================
a3a3180c8;Thomas Wolf;2018-12-11 11:29:45 +0100;Bump up requirements to Python 3.6

==

README.md
==================
e7c0a8ddc;Thomas Wolf;2018-12-11 05:18:00 -0500;Merge pull request #107 from lliimsft/master
Fix optimizer to work with horovod
==
==================
e622790a9;Thomas Wolf;2018-12-11 05:12:04 -0500;Merge pull request #91 from rodgzilla/convert-examples-code-improvement
run_classifier.py improvements
==
==================
df34f2285;Gr√©gory Ch√¢tel;2018-12-10 17:45:23 +0100;Removing the dependency to pandas and using the csv module to load data.

==

examples/run_swag.py
==================
0876b77f7;Gr√©gory Ch√¢tel;2018-12-10 15:34:19 +0100;Change to the README file to add SWAG results.

==

README.md
==================
81e1e2489;Li Li;2018-12-10 02:08:38 -0800;Fix optimizer to work with horovod

==

pytorch_pretrained_bert/optimization.py
==================
174cdbccd;thomwolf;2018-12-09 17:04:23 -0500;adding save checkpoint and loading in examples

==

examples/run_classifier.py
examples/run_squad.py
==================
1db916b5b;thomwolf;2018-12-09 16:57:51 -0500;compatibility PT 1.0 and 0.4.1

==

tests/optimization_test.py
==================
68f77303b;thomwolf;2018-12-09 16:17:11 -0500;fixing Adam weights skip in TF convert script

==

pytorch_pretrained_bert/convert_tf_checkpoint_to_pytorch.py
==================
a2b6918a1;Thomas Wolf;2018-12-09 15:29:31 -0500;Merge pull request #101 from davidefiocco/patch-1
Adding --do_lower_case for all uncased BERTs examples
==
==================
5c858448d;Thomas Wolf;2018-12-09 15:27:30 -0500;Merge pull request #94 from rodgzilla/fixing-squad-commentary
Fixing the commentary of the `SquadExample` class.
==
==================
c9f67e037;Davide Fiocco;2018-12-07 20:40:56 +0100;Adding --do_lower_case for all uncased BERTs
I had missed those, it should make sense to use them
==

README.md
==================
150f3cd9f;Gr√©gory Ch√¢tel;2018-12-06 19:22:07 +0100;Few typos in README.md

==

README.md
==================
d429c15f2;Gr√©gory Ch√¢tel;2018-12-06 19:19:21 +0100;Removing old code from copy-paste.

==

examples/run_swag.py
==================
4fa7892d6;Gr√©gory Ch√¢tel;2018-12-06 19:18:29 +0100;Wrong line number link to modeling file.

==

README.md
==================
6a26e19ea;Gr√©gory Ch√¢tel;2018-12-06 19:15:08 +0100;Updating README.md with SWAG example informations.

==

README.md
==================
63c45056a;Gr√©gory Ch√¢tel;2018-12-06 18:53:05 +0100;Finishing the code for the Swag task.

==

examples/run_swag.py
==================
fc5a38ac9;Gr√©gory Ch√¢tel;2018-12-06 18:42:23 +0100;Adding the BertForMultipleChoiceClass.

==

pytorch_pretrained_bert/__init__.py
pytorch_pretrained_bert/modeling.py
==================
c45d8ac55;Gr√©gory Ch√¢tel;2018-12-06 16:01:28 +0100;Storing the feature of each choice as a dict for readability.

==

examples/run_swag.py
==================
0812aee2c;Gr√©gory Ch√¢tel;2018-12-06 15:53:07 +0100;Fixing problems in convert_examples_to_features.

==

examples/run_swag.py
==================
f2b873e99;Gr√©gory Ch√¢tel;2018-12-06 15:40:47 +0100;convert_examples_to_features code and small improvements.

==

examples/run_swag.py
==================
83fdbd604;Gr√©gory Ch√¢tel;2018-12-06 14:02:46 +0100;Adding read_swag_examples to load the dataset.

==

examples/run_swag.py
==================
7183cded4;Gr√©gory Ch√¢tel;2018-12-06 13:39:44 +0100;SwagExample class.

==

examples/run_swag.py
==================
fa7daa247;Gr√©gory Ch√¢tel;2018-12-06 13:14:33 +0100;Fixing the commentary of the `SquadExample` class.

==

examples/run_squad.py
==================
a994bf407;Gr√©gory Ch√¢tel;2018-12-05 18:16:30 +0100;Fixing related to issue #83.

==

examples/run_classifier.py
==================
c6d9d5394;Gr√©gory Ch√¢tel;2018-12-05 17:53:09 +0100;Simplifying code for easier understanding.

==

examples/run_classifier.py
==================
793262e8e;Gr√©gory Ch√¢tel;2018-12-05 17:52:39 +0100;Removing trailing whitespaces.

==

examples/run_classifier.py
==================
3ba5470eb;Thomas Wolf;2018-12-05 10:41:05 -0500;Merge pull request #87 from rodgzilla/readme-file-links
Readme file links
==
==================
0a7c8bdca;Gr√©gory Ch√¢tel;2018-12-04 13:43:56 +0100;Fixing badly formatted links.

==

README.md
==================
3113e967d;Gr√©gory Ch√¢tel;2018-12-04 13:40:38 +0100;Adding links to examples files.

==

README.md
==================
04826b0f2;Thomas Wolf;2018-12-02 13:01:04 +0100;Merge pull request #77 from davidefiocco/patch-1
Correct assignement for logits in classifier example
==
==================
e60e8a606;Davide Fiocco;2018-12-02 12:38:26 +0100;Correct assignement for logits in classifier example
I tried to address https://github.com/huggingface/pytorch-pretrained-BERT/issues/76
should be correct, but there's likely a more efficient way.
==

examples/run_classifier.py
==================
063be09b7;Thomas Wolf;2018-12-01 01:15:43 +0100;Merge pull request #75 from davidefiocco/patch-2
Point typo fix
==
==================
4450f5ef6;Thomas Wolf;2018-12-01 01:15:31 +0100;Merge pull request #74 from davidefiocco/patch-1
Update finetuning example in README adding --do_lower_case
==
==================
dc13e276e;Davide Fiocco;2018-12-01 01:02:16 +0100;Point typo fix

==

examples/run_classifier.py
==================
8a8aa59d8;Davide Fiocco;2018-12-01 01:00:05 +0100;Update finetuning example adding --do_lower_case
Should be consistent with the fact that an uncased model is used
==

README.md
==================
836b40be8;Thomas Wolf;2018-11-30 23:33:53 +0100;Merge pull request #72 from NirantK/patch-1
Fix internal hyperlink typo
==
==================
66d50ca6a;Thomas Wolf;2018-11-30 23:10:30 +0100;Merge pull request #73 from huggingface/third-release
Third release
==
==================
f9f3bdd60;thomwolf;2018-11-30 23:05:18 +0100;update readme

==

README.md
==================
52ff0590f;thomwolf;2018-11-30 23:01:10 +0100;tup => tpu

==

README.md
==================
511bce58b;thomwolf;2018-11-30 22:56:02 +0100;update new token classification model

==

pytorch_pretrained_bert/modeling.py
==================
258eb5008;thomwolf;2018-11-30 22:55:33 +0100;bump up version

==

setup.py
==================
d787c6be8;thomwolf;2018-11-30 22:55:26 +0100;improve docstrings and fix new token classification model

==

pytorch_pretrained_bert/modeling.py
==================
ed302a73f;thomwolf;2018-11-30 22:55:03 +0100;add new token classification model

==

pytorch_pretrained_bert/__init__.py
==================
89d47230d;thomwolf;2018-11-30 22:54:53 +0100;clean up classification model output

==

examples/run_classifier.py
==================
7f7c41b0c;thomwolf;2018-11-30 22:54:33 +0100;tests for all model classes with and without labels

==

tests/modeling_test.py
==================
be57c8eee;Nirant;2018-12-01 02:43:25 +0530;Fix internal hyperlink typo

==

README.md
==================
8c7267f1c;Thomas Wolf;2018-11-30 18:23:46 +0100;Merge pull request #70 from deepset-ai/fix_lm_loss
fix typo in input for masked lm loss function
==
==================
7b3bb8c00;Malte Pietsch;2018-11-30 16:52:50 +0100;fix typo in input for masked lm loss function

==

pytorch_pretrained_bert/modeling.py
==================
257a35134;thomwolf;2018-11-30 14:23:09 +0100;fix pickle dump in run_squad example

==

examples/run_squad.py
==================
c588453a0;thomwolf;2018-11-30 14:22:40 +0100;fix run_squad

==

examples/run_squad.py
==================
d6f06c03f;thomwolf;2018-11-30 14:09:06 +0100;fixed loading pre-trained tokenizer from directory

==

pytorch_pretrained_bert/modeling.py
pytorch_pretrained_bert/tokenization.py
==================
532a81d3d;thomwolf;2018-11-30 13:57:01 +0100;fixed doc_strings

==

pytorch_pretrained_bert/modeling.py
==================
296f00613;thomwolf;2018-11-30 13:56:53 +0100;added BertForTokenClassification model

==

README.md
pytorch_pretrained_bert/modeling.py
==================
298107fed;thomwolf;2018-11-30 13:56:02 +0100;Added new bert models

==

README.md
pytorch_pretrained_bert/modeling.py
pytorch_pretrained_bert/tokenization.py
==================
054144255;thomwolf;2018-11-30 13:47:33 +0100;add do_lower_case in examples

==

examples/extract_features.py
examples/run_classifier.py
==================
3951c2c18;Thomas Wolf;2018-11-28 14:59:08 +0100;Merge pull request #60 from davidefiocco/patch-1
Updated quick-start example with `BertForMaskedLM`
==
==================
ec2c339b5;Davide Fiocco;2018-11-28 14:53:46 +0100;Updated quick-start example with `BertForMaskedLM`
As `convert_ids_to_tokens` returns a list, the code in the README currently throws an `AssertionError`, so I propose I quick fix.
==

README.md
==================
21f019641;Thomas Wolf;2018-11-28 12:39:45 +0100;Merge pull request #58 from lliimsft/master
Bug fix in examples;correct t_total for distributed training;run pred‚Ä¶
==
==================
0aaedcc02;Li Li;2018-11-27 01:08:37 -0800;Bug fix in examples;correct t_total for distributed training;run prediction for full dataset

==

examples/run_classifier.py
examples/run_squad.py
==================
32167cdf4;thomwolf;2018-11-26 23:33:22 +0100;remove convert_to_unicode and printable_text from examples

==

examples/extract_features.py
examples/run_classifier.py
examples/run_squad.py
notebooks/Comparing-TF-and-PT-models-MLM-NSP.ipynb
pytorch_pretrained_bert/tokenization.py
==================
ce37b8e48;thomwolf;2018-11-26 10:45:48 +0100;bump version in setup.py

==

setup.py
==================
05053d163;thomwolf;2018-11-26 10:45:13 +0100;update cache_dir in readme and examples

==

README.md
examples/run_classifier.py
examples/run_squad.py
pytorch_pretrained_bert/__init__.py
==================
63ae5d213;thomwolf;2018-11-26 10:21:56 +0100;added cache_dir option in from_pretrained

==

pytorch_pretrained_bert/modeling.py
==================
029bdc0d5;thomwolf;2018-11-26 09:56:41 +0100;fixing readme examples

==

README.md
==================
ebaacba38;thomwolf;2018-11-26 09:55:15 +0100;fixing typo in docstring

==

pytorch_pretrained_bert/modeling.py
==================
870d71636;thomwolf;2018-11-26 09:51:34 +0100;fixing target size in crossentropy losses

==

pytorch_pretrained_bert/modeling.py
==================
982339d82;thomwolf;2018-11-23 12:22:12 +0100;fixing unicode error

==

pytorch_pretrained_bert/tokenization.py
==================
60e01ac42;Thomas Wolf;2018-11-21 12:08:30 +0100;fix link in readme

==

README.md
==================
6b2136a8a;thomwolf;2018-11-20 10:12:44 +0100;fixing weights decay in run_squad example

==

examples/run_squad.py
==================
061eeca84;Thomas Wolf;2018-11-20 10:11:46 +0100;Merge pull request #32 from xiaoda99/master
Fix ineffective no_decay bug when using BERTAdam
==
==================
fd32ebed8;Thomas Wolf;2018-11-20 10:09:50 +0100;Merge pull request #42 from weiyumou/master
Fixed UnicodeDecodeError: 'ascii' codec can't decode byte 0xc2
==
==================
eed255a58;thomwolf;2018-11-20 10:02:57 +0100;fixing CLI typo in readme

==

README.md
==================
2f21497d3;thomwolf;2018-11-20 10:01:21 +0100;fixing param.grad is None in fp16 examples

==

examples/run_classifier.py
examples/run_squad.py
==================
9ff2b7d86;weiyumou;2018-11-19 23:13:10 -0500;Fixed README typo

==

README.md
==================
37b6c9b21;weiyumou;2018-11-19 23:01:28 -0500;Fixed UnicodeDecodeError: 'ascii' codec can't decode byte 0xc2 in position 3793: ordinal not in range(128)

==

pytorch_pretrained_bert/tokenization.py
==================
da73925f6;Thomas Wolf;2018-11-19 20:58:48 +0100;fix typos

==

README.md
==================
6f4be31d0;Thomas Wolf;2018-11-19 20:54:46 +0100;Merge pull request #40 from joelgrus/patch-1
update pip package name
==
==================
dd56cfd89;Joel Grus;2018-11-19 09:50:34 -0800;update pip package name

==

README.md
==================
6c4789e4e;xiaoda99;2018-11-18 16:16:21 +0800;Fix ineffective no_decay bug

==

examples/run_classifier.py
==================
956c91734;Thomas Wolf;2018-11-17 23:25:23 +0100;fix typos in readme

==

README.md
==================
27ee0fff3;thomwolf;2018-11-17 23:04:44 +0100;add no_cuda args in extract_features

==

examples/extract_features.py
==================
aa50fd196;thomwolf;2018-11-17 23:01:05 +0100;remove unused arguments in example scripts

==

examples/run_classifier.py
examples/run_squad.py
==================
7c91e51c2;Thomas Wolf;2018-11-17 22:54:15 +0100;update links in readme

==

README.md
==================
e11310170;Thomas Wolf;2018-11-17 12:36:35 +0100;fix typos in readme

==

README.md
==================
4132a028a;Thomas Wolf;2018-11-17 12:21:48 +0100;Merge pull request #29 from huggingface/first-release
First release
==
==================
47a7d4ec1;thomwolf;2018-11-17 12:21:35 +0100;update examples from master

==
==================
c8cba6774;thomwolf;2018-11-17 12:19:16 +0100;clean up readme and examples

==

README.md
examples/run_classifier.py
setup.py
==================
757750d6f;thomwolf;2018-11-17 11:58:14 +0100;fix tests

==

README.md
examples/extract_features.py
examples/run_classifier.py
examples/run_squad.py
pytorch_pretrained_bert/__init__.py
pytorch_pretrained_bert/optimization.py
tests/modeling_test.py
tests/optimization_test.py
tests/tokenization_test.py
==================
a99b97173;thomwolf;2018-11-17 10:43:39 +0100;bump up version minor

==

requirements.txt
setup.py
==================
4e46affc3;thomwolf;2018-11-17 10:30:54 +0100;updating examples

==

examples/extract_features.py
examples/run_classifier.py
examples/run_squad.py
setup.py
==================
d0673c7db;thomwolf;2018-11-17 08:59:29 +0100;fix links

==

README.md
==================
68b937aa4;thomwolf;2018-11-17 08:55:56 +0100;sub  section overviews

==

README.md
==================
c54d8b184;thomwolf;2018-11-17 08:46:17 +0100;fixing links in readme

==

README.md
==================
f920eff8c;thomwolf;2018-11-17 08:42:45 +0100;update readme

==

README.md
notebooks/Comparing TF and PT models_MLM_NSP.ipynb
setup.py
==================
886cb4979;thomwolf;2018-11-16 14:31:15 +0100;updating readme and notebooks

==

README.md
notebooks/Comparing TF and PT models_MLM_NSP.ipynb
notebooks/Comparing-TF-and-PT-models-MLM-NSP.ipynb
notebooks/Comparing-TF-and-PT-models-SQuAD.ipynb
notebooks/Comparing-TF-and-PT-models.ipynb
pytorch_pretrained_bert/optimization.py
==================
fd647e8c8;thomwolf;2018-11-16 11:04:31 +0100;comparison masked LM ok

==

notebooks/Comparing TF and PT models_MLM_NSP.ipynb
==================
02173a1a0;thomwolf;2018-11-15 21:49:12 +0100;fixing error in isnan test for optimizer_on_cpu & fp16

==

run_squad.py
==================
cba85a67b;thomwolf;2018-11-15 21:47:41 +0100;fix nan in optimizer_on_cpu

==

examples/run_squad.py
==================
1de35b624;thomwolf;2018-11-15 20:56:10 +0100;preparing for first release

==

CONTRIBUTING.md
README.md
__init__.py
bin/pytorch_pretrained_bert
examples/extract_features.py
examples/run_classifier.py
examples/run_squad.py
modeling.py
notebooks/Comparing TF and PT models SQuAD predictions.ipynb
notebooks/Comparing TF and PT models.ipynb
notebooks/Comparing TF and PT models_MLM_NSP.ipynb
pytorch_pretrained_bert/__init__.py
pytorch_pretrained_bert/__main__.py
pytorch_pretrained_bert/convert_tf_checkpoint_to_pytorch.py
pytorch_pretrained_bert/file_utils.py
pytorch_pretrained_bert/modeling.py
pytorch_pretrained_bert/optimization.py
pytorch_pretrained_bert/tokenization.py
requirements.txt
setup.py
tests/tokenization_test.py
==================
8513741b5;Thomas Wolf;2018-11-13 17:00:09 +0100;Merge pull request #17 from lukovnikov/master
activation function in BERTIntermediate
==
==================
470076e41;lukovnikov;2018-11-13 16:49:26 +0100;Merge remote-tracking branch 'origin/master'

==
==================
9f3cd2718;lukovnikov;2018-11-13 16:48:59 +0100;clean up pr

==

hf_bert/__init__.py
==================
3d4c7a6f5;Denis;2018-11-13 16:48:43 +0100;Delete __init__.py

==

hf_bert/__init__.py
==================
d64db6dfb;lukovnikov;2018-11-13 16:41:01 +0100;clean up pr

==

modeling.py
==================
7ba83730c;lukovnikov;2018-11-13 16:31:20 +0100;clean up pr

==

convert_tf_checkpoint_to_pytorch.py
modeling.py
==================
fa0c5a2ea;lukovnikov;2018-11-13 16:24:53 +0100;clean up pr

==

convert_tf_checkpoint_to_pytorch.py
==================
f4d79f44c;lukovnikov;2018-11-13 16:22:23 +0100;Merge remote-tracking branch 'upstream/master'

==
==================
5cd8d7ad2;Thomas Wolf;2018-11-13 16:19:28 +0100;Merge pull request #16 from donatasrep/master
Excluding AdamWeightDecayOptimizer internal variables from restoring
==
==================
20d07b3a7;Donatas Repecka;2018-11-13 16:56:25 +0200;Excluding AdamWeightDecayOptimizer internal variables from restoring

==

convert_tf_checkpoint_to_pytorch.py
==================
278fd28a3;Thomas Wolf;2018-11-13 09:34:49 +0100;added results for 16-bit fine-tuning in readme

==

README.md
==================
d940eeda5;thomwolf;2018-11-12 15:26:46 +0100;typo

==

README.md
==================
1cf0a16c6;thomwolf;2018-11-12 15:24:47 +0100;cleaning up readme

==

README.md
==================
66b009087;thomwolf;2018-11-12 15:15:02 +0100;add fp16 training

==

README.md
modeling.py
run_classifier.py
run_squad.py
==================
5dfd19060;Thomas Wolf;2018-11-12 12:39:57 +0100;fix typo in readme

==

README.md
==================
fa1aa81f2;Thomas Wolf;2018-11-12 08:37:43 +0100;fix typo in readme bach examples

==

README.md
==================
6d6b916f4;Thomas Wolf;2018-11-11 17:00:49 +0100;update to BERT-large results

==

README.md
==================
c4bfc646f;Thomas Wolf;2018-11-11 16:59:35 +0100;Add results of fine-tuning BERT-large on GPUs

==

README.md
==================
48930a4cf;Thomas Wolf;2018-11-10 22:27:45 +0100;Merge pull request #2 from elyase/patch-1
Port tokenization for the multilingual model
==
==================
a81a1ef8e;thomwolf;2018-11-10 16:11:14 +0100;fixing learning rate schedule when using gradient_accumulation_steps

==

run_classifier.py
run_squad.py
==================
ea85cca8a;thomwolf;2018-11-09 11:42:37 +0100;adding optimize_on_cpu explanation in readme

==

README.md
==================
5f04aa00e;thomwolf;2018-11-09 11:28:14 +0100;option to perform optimization and keep the optimizer averages on CPU

==

run_squad.py
==================
9e95cd8cd;thomwolf;2018-11-09 11:23:55 +0100;clean up optimizer from unused functions

==

optimization.py
==================
34a1a0109;thomwolf;2018-11-09 09:31:20 +0100;update code comment

==

modeling.py
==================
34bdc8b54;thomwolf;2018-11-09 09:19:45 +0100;remove duplicate accumulate gradient step arguments

==

run_classifier.py
run_squad.py
==================
0c24db9d5;Thomas Wolf;2018-11-09 09:11:59 +0100;update results for SQuAD

==

README.md
==================
2c5d993ba;thomwolf;2018-11-08 21:22:22 +0100;update readme - fix SQuAD model on multi-GPU

==

README.md
modeling.py
==================
4850ec588;Gopal Krishna;2018-11-09 01:30:02 +0530;fixed small typos in the README.md (#8)

==

README.md
==================
3bfbc2137;Thomas Wolf;2018-11-08 00:44:17 +0100;updating pytest command

==

README.md
==================
0ed769619;Thomas Wolf;2018-11-08 00:39:42 +0100;Updated MRPC results

==

README.md
==================
48d4a5317;thomwolf;2018-11-07 23:51:12 +0100;typo fix in output tuple

==

run_classifier.py
==================
d92a7f772;Thomas Wolf;2018-11-07 23:37:55 +0100;Removing note on run_squad.py example

==

README.md
==================
5c0838d84;Thomas Wolf;2018-11-07 23:36:46 +0100;Merge pull request #7 from huggingface/develop
Develop
==
==================
efeb6b1a0;Thomas Wolf;2018-11-07 23:35:42 +0100;Merge branch 'master' into develop

==
==================
dbc318a4c;thomwolf;2018-11-07 22:22:55 +0100;cleaning up - speeding up a bit multi-gpu

==

modeling.py
run_classifier.py
run_squad.py
==================
6bb7510a5;thomwolf;2018-11-07 22:12:41 +0100;fixing pre-processing bug - averaging loss for gradient accumulation - no_grad on evaluation

==

run_classifier.py
run_squad.py
==================
bd91ae654;lukovnikov;2018-11-06 18:21:44 +0100;moved bert to qelos-util

==

hf_bert/__init__.py
modeling.py
tests/mytest.py
==================
4e5218843;lukovnikov;2018-11-06 17:47:03 +0100;bert weight loading from tf

==

convert_tf_checkpoint_to_pytorch.py
modeling.py
tests/mytest.py
==================
a1126237a;thomwolf;2018-11-06 17:31:15 +0100;clean up logits extraction logic

==

run_squad.py
==================
2a97fe220;thomwolf;2018-11-06 17:26:33 +0100;fixing weights initialization in the model and out of span clamping

==

modeling.py
==================
907d3569c;thomwolf;2018-11-06 11:13:43 +0100;cleaning up SQuAD notebook - more explanation - fixing error

==

notebooks/Comparing TF and PT models SQuAD predictions.ipynb
==================
1a5bbd83d;Thomas Wolf;2018-11-06 08:53:01 +0100;Updating run_squad information in readme

==

README.md
==================
79e1b95e7;Thomas Wolf;2018-11-06 08:38:02 +0100;fix link in readme

==

README.md
==================
886f595c3;Knut Ole Sj√∏li;2018-11-06 00:34:18 +0100;Fix typo in subheader (#4)

==

README.md
==================
4d124baf8;Yaser Martinez Palenzuela;2018-11-05 23:04:29 +0100;Add test for Chinese tokenization

==

tests/tokenization_test.py
==================
59d4cc5f2;Thomas Wolf;2018-11-05 22:47:24 +0100;typos

==

README.md
==================
0ce2f496d;Yaser Martinez Palenzuela;2018-11-05 22:34:12 +0100;Port tokenization for the multilingual model

==

tokenization.py
==================
d983eecdd;Thomas Wolf;2018-11-05 21:29:04 +0100;more readme typo fixes

==

README.md
==================
8f91b4de9;Thomas Wolf;2018-11-05 21:24:14 +0100;more typo fixes

==

README.md
==================
7316b0d6d;Thomas Wolf;2018-11-05 21:22:45 +0100;fix typo

==

README.md
==================
d130cb513;Clement;2018-11-05 15:09:24 -0500;typos

==

README.md
==================
2a8fee495;Clement;2018-11-05 15:04:06 -0500;typos

==

README.md
==================
f968b1165;Clement;2018-11-05 14:59:44 -0500;typo

==

README.md
==================
88e793f31;thomwolf;2018-11-05 16:14:19 +0100;fix typos

==

README.md
==================
3914eed50;thomwolf;2018-11-05 16:09:27 +0100;update readme

==

README.md
==================
bab5d1307;thomwolf;2018-11-05 16:09:21 +0100;update optimizer documentation

==

optimization.py
==================
7394eb47a;thomwolf;2018-11-05 15:35:44 +0100;update readme

==

README.md
==================
e6646751a;thomwolf;2018-11-05 15:02:50 +0100;update notebooks

==

notebooks/Comparing TF and PT models SQuAD predictions.ipynb
notebooks/Comparing TF and PT models.ipynb
==================
b705c9eff;thomwolf;2018-11-05 14:55:08 +0100;remove small script, moved notebooks to notebook folder

==

notebooks/Comparing TF and PT models SQuAD predictions.ipynb
notebooks/Comparing TF and PT models.ipynb
run_squad_small.py
==================
3a301d443;thomwolf;2018-11-05 14:53:43 +0100;update gitignore

==

.gitignore
==================
711d3f9f2;thomwolf;2018-11-05 14:53:03 +0100;remove tensorflow_code

==

Comparing TF and PT models SQuAD predictions.ipynb
tensorflow_code/create_pretraining_data.py
tensorflow_code/extract_features.py
tensorflow_code/modeling.py
tensorflow_code/modeling_test.py
tensorflow_code/optimization.py
tensorflow_code/optimization_test.py
tensorflow_code/run_classifier.py
tensorflow_code/run_pretraining.py
tensorflow_code/run_squad.py
tensorflow_code/tokenization.py
tensorflow_code/tokenization_test.py
==================
7875b1a8e;thomwolf;2018-11-05 14:50:44 +0100;notebook update

==

Comparing TF and PT models SQuAD predictions.ipynb
==================
c3527cfbc;thomwolf;2018-11-05 14:18:48 +0100;ignore SQuAD targets outside of seq_length

==

modeling.py
==================
1b99cdf71;thomwolf;2018-11-05 13:54:54 +0100;script that use a small portion of squad only

==

run_squad_small.py
==================
2f4765d3e;thomwolf;2018-11-05 13:46:14 +0100;fix multi-gpu squad loss

==

Comparing TF and PT models SQuAD predictions.ipynb
modeling.py
==================
955cee33a;thomwolf;2018-11-05 13:21:53 +0100;updating SQuAD comparison

==

Comparing TF and PT models SQuAD predictions.ipynb
==================
5622d8320;thomwolf;2018-11-05 13:21:24 +0100;allowing to load small number of examples

==

tensorflow_code/run_squad.py
==================
a725db4f6;thomwolf;2018-11-05 13:21:11 +0100;fixing BertForQuestionAnswering loss computation

==

modeling.py
==================
bb5ce67a1;thomwolf;2018-11-05 12:11:32 +0100;adding back tf code + adding models comparison on SQuAD

==

Comparing TF and PT models SQuAD predictions.ipynb
Comparing TF and PT models.ipynb
tensorflow_code/create_pretraining_data.py
tensorflow_code/extract_features.py
tensorflow_code/modeling.py
tensorflow_code/modeling_test.py
tensorflow_code/optimization.py
tensorflow_code/optimization_test.py
tensorflow_code/run_classifier.py
tensorflow_code/run_pretraining.py
tensorflow_code/run_squad.py
tensorflow_code/tokenization.py
tensorflow_code/tokenization_test.py
==================
290633b88;VictorSanh;2018-11-04 17:31:50 -0500;Fix `args.gradient_accumulation_steps` used before assigment.

==

run_classifier.py
run_squad.py
==================
649e9774c;VictorSanh;2018-11-04 17:19:40 -0500;Fix bug train_batch_size not an int.
Division makes args.train_batch_size becoming a float.
cc @thomwolf

==

run_classifier.py
run_squad.py
==================
d55c3ae83;VictorSanh;2018-11-04 16:28:10 -0500;Small logger bug (multi-gpu, distribution) in training

==

run_classifier.py
run_squad.py
==================
3d291dea4;thomwolf;2018-11-04 21:27:19 +0100;clean up tests

==

tests/optimization_test.py
tests/tokenization_test.py
==================
87da161c2;thomwolf;2018-11-04 21:27:10 +0100;finishing model test

==

tests/modeling_test.py
==================
d69b0b0e9;thomwolf;2018-11-04 21:26:54 +0100;fixes + clean up + mask is long

==

run_squad.py
==================
3ddff783c;thomwolf;2018-11-04 21:26:44 +0100;clean up + mask is long

==

run_classifier.py
==================
88c103799;thomwolf;2018-11-04 21:26:18 +0100;update requirements

==

requirements.txt
==================
d0cb9fa2a;thomwolf;2018-11-04 21:26:11 +0100;clean up model

==

modeling.py
==================
6cc651778;thomwolf;2018-11-04 21:26:03 +0100;update readme

==

README.md
==================
efb44a831;thomwolf;2018-11-04 21:25:55 +0100;distributed in extract features

==

extract_features.py
==================
d9d7d1a46;thomwolf;2018-11-04 21:25:36 +0100;update float()

==

Comparing TF and PT models.ipynb
==================
c6207d85b;thomwolf;2018-11-04 15:34:00 +0100;remove old methods

==

run_classifier.py
==================
965b2565a;thomwolf;2018-11-04 15:32:04 +0100;add distributed training

==

run_classifier.py
run_squad.py
==================
1ceac85e2;thomwolf;2018-11-04 15:26:14 +0100;add gradient accumulation

==

run_classifier.py
run_squad.py
==================
6b0da96b4;thomwolf;2018-11-04 15:17:55 +0100;clean up

==

run_classifier.py
run_squad.py
==================
834b485b2;thomwolf;2018-11-04 12:07:38 +0100;logging + update copyright

==

convert_tf_checkpoint_to_pytorch.py
extract_features.py
modeling.py
optimization.py
run_classifier.py
run_squad.py
tokenization.py
==================
1701291ef;thomwolf;2018-11-04 11:54:57 +0100;multi-gpu cleanup

==

run_classifier.py
run_squad.py
==================
5ee171689;thomwolf;2018-11-04 11:45:44 +0100;what's in loss again

==

run_squad.py
==================
0b7a20c65;thomwolf;2018-11-04 11:07:34 +0100;add tqdm, clean up logging

==

modeling.py
run_squad.py
==================
d4e3cf352;thomwolf;2018-11-04 10:54:16 +0100;add numpy import

==

run_squad.py
==================
cf366417d;thomwolf;2018-11-04 09:56:00 +0100;remove run_squad_pytorch

==

run_squad_pytorch.py
==================
26bdef432;thomwolf;2018-11-04 09:53:29 +0100;fixing verbose_argument

==

run_squad.py
==================
d6418c5ef;thomwolf;2018-11-03 23:52:35 +0100;tweaking the readme

==

README.md
==================
3b70b270e;thomwolf;2018-11-03 23:39:55 +0100;update readme

==

README.md
==================
eaa6db92f;thomwolf;2018-11-03 23:35:16 +0100;Merge branch 'master' of https://github.com/huggingface/pytorch-pretrained-BERT

==
==================
f8276008d;thomwolf;2018-11-03 23:35:14 +0100;update readme, file names, removing TF code, moving tests

==

Comparing TF and PT models.ipynb
README.md
convert_tf_checkpoint_to_pytorch.py
convert_tf_checkpoint_to_pytorch_special_edition.py
extract_features.py
modeling.py
optimization.py
run_classifier.py
run_squad.py
samples/input.txt
samples/sample_text.txt
tensorflow_code/create_pretraining_data.py
tensorflow_code/extract_features.py
tensorflow_code/modeling.py
tensorflow_code/modeling_test.py
tensorflow_code/optimization.py
tensorflow_code/optimization_test.py
tensorflow_code/run_classifier.py
tensorflow_code/run_pretraining.py
tensorflow_code/tokenization.py
tensorflow_code/tokenization_test.py
tests/modeling_test.py
tests/optimization_test.py
tests/tokenization_test.py
tokenization.py
==================
f18ae210e;Ubuntu;2018-11-03 22:34:37 +0000;fix typo

==

modeling_pytorch.py
==================
3c24e4bef;VictorSanh;2018-11-03 18:03:17 -0400;Multi-Gpu loss - Cleaning

==

run_squad_pytorch.py
==================
5de1517d6;Tim Rault;2018-11-03 22:40:50 +0100;WIP modeling_test_pytorch.py

==

modeling_test_pytorch.py
==================
1ba5b58c2;VictorSanh;2018-11-03 17:10:23 -0400;fix typo

==

run_classifier_pytorch.py
==================
5858e8e4d;VictorSanh;2018-11-03 16:48:24 -0400;Fix both loss and eval metrics -> more coherence on the loss (eval vs train and tf vs pt)

==

run_classifier_pytorch.py
==================
cd09cd5b4;VictorSanh;2018-11-03 15:38:30 -0400;Fix import on initalization

==

run_classifier_pytorch.py
run_squad_pytorch.py
==================
ec66841af;Tim Rault;2018-11-03 19:12:20 +0100;WIP

==

modeling_test_pytorch.py
==================
139873f6e;thomwolf;2018-11-03 19:06:17 +0100;Merge branch 'master' of https://github.com/huggingface/pytorch-pretrained-BERT

==
==================
04287a4d6;thomwolf;2018-11-03 19:06:15 +0100;special edition script

==

convert_tf_checkpoint_to_pytorch_special_edition.py
modeling_pytorch.py
run_classifier_pytorch.py
==================
a1af5247e;VictorSanh;2018-11-03 14:00:36 -0400;Add seed in initialization

==

run_classifier_pytorch.py
run_squad_pytorch.py
==================
4faeb38b5;Ubuntu;2018-11-03 17:52:51 +0000;Fix loss loss logging for multi-gpu compatibility

==

run_classifier_pytorch.py
==================
25f73add0;thomwolf;2018-11-03 17:56:34 +0100;update optimizer run_squad

==

run_squad_pytorch.py
==================
f514cbbf3;thomwolf;2018-11-03 17:52:44 +0100;update run_squad with tqdm

==

run_squad_pytorch.py
==================
cb76c1ddd;thomwolf;2018-11-03 17:40:12 +0100;add model.zero_grad()

==

run_classifier_pytorch.py
run_squad_pytorch.py
==================
a4086c5de;thomwolf;2018-11-03 17:38:17 +0100;Merge branch 'master' of https://github.com/huggingface/pytorch-pretrained-BERT

==
==================
088ad4588;thomwolf;2018-11-03 17:38:15 +0100;fixing optimization

==

optimization_pytorch.py
optimization_test_pytorch.py
run_classifier_pytorch.py
tensorflow_code/optimization_test.py
==================
8bd6b235b;VictorSanh;2018-11-03 10:27:59 -0400;typo on tokenization

==

run_squad_pytorch.py
==================
2c55568c4;VictorSanh;2018-11-03 10:27:38 -0400;`scatter_` and `scatter`

==

modeling_pytorch.py
==================
a6efe1235;VictorSanh;2018-11-03 10:10:34 -0400;Merge pull request #1 from huggingface/multi-gpu-support
Create DataParallel model if several GPUs
==
==================
5f432480c;VictorSanh;2018-11-03 10:10:01 -0400;Create DataParallel model if several GPUs

==

extract_features_pytorch.py
run_classifier_pytorch.py
run_squad_pytorch.py
==================
5889765a7;VictorSanh;2018-11-03 09:18:44 -0400;Update README.md

==

README.md
==================
8c932e37f;VictorSanh;2018-11-03 09:08:05 -0400;Update the comparison notebook

==

Comparing TF and PT models.ipynb
==================
391a4ec2f;VictorSanh;2018-11-03 08:25:15 -0400;Small typo in `trange`
I seriously don't understand why they defined num_train_epochs as a float in the originial tf code.
I Will change it at the end to avoir merge conflicts for now.

==

run_classifier_pytorch.py
==================
5676d6f79;VictorSanh;2018-11-03 08:17:22 -0400;Remove BERT pretraining files for now

==

create_pretraining_data_pytorch.py
run_pretraining_pytorch.py
==================
8ec457d37;Tim Rault;2018-11-03 12:35:21 +0100;Fix imports

==

tensorflow_code/create_pretraining_data.py
tensorflow_code/extract_features.py
tensorflow_code/modeling_test.py
tensorflow_code/optimization_test.py
tensorflow_code/run_classifier.py
tensorflow_code/run_pretraining.py
tensorflow_code/run_squad.py
tensorflow_code/tokenization_test.py
==================
852e4b3c0;thomwolf;2018-11-03 12:23:04 +0100;Merge branch 'master' of https://github.com/huggingface/pytorch-pretrained-BERT

==
==================
0d8d2285b;thomwolf;2018-11-03 12:23:00 +0100;fix optimization_test

==

optimization_test_pytorch.py
requirements.txt
run_classifier_pytorch.py
==================
574e20a92;Tim Rault;2018-11-03 12:22:06 +0100;Move modeling_test.py to /tensorflow_code

==

tensorflow_code/modeling_test.py
==================
45efc9d80;thomwolf;2018-11-03 11:46:18 +0100;removing f-string

==

run_classifier_pytorch.py
==================
4df602745;thomwolf;2018-11-03 11:33:10 +0100;clean up

==

tensorflow_code/create_pretraining_data.py
tensorflow_code/extract_features.py
tensorflow_code/modeling.py
tensorflow_code/optimization.py
tensorflow_code/optimization_test.py
tensorflow_code/run_classifier.py
tensorflow_code/run_pretraining.py
tensorflow_code/run_squad.py
tensorflow_code/tokenization.py
tensorflow_code/tokenization_test.py
==================
e6d106a01;thomwolf;2018-11-03 10:47:10 +0100;comment on gelu function

==

modeling_pytorch.py
==================
01b1a0534;thomwolf;2018-11-03 03:11:15 +0100;Merge branch 'master' of https://github.com/huggingface/pytorch-pretrained-BERT

==
==================
8aa22af0c;thomwolf;2018-11-03 03:11:13 +0100;fixing model

==

Comparing TF and PT models.ipynb
extract_features_pytorch.py
modeling_pytorch.py
==================
72ab10399;VictorSanh;2018-11-02 18:06:21 -0400;Fix loss
Please review @thomwolf but i think this is equivqlent (and it mimics the loss computation of the original loss)

==

modeling_pytorch.py
==================
25d5ca48e;VictorSanh;2018-11-02 17:57:46 -0400;Fix scatter LopngTensor

==

modeling_pytorch.py
==================
e6a710f68;VictorSanh;2018-11-02 17:54:22 -0400;device

==

modeling_pytorch.py
==================
3ebf1a13c;VictorSanh;2018-11-02 17:49:35 -0400;Fix loss computation for indexes bigger than max_seq_length.

==

modeling_pytorch.py
==================
629bd006b;Tim Rault;2018-11-02 17:50:17 +0100;Convert optimization_test.py to PyTorch

==

optimization_test_pytorch.py
==================
38f740a1d;VictorSanh;2018-11-02 11:29:45 -0400;Fix bug writing predictions in run_squad_pytorch

==

run_squad_pytorch.py
==================
ee29871f8;VictorSanh;2018-11-02 11:07:32 -0400;Debug run_squad_pytorch

==

run_squad_pytorch.py
==================
101eabff9;VictorSanh;2018-11-02 10:44:08 -0400;Debug run_squad_pytorch

==

run_squad_pytorch.py
==================
bb0a51033;VictorSanh;2018-11-02 10:16:07 -0400;Print for debug run_squad

==

run_squad_pytorch.py
==================
c84315ec3;thomwolf;2018-11-02 15:11:16 +0100;model fixes + ipnb fixes

==

Comparing TF and PT models.ipynb
extract_features_pytorch.py
modeling_pytorch.py
==================
3ff2ec5eb;Tim Rault;2018-11-02 14:42:05 +0100;Move command-line argparse arguments into main() function

==

run_classifier_pytorch.py
run_pretraining_pytorch.py
run_squad_pytorch.py
==================
c9690e57f;thomwolf;2018-11-02 14:25:21 +0100;adding jupyter, updating extract features adding simple test file

==

Comparing TF and PT models.ipynb
extract_features_pytorch.py
input.txt
==================
844b2f0e6;VictorSanh;2018-11-02 08:57:15 -0400;Small update Readme

==

README.md
==================
49006d066;Tim Rault;2018-11-02 10:04:41 +0100;Bug fix type=bool -> action='store_true' in argparse

==

create_pretraining_data_pytorch.py
run_pretraining_pytorch.py
run_squad_pytorch.py
tokenization_test_pytorch.py
==================
beb59080b;VictorSanh;2018-11-02 04:36:42 -0400;Fix size compatibility for model.forward
Error was coming from "modeling_pytorch.py", line 484, in forward: start_loss = loss_fct(start_logits, start_positions) --> ValueError: Expected target size (12, 1), got torch.Size([12])

==

run_squad_pytorch.py
==================
8cbe7d6af;VictorSanh;2018-11-02 04:09:53 -0400;FIX errors in loading eval Dataset in `run_squad_pytorch`

==

run_squad_pytorch.py
==================
833c3a7a2;VictorSanh;2018-11-02 04:00:00 -0400;FIX errors in loading Dataset in `run_squad_pytorch`

==

run_squad_pytorch.py
==================
72d69a4ef;VictorSanh;2018-11-02 03:37:39 -0400;Update README

==

README.md
==================
62ac7e9a6;VictorSanh;2018-11-02 03:32:35 -0400;Fix small bug in `run_squad_pytorch.py`

==

run_squad_pytorch.py
==================
98b9771df;VictorSanh;2018-11-02 03:02:06 -0400;Quick fix metrics evaluation on run_classif_pytorch

==

run_classifier_pytorch.py
==================
bf65d4dbb;VictorSanh;2018-11-02 02:51:07 -0400;Begin Updating the README.md

==

README.md
==================
b54de837c;VictorSanh;2018-11-02 02:46:17 -0400;Quick fix on eval accuracy

==

run_classifier_pytorch.py
==================
1d53f9cb7;VictorSanh;2018-11-02 01:39:15 -0400;Fix cuda compability for evaluation

==

run_classifier_pytorch.py
==================
1d8511f8f;VictorSanh;2018-11-02 01:12:52 -0400;FIX small bugs in `run_classifier_pytorch.py`

==

run_classifier_pytorch.py
==================
936eb4c3a;VictorSanh;2018-11-02 01:11:25 -0400;FIX small bugs in `run_classifier_pytorch.py`

==

run_classifier_pytorch.py
==================
cc228089e;VictorSanh;2018-11-02 00:02:36 -0400;Small fix to ensure Python3 compativility.

==

tokenization_test_pytorch.py
==================
ebfffa0ab;thomwolf;2018-11-02 04:32:18 +0100;updated extract_features

==

extract_features_pytorch.py
run_squad_pytorch.py
==================
9af479b3b;thomwolf;2018-11-02 04:12:20 +0100;conversion run_squad ok

==

run_squad_pytorch.py
==================
8e81e5e6f;thomwolf;2018-11-02 04:07:52 +0100;working on squad

==

run_classifier_pytorch.py
run_squad_pytorch.py
==================
e61db0d1c;thomwolf;2018-11-02 03:56:14 +0100;run_squad WIP

==

extract_features_pytorch.py
modeling_pytorch.py
run_squad_pytorch.py
==================
c0065af6c;thomwolf;2018-11-02 03:04:34 +0100;implemented BertForQuestionAnswering

==

modeling_pytorch.py
==================
5383fca45;thomwolf;2018-11-02 01:56:25 +0100;update name

==

convert_tf_checkpoint_to_pytorch.py
==================
dee09a40b;thomwolf;2018-11-02 01:52:54 +0100;various fixes

==

modeling_pytorch.py
run_classifier_pytorch.py
==================
2c731fd12;thomwolf;2018-11-02 01:38:22 +0100;small tweaks

==

modeling_pytorch.py
run_classifier_pytorch.py
==================
9343a2311;thomwolf;2018-11-02 01:31:31 +0100;model training loop working ‚Äì still have to check that everything is exactly same

==

modeling_pytorch.py
run_classifier_pytorch.py
==================
f690f0e16;thomwolf;2018-11-02 00:27:50 +0100;run_classifier WIP + added classifier head and initialization to the model

==

modeling_pytorch.py
run_classifier_pytorch.py
==================
4a0b59e98;thomwolf;2018-11-01 21:05:04 +0100;run_classifier WIP

==

modeling_pytorch.py
optimization_pytorch.py
run_classifier_pytorch.py
==================
7af7f8173;VictorSanh;2018-11-01 15:44:52 -0400;Fix oubli

==

tokenization_test_pytorch.py
==================
e1bb7904d;thomwolf;2018-11-01 20:11:14 +0100;optimization_pytorch from OpenAI with adapted HP - check if there is weights decay on bias

==

optimization_pytorch.py
==================
d3a8df6b9;VictorSanh;2018-11-01 14:17:55 -0400;typos in `input_fn_builder`

==

run_classifier_pytorch.py
==================
836faed98;VictorSanh;2018-11-01 14:17:12 -0400;wip

==

run_classifier_pytorch.py
==================
960ef4df3;thomwolf;2018-11-01 19:12:31 +0100;probably ok weights convertion script

==

convert_tf_checkpoint.py
==================
ab0e8932a;thomwolf;2018-11-01 18:00:20 +0100;convertion script WIP

==

convert_tf_checkpoint.py
modeling_pytorch.py
==================
5581edb4f;Tim Rault;2018-11-01 17:55:40 +0100;Merge remote-tracking branch 'origin/master'

==
==================
cdd39d216;Tim Rault;2018-11-01 17:55:30 +0100;Remove TensorFlow dependence in `tokenization_test_pytorch.py`

==

tokenization_test_pytorch.py
==================
c5d532e5f;thomwolf;2018-11-01 17:40:05 +0100;added conversion script

==

convert_tf_checkpoint.py
modeling_pytorch.py
==================
90d360a7a;VictorSanh;2018-11-01 12:09:23 -0400;WIP

==

run_classifier_pytorch.py
==================
f8e7c95db;Tim Rault;2018-11-01 15:16:00 +0100;Remove TensorFlow dependence in `tokenization_pytorch.py`

==

tokenization_pytorch.py
==================
71557b165;thomwolf;2018-11-01 14:48:15 +0100;working on model

==

modeling_pytorch.py
==================
8627a675c;Tim Rault;2018-11-01 12:49:45 +0100;Convert flags to argparse in 'create_pretraining_data_pytorch.py'

==

create_pretraining_data_pytorch.py
==================
2f49360de;Tim Rault;2018-11-01 11:59:44 +0100;Convert flags to argparse in 'extract_features_pytorch.py'

==

extract_features_pytorch.py
==================
04a360bb1;Tim Rault;2018-11-01 11:40:26 +0100;Convert flags to argparse in 'run_pretraining_pytorch.py'

==

run_pretraining_pytorch.py
==================
371945cf0;Tim Rault;2018-11-01 11:17:49 +0100;Convert flags to argparse in 'run_squad_pytorch.py'

==

run_squad_pytorch.py
==================
8163baab6;Tim Rault;2018-11-01 09:39:04 +0100;Convert indentation from 2 spaces to 4 spaces

==

create_pretraining_data.py
extract_features.py
modeling.py
modeling_test.py
optimization.py
optimization_test.py
run_classifier.py
run_pretraining.py
run_squad.py
tokenization.py
tokenization_test.py
==================
555b7d66c;VictorSanh;2018-11-01 02:10:46 -0400;`input_fn_builder` WIP

==

run_classifier_pytorch.py
==================
f8e347b55;VictorSanh;2018-11-01 01:33:01 -0400;Convert all DataProcessors, _truncate_seq_pair and convert_examples_to_features

==

run_classifier_pytorch.py
==================
b1dade34d;VictorSanh;2018-11-01 01:05:11 -0400;Convert flags to argparse in `run_classifier_pytorch.py`

==

run_classifier_pytorch.py
==================
1b95daa0c;thomwolf;2018-11-01 02:22:50 +0100;model conversion WIP

==

modeling_pytorch.py
==================
da017ac91;thomwolf;2018-10-31 19:44:49 +0100;adding pytorch model file

==

modeling_pytorch.py
==================
13ee61e4d;thomwolf;2018-10-31 18:46:03 +0100;switch to full google code

==

.gitignore
CONTRIBUTING.md
LICENSE
README.md
__init__.py
bert_model.py
create_pretraining_data.py
data_processor.py
download_weights.sh
extract_features.py
modeling.py
modeling_test.py
optimization.py
optimization_test.py
run_classifier.py
run_pretraining.py
run_squad.py
sample_text.txt
tokenization.py
tokenization_test.py
==================
12e013dba;thomwolf;2018-10-30 23:09:09 +0100;added wordpiece - updated readme

==

README.md
bert_model.py
data_processor.py
example.py
==================
ccce66be2;thomwolf;2018-10-30 20:18:49 +0100;getting ready

==

.gitignore
bert_model.py
data_processor.py
download_weights.sh
example.py
==================
43badf217;Thomas Wolf;2018-10-29 14:56:02 +0100;Initial commit

==

.gitignore
README.md
