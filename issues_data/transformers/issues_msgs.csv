ID,Title,Created At,Closed At,Issues,Errors,Messages
16325,b'Implement HybridNets: End-to-End Perception Network',2022-03-22T08:52:31Z,,New model,,
16324,b'Add type hints for Pegasus model (PyTorch)',2022-03-22T08:38:52Z,,,,
16323,b'Funnel type hints',2022-03-22T06:42:45Z,,,,
16322,"b""RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB with 8 Ampere GPU's""",2022-03-22T05:35:00Z,,,`RuntimeError,"`RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 39.59 GiB total capacity; 33.48 GiB already allocated; 3.19 MiB free; 34.03 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation. See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF`"
16321,b'First token misses the first character in its `offsets_mapping` WHEN `add_prefix_space=True` is used',2022-03-22T05:31:02Z,,,,
16320,b'Can we support the trace of ViT and Swin-Transformer based on torch.fx?',2022-03-22T01:52:27Z,,,,
16319,b'ibert seems to be quite slow in quant_mode = True',2022-03-22T01:19:25Z,,,,
16318,b'Fix the issue #16317 in judging module names starting with base_model_prefix',2022-03-22T00:39:10Z,,,,
16317,"b'[modeling_utils] Keys are still reported as ""missing"" during model loading even though specified in `_keys_to_ignore_on_load_missing`'",2022-03-22T00:21:52Z,,,,
16316,b'Fix Respect output device in the Pipeline base impl.',2022-03-21T23:44:18Z,,,,
16315,"b'Pipeline always returns to device=""cpu""'",2022-03-21T23:35:38Z,,,,
16314,b'Updates in Trainer to support partial checkpointing for SM Model Parallel library ',2022-03-21T21:34:57Z,,,,
16313,b'Support SageMaker distributed data parallel library v1.4.0',2022-03-21T21:28:27Z,,,,
16312,b'Getting a signal: aborted (core dumped) or Segmentation fault (core dumped) error when trying to train on a single GPU',2022-03-21T21:11:38Z,,,,
16311,b'TF GPT2: clearer model variable naming with @unpack_inputs',2022-03-21T20:12:38Z,,,,
16310,b'Fix BigBirdModelTester',2022-03-21T20:04:53Z,,,,
16309,b'Add LayoutLMv2 OnnxConfig',2022-03-21T19:15:57Z,,,,
16308,b'ONNXConfig: Add a configuration for all available models',2022-03-21T18:15:54Z,,,,
16307,b'added type hints for blenderbot and blenderbot_small (v2)',2022-03-21T18:05:33Z,2022-03-21T19:13:58Z,,,
16306,b'Update Makefile Phonies',2022-03-21T18:03:14Z,2022-03-21T19:28:23Z,,,
16305,b'add xglm conversion script',2022-03-21T17:20:17Z,,,,
16304,b'Finetune Luke',2022-03-21T16:23:14Z,,,,
16303,b'How to do mask prediction with ByT5?',2022-03-21T15:43:35Z,,,,
16302,b'Feature Extractor: correctly pass **kwargs upon creation',2022-03-21T15:01:06Z,,,,
16301,b'fix last element in hidden_states for XGLM',2022-03-21T14:59:35Z,2022-03-21T16:38:52Z,,,
16300,b'Fix Marian conversion script',2022-03-21T14:48:10Z,2022-03-21T16:23:40Z,,,
16299,b'Spanish translation of the file preprocessing.mdx',2022-03-21T14:23:13Z,,,,
16298,b'[FlaxGPTJ] Fix bug in rotary embeddings',2022-03-21T13:41:27Z,2022-03-21T17:07:57Z,,,
16297,b'Fix Bart type hints',2022-03-21T13:41:05Z,,,,
16296,b'Remove disclaimer from Longformer docs',2022-03-21T12:51:02Z,2022-03-21T14:05:47Z,,,
16295,b'Fix Seq2SeqTrainingArguments docs',2022-03-21T12:38:30Z,2022-03-21T17:48:07Z,,,
16294,b'ResNet & VAN: Fixed code sample tests',2022-03-21T12:12:12Z,2022-03-21T15:59:47Z,,,
16293,b'Update for bs_msfp integration',2022-03-21T12:03:13Z,,,,
16292,b'[Community Event] Doc Tests Sprint',2022-03-21T11:53:20Z,,Good First Issue,,
16291,b'Fix a typo (add a coma)',2022-03-21T11:47:16Z,2022-03-21T12:10:24Z,,,
16290,b'Fix XGLM cross attention',2022-03-21T11:41:38Z,2022-03-21T12:07:29Z,,,
16289,b'Changing the default branch from `master` to `main`.',2022-03-21T11:40:02Z,,,,
16288,b'apply_rotary_pos_emb gives different results between PT & Flax',2022-03-21T11:29:16Z,2022-03-21T17:07:57Z,,,
16287,b'creating transformer for tamil language',2022-03-21T11:19:50Z,,,,
16286,b'`self.encoder_attn` not defined for PyTorch XGLM',2022-03-21T10:23:22Z,2022-03-21T12:07:29Z,,AttributeError,"AttributeError: 'XGLMDecoderLayer' object has no attribute 'encoder_attn'"
16285,b'[SegFormer] Remove unused attributes',2022-03-21T08:49:17Z,2022-03-21T16:34:10Z,,,
16284,"b'Add argument ""cache_dir"" for transformers.onnx'",2022-03-21T08:33:22Z,2022-03-21T14:26:44Z,,,
16283,b'Weights of lm_head and input embedding are not tied in google/mt5-base',2022-03-21T03:41:16Z,2022-03-22T02:21:54Z,,,
16282,b'ONNX export results for hidden states/attentions are incorrect if enabled',2022-03-20T20:13:00Z,,,,
16281,b'Adding missing type hints for mBART model (TF)',2022-03-20T17:03:27Z,,,,
16280,b'Update pt flax equivalence tests in pt',2022-03-20T13:49:26Z,,,,
16279,b'Add Flaubert OnnxConfig to Transformers',2022-03-20T10:13:25Z,2022-03-21T20:46:32Z,,,
16278,"b'Connection error, when I run a service in a docker image, which is offline'",2022-03-20T09:18:31Z,2022-03-20T09:20:12Z,,,
16277,b'Typo: Missing a coma in document',2022-03-20T08:10:34Z,2022-03-21T12:10:24Z,,,
16276,b'Freezing layers does not work with torch.utils.checkpoint',2022-03-20T04:54:22Z,,Migration,,
16275,b'Adding type hints & decorator for TFT5',2022-03-20T00:47:47Z,,,,
16274,b'add GPT-J ONNX config to Transformers',2022-03-19T20:12:13Z,,,,
16273,b'added type hints for Blenderbot and BlenderbotSmall (TF & PyTorch)',2022-03-19T19:24:28Z,2022-03-21T18:07:03Z,,,
16272,b'Add type hints for ProphetNet PyTorch',2022-03-19T16:46:39Z,,,,
16271,b'Fix missing output_attentions in PT/Flax equivalence test',2022-03-19T14:50:14Z,,,,
16270,b'added type hints for BART model',2022-03-19T12:59:22Z,2022-03-21T15:18:02Z,,,
16269,b'TypeError loading tokenizer for gpssohi/distilbart-qgen-6-6',2022-03-19T11:03:16Z,2022-03-21T17:21:19Z,,"OSError, TypeError","OSError: Can't load config for 'gpssohi/distilbart-qgen-6-6'. Make sure that:TypeError: special token bos_token has to be either str or AddedToken but got: <class 'dict'>"
16268,b'RuntimeError opening google/pegasus-xsum',2022-03-19T10:46:25Z,2022-03-21T17:44:55Z,,RuntimeError,"RuntimeError: a view of a leaf Variable that requires grad is being used in an in-place operation."
16267,b'Add type hints transfoxl',2022-03-19T09:18:52Z,2022-03-21T15:04:14Z,,,
16266,"b""Removed the 'optional' string (in DETR post_process)""",2022-03-19T02:40:11Z,2022-03-21T11:39:45Z,,,
16265,b'Add OFA',2022-03-18T21:29:57Z,,,,
16264,b'Reorganize file utils',2022-03-18T20:55:48Z,,,,
16263,b'Add type annotations of config for vision models',2022-03-18T19:59:52Z,,,,
16262,b'Do you fine-tunde both encoder and decoder ',2022-03-18T19:23:00Z,,,,
16261,b'GPT2 TensorFlow Type Hints',2022-03-18T18:22:35Z,2022-03-21T16:11:03Z,,,
16260,b'TF - update (vision_)encoder_decoder past variable',2022-03-18T17:58:27Z,2022-03-21T19:55:41Z,,,
16259,b'[WIP] add `has_attentions` as done in PyTorch side',2022-03-18T17:04:19Z,2022-03-19T10:44:18Z,,,
16258,b'Update flaubert with TF decorator',2022-03-18T16:56:10Z,2022-03-18T17:57:55Z,,,
16257,b'Added type hints for PyTorch T5 model',2022-03-18T16:25:44Z,2022-03-21T16:17:52Z,,,
16256,b'Cannot replicate the scores from the pipeline',2022-03-18T16:15:17Z,,,,
16255,b'Add TF ViT MAE',2022-03-18T15:51:08Z,,,,
16254,b'update jax version and re-enable some tests',2022-03-18T14:20:37Z,2022-03-18T15:45:39Z,,,
16253,b'Add Slack notification support for doc tests',2022-03-18T13:28:14Z,2022-03-21T10:33:19Z,,,
16252,b'Weird PanicException when trying to save tokenizer using `tokenizer.save_pretrained`',2022-03-18T13:01:24Z,,,"pyo3_runtime.PanicException, subprocess.CalledProcessError","pyo3_runtime.PanicException: no entry found for keysubprocess.CalledProcessError: Command '['/home/ec2-user/anaconda3/envs/pytorch_latest_p36/bin/python', 'run.py', '--model_name_or_path', 'YituTech/conv-bert-base', '--train_file', '../data/train.csv', '--validation_file', '../data/val.csv', '--test_file', '../data/test.csv', '--max_length', '128', '--per_device_train_batch_size', '32', '--learning_rate', '2e-5', '--num_train_epochs', '10', '--output_dir', 'conv-bert-base/']' returned non-zero exit status 1."
16251,b'ViT-mae missing parameter at comments of `copied  from`',2022-03-18T12:26:05Z,2022-03-18T19:41:56Z,,,
16250,b'Aggressive PT/TF equivalence test on PT side',2022-03-18T11:37:07Z,2022-03-18T17:51:25Z,,,
16249,b'is this a bug in TFTrainer.get_train_tfdataset?',2022-03-18T10:49:12Z,,,,
16248,b'NER at the Inference Time',2022-03-18T08:14:22Z,,,,
16247,b'Update XLM with TF decorator',2022-03-18T07:52:58Z,2022-03-18T14:07:03Z,,,
16246,b'[Constrained Beam Search] Adding Notebook Example & Minor Typo Fix',2022-03-18T06:18:05Z,2022-03-18T16:04:43Z,,,
16245,b'Load local dataset error',2022-03-18T03:14:35Z,2022-03-18T03:28:23Z,,,
16244,b'Add missing type hints for PyTorch Longformer models',2022-03-18T00:03:14Z,2022-03-21T17:09:03Z,,,
16243,b'Update troubleshoot with more content',2022-03-17T22:31:23Z,2022-03-21T16:37:18Z,Documentation,,
16242,b'Add unpack_inputs decorator for ctrl',2022-03-17T22:26:05Z,2022-03-18T15:33:24Z,,,
16241,b'[xtreme-s] Update Minds14 results',2022-03-17T21:13:52Z,2022-03-21T18:34:00Z,,,
16240,b'add type hints for gpt2 model classes (PyTorch)',2022-03-17T19:59:06Z,,,,
16239,b'Make `add-new-model-like` work in an env without all frameworks',2022-03-17T19:40:35Z,2022-03-21T08:29:05Z,,,
16238,b'[Deepspeed] non-HF Trainer doc update',2022-03-17T19:36:37Z,2022-03-17T20:33:56Z,,,
16237,b'Draft a guide with our code quirks for new models',2022-03-17T19:14:09Z,2022-03-21T11:44:03Z,,,
16236,b'[FlaxSpeechEncoderDecoderModel] Skip from_encoder_decoder_pretrained',2022-03-17T18:45:23Z,2022-03-17T19:05:14Z,,,
16235,b'Enable the use of private models in example scripts using use_auth_token',2022-03-17T17:52:48Z,,,,
16234,b'Loading from AutoModel gives ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds',2022-03-17T17:16:56Z,2022-03-17T17:18:45Z,,ValueError,"ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds"
16233,b'How to pass args of generate() method via batch_decode',2022-03-17T17:03:58Z,2022-03-18T14:05:38Z,,,
16232,b'A question about the cross-attention layer shape in encoder-decoder model',2022-03-17T16:46:41Z,2022-03-18T04:38:01Z,,,
16231,b'fix(flax): generate with logits processor/warper',2022-03-17T16:28:49Z,2022-03-17T18:39:17Z,,,
16230,b'Fix TFBert `past_key_values`',2022-03-17T16:06:05Z,2022-03-18T19:56:54Z,,,
16229,b'Translate installation.mdx to Spanish',2022-03-17T15:33:50Z,,,,
16228,b'TFLongformer: Add missing type hints and unpack inputs decorator',2022-03-17T14:16:37Z,2022-03-21T22:56:17Z,,,
16227,b'Fix Type Hint of Nan/Inf Logging Filter Arg',2022-03-17T12:52:54Z,2022-03-17T15:05:38Z,,,
16226,b'add unpack_inputs decorator for marian',2022-03-17T12:33:41Z,2022-03-17T18:23:40Z,,,
16225,b'Truncation Error is inconsistent between fast and standard (non-fast) tokenizers',2022-03-17T12:08:10Z,,,,
16224,b'Skip equivalence test for TransfoXL',2022-03-17T12:04:46Z,2022-03-17T13:03:08Z,,,
16223,b'Add type hints for XLM TensorFlow',2022-03-17T11:58:50Z,,,,
16222,b'Attention mask is important in the case of batching...',2022-03-17T11:23:56Z,2022-03-18T09:02:13Z,,,
16221,"b'token classification pipeline does not use attention mask, affects predictions'",2022-03-17T11:02:00Z,2022-03-18T09:02:12Z,,,
16220,b'[Flax] remove jax.ops.index',2022-03-17T10:44:08Z,2022-03-17T16:51:43Z,,,
16219,b'MaskFormer: fix device on test',2022-03-17T09:49:59Z,2022-03-17T12:11:55Z,,,
16218,b'[Tests] Fix DiT test',2022-03-17T08:20:54Z,2022-03-17T09:53:57Z,,,
16217,b'Fix readmes',2022-03-17T06:25:39Z,2022-03-17T11:47:01Z,,,
16216,b'Question about `query_length` in modeling_t5.py ',2022-03-17T05:28:31Z,,,,
16215,b'Framework split for Spanish version of doc quicktour.mdx',2022-03-17T04:28:56Z,2022-03-21T11:37:45Z,,,
16214,b'Add type hints to xlnet',2022-03-17T02:17:13Z,2022-03-21T13:04:18Z,,,
16213,b'ValueError: HFArgumentParser on running run_glue.py',2022-03-16T23:59:34Z,,,,
16212,b'Adding Unpack Decorator For DPR model',2022-03-16T22:38:33Z,2022-03-17T12:33:03Z,,,
16211,b'SegFormer Quantized Model (Int8) not loading weights properly.',2022-03-16T20:36:17Z,,,,
16210,b'Using pooler/hidden states output of an AutoModel vs. XForSequenceClassification for training another classifier?',2022-03-16T19:37:58Z,,,,
16209,b'Fix reproducibility in Training for PyTorch 1.11',2022-03-16T19:11:31Z,2022-03-17T11:42:58Z,,,
16208,b'Fix typos in docstrings of data_collator.py',2022-03-16T17:56:41Z,2022-03-17T11:21:20Z,,,
16207,b'rembert/splinter (torch) type annotations',2022-03-16T16:59:57Z,,,,
16206,b'Fix generation min length',2022-03-16T16:30:32Z,2022-03-16T17:49:24Z,,,
16205,b'Clinical longformer - IndexError: index out of range in self',2022-03-16T16:29:54Z,,,IndexError,"IndexError: index out of range in self"
16204,b'Doctests - reporting tool',2022-03-16T16:23:00Z,2022-03-18T13:28:36Z,,,
16203,b'Training MLM model XLM Roberta large on google machine specs not fast',2022-03-16T16:07:05Z,,,,
16202,b'TF: add beam search tests',2022-03-16T14:58:32Z,2022-03-16T15:44:34Z,,,
16201,b'VAN: update modules names',2022-03-16T14:27:32Z,2022-03-17T09:25:10Z,,,
16200,b'Added type hints for Pytorch Marian calls',2022-03-16T14:20:07Z,,,,
16199,b'Add GLPN',2022-03-16T14:06:54Z,2022-03-22T07:51:13Z,,,
16198,b'Fix loading CLIPVisionConfig and CLIPTextConfig',2022-03-16T13:57:01Z,2022-03-16T15:24:04Z,,,
16197,b'Fix-clip-configs',2022-03-16T13:53:34Z,2022-03-16T13:54:52Z,,,
16196,b'ResNet: update modules names',2022-03-16T13:47:56Z,2022-03-16T14:59:56Z,,,
16195,b'Gpt2 large for onnx exportation and int8 quantization',2022-03-16T13:36:40Z,,New model,,
16194,b'clearer model variable naming: blenderbot_small',2022-03-16T13:28:46Z,2022-03-16T14:03:59Z,,,
16193,b'Minor fixes to XTREME-S',2022-03-16T12:46:06Z,2022-03-16T13:23:00Z,,,
16192,b'clearer model variable naming: blenderbot',2022-03-16T12:13:22Z,2022-03-16T12:37:08Z,,,
16191,b'layoutlmv2 visual backbone pretrained on publaynet ',2022-03-16T11:29:13Z,2022-03-21T07:41:37Z,,,
16190,b'Initialized DETR backbone weights do not match with actual pretrained weights ',2022-03-16T10:26:19Z,,,,
16189,b'Update a CI job step name',2022-03-16T10:11:14Z,2022-03-16T15:19:38Z,,,
16188,b'RegNet',2022-03-16T09:08:16Z,,,,
16187,b'Fix `LayoutLMv2` tokenization docstrings',2022-03-16T07:59:28Z,,,,
16186,b'Truncation when tokenizer does not have `max_length` defined',2022-03-16T06:57:09Z,,,RuntimeError,"RuntimeError: The size of tensor a (515) must match the size of tensor b (512) at non-singleton dimension 1"
16185,b'CLIPVisionModel errors on trying to load openai/clip-vit-base-patch16',2022-03-16T05:46:56Z,2022-03-16T15:24:01Z,,RuntimeError,"RuntimeError: Error(s) in loading state_dict for CLIPVisionModel:"
16184,b'Translated version of model_sharing.mdx doc to spanish',2022-03-16T03:39:11Z,,,,
16183,b'[Xtreme-S] fix some namings',2022-03-16T00:14:18Z,2022-03-16T00:21:31Z,,,
16182,b'Fix links in guides',2022-03-15T22:53:14Z,2022-03-18T21:16:17Z,Documentation,,
16181,b'unpack_input decorator for tf_convnext',2022-03-15T21:42:38Z,2022-03-16T14:01:33Z,,,
16180,b'Small fixes to the documentation',2022-03-15T21:26:48Z,2022-03-18T21:48:28Z,,,
16179,b'rembert type annotations and dependencies',2022-03-15T20:30:41Z,2022-03-16T16:48:47Z,,,
16178,b'clearer model variable naming: funnel',2022-03-15T19:59:59Z,2022-03-16T10:37:48Z,,,
16177,b'type annotations for rembert w/ copies from bert (torch)',2022-03-15T17:32:41Z,2022-03-15T20:19:18Z,,,
16176,b'Translate accelerate.mdx from english to spanish',2022-03-15T17:26:16Z,,,,
16175,b'Add type hints for Reformer PyTorch',2022-03-15T17:12:46Z,2022-03-15T17:59:59Z,,,
16174,b'Add type hints for Perceiver Pytorch',2022-03-15T17:12:07Z,2022-03-15T17:56:58Z,,,
16173,b'Attempt to make load `from_tf=True` more efficiently',2022-03-15T16:32:09Z,,,,
16172,b'Add type hints for XLM (TensorFlow)',2022-03-15T15:52:33Z,,,,
16171,b'bert: properly mention deprecation of TF2 conversion script',2022-03-15T15:23:51Z,,,,
16170,b'[MT5Config] add relative_attention_max_distance in config',2022-03-15T14:58:35Z,2022-03-15T15:26:52Z,,,
16169,b'Add missing type hint for XLM (TensorFlow)',2022-03-15T14:47:47Z,2022-03-15T15:10:18Z,,,
16168,b'Fix FlaxRoFormerClassificationHead activation',2022-03-15T14:22:17Z,2022-03-17T10:45:50Z,,,
16167,"b""Fix some Flax models' `hidden_states`""",2022-03-15T13:30:59Z,2022-03-15T18:06:47Z,,,
16166,b'update transformer XL with tf decorator',2022-03-15T12:42:32Z,2022-03-15T14:00:19Z,,,
16165,b'value check for typical sampling',2022-03-15T11:20:17Z,2022-03-18T16:05:29Z,,,
16164,b'Add type hints for Pegasus model (PyTorch)',2022-03-15T10:58:55Z,2022-03-22T08:51:15Z,,,
16163,b'added type hints to yoso',2022-03-15T09:45:22Z,2022-03-15T14:04:32Z,,,
16162,b'ResNetD',2022-03-15T08:47:22Z,,,,
16161,b'How to resume run_t5_mlm_flax.py script?',2022-03-15T03:47:17Z,2022-03-15T15:06:28Z,,,
16160,b' `TFBertModel.predict()` and `TFBertModel()` return different results.',2022-03-15T01:36:05Z,2022-03-15T14:19:46Z,,,
16159,b'Text2Speech classes',2022-03-15T00:01:44Z,,,,
16158,b'Added spanish translation of quicktour.mdx',2022-03-14T22:36:12Z,2022-03-15T12:07:35Z,,,
16157,b'Implement Maximal Update Parametrization (muP)',2022-03-14T22:08:55Z,,,,
16156,b'Add type hints OpenAIGPT',2022-03-14T21:13:07Z,,,,
16155,b'Configurable Relative Position Max. Distance',2022-03-14T20:55:53Z,2022-03-15T12:05:34Z,,,
16154,b'Shift responsibilities a bit for issues',2022-03-14T20:39:33Z,2022-03-15T10:07:17Z,,,
16153,b'Change assertion to warning when passing past_key_value to T5 encoder',2022-03-14T20:01:24Z,2022-03-18T11:52:55Z,,,
16152,b'clearer model variable naming: pegasus',2022-03-14T19:40:54Z,2022-03-15T13:46:00Z,,,
16151,b'Add/type annotations/model vision',2022-03-14T19:36:25Z,2022-03-16T20:27:54Z,,,
16150,b'clearer model variable naming: xlnet',2022-03-14T19:14:26Z,2022-03-15T17:50:30Z,,,
16149,b'Translation from english to spanish of file pipeline_tutorial.mdx',2022-03-14T18:58:58Z,,,,
16148,b'[Flax] improve large model init and loading ',2022-03-14T18:29:15Z,,,,
16147,b'update albert with tf decorator',2022-03-14T17:49:07Z,2022-03-14T18:09:19Z,,,
16146,b'clearer model variable naming: Deberta',2022-03-14T17:46:12Z,2022-03-15T16:53:26Z,,,
16145,b'clearer model variable naming: Tapas',2022-03-14T17:28:42Z,2022-03-15T16:52:56Z,,,
16144,b'Add type hints for XLM TensorFlow',2022-03-14T17:20:51Z,2022-03-14T20:15:38Z,,,
16143,b'clearer model variable naming: ELECTRA',2022-03-14T17:19:31Z,2022-03-14T18:10:07Z,,,
16142,b'Use templates',2022-03-14T16:15:28Z,2022-03-15T12:07:56Z,,,
16141,b'Dcoker images runtime -> devel',2022-03-14T15:37:02Z,2022-03-14T16:37:20Z,,,
16140,b'Fixes Loss for TransfoXL when using Trainer API v2',2022-03-14T15:32:40Z,2022-03-17T09:49:25Z,,,
16139,b'Use `HF_ENDPOINT` for custom endpoints',2022-03-14T15:08:11Z,2022-03-14T17:26:24Z,,,
16138,b'Steps strategy fix for PushtoHubCallback and changed docstring',2022-03-14T12:59:05Z,2022-03-14T13:37:07Z,,,
16137,b'[Doctests] up',2022-03-14T11:41:16Z,2022-03-16T16:45:26Z,,,
16136,"b'Add warning message if model uses `input_ids` that include padding tokens, but no `attention_mask` is provided.'",2022-03-14T11:21:32Z,,Good First Issue,,
16135,b'Improve EncoderDecoderModel docs',2022-03-14T11:14:12Z,,Good First Issue,,
16134,b'Add possibility to evaluate predefined completions based on logits softmax',2022-03-14T10:19:00Z,2022-03-18T07:57:32Z,,,
16133,b'[Generate Docs] Correct docs',2022-03-14T09:50:48Z,2022-03-17T19:05:28Z,,,
16132,"b'typo ""conaining"" -> ""containing""'",2022-03-14T09:36:22Z,2022-03-15T11:08:54Z,,,
16131,b'integrations: mlflow: skip start_run() if a run is already active and sanity check on enabling integration',2022-03-14T09:01:01Z,2022-03-17T20:39:58Z,,,
16130,b'Nystromformer Pretrained model with longer sequence length ',2022-03-14T05:34:04Z,,,,
16129,b'Better input variable naming for OpenAI (TF)',2022-03-14T00:02:48Z,2022-03-14T15:25:46Z,,,
16128,b'Improve model variable naming - CLIP [TF] ',2022-03-13T23:21:36Z,2022-03-14T15:26:41Z,,,
16127,b'Add type hints for GPTNeo PyTorch',2022-03-13T23:20:59Z,2022-03-14T19:26:12Z,,,
16126,b'Add type hints for SqueezeBert PyTorch',2022-03-13T23:02:54Z,2022-03-14T16:21:08Z,,,
16125,b'finetuning without trainer seems not data parallel when using deepspeed?',2022-03-13T21:52:08Z,2022-03-15T09:17:04Z,,,
16124,b'Update convert_marian_to_pytorch.py',2022-03-13T21:05:01Z,2022-03-14T11:15:38Z,,,
16123,b'Add type hints for FNet PyTorch',2022-03-13T16:27:36Z,2022-03-14T17:11:19Z,,,
16122,b'Remove `pos` from _build_network_inputs',2022-03-13T14:49:52Z,,,,
16121,b'Add type hints for PoolFormer in Pytorch',2022-03-13T09:40:46Z,2022-03-14T16:14:04Z,,,
16120,b'RAM OOM with a large save_steps using trainer API for MLM training',2022-03-13T09:38:46Z,,,,
16119,b'[ViTMAE] Add copied from statements and fix prefix',2022-03-13T09:31:00Z,2022-03-14T14:05:15Z,,,
16118,b'Add flaubert types',2022-03-13T09:18:41Z,2022-03-15T16:57:45Z,,,
16117,b'Update expected slices for pillow > 9',2022-03-13T09:01:01Z,2022-03-18T08:46:45Z,,,
16116,"b'Change unpacking of TF mpnet, rag, roformer inputs to use decorator'",2022-03-13T08:18:46Z,2022-03-14T12:13:47Z,,,
16115,b'Fixed Error Raised Due to Wrongly Accessing Training Sample',2022-03-13T07:35:50Z,2022-03-21T11:54:55Z,,,
16114,"b'Issue in Tutorial - ""Fine-tune a pretrained model""'",2022-03-13T07:29:43Z,2022-03-21T11:54:54Z,,KeyError,"KeyError: ""Invalid key: 100. Please first select a split. For example: `my_dataset_dictionary['train'][100]`. Available splits: ['test', 'train']"""
16113,b'Fix broken links',2022-03-13T07:13:03Z,,,,
16112,b'Change unpacking of TF layoutlm inputs to use decorator',2022-03-13T06:56:32Z,2022-03-15T13:47:45Z,,,
16111,b'Add type hints for Luke in PyTorch',2022-03-13T06:06:18Z,2022-03-14T15:55:04Z,,,
16110,b'Change unpacking of TF mobilebert inputs to use decorator',2022-03-13T06:05:06Z,2022-03-14T13:15:08Z,,,
16109,b'Add type annotations for GPTJ',2022-03-12T22:07:50Z,,,,
16108,b'Add type hints to XLM model (PyTorch)',2022-03-12T18:29:51Z,2022-03-12T19:28:49Z,,,
16107,b'Add type hints for TFDistilBert',2022-03-12T16:49:26Z,2022-03-14T15:39:59Z,,,
16106,b'Add type annotations for CLIP (torch) (#16059)',2022-03-12T14:58:40Z,2022-03-14T16:56:04Z,,,
16105,b'Added missing type hints - Deberta V1 and V2',2022-03-12T14:58:25Z,2022-03-14T15:12:23Z,,,
16104,b'Added missing type hints - ELECTRA TF',2022-03-12T14:39:02Z,2022-03-14T16:28:35Z,,,
16103,b'Added missing type hints - ELECTRA PyTorch',2022-03-12T14:19:35Z,2022-03-14T14:53:57Z,,,
16102,b'Add unpack_input decorator to ViT model',2022-03-12T13:39:38Z,2022-03-12T15:05:14Z,,,
16101,b'Different weights in ViTMAEModel and ViTMAEForPreTraining',2022-03-12T11:52:07Z,2022-03-14T14:05:14Z,,,
16100,b'Tf t5 clearer var naming (unpack_inputs decorator tensorflow)',2022-03-12T10:08:13Z,2022-03-12T12:20:02Z,,,
16099,b'Add type annotations for segformer pytorch',2022-03-12T10:06:54Z,2022-03-12T12:37:10Z,,,
16098,"b""cannot import name 'Text2TextGenerationPipeline""",2022-03-12T09:50:29Z,,,,
16097,b'add unpack_inputs decorator to mbart tf',2022-03-12T08:29:10Z,2022-03-12T12:30:43Z,,,
16096,b'Any reasons not using `processors` or `pipelines` modules in example codes?',2022-03-12T07:36:29Z,,,,
16095,b'Question about the why num_beams is multiplied by 2 in Beamsearch',2022-03-11T22:57:25Z,2022-03-17T17:27:35Z,,,
16094,b'Change unpacking of TF Bart inputs to use decorator',2022-03-11T22:46:28Z,2022-03-12T12:06:56Z,,,
16093,b'[ZeRO] Fixes issue with embedding resize',2022-03-11T22:41:17Z,2022-03-11T23:13:12Z,DeepSpeed,,
16092,"b'Add type annotations for Beit, ViT and Deit models - Pytorch'",2022-03-11T22:30:30Z,2022-03-14T19:03:38Z,,,
16091,b'Adding type hints for BigBird pytorch',2022-03-11T21:42:26Z,,,,
16090,b'Adding type hints for Distilbert',2022-03-11T21:36:12Z,2022-03-16T14:54:51Z,,,
16089,b'Add missing type hints for all flavors of LayoutLMv2 PyTorch models.',2022-03-11T18:48:00Z,2022-03-13T18:54:02Z,,,
16088,b'Add type annotations for ImageGPT',2022-03-11T18:45:59Z,2022-03-11T19:16:14Z,,,
16087,b'[Fix doc example] Fix first example for the custom_datasets tutorial',2022-03-11T18:44:10Z,2022-03-15T12:17:52Z,,,
16086,b'Add missing type hints for all flavors of RoBERTa PyTorch models.',2022-03-11T18:12:04Z,2022-03-11T19:40:50Z,,,
16085,b'[Fix doc example] FSMT',2022-03-11T17:58:12Z,2022-03-11T20:21:31Z,,,
16084,b'Fix Typo in Argument of FlaxWav2Vec2ForPreTrainingModule',2022-03-11T17:54:07Z,,,,
16083,b'[Fix doc example] Fix checkpoint name in docstring example',2022-03-11T17:35:15Z,2022-03-14T15:19:18Z,,NameError,"NameError: name '_CHECKPOINT_FOR_DOC' is not defined"
16082,b'Fix ProphetNetTokenizer',2022-03-11T17:21:28Z,2022-03-14T13:02:42Z,,,
16081,b'Rebuild deepspeed',2022-03-11T17:21:16Z,2022-03-11T19:35:49Z,,,
16080,b'CUDA Error with Typical Sampling mass of 1.0',2022-03-11T17:05:24Z,2022-03-18T16:05:28Z,,RuntimeError,"RuntimeError: CUDA error: device-side assert triggered"
16079,b'Fix and document Zero Shot Image Classification',2022-03-11T16:35:23Z,2022-03-14T07:50:36Z,,,
16078,"b""Replace all deprecated `jax.ops` operations with jnp's `at`""",2022-03-11T16:24:56Z,2022-03-16T09:08:56Z,,,
16077,b'Run daily doctests without time-out at least once',2022-03-11T16:17:26Z,2022-03-11T17:04:18Z,,,
16076,b'[Fix doc example] Fix 2 PyTorch Vilt docstring examples',2022-03-11T15:32:26Z,2022-03-15T12:35:02Z,,,
16075,b'[WIP] Do not merge! Test PR for GH actions',2022-03-11T14:55:52Z,2022-03-18T21:48:42Z,,,
16074,b'Add type annotations for BERT and copies',2022-03-11T14:23:43Z,2022-03-11T16:13:29Z,,,
16073,b'Add TFCamembertForCausalLM and ONNX integration test',2022-03-11T13:57:16Z,2022-03-14T07:40:43Z,,,
16072,b'Fix torch-scatter version',2022-03-11T13:47:23Z,2022-03-11T14:03:28Z,,,
16071,"b'Want to push a branch for a PR that fixes Deberta classification dropout and armonizes the dropout class, but have no permissions.'",2022-03-11T12:52:08Z,2022-03-15T13:49:12Z,,,
16070,b'Improve Swin for VisionEncoderDecoder',2022-03-11T12:15:24Z,2022-03-15T08:59:48Z,,,
16069,"b""[INFO|modelcard.py:460] 2022-03-11 19:03:24,502 >> Dropping the following result as it does not have all the necessary fields: {'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}""",2022-03-11T11:32:27Z,2022-03-21T01:20:17Z,,,
16068,b'CUDA out of memory when resuming from checkpoint (not always)',2022-03-11T10:45:42Z,2022-03-14T09:24:59Z,,RuntimeError,"RuntimeError: CUDA out of memory. Tried to allocate 972.00 MiB (GPU 3; 15.78 GiB total capacity; 11.59 GiB already allocated; 1.02 GiB free; 13.75 GiB reserved in total by PyTorch)"
16067,b'How to load google-research/bert .ckpt weight',2022-03-11T10:16:28Z,2022-03-20T17:03:45Z,,,
16066,b'Remove assertion over possible activation functions in DistilBERT',2022-03-11T09:59:45Z,2022-03-11T13:27:59Z,,,
16065,"b""Make sure `'torch.dtype'` has str-type value in config and all nested dicts for JSON serializability """,2022-03-11T05:42:40Z,2022-03-11T17:00:11Z,,,
16064,b'Update configuration_big_bird.py',2022-03-11T05:36:48Z,,,,
16063,b'FIX: updating doc/example for fine-tune for downstream Token Classification',2022-03-10T20:54:13Z,2022-03-10T21:21:56Z,,,
16062,b'Move QDQBert in just PyTorch block',2022-03-10T20:53:17Z,2022-03-11T12:58:03Z,,,
16061,b'Fix a TF test name (LayoutLMModelTest)',2022-03-10T20:15:19Z,2022-03-11T10:22:36Z,,,
16060,b'Unable to load Wav2Vec2 fine-tuned models from local files',2022-03-10T20:14:36Z,,,"UnicodeDecodeError, OSError","UnicodeDecodeError: 'utf-8' codec can't decode byte 0x80 in position 64: invalid start byteOSError: Can't load tokenizer for './training/checkpoint-90500/'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure './training/checkpoint-90500/' is the correct path to a directory containing all relevant tokenizer files."
16059,b'Add missing type hints',2022-03-10T19:06:15Z,,Good First Issue,,
16058,b'[WIP] Flax Speech Recognition Seq2Seq',2022-03-10T18:26:40Z,,,,
16057,b'Adding type hints for TFRoBERTa',2022-03-10T18:19:42Z,2022-03-11T16:13:48Z,,,
16056,b'Fix Loading of Flax(Speech)EncoderDecoderModel kwargs from PreTrained Encoder-Decoder Checkpoints',2022-03-10T18:04:33Z,2022-03-14T09:12:29Z,,TypeError,"TypeError: __init__() got an unexpected keyword argument 'add_adapter'"
16055,b'[Test] different update approach',2022-03-10T17:53:41Z,2022-03-10T17:53:49Z,,,
16054,b'`torch.dtype` not JSON serializable if config is nested dict',2022-03-10T17:23:06Z,2022-03-11T17:00:11Z,,`TypeError,`TypeError: Object of type dtype is not JSON serializable`
16053,b'bug in model.generate() / beam_search score per token',2022-03-10T16:35:50Z,2022-03-15T10:03:39Z,,,
16052,b'Update ArrowWriter path in examples',2022-03-10T15:52:08Z,2022-03-10T19:11:19Z,,,
16051,b'TF: clearer model variable naming',2022-03-10T15:30:36Z,,Good First Issue,,
16050,b'Text Generation Pipeline not using Target Tokenizer',2022-03-10T15:14:25Z,,,,
16049,b'Use the target tokenizer in text generation pipeline',2022-03-10T15:03:09Z,,,,
16048,b'MarianMTModel is not trained after finished training and loading',2022-03-10T15:02:12Z,2022-03-10T21:59:07Z,,,
16047,b'Spanish translation of the file training.mdx',2022-03-10T14:54:13Z,2022-03-14T14:12:38Z,,,
16046,b'Docker image nightly torch job',2022-03-10T13:28:35Z,,,,
16045,b'Fix duplicate arguments passed to dummy inputs in ONNX export',2022-03-10T13:02:48Z,2022-03-10T19:19:46Z,,,
16044,b'Fix tf pytorch test in auto',2022-03-10T11:40:09Z,,,,
16043,b'DeBERTa/DeBERTa-v2/SEW Support for torch 1.11',2022-03-10T11:28:32Z,2022-03-10T14:01:05Z,,,
16042,b'[README] fix url for Preprocessing tutorial',2022-03-10T10:58:23Z,2022-03-10T11:09:05Z,,,
16041,b'Fix Bug in Flax-Speech-Encoder-Decoder Test',2022-03-10T10:33:55Z,2022-03-10T11:09:29Z,,,
16040,b'Use decoder_inputs in generate function',2022-03-10T10:31:17Z,,,,
16039,b'How to add uppercase letter in chinese-roberta-wwm-ext-large vocab?',2022-03-10T08:14:11Z,,,,
16038,b'what us the difference between Trainer and Seq2SeqTrainer ?',2022-03-10T08:00:26Z,2022-03-10T08:42:43Z,,,
16037,"b""The documentation of transformers.generation_utils.GenerationMixin.generate doesn't match the code of it""",2022-03-10T05:10:30Z,,,,
16036,b'RuntimeError: [enforce fail at CPUAllocator.cpp:68] Out of memory during batched inference',2022-03-10T04:05:46Z,2022-03-14T22:38:05Z,,,
16035,"b'raise ValueError(f""Unrecognized tokenizer_type {tokenizer_type}"") ValueError: Unrecognized tokenizer_type BertWordPieceCase'",2022-03-10T03:40:04Z,,,ValueError,"ValueError: Unrecognized tokenizer_type BertWordPieceCase"
16034,b'Make BigBird model compatiable to fp16 dtype.',2022-03-10T01:13:21Z,,,RuntimeError,"RuntimeError: expected scalar type Float but found Half"
16033,b'Fix dependency error message in ServeCommand',2022-03-10T00:00:19Z,2022-03-10T10:35:27Z,,,
16032,b'how can we use the model output to do predict?',2022-03-09T21:23:44Z,2022-03-10T07:19:57Z,,,
16031,b'Fix  TFDebertaV2ConvLayer in TFDebertaV2Model',2022-03-09T20:55:21Z,2022-03-10T11:23:46Z,,,
16030,b'Framework split',2022-03-09T20:20:13Z,2022-03-15T14:13:34Z,,,
16029,"b""Don't compute metrics in LM examples on TPU""",2022-03-09T19:25:37Z,2022-03-10T12:44:51Z,,,
16028,b'Update to 4.18.0dev0',2022-03-09T18:09:42Z,2022-03-09T18:19:58Z,,,
16027,b'Visual Attention Network (VAN)',2022-03-09T18:07:13Z,2022-03-15T07:47:13Z,,,
16026,"b""TypeError: forward() got an unexpected keyword argument 'return_dict' BERT CLASSIFICATION HUGGINFACE with tuning""",2022-03-09T17:03:18Z,,,TypeError,"TypeError: forward() got an unexpected keyword argument 'return_dict'"
16025,b'[CI] switching CI to pytorch-1.11',2022-03-09T16:51:11Z,2022-03-15T11:06:54Z,Testing,,
16024,b'Doc builder fix push 2',2022-03-09T16:47:02Z,2022-03-09T16:55:10Z,,,
16023,b'Fix warning message in ElectraForCausalLM',2022-03-09T16:43:41Z,2022-03-09T22:27:15Z,,,
16022,"b""JAX 0.2.22: Replace all deprecated `jax.ops` operations with jnp's `at`""",2022-03-09T16:32:36Z,2022-03-16T16:22:25Z,,,
16021,b'Fix Bug in Flax Seq2Seq Models',2022-03-09T16:06:42Z,2022-03-10T13:58:05Z,,TypeError,"TypeError: ones_like requires ndarray or scalar arguments, got <class 'NoneType'> at position 0."
16020,b'Build the doc in a seperate folder then move it',2022-03-09T16:05:56Z,2022-03-10T12:44:29Z,,,
16019,b'Doc build bug?',2022-03-09T15:28:09Z,2022-03-11T14:00:30Z,,,
16018,b'Choose framework for ONNX export',2022-03-09T15:01:58Z,2022-03-14T15:47:29Z,,,
16017,b'Update build_documentation.yml',2022-03-09T14:47:08Z,2022-03-09T15:23:38Z,,,
16016,"b""what's the difference between Megatron-gpt2 and GPT2 inside transformers?""",2022-03-09T14:24:15Z,,,,
16015,b'Make transformers.utils.fx. _SUPPORTED_MODELS unique',2022-03-09T13:53:22Z,2022-03-15T09:15:03Z,,,
16014,b'Swag example: Update doc format',2022-03-09T12:27:20Z,2022-03-09T13:25:35Z,,,
16013,b'Learning rate finder for the trainer ',2022-03-09T10:43:28Z,2022-03-10T10:20:01Z,,,
16012,b'Fix MaskFormer failing test on master',2022-03-09T10:37:13Z,2022-03-09T14:51:57Z,,,
16011,b'Removed an outdated check about hdf5_version',2022-03-09T10:05:48Z,2022-03-09T13:21:24Z,,,
16010,b'Beam search uses large amounts of VRAM even with depth of 1',2022-03-09T09:42:10Z,,,,
16009,b'Fix github actions comment',2022-03-09T09:22:10Z,2022-03-09T13:39:04Z,,,
16008,"b'resize_token_embeddings() failed with GPT-J, after sync to the latest DeepSpeed 0.6.1'",2022-03-09T09:19:51Z,2022-03-11T23:13:11Z,,RuntimeError,"RuntimeError: The expanded size of the tensor (50400) must match the existing size (0) at non-singleton dimension 0.  Target sizes: [50400].  Tensor sizes: [0]"
16007,b'TrOCR Backslash problem',2022-03-09T07:58:27Z,2022-03-11T08:53:14Z,,,
16006,b'Recommended way of exporting encoder-decoder model to ONNX with `transformers[onnx]`',2022-03-09T05:02:57Z,,,`ValueError,"`ValueError: Model requires 4 inputs. Input Feed contains 2`. "
16005,b'`mlm` training fails due to large message size for `nested_gather` on torch_xla',2022-03-09T04:20:50Z,,,RuntimeError,"RuntimeError: tensorflow/compiler/xla/xla_client/mesh_service.cc:377 : Failed to meet rendezvous 'nested_gather': Received message larger than max (950146944 vs. 4194304) (8)"
16004,b'Fix wav2vec2 export onnx model with attention_mask error',2022-03-09T03:22:47Z,,,,
16003,b'Multiclass image classification with ViT - computer vision',2022-03-09T01:53:05Z,,,,
16002,b'Translate to Spanish of training.mdx',2022-03-09T01:22:21Z,2022-03-09T03:29:19Z,,,
16001,b'Update troubleshoot guide',2022-03-08T20:40:11Z,2022-03-11T19:05:44Z,Documentation,,
16000,b'removing azureml-specific code - now covered by MLflow',2022-03-08T20:32:59Z,,,,
15999,b'Sharded DDP returns extra predictions EvalPrediction',2022-03-08T18:56:04Z,,,,
15998,b'Uncomment to see error',2022-03-08T18:30:15Z,2022-03-09T08:47:21Z,,,
15997,b'Freeze Feature Encoder in FlaxSpeechEncoderDecoder',2022-03-08T17:31:44Z,2022-03-10T08:59:20Z,,,
15996,b'Update for doc-builder -> hf-doc-utils',2022-03-08T16:54:15Z,2022-03-08T17:30:55Z,,,
15995,b'Add FlaxBartForCausalLM',2022-03-08T16:45:54Z,2022-03-09T18:53:01Z,,,
15994,b'Adding new train_step logic to make things less confusing for users',2022-03-08T16:40:08Z,,,,
15993,b'Unclear message with `add-new-model-like` and no flax installed',2022-03-08T16:30:18Z,2022-03-08T16:34:54Z,,AttributeError,"AttributeError: module transformers.models.auto has no attribute modeling_flax_auto"
15992,b'Translation of documentation into Spanish',2022-03-08T16:05:06Z,2022-03-09T01:24:28Z,,,
15991,b'Add DPT',2022-03-08T14:56:41Z,,,,
15990,b'FeaturesManager assumes only one of Torch or TensorFlow is installed',2022-03-08T14:43:16Z,2022-03-14T15:47:29Z,,,
15989,b'Use tiny models for get_pretrained_model in TFEncoderDecoderModelTest',2022-03-08T14:25:07Z,2022-03-09T16:16:26Z,,,
15988,"b'[Docs] Improve PyTorch, Flax generate API'",2022-03-08T13:45:05Z,2022-03-10T10:54:46Z,,,
15987,b'add doctests for bart like seq2seq models',2022-03-08T13:22:49Z,2022-03-09T19:30:38Z,,,
15986,b'Swin support for any input size',2022-03-08T13:21:36Z,2022-03-16T17:38:25Z,,,
15985,b'Add the XTREME-S fine-tuning example',2022-03-08T12:29:59Z,2022-03-15T23:21:06Z,,,
15984,b'Add Document Image Transformer (DiT)',2022-03-08T12:17:24Z,2022-03-10T10:34:44Z,,,
15983,b'TFEncoderDecoderModel generate() gvies different results after #15562',2022-03-08T11:17:24Z,,,,
15982,b'Marian cannot be fully serialized because it accesses the filesystem after the object instantiation',2022-03-08T10:26:18Z,,,OSError,"OSError: Not found: ""/container_e165_1645611551581_313304_01_000001/tmp/model_dir/source.spm"": No such file or directory Error #2"
15981,b'[Env Command] Add hf hub to env version command',2022-03-08T10:10:43Z,2022-03-08T13:03:03Z,,,
15980,b'Bad error message when downloading private model without being logged in.',2022-03-08T10:06:05Z,,,OSError,"OSError: NewT5/dummy_model is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'"
15979,b'Fix TFEncDecModelTest - Pytorch device',2022-03-08T07:18:58Z,2022-03-08T12:37:20Z,,,
15978,b'Add custom classifcation dataset - computer vision',2022-03-08T05:48:36Z,2022-03-08T09:04:22Z,,,
15977,b'Add FlaxConvNext',2022-03-08T04:27:37Z,,,,
15976,b'Deadlock when loading the model in multiprocessing context',2022-03-08T01:50:06Z,,,,
15975,b'add special tokens does not work in GPT2Tokenizer',2022-03-08T01:17:08Z,2022-03-08T13:07:53Z,,,
15974,b'Models traced with HFTracer cannot be TorchScripted or serialized',2022-03-08T00:39:09Z,,,,
15973,b'error invoking create_optimizer from Jupyter lab',2022-03-08T00:13:54Z,,,ImportError,"ImportError: "
15972,b'Make `pos` optional in `PerceiverAudioPreprocessor` to avoid crashing `PerceiverModel` operation',2022-03-07T20:27:59Z,2022-03-09T14:48:53Z,,,
15971,"b""PerceiverAudioPreprocessor: forward() missing 1 required positional argument: 'pos'""",2022-03-07T20:19:08Z,,Good First Issue,,
15970,b'Unigram tokenizer Result is Incorrect',2022-03-07T19:06:49Z,,,,
15969,b'[Doctests] Move doctests to new GPU & Fix bugs',2022-03-07T15:55:06Z,2022-03-09T12:09:56Z,,,
15968,b'How to use GPT-2 for predicting the next word in batch',2022-03-07T15:16:25Z,2022-03-08T04:34:21Z,,,
15967,b'Fix broken code blocks in README.md',2022-03-07T13:48:02Z,2022-03-09T16:07:52Z,,,
15966,"b'EvalPrediction does not allow for ""sources"" parameter which ""sari"" metric requires'",2022-03-07T12:26:00Z,,,,
15965,b'remove re-defination of FlaxWav2Vec2ForCTCModule',2022-03-07T11:26:50Z,2022-03-07T13:58:44Z,,,
15964,b'Feature Extractor accepts `segmentation_maps`',2022-03-07T09:49:11Z,,,,
15963,b'Speedup T5 Flax training by using Numpy instead of JAX for batch shuffling',2022-03-07T08:26:23Z,2022-03-08T11:18:38Z,,,
15962,"b""longformer-large's hidden_states and last_hidden_states have different size in sequence_length""",2022-03-07T07:35:09Z,2022-03-11T10:23:35Z,,,
15961,"b""Seed _get_train_sampler's generator with arg seed to improve reproducibility""",2022-03-07T06:53:54Z,2022-03-08T18:45:41Z,,,
15960,"b""How to get T5 encoder&decoder's hidden states and keep requires_grad=True""",2022-03-07T06:45:36Z,2022-03-07T07:02:31Z,,,
15959,b'ValueError: DebertaV2Model does not support gradient checkpointing.',2022-03-07T06:33:53Z,,New model,ValueError,ValueError: DebertaV2Model does not support gradient checkpointing!
15958,b'Blenderbot 1.0B Distilled eats up memory over many inferences',2022-03-07T04:28:02Z,,,,
15957,b'mBART tokenizer not following expected target token order',2022-03-07T04:03:24Z,,,,
15956,"b""clean_up_tokenization_spaces=False does not behave correctly in AutoTokenizer's decode function """,2022-03-06T21:45:51Z,,,,
15955,b'Unable to run PPLM potato example',2022-03-06T18:48:36Z,,,AttributeError,"AttributeError: 'tuple' object has no attribute 'shape'"
15954,b'Make is_thing_map in Feature Extractor post_process_panoptic_segmentation defaults to all instances',2022-03-06T12:24:44Z,2022-03-07T18:10:33Z,,,
15953,b'add simple multi gpu complet',2022-03-05T22:01:30Z,2022-03-05T22:02:14Z,,,
15952,b'Set scale_embedding to False in some TF tests',2022-03-05T16:37:26Z,2022-03-07T21:14:33Z,,,
15951,b'Support modern list type hints in HfArgumentParser',2022-03-05T15:19:16Z,2022-03-07T15:22:48Z,,,
15950,"b""HfArgumentParser doesn't recognize new list type hint syntax""",2022-03-05T15:02:09Z,2022-03-07T15:22:48Z,,,
15949,b'[Tests] Fix ViTMAE integration test',2022-03-05T09:09:06Z,2022-03-08T09:49:45Z,,,
15948,b'is_index_masked and is_index_global_attn in Longformer',2022-03-05T07:36:28Z,2022-03-08T15:40:01Z,,,
15947,b'Tranformers documentation translation to Spanish',2022-03-05T06:03:55Z,,Documentation,,
15946,b'[HELP NEEDED] Use target tokenizer for all functionality in target side',2022-03-05T00:31:05Z,,,,
15945,b'Unable to run run_glue.py offline',2022-03-04T21:45:17Z,2022-03-04T21:57:25Z,,,
15944,b'TF generate refactor - past without encoder outputs',2022-03-04T20:29:21Z,2022-03-08T14:46:44Z,,,
15943,b'MarianTokenizer: get vocab length in `as_target_tokenizer` mode',2022-03-04T17:49:34Z,2022-03-05T00:31:08Z,,,
15942,b'Made MaskFormerModelTest faster',2022-03-04T17:23:42Z,2022-03-04T18:11:48Z,,,
15941,b'[LayoutLMv2] Update requires_backends of feature extractor',2022-03-04T17:12:28Z,2022-03-04T17:53:55Z,,,
15940,b'Override _pad in LEDTokenizer to deal with global_attention_mask',2022-03-04T15:54:34Z,2022-03-18T12:30:09Z,,ValueError,"ValueError: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length."
15939,b'Fix LayoutLMv2 test',2022-03-04T13:56:44Z,2022-03-08T09:49:30Z,,,
15938,b'Backprop Test for Freeze FlaxWav2Vec2 Feature Encoder',2022-03-04T12:48:55Z,2022-03-07T17:10:16Z,,,
15937,b'Add `ForInstanceSegmentation` models to `image-segmentation` pipelines',2022-03-04T11:24:15Z,2022-03-09T09:19:05Z,,,
15936,b'Returning outputs only when asked for for MaskFormer.',2022-03-04T10:34:08Z,2022-03-08T10:17:57Z,,,
15935,b'parm:logging_dir not works when ',2022-03-04T10:02:45Z,,,,
15934,b'Adding `MODEL_FOR_INSTANCE_SEGMENTATION_MAPPING`',2022-03-04T09:46:23Z,2022-03-04T12:56:15Z,,,
15933,"b""TypeError: meshgrid() got an unexpected keyword argument 'indexing'""",2022-03-04T06:51:03Z,2022-03-05T02:45:46Z,,,
15932,b'Update comments in class BatchEncoding',2022-03-04T03:11:06Z,,,,
15931,b'Update training scripts docs',2022-03-04T00:07:54Z,2022-03-07T19:29:14Z,Documentation,,
15930,b'issues in `generate()`',2022-03-03T21:07:27Z,,,,
15929,"b""Tests for MaskFormerFeatureExtractor's post_process*** methods""",2022-03-03T19:36:46Z,2022-03-04T17:04:19Z,,,
15928,b'Fix #15898',2022-03-03T19:10:49Z,2022-03-03T19:41:03Z,,,
15927,b'Privacy&Security: Network Contact Every Model Load',2022-03-03T17:58:10Z,,,,
15926,b'Update doc test readme',2022-03-03T17:55:15Z,2022-03-03T23:22:39Z,,,
15925,b'Non-unique `local` in toctree',2022-03-03T17:11:44Z,,,,
15924,b'Re-enabling all fast pipeline tests.',2022-03-03T16:54:26Z,2022-03-04T08:53:01Z,,,
15923,b'Large change to enable MaskFormerForInstanceSegmentation',2022-03-03T16:29:14Z,2022-03-08T10:59:55Z,,,
15922,b'Do a pull in case docs were updated during build',2022-03-03T15:31:16Z,2022-03-08T12:19:41Z,,,
15921,b'Simplify release utils',2022-03-03T15:26:57Z,2022-03-09T13:47:59Z,,,
15920,b'Fix Embedding Module Bug in Flax Models',2022-03-03T15:19:28Z,2022-03-07T17:17:45Z,,,
15919,b'camembert tokinizer ',2022-03-03T14:57:58Z,,,TypeError,TypeError: 'NoneType' object is not callable
15918,"b""Do not change the output from tuple to list - to match PT's version""",2022-03-03T14:39:53Z,2022-03-04T16:50:25Z,,,
15917,b'Enabling MaskFormer in pipelines',2022-03-03T14:16:08Z,2022-03-03T15:31:42Z,,,
15916,b'Minor fixes for MaskFormer',2022-03-03T14:11:45Z,2022-03-03T18:35:48Z,,,
15915,b'Maskformer',2022-03-03T14:04:19Z,2022-03-03T14:04:43Z,,,
15914,b'query() of generator `max_length` being succeeded',2022-03-03T13:58:22Z,2022-03-04T09:33:23Z,,,
15913,b'Support CLIPTokenizerFast for CLIPProcessor',2022-03-03T13:04:08Z,2022-03-04T10:57:10Z,,,
15912,b'[WIP]Resnet with variants',2022-03-03T12:59:50Z,,,,
15911,b'[Doctests] Fix ignore bug and add more doc tests',2022-03-03T12:44:51Z,2022-03-03T15:01:56Z,,,
15910,b'Flax XLM-RoBERTa',2022-03-03T12:09:35Z,2022-03-04T13:36:28Z,New model,,
15909,b'[Tests] Add attentions_option to ModelTesterMixin',2022-03-03T12:04:52Z,2022-03-10T11:00:30Z,,,
15908,b'RFC -- TF: unpack model inputs through a decorator',2022-03-03T11:34:50Z,2022-03-10T13:31:35Z,,,
15907,b'TF: Unpack model inputs through a decorator ',2022-03-03T10:49:53Z,2022-03-10T13:31:35Z,,,
15906,b'[Fix link in pipeline doc]',2022-03-03T10:00:41Z,2022-03-03T12:43:13Z,,,
15905,b'Add vision models to doc tests',2022-03-03T08:12:44Z,2022-03-03T18:46:31Z,,,
15904,b'Incomplete padding support for Funnel Transformer.',2022-03-03T08:10:02Z,,,,
15903,b'Fix doc links in release utils',2022-03-02T22:51:45Z,2022-03-02T23:06:31Z,,,
15902,b'[WIP] Add Fusion-in-Decoder',2022-03-02T19:50:46Z,,,,
15901,"b""tiny tweak to allow BatchEncoding.token_to_char when token doesn't correspond to chars""",2022-03-02T19:42:10Z,,,,
15900,b'Add missing support for Flax XLM-RoBERTa',2022-03-02T17:02:26Z,2022-03-04T13:36:28Z,,,
15899,b'Missing tags on Docker Hub after version 4.9.1',2022-03-02T16:54:44Z,,,,
15898,b'Error using evaluation in run_clm.py',2022-03-02T16:23:37Z,2022-03-03T19:41:03Z,,AttributeError,"AttributeError: 'tuple' object has no attribute 'argmax' "
15897,b'Update readme with how to train offline and fix BPE command',2022-03-02T16:20:12Z,,,,
15896,b'Fix a TF Vision Encoder Decoder test',2022-03-02T16:19:52Z,2022-03-03T12:21:31Z,,,
15895,b'Fix SegformerForImageClassification',2022-03-02T16:03:04Z,2022-03-02T20:57:39Z,,,
15894,b'testing github pr',2022-03-02T15:32:19Z,2022-03-02T16:26:23Z,,,
15893,b'Updating the slow tests:',2022-03-02T15:23:40Z,2022-03-04T11:32:20Z,,,
15892,b'[XGLM] run sampling test on CPU to be deterministic',2022-03-02T15:22:07Z,2022-03-02T16:55:49Z,,,
15891,b'Update delete-dev-doc job to match build-dev-doc',2022-03-02T15:21:07Z,2022-03-02T21:18:54Z,,,
15890,b'The tests were not updated after the addition of `torch.diag`',2022-03-02T15:13:13Z,2022-03-03T14:33:49Z,,,
15889,b'[SegFormer] Add deprecation warning',2022-03-02T14:25:42Z,2022-03-02T15:20:47Z,,,
15888,b'CLIPProcessor with CLIPTokenizerFast',2022-03-02T11:48:52Z,2022-03-04T10:57:10Z,,,
15887,b'Fix Bug in FlaxWav2Vec2 Slow Test',2022-03-02T10:41:04Z,2022-03-02T15:02:27Z,,,
15886,b'Update CLIPFeatureExtractor to convert PIL image to RGB',2022-03-02T10:09:54Z,,,ValueError,"ValueError: operands could not be broadcast together with shapes (4,224,224) (3,) `"
15885,b'After vocabulary extension the tokenizer keeps on running. ',2022-03-02T09:24:05Z,,,,
15884,b'Fix tiny typo in docs',2022-03-02T08:55:11Z,2022-03-02T14:37:05Z,,,
15883,b'Failed to find input with name: attention_mask in the model input def list',2022-03-02T05:53:36Z,2022-03-04T09:42:50Z,,RuntimeError,"RuntimeError: Failed to find input with name: attention_mask in the model input def list"
15882,b'Remove stash for now',2022-03-02T03:25:18Z,2022-03-02T03:36:19Z,,,
15881,b'fix deepspeed tests',2022-03-02T01:21:00Z,2022-03-02T03:27:28Z,,,
15880,b'Broken link error when using the CLIPTokenizerFast class',2022-03-01T23:16:05Z,2022-03-02T09:59:45Z,,"```requests.exceptions.HTTPError, requests.exceptions.HTTPError","```requests.exceptions.HTTPError: 502 Server Error: Bad Gateway for url: https://huggingface.co/openai/clip-vit-base-patch16/resolve/main/vocab.json``` when trying to use the CLIP tokenizer. requests.exceptions.HTTPError: 502 Server Error: Bad Gateway for url: https://huggingface.co/openai/clip-vit-base-patch16/resolve/main/vocab.json"
15879,b'[Bart] Fix implementation note doc',2022-03-01T22:56:35Z,2022-03-02T09:24:33Z,,,
15878,b'label_attention_mask in Bart for conditional sequence generation and other seq2seq models.',2022-03-01T20:39:19Z,,,,
15877,b'Updates in Trainer to support new features in SM Model Parallel library',2022-03-01T19:53:28Z,2022-03-02T12:55:14Z,,,
15876,b'Passing in num_labels to ConvNextForImageClassification.from_pretrained raises size mismatch error',2022-03-01T19:51:36Z,2022-03-01T20:02:33Z,,RuntimeError,"RuntimeError: Error(s) in loading state_dict for ConvNextForImageClassification:"
15875,b'Add ONNX support for Blenderbot and BlenderbotSmall',2022-03-01T17:15:26Z,,,,
15874,b'Bump up doc node version to 16',2022-03-01T17:02:58Z,2022-03-01T17:37:58Z,,,
15873,b'Freeze FlaxWav2Vec2 Feature Encoder',2022-03-01T16:54:48Z,2022-03-03T13:17:13Z,,,
15872,b'Scatter should run on CUDA',2022-03-01T16:40:29Z,2022-03-01T16:47:18Z,,,
15871,b'How to go about utilizing MBART for conditional generation with beam search in ONNXRuntime with TensorRT/CUDA',2022-03-01T16:31:08Z,,,,
15870,b'TF: Update QA example',2022-03-01T16:26:13Z,2022-03-02T10:38:14Z,,,
15869,b'Mismatch between beam search score transition probabilities and beam sequence scores',2022-03-01T16:20:06Z,,,,
15868,b'TF: Update multiple choice example',2022-03-01T15:15:14Z,2022-03-08T13:16:34Z,,,
15867,b'IndexError: index out of range in self',2022-03-01T14:50:33Z,2022-03-05T08:19:57Z,,IndexError,"IndexError: index out of range in self"
15866,b'Remove attention from padding',2022-03-01T12:37:56Z,2022-03-03T16:27:44Z,,,
15865,b'use python 3.7 for flax self-push tests',2022-03-01T11:10:15Z,2022-03-01T17:26:31Z,,,
15864,b'DebertaForSequenceClassification loss computation',2022-03-01T10:32:25Z,2022-03-01T15:01:32Z,,,
15863,b'Adding timestamps for CTC with LM in ASR pipeline.',2022-03-01T09:34:02Z,2022-03-02T09:49:05Z,,,
15862,b'Adding license file to some of the BERT models',2022-03-01T07:08:20Z,,,,
15861,b'Unable to run Speech2Text example in documentation',2022-03-01T06:34:49Z,2022-03-03T15:51:47Z,,TypeError,"TypeError: forward() got an unexpected keyword argument 'input_ids'"
15860,b'Add PT + TF automatic builds',2022-02-28T21:29:04Z,2022-03-01T13:55:11Z,,,
15859,b'set python version to 3.7 for flax tests',2022-02-28T20:15:01Z,2022-02-28T20:16:41Z,,,
15858,b'set python version to 3.7 for flax tests',2022-02-28T20:08:09Z,2022-02-28T20:16:51Z,,,
15857,b'Supporting multiple evaluation datasets in `Trainer` and `Seq2seqTrainer`',2022-02-28T18:50:19Z,,,,
15856,b'Fix (deprecated) ONNX exporter to account for new tf2onnx API',2022-02-28T17:13:05Z,2022-02-28T19:17:44Z,,,
15855,b'Update TF LM examples',2022-02-28T16:32:11Z,2022-03-01T14:12:58Z,,,
15854,b'Add time stamps for wav2vec2 with lm',2022-02-28T16:09:41Z,2022-03-01T16:03:06Z,,,
15853,b'Add TF benchmark examples',2022-02-28T14:57:00Z,2022-03-02T13:07:10Z,,,
15852,b'Add TF generate sample tests with all logit processors',2022-02-28T13:32:34Z,2022-03-02T09:48:12Z,,,
15851,b'[vision] Add problem_type support',2022-02-28T13:24:44Z,2022-03-01T17:09:53Z,,,
15850,"b""No such file or directory: '../datasets/RE/${DATA}/${SPLIT}/cached_train_BertTokenizer_128_sst-2.lock'""",2022-02-28T13:06:19Z,,,FileNotFoundError,"FileNotFoundError: [Errno 2] No such file or directory: '../datasets/RE/${DATA}/${SPLIT}/cached_train_BertTokenizer_128_sst-2.lock'"
15849,b'Rename semantic segmentation outputs',2022-02-28T10:53:27Z,,,,
15848,b'[Benchmark tools] Deprecate all',2022-02-28T10:23:54Z,2022-03-01T10:26:20Z,,,
15847,b'[UniSpeechSat] Revert previous incorrect change of slow tests',2022-02-28T09:26:20Z,2022-02-28T10:23:13Z,,,
15846,b'[TF-PT-Tests] Fix PyTorch - TF tests for different GPU devices',2022-02-28T09:09:23Z,2022-02-28T20:46:47Z,,,
15845,b'Decision transformer gym',2022-02-28T09:05:57Z,,,,
15844,"b'Error:""TypeError: \'NoneType\' object is not callable"" with model specific Tokenizers'",2022-02-28T08:09:33Z,,,"Error:""TypeError","Error:""TypeError: 'NoneType' object is not callable"" with Tokenizers"
15843,b'Fixing the timestamps with chunking.',2022-02-28T08:09:21Z,2022-02-28T20:00:22Z,,,
15842,b'Output embedding from each self-attention head from each encoder layer',2022-02-28T07:42:55Z,2022-02-28T10:40:38Z,,,
15841,b'Make Flax pt-flax equivalence test more aggressive',2022-02-27T12:05:32Z,2022-03-18T17:15:36Z,,,
15840,b'Timestamps in AutomaticSpeechRecognitionPipeline not aligned in sample space',2022-02-26T21:50:50Z,2022-02-28T20:00:22Z,,,
15839,b'Make TF pt-tf equivalence test more aggressive',2022-02-26T17:05:21Z,2022-03-14T12:31:32Z,,,
15838,b'Issue running run_glue.py at test time',2022-02-25T18:59:41Z,,,KeyError,"KeyError: 'Field ""sentence2"" does not exist in table schema'"
15837,b'Problems with convert_blenderbot_original_pytorch_checkpoint_to_pytorch.py ',2022-02-25T18:00:47Z,,,"KeyError, RuntimeError","KeyError: 'model.encoder.layernorm_embedding.weight'RuntimeError: Error(s) in loading state_dict for BartModel:"
15836,b'Inference for multilingual models',2022-02-25T17:52:34Z,2022-03-01T21:10:31Z,Documentation,,
15835,b'[FlaxT5 Example] Fix flax t5 example pretraining',2022-02-25T17:46:46Z,2022-03-04T16:04:43Z,,,
15834,b'Embedding index getting out of range while running onlplab/alephbert-base model',2022-02-25T16:40:48Z,,,IndexError,"IndexError: index out of range in self"
15833,b'[examples/summarization and translation] fix readme',2022-02-25T14:51:34Z,2022-02-25T16:28:16Z,,,
15832,b'Adding Decision Transformer Model (WIP)',2022-02-25T14:43:27Z,2022-02-28T09:03:11Z,,,
15831,b'support new marian models',2022-02-25T13:27:44Z,2022-03-10T18:41:57Z,,,
15830,b'Re-enable doctests for task_summary',2022-02-25T12:28:23Z,2022-02-25T16:14:01Z,,,
15829,"b'BART model parameters should not contain biases for Q, K and V'",2022-02-25T11:42:41Z,2022-02-25T14:55:54Z,,,
15828,b'Re-enable doctests for the quicktour',2022-02-25T10:12:49Z,2022-02-25T16:46:38Z,,,
15827,b'Absence',2022-02-25T09:41:51Z,,,,
15826,b'Fix semantic segmentation pipeline test',2022-02-25T07:51:20Z,2022-02-25T08:21:29Z,,,
15825,b'Framework split model report',2022-02-24T21:52:51Z,2022-02-25T17:00:00Z,,,
15824,b'HFTracer.trace should use self.graph to be compatible with torch.fx.Tracer',2022-02-24T19:33:00Z,2022-02-25T14:54:45Z,,,
15823,b'Fine tune TrOCR using bert-base-multilingual-cased',2022-02-24T18:55:12Z,2022-03-07T16:20:31Z,,,
15822,b'[TFXLNet] Correct tf xlnet generate',2022-02-24T18:08:47Z,2022-02-24T18:23:34Z,,,
15821,b'addeing tokens to tokenizer for training and inference worsens  prediction immensely',2022-02-24T16:54:01Z,2022-02-25T10:29:29Z,,,
15820,b'fix megatron bert convert state dict naming',2022-02-24T16:09:27Z,,,,
15819,b'How to enable multi-heads outputs in segformer?',2022-02-24T15:59:39Z,2022-03-15T19:54:17Z,,,
15818,b'[Unispeech] Fix slow tests',2022-02-24T12:57:16Z,2022-02-24T18:08:55Z,,,
15817,b'Add ONNX Runtime quantization for text classification notebook',2022-02-24T12:06:36Z,2022-02-25T16:29:36Z,,,
15816,b'Return entities extracted from the raw input for TokenClassificationPipeline ',2022-02-24T11:36:44Z,,,,
15815,b'[Barthez Tokenizer] Fix saving',2022-02-24T11:33:57Z,2022-02-24T18:09:10Z,,,
15814,b'Fix from_pretrained with default base_model_prefix',2022-02-24T07:54:41Z,2022-02-24T10:43:51Z,,,
15813,b'Add OFA to transformers',2022-02-24T07:42:42Z,,New model,,
15812,b'Evaluate on subset of evaluation dataset during training',2022-02-24T06:56:09Z,,,,
15811,b'update world_size and process_index for SMP ',2022-02-24T03:51:29Z,,,,
15810,b'repeat tokenization',2022-02-24T03:06:24Z,,,,
15809,b'Update distributed sampler for smp',2022-02-24T00:58:20Z,,,,
15808,b'Audio/vision task guides',2022-02-24T00:28:59Z,2022-03-11T22:43:50Z,Documentation,,
15807,"b'Fixes the ""push"" CI run'",2022-02-23T22:35:54Z,2022-02-24T18:30:17Z,,,
15806,b'Fix model templates',2022-02-23T20:59:18Z,2022-02-23T23:27:30Z,,,
15805,b'Fix add-new-model-like when old model checkpoint is not found',2022-02-23T20:58:00Z,2022-02-24T07:58:19Z,,,
15804,b'update dp_rank to rdp_rank for opt_state_dict',2022-02-23T20:38:55Z,,,,
15803,b'Fix build_documentation CI',2022-02-23T20:29:11Z,2022-02-23T20:53:52Z,,,
15802,b'Test if github action stops running when doc fails',2022-02-23T20:08:01Z,2022-02-23T20:27:00Z,,,
15801,b'run_glue_no_trainer.py',2022-02-23T19:36:39Z,,,,
15800,b'Fix docs on master',2022-02-23T19:33:44Z,2022-02-23T19:35:55Z,,,
15799,b'How to Use Transformers pipeline with multiple GPUs',2022-02-23T19:08:36Z,,,,
15798,b'Fix indent in doc-builder CI',2022-02-23T18:57:57Z,2022-02-23T19:01:33Z,,,
15797,b'Unable to load a pretrained model from disc using transformers api',2022-02-23T18:34:35Z,2022-02-24T09:28:49Z,,AttributeError,"AttributeError: 'BertModel' object has no attribute 'keys'"
15796,b'Create optimizer after model creation for SMP',2022-02-23T18:28:59Z,,,,
15795,b'Support PEP 563 for HfArgumentParser',2022-02-23T18:01:27Z,2022-03-17T17:51:38Z,,,
15794,b'Html docs 4.16.2',2022-02-23T16:30:32Z,2022-02-23T18:24:45Z,,,
15793,b'TF generate refactor - Sample',2022-02-23T16:08:48Z,2022-03-02T16:13:54Z,,,
15792,b'Adding the option to return_timestamps on pure CTC ASR models.',2022-02-23T15:29:43Z,2022-02-25T13:06:45Z,,,
15791,b'[ViLT] Add link to notebooks',2022-02-23T13:57:49Z,2022-03-01T16:44:20Z,,,
15790,b'[ViLT] Fix checkpoint url in config',2022-02-23T13:11:06Z,2022-02-23T13:51:41Z,,,
15789,b'[CLIP] fix gradient checkpointing',2022-02-23T12:59:15Z,2022-02-23T13:30:06Z,,,
15788,b'How can I create a longformer model without position embedding?',2022-02-23T12:38:52Z,2022-02-23T16:17:16Z,,,
15787,b'A question about BertForSequenceClassification',2022-02-23T12:35:07Z,2022-02-24T02:18:21Z,,,
15786,b'TF XLA greedy generation',2022-02-23T12:30:45Z,2022-03-15T13:19:20Z,,,
15785,b'Handling preprocessing with token-classification pipeline',2022-02-23T11:49:42Z,,,,
15784,b'Trainer is not so compatible with customized optimizer',2022-02-23T10:45:03Z,,,AssertionError,"AssertionError: Different devices for the bucket and the param, cannot proceed: cuda:2 - cpu"
15783,b'[Bug] Gradient Checkpointing In CLIP Model',2022-02-23T10:33:36Z,2022-02-23T13:30:06Z,,,
15782,b'Supporting Merges.txt files than contain an endline. (`hf-internal-testing/tiny-clip` for instance)',2022-02-23T10:31:32Z,2022-02-23T10:51:48Z,,,
15781,b'Add Semantic Segmentation for ConvNext',2022-02-23T08:32:54Z,,,,
15780,"b'how to save a model with additional layers, so it can be loaded using .from_pretrained()?'",2022-02-23T06:41:50Z,,,,
15779,b'LMV2Processor Decoder string not matching with original string',2022-02-23T06:40:54Z,,,,
15778,b'Tfperceiver',2022-02-23T06:12:37Z,,,,
15777,b'infer_framework_load_model function docs typo and grammar fixes',2022-02-23T01:55:41Z,,,,
15776,b'Fix dummy_inputs() to dummy_inputs in symbolic_trace doc string',2022-02-23T00:42:41Z,2022-02-25T10:32:23Z,,,
15775,b'Fix Issue 15003: SentencePiece Tokenizers Not Adding Special Tokens in `convert_tokens_to_string`',2022-02-22T23:10:24Z,,,,
15774,b'Fix tf.concatenate + test past_key_values for TF models',2022-02-22T20:44:46Z,2022-02-25T16:11:47Z,,,
15773,b'[WIP] Add fairseq FastSpeech2',2022-02-22T20:37:19Z,,,,
15772,b'Cuda error when using T5 with the new Apex FusedRMSNorm ',2022-02-22T17:33:21Z,,,RuntimeError,"RuntimeError: CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`"
15771,b'shift_tokens_right function missing for mt5 models',2022-02-22T16:23:32Z,,,AttributeError,"AttributeError: module 'transformers.models.mt5.modeling_flax_mt5' has no attribute 'shift_tokens_right'"
15770,b'Resnet',2022-02-22T15:05:08Z,2022-03-14T18:57:56Z,,,
15769,b'Docstring says target_sizes is optional (in DETR post_process) but the code disagrees',2022-02-22T15:02:33Z,,"Good First Issue, Good First Documentation Issue",,
15768,b'[doc] custom_models: mention security features of the Hub',2022-02-22T13:06:11Z,2022-02-23T16:40:06Z,,,
15767,b'Cleanup transformers-cli',2022-02-22T12:58:57Z,2022-02-22T20:58:05Z,,,
15766,b'[Discussion] Loading and initialising large models with Flax',2022-02-22T10:38:19Z,,,,
15765,b'do not support deepcopy when using deepspeed?',2022-02-22T10:23:18Z,2022-02-23T07:19:00Z,,TypeError,"TypeError: __init__() missing 1 required positional argument: 'parent_module'"
15764,b'Resnet weights should not be downloaded when building DETR by from_pretrained',2022-02-22T08:55:47Z,,,,
15763,b'Add TUNet',2022-02-22T06:51:17Z,,New model,,
15762,b'Simplify selection of indices in FlaxCLIPTextTransformer to support JAX model conversion tooling',2022-02-22T05:55:20Z,,,,
15761,b'Constrained Beam Search [*With* Disjunctive Decoding]',2022-02-22T04:44:34Z,2022-03-04T17:18:35Z,,,
15760,b'Save Finetuned XLM-RoBERTa on Tf-Keras',2022-02-22T04:22:13Z,2022-02-24T10:18:13Z,,,
15759,b'Add EfficientNet Model - PyTorch',2022-02-22T04:04:14Z,,New model,,
15758,b'Fix `HfArgumentParser` when passing a generator',2022-02-22T01:45:38Z,2022-02-22T23:16:39Z,,,
15757,b'Add support to export Blenderbot models to onnx',2022-02-21T15:06:02Z,,,```ValueError,"```ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds```"
15756,b'added link to our writing-doc document',2022-02-21T12:58:48Z,2022-02-22T08:57:28Z,,,
15755,b'TF train_step docstring',2022-02-21T12:53:35Z,2022-02-22T11:18:35Z,,,
15754,b'Diverging PT-Flax Wav2Vec2 Hidden-States',2022-02-21T12:51:46Z,2022-02-22T20:03:43Z,,,
15753,b'TF Train step docstring',2022-02-21T12:50:09Z,2022-02-21T12:50:41Z,,,
15752,b'[WiP] support new Marian models',2022-02-21T12:35:02Z,2022-02-25T13:26:51Z,,,
15751,"b'[M2M100, XGLM] fix create_position_ids_from_inputs_embeds'",2022-02-21T10:58:11Z,2022-02-23T09:46:42Z,,,
15750,b'Add TFConvNextModel',2022-02-21T10:44:01Z,2022-02-25T17:19:17Z,,,
15749,b'[tests] Improve forward_signature test',2022-02-21T10:25:25Z,,,,
15748,b'Fix segformer reshape last stage',2022-02-21T10:02:38Z,,WIP,,
15747,b'Add PreLN to fsmt module',2022-02-21T04:59:03Z,,,,
15746,b'Add model specific output classes to PoolFormer model docs',2022-02-21T04:55:37Z,2022-02-25T12:43:56Z,,,
15745,b'BertForSequenceClassification defines criterion at every forward pass',2022-02-20T21:14:07Z,2022-02-20T21:35:39Z,,,
15744,b'Problems loading csv dataset in examples `run_summarization.py`',2022-02-20T20:37:04Z,2022-02-21T10:58:27Z,,,
15743,b'changed documentation for Trainer.predict()',2022-02-20T18:30:10Z,,,,
15742,b'Documentation error in Trainer.predict when output_hidden_states is True',2022-02-20T18:18:24Z,,,,
15741,b'Fix undoing preprocessing step in summarization example',2022-02-20T14:33:03Z,2022-02-21T10:10:15Z,,,
15740,b'Fix minor comment typos',2022-02-20T14:26:17Z,2022-02-21T11:41:27Z,,,
15739,b'Add compatibility for Postponed Evaluation of Annotations (PEP 563)',2022-02-20T09:20:15Z,2022-03-17T17:51:38Z,Good First Issue,,
15738,b'Can I training an XLM model from scratch by transformers?',2022-02-20T05:02:11Z,,,,
15737,b'[WIP] Integrate OSLO for tensor parallelism support',2022-02-20T02:26:30Z,,,,
15736,b'`XGLMForCausalLM` does not compute `position_ids` correctly when using `inputs_embeds`',2022-02-20T00:39:17Z,2022-02-23T09:46:42Z,,,
15735,b'`DebertaTokenizer` always assigns token type ID 0',2022-02-19T19:42:09Z,,,,
15734,b'Self attention in T5 decoder does not work as expected',2022-02-19T18:01:41Z,,,,
15733,"b'TokenizerFast.from_file() is stuck when loading a large tokenizer.json with tokens added and ""pre-trained"" starting from an existing, trained model'",2022-02-19T17:51:28Z,,,,
15732,b'Deberta v2 code simplification',2022-02-19T09:15:42Z,2022-03-21T09:15:38Z,,,
15731,b'\xf0\x9f\xa7\xbc  NLP task guides',2022-02-18T22:42:25Z,2022-02-23T19:58:33Z,,,
15730,b'Add layer_idx to CrossAttention of GPT2 model',2022-02-18T21:57:28Z,2022-02-21T16:31:39Z,,TypeError,"TypeError: unsupported operand type(s) for +: 'NoneType' and 'int'"
15729,b'[Test refactor 5/5] Build docker images',2022-02-18T21:51:13Z,2022-02-23T20:48:19Z,,,
15728,b'[Test refactor 4/5] Improve the scheduled tests',2022-02-18T21:51:08Z,2022-02-23T20:48:05Z,,,
15727,b'[Test refactor 3/5] Notification service improvement',2022-02-18T21:51:04Z,2022-02-23T20:47:00Z,,,
15726,b'[Test refactor 2/5] Tests fetcher',2022-02-18T21:50:59Z,2022-02-23T20:46:38Z,,,
15725,b'[Test refactor 1/5] Per-folder tests reorganization',2022-02-18T21:50:51Z,2022-02-23T20:46:29Z,,,
15724,b'Need more understanding of the function: get_visual_embeddings(image_path)',2022-02-18T18:16:39Z,,,,
15723,b'[WIP] [doc] performance/scalability revamp',2022-02-18T17:36:35Z,,,,
15722,b'Revert changes in logit size for semantic segmentation models',2022-02-18T15:32:24Z,2022-02-24T14:52:52Z,,,
15721,b'Add missing PLBart entry in README',2022-02-18T15:24:24Z,2022-02-18T20:11:42Z,,,
15720,b'Drop support for Python 3.6',2022-02-18T14:10:49Z,,,,
15719,b'style_doc handles decorators in examples',2022-02-18T13:37:07Z,2022-02-18T13:49:54Z,,,
15718,b'Fix SiluActivation',2022-02-18T10:25:08Z,2022-02-18T10:57:39Z,,,
15717,b'revert temporary addition to test next version of CLIPTokenizerFast',2022-02-18T09:56:06Z,2022-02-21T17:30:11Z,,,
15716,b'Converting mBART to ONNX format',2022-02-18T09:34:15Z,2022-02-21T09:44:40Z,,,
15715,"b""Trainer can't estimate SpeechEncoderDecoder tokens for `floating_point_ops`""",2022-02-18T09:15:34Z,2022-02-22T06:08:32Z,,,
15714,b'Misplaced sentence in https://api-inference.huggingface.co/docs/curl/html/detailed_parameters.html#question-answering-task',2022-02-18T08:33:15Z,2022-02-18T14:37:20Z,,,
15713,b'add image2text generation',2022-02-18T06:14:00Z,,New model,,
15712,b'Error while finetuning XLM-R on Tensorflow-Keras',2022-02-18T05:54:32Z,,,**InvalidArgumentError,"**InvalidArgumentError: indices[2,268] = 124030 is not in [0, 50265) [[node tf_roberta_for_sequence_classification_1/roberta/embeddings/Gather (defined at /usr/local/lib/python3.7/dist-packages/transformers/models/roberta/modeling_tf_roberta.py:149) ]] [Op:__inference_train_function_82886]"
15711,b'Fix `HfDeepSpeedConfig` argument in `Trainer`',2022-02-18T03:55:13Z,2022-02-18T17:00:02Z,DeepSpeed,,
15710,b'No self-hosted',2022-02-18T00:55:16Z,2022-03-01T19:05:54Z,,,
15709,b'[WIP] No self-hosted',2022-02-18T00:38:43Z,2022-02-18T00:54:25Z,,,
15708,b'[WIP] No self-hosted',2022-02-18T00:31:05Z,2022-02-18T00:38:29Z,,,
15707,b'[WIP] No self-hosted',2022-02-18T00:21:00Z,2022-02-18T00:30:50Z,,,
15706,b'Fix auto model tests',2022-02-17T22:32:06Z,2022-02-18T13:50:23Z,,,
15705,b'AutoModelForSequenceClassification not learning if model is initialized inside a function scope',2022-02-17T22:21:08Z,2022-02-18T17:37:12Z,,,
15704,b'TF text classification examples',2022-02-17T22:07:55Z,2022-02-21T17:17:59Z,,,
15703,b'Fine-Tune DETR on custom dataset (less than 250 labels)',2022-02-17T17:01:56Z,,,,
15702,b'Fix DETR model deprecation warnings for int div',2022-02-17T16:46:40Z,2022-02-18T12:14:44Z,,,
15701,b'add VisionTextDualEncoder and CLIP fine-tuning script',2022-02-17T15:49:53Z,2022-02-21T15:10:59Z,,,
15700,b'confusion about past_key_values in GPT2 ',2022-02-17T14:54:04Z,2022-02-21T00:59:01Z,,,
15699,b'fix bug in PT speech-encoder-decoder',2022-02-17T14:43:45Z,2022-02-18T17:20:24Z,,,
15698,b'Image classification example fails',2022-02-17T14:42:31Z,2022-02-21T12:24:12Z,,datasets.utils.info_utils.NonMatchingSplitsSizesError,"datasets.utils.info_utils.NonMatchingSplitsSizesError: [{'expected': SplitInfo(name='train', num_bytes=7503250, num_examples=23422, dataset_name='cats_vs_dogs'), 'recorded': SplitInfo(name='train', num_bytes=7871070, num_examples=23410, dataset_name='cats_vs_dogs')}]"
15697,b'Fix docs for decoder_input_ids in BART.',2022-02-17T14:03:28Z,,,,
15696,b'Fix shape',2022-02-17T11:17:07Z,2022-02-17T13:42:15Z,,,
15695,b'IndexError while applying stride for ASR',2022-02-17T10:54:33Z,,,IndexError,"IndexError: index 83 is out of bounds for dimension 1 with size 83"
15694,b'Tokenizer offset_mapping problem',2022-02-17T10:24:43Z,,,,
15693,"b'Incorrect information in ""Getting started"" regarding API tokens'",2022-02-17T10:13:01Z,2022-02-17T14:42:05Z,,,
15692,b'Typo in https://api-inference.huggingface.co/docs/curl/html/detailed_parameters.html#summarization-task',2022-02-17T10:07:57Z,2022-02-18T14:36:35Z,,,
15691,b'About `decoder_input_ids` in BART doc',2022-02-17T09:34:34Z,,,,
15690,"b'Adding a model, more doc for pushing to the hub'",2022-02-17T07:49:56Z,2022-02-18T08:11:19Z,,,
15689,b'VisionEncoderDecoder Error during training',2022-02-17T05:10:15Z,2022-02-18T02:44:03Z,,TypeError,"TypeError: forward() got an unexpected keyword argument 'attention_mask'"
15688,b'Minor fix on README.md',2022-02-16T20:40:52Z,2022-02-17T13:38:32Z,,,
15687,b'Time stamps for CTC models',2022-02-16T17:41:13Z,2022-02-22T18:26:44Z,,,
15686,b'Fix Funnel configuration doc',2022-02-16T16:27:06Z,2022-02-16T16:50:37Z,,,
15685,b'Fix Funnel configuration doc',2022-02-16T16:07:04Z,2022-02-16T16:21:19Z,,,
15684,b'Add initializer_std to TFFunnelModelTester with a default value 0.02',2022-02-16T15:40:37Z,2022-02-18T11:20:07Z,,,
15683,b'[Wav2Vec2ProcessorWithLM] Fix auto processor with lm',2022-02-16T15:15:00Z,2022-02-16T16:33:34Z,,,
15682,b'Maskformer',2022-02-16T12:51:21Z,2022-03-02T14:48:20Z,,,
15681,b'Fix prepare_for_model error inconsistency',2022-02-16T12:49:42Z,,,,
15680,b'\xf0\x9f\x94\xa5 Remove build_doc_test github action',2022-02-16T12:37:26Z,2022-02-16T13:06:26Z,,,
15679,b'Tokenizer prepare_for_model Error inconsistency',2022-02-16T12:11:41Z,,,"TypeError, ValueError","TypeError: unsupported operand type(s) for +: 'int' and 'list'ValueError: Asking to return token_type_ids while setting add_special_tokens to False results in an undefined behavior. Please set add_special_tokens to True or set return_token_type_ids to None."
15678,b'HTML dev docs',2022-02-16T10:32:01Z,2022-02-23T18:43:22Z,,,
15677,b'one of the variables needed for gradient computation has been modified by an inplace operation',2022-02-16T09:49:21Z,,,RuntimeError,"RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [287, 768]], which is output 0 of TanhBackward, is at version 2; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True)."
15676,b'Gelu10',2022-02-16T09:36:19Z,2022-02-22T17:21:16Z,,,
15675,"b'How can I use ""accelerate launch"" command to run  training job on Multi-GPU?'",2022-02-16T09:30:53Z,,,,
15674,b'model.generate() using a user specified keyword argument',2022-02-16T07:38:01Z,,,,
15673,b'DebertaForMaskedLM cannot load the parameters in the MLM head',2022-02-16T07:14:46Z,,,,
15672,"b'Unable to generate chunks (If length is greater than 512 in bert), we can use to split into chunks'",2022-02-16T06:52:40Z,,,ValueError,"ValueError: Arguments can't be understood"
15671,b'Fix vit test',2022-02-15T21:03:51Z,2022-02-15T23:55:38Z,,,
15670,b'Fix model equivalence tests',2022-02-15T20:51:50Z,2022-02-15T23:55:22Z,,,
15669,b'Add register method to AutoProcessor',2022-02-15T19:53:05Z,2022-02-16T14:13:33Z,,,
15668,b'Add push_to_hub method to processors',2022-02-15T19:28:09Z,2022-02-16T02:14:04Z,,,
15667,b'Add image classification notebook',2022-02-15T19:27:22Z,2022-02-17T12:14:01Z,,,
15666,b'Add Video Vision Transformer',2022-02-15T18:58:35Z,,New model,,
15665,b'Fix dec_attn_mask in TFTransfoXLMainLayer',2022-02-15T18:50:07Z,2022-02-16T11:53:26Z,,,
15664,b'Why are certain models with a higher WER (on the eval set) performing better or as good as  models with a lower WER \xe2\x80\x93 when tested on the test set?',2022-02-15T18:48:33Z,2022-02-24T08:59:42Z,,,
15663,b'\xf0\x9f\xa4\x97 Transformers **Trainer** API raises exception on train if triggered from an already started ML Flow run.',2022-02-15T18:14:36Z,2022-03-17T20:39:57Z,,Exception,"Exception: Run with UUID fad5d86248564973ababb1627466c0cb is already active. To start a new run, first end the current run with mlflow.end_run(). To start a nested run, call start_run with nested=True"
15662,"b""cannot import name 'CONFIG_MAPPING' from 'transformers' (unknown location)""",2022-02-15T17:27:33Z,2022-02-17T11:04:39Z,,,
15661,b'GPT-2 pretrained model fails to load when TF v2 behaviour is disabled',2022-02-15T16:49:05Z,,,ValueError,"ValueError: in user code:"
15660,b'[pipeline doc] fix api',2022-02-15T16:46:57Z,2022-02-15T18:13:08Z,,,
15659,b'Add section about doc testing',2022-02-15T13:51:24Z,2022-02-15T15:56:31Z,,,
15658,b'Add ONNX export for ViT',2022-02-15T12:42:25Z,2022-03-09T16:37:00Z,,,
15657,b'Usage examples for logger',2022-02-15T10:14:42Z,2022-02-16T09:15:14Z,,,
15656,b'Is it fine if we do not pass the optimizer through accelerator.prepare() in DDP?',2022-02-15T10:02:03Z,2022-02-21T23:19:05Z,,,
15655,b'[SpeechEncoderDecoder] Make sure no EOS is generated in test',2022-02-15T07:59:47Z,2022-02-15T08:13:56Z,,,
15654,b'LayoutLMv2Model can not be imported from transformers',2022-02-15T04:15:07Z,2022-02-15T05:42:43Z,,`ImportError,"`ImportError: cannot import name 'LayoutLMv2Model' from transformers (unknown location)`"
15653,b'Updated the RAG training with latest Pytorch Lightning library and the RAY',2022-02-15T04:06:36Z,2022-02-15T15:53:05Z,,,
15652,b'add a network debug script and document it',2022-02-15T03:51:03Z,2022-02-15T16:48:01Z,,,
15651,b'Add a missing space in a deprecation message',2022-02-15T03:50:36Z,2022-02-16T00:12:31Z,,,
15650,"b'Inference API with GPT2 {""error"": ""Unknown error""}'",2022-02-14T23:55:19Z,,,,
15649,b'Allow custom code for Processors',2022-02-14T20:29:28Z,2022-02-15T14:44:35Z,,,
15648,b'TrOCR not working anymore after 4.16.2 update',2022-02-14T19:07:38Z,,,,
15647,b'Enable `image-segmentation` on `AutoModelForSemanticSegmentation`',2022-02-14T17:47:50Z,2022-02-23T16:20:27Z,,,
15646,b'Add `decoder_kwargs` to send to LM on asr pipeline.',2022-02-14T14:24:21Z,2022-02-15T16:53:24Z,,,
15645,b'Re-export `KeyDataset`.',2022-02-14T11:47:37Z,2022-02-15T16:49:39Z,,,
15644,b'Fix typo on examples/pytorch/question-answering',2022-02-14T02:49:30Z,2022-02-22T18:51:07Z,,,
15643,"b""Fix TFSequenceSummary's activation""",2022-02-13T19:02:11Z,2022-02-15T19:15:42Z,,,
15642,b'GPT-NeoX-20B Integration',2022-02-13T17:29:43Z,,,,
15641,b'Update bad_words_ids usage',2022-02-13T13:38:21Z,2022-02-15T15:44:35Z,,,
15640,b'Add support for ONNX-TensorRT conversion for GPT-J6B (and possible bug in rotary embedding)',2022-02-13T13:04:20Z,,,,
15639,b'Pooled results for DistilBert',2022-02-13T05:31:07Z,2022-02-15T14:04:17Z,,,
15638,b'fix bug for the log of RNG states are not properly loaded lead to exception.',2022-02-12T15:36:50Z,2022-02-15T01:30:55Z,,,
15637,b'fix bug for the log of  RNG states are not properly loaded  exception.',2022-02-12T15:23:32Z,2022-02-12T15:27:54Z,,,
15636,b'Report only the failed imports in `requires_backends`',2022-02-12T10:37:47Z,2022-02-14T15:35:20Z,,,
15635,b'Loading a fairseq trained wav2vec2 model with transformers',2022-02-12T03:01:02Z,,,UnicodeDecodeError,"UnicodeDecodeError: 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte"
15634,b'Register feature extractor',2022-02-11T22:19:31Z,2022-02-14T18:35:16Z,,,
15633,b'Register feature extractor',2022-02-11T21:03:53Z,2022-02-11T22:18:56Z,,,
15632,b'Add push to hub to feature extractor',2022-02-11T20:00:16Z,2022-02-11T22:14:01Z,,,
15631,b'Remove redundant error logging in from_pretrained() method',2022-02-11T19:59:16Z,2022-02-14T17:03:07Z,,,
15630,b'Custom feature extractor',2022-02-11T18:59:30Z,2022-02-11T21:43:55Z,,,
15629,b'Fix _configuration_file argument getting passed to model',2022-02-11T17:15:20Z,2022-02-11T18:46:09Z,,,
15628,b'make style produces a lot of reformatted lines',2022-02-11T16:58:25Z,2022-02-11T17:05:36Z,,,
15627,b'unable to pass create vocabulary_from_data when using concatenate_datasets() method to train a combined dataset model ',2022-02-11T16:54:57Z,2022-02-12T13:08:06Z,,,
15626,b'[Fix doc example] FlaxVisionEncoderDecoder',2022-02-11T16:34:16Z,2022-02-14T11:48:23Z,,,
15625,b'Enable ONNX export when PyTorch and TensorFlow installed in the same env',2022-02-11T14:50:33Z,2022-02-11T15:25:07Z,,,
15624,"b'Mark ""code in the Hub"" API as experimental'",2022-02-11T14:39:09Z,2022-02-11T14:55:31Z,,,
15623,b'Add TF implementation of GPT-J',2022-02-11T14:29:14Z,,,,
15622,b'Add support for bitsandbytes',2022-02-11T13:01:13Z,,WIP,,
15621,b'TPU slow finetuning T5-base',2022-02-11T12:00:05Z,,,,
15620,b'[RFC] Add framework argument to ONNX export',2022-02-11T08:50:41Z,2022-02-11T15:25:07Z,,"transformers.file_utils.EntryNotFoundError, OSError","transformers.file_utils.EntryNotFoundError: 404 Client Error: Entry Not Found for url: https://huggingface.co/keras-io/transformers-qa/resolve/main/pytorch_model.binOSError: keras-io/transformers-qa does not appear to have a file named pytorch_model.bin but there is a file for TensorFlow weights. Use `from_tf=True` to load this model from those weights."
15619,b'Why is my train_samples_per_second graph growing?',2022-02-11T08:10:35Z,2022-03-21T15:06:34Z,,,
15618,b'DDP training hangs with `run_glue.py` and `run_seq2seq.py`',2022-02-11T03:49:14Z,2022-03-21T15:06:35Z,,,
15617,b'Fix typo in speech2text2 doc',2022-02-11T02:47:19Z,2022-02-15T12:54:35Z,,,
15616,b'Implementation of activations as pytorch modules',2022-02-11T01:37:54Z,2022-02-16T19:37:52Z,,,
15615,b'Fix broken link in CTRL docs',2022-02-11T00:02:38Z,2022-02-11T18:33:55Z,,,
15614,b'Fix grammar in tokenizer_summary docs',2022-02-10T22:30:06Z,2022-02-11T21:51:30Z,,,
15613,b'Flax Speech-Encoder-Decoder Model',2022-02-10T16:55:21Z,2022-02-28T11:22:36Z,,,
15612,b'TF: Add informative warning for inexistent CPU backprop ops',2022-02-10T16:54:38Z,2022-02-11T16:16:27Z,,,
15611,b'Small clean up generate',2022-02-10T16:47:25Z,2022-02-10T17:29:27Z,,,
15610,"b""T5 AttributeError: 'T5Encoder' object has no attribute 'main_input_name'""",2022-02-10T16:46:18Z,,,AttributeError,"AttributeError: 'T5Encoder' object has no attribute 'main_input_name'"
15609,b'Documentation of DataCollatorForLanguageModeling',2022-02-10T16:43:05Z,,,,
15608,b'\xe2\x9d\x93 T5 pre-training dataset',2022-02-10T16:31:41Z,,,,
15607,b'fixed pipeline code',2022-02-10T15:16:32Z,2022-02-22T18:46:22Z,,,
15606,b'Add aws studio notebooks',2022-02-10T15:10:58Z,2022-02-11T17:02:02Z,,,
15605,b'Auto tokenizer for question-context tokenization',2022-02-10T14:52:32Z,,,,
15604,b'Add local and TensorFlow ONNX export examples to docs',2022-02-10T14:41:27Z,2022-02-10T15:31:00Z,,,
15603,b'Fix Seq2SeqTrainer for VisionEncoderDecoderModel',2022-02-10T14:32:20Z,2022-02-10T15:26:15Z,,TypeError,"TypeError: forward() got an unexpected keyword argument 'attention_mask'"
15602,b'Fix deepspeed-zero-inference hashlink',2022-02-10T14:20:38Z,2022-02-10T16:24:04Z,,,
15601,"b""Rm unneeded `<a =''>`""",2022-02-10T10:58:19Z,2022-02-10T14:16:31Z,,,
15600,b'[deepspeed docs] Correct JSON format',2022-02-10T10:51:20Z,2022-02-10T17:02:03Z,,,
15599,b'Run ViT examples on PyTorch XLA out of the box',2022-02-10T10:08:17Z,,,,
15598,b'gpt2 error using torch.jit.trace ',2022-02-10T09:53:41Z,,,RuntimeError,"RuntimeError: Tracer cannot infer type of CausalLMOutputWithCrossAttentions(loss=None, logits=tensor([[[  1.6039, -14.8921, -12.7946,  ..., -12.5804, -13.8293, -13.4610],"
15597,b'How to implement generate function for seperate encoder decoder T5 model?',2022-02-10T08:48:14Z,2022-03-20T15:01:52Z,,,
15596,b'Add example batch size to all commands',2022-02-10T07:53:26Z,2022-02-10T13:52:07Z,,,
15594,b'[research_projects] deal with security alerts',2022-02-10T04:39:16Z,2022-02-11T19:31:09Z,,,
15593,b'Fix bugs of s2t fairseq model converting',2022-02-10T02:20:28Z,,,,
15592,b'Update TAPAS script to convert TAPAS-NQ weights (re-attempt #11907)',2022-02-10T02:13:37Z,,,,
15591,b'T5 encoder past_key_values',2022-02-10T00:58:24Z,,,,
15590,b'Fix ASR pipelines from local directories with wav2vec models that have language models attached',2022-02-09T23:40:07Z,2022-02-15T12:45:08Z,,,
15589,"b""ASR pipelines won't load local Wav2Vec models with language models attached""",2022-02-09T23:31:57Z,2022-02-15T12:45:08Z,,requests.exceptions.HTTPError,"requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/api/models//revision/main "
15588,b'[ViTMAE] Add link to script',2022-02-09T21:52:27Z,2022-02-09T22:33:40Z,,,
15587,b'Expand tutorial for custom models',2022-02-09T21:30:17Z,2022-02-09T22:44:29Z,,,
15586,b'Add SimMIM',2022-02-09T20:55:13Z,2022-02-17T18:44:55Z,,,
15595,b'Dynamic padding did not work as expected for custom audio dataset',2022-02-09T19:57:10Z,2022-03-20T15:01:55Z,bug,TypeError,"TypeError: '<' not supported between instances of 'NoneType' and 'int'"
15585,b'[deepspeed docs] misc additions',2022-02-09T19:55:35Z,2022-02-11T18:54:04Z,,,
15584,b'[Flax] abstract init model',2022-02-09T18:04:53Z,2022-03-20T15:01:56Z,,,
15583,b'Add TF implementation of GPT-J model',2022-02-09T17:16:32Z,,,,
15582,b'train_file and validation_file in jsonl format with examples/pytorch/language-modeling/run_clm.py  ',2022-02-09T17:15:49Z,2022-03-20T15:01:57Z,,,
15581,b'Flax model much slower than Pytorch',2022-02-09T16:47:14Z,2022-02-11T08:00:30Z,,Notebook,"Notebook: https://colab.research.google.com/drive/18ouY4S9rYtlEC1dht_w8WU4fFKnP9yhP#scrollTo=FLfV3ymlwRJC"
15580,b'Fix tests hub failure',2022-02-09T15:57:05Z,2022-02-09T17:27:59Z,,,
15579,b'Click new version',2022-02-09T15:27:25Z,2022-02-09T15:28:43Z,,,
15578,b'Documentation error ',2022-02-09T14:44:40Z,2022-03-11T15:48:03Z,,ImportError,"ImportError: cannot import name 'KeyDataset' from 'transformers.pipelines.base' (/usr/local/lib/python3.7/dist-packages/transformers/pipelines/base.py)"
15577,b'Tokenizers dictionaries not being saved to cache.',2022-02-09T12:16:49Z,2022-02-10T14:47:47Z,,`requests.exceptions.HTTPError,"`requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/api/models/openai/CLIP/ViT-B/32`"
15576,b'i cannot connect the hf webset',2022-02-09T12:06:18Z,2022-03-20T15:01:58Z,,,
15575,b'Inference using ONNX model exported by transformers.onnx raises [ONNXRuntimeError] INVALID_ARGUMENT : Unexpected input data type',2022-02-09T09:59:04Z,2022-02-10T14:06:16Z,,,
15574,b'PyTorch Cuda Tensors not supported in DETR model',2022-02-09T09:53:46Z,2022-03-20T15:01:59Z,,,
15573,b'Transformers Pipeline Error',2022-02-09T07:34:54Z,2022-03-11T16:05:05Z,,,
15572,b'logger.warn --> logger.warning',2022-02-09T05:51:58Z,2022-02-09T13:20:06Z,,,
15571,b'[Flax tests] fix test_model_outputs_equivalence',2022-02-08T22:06:45Z,2022-02-09T11:26:48Z,,,
15570,b'm2m100 model do not support eval / predict on deepspeed + fp16 enviroment?',2022-02-08T21:47:53Z,2022-02-10T21:23:22Z,,,
15569,b'Make sure custom configs work with Transformers',2022-02-08T19:45:55Z,2022-02-09T15:04:45Z,,,
15568,b'update serving_output for some TF models',2022-02-08T19:31:39Z,2022-02-09T17:32:51Z,,,
15567,b'TF MT5 embeddings resize',2022-02-08T19:27:17Z,2022-02-11T17:35:11Z,,,
15566,b'Add Wav2Vec2 Adapter Weights to Flax',2022-02-08T18:48:29Z,2022-02-09T15:24:41Z,,,
15565,b'Upgrade black to version ~=22.0',2022-02-08T18:43:01Z,2022-02-09T14:28:58Z,,,
15564,b'How to fine-tune NLP tasks docs',2022-02-08T18:23:22Z,2022-02-18T22:28:27Z,Documentation,,
15563,b'Add codecarbon callback to docs',2022-02-08T17:22:54Z,2022-02-08T19:10:53Z,,,
15562,b'TF generate refactor - Greedy Search',2022-02-08T17:22:28Z,2022-02-15T16:54:43Z,,,
15561,b'[Flax tests/FlaxBert] make from_pretrained test faster',2022-02-08T17:12:01Z,2022-02-09T15:48:09Z,,,
15560,b'Output ragged tensors from TF predict()',2022-02-08T16:24:38Z,2022-02-16T14:04:37Z,,,
15559,b'BART Large generate predictions are wonky',2022-02-08T15:23:50Z,,,,
15558,b'[GPTJ] fix docs',2022-02-08T14:25:10Z,2022-02-08T14:54:20Z,,,
15557,b'unable to pass through create vocabulary_from_data when using a custom dataset loaded from disk which is a result of concatenating multiple datasets',2022-02-08T13:50:26Z,,,,
15556,b'Unable to ignore pad tokens when using `decoder_input_ids` and `decoder_attention_mask` with `BartForConditionalGeneration`',2022-02-08T08:49:58Z,,,"Sidenote, RuntimeError","Sidenote: My actual task is not to replicate the default output when using pad tokens in `decoder_input_ids` just for the sake of using pad tokens but requires resolving this issue.RuntimeError: The size of tensor a (3) must match the size of tensor b (4) at non-singleton dimension 3"
15555,b'[Bug Fix] Beam search example in docs fails & a fix (integrating `max_length` in `BeamScorer.finalize()`)',2022-02-08T08:29:57Z,2022-03-07T08:10:18Z,,,
15554,b'feat(flax): allow encoder_outputs in generate',2022-02-08T03:47:26Z,2022-02-08T16:53:23Z,,,
15553,b'DataCollatorWithPadding that pads attention_mask',2022-02-08T00:08:42Z,2022-02-08T16:12:34Z,,,
15552,b'Improved documentation/blog post around `generate()`',2022-02-07T23:40:55Z,,,,
15551,b'[trainer docs] document how to select specific gpus',2022-02-07T22:08:05Z,2022-02-09T18:12:29Z,,,
15550,b'DeepMind Retro',2022-02-07T20:46:59Z,,New model,,
15549,b'PoC for a ProcessorMixin class',2022-02-07T20:28:47Z,2022-02-09T14:24:49Z,,,
15548,b'Fine-tune blenderbot 2.0 to retrieve from my own documents instead internet',2022-02-07T20:26:01Z,2022-03-19T15:02:02Z,,,
15547,b'Model card reports wrong evaluation metrics when load_best_model_at_end=True',2022-02-07T17:11:58Z,2022-03-19T15:02:02Z,,,
15546,b'gptj doc tied weights mistake?',2022-02-07T17:02:34Z,2022-02-08T14:54:19Z,,,
15545,b'Not able to load CharacterBERT',2022-02-07T15:43:48Z,2022-02-09T13:08:02Z,,KeyError,"KeyError: 'character_bert'"
15544,b'[SpeechRecognition Seq2Seq] CUDA out of memory when training on GPU',2022-02-07T15:36:34Z,2022-02-08T15:25:42Z,,`RuntimeError,"`RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 15.78 GiB total capacity; 13.46 GiB already allocated; 5.25 MiB free; 13.93 GiB reserved in total by PyTorch)`"
15543,b'Move generic PyTorch utils function from modeling_utils.py to pytorch_utils',2022-02-07T15:03:51Z,,Good First Issue,,
15542,b'Save DistilBert Model and Convert',2022-02-07T14:19:18Z,2022-02-14T09:23:23Z,,ValueError,"ValueError: Expected an object of type `Trackable`, such as `tf.Module` or a subclass of the `Trackable` class, for export. Got DistilBertForTokenClassification("
15541,b'[ASR pipeline] correct asr pipeline for seq2seq models',2022-02-07T12:24:45Z,2022-02-07T14:35:44Z,,,
15540,"b'Revert ""Handle PyTorch to Flax conversion of 1D convolutions""'",2022-02-07T10:30:17Z,2022-02-07T11:33:49Z,,,
15539,b'[Trainer] Deeper length checks for IterableDatasetShard',2022-02-07T09:56:37Z,2022-02-07T15:34:56Z,,,
15538,b'Keep waiting for push command to finish at the end of running run_speech_recognition_ctc.py ',2022-02-07T06:39:58Z,2022-03-18T15:06:13Z,,Notes,"Notes:  You may change `--num_train_epochs=""100""` to shorter runn as the behaviour still persist."
15537,b'Fix LongformerModel hidden states',2022-02-06T13:38:08Z,2022-02-18T12:56:53Z,,,
15536,b'Error when passing encoder_outputs as tuple to EncoderDecoder models',2022-02-06T11:29:30Z,,,AttributeError,"AttributeError: 'tuple' object has no attribute 'last_hidden_state'"
15535,b'vits support? ',2022-02-06T07:57:52Z,,New model,,
15534,"b""Cannot set deterministic=False for FlaxRobertaPreTrainedModel, therefore dropout doesn't work?""",2022-02-06T00:23:43Z,,,,
15533,b'Porting Compressive Transformer to Huggingface',2022-02-05T16:40:20Z,,New model,,
15532,b'Error converting fine-tuned GPTNeoForCausalLM model to ONNX',2022-02-05T13:41:39Z,2022-03-16T15:07:12Z,,"ValueError, IndexError","ValueError: The type of axis index is expected to be an integerIndexError: dimension specified as -2 but tensor has no dimensions"
15531,b'Add PoolFormer',2022-02-05T13:22:18Z,2022-02-17T12:16:37Z,,,
15530,"b""Make TF Wav2Vec2 outputs the same as PT's version""",2022-02-05T08:41:34Z,2022-02-07T17:09:57Z,,,
15529,b'feat: add debertav2 fast tokenizer',2022-02-05T08:00:51Z,,,,
15528,b'How to use pipeline with Pytorch framework on Windows?',2022-02-05T07:52:54Z,2022-03-16T15:07:13Z,,,
15527,b'Make Swin work with VisionEncoderDecoderModel',2022-02-05T07:32:56Z,2022-02-14T16:33:35Z,,,
15526,b'SwinTransformer as encoder and Bart as decoder',2022-02-04T23:36:06Z,2022-02-10T15:26:15Z,,AttributeError,"AttributeError: 'SwinConfig' object has no attribute 'hidden_size'"
15525,b'Wav2Vec2 for long audio with N-gram Language model',2022-02-04T22:07:52Z,,,,
15524,b'Unable to Import KeyDataset for Pipeline Iterator in 4.16.2',2022-02-04T20:33:53Z,2022-02-15T16:49:39Z,,ImportError,"ImportError: cannot import name 'KeyDataset' from 'transformers.pipelines.base'"
15523,b'Various issues with  accelerate launch command for Large example',2022-02-04T19:26:14Z,,,,
15522,b'Getting Error  ',2022-02-04T18:31:04Z,2022-02-04T21:33:34Z,,,
15521,b'Add Wav2Vec2 Adapter Weights to Flax',2022-02-04T17:36:51Z,2022-02-08T17:23:08Z,,,
15520,b'Warn if using a CTC+LM model with no decoder (asr pipeline)',2022-02-04T16:54:00Z,2022-02-16T12:18:04Z,,,
15519,b'Handle PyTorch to Flax conversion of 1D convolutions',2022-02-04T15:17:22Z,2022-02-04T16:08:03Z,,,
15518,b'How to load bert_base-augmented-batch_size=128-lr=2e-5-max_gloss=6 model in offline mode in jupyter nb',2022-02-04T14:00:53Z,2022-02-04T15:19:01Z,,HTTPError,"HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/api/models//bert_base-augmented-batch_size=128-lr=2e-5-max_gloss=6"
15517,b'* How to convert pytorch.bin to  *.ckpt files',2022-02-04T13:00:21Z,2022-02-10T19:36:10Z,Migration,,
15516,b'T5Tokenizer loses most special tokens after I add a new special token.',2022-02-04T12:50:07Z,2022-03-09T03:13:08Z,,,
15515,b'Add attention_scores as output in RoBERTa',2022-02-04T11:12:02Z,2022-02-09T09:26:16Z,,,
15514,b'Download models from a private hub',2022-02-04T10:58:13Z,2022-03-14T17:26:24Z,"Feature request, private-hub",,
15513,b'How can I see the masked words during pre-learning by MLM?',2022-02-04T10:36:36Z,2022-02-04T21:48:44Z,,,
15512,b'Benchmarking with T5 or T5-small fails',2022-02-04T09:05:07Z,,,ValueError,ValueError: too many values to unpack (expected 2)
15511,b'Fix TF T5/LED missing cross attn in retrun values',2022-02-04T07:17:00Z,2022-02-07T16:41:49Z,,,
15510,b'Fix TFRemBertEncoder all_hidden_states',2022-02-04T06:45:32Z,2022-02-04T16:32:14Z,,,
15509,b'Fix TFElectraForMultipleChoice',2022-02-04T06:13:45Z,2022-02-04T15:25:37Z,,,
15508,"b'TrainingArguments --learning_rate should not be used to set both ""lr"" and ""warmup_max_lr"" in DeepSpeed'",2022-02-04T03:41:29Z,2022-02-05T03:52:01Z,DeepSpeed,,
15507,b'Add Data2Vec',2022-02-03T21:38:08Z,2022-03-01T10:09:20Z,,,
15506,b'[deepspeed docs] memory requirements',2022-02-03T17:36:14Z,2022-02-03T18:55:14Z,,,
15505,"b""ValueError: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length.""",2022-02-03T17:32:00Z,2022-02-04T10:48:51Z,,ValueError,"ValueError: too many dimensions 'str'"
15504,b'Add implementation of typical sampling',2022-02-03T14:49:23Z,2022-02-09T15:48:41Z,,,
15503,b'[Flax tests] Disable scheduled GPU tests',2022-02-03T14:38:20Z,2022-02-03T16:12:14Z,,,
15502,b'Timestamps for Wav2Vec 2.0 models and/or ASR pipelines',2022-02-03T13:42:59Z,2022-02-22T18:26:44Z,,,
15501,b'Add general vision docstrings',2022-02-03T12:51:44Z,2022-02-03T16:47:22Z,,,
15500,b'Allow training from multiple languages for multilingual seq2seq models (varying forced_bos_token_id)',2022-02-03T11:56:01Z,2022-03-13T15:02:06Z,,,
15499,b'Perceiver IO : How to preprocess raw audio into vector  ',2022-02-03T10:56:12Z,2022-03-13T15:02:07Z,,,
15498,b'[torch_int_div] Correct true division in generation',2022-02-03T10:44:09Z,2022-02-07T15:04:18Z,,,
15497,"b""ImportError: cannot import name 'AdamW' from 'transformers' (unknown location)""",2022-02-03T07:26:27Z,2022-03-13T15:02:08Z,,ImportError,"ImportError: cannot import name 'AdamW' from 'transformers' (unknown location)"
15496,b'run_summarization fails with RuntimeError: CUDA error: device-side assert triggered when using multi GPU',2022-02-03T06:57:26Z,2022-02-04T06:49:07Z,,RuntimeError,"RuntimeError: Caught RuntimeError in replica 0 on device 0."
15495,b'Does Hugging face defaults allow to log mlflow artifacts and name every run of mlflow log?',2022-02-03T06:55:39Z,2022-03-13T15:02:09Z,,,
15494,b'fix TFMarianMTModel output',2022-02-03T06:20:41Z,2022-02-03T11:57:29Z,,,
15493,b'[deepspeed] fix a bug in a test',2022-02-03T05:50:34Z,2022-02-03T16:55:46Z,,,
15492,b'Remove loss from some flax models docs & examples',2022-02-03T05:48:53Z,2022-02-03T20:39:46Z,,,
15491,b'Support for Monotonic Mulithead Attention based Simultaneous Speech-to-text Translation',2022-02-03T04:59:01Z,,New model,,
15490,b'How to convert  tf_model.h5 to tf_model.ckpt',2022-02-03T02:11:35Z,2022-02-10T17:27:27Z,,,
15489,b'Create a custom model guide',2022-02-03T00:28:48Z,2022-02-07T18:34:56Z,Documentation,,
15488,b'[parallelism docs] Megatron-Deepspeed info',2022-02-03T00:27:09Z,2022-02-04T19:15:14Z,,,
15487,b'502 Server Error: Bad Gateway for url: https://huggingface.co/api/models/t5-base',2022-02-03T00:20:32Z,2022-02-06T20:15:02Z,,,
15486,b'[deepspeed docs] DeepSpeed ZeRO Inference',2022-02-03T00:12:38Z,2022-02-04T21:51:02Z,,,
15485,b'Number-specific tokenization changes',2022-02-02T20:56:03Z,,New model,,
15484,b'ASR pipeline never takes into account the beam width of ngram',2022-02-02T19:20:54Z,2022-02-15T16:53:24Z,,,
15483,"b""Isn't `transformers.utils.fx` compatible with torch 1.10+ ?""",2022-02-02T18:39:35Z,2022-02-02T18:52:52Z,,ImportError,"ImportError: Found an incompatible version of torch. Found version 1.10.0, but only version 1.9 is supported."
15482,b'Fix labels stored in model config for token classification examples',2022-02-02T18:33:47Z,2022-02-02T19:23:43Z,,,
15481,b'Fic docstring of ASR pipeline',2022-02-02T16:19:25Z,2022-02-02T17:12:22Z,,,
15480,b'fix error posted in issue #15448',2022-02-02T15:06:03Z,2022-02-02T15:45:51Z,,,
15479,b'Wrong/inconsistent behaviour in EncoderDecoderModel and generate method',2022-02-02T13:10:11Z,,,,
15478,b'Pretrained model for sequence to sequence question answering',2022-02-02T12:50:30Z,2022-03-04T17:01:21Z,,,
15477,b'tf.concatenate does not exist',2022-02-02T12:28:36Z,,,,
15476,b'Add Adapter Weighs to Flax',2022-02-02T11:19:19Z,2022-02-09T15:24:41Z,,ValueError,"ValueError: operands could not be broadcast together with shapes (2,2,768) (2,15,768) "
15475,b'Log custom mlflow artifact using trainer',2022-02-02T10:45:10Z,2022-03-13T15:02:12Z,,PermissionError,"PermissionError: [Errno 13] Permission denied: '/opt/mlflow'"
15474,b'token-classification example looses ner_tags labels in trained model',2022-02-02T10:34:23Z,2022-02-02T19:23:43Z,,,
15473,b'Add preprocess_logits_for_metrics Trainer param',2022-02-02T07:06:19Z,2022-02-03T17:07:20Z,,,
15472,b'Wav2Vec2 - TypeError: Concatenation operation is not implemented for NumPy arrays',2022-02-02T04:32:07Z,,,TypeError,"TypeError: Concatenation operation is not implemented for NumPy arrays, use np.concatenate() instead. Please do not rely on this error; it may not be given on all Python implementations."
15471,"b'RuntimeError: CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling cublasSgemmStridedBatched (handle, opa, opb, m, n, k, &alpha, a, lda, stridea, b, ldb, strideb, &beta, c, ldc, stridec, num_batches)'",2022-02-02T04:23:28Z,2022-03-13T15:02:13Z,,RuntimeError,"RuntimeError: CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling `cublasSgemmStridedBatched( handle, opa, opb, m, n, k, &alpha, a, lda, stridea, b, ldb, strideb, &beta, c, ldc, stridec, num_batches)`"
15470,b'Issue with long contexts or AutoTokenizer.. Unsure which one is it is more of',2022-02-01T21:43:23Z,2022-02-01T22:01:19Z,,,
15469,b'Standardize semantic segmentation models outputs',2022-02-01T21:23:48Z,2022-02-04T19:52:08Z,,,
15468,b'[Wav2Vec2FeatureExtractor] Align documentation with code',2022-02-01T19:56:20Z,2022-02-23T17:39:41Z,,,
15467,b'New and better T5 checkpoints from scaling transformers paper',2022-02-01T19:45:49Z,,New model,,
15466,b'Preprocess/transform logits before caching them for computing metrics.',2022-02-01T19:05:11Z,2022-02-03T17:07:20Z,,,
15465,b'[Wav2Vec2ProcessorWithLM] add alpha & beta to batch decode & decode',2022-02-01T16:54:17Z,2022-02-02T11:59:40Z,,,
15464,b'Convert T5x models to PyTorch',2022-02-01T16:19:04Z,,,,
15463,b'`Trainer.push_to_hub` always tries to push to the Hub',2022-02-01T16:00:56Z,2022-02-01T20:49:04Z,,,
15462,b'Update README.md',2022-02-01T14:23:34Z,2022-02-01T15:04:31Z,,,
15461,b'[BartTokenizer] remove inheritance on RobertaTokenizer',2022-02-01T13:22:05Z,2022-02-01T19:59:24Z,,,
15460,b'Inspect inner layers of Transformer models as in TensorFlow/Keras',2022-02-01T12:52:48Z,2022-02-01T16:14:35Z,,,
15459,b'Masking in T5Attention',2022-02-01T11:52:33Z,2022-02-01T14:54:19Z,,,
15458,b'Support dynamical input size and shape for TrOcr input image',2022-02-01T11:30:27Z,2022-03-12T15:01:50Z,,,
15457,b'apply torch int div to layoutlmv2',2022-02-01T11:10:25Z,,,,
15456,b'fix set truncation attribute in `__init__` of `PreTrainedTokenizerBase`',2022-02-01T10:28:56Z,2022-02-02T22:18:10Z,,,
15455,b'Fixing overriding from_pretrained with `truncation_side`.',2022-02-01T10:26:44Z,2022-02-01T11:22:38Z,,,
15454,b'replace assert with exception for padding_side arg in `PreTrainedTokenizerBase` `__init__`',2022-02-01T10:15:34Z,2022-02-01T15:13:58Z,,,
15453,b'fix from_vision_text_pretrained doc example',2022-02-01T08:06:21Z,2022-02-01T11:20:22Z,,,
15452,b'Unseen decoded words during inference with Wav2Vec2ProcessorWithLM',2022-02-01T06:30:15Z,2022-03-12T15:01:52Z,,,
15451,b'Adding RelationExtraction head to layoutLMv2 and layoutXLM models',2022-02-01T04:52:44Z,,New model,,
15450,b'Add class weight support for classification',2022-02-01T01:20:29Z,,,,
15449,b'Add class re-weighting mechanism for classification tasks',2022-02-01T01:19:01Z,2022-03-12T15:01:53Z,,,
15448,"b'line 111 in convert_megatron_bert_checkpoint.py cause error ""AttributeError: \'Namespace\' object has no attribute \'get\'""'",2022-01-31T20:26:53Z,2022-02-02T19:52:23Z,,,
15447,b'Tokenizers can not pad tensorized inputs',2022-01-31T19:51:14Z,2022-03-12T15:01:53Z,,"RuntimeError, ValueError","RuntimeError: Boolean value of Tensor with more than one value is ambiguousValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
15446,b'[generate] fix synced_gpus default',2022-01-31T19:15:47Z,2022-01-31T21:58:27Z,,,
15445,b'skip large generations pipeline test for XGLM',2022-01-31T19:09:33Z,2022-01-31T21:53:17Z,,,
15444,"b'[M2M100, XGLM] fix positional emb resize'",2022-01-31T18:56:55Z,2022-02-01T13:32:55Z,,,
15443,"b'Fix FlaxDataCollatorForT5MLM for texts with ""<pad>""'",2022-01-31T18:53:50Z,2022-03-04T02:23:40Z,,,
15442,b'Misfiring tf warnings',2022-01-31T18:38:57Z,2022-01-31T19:18:00Z,,,
15441,b'[XGLMTokenizer] correct positional emb size',2022-01-31T18:14:39Z,2022-01-31T18:47:49Z,,,
15440,b'tokenizer truncation_side is not set up with from_pretrained call',2022-01-31T18:12:03Z,2022-02-02T22:18:10Z,,,
15439,b'Change REALM checkpoint to new ones',2022-01-31T17:28:48Z,2022-01-31T17:50:20Z,,,
15438,b'Harder check for IndexErrors in QA scripts',2022-01-31T17:16:37Z,2022-02-01T20:49:14Z,,,
15437,b'Error when group_by_length is used with an IterableDataset',2022-01-31T16:57:23Z,2022-01-31T20:33:16Z,,,
15436,b'use mean instead of elementwise_mean in XLMPredLayer',2022-01-31T16:53:46Z,2022-02-01T18:08:17Z,,,
15435,b'Fix spurious warning in TF TokenClassification models',2022-01-31T16:19:52Z,2022-01-31T17:09:17Z,,,
15434,b'[Swin] Add missing header',2022-01-31T16:12:27Z,2022-01-31T16:15:54Z,,,
15433,b'Add doc for add-new-model-like command',2022-01-31T15:48:07Z,2022-01-31T16:10:46Z,,,
15432,b'add t5 ner finetuning',2022-01-31T15:19:57Z,2022-01-31T16:03:07Z,,,
15431,"b""Can't push a model to hub""",2022-01-31T14:54:50Z,2022-02-01T03:22:36Z,,,
15430,b'Update README.md',2022-01-31T14:46:29Z,2022-01-31T14:48:20Z,,,
15429,b'[RobertaTokenizer] remove inheritance on GPT2Tokenizer',2022-01-31T14:42:17Z,2022-01-31T18:50:25Z,,,
15428,b'[Robust Speech Challenge] Add missing LR parameter',2022-01-31T14:10:12Z,2022-01-31T14:50:56Z,,,
15427,b'[XGLM] fix gradient checkpointing',2022-01-31T14:00:15Z,2022-01-31T14:51:50Z,,,
15426,b'Higher memory allocation for GPT-2 forward pass when not using past_key_values.',2022-01-31T13:27:12Z,2022-03-10T15:07:13Z,,,
15425,b'Partial Torch Memory Efficient Attention',2022-01-31T13:25:37Z,2022-03-10T15:07:14Z,,,
15424,"b""error when running official run_speech_recognition_ctc.py (ValueError: 'a' cannot be empty unless no samples are taken)""",2022-01-31T13:09:21Z,2022-03-10T15:07:15Z,,ValueError,"ValueError: 'a' cannot be empty unless no samples are taken`"
15423,b'Update modeling_wav2vec2.py',2022-01-31T12:51:12Z,2022-01-31T20:22:11Z,,,
15422,"b""XGLM Bug: break: module 'torch.utils' has no attribute 'checkpoint'""",2022-01-31T12:30:24Z,2022-01-31T14:51:50Z,,,
15421,b'[Trainer] suppress warning for length-related columns',2022-01-31T12:03:03Z,2022-01-31T17:51:29Z,,,
15420,b'Bug in T5 Tokenizer - Adding space after special tokens',2022-01-31T12:00:12Z,,,,
15419,"b""Add option to resize like torchvision's Resize""",2022-01-31T10:28:57Z,2022-02-02T08:44:22Z,,,
15418,"b""Notify user if group_by_length won't be used""",2022-01-31T08:33:37Z,2022-01-31T20:33:16Z,,,
15417,b'Support for larger XGLM models',2022-01-31T08:27:34Z,2022-01-31T16:33:06Z,New model,,
15416,b'Constrained Beam Search [without disjunctive decoding]',2022-01-31T07:36:12Z,2022-02-09T15:59:27Z,,,
15415,b'Performance of T5',2022-01-31T02:38:56Z,2022-01-31T09:28:08Z,,,
15414,b'[Hotfix] Fix Swin model outputs',2022-01-30T21:05:55Z,2022-01-31T15:32:14Z,,,
15413,b'add scores to Wav2Vec2WithLMOutput',2022-01-30T18:58:45Z,2022-02-15T15:40:51Z,,,
15412,b'How to use multi-node multi-gpu cluster for large batch beam-search decoding',2022-01-30T18:14:22Z,2022-03-10T15:07:17Z,,,
15411,b'[JAX/FLAX]: Language modeling example throws RESOURCE_EXHAUSTED error for large datasets',2022-01-30T17:20:28Z,2022-03-19T15:02:09Z,,,
15410,b'Add SegformerFeatureExtractor to Auto API',2022-01-30T08:26:51Z,2022-01-31T10:38:08Z,,,
15409,b'Wav2Vec2 models must either throw or deal with add_apater',2022-01-29T22:42:40Z,2022-02-07T16:03:12Z,,,
15408,b'Fix additional DataTrainingArguments documentation',2022-01-29T22:12:35Z,2022-01-31T12:45:11Z,,,
15407,b'adds decoding scores to Wav2Vec2DecoderWithLMOutput',2022-01-29T21:29:38Z,2022-01-30T14:52:49Z,,,
15406,b'[XGLMTokenizer] fix init and add in AutoTokenizer',2022-01-29T20:32:19Z,2022-01-30T14:35:53Z,,,
15405,b'AutoTokenizer does not support XGLMTokenizer',2022-01-29T16:40:47Z,2022-01-30T14:35:53Z,,,
15404,b'what is the equivalent manner for those lines?',2022-01-29T16:03:12Z,2022-02-18T21:36:38Z,,,
15403,b'Correct eos_token_id settings in generate',2022-01-29T12:36:36Z,2022-02-02T23:24:40Z,,,
15402,b'XLNet config num_labels and _num_labels',2022-01-29T06:13:33Z,2022-02-01T09:18:11Z,,,
15401,b'Running SQuAD 1.0 sample command raises `IndexError`',2022-01-29T06:11:18Z,2022-02-01T20:49:13Z,,IndexError,"IndexError: list index out of range"
15400,"b'[deepspeed doc] fix import, extra notes'",2022-01-29T01:30:51Z,2022-01-31T16:28:11Z,,,
15399,b'[deepspeed] `bigscience/T0*` multi-gpu inference with ZeRO',2022-01-28T23:54:16Z,2022-02-02T22:18:12Z,DeepSpeed,AttributeError,"AttributeError: module 'transformers.deepspeed' has no attribute 'initialize'"
15398,b'Error with run_seq2seq_qa.py official script (pyarrow.lib.ArrowInvalid: Column 4 named labels expected length 1007 but got length 1000)',2022-01-28T19:22:23Z,,,,
15397,b'[activations] pytorch-1.11+ Tanh Gelu Approximation ',2022-01-28T17:57:58Z,,"Performance, WIP",,
15396,"b""ImportError: cannot import name 'ImageGPTFeatureExtractor' from 'transformers' (unknown location)""",2022-01-28T17:29:45Z,2022-01-31T19:53:56Z,,ImportError,"ImportError: cannot import name 'ImageGPTFeatureExtractor' from 'transformers' (unknown location)"
15395,b'Make links explicit',2022-01-28T17:06:20Z,2022-01-28T17:31:23Z,,,
15394,b'Use argument for preprocessing workers in run_summairzation',2022-01-28T16:10:46Z,2022-01-28T23:34:10Z,,,
15393,"b""snapshot_download() got an unexpected keyword argument 'allow_regex'""",2022-01-28T16:09:01Z,2022-03-07T08:38:36Z,,TypeError,"TypeError: snapshot_download() got an unexpected keyword argument 'allow_regex'"
15392,b'Logits size does not match vocabulary size when using pyctcdecode for a fine-tuned wav2vec 2.0 model',2022-01-28T16:05:21Z,2022-02-03T12:53:02Z,,,
15391,b'[Fix doc example] FlaxMarianPreTrainedModel',2022-01-28T15:18:51Z,2022-01-28T15:53:25Z,,,
15390,"b""can't use with pytorch dataloader""",2022-01-28T14:44:50Z,2022-03-07T15:04:49Z,,RuntimeError,RuntimeError: einsum(): the number of subscripts in the equation (3) does not match the number of dimensions (4) for operand 0 and no ellipsis was given
15389,b'Fine Tuned GPT2 model performs very poorly on token classification task',2022-01-28T12:42:22Z,2022-01-31T15:51:57Z,,,
15388,b'Prepare deprecated ONNX exporter for torch v1.11',2022-01-28T11:57:23Z,2022-01-28T15:32:47Z,,,
15387,b'Evidentiality-guided Generator - Retrieval model',2022-01-28T11:46:12Z,,New model,,
15386,b'Adding a test for hashing tokenizer state.',2022-01-28T10:37:24Z,2022-03-07T15:04:50Z,,,
15385,b'Force use_cache to be False in PyTorch',2022-01-28T09:08:06Z,2022-02-08T15:20:54Z,,,
15384,"b""Unsupported output type of N11onnxruntime22SequenceTensorTypeBaseE. Can't constant fold SequenceEmpty node 'SequenceEmpty_5330'""",2022-01-28T07:56:59Z,2022-03-07T15:04:51Z,,,
15383,b'Validate models are loadable',2022-01-28T05:50:49Z,2022-03-07T15:04:52Z,,,
15382,b'Update modeling_blenderbot.py',2022-01-28T05:30:01Z,2022-03-07T15:04:53Z,,,
15381,b'Does huggingface allow to perform n-fold cross validation and custom loss function using Hugging face Trainer and save best model?',2022-01-28T04:21:19Z,2022-03-10T15:07:19Z,,,
15380,b'[docs] fix wrong file name in `pr_check`',2022-01-28T02:22:31Z,2022-01-28T12:52:01Z,,,
15379,b'Save code of registered custom models',2022-01-27T21:05:05Z,2022-02-02T15:44:37Z,,,
15378,b'Add init to BORT',2022-01-27T19:41:36Z,2022-01-27T20:16:55Z,,,
15377,"b""No module named 'transformers.models.bort'""",2022-01-27T18:42:49Z,2022-02-08T06:07:08Z,,"ModuleNotFoundError, RuntimeError","ModuleNotFoundError: No module named 'transformers.models.bort'RuntimeError: Failed to import transformers.models.bort because of the following error (look up to see its traceback):"
15376,b'Fix tests_fetcher',2022-01-27T16:27:03Z,2022-01-27T19:17:49Z,,,
15375,b'Example script for PushToHubCallback',2022-01-27T15:56:23Z,2022-01-27T16:16:25Z,,,
15374,b'Add proper documentation for Keras callbacks',2022-01-27T15:41:05Z,2022-01-27T15:51:38Z,,,
15373,b'Super-small fix stops us confusing Keras console logging by modifying\xe2\x80\xa6',2022-01-27T15:40:20Z,2022-01-27T15:43:43Z,,,
15372,b'Run LayoutXLM without image input not possible',2022-01-27T14:40:03Z,2022-01-28T10:40:30Z,,,
15371,b'Add a device argument to the eval script',2022-01-27T14:36:56Z,2022-01-27T14:58:55Z,,,
15370,b'Implement fixes for TrainingArguments doc',2022-01-27T14:18:57Z,2022-01-27T15:25:43Z,,,
15369,b'Bump numpy from 1.19.2 to 1.21.0 in /examples/research_projects/lxmert',2022-01-27T13:30:34Z,2022-01-27T19:46:15Z,dependencies,,
15368,b'Bump notebook from 6.1.5 to 6.4.1 in /examples/research_projects/visual_bert',2022-01-27T13:30:22Z,2022-01-27T19:45:58Z,dependencies,,
15367,b'Bump numpy from 1.19.2 to 1.21.0 in /examples/research_projects/visual_bert',2022-01-27T13:30:15Z,2022-01-27T19:45:18Z,dependencies,,
15366,b'Issues with custom dataset in Wav2Vec2',2022-01-27T12:32:24Z,2022-03-01T20:53:15Z,,,
15365,b'Fix initialization of mutable types',2022-01-27T11:04:33Z,2022-03-06T15:01:38Z,,,
15364,b'Implementation of activations as subclasses of the torch.nn.Module',2022-01-27T10:55:20Z,2022-02-16T19:37:52Z,,,
15363,b'fix bug in run_clm.py',2022-01-27T09:26:13Z,2022-03-06T15:01:39Z,,,
15362,"b'Unexpected shape of ""past_key_values"" of ProphetNetDecoder.forward()'",2022-01-27T05:15:05Z,2022-03-07T15:04:55Z,,,
15361,b'Set syncfree AdamW as the default optimizer for xla:gpu device in amp mode',2022-01-27T02:59:08Z,2022-01-28T01:05:31Z,,,
15360,b'Add tensor parallelism related mappings',2022-01-26T23:42:19Z,2022-02-07T16:55:41Z,,,
15359,b'Error when running the T5 pre-training script',2022-01-26T20:33:01Z,,,ValueError,"ValueError: cannot reshape array of size 65471 into shape (64,newaxis)"
15358,"b'Ignored unknown kwarg option direction, while running run_mlm.py (pytorch)'",2022-01-26T19:37:18Z,2022-01-27T20:24:22Z,,,
15357,b'WIP Create test_speech_recognition_deepspeed.py',2022-01-26T19:03:55Z,2022-02-11T10:53:03Z,DeepSpeed,,
15356,b'Fix TFLEDModel',2022-01-26T18:54:54Z,2022-01-31T18:35:54Z,,,
15355,b'[docs] post-PR merge fix',2022-01-26T18:49:04Z,2022-01-26T19:23:32Z,,,
15354,"b""GeneratorExp aren't supported by torch.jit.script when I try to export a previously trained model  'google/vit-base-patch16-224-in21k'.""",2022-01-26T18:47:55Z,,Good First Issue,UnsupportedNodeError,"UnsupportedNodeError: GeneratorExp aren't supported:"
15353,b'Fix YosoConfig doc',2022-01-26T18:33:48Z,2022-01-26T20:06:27Z,,,
15352,b'Allow relative imports in dynamic code',2022-01-26T17:58:32Z,2022-01-27T19:48:00Z,,,
15351,b'Fix KerasMetricCallback prediction with generate() and inference of column names',2022-01-26T17:41:15Z,2022-01-27T14:13:23Z,,,
15350,b'Update doc writing guide',2022-01-26T17:37:49Z,2022-01-26T17:54:12Z,,,
15349,b'-',2022-01-26T15:52:38Z,2022-01-26T15:53:34Z,,,
15348,"b""Fix 'eval_split_name' described as defaulting to 'train'""",2022-01-26T14:28:05Z,2022-01-26T15:19:38Z,,,
15347,b'A few minor questions regarding run_summarization.py in examples',2022-01-26T12:56:45Z,2022-03-02T09:21:32Z,,,
15346,b'Fix deepspeed docs',2022-01-26T10:25:52Z,2022-01-26T12:24:33Z,,,
15345,b'DebertaV2 For run_qa.py',2022-01-26T10:12:46Z,2022-01-30T16:23:26Z,,ValueError,"ValueError: This example script only works for models that have a fast tokenizer. Checkout the big table of models at https://huggingface.co/transformers/index.html#supported-frameworks to find the model types that meet this requirement"
15344,b'wav2vec with LM leads to CPU OOM',2022-01-26T10:11:09Z,2022-01-27T10:36:26Z,,,
15343,b'Fix `bad_words_ids` not working with sentencepiece-based tokenizers',2022-01-26T10:10:43Z,2022-01-28T11:39:55Z,,,
15342,b'random word masking index shouble be great than 0',2022-01-26T07:51:42Z,2022-03-05T15:01:57Z,,,
15341,b'ValueError: No valid checkpoint found in output directory',2022-01-26T06:11:14Z,,,ValueError,"ValueError: No valid checkpoint found in output directory (./outputs/)"
15340,b'CPU OOM when using IterableDataset with dataloader_num_workers > 0',2022-01-26T05:24:24Z,2022-03-06T15:01:42Z,,,
15339,"b'how can i write tensorboard when i train model with the script ""run_speech_recognition_ctc.py"" '",2022-01-26T03:42:15Z,2022-02-16T09:26:12Z,,,
15338,b'preprocessing_num_workers missing in run_summarization_no_trainer',2022-01-26T01:46:32Z,2022-01-28T23:34:10Z,,,
15337,b'Fix table formatting in SegFormer docs',2022-01-26T01:45:36Z,2022-01-26T12:10:00Z,,,
15336,b'Perplexity VERY high but generated text coherent',2022-01-26T00:48:45Z,2022-03-05T15:01:58Z,,,
15335,b'Fix code format for Accelerate doc',2022-01-25T23:23:02Z,2022-01-27T19:49:04Z,,,
15334,b'Documentation for SegFormer includes improperly-formatted table ',2022-01-25T20:57:49Z,2022-01-26T12:10:00Z,,,
15333,b'Fine-tune wave2vec trained checkpoint',2022-01-25T20:21:58Z,2022-03-05T15:02:00Z,,,
15332,b'Fix missing eps arg for LayerNorm in ElectraGeneratorPredictions',2022-01-25T19:56:13Z,2022-01-28T23:32:27Z,,,
15331,b'Using HuggingFace Models for Text Translation of sensitive data',2022-01-25T18:20:14Z,2022-01-26T16:23:51Z,,,
15330,b'Deepspeed Wav2vec xlsr bug',2022-01-25T17:51:48Z,2022-03-16T15:07:19Z,,RuntimeError,"RuntimeError: CUDA error: an illegal memory access was encountered"
15329,b'[WIP][Doctests] Fix rst -> mdx',2022-01-25T16:26:30Z,2022-02-01T13:36:56Z,,,
15328,b'improve saving strategy of sentencepiece tokenizer',2022-01-25T15:39:59Z,2022-01-27T15:24:51Z,,,
15327,b'Push to hub save',2022-01-25T15:02:35Z,2022-01-27T14:00:54Z,,,
15326,b'Improve DistilBert attn mask',2022-01-25T15:01:26Z,,,,
15325,b'OSError: You seem to have cloned a repository without having git-lfs installed. Please install git-lfs and run `git lfs install` followed by `git lfs pull` in the folder you cloned.',2022-01-25T14:01:00Z,2022-01-25T15:12:52Z,,"UnpicklingError, OSError","UnpicklingError: invalid load key, 'v'.OSError: You seem to have cloned a repository without having git-lfs installed. Please install git-lfs and run `git lfs install` followed by `git lfs pull` in the folder you cloned."
15324,b'[Tests] Fix test',2022-01-25T13:49:30Z,2022-01-25T14:48:25Z,,,
15323,b'MarianMT models translating valid Chinese sentences to empty string',2022-01-25T11:51:11Z,2022-03-05T15:02:01Z,,,
15322,b'modeling_visual_bert in case of self.bypass_transformer=True',2022-01-25T09:29:41Z,2022-03-05T15:02:02Z,,,
15321,b'[Do not merge] Chase more pt tf inconsistency',2022-01-25T09:17:31Z,,,,
15320,b'Fix `bad_word_ids` not working with sentencepiece-based tokenizers',2022-01-25T08:22:34Z,2022-01-26T10:03:14Z,,,
15319,b'fix the `tokenizer_config.json` file for the slow tokenizer when a fast version is available',2022-01-24T18:36:43Z,2022-02-01T15:48:25Z,,,
15318,b'Fixing support `batch_size` and `num_return_Sequences` in `text-generation` pipeline',2022-01-24T18:28:10Z,2022-01-28T11:15:30Z,,,
15316,b'GPT-Neo batch inferencing with sampling results unexpected output',2022-01-24T15:22:48Z,2022-01-28T11:15:30Z,,,
15315,b'[WIP] Add Maskformer to the library',2022-01-24T14:45:10Z,2022-02-16T12:50:42Z,,,
15314,b'[LayoutLMV2 Tests] Make sure input is on GPU',2022-01-24T14:12:44Z,2022-01-24T14:54:47Z,,,
15313,b'Push to hub training argument not pushing',2022-01-24T13:49:26Z,2022-01-27T14:00:54Z,,,
15312,b'Replace NystromformerTokenizer with AutoTokenizer',2022-01-24T13:00:03Z,2022-01-24T15:33:43Z,,,
15311,b'Decoding with Wav2Vec2 with Language Model',2022-01-24T11:21:13Z,2022-01-27T12:47:45Z,,"ValueError, `ValueError","ValueError: Input logits of size 586, but vocabulary is size 44`ValueError: Input logits of size 586, but vocabulary is size 44`"
15310,b'Update eval.py',2022-01-24T10:37:34Z,2022-01-24T10:46:38Z,,,
15309,b'Add ASR CTC streaming example ',2022-01-24T10:19:35Z,2022-02-07T15:35:38Z,,,
15308,b'Minimal model for writing tests.',2022-01-24T09:58:14Z,2022-01-27T13:00:56Z,,,
15307,b'Robust Speech Challenge Evaluation File does not use entire dataset for metric calculation',2022-01-24T09:56:28Z,2022-01-24T10:46:38Z,,,
15306,b'Remove old debug code leftover.',2022-01-24T09:27:03Z,2022-01-24T12:27:45Z,,,
15303,b'Nan when training LayoutLM_V2 Model',2022-01-23T19:20:08Z,2022-03-03T15:07:35Z,,,
15302,"b""RuntimeError: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.cuda.FloatTensor instead (while checking arguments for embedding)""",2022-01-23T17:39:29Z,2022-03-03T15:07:36Z,,,
15301,b'Getting error while saving model',2022-01-23T17:22:50Z,2022-03-03T15:07:37Z,,TypeError,"TypeError: 'in <string>' requires string as left operand, not ShardedDDPOption"
15300,b'Added missing code in exemplary notebook - custom datasets fine-tuning',2022-01-23T16:46:35Z,2022-01-25T22:26:18Z,,,
15299,b'[WIP] Positive Constraint Decoding PR #1',2022-01-23T11:46:30Z,2022-01-31T13:42:02Z,,,
15298,b'Fix the inconsistency of loss calculation between PT/TF XLNetLMHeadModel',2022-01-23T10:26:09Z,2022-01-29T15:08:35Z,,,
15297,b'Fix and improve REALM fine-tuning',2022-01-23T09:52:43Z,2022-03-03T13:10:15Z,,,
15296,b'Question: how to evaluate word similarity with context?',2022-01-23T09:14:31Z,2022-03-02T15:03:48Z,,,
15295,b'feat(flax): leave restored weights on CPU',2022-01-23T01:55:05Z,,WIP,,
15294,b'Fix loss calculation in TFXXXForTokenClassification models',2022-01-22T15:35:59Z,2022-01-31T16:43:08Z,,,
15293,b'remove references to PDF reading via PIL',2022-01-22T14:17:05Z,,,,
15292,b'Padding idx in modeling RoBERTa',2022-01-22T08:44:03Z,2022-01-28T08:30:03Z,,`AssertionError,`AssertionError: Padding_idx must be within num_embeddings`
15291,b'[Fix doc example] fix missing import jnp',2022-01-22T07:28:10Z,2022-01-24T13:54:24Z,,,
15290,b'Update CONTRIBUTING.md',2022-01-22T06:37:53Z,2022-01-24T12:21:32Z,,,
15289,b'TrOCR small processors are all broken ',2022-01-21T22:47:08Z,2022-01-24T15:19:39Z,,TypeError,"TypeError: expected str, bytes or os.PathLike object, not NoneType"
15288,b'Update model share tutorial',2022-01-21T22:10:38Z,2022-01-29T00:49:26Z,Documentation,,
15287,b'Avoid using get_list_of_files',2022-01-21T21:17:28Z,2022-01-25T14:41:21Z,,,
15286,b'Fix a typo in tag addition',2022-01-21T19:48:56Z,2022-01-24T12:21:42Z,,,
15285,b'fix: Vocabulary build issue in run_speech_recognition.py',2022-01-21T19:34:01Z,2022-03-01T15:08:12Z,,,
15284,b'RagSequenceForGeneration without retriever',2022-01-21T18:59:52Z,2022-01-25T10:25:14Z,,AssertionError,"AssertionError: Make sure that `context_input_ids` are passed, if no `retriever` is set. Alternatively, you can set a retriever using the `set_retriever(...)` function."
15283,b'Saved slow tokenizers cannot be loaded in `AutoTokenizer` after environment change',2022-01-21T18:58:43Z,2022-02-01T15:50:09Z,,Exception,"Exception: No such file or directory (os error 2)"
15282,b'Self-Attention Layers for Perceiver Decoder',2022-01-21T16:55:24Z,2022-02-21T15:27:40Z,,,
15281,b'pytorch NER example dataset deleted',2022-01-21T15:54:19Z,2022-01-24T15:21:47Z,,,
15280,b'Fix processors',2022-01-21T15:11:48Z,2022-02-14T15:10:03Z,,,
15279,b'run_tests_pipelines_torch is not deterministic',2022-01-21T15:01:41Z,2022-02-20T17:02:36Z,,,
15278,b'Fixes Benchmark example link',2022-01-21T14:49:13Z,2022-01-21T14:52:13Z,,,
15277,b'Add ConvNeXT',2022-01-21T13:49:52Z,2022-02-07T15:11:37Z,,,
15276,b'Fix',2022-01-21T12:56:37Z,2022-01-21T13:46:15Z,,,
15275,b'Inference of finetuned wav2vec2-xls-r-300m model using the ASR pipeline does not remove special tokens.',2022-01-21T12:07:19Z,,,,
15274,b'[Robust Speech Challenge] Add timeline',2022-01-21T11:48:56Z,2022-01-21T16:12:10Z,,,
15273,b'Remove Longformers from ONNX-supported models',2022-01-21T11:26:17Z,2022-02-07T16:32:14Z,,,
15272,b'[PyTorch-nightly-test] Fix Wav2Vec2 LM & Phoneme tests',2022-01-21T11:11:55Z,2022-01-24T09:53:54Z,,,
15271,b'Move BART + ONNX example to research_projects',2022-01-21T10:44:25Z,2022-01-21T13:47:34Z,,,
15270,b'Prepare ONNX export for torch v1.11',2022-01-21T10:12:05Z,2022-01-21T13:28:19Z,,,
15269,b'Whether instantiation is premature?',2022-01-21T08:35:22Z,2022-03-04T17:53:54Z,,,
15268,b'[Fix doc example] TFLayoutLMForTokenClassification: missing import tf',2022-01-21T08:05:56Z,2022-01-21T16:18:12Z,,,
15267,b'Benchmark link in transformers/notebooks/README.md is broken',2022-01-21T05:28:18Z,2022-01-21T14:52:13Z,,,
15266,b'Require `tokenizers>=0.11.1`',2022-01-21T04:46:01Z,2022-02-15T10:46:12Z,,,
15265,b'Movement Prune',2022-01-21T04:10:47Z,2022-02-28T15:08:21Z,,,
15264,b'[Kernel Fusion] training benchmarks of AOTAutograd (multiple models)',2022-01-21T02:51:00Z,,"Performance, WIP, Kernel Fusion",,
15263,b'Add \xf0\x9f\xa4\x97 Accelerate tutorial',2022-01-21T00:03:25Z,2022-01-25T19:46:11Z,Documentation,,
15262,"b'Remove ""inputs"" in tf common test script (no longer required)'",2022-01-20T21:06:35Z,2022-02-01T10:09:50Z,,,
15261,b'Refine errors for pretrained objects',2022-01-20T20:25:45Z,2022-01-21T20:00:10Z,,,
15260,b'Fine Tunning the Pytorch script',2022-01-20T19:03:41Z,2022-02-28T15:08:22Z,,,
15259,b'Update fine-tune docs',2022-01-20T18:55:23Z,2022-02-02T00:28:13Z,Documentation,,
15258,b'Fix crash when logs are empty because Keras has wiped them out of spite',2022-01-20T18:13:06Z,2022-01-20T18:40:49Z,,,
15257,b'Fix code examples',2022-01-20T17:04:38Z,2022-01-20T20:51:51Z,,,
15256,"b""Fix TF Causal LM models' returned logits""",2022-01-20T16:47:47Z,2022-02-01T11:04:08Z,,,
15255,b'Tentative workflow improvement',2022-01-20T15:09:05Z,2022-01-20T18:51:19Z,,,
15254,b'Export LayoutLMv2 to TorchScript',2022-01-20T15:04:58Z,2022-03-01T15:08:14Z,,RuntimeError,"RuntimeError: The size of tensor a (241) must match the size of tensor b (290) at non-singleton dimension 1"
15253,b'Make sure to raise NotImplementedError with correct method name',2022-01-20T14:39:38Z,2022-01-20T15:37:35Z,,,
15252,b'corrected typo in robust speech event README.md regarding OVH Cloud link',2022-01-20T14:34:41Z,2022-01-21T15:49:09Z,,,
15251,b'Tentative workflow improvement',2022-01-20T14:31:53Z,2022-01-20T15:06:41Z,,,
15250,b'Add PoolFormer',2022-01-20T13:42:50Z,2022-02-04T13:08:03Z,,,
15249,b'Soft length regulation',2022-01-20T13:27:13Z,2022-01-20T13:27:42Z,,,
15248,b'Cannot load BART-base model',2022-01-20T11:58:43Z,2022-01-20T14:09:41Z,,OSError,"OSError: file facebook/bart-base/config.json not found                                                                                                                                   "
15247,b'[Wav2Vec2ProcessorWithLM] improve multi processing',2022-01-20T11:35:22Z,2022-01-21T17:30:10Z,,,
15246,b'Update README.md',2022-01-20T10:40:17Z,2022-01-20T10:40:26Z,,,
15245,b'Add soft length regulation for sequence generation',2022-01-20T10:24:09Z,2022-03-11T18:36:45Z,,,
15244,b'Evaluation Padding_idx Error when using BERTScore and Deepspeed Zero3 but not with Zero2',2022-01-20T09:59:18Z,2022-02-27T15:01:58Z,,AssertionError,"AssertionError: Padding_idx must be within num_embeddings"
15243,b'Update pipelines.mdx',2022-01-20T09:39:10Z,2022-01-20T13:46:48Z,,,
15242,b'[ViTMAE] Add image pretraining script',2022-01-20T09:23:47Z,2022-01-21T11:11:08Z,,,
15241,b'Updating InfNanLogitsprocess to also remove negative infinity.',2022-01-20T08:51:31Z,2022-01-21T15:43:17Z,,,
15240,b'[Fix doc example] missing import',2022-01-20T08:46:05Z,2022-01-20T13:47:24Z,,,
15239,b'Update README.md',2022-01-20T08:46:04Z,2022-01-20T08:46:50Z,,,
15238,b'Fix a bug that QuestionAnsweringPipeline ignores max_seq_len parameter',2022-01-20T08:03:58Z,2022-02-14T12:18:40Z,,,
15237,b'why training bigbird-512 model is much slower than bert-512?',2022-01-20T05:39:25Z,2022-01-24T06:30:22Z,,,
15236,b'wav2vec2_with_lm',2022-01-20T02:56:46Z,2022-02-27T15:01:59Z,,,
15235,b'Specify providers explicitly in ORT session initialization',2022-01-20T01:27:04Z,2022-01-21T14:49:30Z,,,
15234,b'Fixes tf_default_data_collator sometimes guessing the wrong dtype for labels',2022-01-19T18:09:03Z,2022-01-20T14:26:52Z,,,
15233,b'Adapt Common Voice Talk Title and Abstract',2022-01-19T17:00:44Z,2022-01-19T17:03:18Z,,,
15232,b'Wav2Vec2ForPreTraining doc example has None loss',2022-01-19T16:48:07Z,,"Good First Issue, Good First Documentation Issue",,
15231,b'Fix PR number',2022-01-19T15:50:14Z,2022-01-19T16:00:17Z,,,
15230,b'Adds missing module_specs for usages of _LazyModule',2022-01-19T15:46:50Z,2022-01-21T12:30:13Z,,,
15229,b'Is there any script for running RACE dataset?',2022-01-19T15:34:34Z,2022-01-20T08:11:18Z,,,
15228,b'Fix typo in BERT tokenization file',2022-01-19T14:39:18Z,2022-01-19T15:16:19Z,,,
15227,b'[Speech Event] Fix speech event readme',2022-01-19T14:24:28Z,2022-01-19T14:30:04Z,,,
15226,b'Correct Speech Event Readme',2022-01-19T14:22:11Z,2022-01-19T14:23:00Z,,,
15225,"b""[Fix doc example] TFFunnelTokenizer' is not defined""",2022-01-19T13:37:30Z,2022-01-19T14:06:00Z,,,
15224,b'Copy of the custom modeling file when saving a model',2022-01-19T12:01:16Z,,,,
15223,b'where is the 4.16.0dev??',2022-01-19T11:41:04Z,2022-02-27T15:02:00Z,,,
15222,b'Is it possible to support Wav2Vec in ZeroShotClassificationPipeline?',2022-01-19T11:17:53Z,2022-02-27T15:02:01Z,,,
15221,b'[ViTMAE] Various fixes',2022-01-19T11:04:39Z,2022-01-19T14:27:57Z,,,
15220,b'Wav2vec code not working with kenlm n-gram',2022-01-19T10:46:36Z,2022-02-28T15:08:24Z,,,
15219,b'Make chuking smartly (long files) work on asr ctc_with_lm.',2022-01-19T10:17:00Z,2022-01-19T20:04:26Z,,,
15218,b'MobileBertForPreTraining means  IB-BERT PreTraining? ',2022-01-19T08:34:55Z,2022-02-28T15:08:25Z,,,
15217,b'[ONNX] transformer.onnx exporting fails for longformer ',2022-01-19T08:02:06Z,,,`RuntimeError,"`RuntimeError: 0INTERNAL ASSERT FAILED at ""../torch/csrc/jit/ir/alias_analysis.cpp"":584, please report a bug to PyTorch. We don't have an op for aten::constant_pad_nd but it isn't a special case.  Argument types: Tensor, int[], bool, `"
15216,b'DebertaForMaskedLM cannot load the parameters in the MLM head from microsoft/deberta-base',2022-01-19T07:54:15Z,2022-03-16T15:07:24Z,,,
15215,b'ALL YOUR BASE ARE BELONG TO 504.',2022-01-19T05:36:34Z,2022-02-27T15:02:02Z,,,
15214,b'Add support for BERT SequenceClassification conversion to ONNX',2022-01-18T23:34:23Z,2022-01-20T05:16:03Z,,,
15213,b'[WIP] [doc] performance/scalability revamp',2022-01-18T22:38:28Z,2022-02-18T17:38:22Z,,,
15212,b'ValueError: transformers.models.auto.__spec__ is None. causing import errors',2022-01-18T18:28:46Z,2022-01-21T12:30:12Z,,ValueError,"ValueError: transformers.models.auto.__spec__ is None"
15211,b'Add FastTokenizer to REALM',2022-01-18T18:00:40Z,2022-01-19T14:19:36Z,,,
15210,b'Build dev documentation',2022-01-18T16:39:35Z,2022-01-19T13:47:35Z,,,
15209,b'Build dev documentation',2022-01-18T15:59:11Z,2022-01-18T16:39:04Z,,,
15208,b'Build dev documentation',2022-01-18T15:47:57Z,2022-01-18T15:54:42Z,,,
15207,b'Rename compute_loss in TF models',2022-01-18T15:17:32Z,2022-01-19T13:29:07Z,,,
15206,b'Different generations with the NLP model MarianMT of HuggingFace',2022-01-18T15:17:05Z,2022-03-02T15:03:51Z,,,
15205,b'Remove dependency to quiet Dependabot',2022-01-18T14:39:05Z,2022-01-18T14:44:35Z,,,
15204,b'Ignore empty subfolders when identifying submodules',2022-01-18T14:35:23Z,2022-01-18T14:48:46Z,,,
15203,b'How can i add new_token ?',2022-01-18T14:25:58Z,2022-01-19T15:33:07Z,,,
15202,b'Copies and docstring styling',2022-01-18T14:11:29Z,2022-01-18T14:16:56Z,,,
15201,b'[MBartTokenizer] remove dep on xlm-roberta tokenizer',2022-01-18T13:43:55Z,2022-01-18T15:02:57Z,,,
15200,b'[ASR pipeline] correct with lm pipeline',2022-01-18T13:37:28Z,2022-01-18T14:36:22Z,,,
15199,b'GPT2: masked_bias should be sufficiently small instead of -1e4',2022-01-18T12:55:48Z,2022-02-26T15:01:45Z,,,
15198,b'[examples/Flax] add a section about GPUs',2022-01-18T11:23:43Z,2022-01-31T18:20:53Z,,,
15197,b'Question about fine-tuning BeitForSemanticSegmentation model',2022-01-18T11:09:02Z,2022-01-18T14:00:08Z,,,
15196,b'Wav2Vec2ForCTC fine-tuning best practices',2022-01-18T11:06:43Z,2022-03-05T15:02:06Z,,,
15195,b'Tensorboard missing values',2022-01-18T10:49:28Z,,,,
15194,"b'`is_ctc` needs to be updated to `self.type == ""ctc"".'",2022-01-18T10:37:24Z,2022-01-18T11:20:10Z,,,
15193,b'M2M100 support for ONNX export',2022-01-18T10:04:22Z,2022-03-02T09:03:15Z,,,
15192,b'Question about fine-tuning with vision transformer (VIT)',2022-01-18T04:49:13Z,2022-01-18T13:50:15Z,,,
15191,"b""can't set gradient_checkpointing=True when using the DistributedDataParallel mode""",2022-01-18T03:57:06Z,2022-02-26T15:01:47Z,,,
15190,"b""`AutoConfig` doesn't support `gpt-neox`""",2022-01-18T00:35:22Z,2022-01-18T00:40:14Z,,KeyError,"KeyError: 'gpt-neox'"
15189,b'run_t5_mlm_flax.py Multi GPU Training',2022-01-17T23:46:19Z,2022-01-19T23:52:51Z,,,
15188,b'Mark bad tokenizers version',2022-01-17T19:54:55Z,2022-01-17T20:20:58Z,,,
15187,b'[Fix doc example] TFRagModel',2022-01-17T19:47:25Z,2022-01-18T12:16:30Z,,,
15186,b'Error when code examples are improperly closed',2022-01-17T19:21:03Z,2022-01-18T12:27:34Z,,,
15185,b'Longformer Set Global Attention Layers Non-Trainable',2022-01-17T17:41:29Z,2022-02-26T15:01:48Z,,,
15184,b'[doc] new MoE paper',2022-01-17T17:01:08Z,2022-01-17T17:10:52Z,,,
15183,b'ValueError When Padding Long Sequences To Small Max Length (Pytorch and Tensorflow)',2022-01-17T16:11:09Z,2022-01-17T17:41:10Z,,,
15182,b'Fix incorrect max_seq_length bug in pytorch training scripts',2022-01-17T15:40:19Z,2022-01-19T11:53:59Z,,,
15181,b'Bug with max_seq_length argument in training scripts',2022-01-17T15:39:29Z,2022-01-19T11:53:57Z,,,
15180,b'Fix deprecation warnings for int div',2022-01-17T14:08:14Z,2022-01-18T12:28:53Z,,,
15179,b'Unclear documentation regarding sequence length for all-MiniLM-L6-v2',2022-01-17T13:53:32Z,2022-02-24T15:07:46Z,,,
15178,b'Fix dtype issue in TF BART',2022-01-17T13:13:06Z,2022-01-17T14:02:55Z,,,
15177,b'processing texts longer than 512 tokens with token-classification pipeline ',2022-01-17T12:58:19Z,2022-02-24T15:07:47Z,,,
15176,b'Bug in seq2seq fine tuning section of Automatic Speech Recognition Examples',2022-01-17T11:30:36Z,2022-01-18T04:23:20Z,,```TypeError,"```TypeError: __init__() got an unexpected keyword argument 'add_adapter'```"
15175,b'Compute loss independent from decoder for TF EncDec models (as #14139)',2022-01-17T08:50:49Z,2022-02-10T14:47:03Z,,,
15174,b'Fix handling of multiple stopping criteria',2022-01-17T06:47:20Z,2022-02-24T15:07:48Z,,,
15173,b'Add class LayoutLMv2ForRelationExtraction',2022-01-17T03:32:24Z,,,,
15172,b'Fix encoder-decoder models when labels is passed',2022-01-16T13:28:33Z,2022-01-26T09:14:46Z,,AttributeError,"AttributeError: 'tuple' object has no attribute 'reshape'"
15171,b'RuntimeError: expected scalar type Half but found Float',2022-01-16T13:17:24Z,2022-01-25T15:55:34Z,,RuntimeError,"RuntimeError: expected scalar type Half but found Float"
15170,b'Using the accelerate MLM example still results in CUDA out of memory',2022-01-16T05:16:57Z,2022-01-18T16:15:22Z,,`RuntimeError,"`RuntimeError: CUDA out of memory. Tried to allocate 512.00 MiB (GPU 0; 14.76 GiB total capacity; 13.49 GiB already allocated; 419.75 MiB free; 13.54 GiB reserved in total by PyTorch)`"
15169,"b'model.generate with prefix_allowed_tokens_fn throws RuntimeError: probability tensor contains either `inf`, `nan` or element < 0'",2022-01-16T03:40:27Z,2022-03-14T15:06:47Z,,RuntimeError,"RuntimeError: probability tensor contains either `inf`, `nan` or element < 0"
15168,b'Add Model Interpretability Module',2022-01-15T14:19:23Z,2022-02-23T15:07:32Z,,,
15167,b'Enable tqdm toggling',2022-01-15T00:44:22Z,2022-01-18T22:52:35Z,,,
15166,b'Add FastSpeech2',2022-01-14T21:53:58Z,,New model,,
15165,b'Update tutorial docs',2022-01-14T18:48:16Z,2022-02-02T00:31:35Z,Documentation,,
15164,b'[WIP] add ctc speech streaming',2022-01-14T18:33:58Z,2022-01-24T14:28:17Z,,,
15163,b'[Speech models] Disable non-existing chunking in tests',2022-01-14T17:28:04Z,2022-01-16T16:15:20Z,,,
15162,b'Update from keras2onnx to tf2onnx',2022-01-14T17:01:54Z,2022-01-14T17:35:39Z,,,
15161,b'Add in `model.generate` support to encoder outputs  for flax models',2022-01-14T15:54:05Z,2022-02-08T16:53:23Z,,,
15160,b'Fix typo in test_configuration_common.py',2022-01-14T13:43:55Z,2022-01-14T13:54:02Z,,,
15159,b'Fix RuntimeError on generation_utils.py',2022-01-14T12:57:58Z,2022-02-07T14:41:17Z,,RuntimeError,"RuntimeError: Integer division of tensors using div or / is no longer supported, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead."
15158,b'fix BertTokenizerFast `tokenize_chinese_chars` arg',2022-01-14T12:19:38Z,2022-01-14T13:22:04Z,,,
15157,b'Fix RuntimeError on generation_utils.py',2022-01-14T12:15:36Z,2022-01-14T12:19:03Z,,RuntimeError,"RuntimeError: Integer division of tensors using div or / is no longer supported, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead."
15156,b'the `tokenize_chinese_chars` argument is not always taken into account with the fast version of the bert tokenizer',2022-01-14T12:12:46Z,2022-01-14T13:22:04Z,,,
15155,b'[Robust Speech Event] Add guides',2022-01-14T11:51:05Z,2022-01-18T17:44:49Z,,,
15154,b'Fixing flaky test (hopefully).',2022-01-14T10:32:00Z,2022-01-14T15:47:03Z,,,
15153,b'It is better to add a function to train additon tokens for the pre_trained tokenizer. esp. for the language like Chinese.',2022-01-14T08:46:37Z,2022-03-07T15:04:59Z,,,
15152,b'[Fix doc example] UniSpeechSatForPreTraining',2022-01-14T08:31:50Z,2022-01-17T23:34:06Z,,,
15151,b'Build dev doc',2022-01-14T07:50:45Z,2022-02-04T17:04:41Z,,,
15150,b'[summarization example] better error message',2022-01-13T21:53:37Z,2022-01-13T22:44:51Z,,,
15149,b'[deepspeed tests] fix summarization',2022-01-13T21:39:13Z,2022-01-13T21:48:52Z,,,
15148,b'Better dummies',2022-01-13T21:12:15Z,2022-01-14T15:59:42Z,,,
15147,b'[doc] performance: Efficient Software Prebuilds',2022-01-13T20:07:17Z,2022-01-15T02:25:21Z,,,
15146,b'Add TF glu activation function',2022-01-13T20:04:26Z,2022-01-14T10:42:08Z,,,
15145,b'Is it possible to use ONNX for summarisation with BART yet?',2022-01-13T19:38:28Z,2022-02-21T15:02:50Z,,ValueError,"ValueError: Model requires 52 inputs. Input Feed contains 2"
15144,b'Make sure all submodules are properly registered',2022-01-13T18:28:38Z,2022-01-14T12:37:51Z,,,
15143,b'[Fix doc example] - OpenAIGPTDoubleHeadsModel',2022-01-13T17:24:48Z,2022-01-14T12:42:13Z,,,
15142,b'Update model_sharing.mdx',2022-01-13T17:20:50Z,2022-01-13T17:26:03Z,,,
15141,b'Check the repo consistency in model templates test',2022-01-13T16:11:24Z,2022-01-14T09:52:39Z,,,
15140,b'AutoTokenizer | TypeError: an integer is required (got type NoneType)',2022-01-13T15:40:30Z,2022-01-14T10:17:17Z,,TypeError,"TypeError: an integer is required (got type NoneType)"
15139,b'Error when running TFT5ForConditionalGeneration with tensorflow-cpu==2.8.0-rc0',2022-01-13T15:28:09Z,2022-01-19T13:29:07Z,,,
15138,b'[TBD] discrepancy regarding the `tokenize` method behavior - should the unknown token be included or not',2022-01-13T15:20:11Z,,,,
15137,b'Add test suite for flaubert tokenizer',2022-01-13T15:10:15Z,,,,
15136,"b""AutoTokenizer | ValueError: Couldn't instantiate the backend tokenizer from one of:""",2022-01-13T14:42:33Z,2022-01-14T14:12:54Z,,ValueError,"ValueError: Couldn't instantiate the backend tokenizer from one of: "
15135,b'Error when running a wandb sweeps on run_summarization.py',2022-01-13T12:35:06Z,2022-01-14T16:17:34Z,,ValueError,"ValueError: signal only works in main thread"
15134,b'doc-builder -> doc-build',2022-01-13T10:24:39Z,2022-01-13T11:02:24Z,,,
15133,b'Add from_encoder_decoder_pretrained to some dummy obj',2022-01-13T09:42:58Z,2022-01-13T14:43:01Z,,,
15132,b'Faster model templates',2022-01-13T09:18:38Z,2022-02-21T15:02:51Z,,,
15131,b'Electra model class support loading weights from other types of BERTs',2022-01-13T08:27:02Z,2022-02-21T15:02:52Z,,,
15130,b' run_summarization.py download datasets error',2022-01-13T04:29:48Z,2022-02-21T15:02:53Z,,NotADirectoryError,"NotADirectoryError: [Errno 20] Not a directory: '/root/.cache/huggingface/datasets/downloads/1bc05d24fa6dda2468e83a73cf6dc207226e01e3c48a507ea716dc0421da583b/cnn/stories'"
15129,b'[examples/flax/language-modeling] set loglevel',2022-01-12T22:38:07Z,2022-01-13T14:17:28Z,,,
15128,b'Example script to edit kenlm arpa file does not work correctly in kaggle notebook',2022-01-12T21:54:51Z,2022-02-12T16:31:42Z,,,
15127,b'[Community Event] Robust Speech Challenge',2022-01-12T17:51:49Z,2022-02-21T15:02:54Z,,,
15126,b'Text generation with TAPAS as encoder',2022-01-12T17:50:03Z,2022-02-21T15:02:55Z,,,
15125,b'mBART support for run_summarization.py',2022-01-12T14:30:19Z,2022-01-12T21:39:34Z,,,
15124,b'[Fix doc example] - ProphetNetDecoder',2022-01-12T13:56:11Z,2022-01-13T11:45:30Z,,,
15123,b'Error while converting distilbart-mnli-12-1 model to ONNX',2022-01-12T13:34:13Z,2022-02-03T09:11:14Z,,,
15122,b'fix: switch from slow to generic tokenizer class',2022-01-12T10:22:17Z,2022-01-12T14:12:43Z,,,
15121,b'Add ONNX configuration classes to docs',2022-01-12T10:16:44Z,2022-01-12T15:33:32Z,,,
15120,b'Add MAE',2022-01-12T09:34:03Z,2022-01-18T15:21:32Z,,,
15119,b'add model scaling section',2022-01-12T09:24:58Z,2022-02-09T14:27:30Z,,,
15118,"b""ImportError: cannot import name 'CpmTokenizer' from 'transformers'""",2022-01-12T08:42:25Z,2022-02-20T15:01:57Z,,ImportError,"ImportError: cannot import name 'CpmTokenizer' from 'transformers' (/root/anaconda3/lib/python3.9/site-packages/transformers/__init__.py)"
15117,b'Reason for returning dequantized (fp.32) value at every layer of I-BERT.',2022-01-12T08:05:19Z,2022-02-20T15:01:58Z,,,
15116,b'Fix QA trainer to properly log eval metrics',2022-01-12T07:48:08Z,2022-01-14T13:01:49Z,,,
15115,b'OOM error on Pretraining Albert with batch size 8 ',2022-01-12T07:13:55Z,2022-03-16T15:07:28Z,,RuntimeError,"RuntimeError: CUDA out of memory. Tried to allocate 4.88 GiB (GPU 0; 15.78 GiB total capacity; 7.63 GiB already allocated; 2.08 Gi"
15114,b'Trying to train the TFWav2Vec2ForCTC model',2022-01-11T21:11:43Z,2022-02-11T16:16:26Z,,OperatorNotAllowedInGraphError,"OperatorNotAllowedInGraphError: in user code:"
15113,b'Add TFSpeech2Text',2022-01-11T20:57:12Z,2022-02-08T16:27:23Z,TensorFlow,,
15112,b'How to efficiently tokenize unknown tokens in GPT2',2022-01-11T19:56:45Z,2022-02-20T15:02:00Z,,,
15111,b'Update TF test_step to match train_step',2022-01-11T17:36:43Z,2022-01-11T19:05:39Z,,,
15110,b'Build dev doc',2022-01-11T17:33:03Z,2022-01-14T07:50:19Z,,,
15109,b'Why is Marian to Torch converter hardcoded for tied vocab ?',2022-01-11T17:25:22Z,2022-03-10T18:41:56Z,,,
15108,b'Support custom StoppingCriteria in model.generate',2022-01-11T16:41:21Z,2022-01-12T11:29:08Z,,,
15107,b'Weird evaluation result when using distributed training',2022-01-11T16:40:13Z,2022-03-08T15:03:27Z,,,
15106,"b'Add ""open in hf spaces"" gradio button issue #73'",2022-01-11T16:14:45Z,2022-01-14T15:12:31Z,,,
15105,b'Doc styler tip',2022-01-11T15:35:10Z,2022-01-11T16:45:40Z,,,
15104,b'Fix failing W2V2 test',2022-01-11T13:24:17Z,2022-01-11T14:57:34Z,,,
15103,b'Add PerceiverForTokenClassification',2022-01-11T13:18:43Z,2022-02-19T15:02:09Z,,,
15102,b'Print out durations of all scheduled tests',2022-01-11T13:06:47Z,2022-01-11T13:16:00Z,,,
15101,b'Cleanup load_weight_prefix in TFEncoderDecoderModel',2022-01-11T10:10:47Z,2022-02-03T15:11:53Z,,,
15100,b'Fix cookiecutter',2022-01-11T09:43:39Z,2022-01-11T10:57:26Z,,,
15099,b'change metric_key_prefix in seq2seq_trainer.py',2022-01-11T09:26:09Z,2022-01-11T12:44:30Z,,,
15098,b'Get started docs',2022-01-10T17:56:17Z,2022-01-29T01:01:37Z,Documentation,,
15097,b'GPT-J Tokenizer model_max_length=1024 despite n_positions=2048',2022-01-10T17:50:16Z,2022-01-11T13:45:52Z,,,
15096,b'Add test to check reported training loss',2022-01-10T17:34:59Z,2022-01-11T08:14:11Z,,,
15095,b'Take gradient accumulation into account when defining samplers',2022-01-10T17:06:28Z,2022-01-11T08:16:39Z,,,
15094,b'Happy New Year!',2022-01-10T16:52:42Z,2022-01-10T17:05:58Z,,,
15093,b'[DOC] fix doc examples for bart-like models',2022-01-10T16:40:47Z,2022-01-10T17:13:28Z,,,
15092,b'[Fix doc example] Speech2TextForConditionalGeneration',2022-01-10T16:27:39Z,2022-01-11T09:04:23Z,,,
15091,b'Add YOSO',2022-01-10T16:08:51Z,2022-01-26T18:18:30Z,,,
15090,b'Adding Tensorflow Perceiver Model',2022-01-10T15:11:15Z,2022-02-22T20:51:27Z,,,
15089,b'Trainer not keeping best model checkpoint with save_total_limit=1',2022-01-10T14:42:30Z,2022-01-10T16:31:33Z,,,
15088,b'Change assignee for tokenizers',2022-01-10T14:21:23Z,2022-01-10T14:22:49Z,,,
15087,"b""optimum | ModuleNotFoundError: No module named 'optimum.intel.lpot'""",2022-01-10T10:27:57Z,2022-01-11T12:48:59Z,,ModuleNotFoundError,"ModuleNotFoundError: No module named 'optimum.intel.lpot'"
15086,b'UserWarning: __floordiv__ is deprecated',2022-01-10T09:56:25Z,2022-02-09T17:07:46Z,,,
15085,b'Add Swin Transformer',2022-01-10T08:38:01Z,2022-01-21T11:10:42Z,,,
15084,b'electra is added to onnx supported model',2022-01-10T04:44:01Z,2022-02-08T14:47:49Z,,,
15083,b'[Wav2Vec2 Speech Event] Add speech event v2',2022-01-09T22:38:35Z,2022-01-10T09:46:21Z,,,
15082,b'Pytorch T5 pre-training script request',2022-01-09T13:18:56Z,,,,
15081,b'Script run_mlm_no_trainer.py error',2022-01-09T13:16:04Z,2022-01-11T13:06:34Z,,RuntimeError,"RuntimeError: Caught RuntimeError in replica 0 on device 0."
15080,b'explicitly load local file',2022-01-09T11:59:48Z,2022-02-17T15:06:53Z,,,
15079,b'[Fix doc example] Wrong checkpoint name',2022-01-09T09:27:46Z,2022-01-18T15:43:21Z,,,
15078,b'pegasus',2022-01-08T23:19:56Z,2022-01-11T13:07:02Z,,,
15077,b'train_new_from_iterator missing from GPT2Tokenizer',2022-01-08T22:00:00Z,2022-01-12T14:12:43Z,,,
15076,b'[Fix doc example] RagModel',2022-01-08T12:18:27Z,2022-01-10T14:28:40Z,,,
15075,b'[Benchmark]',2022-01-08T05:05:17Z,2022-02-16T15:06:47Z,,,
15074,b'TF Bert inference - support `np.ndarray` optional arguments',2022-01-07T18:38:26Z,2022-01-14T15:19:05Z,,,
15073,b'[VisionTextDualEncoder] Add token_type_ids param',2022-01-07T17:52:49Z,2022-01-07T19:02:49Z,,,
15072,b'[JAX/FLAX]: CLM Tokenizer Training confusion',2022-01-07T16:46:04Z,,WIP,,
15071,b'Pipeline ASR with LM.',2022-01-07T15:51:19Z,2022-01-12T08:28:20Z,,,
15070,b'Update Trainer code example',2022-01-07T15:35:51Z,2022-01-19T19:15:13Z,,,
15069,b'How to use an unpretrained model?',2022-01-07T14:11:19Z,2022-01-09T06:47:15Z,,,
15068,b'fix: #14486 do not use BertPooler in DPR',2022-01-07T13:01:51Z,2022-01-18T15:36:12Z,,,
15067,b'fix CLIP fast tokenizer and change some properties of the slow version',2022-01-07T12:24:00Z,2022-02-18T09:21:30Z,,,
15066,b'How do I change the classification head of a model from multi-label to multi-class?',2022-01-07T12:02:04Z,2022-01-08T11:07:11Z,,,
15065,b'compile error when installing transformers[flax]',2022-01-07T11:16:22Z,2022-01-07T11:32:03Z,,,
15064,b'Question: CANINE with (pre-trained) LM head',2022-01-07T11:14:56Z,2022-01-07T12:44:40Z,,,
15063,b'Not able to log training and validation loss to visualise in tensor-board as tfevents?',2022-01-07T06:18:59Z,2022-02-14T15:07:54Z,,,
15062,"b""Import Error : cannot import name 'create_repo' from 'huggingface_hub'""",2022-01-07T06:05:00Z,2022-03-17T15:01:53Z,,,
15061,b'Add CharacterBERT model [WIP]',2022-01-07T05:33:05Z,,,,
15060,b'Feature request: m2m100_418M support on onnx',2022-01-06T20:58:26Z,,,,
15059,b'Adding additional layers to TFHubertModel throws OperatorNotAllowedInGraphError',2022-01-06T17:53:58Z,2022-02-11T16:16:26Z,,OperatorNotAllowedInGraphError,"OperatorNotAllowedInGraphError: in user code:"
15058,b'Model summary doc page horizontal banners',2022-01-06T17:01:36Z,2022-01-10T15:06:14Z,,,
15057,b'[VisionTextDualEncoder] Fix doc example',2022-01-06T16:34:04Z,2022-01-06T16:59:06Z,,,
15056,b'Fix usage of additional kwargs in `from_encoder_decoder_pretrained` in encoder-decoder models',2022-01-06T16:20:39Z,2022-01-19T22:00:33Z,,TypeError,"TypeError: __init__() got an unexpected keyword argument 'add_adapter'"
15055,"b""Return type of ViTFeatureExtractor does not match 'return_tensors' parameter when input is torch.Tensor or PIL.Image.Image""",2022-01-06T15:21:08Z,,WIP,"**Note**, >ValueError, >TypeError","**Note**: Since `typing.List` is deprecated since python 3.9, I am using `builtins.list` in the following contents.>ValueError: Unable to create tensor, you should probably activate padding with 'padding=True' to have batched tensors with the same length.>TypeError: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'PIL.Image.Image'>"
15054,b'How can I update special token ids?',2022-01-06T14:18:00Z,2022-02-13T15:01:43Z,,,
15053,b'Add Detectron2 to Github actions',2022-01-06T13:34:55Z,2022-01-06T13:53:59Z,,,
15052,b'Multilabel Token Classification in trainer',2022-01-06T12:03:28Z,2022-01-17T13:33:34Z,,,
15051,"b""Trainer model __init__() got an unexpected keyword argument 'prediction_loss_only'""",2022-01-06T09:35:41Z,2022-02-20T15:02:03Z,,TypeError,"TypeError: __init__() got an unexpected keyword argument 'prediction_loss_only'"
15050,b'Cannot fine-tune google/mobilebert-uncased using native Pytorch',2022-01-06T07:24:27Z,2022-02-13T15:01:44Z,,,
15049,b'Inference API: Error with GPU inference',2022-01-06T06:58:05Z,2022-02-15T15:06:34Z,,{'error',"{'error': 'CUDA out of memory, try a smaller payload', 'warnings': ['Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.']}"
15048,"b'word_ids method is not available on fast tokenizers when using ""prepare_for_model""'",2022-01-05T22:10:00Z,2022-02-14T15:07:57Z,,ValueError,"ValueError: word_ids() is not available when using Python-based tokenizers"
15047,b'[Trainer] Very different loss reported during training vs. at the end.',2022-01-05T20:01:34Z,2022-03-09T15:06:31Z,,,
15046,b'Adding support for `microphone` streaming within pipeline.',2022-01-05T18:02:11Z,2022-02-02T14:12:12Z,,,
15045,b'[FX] `symbolic_trace` yields a TraceError for `BertModel`',2022-01-05T16:08:56Z,2022-01-06T10:06:13Z,,torch.fx.proxy.TraceError,"torch.fx.proxy.TraceError: Proxy object cannot be iterated. This can be attempted when the Proxy is used in a loop or as a *args or **kwargs function argument. See the torch.fx docs on pytorch.org for a more detailed explanation of what types of control flow can be traced, and check out the Proxy docstring for help troubleshooting Proxy iteration errors"
15044,b'[Fix doc examples] missing from_pretrained',2022-01-05T15:51:26Z,2022-01-07T15:55:59Z,,,
15043,b'[SpeechEncoderDecoder] Fix from pretrained',2022-01-05T15:30:02Z,2022-01-05T15:54:40Z,,,
15042,b'[CLIP] Fix TF test',2022-01-05T13:50:55Z,2022-01-05T15:58:43Z,,,
15041,b'[CLIP] Fix PT test',2022-01-05T12:33:47Z,2022-01-05T13:21:04Z,,,
15040,b'[Wav2Vec2ProcessorWithLM] improve decoder download',2022-01-05T11:30:02Z,2022-01-11T10:59:38Z,,,
15039,"b""Wav2Vec2 bart-large - finetuning failing with ValueError: tokenizer has to be of type....but is <class 'transformers.models.bart.tokenization_bart_fast.BartTokenizerFast'""",2022-01-05T07:19:07Z,2022-02-20T15:02:04Z,,"AttributeError, alueError","AttributeError: 'str' object has no attribute 'from_pretrained'alueError: tokenizer has to be of type <class 'type'>, but is <class 'transformers.models.bart.tokenization_bart_fast.BartTokenizerFast'"
15038,b'BERT model from pipeline hangs with multiprocessing pool',2022-01-05T07:05:57Z,2022-01-05T09:25:04Z,,,
15037,b'Wrap Roberta integration test forward passes with torch.no_grad()',2022-01-05T02:01:01Z,2022-01-06T13:48:50Z,,,
15036,b'use block_size instead of max_seq_length in tf run_clm example',2022-01-05T00:30:30Z,2022-01-12T13:57:01Z,,,
15035,b'Issue with `stas/tiny-wmt19-en-de` model',2022-01-04T21:53:00Z,2022-01-04T21:56:15Z,,,
15034,"b'ValueError: Layer weight shape (30522, 768) not compatible with provided weight shape torch.Size([1, 15, 3072])'",2022-01-04T19:44:26Z,2022-02-09T05:05:57Z,TensorFlow,ValueError,"ValueError: Layer weight shape (30522, 768) not compatible with provided weight shape torch.Size([1, 15, 3072])"
15033,"b""Fix doc example: mask_time_indices (numpy) has no attribute 'to'""",2022-01-04T16:23:22Z,2022-01-05T10:34:08Z,,,
15032,b'Removing tokens from the tokenizer',2022-01-04T15:59:34Z,2022-02-07T20:55:34Z,,,
15031,b'[DocTests Speech] Add doc tests for all speech models',2022-01-04T14:32:26Z,2022-01-27T13:29:31Z,,,
15030,b'Enabling `TF` on `image-classification` pipeline.',2022-01-04T09:41:59Z,2022-01-06T13:16:01Z,,,
15029,b'Hotfix `chunk_length_s` instead of `_ms`.',2022-01-04T09:06:21Z,2022-01-04T13:07:45Z,,,
15028,b'add test checking the offsets for an input splitted into words for different `add_prefix_space` and `trim_offsets` args',2022-01-04T09:04:31Z,2022-02-11T15:07:19Z,,,
15027,b'Adding QoL for `batch_size` arg (like others enabled everywhere).',2022-01-04T08:46:21Z,2022-01-05T11:16:23Z,,,
15026,b'[Benchmark] HF Trainer on A100',2022-01-04T05:41:33Z,,"Benchmarks, WIP",,
15025,b'Which model checkpoint should be selected for evaluation\xef\xbc\x9f',2022-01-04T05:35:21Z,2022-02-11T15:07:21Z,,,
15024,b'Training causal language models from scratch without grouping independent data samples into blocks',2022-01-04T05:28:53Z,2022-02-11T15:07:22Z,,,
15023,b'[doc] normalize HF Transformers string',2022-01-04T04:36:19Z,2022-01-10T16:44:34Z,,,
15022,b'Enable AMP for xla:gpu device in trainer class',2022-01-04T04:19:15Z,2022-01-13T20:21:00Z,,,
15021,b'AutoTokenizer unable to load pre-trained bert-base-uncased tokenizer',2022-01-04T02:12:37Z,2022-01-04T15:00:37Z,,ValueError,"ValueError: Calling BertTokenizerFast.from_pretrained() with the path to a single file or url is not supported for this tokenizer. Use a model identifier or the path to a directory instead."
15020,b'[Trainer] finetuning: larger batch-size leading to a worse train loss',2022-01-03T23:15:04Z,2022-01-04T02:56:12Z,,,
15019,b'Make OpenAIGPTTokenizer work with SpaCy 2.x and 3.x',2022-01-03T22:44:39Z,2022-01-10T12:53:20Z,,,
15018,b'[doc] Update parallelism.mdx',2022-01-03T20:04:43Z,2022-01-04T17:58:28Z,,,
15017,b'[Examples] Correct run ner label2id for fine-tuned models',2022-01-03T19:06:05Z,2022-01-24T20:18:05Z,,,
15016,"b""Fix doc examples: name 'torch' is not defined""",2022-01-03T17:41:58Z,2022-01-03T18:11:48Z,,,
15015,b'[Tests] Correct Wav2Vec2 & WavLM tests',2022-01-03T17:21:27Z,2022-01-03T19:19:04Z,,,
15014,b'Update check_repo.py',2022-01-03T17:05:20Z,2022-01-10T11:55:43Z,,,
15013,b'[doc] Update parallelism.mdx',2022-01-03T16:07:34Z,2022-01-03T19:49:27Z,,,
15012,b'Remove old asserts.',2022-01-03T15:26:12Z,2022-01-06T14:45:41Z,,,
15011,b'ViTFeatureExtractor PyTorch Batch problem',2022-01-03T14:45:35Z,2022-02-10T15:06:29Z,,,
15010,b'update the file list for doc testing',2022-01-03T14:35:22Z,2022-01-31T17:46:10Z,,,
15009,b'Recovering on corrupted files on disk.',2022-01-03T13:53:44Z,,,,
15008,b'Fixing t2t pipelines lists outputs.',2022-01-03T13:23:57Z,2022-01-03T13:49:58Z,,,
15007,b'Unexpected outputs of randomly initialized `T5ForConditionalGeneration`.',2022-01-03T09:58:31Z,2022-01-03T10:00:13Z,,,
15006,"b'""total_flos"" showing much bigger number than expected'",2022-01-03T07:47:54Z,2022-02-13T15:01:46Z,,,
15005,b'Add Flax RoFormer',2022-01-02T16:15:56Z,2022-01-04T12:23:10Z,,,
15004,b'How to define the self-attention layer with transformers',2022-01-02T15:15:28Z,2022-01-03T01:53:44Z,,,
15003,"b""AlbertTokenizer doesn't decode special tokens properly""",2022-01-02T14:00:36Z,,Good First Issue,,
15002,b'Fix a little typo',2022-01-01T21:12:22Z,2022-01-04T11:59:48Z,,,
15001,b'Fix TFEncoderDecoder labels handling #14357',2022-01-01T16:02:42Z,2022-01-12T14:29:10Z,TensorFlow,,
15000,"b'Fixing QA when `sequence_ids` is None (instead of 0, 1).'",2022-01-01T11:02:50Z,2022-01-01T12:00:49Z,,,
14999,b'fix model table cell text alignment',2021-12-31T21:42:59Z,2022-01-10T11:44:12Z,,,
14998,b'How to boost the speed of one sentence Marian Translation(no batches)?',2021-12-31T14:57:54Z,2022-02-07T15:07:12Z,,,
14997,"b""No module named 'transformers.models.fnet.modeling_fnet'""",2021-12-31T08:39:39Z,2022-02-07T15:07:13Z,,,
14996,b'[Benchmarks] index',2021-12-31T06:41:00Z,,"Benchmarks, WIP",,
14995,"b'BertTokenizer can\'t split the string in the form of ""word+special_token""correctly.'",2021-12-31T04:17:23Z,2022-01-18T14:37:15Z,Discussion,,
14994,b'Allow training to resume even if RNG states are not properly loaded',2021-12-30T21:40:02Z,2021-12-30T22:03:20Z,,,
14993,b'Unexpected usage of `next_token_scores` and `beam_scores` in `beam_sample()`',2021-12-30T21:35:36Z,2022-02-07T15:07:14Z,,,
14992,b'Add model like',2021-12-30T21:25:56Z,2022-01-24T20:25:11Z,,,
14991,b'Fix saving FlaubertTokenizer configs',2021-12-30T16:20:39Z,2022-01-11T18:19:33Z,,,
14990,"b'When using bert-base-chinese model, except for the first one, other uppercase English letters that are the same in succession will be ignored. And the input_id of different uppercase English characters is the same'",2021-12-30T13:57:04Z,2022-02-07T15:07:15Z,,,
14989,b'EncoderDecoderModel loss decreased unbelievably and the generated text were repetitive 4.12.0',2021-12-30T12:19:17Z,2022-03-05T15:02:12Z,,,
14988,b'Adding `num_return_sequences` support for text2text generation.',2021-12-30T10:31:20Z,2021-12-30T15:17:16Z,,,
14987,b'the shape of trainer.predict().predictions is inconsistent with the input dataset',2021-12-30T09:40:08Z,2022-02-07T15:07:16Z,Migration,,
14986,b'can I use different pre-trained model for autoTokenizer and autoModel?',2021-12-30T09:36:58Z,2022-03-05T15:02:13Z,Migration,,
14985,b'very large model on multi gpu',2021-12-30T05:40:38Z,2022-02-08T15:09:57Z,,,
14984,b'Finetune M2M on multiple language pairs',2021-12-30T04:47:14Z,2022-03-12T15:02:01Z,,,
14983,b'Fix Code block speech pretraining example',2021-12-29T22:15:04Z,2022-01-04T11:59:20Z,,,
14982,b'Resubmit changes after rebase to master',2021-12-29T20:32:29Z,2022-01-07T07:34:12Z,,,
14981,b'Fixing a pathological case for slow tokenizers',2021-12-29T18:29:51Z,2021-12-30T08:10:34Z,,,
14980,b'[Generate] correct encoder_outputs are passed without attention_mask',2021-12-29T17:13:36Z,2021-12-30T09:16:03Z,,,
14979,b'Add `with torch.no_grad()` to DistilBERT integration test forward pass',2021-12-29T16:09:48Z,2022-01-12T15:42:39Z,,,
14978,b'How to instantiate custom T5ForConditionalGeneration',2021-12-29T13:52:03Z,2021-12-29T18:44:15Z,,,
14977,b'Convering tf to torch: How to set custom embedding_size when using load_tf_weights_in_bert?',2021-12-29T11:04:57Z,2022-01-28T15:26:38Z,,ValueError,"ValueError: Pointer shape torch.Size([312]) and array shape (128,) mismatched"
14976,"b""Model stopped training once I introduced << report_to = 'wandb' >> in TrainingArguments""",2021-12-29T11:00:40Z,2022-02-27T15:02:10Z,,,
14975,b'Custom constructed and trained `tokenizers.Tokenizer` for Albert error.',2021-12-29T09:37:27Z,2021-12-29T12:53:55Z,,ValueError,"ValueError: This tokenizer does not have a mask token which is necessary for masked language modeling. You should pass `mlm=False` to train on causal language modeling instead."
14974,b'How to save the fine-tuned model',2021-12-29T08:35:53Z,2022-02-06T15:01:37Z,,,
14973,b'Issue with Jiva/xlm-roberta-large-it-mnli',2021-12-29T08:35:53Z,2022-01-03T16:41:57Z,,OSError,"OSError: Can't load config for 'Jiva/xlm-roberta-large-it-mnli'. Make sure that:"
14972,b'Ability to save model outcomes in tabular format CSV file?',2021-12-29T07:42:11Z,2022-02-10T15:06:31Z,,,
14971,b'[Request] PerceiverForTokenClassification for NER task',2021-12-29T05:44:46Z,2022-02-06T15:01:38Z,,,
14970,b'Replace assertion with exception',2021-12-29T04:28:53Z,2021-12-29T15:09:54Z,,,
14969,b'Documentation in CTRL linksto 404',2021-12-28T17:52:21Z,2022-03-07T08:53:50Z,,,
14968,b'GPT-2 generate degenerates into producing garbage after a while',2021-12-28T16:55:43Z,2021-12-29T15:27:36Z,,,
14967,b'Update run_speech_recognition_seq2seq.py (max_eval_samples instead of train_samples)',2021-12-28T15:25:11Z,2022-01-06T16:26:45Z,,,
14966,b'huggingface_pytorch-pretrained-bert_bert.ipynb -- RuntimeError: Cannot find callable bertTokenizer in hubconf',2021-12-28T15:10:55Z,2022-02-06T15:01:39Z,,RuntimeError,"RuntimeError: Cannot find callable bertTokenizer in hubconf"
14965,b'[Speech Recognition Examples] Update README.md',2021-12-28T12:33:58Z,2021-12-28T12:41:27Z,,,
14964,b'[Tests] Speed up tokenizer tests',2021-12-28T11:37:27Z,2021-12-28T16:02:50Z,,,
14963,"b""Add 'with torch.no_grad()' to BertGeneration integration test forward passes""",2021-12-28T10:44:58Z,2022-01-06T15:39:13Z,,,
14962,b'[Speech recognition examples] num_processing_workers not allowed to be set',2021-12-28T10:31:48Z,2022-01-31T10:14:01Z,,ValueError,"ValueError: External features info don't match the dataset:"
14961,"b""Add 'with torch.no_grad()' to BEiT integration test forward passes""",2021-12-28T10:16:52Z,2022-01-31T20:12:11Z,,,
14960,b'Missing parameters when iterating over `module.parameters()`',2021-12-28T10:06:35Z,2021-12-28T18:54:51Z,,,
14959,"b""[Wav2Vec2] Rename model's feature extractor to feature encoder""",2021-12-28T09:36:22Z,2021-12-28T19:33:23Z,,,
14958,b'[WavLM] give model more precision tolerance in tests',2021-12-28T09:33:04Z,2021-12-28T10:07:05Z,,,
14956,b'[megatron convert] PYTHONPATH requirements',2021-12-28T05:31:55Z,2022-01-05T09:09:52Z,,,
14955,b'[doc] :class: hunt',2021-12-28T00:01:22Z,2021-12-28T01:17:38Z,,,
14954,b'[doc] :obj: hunt',2021-12-27T23:11:13Z,2021-12-27T23:49:48Z,,,
14953,b'Doc styler examples',2021-12-27T22:44:00Z,2021-12-28T00:07:46Z,,,
14952,b'Convert last rst file',2021-12-27T21:53:35Z,2021-12-27T22:09:37Z,,,
14951,b'[doc] consistent True/False/None default format',2021-12-27T21:12:36Z,2021-12-27T22:31:40Z,,,
14950,b'Doc styler v2',2021-12-27T20:57:42Z,2021-12-27T21:31:21Z,,,
14949,"b'[doc] post-conversion: incosistent ""default to""'",2021-12-27T20:13:10Z,2021-12-27T22:31:40Z,,,
14948,"b""[deepspeed] saving checkpoint fallback when fp16 weights aren't saved""",2021-12-27T18:59:07Z,2022-01-28T19:05:47Z,,,
14947,b'Improve truncation_side',2021-12-27T18:36:28Z,2022-01-03T15:18:39Z,,,
14946,b'Fix duplicate call to save_checkpoint when using deepspeed',2021-12-27T17:52:26Z,2021-12-27T19:25:27Z,DeepSpeed,,
14945,b'Unable to see total_flos in ViT training logs',2021-12-27T16:29:45Z,2021-12-28T10:02:46Z,,,
14944,b'Map model_type and doc pages names',2021-12-27T16:28:23Z,2022-01-03T10:08:56Z,,,
14943,b'Different evaluation results ',2021-12-27T12:12:50Z,2022-02-04T15:03:12Z,,,
14942,"b'when I use ""convert_pytorch_checkpoint_to_tf"", I meet some problems'",2021-12-27T10:53:38Z,2022-02-04T15:03:13Z,,,
14941,b'Enabling `tokenizers` upgrade.',2021-12-27T10:04:57Z,2021-12-30T16:30:58Z,,,
14940,b'Fix duplicate call to save_checkpoint when using deepspeed ',2021-12-27T08:57:56Z,2021-12-27T18:44:39Z,,,
14939,b'Cannot Convert Megatron GPT checkpoint',2021-12-27T08:46:35Z,2022-01-05T09:09:52Z,,ModuleNotFoundError,"ModuleNotFoundError: No module named 'megatron.model.enums'"
14938,b'Question: Object of type EncoderDecoderConfig is not JSON serializable',2021-12-27T05:56:19Z,2022-01-04T12:19:47Z,,TypeError,"TypeError: Object of type EncoderDecoderConfig is not JSON serializable"
14937,b'Cannot instantiate model under dopamine',2021-12-27T03:46:05Z,2022-02-06T15:01:41Z,,ValueError,"ValueError: in user code:"
14936,b'A warning is raised when using DistributedDataParallel of PyTorch',2021-12-27T03:21:29Z,2022-02-04T15:03:14Z,,,
14935,b'[performance doc] Power and Cooling',2021-12-27T03:00:05Z,2022-01-10T17:21:05Z,,,
14934,b'[benchmark tool] trainer-benchmark.py',2021-12-27T01:35:59Z,,"Performance, WIP",,
14933,b'Add parameters to make custom backbone for detr',2021-12-26T14:54:53Z,2022-02-02T15:06:19Z,,,
14932,b'Flax wav2vec2 pretrain',2021-12-26T13:48:45Z,2022-03-02T15:03:58Z,,,
14931,b'AutoTokenizer hash value got change after datasets.map',2021-12-26T11:48:42Z,,,,
14930,b'fix to issue #14833 in data_collator - consider no labels',2021-12-26T10:44:42Z,2021-12-27T16:48:48Z,,,
14929,b'VQA model inferences',2021-12-26T02:25:39Z,2022-01-08T09:40:37Z,,,
14928,b'[WIP] Fast tokenizer for debertaV2',2021-12-26T00:56:42Z,,,,
14926,b'Facing Problems with RobertaForSequenceClassification.from_pretrained()',2021-12-25T09:19:55Z,2021-12-25T11:14:41Z,,requests.exceptions.HTTPError,"requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/api/models/downloads/roberta-classifier/dataset-eval/config.json``"
14923,b'[WIP] DeBERTav2 Fast Tokenizer - fixes #14712',2021-12-25T04:44:27Z,2021-12-27T03:23:01Z,,,
14922,b'Using Huggingface Trainer in Colab -> Disk Full',2021-12-25T01:18:14Z,2022-02-01T15:06:46Z,,,
14921,b'Inference API: return token scores for text-generation models',2021-12-24T18:22:41Z,2022-02-08T15:10:01Z,,,
14920,b'Use tqdm.auto in Pipeline docs',2021-12-24T16:41:10Z,2022-01-10T15:28:35Z,,,
14919,b'Multiprocessing for pipeline',2021-12-24T10:42:48Z,2022-02-07T15:07:18Z,,,
14918,"b""Why I met Type 'seq(tensor(int64))' of operator (MemcpyFromHost) is invalid when using onnxruntime.InferenceSession() in GPU, and How to resolve it? On emergency hold\xef\xbc\x8cthanks!""",2021-12-24T07:58:20Z,2022-01-31T15:06:25Z,,,
14917,b'Fix Perceiver docs',2021-12-24T07:37:04Z,2021-12-24T10:28:47Z,,,
14916,"b'Add Deepspeed Transformer kernel, but encounter error  IndexError: tuple index out of range'",2021-12-24T07:30:28Z,2021-12-27T03:00:34Z,,"IndexError, RuntimeError","IndexError: tuple index out of rangeRuntimeError: CUDA error: an illegal memory access was encountered"
14915,b'ConnectionError',2021-12-24T04:24:18Z,2022-01-31T15:06:26Z,,,
14914,b'How to update config after model inited?',2021-12-24T03:56:35Z,2022-02-01T15:06:47Z,,,
14913,b'[Benchmark] Deepspeed +fp16/bf16 on a 8xA100 node',2021-12-23T21:22:28Z,2022-02-01T15:06:48Z,,,
14912,b'[doc] install - add link to jax installation ',2021-12-23T20:37:26Z,2021-12-23T21:12:59Z,,,
14911,b'How to generate output using custom embeddings?',2021-12-23T19:40:59Z,2022-01-31T15:06:27Z,,,
14910,b'[WavLM] fix wavlm docs',2021-12-23T19:29:30Z,2021-12-23T22:17:21Z,,,
14909,"b""remove absl workaround as it's no longer needed""",2021-12-23T18:41:42Z,2021-12-29T22:18:04Z,,,
14908,"b""Can't load tokenizer for 'microsoft/wavlm-base' when using Wav2Vec2Processor as in docs""",2021-12-23T18:06:39Z,2021-12-23T22:17:20Z,,OSError,"OSError: Can't load tokenizer for 'microsoft/wavlm-base'. Make sure that:"
14907,b'[jax] absl issues',2021-12-23T17:54:43Z,2021-12-31T06:12:21Z,,,
14906,b'Better logic for getting tokenizer config in AutoTokenizer',2021-12-23T17:28:35Z,2021-12-23T19:18:08Z,,,
14905,b'Generate does not take into account config.decoder.eos_token_id',2021-12-23T17:18:02Z,,Good First Issue,,
14904,b'Update ONNX docs',2021-12-23T17:17:32Z,2022-01-11T17:06:05Z,,,
14903,b'Fix failing GPU trainer tests',2021-12-23T17:03:19Z,2021-12-23T18:59:33Z,,,
14902,b'[Tests] Update speech diarization and WavLM tolerances',2021-12-23T16:45:52Z,2021-12-23T16:53:56Z,,,
14901,"b'Adding tokens to pretrained model ""Helsinki-NLP/opus-tatoeba-en-ja"" using tokens from vietnamese not working'",2021-12-23T16:17:22Z,2022-01-31T15:06:28Z,,,
14900,b'[AutoTokenizer] Fix incorrect from pretrained',2021-12-23T16:14:54Z,2021-12-23T16:22:33Z,,,
14899,b'GPT-J: Implement Memory Efficient Attention',2021-12-23T16:01:23Z,2022-01-31T15:06:30Z,,,
14898,b'Add 3D attention_mask input support',2021-12-23T14:02:35Z,2022-02-13T15:01:50Z,,,
14897,b'add custom stopping criteria to human eval script',2021-12-23T13:08:53Z,2021-12-23T13:59:12Z,,,
14896,b'Large audio chunking for the existing ASR pipeline',2021-12-23T12:39:15Z,2022-01-03T15:54:17Z,,,
14895,b'Add ViLT',2021-12-23T11:11:40Z,2022-01-19T18:52:00Z,,,
14894,b'Set `run_name` in MLflowCallback',2021-12-23T09:29:36Z,2021-12-23T15:53:33Z,,,
14893,b'support the trocr small models',2021-12-23T07:42:06Z,2022-01-10T14:28:04Z,,,
14892,b'[doc] bug in docstring conversion',2021-12-23T04:13:42Z,2021-12-23T18:20:35Z,,,
14891,b'Turn of do_sample for T0pp in Inference API',2021-12-23T04:01:43Z,2021-12-24T14:44:09Z,,,
14890,b'[doc] post-porting',2021-12-23T03:52:52Z,2021-12-23T18:19:34Z,,,
14889,b'[logging] unable to turn off tqdm logging',2021-12-23T02:25:13Z,2022-01-18T22:52:35Z,,,
14888,b'Convert rst files',2021-12-22T20:27:27Z,2021-12-22T21:14:35Z,,,
14887,b'Properly indent return block',2021-12-22T17:26:03Z,2021-12-22T17:28:45Z,,,
14886,"b'Running MLM pretraining with not ""line_by_line"" big dataset'",2021-12-22T16:41:55Z,2022-01-30T15:01:41Z,,,
14885,b'Fix installation instructions for BART ONNX example',2021-12-22T16:29:43Z,2021-12-23T09:05:32Z,,,
14884,b'TrOCR processor cannot be loaded from AutoProcessor',2021-12-22T14:40:51Z,2022-02-23T15:07:39Z,,KeyError,"KeyError: <class 'transformers.models.vision_encoder_decoder.configuration_vision_encoder_decoder.VisionEncoderDecoderConfig'"
14883,b'Fix pytorch image classification example',2021-12-22T13:31:23Z,2021-12-22T13:42:19Z,,,
14882,b'core dumps run_onnx_exporter.py in gpu.',2021-12-22T11:39:35Z,2022-02-23T15:07:40Z,,,
14881,b'[AutoProcessor] Correct AutoProcessor and automatically add processor\xe2\x80\xa6',2021-12-22T11:38:26Z,2021-12-30T08:56:43Z,,,
14880,b'Add (M)Luke model training for Token Classification in the examples',2021-12-22T10:50:34Z,2022-01-31T12:58:18Z,,,
14879,b'Fix Perceiver code example',2021-12-22T09:34:10Z,2021-12-22T13:18:04Z,,,
14878,b'Model trains on 1 node 8xA100 but hits OOM in 4 nodes 8xA100',2021-12-22T05:42:29Z,2021-12-29T22:09:55Z,,,
14877,b'dataset of Helsinki-NLP/opus-mt-en-zh',2021-12-22T03:50:36Z,2022-01-30T15:01:42Z,,,
14876,b'Add XGLM models',2021-12-22T02:13:13Z,2022-01-28T17:55:24Z,WIP,,
14875,b'Add `in_chans` to `DetrModel`',2021-12-22T01:28:13Z,2022-02-26T15:01:54Z,,,
14874,b'Fix doc mistakes',2021-12-21T23:35:40Z,2021-12-21T23:54:42Z,,,
14873,b'Fix `FlaxMarianMTModel` return block.',2021-12-21T22:40:38Z,2021-12-21T22:57:37Z,,,
14872,b'Fixes in marian doc',2021-12-21T22:11:43Z,2021-12-21T22:17:03Z,,,
14871,b'Fix FLAX_MULTIPLE_CHOICE_SAMPLE typo',2021-12-21T21:44:06Z,2021-12-21T21:54:10Z,,,
14870,b'Error while reproducing example for PerceiverForMultimodalAutoencoding',2021-12-21T20:59:11Z,2021-12-22T13:18:04Z,,RuntimeError,"RuntimeError: The expanded size of the tensor (833) must match the existing size (831) at non-singleton dimension 2.  Target sizes: [1, 704, 833].  Tensor sizes: [1, 831]"
14869,b'Fine-tuning Wav2Vec2: Concern that pretrained weights are being reinitialized',2021-12-21T19:36:07Z,2022-01-05T13:16:03Z,,,
14868,b'Adds IBERT to models exportable with ONNX',2021-12-21T18:31:35Z,2022-01-11T11:17:08Z,,,
14867,b'Keras metric callback',2021-12-21T17:18:11Z,2021-12-22T20:35:39Z,,,
14866,b'Mass conversion of documentation from rst to Markdown',2021-12-21T17:04:15Z,2021-12-21T20:06:33Z,,,
14865,b'Convert model files from rst to mdx',2021-12-21T16:58:22Z,2021-12-22T08:27:30Z,,,
14864,b'Add Flax image captioning example',2021-12-21T16:51:12Z,2022-01-06T13:00:54Z,,,
14863,b'Open discussion for design decisions.',2021-12-21T16:05:51Z,2021-12-22T13:33:51Z,,,
14862,b'Cache the files in get_fast_tokenizer_file()',2021-12-21T14:10:10Z,2022-02-20T15:02:08Z,,,
14861,b'[Wav2vec2] RuntimeError: CUDA error: an illegal memory access was encountered',2021-12-21T07:25:24Z,2021-12-22T17:07:43Z,,RuntimeError,"RuntimeError: CUDA error: an illegal memory access was encountered"
14860,b'Huggingface Transformers fastTokenizer for DeBERTa v3',2021-12-21T04:33:09Z,2021-12-22T08:24:41Z,,`NotImplementedError,"`NotImplementedError: return_offset_mapping is not available when using Python tokenizers.To use this feature, change your tokenizer to one deriving from transformers.PreTrainedTokenizerFast.`"
14859,b'A potential bug in ModuleUtilsMixin.get_extended_attention_mask',2021-12-21T03:55:14Z,,,,
14858,b'[doc porting] several docs',2021-12-21T02:51:34Z,2021-12-21T17:55:26Z,,,
14857,b'Only create the model card on process 0',2021-12-21T02:49:56Z,2021-12-21T11:34:47Z,,,
14856,b'[Generate] Remove attention_mask and integrate model_main_input_name',2021-12-20T22:24:34Z,2021-12-23T18:43:37Z,,,
14855,b'Make the onnx submodule init lazy',2021-12-20T20:48:48Z,2021-12-21T08:11:26Z,,,
14854,b'[Bart] better error message',2021-12-20T19:22:09Z,2021-12-21T10:57:42Z,,,
14853,b'replace native python deprecated floor function by torch version ',2021-12-20T19:11:18Z,2022-02-01T11:10:37Z,,,
14852,b'Replace commit sha by commit url for update jobs',2021-12-20T18:58:15Z,2021-12-21T16:17:12Z,,,
14851,b'Fine-tune GPT-J with SageMaker Model Parallelism ',2021-12-20T18:04:16Z,2022-03-20T15:02:14Z,,,
14850,b'Convert docstrings of modeling files',2021-12-20T17:45:35Z,2021-12-21T10:37:32Z,,,
14849,b'[doc] typo',2021-12-20T16:26:25Z,2021-12-20T17:20:21Z,,,
14848,b'[ASR example] Improve example + add more examples',2021-12-20T12:51:43Z,2021-12-21T12:12:22Z,,,
14847,b'Add SD and SV heads for WavLM',2021-12-20T12:30:19Z,2021-12-20T13:40:56Z,,,
14846,b'Train Bart-Large multi-labels',2021-12-20T10:43:11Z,2022-01-28T15:02:43Z,,,
14845,b'[WavLM] Fix slow tests',2021-12-20T10:34:53Z,2021-12-20T11:06:43Z,,,
14844,b'inconsistent BertTokenizer and BertTokenizerFast',2021-12-20T09:03:55Z,,,,
14843,b'T5ForConditionalGeneration lm_head not initialized from pretrained checkpoint',2021-12-20T07:09:20Z,2021-12-20T21:29:06Z,,,
14842,b'Fix dead link to benchmarks.ipynb',2021-12-20T06:10:06Z,2021-12-20T14:08:05Z,,,
14841,b'Unable to save trained model in path',2021-12-20T05:32:04Z,2021-12-27T13:54:09Z,,,
14840,b'trainer.create_model_card should be run by process 0 only in distributed training',2021-12-20T04:06:10Z,2021-12-21T11:34:47Z,,,
14839,b'Fine-tuning GPT-J-6B in colab: 8-bit weights with low-rank adaptors',2021-12-19T23:53:52Z,,"New model, Quantization",,
14838,b'Subword Tokenization Bug after Non-Space Word Boundaries (AlbertTokenizerFast)',2021-12-19T23:22:21Z,2021-12-23T01:06:47Z,,,
14837,b'Adding S4 Model',2021-12-19T20:03:38Z,2022-03-20T15:02:15Z,,,
14836,b'Segmentation fault (core dumped) when conversion of GPT-J to onnx',2021-12-19T17:39:23Z,2022-01-20T13:15:07Z,,,
14835,b'Update CONTRIBUTING.md',2021-12-19T16:57:03Z,2021-12-20T13:42:03Z,,,
14834,b'eval_loss is nan for GPT2 trained with fp16 + deepseed on 8xA40s',2021-12-19T15:59:33Z,2021-12-21T01:35:26Z,,,
14833,b'Seq2SeqTrainer.evaluation_loop requires `labels` due to DataCollatorForSeq2Seq',2021-12-19T14:21:57Z,2022-01-27T15:06:29Z,,,
14832,b'Fix the wrong attention mask in TransformerXL by shifting `mask_shift_len` with 1',2021-12-19T08:25:43Z,,,,
14831,"b""FlaxVisionEncoderDecoderModel has no attribute 'from_encoder_decoder_pretrained' ?""",2021-12-19T06:47:45Z,2021-12-20T16:05:23Z,,`AttributeError,"`AttributeError: type object 'FlaxVisionEncoderDecoderModel' has no attribute 'from_encoder_decoder_pretrained'`"
14830,b'Adafactor lacking min_dim_size_to_factor',2021-12-19T04:32:29Z,2022-01-27T15:06:30Z,,,
14829,"b""[Wav2Vec2 Phoneme] Let phonemizer lang default to tokenizer's settings""",2021-12-19T00:59:06Z,2021-12-20T10:47:32Z,,,
14828,"b""Unable to save pretrained model after finetuning :  trainer.save_pretrained(modeldir) AttributeError: 'Trainer' object has no attribute 'save_pretrained'""",2021-12-18T23:02:55Z,2021-12-19T17:54:17Z,,,
14827,b'as_target_tokenizer is not an attribute of PreTrainedTokenizerBase and Bart/RobertaTokenizer',2021-12-18T19:17:46Z,2022-01-27T15:06:31Z,,AttributeError,"AttributeError: 'RobertaTokenizer' object has no attribute 'as_target_tokenizer'"
14826,b'Add support for TPU training with CodeParrot!',2021-12-18T15:35:46Z,2022-02-08T15:10:06Z,,,
14825,b'Flax/Roberta - Tokenizer',2021-12-18T12:02:26Z,,,,
14824,b'preprocessing_num_workers coredump',2021-12-18T11:45:19Z,2022-01-25T15:06:31Z,,,
14823,b'How to use Roberta as the Encoder and a randomly initialized TransformerDecoder as the Decoder?',2021-12-18T08:56:36Z,2021-12-21T02:17:08Z,,,
14822,b'fp16 flag silently fails',2021-12-18T02:17:29Z,2022-01-25T15:06:32Z,,,
14821,"b""Add 'with torch.no_grad()' to DeBERTa integration test forward pass""",2021-12-18T00:34:37Z,2021-12-20T14:25:34Z,,,
14820,"b""Add 'with torch.no_grad()' to BERT integration test forward pass""",2021-12-18T00:05:32Z,2021-12-20T14:28:17Z,,,
14819,b'RFC: Integrating bitsandbytes 8-bit optimizer / adding Embedding Norm',2021-12-17T22:14:19Z,,WIP,,
14818,b'a',2021-12-17T21:35:14Z,2021-12-17T22:07:53Z,,,
14817,b'Roberta Classification Head',2021-12-17T21:32:23Z,2021-12-20T15:04:17Z,,,
14816,b'[examples/summarization] deal with None in data records',2021-12-17T19:22:59Z,2021-12-21T17:17:28Z,,TypeError,"TypeError: can only concatenate str (not ""NoneType"") to str"
14815,b'[Generate] Correct input_ids detection',2021-12-17T14:20:55Z,2021-12-17T15:08:54Z,,,
14814,b'Support on Mixture of expert models',2021-12-17T14:05:20Z,2022-01-24T15:02:21Z,New model,,
14813,b'[Perceiver] Skip multi-gpu tests for now',2021-12-17T13:21:54Z,2021-12-20T14:22:50Z,,,
14812,b'Enable ONNX export for `VisionDecoderEncoderModel`',2021-12-17T12:01:58Z,2022-02-02T15:06:24Z,,,
14811,b'[WavLM] Layerdrop is not allowed for first layer',2021-12-17T11:00:37Z,2021-12-17T12:30:18Z,,,
14810,b'Fix Perceiver multi GPU test',2021-12-17T09:00:10Z,2021-12-17T09:33:27Z,,,
14809,"b""How does decoder's weight shared with input embeddings?""",2021-12-17T03:45:02Z,2022-01-24T15:02:23Z,,,
14808,"b""Add 'with torch.no_grad()' to ALBERT integration test forward pass""",2021-12-16T22:34:36Z,2021-12-23T09:23:39Z,,,
14807,b'[WIP] Get started docs',2021-12-16T22:28:25Z,2022-01-10T17:02:07Z,Documentation,,
14806,b'Convert rst to mdx bert',2021-12-16T21:40:29Z,2021-12-17T16:13:34Z,,,
14805,b'[WavLM] Correct position bias computation',2021-12-16T21:33:30Z,2021-12-16T21:42:58Z,,,
14804,b'[Benchmark] google/pegasus-wikihow',2021-12-16T20:11:39Z,2022-01-24T15:02:24Z,,,
14803,b'Add a main_input_name attribute to all models',2021-12-16T19:50:58Z,2021-12-20T16:19:08Z,,,
14802,b'[Seq2SeqTrainer] Remove model input name hack',2021-12-16T18:19:25Z,2021-12-20T09:53:49Z,,,
14801,b'[ImageGPT] Deprecate pixel_values input name to input_ids',2021-12-16T17:32:02Z,2021-12-17T19:05:23Z,,,
14800,b'Update CONTRIBUTING.md',2021-12-16T15:34:54Z,2021-12-16T15:40:57Z,,,
14799,b'Update CONTRIBUTING.md',2021-12-16T14:39:29Z,2021-12-16T15:24:26Z,,,
14798,b'CUDA error: device-side assert triggered while training Marian MT ',2021-12-16T14:31:22Z,2022-01-23T15:02:57Z,,RuntimeError,"RuntimeError: CUDA error: device-side assert triggered"
14797,b'Unable so save quantized model for mobile using Speech2Text',2021-12-16T14:25:28Z,2022-01-23T15:02:58Z,,torch.jit.frontend.UnsupportedNodeError,"torch.jit.frontend.UnsupportedNodeError: GeneratorExp aren't supported:"
14796,b'Train step fix',2021-12-16T14:23:50Z,2021-12-16T16:08:14Z,,,
14795,b'Remove `require_datasets` testing utility ',2021-12-16T13:48:48Z,2021-12-16T19:34:15Z,,,
14794,b'some error when I run the pytorch example wav2vec2 by rum_common_voice.py',2021-12-16T13:25:29Z,2021-12-17T01:52:54Z,New model,requests.exceptions.HTTPError,"requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/api/models/facebookav2vec2-base/revision/main"
14793,b'Can I  print mlm and nsp loss instead of their sum while train the BertForPretraning model?',2021-12-16T12:58:25Z,2022-01-23T15:02:59Z,,,
14792,b'Add Speech Seq2Seq Training script',2021-12-16T11:46:34Z,2021-12-28T09:20:51Z,,,
14791,b'Eval rouge = 100.0 on gem/wiki_lingua_english_en',2021-12-16T05:39:32Z,2022-01-23T15:03:00Z,,,
14790,b'Post sphinx-clean up and contributing guide updates',2021-12-15T20:31:09Z,2021-12-16T14:29:27Z,,,
14789,b'Add tqdm to pipeline',2021-12-15T20:22:44Z,2022-01-23T15:03:01Z,,,
14788,b'Fix the build documentation job',2021-12-15T19:26:48Z,2021-12-16T14:35:20Z,,,
14787,b'Move import to avoid circular import',2021-12-15T18:33:37Z,2021-12-15T18:34:42Z,,,
14786,b'Improve Perceiver docs',2021-12-15T16:53:50Z,2021-12-15T17:02:06Z,,,
14785,b'Share custom pipelines on Huggingface Hub',2021-12-15T16:23:11Z,2022-02-19T15:02:20Z,,,
14784,b'[Generate] Make generate multi-modal',2021-12-15T16:16:34Z,2021-12-16T17:03:55Z,,,
14783,b'Update Perceiver code examples',2021-12-15T15:18:58Z,2021-12-15T16:06:38Z,,,
14782,b'Support Tensorflow tensors as input in tokenizers - [Ongoing]',2021-12-15T14:32:10Z,2022-01-11T20:41:30Z,,,
14781,b'Removes images to put them in a dataset',2021-12-15T14:13:22Z,2021-12-16T09:42:02Z,,,
14780,"b""Fix the value error typo of AdamW's betas' valid values checking""",2021-12-15T11:47:57Z,2021-12-21T14:44:09Z,,,
14779,b'Add custom `stopping_criteria` and  `logits_processor` to `generate`',2021-12-15T11:28:36Z,2021-12-21T15:47:41Z,,,
14778,b'Nan when training LayoutLM_V2 Model',2021-12-15T10:50:16Z,2022-01-23T15:03:03Z,,,
14777,b'Added forward pass of test_inference_image_classification_head ',2021-12-15T08:02:31Z,2022-01-17T12:22:42Z,,,
14776,b'Simplify T5 docs',2021-12-15T04:11:42Z,2021-12-15T13:59:12Z,,,
14775,b'Errors in running Perceiver example with transformers-4.14.0.dev0',2021-12-15T03:50:13Z,2021-12-15T16:06:38Z,,ValueError,"ValueError: Expected input batch_size (2048) to match target batch_size (33)."
14774,b'Fix the doc_build_test job',2021-12-15T03:07:31Z,2021-12-15T08:40:17Z,,,
14773,b'Failed to import transformers.trainer',2021-12-15T02:18:57Z,2021-12-15T17:47:01Z,,"ImportError, RuntimeError","ImportError: /opt/conda/lib/python3.8/site-packages/amp_C.cpython-38-x86_64-linux-gnu.so: undefined symbol: _ZN3c105ErrorC2ENS_14SourceLocationENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERuntimeError: Failed to import transformers.trainer because of the following error (look up to see its traceback):"
14772,b'A bug in T5LayerNorm',2021-12-15T01:50:03Z,2021-12-16T02:29:53Z,,,
14771,b'Update Using AWS Inferentia to run HuggingFace TorchScript model',2021-12-15T00:23:07Z,2022-01-07T07:44:26Z,,,
14770,b'Adding new tokens to various models changes tokenization of adjacent elements in strings',2021-12-14T19:48:25Z,2022-01-24T15:02:26Z,,,
14769,b'Fix preprocess_function in run_summarization_flax.py',2021-12-14T17:17:18Z,2021-12-15T10:36:28Z,,,
14768,b'Length penalty for beam search',2021-12-14T15:45:30Z,2021-12-14T16:17:12Z,,,
14767,b'Is there a way to batch examples by max number of tokens using tokenizers and datasets?',2021-12-14T15:08:20Z,2022-01-14T15:06:45Z,,,
14766,b'Nan when training LayoutLM_V2 Model',2021-12-14T14:34:44Z,2021-12-15T10:48:24Z,,,
14765,b'Adding documentation on how to overload `SacreBLEU` arguments',2021-12-14T13:47:27Z,2021-12-15T08:41:13Z,,,
14764,b'finetune_wav2vec2_xlsr_turkish.sh can not find wav',2021-12-14T13:44:25Z,2022-01-22T15:01:44Z,,RuntimeError,"RuntimeError: Error loading audio file: failed to open file."
14763,b'Question about how to modify token embedding before sending to bert',2021-12-14T11:39:34Z,2021-12-15T01:04:45Z,,,
14762,b'make docs failing',2021-12-14T11:34:58Z,2021-12-16T14:29:26Z,,,
14761,"b'TypeError: stat: path should be string, bytes, os.PathLike or integer, not NoneType'",2021-12-14T10:03:39Z,2022-01-22T15:01:45Z,,**Error,"**Error: TypeError: stat: path should be string, bytes, os.PathLike or integer, not NoneType**"
14760,b'Addition of Swin Transformer for Computer Vision',2021-12-14T09:46:26Z,2022-01-21T11:10:42Z,New model,,
14759,b'KeyError: 337 when training a hugging face model using pytorch',2021-12-14T06:37:40Z,2022-02-28T15:08:38Z,,KeyError,KeyError: 337**
14758,b'SacreBLEU uses incorrect tokenizer for Japanese',2021-12-14T02:36:39Z,2021-12-14T10:15:03Z,,,
14757,b'[doc] performance: groups of operations by compute-intensity',2021-12-14T00:37:16Z,2021-12-15T03:01:23Z,,,
14756,b'Add an argument to set bucket_cap_mb for PyTorch DDP',2021-12-13T22:57:46Z,2021-12-20T13:41:40Z,,,
14755,b'Update Table of Contents',2021-12-13T22:04:26Z,2021-12-13T22:15:19Z,,,
14754,b'PoC for conserving old links',2021-12-13T19:28:39Z,2021-12-15T19:40:47Z,,,
14753,b'Convert Trainer doc page to MarkDown',2021-12-13T17:32:00Z,2021-12-13T18:09:50Z,,,
14752,b'update the arguments `add_prefix_space` and `trim_offsets` in `backend_tokenizer.post_processor` of `RobertaTokenizerFast`',2021-12-13T17:20:01Z,2021-12-22T09:51:55Z,,,
14751,b'Small fixes for the doc',2021-12-13T16:12:14Z,2021-12-13T16:17:01Z,,,
14750,b'Improve perceiver',2021-12-13T15:47:55Z,2021-12-13T17:46:49Z,,,
14749,b'Feature/fix slow test in mluke',2021-12-13T14:29:36Z,2021-12-22T11:35:59Z,,,
14748,b'Fix the perceiver docs',2021-12-13T14:11:12Z,2021-12-13T14:29:47Z,,,
14747,b'Update docs',2021-12-13T13:42:39Z,2022-01-20T15:06:04Z,,,
14746,b'Change how to load config of XLNetLMHeadModel',2021-12-13T13:17:41Z,2021-12-13T17:34:27Z,,,
14745,b'Skip Perceiver tests',2021-12-13T13:08:12Z,2021-12-13T13:13:39Z,,,
14744,b'Deprecates AdamW and adds `--optim`',2021-12-13T12:32:01Z,2022-01-13T16:14:52Z,,,
14743,b'Batch size affecting output when using GPT2Model',2021-12-13T10:35:51Z,2022-03-07T15:05:09Z,,,
14742,b'Swap TF and PT code inside two blocks',2021-12-13T09:43:05Z,2021-12-13T15:31:11Z,,,
14741,b'TF and PT code confuse in the documentation ',2021-12-13T09:35:57Z,2021-12-13T15:31:11Z,,,
14740,b'add support to DistilBertLMHeadModel',2021-12-13T09:32:07Z,2022-01-20T15:06:06Z,,,
14739,b'Fixing tests for Perceiver',2021-12-13T09:31:30Z,2021-12-14T08:43:07Z,,,
14738,b'Mention no images added to repository',2021-12-13T08:50:53Z,2021-12-13T17:21:26Z,,,
14737,b'Add support to DistilBertLMHeadModel',2021-12-13T00:49:54Z,2022-01-20T15:06:07Z,,,
14736,"b""XLNetLMHeadModel has no attribute 'from_config'""",2021-12-12T22:01:29Z,2021-12-13T17:34:26Z,,AttributeError,"AttributeError: type object 'XLNetLMHeadModel' has no attribute 'from_config'"
14735,b'Avoid using tf.tile in embeddings for TF models',2021-12-12T16:42:13Z,2021-12-13T17:30:46Z,,,
14734,b'Fix: change tooslow to slow',2021-12-12T15:35:57Z,2021-12-13T16:12:58Z,,,
14733,b'Do we have pretrain (from scratch) scripts for BART? ',2021-12-12T13:14:43Z,2021-12-12T13:21:34Z,,,
14732,b'Add ability to get a list of supported pipeline tasks',2021-12-12T00:46:05Z,2021-12-13T13:31:51Z,,,
14731,b'Possible simplification in T5 docs',2021-12-11T22:05:55Z,2021-12-15T13:59:11Z,,,
14730,b'Fix docs pointers for DeepSpeed',2021-12-11T19:07:11Z,2021-12-13T18:09:50Z,,,
14729,b'Add `ElectraForCausalLM` -> Enable Electra encoder-decoder model',2021-12-11T19:03:09Z,2021-12-27T11:37:52Z,,,
14728,b'Include documentation on linking to transformers docs with Intersphinx',2021-12-11T14:05:35Z,2022-01-18T15:07:17Z,,,
14727,b'Difference between vocab_size in model T5forConditionalGeneration \xe2\x80\x9ct5-small\xe2\x80\x9d and its corresponding Tokenizer \xe2\x80\x9ct5-small\xe2\x80\x9d',2021-12-11T05:15:06Z,2021-12-12T13:43:23Z,,,
14726,b'[CI/pt-nightly] switch to cuda-11.3',2021-12-11T01:16:46Z,2021-12-13T14:53:48Z,,,
14725,b'[doc] document MoE model approach and current solutions',2021-12-11T00:24:13Z,2021-12-11T02:24:38Z,,,
14724,b'Update transformers metadata',2021-12-10T23:24:50Z,2021-12-13T16:46:04Z,,,
14723,b'Add Speaker Diarization and Verification heads',2021-12-10T23:19:46Z,2021-12-16T16:22:14Z,,,
14722,b'Fix broken links to distillation on index page of documentation',2021-12-10T22:17:25Z,2021-12-15T02:55:33Z,,,
14721,b'Fix special character in MDX',2021-12-10T20:35:14Z,2021-12-10T21:02:49Z,,,
14720,b'TF model cards',2021-12-10T20:19:29Z,2021-12-15T14:57:53Z,,,
14719,b'Fixing tests for perceiver (texts)',2021-12-10T18:33:27Z,2021-12-11T00:39:00Z,,,
14718,b'Automatically build doc notebooks',2021-12-10T17:28:14Z,2021-12-10T19:20:56Z,,,
14717,"b'Support for Kaldi formatted Audio files, especially ""segments""'",2021-12-10T17:02:07Z,2022-01-30T15:01:46Z,,,
14716,b'Adding support for multiple mask tokens.',2021-12-10T14:25:46Z,2021-12-14T15:46:16Z,,,
14715,b'Update bug-report.md',2021-12-10T14:08:32Z,2021-12-12T12:30:44Z,,,
14714,b'IterableDatasetShard should use per device batch size instead of real\xe2\x80\xa6',2021-12-10T13:40:13Z,2021-12-22T12:52:07Z,,,
14713,b'[Adafactor] Fix adafactor',2021-12-10T12:07:31Z,2021-12-12T12:31:46Z,,,
14712,b'DeBERTa V3 Fast Tokenizer',2021-12-10T11:48:12Z,,Good First Issue,,
14711,b'Adding `Perceiver` to `AutoTokenizer`.',2021-12-10T11:35:06Z,2021-12-10T14:29:19Z,,,
14710,b'GPT-NEO Incosistent inference TPU vs GPU',2021-12-10T05:15:33Z,2022-01-17T15:01:48Z,,,
14709,b'[WIP] add noisy-average new word embed init',2021-12-10T00:53:05Z,2022-01-17T15:01:49Z,,,
14708,b'[WIP] [performance doc] faster/leaner optimizers',2021-12-09T18:49:27Z,,WIP,,
14707,b'Erroneous 404 warning when using AutoTokenizer.from_pretrained',2021-12-09T18:12:40Z,,Good First Issue,,
14706,b'[chinese wwm] load_datasets behavior not as expected when using run_mlm_wwm.py script',2021-12-09T17:47:05Z,2021-12-24T09:57:14Z,,,
14705,b'Fix : wrong link in the documentation (ConvBERT vs DistilBERT)',2021-12-09T15:49:42Z,2021-12-09T16:35:23Z,,,
14704,b'Fix typo in toctree',2021-12-09T14:23:37Z,2021-12-09T14:25:32Z,,,
14703,b'Fix Perceiver tests',2021-12-09T12:49:57Z,2021-12-09T13:32:36Z,,,
14702,b'MarianForCausalLM doc example not working',2021-12-09T11:36:07Z,2022-02-20T15:02:12Z,,"TypeError, RuntimeError","TypeError: expected str, bytes or os.PathLike object, not NoneTypeRuntimeError: Error(s) in loading state_dict for MarianForCausalLM:"
14701,b'Fix doc examples: ... takes no keyword arguments',2021-12-09T10:21:47Z,2021-12-23T09:07:21Z,,,
14700,b'Onnx enable tasks for supported models (part 2)',2021-12-09T10:09:51Z,2021-12-22T13:43:11Z,,,
14699,b'Fix doc examples: KeyError',2021-12-09T10:02:40Z,2021-12-10T07:56:38Z,,,
14698,b'Fix doc examples: cannot import name',2021-12-09T09:41:29Z,2021-12-13T15:36:50Z,,ImportError,"ImportError: cannot import name 'UniSpeechSatFeatureExtractor' from 'transformers' (/home/ydshieh/Desktop/ydshieh/transformers/src/transformers/__init__.py)"
14697,b'Fix doc examples: modify config before super().__init__',2021-12-09T09:19:01Z,2021-12-13T11:50:03Z,,,
14696,b'Occasional Can not load roberta-base tokenizer ',2021-12-09T08:45:53Z,2022-01-23T15:03:05Z,,OSError,"OSError: Can't load config for 'roberta-base'. Make sure that:"
14695,b'Improve documentation of some models',2021-12-09T08:26:15Z,2021-12-13T12:24:36Z,,,
14694,b'The loss function should ignore tokens whose index is set to - 100',2021-12-09T08:26:02Z,2021-12-09T09:43:50Z,,,
14693,b'Multiple answers for QA ',2021-12-09T04:11:27Z,2022-01-16T15:02:05Z,,,
14692,b'LongformerTokenizer Error',2021-12-09T02:37:23Z,2021-12-09T05:20:42Z,,TypeError,"TypeError: 'LongformerTokenizer' object is not callable"
14691,b'Fix `AttributeError` from `PreTrainedTokenizerFast.decoder`',2021-12-08T23:00:25Z,2021-12-23T09:19:25Z,,AttributeError,"AttributeError: 'tokenizers.Tokenizer' object has no attribute '_tokenizer'"
14690,b'Make MLuke tokenizer tests slow',2021-12-08T20:49:22Z,2021-12-08T20:59:58Z,,,
14689,b'Fix doc examples: unexpected keyword argument',2021-12-08T20:34:59Z,2021-12-10T16:44:09Z,,,
14688,b'[trainer] support UserDict inputs (torch-nightly)',2021-12-08T20:15:39Z,2021-12-08T20:21:43Z,,,
14687,"b""Fix doc examples: name '...' is not defined""",2021-12-08T20:00:34Z,2021-12-08T21:39:36Z,,,
14686,b'Move pyctcdecode',2021-12-08T19:16:12Z,2021-12-08T20:41:58Z,,,
14685,b'Fix wrong checkpoint paths in doc examples',2021-12-08T19:11:29Z,2021-12-08T19:25:49Z,,,
14684,b'Put back open in colab markers',2021-12-08T18:55:29Z,2021-12-09T17:00:06Z,,,
14683,b'Revert open-in-colab and add perceiver',2021-12-08T18:52:24Z,2021-12-08T18:52:31Z,,,
14682,b'add str hub token to repository when provided else fallback to default',2021-12-08T18:51:25Z,2021-12-09T13:42:24Z,,,
14681,b'Fixes in init',2021-12-08T18:11:29Z,2021-12-08T18:42:22Z,,,
14680,b'Improvements to Comet Integration',2021-12-08T17:54:13Z,2021-12-08T18:39:10Z,,,
14679,"b'Revert ""Added support for other features for already supported models""'",2021-12-08T17:41:40Z,2021-12-08T18:04:41Z,,,
14678,"b""Fix doc examples: 'CausalLMOutput...' object has no attribute 'last_hidden_state'""",2021-12-08T17:07:04Z,2021-12-10T13:55:54Z,,,
14677,b'[Feature request] Doc example copy button - option to remove input prompts and outputs',2021-12-08T14:21:12Z,2021-12-08T14:52:18Z,,,
14676,b'Fix doc builder',2021-12-08T14:14:19Z,2021-12-08T14:14:37Z,,,
14675,b'[AutoProcessor] Add Wav2Vec2WithLM & small fix',2021-12-08T12:57:11Z,2021-12-08T14:51:28Z,,,
14674,b'Loading t5 from config file',2021-12-08T12:22:52Z,2021-12-13T08:09:04Z,,,
14673,b'Output sequences are different between SummarizationPipeline and model.generate',2021-12-08T09:41:24Z,2021-12-14T08:13:29Z,,,
14672,b'PT CausalLM models config issue',2021-12-08T08:55:19Z,2021-12-13T11:50:03Z,,AssertionError,"AssertionError: <class 'transformers.models.bart.modeling_bart.BartForCausalLM'> has to be configured as a decoder."
14671,b'fix deprecated tf method',2021-12-08T07:31:52Z,2021-12-08T13:43:21Z,,,
14670,b'tf.matrix_band_part have been deprecated',2021-12-08T06:53:37Z,2021-12-08T07:12:50Z,,,
14669,b'[logging] implement warning_advice / TRANSFORMERS_NO_ADVISORY_WARNINGS',2021-12-08T00:38:28Z,2021-12-21T04:48:39Z,,,
14668,b'Generate: Passing encoder_hidden_states in with num_beams > 1 raises an error',2021-12-08T00:29:29Z,2022-01-15T15:02:03Z,,RuntimeError,"RuntimeError: The size of tensor a (96) must match the size of tensor b (32) at non-singleton dimension 0"
14667,"b'Wrong link in the documentation (ConvBERT, BART, Fnet)'",2021-12-07T22:31:43Z,2021-12-09T16:35:23Z,,,
14666,b'Code parrot minor fixes/niceties',2021-12-07T21:18:57Z,2021-12-13T08:30:50Z,,,
14665,b'Convert tutorials',2021-12-07T21:06:32Z,2021-12-08T18:19:47Z,,,
14664,b'Small fix for GPT2OnnxConfig',2021-12-07T20:27:19Z,2021-12-08T15:30:04Z,,,
14663,b'[trainer] conditional ctx managers into one wrapper',2021-12-07T20:13:35Z,2021-12-07T21:04:19Z,,,
14662,b'Add a job to test doc building (for realsies this time)',2021-12-07T18:12:45Z,2021-12-09T12:01:03Z,,,
14661,b' fix: verify jsonlines file in run_translation (#14660)',2021-12-07T16:27:33Z,2021-12-08T18:25:31Z,,,
14660,b'Do we use JSON lines or JSON only for run_translation.py in PyTorch?',2021-12-07T16:24:02Z,2021-12-08T18:25:30Z,,,
14659,b'Add Nystromformer',2021-12-07T16:23:04Z,2022-01-11T13:25:49Z,,,
14658,b'Fixing Dataset for TQA + token-classification.',2021-12-07T13:24:01Z,2021-12-08T08:54:25Z,,,
14657,b'[WIP] Check doc examples',2021-12-07T07:57:50Z,2021-12-07T20:27:42Z,,,
14656,b'[t5/t0/mt5 models] faster/leaner custom layer norm',2021-12-07T03:26:16Z,2022-02-16T00:49:58Z,"Performance, WIP",,
14655,b'Convert tutorials',2021-12-07T01:15:01Z,2021-12-07T21:06:03Z,,,
14654,b'[Beam Search] Correct returned beam scores',2021-12-07T01:06:32Z,2022-01-24T20:13:21Z,,,
14653,b'Missing weight parameter when loading from deepspeed stage-2',2021-12-06T22:57:51Z,2021-12-08T18:32:22Z,,,
14652,b'[deepspeed] fix --load_best_model_at_end',2021-12-06T22:07:19Z,2021-12-07T05:57:48Z,,,
14651,b'Transformer-XL -100 label padding',2021-12-06T21:46:44Z,2022-01-14T15:02:55Z,,,
14650,b'Make `CLIPFeatureExtractor` accept batch of images as `torch.Tensor`.',2021-12-06T21:20:53Z,,Good First Issue,,
14649,b'Finetuning T5 for 2 tasks with 2 diferent prefixes and different data gives the same result.',2021-12-06T21:13:26Z,2022-01-14T15:02:56Z,,,
14648,"b""LEDTokenizer doesn't pad `global_attention_mask`""",2021-12-06T20:32:12Z,2022-03-18T12:30:08Z,,ValueError,"ValueError: expected sequence of length 4096 at dim 1 (got 3157)"
14647,b'Reproducibility issue with Trainer',2021-12-06T19:34:13Z,2022-01-14T15:02:57Z,,,
14646,b'fix flax examples tests',2021-12-06T18:18:15Z,2021-12-06T19:04:28Z,,,
14645,b'Add a job to test the documentation build',2021-12-06T17:56:02Z,2021-12-06T18:55:59Z,,,
14644,b'Fix syntax for class references',2021-12-06T17:44:19Z,2021-12-06T18:31:28Z,,,
14643,b'fix flax example tests',2021-12-06T17:38:01Z,2021-12-06T17:44:38Z,,,
14642,"b'Encapsulate all forward passes of integration tests with ""with torch.no_grad()""'",2021-12-06T17:03:27Z,,Good First Issue,,
14641,b'doc: mismatch between pooler/d_output',2021-12-06T15:11:52Z,2021-12-06T16:51:53Z,,,
14640,b'Add mLUKE',2021-12-06T14:41:11Z,2021-12-07T05:25:28Z,,,
14639,b'[Hub] 403 when trying to download models',2021-12-06T12:20:27Z,2021-12-06T14:09:03Z,,,
14638,b'Gradient accumulation causing different training curves',2021-12-06T07:22:22Z,2022-01-11T08:16:39Z,,,
14637,b'add flax example tests in CI workflow',2021-12-06T06:16:27Z,2021-12-06T09:20:43Z,,,
14636,b'[Flax examples] remove dependancy on pytorch training args',2021-12-06T06:11:09Z,2021-12-12T03:49:13Z,,,
14635,b'fix typo',2021-12-06T05:21:57Z,2021-12-06T05:22:43Z,,,
14634,b'[WIP] Add Nystromformer',2021-12-06T01:39:22Z,2021-12-07T16:11:52Z,,,
14633,b'Dynamic Inputs for fx traced GPTNeoLM ',2021-12-05T22:48:42Z,2022-01-13T15:07:05Z,,,
14632,"b'to use FX - your torch version must be *exactly* 1.9, even though fx also works in later versions'",2021-12-05T21:24:30Z,2022-02-07T15:07:23Z,,,
14631,b'Add Sliding Window to TokenClassificationPipeline',2021-12-05T21:13:59Z,2022-01-13T15:07:08Z,,,
14630,b'bert-base-uncased weights for BertForPreTraining',2021-12-05T19:03:54Z,2022-01-13T15:07:09Z,,,
14629,b'GPT-NeoX checkpoint conversion?',2021-12-05T12:46:23Z,2022-01-12T15:02:54Z,,,
14628,"b'load_best_model_at_end failed due to ""size mismatch"" when DeepSpeed is used'",2021-12-05T09:52:03Z,2021-12-07T04:25:54Z,,RuntimeError,"RuntimeError: Error(s) in loading state_dict for GPTNeoForCausalLM:"
14627,b'How to separate multiturn dialog context in blenderbot?',2021-12-05T08:11:29Z,2022-01-07T02:43:51Z,,,
14626,b'fix a typo',2021-12-04T21:15:40Z,2021-12-05T06:01:23Z,,,
14625,b'Updated deberta attention',2021-12-04T15:00:09Z,2021-12-22T12:36:09Z,,,
14624,b'updated pytorch token-classification readme',2021-12-04T14:15:05Z,2021-12-06T03:12:51Z,,,
14623,b'Auto processor fix',2021-12-04T10:00:17Z,2021-12-06T17:49:50Z,,,
14622,b'[Quick poll] Give your opinion on the future of the Hugging Face Open Source ecosystem!',2021-12-04T09:11:34Z,2022-01-11T11:24:32Z,,,
14621,b'DeBERTa `p2p` attention type is not used',2021-12-04T08:43:12Z,2021-12-22T12:36:09Z,,,
14620,b'Implement head_mask for Flax BERT and other models copied from BERT',2021-12-03T22:49:23Z,2021-12-17T16:07:00Z,,,
14619,b'Getting ambiguous messages for SummarizationPipeline',2021-12-03T21:28:58Z,2021-12-07T16:30:31Z,,,
14618,b'quick fix SummarizationPipeline error messages',2021-12-03T21:27:21Z,2021-12-07T15:44:29Z,,,
14617,b'[urls to hub] Replace outdated model tags with their now-canonical pipeline types',2021-12-03T18:59:47Z,2021-12-06T09:35:01Z,,,
14616,b'Fix doc builder',2021-12-03T17:03:20Z,2021-12-03T17:09:26Z,,,
14615,b'Bigger batch size decrease the throughput.',2021-12-03T12:43:01Z,2022-01-10T15:02:07Z,,,
14614,"b""datasets doesn't support NSP? should I implement my custom DataCollator or function for dataset.map?""",2021-12-03T12:02:40Z,2022-02-11T15:07:37Z,,,
14613,"b""Running  Pipeline batching code from documentation returns TypeError: _batch_encode_plus() got an unexpected keyword argument 'batch_size' on Colab. """,2021-12-03T11:56:03Z,2021-12-03T15:14:37Z,,TypeError,"TypeError: Caught TypeError in DataLoader worker process 0."
14612,b'Scores returned by beam search are not useful',2021-12-03T11:05:33Z,2022-01-26T15:06:54Z,,,
14611,"b'Got extra_id_%d when generating texts, and %d is negative.'",2021-12-03T10:43:10Z,2022-01-10T15:02:10Z,,,
14610,b'2022 is the year of multi-modality',2021-12-03T10:34:59Z,2021-12-03T16:35:44Z,,,
14609,"b' Unable to load mT5 with MT5Model.from_pretrained(""google/mt5-small"") '",2021-12-03T07:41:39Z,2021-12-16T05:36:57Z,,,
14608,b'[Benchmark] HF Trainer on RTX-3090',2021-12-03T05:56:53Z,,"Benchmarks, WIP",,
14607,"b'[CI] move env print to util, add pt, nccl versions'",2021-12-03T01:26:41Z,2021-12-03T13:18:37Z,,,
14606,b'[trainer] add tf32-mode control',2021-12-03T00:19:05Z,2021-12-03T18:08:59Z,,,
14605,b'[Flax] Add Flax implementation of `RoFormer`',2021-12-02T20:02:33Z,2022-01-04T12:23:10Z,,,
14604,"b""HF space specific issue: can't import `AutoModelForSeq2SeqLM` from `transformers==4.6.1` with `torch` installed""",2021-12-02T18:13:58Z,2022-01-23T15:03:08Z,,,
14603,b'Recover when the `sha` of the local file is not correct.',2021-12-02T14:42:03Z,2022-01-18T15:07:21Z,,,
14602,b'Can you add a fast tokenizer for the ByT5 model? ',2021-12-02T13:47:41Z,2022-01-09T15:01:41Z,,,
14601,"b'Error loading Speech2Text2Processor.from_pretrained(""facebook/wav2vec2-xls-r-2b-21-to-en"")'",2021-12-02T12:28:31Z,2022-01-28T17:24:57Z,,TypeError,"TypeError: expected str, bytes or os.PathLike object, not NoneType"
14600,b'change tf.math.divide with int(/) in distilbert model',2021-12-02T11:29:59Z,2021-12-02T13:13:42Z,,,
14599,b'Add Flax example tests',2021-12-02T10:46:36Z,2021-12-06T05:18:58Z,,,
14598,b'Python 3.6 -> Python 3.7 for TF runs',2021-12-02T09:09:11Z,2021-12-02T09:09:17Z,,,
14597,b'Adds a git pull instruction to the documentation builder',2021-12-02T08:27:00Z,2021-12-02T08:32:39Z,,,
14596,b'Hubert-base Model with new tokenizer is not converging ',2021-12-02T07:36:55Z,2022-01-11T15:06:47Z,,,
14595,b'GPT-2 Model wrapped in DataParallel hangs immediately',2021-12-01T23:26:58Z,2022-01-17T15:01:52Z,,,
14594,b'Rename toctree.yml -> _toctree.yml',2021-12-01T23:18:50Z,2021-12-02T07:58:39Z,,,
14593,b'Update doc img links',2021-12-01T22:37:13Z,2021-12-02T08:01:35Z,,,
14592,"b""Version 4.12.5 doesn't work on sagemaker""",2021-12-01T20:04:59Z,2022-01-12T15:02:57Z,,RuntimeError,"RuntimeError: OrderedDict mutated during iteration#015"
14591,b'update deprecating `transformers-cli` to `huggingface-cli` in docs',2021-12-01T19:23:45Z,2022-01-03T09:05:31Z,,,
14590,b'Doc new front',2021-12-01T18:24:26Z,2021-12-01T19:13:02Z,,,
14589,b'Fix doc interlinks',2021-12-01T18:01:25Z,2021-12-01T18:11:17Z,,,
14588,b'Make DefaultDataCollator importable from root',2021-12-01T17:24:37Z,2021-12-03T20:15:10Z,,,
14587,b'Config overrides option working on any kind of config',2021-12-01T15:09:05Z,2022-01-09T15:01:45Z,,,
14586,b'Add ONNX support for MarianMT models',2021-12-01T13:48:50Z,2021-12-23T12:35:57Z,,,
14585,"b""Deberta's Enhanced Masked Decoder""",2021-12-01T11:31:28Z,2022-01-08T15:02:04Z,,,
14584,b'Add PoolFormer Model',2021-12-01T11:03:41Z,,New model,,
14583,b'Doc misc fixes',2021-12-01T10:35:57Z,2021-12-01T17:15:09Z,,,
14582,b'Add W&B backend for hyperparameter sweep',2021-12-01T08:51:59Z,2022-02-02T19:06:14Z,,,
14581,b'Cannot run Deepspeed inference of GPT-Neo with low_cpu_mem_usage enabled',2021-12-01T08:49:38Z,2021-12-02T05:03:48Z,,RuntimeError,"RuntimeError: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead."
14580,b'[bf16 support] tweaks',2021-12-01T03:19:24Z,2021-12-08T19:33:24Z,,,
14579,b'[doc] bf16/tf32 guide',2021-12-01T02:59:21Z,2021-12-01T22:18:58Z,,,
14578,b'Adapt build command to new CLI tools',2021-11-30T23:54:39Z,2021-12-01T17:41:03Z,,,
14577,b'fix pytorch division warning by using suggested torch.div rounding_mode',2021-11-30T21:47:37Z,2022-02-10T15:38:37Z,,,
14576,b'[Flax] Add FlaxBlenderbotSmall',2021-11-30T21:26:04Z,2021-12-02T08:51:48Z,,,
14575,b'GPT2 large trains on 1 GPU but does not fit in two.',2021-11-30T20:58:09Z,2022-01-08T15:02:06Z,,RuntimeError,"RuntimeError: CUDA out of memory. Tried to allocate 320.00 MiB (GPU 0; 39.59 GiB total capacity; 36.81 GiB already allocated; 205.69 MiB free; 37.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
14574,b'Adafactor does not work with Resnets (or with MAML)',2021-11-30T14:53:54Z,2022-01-07T15:01:59Z,,RuntimeError,"RuntimeError: mat1 must be a matrix, got 4-D tensor"
14573,b'Adding call for contribution for the S4 model',2021-11-30T13:56:03Z,2022-01-10T13:09:51Z,,,
14572,b'Can\xe2\x80\x99t load config for \xe2\x80\x98/data/sentence-transformers_all-mpnet-base-v2\xe2\x80\x99',2021-11-30T11:48:31Z,2022-01-03T11:34:07Z,,,
14571,b'Delete versions.yml from transformers doc',2021-11-30T09:14:48Z,2021-11-30T09:17:23Z,,,
14570,b'Add mLUKE',2021-11-30T07:30:16Z,2021-12-06T09:38:54Z,,,
14569,b'[Deepspeed] add support for bf16 mode',2021-11-30T03:59:02Z,2022-03-12T01:53:53Z,WIP,,
14568,b'Fixes Loss for TransfoXL when using Trainer API',2021-11-29T19:10:11Z,2022-03-14T15:07:00Z,,,
14567,b'Fix index links',2021-11-29T18:25:22Z,2021-12-01T17:29:02Z,,,
14566,b'Fix backend regex',2021-11-29T17:04:30Z,2021-11-30T10:32:20Z,,,
14565,"b""BertForSequenceClassification : 'module 'Torch' has no attribute 'BoolTensor'""",2021-11-29T14:55:59Z,2022-01-06T15:02:00Z,,"AttributeError, RuntimeError","AttributeError: module 'torch' has no attribute 'BoolTensor'RuntimeError: Failed to import transformers.models.bert.modeling_bert because of the following error (look up to see its traceback):"
14564,b'[Generate] Fix generate with inputs_embeds on GPU',2021-11-29T14:34:19Z,2021-11-29T15:10:20Z,,,
14563,b'[Bug] Issue in AutoModelForSequenceClassification while initialization',2021-11-29T11:54:29Z,2021-12-02T11:37:40Z,,,
14562,b'Fix doc interlinks',2021-11-29T11:32:48Z,2021-12-01T17:54:29Z,,,
14561,b'[Bug] `tokenizer.model_max_length` is different when loading model from shortcut or local path',2021-11-29T09:28:04Z,2022-01-06T15:02:02Z,,,
14560,b'[Probably a bug] - T5TokenizerFast is not the same as T5Tokenizer  - it adds id 1 when using batch encode plus ',2021-11-29T08:28:24Z,2022-01-08T15:02:10Z,,,
14559,b'Question about an error occurring while running hf_argparser.py',2021-11-29T07:02:59Z,2021-11-30T09:58:33Z,,TypeError,"TypeError: issubclass() arg 1 must be a class"
14558,b'XLMForSequenceClassification does not work',2021-11-29T05:28:19Z,2022-01-06T15:02:04Z,,,
14557,"b'Fix a Bug, modeling_bert.py, erroneosly switched BCE and CrossEntropy'",2021-11-28T18:33:29Z,2021-11-28T21:14:26Z,,,
14556,"b'Target customized languages using multi-lingual sentence transformer models like ""stsb-xlm-r-multilingual""'",2021-11-28T18:28:11Z,2021-11-30T15:16:32Z,,,
14555,b'Add LayoutLMv2 to models exportable with ONNX',2021-11-28T15:32:52Z,2022-01-06T15:02:05Z,,,
14554,b'Loading from checkpoint without skipping requires the same number of GPUs',2021-11-28T14:03:28Z,2021-12-30T22:03:20Z,,IndexError,"IndexError: tuple index out of range"
14553,b'to support 3 dim attention mask in tf version',2021-11-28T09:24:21Z,2022-01-14T15:03:03Z,TensorFlow,,
14552,b'Can\xe2\x80\x99t run Parallel inference',2021-11-28T08:42:15Z,2022-01-07T15:02:03Z,,,
14551,b'Question about the <mask>',2021-11-28T08:24:00Z,2022-01-06T15:02:07Z,,,
14550,b'Huggingface Missing Larger T5 Flax Models',2021-11-28T01:37:53Z,2022-01-11T15:06:51Z,,,
14549,"b""Update TAPAS tokenization's add_numeric_table_values""",2021-11-27T20:28:18Z,,,,
14548,"b'Does the word embedding matrix of GPT2 load from the checkpoint,  during the fine-tuning?'",2021-11-27T12:42:55Z,2021-11-28T09:07:57Z,,,
14547,b'[Flax] token-classification model steps enumerate start from 1',2021-11-27T11:54:04Z,2021-11-29T16:26:00Z,,,
14546,"b'Fix a Bug, trainer_seq2seq.py, in the else branch at Line 172, generation_inputs should be a dict'",2021-11-27T07:18:53Z,2021-12-07T17:09:18Z,,,
14545,"b'fix bug for trainer_seq2seq, generation_inputs should be a dict befor\xe2\x80\xa6'",2021-11-27T06:52:53Z,2021-11-27T07:19:14Z,,,
14544,b'TAPAS tokenizer is unable to handle indexed dataframes',2021-11-27T04:30:22Z,2022-01-04T15:02:10Z,,,
14543,b'TAPAS tanh activation on the pooling layer',2021-11-27T03:45:53Z,2021-11-27T20:31:26Z,,,
14542,b'CUDA OOM at `self.optimizer.consolidate_state_dict()` in Trainer when using sharded_ddp ',2021-11-26T20:26:27Z,2022-01-04T15:02:11Z,,RuntimeError,"RuntimeError: CUDA error: out of memory"
14541,b'Out of the box GPT-2 CLM hits out of memory on an AWS 8x Nvidia A100 VM',2021-11-26T16:56:07Z,2022-01-04T15:02:12Z,,RuntimeError,"RuntimeError: CUDA out of memory. Tried to allocate 12.27 GiB (GPU 0; 39.59 GiB total capacity; 28.63 GiB already allocated; 9.00 GiB free; 28.71 GiB reserved in total by PyTorch)"
14540,"b""cannot import name 'SpeechEncoderDecoder' from 'transformers' - wav2vec2-xls-r-2b-22-to-16""",2021-11-26T16:51:13Z,2022-01-05T07:05:03Z,,,
14539,b'Two bugs in AdamW',2021-11-26T16:50:56Z,2022-01-13T16:14:52Z,,,
14538,"b""cannot import name 'DataCollatorForSeq2Seq' from 'transformers'""",2021-11-26T16:43:17Z,2021-11-26T17:03:07Z,,,
14537,b'Is the attention_mask in BertSelfAttention applied correctly?',2021-11-26T13:59:11Z,2022-01-03T15:01:48Z,,,
14536,b'Add CodeParrot \xf0\x9f\xa6\x9c codebase',2021-11-26T13:26:10Z,2021-12-02T09:41:35Z,,,
14535,b'[flax] unfreeze initial cache in gpt models',2021-11-26T12:27:22Z,2021-11-26T12:51:47Z,,,
14534,b'Fixes',2021-11-26T09:35:01Z,2021-11-26T09:35:09Z,,,
14533,b'Quicktour updates',2021-11-26T09:08:35Z,2021-11-26T09:09:31Z,,,
14532,b'Difference in the length of positional embeddings produces different results',2021-11-26T04:34:27Z,2022-01-03T15:01:50Z,,,
14531,b'Deepspeed and T5-11B for multitask training',2021-11-26T03:45:55Z,2022-01-07T15:02:05Z,,,
14530,b'Logits warper for batch generation',2021-11-25T20:57:45Z,2022-02-06T15:01:47Z,,,
14529,"b'     added save_directories for _psave_pretrained_pt and _tf, changed model to tf_model and pt_model, enable the notebook to run cleanly from top to bottom without error'",2021-11-25T20:04:22Z,2021-11-26T08:46:08Z,,,
14528,"b""trainer process bar can't move while training""",2021-11-25T16:26:33Z,2022-01-03T15:01:51Z,,,
14527,b'Fix a slow test.',2021-11-25T15:32:35Z,2021-11-25T17:59:33Z,,,
14526,b'Rename ImageGPT',2021-11-25T15:16:31Z,2021-11-29T09:19:12Z,,,
14525,b'fix #14524 (IndexError when mask prob is too low)',2021-11-25T12:55:12Z,2021-12-02T14:05:31Z,,,
14524,b'Computation of mask indices in Wav2vec2Model fails with low probabilities',2021-11-25T12:46:25Z,2021-12-02T14:05:37Z,,IndexError,"IndexError: index 0 is out of bounds for axis 0 with size 0"
14523,b'Examples for speech recognition trainings from scratch',2021-11-25T12:11:25Z,2022-03-20T15:02:19Z,,,
14522,b'Speeding up the models inference by OpenVINO through accurate quantization via NNCF',2021-11-25T10:23:50Z,2022-01-02T15:01:53Z,,,
14521,b'GPT model `generate()` function not correctly skipping the padding tokens indicated by `attention_mask`',2021-11-25T09:05:34Z,2022-01-14T15:03:05Z,,,
14520,b'[CI] clear `~/.cache/torch_extensions` between builds',2021-11-25T02:30:29Z,2021-11-25T08:15:35Z,,ImportError,"ImportError: /github/home/.cache/torch_extensions/py38_cu111/cpu_adam/cpu_adam.so: undefined symbol: curandCreateGenerator"
14519,"b'Make the ""Can\'t load <file> for <model_name>"" error more user-friendly'",2021-11-24T20:21:52Z,2022-01-02T15:01:53Z,,,
14518,"b""Wav2vec2 finetuned model's strange truncated predictions""",2021-11-24T15:37:05Z,2021-11-25T15:29:11Z,,,
14517,b'Update versions.yml format',2021-11-24T15:25:52Z,2021-11-24T16:19:38Z,,,
14516,b'Fix typo in toctree',2021-11-24T14:50:48Z,2021-11-24T14:54:57Z,,,
14515,b'Fix feature extraction utils import',2021-11-24T13:59:12Z,2021-11-24T14:03:21Z,,,
14514,b'LayoutLMv2FeatureExtractor now supports non-English languages when applying Tesseract OCR.',2021-11-24T12:24:40Z,2021-11-29T09:15:08Z,,,
14513,b'\xe2\x9d\x93 Define tokenizer from `tokenizers` as a `PreTrainedTokenizer`',2021-11-24T11:22:22Z,2021-11-24T14:28:58Z,,,
14512,b'Doc new front github actions',2021-11-24T10:55:32Z,2021-11-29T15:22:47Z,,,
14511,b'LayoutXLMProcessor applies the english Tesseract model',2021-11-24T10:39:34Z,2021-11-29T09:15:08Z,,,
14510,b'bidirectional training for GPT2',2021-11-24T10:14:36Z,2022-01-01T15:01:41Z,,,
14509,b'Converted TF model cannot generate line breaks anymore',2021-11-24T09:47:26Z,,,,
14508,b'add cache_dir for tokenizer verification loading',2021-11-24T09:17:05Z,2021-11-24T11:22:03Z,,,
14507,b'Distributed Training with Triplet Loss and DistilRoberta encoder',2021-11-24T08:49:50Z,2022-01-01T15:01:42Z,,RuntimeError,"RuntimeError: Expected to mark a variable ready only once. This error is caused by one of the following reasons: "
14506,b'RuntimeError: The expanded size of the tensor (20) must match the existing size (101) at non-singleton dimension 0.  Target sizes: [20].  Tensor sizes: [101] ',2021-11-24T07:45:13Z,2022-01-01T15:01:43Z,,,
14505,b'FillMaskPipeline assumes model return dict but return_dict is not set',2021-11-24T07:29:12Z,2021-11-24T15:46:22Z,,,
14504,b'Add a new gradient regularization feature',2021-11-24T06:24:31Z,2021-11-30T03:15:21Z,,,
14503,b'Add GPTJForQuestionAnswering',2021-11-23T22:20:48Z,2021-12-06T16:44:11Z,,,
14502,b'DeBERTa-v3 does not preserve spaces before/after additional special tokens in convert_tokens_to_string output',2021-11-23T17:21:32Z,2022-03-20T15:02:22Z,,,
14501,b'[TAPAS] Tiny fix',2021-11-23T16:30:23Z,2022-01-01T15:01:44Z,,,
14500,b'(TF) InternalError: Multiple CPU devices ',2021-11-23T16:20:57Z,2022-03-01T15:08:31Z,,tensorflow.python.framework.errors_impl.InternalError,"tensorflow.python.framework.errors_impl.InternalError: Multiple CPU devices [/job:worker/replica:0/task:0/device:GPU:7,/job:worker/replica:0/task:0/device:CPU:0,/job:localhost/replica:0/task:0/device:CPU:0] [Op:__inference_train_function_800368]"
14499,b'Doc fixes',2021-11-23T16:12:50Z,2021-11-23T16:31:21Z,,,
14498,b'How to get logits from generate() method ?',2021-11-23T15:02:16Z,2021-11-24T14:32:55Z,,,
14497,b'FLAX core dump error on CloudTPU when running run_clm_flax.py',2021-11-23T14:51:36Z,,WIP,,
14496,b'Add necessary new front files',2021-11-23T13:40:16Z,2021-11-23T14:34:17Z,,,
14495,"b""Cannot find 'blob' directory in your 'transformers' repository""",2021-11-23T05:55:01Z,2021-11-27T19:06:26Z,,,
14494,b'Add TAPAS trained on NQ',2021-11-23T04:01:45Z,,New model,,
14493,b'fixes some key names for in LayoutLMv2 / LayoutXLM tokenizers',2021-11-22T19:50:54Z,2021-11-22T21:00:44Z,,,
14492,b'Add model checkpointing to push_to_hub and PushToHubCallback',2021-11-22T18:33:53Z,2021-11-29T17:36:19Z,,,
14491,b'BART + ONNX torch.jit error iterabletree cannot be used as a value',2021-11-22T18:07:23Z,2021-12-08T14:47:26Z,,**RuntimeError,"**RuntimeError: "
14490,b'Feature request: Add built-in support for autorregressive text generation with ONNX models',2021-11-22T17:51:15Z,2021-12-09T19:37:29Z,,,
14489,b'tokenizer.save_pretrained() doest not save an important part of tokenizer_config of FlaubertTokenizer',2021-11-22T12:12:57Z,2022-01-14T10:36:07Z,,,
14488,b'Fatal error in event_loop.c',2021-11-22T12:05:08Z,2021-11-22T12:28:31Z,,,
14487,b'Add Perceiver IO',2021-11-22T10:59:06Z,2021-12-08T13:20:35Z,,,
14486,b'DPR usage of BertPooler',2021-11-22T10:43:15Z,2022-01-18T15:36:12Z,,,
14485,b'Improve `add-new-pipeline` docs a bit',2021-11-22T10:40:47Z,2021-11-22T15:35:49Z,,,
14484,b'Loading from the wrong cache? ',2021-11-22T09:50:38Z,2021-12-30T15:02:05Z,,ValueError,"ValueError: Couldn't instantiate the backend tokenizer from one of: "
14483,b'JAVA Predict',2021-11-22T09:37:13Z,2021-12-30T15:02:06Z,,ValueError,"ValueError: Exception encountered when calling layer ""tf_bert_for_sequence_classification"" (type TFBertForSequenceClassification)."
14482,b'where can I find the dataset bert-base-chinese is pretrained on?',2021-11-22T09:22:51Z,2021-12-30T15:02:07Z,,,
14481,b'Is the index of the vocabulary in Tokenizer the same as the index of WordEmbedding?',2021-11-22T09:17:24Z,2021-11-22T09:49:07Z,,,
14480,b'Fine-tune Integer Bert for question answering task',2021-11-22T07:43:35Z,2021-12-30T15:02:08Z,,,
14479,b'facebook / wav2vec2-base-100k-voxpopuli fails to load on huggingface.co (also on system)',2021-11-21T23:39:13Z,2021-11-22T14:34:22Z,,OSError,"OSError: Can't load tokenizer for 'facebook/wav2vec2-base-100k-voxpopuli'. Make sure that:"
14478,b'Fix dummy objects for quantization',2021-11-21T22:03:08Z,2021-11-21T22:39:20Z,,,
14477,b'Fix sentinel token IDs in data collator for Flax T5 pretraining script',2021-11-21T19:27:24Z,2021-11-29T16:30:17Z,,,
14476,b'Change examples.md <details> to use directly html',2021-11-21T19:07:09Z,2021-11-21T22:01:34Z,,,
14475,b'Where do I find the class documentation',2021-11-21T09:49:28Z,2021-12-23T15:37:59Z,Documentation,,
14474,b'How do I preserve HTML structure when putting data through transformers',2021-11-21T04:42:16Z,2021-12-30T15:02:09Z,,,
14473,b'add Tuple as possible type hint for EvalPredictions label_ids',2021-11-20T20:27:25Z,2021-11-21T15:31:10Z,,,
14472,b'Switch from using sum for flattening lists of lists in group_texts',2021-11-20T17:37:40Z,2021-11-22T21:17:27Z,,,
14471,b'Wav2Vec2ForPreTraining in 4.12 broke SpeechBrain implementation',2021-11-20T17:31:48Z,2022-01-01T15:01:45Z,,,
14470,b'Unable to load DeBERTa-v3 tokenizer',2021-11-20T13:16:24Z,2021-11-26T17:55:07Z,,ValueError,"ValueError: This tokenizer cannot be instantiated. Please make sure you have `sentencepiece` installed in order to use this tokenizer."
14469,b'Enable automatic creation of decoder_input_ids given labels in TF',2021-11-20T12:15:58Z,2022-02-05T15:01:43Z,,,
14468,b'Pretrained bare model have weights that are not beein used',2021-11-20T11:56:04Z,2021-11-21T19:53:13Z,,,
14467,b'`EncoderDecoderModel` `generate` for a `ViT` as encoder',2021-11-20T08:33:08Z,2021-11-27T09:40:44Z,,,
14466,b'[test] add test for --config_overrides',2021-11-20T00:49:11Z,2021-11-22T16:33:43Z,,,
14465,b'Auto processor',2021-11-19T20:12:25Z,2021-11-22T17:17:38Z,,,
14464,b'[OPTIMIZATION] remove a redundant condition statement for empty str judgement in `whitespace_tokenize`',2021-11-19T15:42:48Z,2021-12-28T15:01:55Z,,,
14463,b'Moving pipeline tests from `Narsil` to `hf-internal-testing`.',2021-11-19T15:01:13Z,2021-11-22T09:40:46Z,,,
14462,b'Fixes torch jit tracing for LayoutLMv2 model.',2021-11-19T12:49:14Z,2021-12-30T15:02:10Z,,,
14461,b'Function `ByT5Tokenizer.convert_tokens_to_string()` fails with certain tokens',2021-11-19T12:27:29Z,2021-12-27T09:08:27Z,,,
14460,b'[ImageGPT] Small fixes',2021-11-19T12:01:17Z,2021-11-19T14:15:02Z,,,
14459,b'Add GitPython to quality tools',2021-11-19T11:45:34Z,2021-11-19T13:43:49Z,,,
14458,b'[Tests] Improve vision tests',2021-11-19T11:36:51Z,2021-11-24T14:22:21Z,,,
14457,b'Tracing LayoutLMv2 results in wrong input_shape dimension',2021-11-19T10:32:10Z,2021-11-19T12:54:54Z,,RuntimeError,"RuntimeError: The expanded size of the tensor (561) must match the existing size (512) at non-singleton dimension 1.  Target sizes: [1, 561].  Tensor sizes: [1, 512]"
14456,b'[New Model] DocFormer: End-to-End Transformer for Document Understanding',2021-11-19T07:08:44Z,,New model,,
14455,b'The multi-node / multi-gpu training and repeat logging on each process',2021-11-19T04:44:42Z,2021-12-21T04:48:39Z,WIP,,
14454,"b""GPT2 Generate doesn't pass the user defined past_key_values.""",2021-11-19T02:44:03Z,2022-01-23T15:03:13Z,,,
14453,b'How to implement huggingface BERT + CRF layer?',2021-11-19T01:37:47Z,2022-01-24T15:02:32Z,,,
14452,b'Sampling sequences similar to a given sequence',2021-11-19T01:36:17Z,2021-12-27T15:01:44Z,,,
14451,b'Patrick von Platen',2021-11-18T20:31:39Z,2021-12-27T15:01:45Z,,,
14450,b'[RFC] Amphere/tf32 defaults for transformers',2021-11-18T20:29:10Z,2021-12-03T18:08:58Z,Performance,,
14449,b'OpenAIGPTTokenizer does not work with spacy 3.x installed',2021-11-18T19:27:14Z,2022-01-10T12:53:20Z,,AttributeError,"AttributeError: type object 'EnglishDefaults' has no attribute 'create_tokenizer'"
14448,b'WIP: Add support for bfloat16 in Trainer and T5',2021-11-18T18:52:58Z,2021-11-30T14:38:48Z,,,
14447,"b'[Bert, et al] fix early device assignment'",2021-11-18T18:30:17Z,2021-11-18T19:47:49Z,,,
14445,b'Fix finite IterableDataset test on multiple GPUs',2021-11-18T14:20:28Z,2021-11-18T15:25:06Z,,,
14444,b'Issues with Training VisionEncoderDecoder with Seq2SeqTrainer',2021-11-18T12:20:11Z,2021-11-19T14:21:05Z,,ValueError,"ValueError: You should supply an encoding or a list of encodings to this method that includes input_ids, but you provided ['labels']"
14443,b'[Generation] Allow `inputs_embeds` as an input',2021-11-18T11:58:16Z,2021-11-19T14:35:06Z,,,
14442,b'tokenizer',2021-11-18T08:33:58Z,2021-11-19T09:42:05Z,,,
14441,b'Fix EncoderDecoderModel code example',2021-11-18T08:02:27Z,2021-11-18T10:26:56Z,,,
14440,"b'What does ""is_beam_sample_gen_mode"" mean '",2021-11-18T06:31:52Z,2021-12-20T02:08:18Z,,,
14439,b'(EncoderDecoderModel) Why decoder_start_token_id are different between train and generation?',2021-11-18T06:26:39Z,2021-11-18T10:26:56Z,,,
14438,b'Update file_utils.py',2021-11-17T23:37:28Z,2021-11-19T13:39:08Z,,,
14437,b'Recover Deleted XNLI Instructions',2021-11-17T22:28:15Z,2021-11-18T01:16:47Z,,,
14436,b'BERT outperformed XLNet',2021-11-17T20:42:46Z,2021-12-26T15:01:42Z,,,
14435,b'Ecco package - integration with HuggingFace',2021-11-17T18:23:15Z,2022-01-28T15:02:55Z,,,
14434,b'[Bart] Fix docs',2021-11-17T17:50:16Z,2021-11-17T18:02:34Z,,,
14433,b'Issue doing multi gpu training with TrOCR Transformer',2021-11-17T17:34:45Z,2021-12-27T15:01:46Z,,RuntimeError,"RuntimeError: CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 11.17 GiB total capacity; 10.54 GiB already allocated; 85.44 MiB free; 10.63 GiB reserved in total by PyTorch)"
14432,b'Longformer slower than Roberta',2021-11-17T16:49:49Z,2022-01-02T15:01:58Z,,"""layer_norm_eps""","""layer_norm_eps"": 1e-12,"
14431,b'Add a post init method to all models',2021-11-17T14:26:28Z,2021-11-18T13:38:11Z,,,
14430,"b""`AttributeError: 'BertConfig' object has no attribute 'items'` when saving a tf keras model with `transformers 4.12.4`""",2021-11-17T10:17:09Z,2021-11-17T16:41:18Z,,AttributeError,"AttributeError: 'BertConfig' object has no attribute 'items'"
14429,b'`BertTokenizerFast.vocab.keys()` does not return a fixed order sequence',2021-11-17T09:32:21Z,2021-12-25T15:01:50Z,,,
14428,b'[Benchmark] Tokenizers as Collate functions vs normal in loop tokenizing',2021-11-17T06:22:29Z,2021-11-20T08:22:53Z,,,
14427,b'fix hook removal issue',2021-11-17T05:02:13Z,2021-11-17T16:41:25Z,,,
14426,b'[Deepspeed Inference] HF Integration',2021-11-17T01:57:38Z,,"WIP, Inference",,
14425,b'Add documentation for exporting TorchScript model to accelerator',2021-11-16T23:02:17Z,2021-12-27T15:01:47Z,,,
14424,b'Debug doc',2021-11-16T22:50:21Z,2021-11-16T23:58:07Z,,,
14423,"b""Initial install:  No module named 'tensorflow.python.keras.engine.keras_tensor'""",2021-11-16T20:25:02Z,2021-11-18T13:16:48Z,,"RuntimeError, ModuleNotFoundError","RuntimeError: Failed to import transformers.modeling_tf_utils because of the following error (look up to see its traceback):ModuleNotFoundError: No module named 'tensorflow.python.keras.engine.keras_tensor'"
14422,b'Exporting `sentence-transformers/LaBSE` to ONNX leads to different output',2021-11-16T16:47:21Z,2021-12-25T15:01:52Z,,,
14421,b'[Generation] Make `generate()` method compatible with speech and vision inputs while keeping 100% backward compatibility.',2021-11-16T15:41:59Z,2021-12-17T11:18:13Z,,,
14420,b'Adding support for `hidden_states` and `attentions` in unbatching support.',2021-11-16T15:19:19Z,2021-11-19T14:37:53Z,,,
14419,b'Add forward method to dummy models',2021-11-16T14:00:56Z,2021-11-16T14:24:40Z,,,
14418,b'DebertaTokenizerFast from microsoft/deberta-base returns strange offset_mapping for \xc4\xa0 prefixed token',2021-11-16T13:38:05Z,2021-12-25T15:01:52Z,,,
14417,b'Errors while importing FlaxHybridCLIP checkpoints to FlaxCLIPModel or CLIPModel ',2021-11-16T12:53:53Z,2021-11-16T14:07:38Z,,ValueError,"ValueError: Trying to load the pretrained weight for ('text_projection', 'kernel') failed: checkpoint has shape (768, 512) which is incompatible with the model shape (512, 512). Using `ignore_mismatched_sizes=True` if you really want to load this checkpoint inside this model."
14416,b'Finetune Hubert model : Adding new vocabulary',2021-11-16T11:15:22Z,2021-11-19T04:19:15Z,,,
14415,b'[WIP] Ensure TF model configs can be converted to proper JSON',2021-11-16T10:29:57Z,2021-11-17T20:24:40Z,,,
14414,b'Pipelines fails with IndexError using Bert model with outputs and batch size >= 16',2021-11-16T10:19:25Z,2021-11-19T14:37:53Z,,IndexError,"IndexError: tuple index out of range"
14413,b'Avoid looping when data exhausted',2021-11-16T09:49:53Z,2021-11-16T21:50:05Z,,,
14412,b'Quantization with `transformers.onnx`',2021-11-16T08:45:16Z,2021-11-29T10:08:53Z,,,
14411,"b""Fixed a bug for num_return_sequences don't take effect in Text2TextGenerationPipeline.""",2021-11-16T07:50:39Z,2021-12-30T10:34:31Z,,,
14410,"b""Fixed a bug for num_return_sequences don't take effect in Text2TextGenerationPipeline""",2021-11-16T06:53:26Z,2021-11-16T07:12:56Z,,,
14409,b'support for pytorch-directml',2021-11-15T22:50:10Z,2021-12-25T15:01:53Z,,,
14408,b'Fix gradient_checkpointing backward compatibility',2021-11-15T22:12:48Z,2021-11-16T13:58:43Z,,,
14407,b'[Wav2Vec2] Make sure that gradient checkpointing is only run if needed',2021-11-15T19:50:23Z,2021-11-15T20:03:11Z,,,
14406,"b'Revert ""Fix weight loading issue""'",2021-11-15T18:33:35Z,2021-11-15T22:25:14Z,,,
14405,b'[Gradient checkpointing] Restore backwards compatibility until v5',2021-11-15T18:11:09Z,2021-11-15T21:44:47Z,,,
14404,b'`np.ndarray` not supported anymore for optional arguments at inference for Bert',2021-11-15T15:55:38Z,2022-01-14T15:21:00Z,,,
14403,b'TF models save_pretrained() failed when saved_model=True',2021-11-15T15:28:54Z,2021-11-18T14:48:08Z,,AttributeError,"AttributeError: 'BertConfig' object has no attribute 'items'"
14402,"b""AttributeError: 'NoneType' object has no attribute 'encode_plus' with XLNet tonkenizer""",2021-11-15T14:52:51Z,2021-11-17T20:41:54Z,,AttributeError,"AttributeError: 'NoneType' object has no attribute 'encode_plus'"
14401,b'Quick fix to TF summarization example',2021-11-15T13:44:57Z,2021-11-15T13:45:51Z,,,
14400,b'add embed_scale for bert',2021-11-15T13:40:39Z,2022-01-17T15:01:57Z,,,
14399,b'Fix TFViT',2021-11-15T13:28:22Z,2021-11-15T16:35:34Z,,,
14398,b'TrainingArguments docstring typo fix',2021-11-15T12:46:19Z,2021-12-23T15:01:37Z,,,
14397,b'[Consistency] Automatically set decoder_input_ids for TFEncoderDecoderModel',2021-11-15T12:20:44Z,,Good First Issue,,
14396,b'FlaxGPTJ',2021-11-15T10:16:18Z,2021-12-01T05:27:40Z,,,
14395,"b'force_bos_token_to_be_generated is depricated, should be replaced by forced_bos_token_id in BART documentation'",2021-11-15T08:54:14Z,2021-11-17T18:03:57Z,Good First Documentation Issue,,
14394,b'It seems that `RagSequenceForGeneration.generate` is computing inaccurate loss value',2021-11-15T07:37:51Z,2022-03-05T15:02:25Z,,,
14393,b'Can `BartForConditionalGeneration` use the `sample()` method ?',2021-11-15T04:48:07Z,2021-11-16T02:16:38Z,,ValueError,"ValueError: You have to specify either input_ids or inputs_embeds"
14392,b'[Wav2Vec2] Add New Wav2Vec2 Translation',2021-11-15T00:32:17Z,2021-11-17T13:38:56Z,,,
14391,b'[doc] performance and parallelism updates',2021-11-14T23:07:47Z,2021-11-15T01:19:15Z,,,
14390,b'[Speech2Text2] Enable tokenizers',2021-11-14T20:09:29Z,2021-11-15T15:34:11Z,,,
14389,"b""--config_overrides doesn't appear to work in run_clm.py when trying to specify a larger GPT model""",2021-11-14T19:21:57Z,2021-11-22T16:33:43Z,,,
14388,b'Wav2Vec2 CUDA memory usage doubled in v4.11.3 compared to v4.10.3 with the same batch size',2021-11-14T18:36:10Z,2021-11-15T20:03:11Z,,,
14387,b'Wav2Vec2 Speech Pre-Training After a few epochs the contrastive loss was decreased to zero and the model stopped changing',2021-11-14T09:52:22Z,2021-12-22T15:01:56Z,,,
14386,b'Raise exceptions instead of using asserts  in modeling_openai #12789',2021-11-13T22:31:15Z,2021-11-14T02:34:34Z,,,
14385,b'Replace BertLayerNorm with LayerNorm',2021-11-13T16:32:39Z,2021-11-15T18:25:10Z,,,
14384,b'Token indices sequence length is longer than the specified maximum sequence length ',2021-11-13T11:46:25Z,2021-12-22T15:01:57Z,,RuntimeError,"RuntimeError: CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`"
14382,b'[M2M100Tokenizer] fix _build_translation_inputs',2021-11-13T09:10:21Z,2021-11-13T15:27:12Z,,```TypeError,"```TypeError: M2M100Tokenizer object got multiple values for keyword argument 'return_tensors'```"
14381,b'Which language is available for EncoderDecoderModel pre-trained model?',2021-11-13T09:03:31Z,2021-11-18T10:26:56Z,Good First Documentation Issue,,
14380,b'Using Bart with input_embeds to generate text without input_id return error',2021-11-13T07:35:22Z,2021-12-22T15:01:58Z,,,
14379,b'Tokenizers docs: Specify which class contains `__call__` method',2021-11-13T00:04:09Z,2021-11-28T23:55:38Z,,,
14378,b'Use cross_attention_hidden_size in Encoder-Decoder models',2021-11-12T10:50:17Z,2021-12-06T23:27:32Z,,,
14377,b'minor doc fix',2021-11-12T10:02:40Z,2021-11-12T10:26:42Z,,,
14376,b'Add support for WMT21 tokenizer in M2M100Tokenizer',2021-11-12T09:53:18Z,2021-11-13T08:51:58Z,,,
14375,b'Data type error while fine-tuning Deberta v3 Large using code provided',2021-11-12T09:10:18Z,2021-11-19T09:15:53Z,,TypeError,"TypeError: _softmax_backward_data(): argument 'input_dtype' (position 4) must be torch.dtype, not Tensor"
14374,b'`eos_mask` is possibly supposed to be taken from `decoder_input_ids`',2021-11-11T20:21:46Z,2022-01-11T15:06:57Z,,,
14373,b'[Wav2Vec2 Example] Improve fine-tuning script',2021-11-11T18:47:46Z,2021-11-12T15:35:58Z,,,
14372,b'Fixing requirements for TF LM models and use correct model mappings',2021-11-11T14:46:44Z,2021-11-11T15:34:00Z,,,
14371,b'run_translation.py englisht-german translation failed. RuntimeError: CUDA error: device-side assert triggered',2021-11-11T11:19:34Z,2021-12-19T15:01:48Z,,RuntimeError,"RuntimeError: CUDA error: device-side assert triggered"
14370,b'[flax generate] allow passing params to encode',2021-11-11T10:03:25Z,2021-11-11T11:46:25Z,,,
14369,b'fix loading flax bf16 weights in pt',2021-11-11T09:54:58Z,2021-11-11T15:50:49Z,,,
14368,b'Export LayoutLMv2 to onnx ',2021-11-11T08:54:39Z,,Good First Issue,AssertionError,"AssertionError: You must provide corresponding bounding boxes"
14367,b'Weird assumptions in the PLM collator',2021-11-11T08:16:18Z,2021-12-19T15:01:49Z,,,
14366,b'run_mlm.py Issue | MODEL_FOR_MASKED_LM_MAPPING is None',2021-11-11T08:13:24Z,2021-11-11T15:34:00Z,,AttributeError,"AttributeError: 'NoneType' object has no attribute 'keys'"
14365,"b""Use `AlbertConverter` for FNet instead of using FNet's own converter""",2021-11-11T07:38:33Z,2021-11-12T18:46:40Z,,,
14364,b'Fix mask token handling',2021-11-11T07:13:12Z,2021-12-01T19:16:52Z,,,
14363,b'Comparison Chart (Table) for all the existing BERT models.',2021-11-11T04:14:54Z,2021-12-19T15:01:49Z,,,
14362,b'[testing] solve the port conflict',2021-11-11T03:03:28Z,2021-11-11T03:11:45Z,,,
14361,b'Experimenting with adding proper get_config() and from_config() methods',2021-11-10T18:16:47Z,2021-11-11T14:21:50Z,,,
14360,b'The pytorch example summarization/run_summarization.py do not work with MBart',2021-11-10T17:18:27Z,2022-01-13T14:01:01Z,,ValueError,"ValueError: Make sure that `config.decoder_start_token_id` is correctly defined"
14359,b'How FNet handle PAD token?',2021-11-10T16:04:05Z,2021-12-19T15:01:50Z,,,
14358,b'Added support for other features for already supported models',2021-11-10T15:06:32Z,2021-12-08T17:39:57Z,,,
14357,b'TFEncoderDecoder not handling labels correctly',2021-11-10T14:52:01Z,2022-01-12T14:29:09Z,,ValueError,"ValueError: Exception encountered when calling layer ""encoder"" (type TFBertModel)."
14356,b'[WIP] Adding support for `flax` for `pipelines`.',2021-11-10T14:10:21Z,,WIP,,
14355,b'Improve semantic segmentation models',2021-11-10T12:20:52Z,2021-11-17T14:29:59Z,,,
14354,b'Add WavLM',2021-11-10T12:02:48Z,2021-12-16T17:57:05Z,,,
14353,b'Wav2Vec2 meets phonemes',2021-11-10T12:01:59Z,2021-12-17T18:56:44Z,,,
14352,b' Adding support for raw python `generator` in addition to `Dataset` for pipelines',2021-11-10T09:50:22Z,2021-11-12T08:20:40Z,,,
14351,b'Quantize t5 v1_1 generates nonsense',2021-11-10T08:20:21Z,2022-01-01T15:01:49Z,,,
14350,b'Unable to load/use TFWav2Vec2ForCTC TFLite-model ',2021-11-10T08:20:08Z,2021-11-11T16:33:02Z,,RuntimeError,"RuntimeError: tensorflow/lite/kernels/conv.cc:349 input->dims->data[3] != filter->dims->data[3] (768 != 48)Node number 90 (CONV_2D) failed to prepare."
14349,b'BEIT masked lm',2021-11-10T07:21:03Z,2021-11-10T10:34:10Z,,,
14348,b'enhance rewrite state_dict missing _metadata',2021-11-10T07:08:38Z,2021-11-10T12:25:42Z,,,
14347,b'how to config gpu for run_text_classification?',2021-11-10T06:39:18Z,2021-11-16T09:56:36Z,,,
14346,"b""'tuple' object doesn't have attribute `as_list`""",2021-11-10T01:29:17Z,2022-01-14T15:19:04Z,,AttributeError,"AttributeError: 'tuple' object has no attribute 'as_list'"
14345,b'Support for TF >= 2.7',2021-11-09T23:31:27Z,2021-11-09T23:49:29Z,,,
14344,b'Allow per-version configurations',2021-11-09T23:03:54Z,2021-11-15T21:38:03Z,,,
14343,b'bump flax version',2021-11-09T15:59:11Z,2021-11-09T16:45:22Z,,,
14342,b'Electra model from pretrained not loading correctly',2021-11-09T12:55:48Z,2021-12-18T15:01:39Z,,,
14341,b'remove an irrelevant test from test_modeling_tf_layoutlm',2021-11-09T12:40:32Z,2021-11-09T16:30:17Z,,,
14340,b'layoutlmv2 input tensor shape',2021-11-09T11:44:49Z,2021-11-10T11:01:09Z,,,
14339,b'[Wav2Vec2] PyCTCDecode Integration to support language model boosted decoding',2021-11-09T11:30:18Z,2021-12-08T11:07:55Z,,,
14338,b'Loading RoBERTa pytorch_model.bin checkpoint in fairseq for evaluation',2021-11-09T11:20:32Z,2021-12-18T15:01:40Z,,,
14337,b'Empty sentence and minus translation in opus-mt-de-en model',2021-11-09T09:57:41Z,2022-01-18T15:07:28Z,,,
14336,b'Avoiding the time consuming for downloading the pre-trained models',2021-11-09T08:43:40Z,2021-12-18T15:01:41Z,Migration,,
14335,b'Update Seq2Seq QA example script to use SQuAD metric.',2021-11-09T07:16:14Z,2021-11-09T13:04:23Z,,,
14334,"b""Inference API: Can't load tokenizer using from_pretrained, please update its configuration: No such file or directory (os error 2)""",2021-11-09T01:07:14Z,2021-11-10T08:42:58Z,,,
14333,b'Performance question for pipelines (feature extraction)',2021-11-08T21:56:55Z,2021-11-18T16:29:39Z,,,
14332,b'`SegformerFeatureExtractor` trying to access non-existent `.ndim` attribute',2021-11-08T21:25:04Z,2021-11-17T14:29:59Z,,AttributeError,"AttributeError: ndim"
14331,"b'[deepspeed] Enable multiple test runs on single box, defer to DS_TEST_PORT if set'",2021-11-08T19:35:06Z,2021-11-08T20:40:30Z,,,
14330,b'Can you get Q/A links from LayoutLM',2021-11-08T17:21:21Z,2021-12-18T15:01:41Z,,,
14329,b'[WIP][TF] Fix t5 embeddings',2021-11-08T17:10:32Z,2022-02-09T11:27:31Z,,,
14328,b'BartForConditionalGeneration decoder_input_ids problem',2021-11-08T17:05:03Z,2021-11-08T18:12:59Z,Good First Documentation Issue,,
14327,b'Pipelines: batch size',2021-11-08T17:00:54Z,2021-12-18T15:01:42Z,,,
14326,b'Onnx T5 for Generation',2021-11-08T16:48:25Z,2022-01-02T15:02:00Z,,,
14325,b'Changed relative imports to absolute to allow convert_graph_to_onnx.py to run as a script.',2021-11-08T15:43:57Z,2021-11-08T15:56:44Z,,,
14324,b'[Bert2Bert] allow bert2bert + relative embeddings',2021-11-08T15:29:25Z,2021-11-09T19:26:59Z,,,
14323,"b""Why GPT2ForTokenClassification doesn't shift logits and labels?""",2021-11-08T13:55:49Z,2021-11-09T04:37:43Z,,,
14322,b'Adding some quality of life for `pipeline` function.',2021-11-08T13:26:20Z,2021-11-10T09:18:36Z,,,
14321,b'FX tracing improvement',2021-11-08T10:53:43Z,2022-02-07T21:25:33Z,,,
14320,b'[Marian Conversion] Fix eos_token_id conversion in conversion script',2021-11-08T10:25:35Z,2021-11-08T10:42:35Z,,,
14319,b'[TFWav2Vec2Model] Fix input shapes in TFWav2Vec2WeightNormConv1D',2021-11-08T10:19:23Z,2021-11-08T12:58:29Z,,,
14318,b'[Tests] Update audio classification tests to support torch 1.10',2021-11-08T10:06:13Z,2021-11-08T11:15:56Z,,,
14317,b'Fixing tests on master.',2021-11-08T09:21:42Z,2021-11-08T13:28:26Z,,,
14316,b'Fixing mutable default argument in `pipeline`.',2021-11-08T09:11:03Z,2021-11-08T15:22:28Z,,,
14315,b'Italian roberta model takes 3 minutes to load',2021-11-08T01:03:58Z,2021-12-16T15:01:47Z,,,
14314,b'convert_graph_to_onnx.py quantization still has relative imports which break when running as a script.',2021-11-07T23:23:09Z,2021-11-09T20:08:45Z,,,
14312,b'LED models give: `IndexError: index out of range in self`',2021-11-07T19:32:40Z,2021-12-18T15:01:43Z,,,
14311,b'question answering pipeline throws error when handle_impossible_answer=True',2021-11-07T13:34:10Z,2021-11-08T14:14:23Z,,ValueError,"ValueError: can only convert an array of size 1 to a Python scalar"
14310,b'Update the example of exporting Bart + BeamSearch to ONNX module to resolve comments.',2021-11-07T13:09:48Z,2021-12-06T13:01:52Z,,,
14309,b'trainer gradient_accumulation_steps',2021-11-07T06:59:31Z,2021-12-16T15:01:48Z,,,
14308,b'Pretrained model outputs all zeros on GPU in a docker environment',2021-11-07T04:46:53Z,2021-12-16T15:01:49Z,,,
14307,b'`BatchFeature` performance improvement: convert `List[np.ndarray]` to `np.ndarray` before converting to pytorch tensors',2021-11-07T01:36:08Z,2021-11-10T03:23:08Z,,,
14306,b'`BatchFeature`: Convert `List[np.ndarray]` to `np.ndarray` before converting to pytorch tensors',2021-11-07T01:06:17Z,2021-11-10T03:23:09Z,,,
14305,b'Predictions for pre-tokenized tokens with Roberta have strange offset_mapping',2021-11-06T22:58:28Z,2022-01-10T08:53:19Z,,,
14304,b'transformers_4.13.devo giving error during saving model',2021-11-06T15:20:52Z,2021-12-16T15:01:51Z,,KeyError,"KeyError: 'encoder\\.embed_tokens\\.weight'"
14303,b'Environment errors need better actionable error reporting',2021-11-06T14:42:30Z,2021-12-15T15:01:46Z,,,
14302,"b""tokenize_plus doesn't work when moving from pip to conda""",2021-11-06T14:31:11Z,2021-11-06T23:58:24Z,,**TypeError,"**TypeError: _tokenize() got an unexpected keyword argument 'truncation'**"
14301,"b'Inference with a Dataset using pipeline in a tokenclassification job causes a ValueError(""At least one input is required."")'",2021-11-06T03:46:10Z,2021-12-08T08:54:25Z,,,
14300,b'Remove `DPRPretrainedModel` from docs',2021-11-05T23:37:57Z,2021-11-06T13:41:03Z,,,
14299,b'is pooler output vector directly comparable to vector representation of word?',2021-11-05T23:35:03Z,2022-01-07T15:02:19Z,,,
14298,b'Small change to Wav2Vec2 model to support Tensor-Parallelism with DeepSpeed',2021-11-05T23:10:37Z,2021-11-09T02:00:06Z,,,
14297,b'run_summarization.py - num_update_steps_per_epoch calculation',2021-11-05T22:55:00Z,2021-11-15T13:45:51Z,,,
14296,b'Expand dynamic supported objects to configs and tokenizers',2021-11-05T21:10:13Z,2021-11-08T20:28:25Z,,,
14295,b'Add training scripts for LayoutLMv2 model',2021-11-05T20:02:28Z,2022-01-01T15:01:51Z,,,
14294,b'Add new LFS prune API',2021-11-05T17:48:09Z,2021-11-05T22:58:52Z,,,
14293,b'Add notebook INC quantization for text classification tasks',2021-11-05T13:34:11Z,2021-11-10T11:49:44Z,,,
14292,b'Mutable default value for `model_kwargs` in `pipeline` function',2021-11-05T12:54:35Z,2021-11-08T15:22:28Z,,,
14291,b'[Hubert Docs] Make sure example uses a fine-tuned model',2021-11-05T12:52:52Z,2021-11-05T13:09:57Z,,,
14290,b'transformers GPT-2 has wrong implementation in scale in Attention class',2021-11-05T08:17:28Z,2021-11-05T09:45:08Z,,,
14289,b'[tests] Fix SegFormer and BEiT tests',2021-11-05T07:59:41Z,2021-11-06T14:08:58Z,,,
14288,b'Model.generate encoder decoder',2021-11-05T07:15:44Z,2021-11-05T09:30:51Z,,,
14287,b'Fix typo on PPLM example README',2021-11-05T05:28:09Z,2021-11-06T14:33:47Z,,,
14286,b'Update modeling_tf_bert.py',2021-11-05T05:15:58Z,2021-11-28T09:25:00Z,,,
14285,b'How to prevent tokenizer from outputting certain information',2021-11-05T04:55:06Z,2021-11-22T09:07:29Z,,,
14284,b'static masking for BERT or RoBERTa model',2021-11-05T00:59:20Z,2021-12-13T15:07:02Z,,,
14283,b'Pin TF until tests are fixed',2021-11-05T00:45:02Z,2021-11-05T01:15:42Z,,,
14282,b'Mismatch between sentinel token IDs from T5 data collator and T5 tokenizer',2021-11-04T22:41:03Z,2021-11-29T16:30:17Z,,,
14281,b'Request for advanced documentation on the text generation pipeline.',2021-11-04T17:41:48Z,2021-11-06T14:24:54Z,,,
14280,b'Removing Keras version pinning',2021-11-04T16:56:17Z,2021-11-04T17:58:28Z,,,
14279,b'Handle long answer needs to be updated.',2021-11-04T15:07:02Z,2021-11-06T14:04:31Z,,,
14278,b'How to use several pipelines in parallel',2021-11-04T15:03:47Z,2021-12-06T09:43:17Z,,,
14277,b'QuestionAnsweringPipeline cannot handle impossible answer ',2021-11-04T14:37:42Z,2021-11-10T15:55:37Z,,,
14276,b'improve rewrite state_dict missing _metadata',2021-11-04T13:48:51Z,2021-11-04T14:13:24Z,,,
14275,b'LayoutXLM tokenizer issues after last update',2021-11-04T13:43:21Z,2021-11-04T14:54:46Z,,ValueError,"ValueError: Id not recognized"
14274,b'Fixing mishandling of `ignore_labels`.',2021-11-04T12:30:17Z,2021-11-04T13:47:53Z,,,
14273,b'ConvBertForQuestionAnswering hangs on 8x TPU cores using PyTorch / XLA',2021-11-04T12:14:30Z,2022-02-01T15:07:04Z,,,
14272,"b""TokenClassificationPipeline `TypeError: postprocess() got an unexpected keyword argument 'ignore_labels'`""",2021-11-04T11:49:11Z,2021-11-04T13:47:53Z,,TypeError,"TypeError: postprocess() got an unexpected keyword argument 'ignore_labels'"
14271,"b""ConnectionError: Couldn't reach https://raw.githubusercontent.com/huggingface/datasets/1.15.1/metrics/sacrebleu/sacrebleu.py""",2021-11-04T11:35:30Z,2021-12-13T15:07:04Z,,,
14270,b'how can I use convert_marian_to_pytorch.py to convert translation model from marian to pytorch model?',2021-11-04T08:51:51Z,2021-12-13T15:07:05Z,,,
14269,b'Add `ElectraForCausalLM` to enable constructing Electra-based `EncoderDecoderModel`',2021-11-04T08:27:45Z,2021-12-27T11:37:52Z,,,
14268,"b""rewrite state_dict in self.model.save_pretrained(), causing the '_metadata' it saved to be missing.""",2021-11-04T05:13:23Z,2021-11-04T14:13:24Z,,KeyError,"KeyError: 'model.encoder.layers.0.self_attn.k_proj.weight'"
14267,b'About qa example',2021-11-04T03:20:18Z,2021-11-08T14:29:04Z,,,
14266,b'Another metric with the same name already exists.',2021-11-04T02:57:46Z,2021-12-10T14:33:25Z,,"tensorflow.python.framework.errors_impl.AlreadyExistsError, RuntimeError","tensorflow.python.framework.errors_impl.AlreadyExistsError: Another metric with the same name already exists.RuntimeError: Failed to import transformers.modeling_tf_utils because of the following error (look up to see its traceback):"
14265,b'TensorFlow 2.6 error with JAX/FLAX implementation',2021-11-03T22:34:37Z,2021-12-13T11:23:57Z,,"tensorflow.python.framework.errors_impl.AlreadyExistsError, RuntimeError","tensorflow.python.framework.errors_impl.AlreadyExistsError: Another metric with the same name already exists.RuntimeError: Failed to import transformers.modeling_tf_utils because of the following error (look up to see its traceback):"
14264,b'Quality explain',2021-11-03T20:01:16Z,2021-11-03T21:43:19Z,,,
14263,b'Add more instructions to the release guide',2021-11-03T19:27:31Z,2021-11-03T21:45:41Z,,,
14262,b'Pin Keras cause they messed their release',2021-11-03T18:24:37Z,2021-11-03T19:03:10Z,,,
14261,b'T5 truncation : `generate()` produce a tensor of maximum 115 length',2021-11-03T16:23:47Z,2021-12-13T15:07:06Z,,,
14260,b'Fixing slow pipeline tests',2021-11-03T15:40:25Z,2021-11-04T08:49:55Z,,,
14259,b'Get FLOP count for a model',2021-11-03T14:57:35Z,2021-12-12T15:01:46Z,,,
14258,b'[Wav2Vec2] Adapt conversion script',2021-11-03T10:24:26Z,2021-11-03T10:31:40Z,,,
14257,b'Question: dropping transformers layers',2021-11-03T07:51:17Z,2021-11-07T17:55:37Z,,,
14256,b'Transformer banned words decoding',2021-11-03T06:35:34Z,2021-11-03T06:36:12Z,,,
14255,"b""Please don't remove `prepare_seq2seq_batch` in future versions""",2021-11-03T05:50:06Z,2021-11-15T06:57:27Z,,,
14254,b'Document or add more detail on `DPRPretrainedModel`',2021-11-02T21:00:03Z,2021-11-06T13:41:02Z,,,
14253,b'[deepspeed] zero inference',2021-11-02T20:42:09Z,2021-11-23T22:09:16Z,,,
14252,b'Fast tokenizer converter leads to PanicException: no entry found for key',2021-11-02T20:00:09Z,2022-03-05T15:02:27Z,,pyo3_runtime.PanicException,"pyo3_runtime.PanicException: no entry found for key"
14251,b'Update Transformers to huggingface_hub >= 0.1.0',2021-11-02T18:43:35Z,2021-11-02T22:58:43Z,,,
14250,b'Adding utilities to chunk large audio files and read directly from microphone',2021-11-02T16:59:27Z,,,,
14249,b'Fixes Beit training for PyTorch 1.10+',2021-11-02T16:57:34Z,2021-11-02T17:07:21Z,,,
14248,b'TROCR custom dataset',2021-11-02T16:47:10Z,2021-12-12T15:01:47Z,,,
14247,"b""AttributeError: 'T5Config' object has no attribute 'output_scores'""",2021-11-02T16:13:51Z,2021-11-04T18:43:44Z,,`AttributeError,"`AttributeError: 'T5Config' object has no attribute 'output_scores'`"
14246,b'Add PushToHubCallback in main init',2021-11-02T15:55:21Z,2021-11-02T16:15:15Z,,,
14245,b'[Tests] Fix DistilHubert path',2021-11-02T14:25:17Z,2021-11-02T14:53:50Z,,,
14244,b'Add BartForTokenClassification',2021-11-02T14:23:17Z,2021-11-09T00:23:28Z,,,
14243,b'Wrong max_position_embeddings for roberta-large',2021-11-02T14:21:19Z,2021-11-03T08:50:54Z,,IndexError,"IndexError: index out of range in self"
14242,b'CausalLMHead Models do not register head parameters.',2021-11-02T12:13:10Z,2021-12-10T15:01:51Z,,,
14241,b'Fix of issue #13327: Wrong weight initialization for TF t5 model',2021-11-02T12:04:27Z,2021-11-03T16:20:48Z,,,
14240,b'Add ImageGPT',2021-11-02T10:10:33Z,2021-11-18T15:24:34Z,,,
14239,b'Resolved TROCR Microsoft Large Printed Bug #14238',2021-11-02T07:13:50Z,2022-01-06T15:02:19Z,,,
14238,b'Bug in Microsoft TROCR Large',2021-11-02T07:08:01Z,2021-12-10T15:01:52Z,,ValueError,"ValueError: operands could not be broadcast together with shapes (384,384) (3,)"
14237,b'enable bfloat16 support on t5 model for summarization',2021-11-02T02:02:42Z,2021-12-10T15:01:54Z,,,
14236,b'Added an error when `temperature` is defined but `do_sample` is False.',2021-11-02T01:29:50Z,2021-12-03T21:56:55Z,,,
14234,"b""Trainer bug in BertForQuestionAnswering when Pretrained model wasn't trained on NPS""",2021-11-01T21:44:49Z,2021-12-10T15:01:54Z,,,
14233,b'Cl tohoku japanese roberta',2021-11-01T20:16:39Z,2022-02-20T15:02:19Z,,,
14232,b'improving efficiency of mlflow metric logging',2021-11-01T17:25:27Z,2021-11-01T17:46:12Z,,,
14231,b'Re: Add Japanese RoBERTa Model',2021-11-01T13:35:14Z,2022-02-09T15:07:48Z,,,
14230,b'CoNLL2003 ner_tags order mismatch between the dataset from HF and the pretrained model',2021-11-01T13:29:18Z,2022-01-24T20:18:04Z,,,
14229,b'How can i add a Bi-LSTM layer on top of some model?like xlm_roberta',2021-11-01T13:21:15Z,2021-11-01T16:03:15Z,Migration,,
14228,b'input_ids is None when evaluating',2021-11-01T11:34:30Z,2021-11-02T07:34:37Z,,,
14227,"b""Can't access Huggingface page and cant download models from laptop. Can it be IP ban?""",2021-11-01T11:05:36Z,2021-12-09T15:06:25Z,,`ValueError,"`ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.`"
14226,b'Fixing typo in error message.',2021-11-01T10:46:19Z,2021-11-03T18:28:57Z,,,
14225,b'ChunkPipeline (batch_size enabled on `zero-cls` and `qa` pipelines.',2021-11-01T10:12:47Z,2021-12-27T10:26:21Z,,,
14224,b'Tensor location is already handled',2021-11-01T10:11:48Z,2021-11-01T12:42:27Z,,,
14223,b'Fixing `image-segmentation` tests.',2021-11-01T09:04:17Z,2021-11-01T12:25:34Z,,,
14222,b'Bart model converted ONNX inference ',2021-11-01T03:34:41Z,2022-01-23T15:03:18Z,,,
14221,b'Hidden states of BertForPreTraining (load from tf ckpt of google original bert) not exactly equal to the output of extract_features.py in google original bert',2021-10-31T14:55:47Z,2021-11-03T00:41:07Z,,,
14220,b'Model doc examples for GPT-j-6b are slightly misleading for the GPU context',2021-10-31T13:58:59Z,2021-12-09T15:06:27Z,,,
14219,b'Raising exceptions instead of using assertions for few models',2021-10-31T10:02:03Z,2021-11-01T12:53:14Z,,,
14218,"b'""Size mismatch"" when using the same type of model as pretraining during finetuning while ""num_labels"" is diffierent'",2021-10-31T06:01:12Z,2021-12-08T15:01:48Z,,`RuntimeError,"`RuntimeError: Error(s) in loading state_dict for RobertaForSequenceClassification:"
14217,b'the results of run_glue_no_trainer.py are different from those reported in the paper',2021-10-31T05:55:32Z,2021-12-08T15:01:49Z,,,
14216,b'Fix generation docstring',2021-10-31T03:46:40Z,2021-11-02T08:22:45Z,,,
14215,b'Shapes mismatch triggered at modeling_flax_utils',2021-10-30T23:14:52Z,2021-11-08T12:30:19Z,,,
14214,"b""Can't load tokenizer for 'facebook/hubert-base-ls960'""",2021-10-30T20:33:42Z,2021-12-08T15:01:49Z,,OSError,"OSError: Can't load tokenizer for 'facebook/hubert-base-ls960'. Make sure that:"
14213,"b""fx: don't copy by reference""",2021-10-30T12:45:46Z,2021-12-07T15:02:04Z,,,
14212,b'Update Seq2Seq QA example script to use SQuAD metric.',2021-10-29T17:49:40Z,2021-11-09T07:17:38Z,,,
14211,b'Add a condition for checking labels',2021-10-29T16:10:39Z,2021-10-29T17:12:11Z,,,
14210,b'Add `inference_mode` back to `image_segmentation`',2021-10-29T15:40:57Z,2021-11-29T16:02:55Z,,,
14209,b'Fix pipeline tests env and fetch',2021-10-29T13:06:50Z,2021-10-29T13:35:05Z,,,
14208,b'[QuestionGeneration] RuntimeError: Integer division of tensors using div or / is no longer supported',2021-10-29T12:58:20Z,2021-12-06T15:02:17Z,,RuntimeError,"RuntimeError: Integer division of tensors using div or / is no longer supported, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead."
14207,b'[GPT2] Add lm_attention_mask as a optional argument',2021-10-29T12:43:42Z,2022-02-24T15:08:05Z,,,
14206,"b'Does the \'bad_words_ids\' argument in the ""generate function"" works? '",2021-10-29T11:02:13Z,2021-12-25T15:01:59Z,,,
14205,b'Add option to not load pretrained weights in `AutoModel.from_pretrained()`',2021-10-29T09:16:37Z,2021-10-29T16:01:31Z,,,
14204,b'Fixing image segmentation with inference mode.',2021-10-29T08:38:57Z,2021-10-29T15:24:09Z,,,
14203,b'Intel OpenVINO backend (inference only)',2021-10-29T06:54:07Z,2021-11-29T18:03:31Z,,,
14202,b'Fix the write problem in trainer.py comment',2021-10-29T06:13:25Z,2021-11-01T13:24:04Z,,,
14201,"b""Couldn't reproduce DistilBERT downstream tasks performance on SQuAD dataset.""",2021-10-29T01:45:10Z,2021-12-06T15:02:19Z,,,
14200,b'Trainer batch size auto scaling',2021-10-28T18:56:49Z,2021-12-06T15:02:20Z,,,
14199,b'run_t5_mlm_flax.py ',2021-10-28T18:56:36Z,2022-01-09T15:01:58Z,,,
14198,b'use functional interface for softmax in attention',2021-10-28T18:28:04Z,2021-11-30T16:47:33Z,,,
14197,b'Fix EncoderDecoderModel docs',2021-10-28T15:39:47Z,2021-10-28T16:01:00Z,,,
14196,b'[T5v1_1] Add lm model pretrained models as well',2021-10-28T15:32:00Z,2021-11-01T14:15:15Z,,,
14195,b'Help training TrOCR',2021-10-28T14:58:10Z,2021-12-06T15:20:30Z,,,
14194,"b""AttributeError: 'str' object has no attribute 'squeeze'""",2021-10-28T14:50:09Z,2021-12-05T15:01:48Z,,AttributeError,"AttributeError: 'str' object has no attribute 'squeeze'"
14193,b'Adding support for `truncation` parameter on `feature-extraction` pipeline.',2021-10-28T14:20:41Z,2021-11-03T14:48:01Z,,,
14192,b'Add audio-classification benchmarking results',2021-10-28T12:42:08Z,2021-10-28T12:59:18Z,,,
14191,b'Fix SEW-D implementation differences',2021-10-28T12:38:05Z,2021-10-28T13:22:18Z,,,
14190,b'[GPTJ] enable common tests and few fixes',2021-10-28T12:18:06Z,2021-11-01T17:08:52Z,,,
14189,b'T5-v1.1 loss go to nan when fp16 training was enabled',2021-10-28T10:26:33Z,2021-11-03T02:15:25Z,,,
14188,b'[Flax] Add Flax implementation of `BlenderbotSmallModel`',2021-10-28T09:57:21Z,2021-12-02T08:51:48Z,,,
14187,b'ValueError when converting dialogpt to onnx format',2021-10-28T09:30:49Z,2022-01-09T15:01:59Z,,`ValueError,"`ValueError: The type of axis index is expected to be an integer"
14186,b'Replace assertions with RuntimeError exceptions',2021-10-28T05:56:32Z,2021-10-28T21:17:43Z,,,
14185,b'Can we save tokenized datasets?',2021-10-28T05:44:32Z,2021-12-05T15:01:49Z,,,
14184,"b""Argument 'filter_value' for the _get_logits_warper function""",2021-10-28T00:24:44Z,2021-12-21T17:51:11Z,,,
14183,b'Pipeline feature extraction: tensor size mismatch',2021-10-27T22:38:42Z,2021-11-03T14:48:01Z,,RuntimeError,"RuntimeError: The size of tensor a (578) must match the size of tensor b (512) at non-singleton dimension 1"
14182,b'Add BigBird/BigBird Pegasus to models exportable with ONNX',2021-10-27T20:59:53Z,2021-12-08T15:01:51Z,,,
14181,b'[modeling_utils] respect original dtype in _get_resized_lm_head',2021-10-27T20:25:58Z,2021-10-28T02:01:50Z,,,
14180,b'Generalize problem_type to all sequence classification models',2021-10-27T15:39:51Z,2021-10-29T14:32:57Z,,,
14179,b'one of the variables needed for gradient computation has been modified by an inplace operation',2021-10-27T15:35:19Z,2021-12-05T15:01:50Z,,RuntimeError,"RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.LongTensor [1, 20]] is at version 4; expected version 3 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!"
14178,b'[Pipelines] Fix ASR model types check',2021-10-27T14:00:49Z,2021-10-27T14:17:48Z,,,
14177,b'Add more missing models to models/__init__.py',2021-10-27T13:26:50Z,2021-11-01T10:52:36Z,,,
14176,b'IBert Problems of hugging face pretrained',2021-10-27T12:47:00Z,2021-12-04T15:02:03Z,,,
14175,b'[Gradient checkpointing] Enable for Deberta + DebertaV2 + SEW-D',2021-10-27T12:28:41Z,2021-10-27T13:47:20Z,,,
14174,b'Add DistilHuBERT ',2021-10-27T12:12:24Z,2021-10-27T17:17:31Z,,,
14173,b'How to train bilingual models?',2021-10-27T11:20:51Z,2021-12-04T15:02:04Z,,,
14172,b'Clarify QA examples',2021-10-27T10:09:45Z,2021-11-01T11:52:22Z,,,
14171,b'SEW - Masked Spec errors out in training',2021-10-27T09:20:01Z,2021-10-27T09:23:42Z,,,
14170,b'The pytorch example question-answering/run_qa_beam_search.py do not work',2021-10-27T07:45:16Z,2021-10-29T05:19:47Z,,ValueError,"ValueError: 32005 is not in list"
14169,b'Torch 1.10',2021-10-27T02:22:14Z,2021-10-29T17:43:44Z,,,
14168,b'Add documentation for multi-label classification',2021-10-27T00:26:26Z,2021-11-30T16:34:42Z,,,
14167,b'Fix gelu test for torch 1.10',2021-10-26T22:26:44Z,2021-10-27T02:20:52Z,,,
14166,b'fix typos in error messages in speech recognition example and modelcard.py',2021-10-26T20:21:15Z,2021-10-26T20:36:26Z,,,
14165,b'Remove n_ctx from configs',2021-10-26T16:22:51Z,2021-10-29T09:50:25Z,,,
14164,b'Extracting Neutral sentiment from Hugginface model',2021-10-26T16:17:50Z,2021-12-26T15:01:48Z,,,
14163,b'LayoutLMv2 functionality.',2021-10-26T15:30:19Z,2021-12-01T15:20:43Z,,,
14162,b'Decoding Large Audio Files Using Wav2Vec2ForCTC Model',2021-10-26T15:28:07Z,2021-11-26T15:08:12Z,,,
14161,"b""[Speech Recognition] - Distributed training: Make sure vocab file removal and creation don't interfer """,2021-10-26T13:31:56Z,2021-10-26T13:59:33Z,,,
14160,b'Support for new LayoutLMv2 in Auto Classes (specifically AutoModelForMaskedLM)',2021-10-26T12:43:32Z,2021-12-03T15:01:55Z,,,
14159,b'run_speech_recognition_ctc.py throwing error when using own dataset',2021-10-26T12:31:20Z,2021-12-03T15:01:56Z,,pyarrow.lib.ArrowNotImplementedError,"pyarrow.lib.ArrowNotImplementedError: Unsupported cast from struct<train: struct<name: string, num_bytes: int64, num_examples: int64, dataset_name: null>> to list using function cast_list"
14158,b'Add SEW CTC models',2021-10-26T11:57:49Z,2021-10-27T09:21:09Z,,,
14157,b'[Trainer] Push to hub takes too much space for local `.git` folder',2021-10-26T11:53:22Z,2021-11-05T22:58:52Z,,,
14156,"b""User-defined callback can't use logging""",2021-10-26T11:20:36Z,2021-10-28T02:01:11Z,,,
14155,b'Include Keras tensor in the allowed types',2021-10-26T10:13:05Z,2021-10-26T14:09:00Z,,,
14154,b'[Speech Recognition CTC] Add auth token to fine-tune private models',2021-10-26T09:27:07Z,2021-10-26T11:08:18Z,,,
14153,b'how to reproduce distilbert pretrain on TF2.x?',2021-10-26T07:03:31Z,2021-12-03T15:01:57Z,,,
14152,b'How to save wrapped DistilBERT without using `save_pretrained`?',2021-10-26T06:20:48Z,2021-11-25T19:02:07Z,,IndexError,"IndexError: list index out of range"
14151,b'Add vision_encoder_decoder to models/__init__.py',2021-10-25T21:14:20Z,2021-10-26T11:36:17Z,,,
14150,b'Typo on ner accelerate example code',2021-10-25T19:10:36Z,2021-10-26T20:23:41Z,,,
14149,b'Examples: Tokenize only max_samples used for train/eval/prediction',2021-10-25T18:28:53Z,2022-01-07T15:02:23Z,,,
14148,b'Add TFVisionEncoderDecoderModel',2021-10-25T16:36:25Z,2022-01-10T18:30:15Z,,,
14147,"b'OnnxRuntime error "" The input tensor cannot be reshaped to the requested shape.""'",2021-10-25T15:44:54Z,2021-12-04T15:02:06Z,,`onnxruntime.capi.onnxruntime_pybind11_state.RuntimeException,"`onnxruntime.capi.onnxruntime_pybind11_state.RuntimeException: [ONNXRuntimeError] : 6 : RUNTIME_EXCEPTION : Non-zero status code returned while running Reshape node. Name:'Reshape_109' Status Message: /Users/runner/work/1/s/onnxruntime/core/providers/cpu/tensor/reshape_helper.h:42 onnxruntime::ReshapeHelper::ReshapeHelper(const onnxruntime::TensorShape &, std::vector<int64_t> &, bool) gsl::narrow_cast<int64_t>(input_shape.Size()) == size was false. The input tensor cannot be reshaped to the requested shape. Input shape:{2}, requested shape:{1,1}`"
14146,b'Questions when training language models from scratch',2021-10-25T15:29:04Z,2021-10-26T14:40:52Z,,,
14145,b'Add Camembert to exportable models with ONNX',2021-10-25T15:19:29Z,2021-10-27T17:48:58Z,,,
14144,b'Ner naming fixes 0.8',2021-10-25T14:36:48Z,2021-10-25T14:37:04Z,,,
14143,"b""Fix AttributeError: 'MMBTConfig' object has no attribute 'use_return_dict'""",2021-10-25T14:28:37Z,2021-12-10T15:01:57Z,,,
14142,b'Replace assertions with ValueError exception',2021-10-25T13:09:23Z,2021-10-26T21:14:29Z,,,
14141,b'Enable DefaultDataCollator class',2021-10-25T13:06:44Z,2021-10-25T14:04:55Z,,,
14140,b'Remove unneeded `to_tensor()` in TF inline example',2021-10-25T13:02:28Z,2021-10-25T14:04:36Z,,,
14139,b'[Design proposal] Fix EncoderDecoderModel classes to be more like BART and T5',2021-10-25T10:36:07Z,2021-10-28T13:29:05Z,,,
14138,b'wrong cache_dir is used when tokenizer is trying to infer config_tokenizer_class',2021-10-25T09:53:31Z,2021-11-24T11:22:03Z,,,
14137,"b""ImportError: cannot import name 'TrOCRProcessor'""",2021-10-25T08:41:03Z,2021-12-03T15:02:01Z,,ImportError,"ImportError: cannot import name 'TrOCRProcessor'"
14136,b'Fix some writing issues in the docs',2021-10-25T01:34:42Z,2021-10-25T11:48:02Z,,,
14135,"b""[ASR example] Can't reproduce the same WER with the same seed""",2021-10-25T01:19:25Z,2022-01-11T15:07:04Z,,,
14134,b'Fix rendering of examples version links',2021-10-25T00:11:21Z,2021-10-25T11:45:45Z,,,
14133,b'Added Beit model ouput class',2021-10-24T21:03:34Z,2021-11-02T17:29:15Z,,,
14132,b'[Seq2Seq] (Byt5) zero loss',2021-10-24T12:46:02Z,2021-10-25T12:58:26Z,,,
14131,b'Replace assert of data/data_collator.py by ValueError',2021-10-24T12:12:53Z,2021-10-27T16:19:10Z,,,
14130,b'DebertaForMultipleChoice',2021-10-23T19:21:24Z,2021-12-01T15:02:48Z,,,
14129,b'[Flax] Fix eval and data_args usage in streaming example',2021-10-23T15:25:29Z,,WIP,,
14128,"b'Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.'",2021-10-23T10:00:07Z,2022-01-24T15:02:38Z,,,
14127,b'Can deepspeed and gradient checkpointing be used at the same time?',2021-10-23T09:31:05Z,2021-11-02T19:04:55Z,,,
14126,b'Fix some typos in the docs',2021-10-23T00:06:16Z,2021-10-25T11:40:44Z,,,
14125,b'Pipeline seems slower in 4.11+',2021-10-22T20:00:39Z,2021-11-01T19:03:07Z,,,
14124,b'Fix lazy init to stop hiding errors in import',2021-10-22T17:33:42Z,2021-10-25T20:53:48Z,,"AttributeError, RuntimeError","AttributeError: module 'huggingface_hub.hf_api' has no attribute 'DatasetInfo'RuntimeError: Failed to import transformers.pipelines because of the following error (look up to see its traceback):"
14123,b'make test fails on `tests/test_benchmark_tf.py` ',2021-10-22T16:50:31Z,2022-01-07T15:02:25Z,,,
14122,b'Rename variables with unclear naming',2021-10-22T13:15:20Z,2021-10-22T17:05:45Z,,,
14121,b'[TPU tests] Enable first TPU examples pytorch',2021-10-22T12:09:20Z,2021-10-27T23:22:29Z,,,
14119,b'[wav2vec2] Add missing --validation_split_percentage data arg',2021-10-22T11:11:21Z,2021-10-22T17:04:55Z,,,
14118,b'Adding `handle_long_generation` paramters for `text-generation` pipeline.',2021-10-22T11:06:17Z,2021-10-29T13:29:28Z,,,
14117,b'Replace assertions with valueError Exeptions',2021-10-22T10:04:54Z,2021-10-22T11:45:33Z,,,
14116,b'[tests] fix hubert test sort',2021-10-22T08:41:47Z,2021-10-22T09:01:27Z,,,
14115,"b'Add LayoutXLMProcessor (and LayoutXLMTokenizer, LayoutXLMTokenizerFast)'",2021-10-22T07:55:51Z,2021-11-03T07:59:45Z,,,
14114,b'[BUG] Tokenizer offset',2021-10-22T06:51:06Z,2021-11-30T15:01:52Z,,,
14113,"b'Add a parameter ""device "" in Tokenizer.__call__()'",2021-10-22T05:37:20Z,2021-11-30T15:01:53Z,,,
14112,b'Update TP parallel GEMM image',2021-10-22T02:32:16Z,2021-10-22T19:57:49Z,,,
14111,b'Replace asserts with exceptions',2021-10-21T23:43:16Z,2021-10-25T09:33:27Z,,,
14110,b'LayoutLMv2 model not supporting training on more than 1 GPU when using PyTorch Data Parallel',2021-10-21T23:37:49Z,,Good First Issue,RuntimeError,"RuntimeError: Caught RuntimeError in replica 1 on device 1."
14109,b'[deepspeed integration] HF Trainer takes over GPUs for DP',2021-10-21T21:24:27Z,2021-11-22T19:27:35Z,DeepSpeed,,
14108,b'Fix a typo in preprocessing docs',2021-10-21T20:51:13Z,2021-10-21T21:00:27Z,,,
14107,b'GPT-J Models Cannot Load If Tokens Have Been Resized Using resize_token_embeddings Method',2021-10-21T20:00:31Z,2021-11-01T17:08:52Z,,RuntimeError,"RuntimeError: Error(s) in loading state_dict for GPTJForCausalLM:"
14106,b'Automatic Mixed Precision Support for (some) Flax Transformers',2021-10-21T19:54:35Z,2021-12-24T15:01:44Z,,,
14105,b'T5ForConditionalGeneration `prepare_inputs_for_generation` causes problems for `sample` function',2021-10-21T19:20:46Z,2021-11-03T07:40:15Z,,,
14104,b'Add Multiple Choice Pytorch Example for Hellaswag',2021-10-21T17:35:48Z,2021-11-30T15:01:55Z,,,
14103,b'Keras callback for prompting metrics',2021-10-21T15:37:51Z,2021-12-22T14:59:57Z,,,
14102,b'Fix typo in comment',2021-10-21T15:12:36Z,2021-10-21T19:29:17Z,,,
14101,b'Comments typo',2021-10-21T15:09:11Z,2021-10-21T19:29:17Z,,,
14100,b'bert onnx to tensorrt wrong',2021-10-21T13:30:02Z,2022-01-11T15:07:07Z,,,
14099,b'[Examples] Add audio classification notebooks',2021-10-21T12:40:59Z,2021-10-21T16:15:47Z,,,
14098,b'Replace assertion with ValueError exception',2021-10-21T11:52:13Z,2021-10-21T19:31:01Z,,,
14097,b'`T5ForSequenceClassification`',2021-10-21T09:47:41Z,2022-02-27T15:02:23Z,,,
14096,b'Add BeitForSemanticSegmentation',2021-10-21T09:32:37Z,2021-11-01T18:55:46Z,,,
14095,b'saving the model after each epoch completion',2021-10-21T09:06:31Z,2021-10-21T09:54:07Z,,,
14094,b'fix typo in license docstring',2021-10-21T08:40:25Z,2021-10-21T19:31:32Z,,,
14093,b'[ASR] Small fix model card creation',2021-10-21T08:22:44Z,2021-10-21T08:30:02Z,,,
14092,b'Different result come from local model and API',2021-10-21T07:27:00Z,2021-10-21T07:45:40Z,,,
14091,b'Replace assertions with ValueError exceptions',2021-10-21T05:53:47Z,2021-10-21T22:07:18Z,,,
14090,b'Fix assertion in models',2021-10-21T05:14:37Z,2021-10-22T14:03:09Z,,,
14089,b'Shift operation in loss computation for seq2seq model',2021-10-21T02:56:45Z,2021-11-28T15:01:58Z,,,
14088,b'Change asserts in src/transformers/models/xlnet/ to raise ValueError',2021-10-20T23:58:49Z,2021-10-21T11:27:33Z,,,
14087,b'Fix broken link in the translation section of task summaries',2021-10-20T18:56:04Z,2021-10-20T19:10:57Z,,,
14086,b'Unexpected sequences_scores in BeamSearchDecoderOnlyOutput',2021-10-20T18:51:32Z,2022-01-26T15:07:06Z,,,
14085,b'Fix ignore_mismatched_sizes',2021-10-20T18:49:49Z,2021-10-21T16:31:30Z,,,
14084,b'[new model] `GPTMeg`  (a clone of `GPT2` with a few tiny changes)',2021-10-20T16:11:28Z,,WIP,,
14083,b'FLAX-T5 - TPU not found Colab',2021-10-20T12:11:55Z,2021-12-04T15:02:09Z,,,
14082,b'Fix convert for newer megatron-lm bert model',2021-10-20T11:36:23Z,2022-01-08T19:33:56Z,,,
14081,b'[Feature Contribution] Disjunctive Positive Constraint Decoding (adding `force_tokens` to `model.generate()`)',2021-10-20T10:54:57Z,2022-02-09T15:59:27Z,,,
14080,b'Add SEW mappings to AutoTokenizer and AutoFeatureExtractor',2021-10-20T10:54:40Z,2021-10-20T11:04:43Z,,,
14079,b'[ASR] Make speech recognition example more general to load any tokenizer',2021-10-20T10:53:43Z,2021-10-20T11:01:42Z,,,
14078,b'gramformer installation error.',2021-10-20T10:28:31Z,2021-11-28T15:02:00Z,,,
14077,b'Fix assert in src/transformers/data/datasets/language_modeling.py',2021-10-20T09:53:21Z,2021-10-20T11:54:39Z,,,
14076,b'Fix test_configuration_tie in FlaxEncoderDecoderModelTest',2021-10-20T08:03:01Z,2021-11-02T10:02:42Z,,,
14075,b'Add missing autocast() in Trainer.prediction_step()',2021-10-20T07:05:20Z,2021-10-20T11:51:30Z,,,
14074,b'Replace assertions in src/transformers/data/datasets/language_modeling.py',2021-10-20T06:40:47Z,2021-10-20T06:41:49Z,,,
14073,b'ignore_mismatched_sizes do not work propoerly',2021-10-20T05:36:23Z,2021-10-21T16:31:29Z,,RuntimeError,"RuntimeError: Error(s) in loading state_dict for BertModel:"
14072,b'replace assert with exception in src/transformers/utils/model_pararallel_utils.py',2021-10-20T03:41:56Z,2021-10-20T11:43:45Z,,,
14071,b'Trainer._load_rng_state() path fix (#14069)',2021-10-20T01:12:15Z,2021-10-20T02:06:20Z,,,
14070,"b""AttributeError: 'BertTokenizer' object has no attribute 'encode_plus'""",2021-10-19T23:18:35Z,2021-10-25T19:02:56Z,,,
14069,b'Trainer._load_rng_state() misbehavior',2021-10-19T21:16:12Z,2021-10-20T02:06:19Z,,,
14068,b'Which is better ?',2021-10-19T20:34:35Z,2021-12-03T15:02:08Z,,,
14067,b'Add ASR colabs',2021-10-19T18:24:17Z,2021-10-20T09:51:42Z,,,
14066,b'Add QDQBert model and quantization examples of SQUAD task',2021-10-19T17:15:45Z,2021-11-19T18:33:40Z,,,
14065,b'Unexpected generated token probabilities derived from scores',2021-10-19T17:05:34Z,2021-10-20T15:13:36Z,,,
14064,b'update to_py_obj to support np.number',2021-10-19T16:34:42Z,2021-10-19T18:30:53Z,,TypeError,"TypeError: Can't convert 110 to Sequence"
14063,b'[WIP] Tail free sampling implementation',2021-10-19T14:52:16Z,2022-01-02T15:02:07Z,,,
14062,b'Put `load_image` function in `image_utils.py` & fix image rotation issue',2021-10-19T12:46:31Z,2021-11-03T13:53:05Z,,,
14061,b'Replace assertions with ValueError exceptions',2021-10-19T12:08:52Z,2021-10-21T11:32:28Z,,,
14060,b'[WIP] Verify PT <-> Flax equivalence tests tolerance value',2021-10-19T11:20:53Z,2021-10-19T13:39:53Z,,,
14059,b'Add Camembert to models exportable with ONNX',2021-10-19T08:01:45Z,2021-10-26T09:22:23Z,,,
14058,b'with op Tile must be a compile-time constant.',2021-10-19T02:54:22Z,2022-02-07T15:07:31Z,TensorFlow,InvalidArgumentError,"InvalidArgumentError: 9 root error(s) found."
14057,b'Add QDQBert model and QAT example of SQUAD task',2021-10-18T22:05:20Z,2021-11-05T23:34:41Z,,,
14056,b'Fix typo',2021-10-18T15:34:46Z,2021-10-18T22:03:39Z,,,
14055,b'Fix label attribution in token classification examples',2021-10-18T14:09:41Z,2021-10-20T11:55:15Z,,,
14054,b'Fix save when laod_best_model_at_end=True',2021-10-18T13:46:10Z,2021-10-18T14:22:58Z,,,
14053,b'[torch.fx] Abstract dynamic tracer',2021-10-18T12:50:47Z,2021-10-19T09:43:06Z,,,
14052,b'[Examples] Use Audio feature in speech classification',2021-10-18T12:49:52Z,2021-10-20T09:22:43Z,,,
14051,"b'In `Trainer`, `evaluation_strategy` defaults to `no`, but `save_strategy` defaults to `steps`. Why?'",2021-10-18T12:49:40Z,2021-10-18T13:09:47Z,,ValueError,"ValueError: --load_best_model_at_end requires the save and eval strategy to match, but found"
14050,b'TypeError: Inputs to a layer should be tensors. Got: last_hidden_state',2021-10-18T12:26:21Z,2021-11-25T15:01:55Z,,TypeError,TypeError: Inputs to a layer should be tensors. Got: last_hidden_state
14049,b'fix typo',2021-10-18T11:26:26Z,2021-10-18T22:03:12Z,,,
14048,b'Update SEW integration test tolerance',2021-10-18T10:58:44Z,2021-10-18T10:58:59Z,,,
14047,b'Add TF<>PT and Flax<>PT everywhere',2021-10-18T10:21:05Z,2021-10-25T21:55:08Z,,,
14046,b'[Flax] Clip fix test',2021-10-18T10:14:32Z,2021-10-18T10:59:18Z,,,
14045,b'[Speech] Move all examples to new audio feature',2021-10-18T09:46:41Z,2021-10-18T10:52:40Z,,,
14044,b'Fixes typo in `modeling_speech_to_text`',2021-10-18T07:57:49Z,2021-10-18T08:24:25Z,,,
14043,b'Running `run_ner_no_trainer.py` with `--label_all_tokens` falsifies seqeval results',2021-10-18T07:42:40Z,2021-10-20T11:55:15Z,,,
14042,b'Bug in the Flaubert tokenizer_config.json do_lowercase option',2021-10-18T07:40:03Z,2021-11-25T15:01:56Z,,,
14041,b'update to_py_obj to support np.int type',2021-10-18T05:39:48Z,2021-10-19T16:17:01Z,,TypeError,"TypeError: Can't convert 110 to Sequence"
14040,b'[Speech] Refactor Examples',2021-10-17T21:01:44Z,2021-10-18T15:43:35Z,,,
14039,b'Enabling Discussion on github',2021-10-17T10:47:30Z,2021-11-25T15:01:57Z,,,
14038,b'Add cross attentions to TFGPT2Model',2021-10-17T10:14:38Z,2021-11-03T08:54:34Z,,,
14037,b'id2label is list but we need dict to load models properly from tf.saved_model',2021-10-17T08:08:04Z,2021-12-07T15:02:11Z,,,
14036,b'[Gradient checkpoining] Update Wav2Vec scripts',2021-10-16T19:42:30Z,2021-11-17T17:37:22Z,,,
14035,b'How to untie input and output word embeddings (make embeddings independent) for Bart?',2021-10-16T19:01:22Z,2021-12-21T15:02:04Z,,,
14033,"b""Text Generation Pipeline doesn't take Truncation = True""",2021-10-16T05:59:43Z,2021-10-29T13:29:28Z,,,
14032,b'[feature request] a tool to clone existing models to make new models with small changes',2021-10-16T04:46:41Z,2022-01-24T20:25:11Z,Feature request,,
14031,b'cannot load character bert',2021-10-15T21:29:54Z,2021-10-16T23:50:52Z,,KeyError,"KeyError: 'character_bert'"
14030,b'Add `LayoutXLMTokenizer` and `LayoutXLMTokenizerFast`',2021-10-15T21:07:46Z,2021-10-22T17:09:06Z,,,
14029,b'Fix: replace assert statements with exceptions in file src/transformers/models/lxmert/modeling_lxmert.py',2021-10-15T19:35:48Z,2021-10-15T19:56:07Z,,,
14028,b'[Docs] More general docstrings',2021-10-15T19:09:01Z,2021-10-15T22:48:38Z,,,
14027,b'[Speech Examples] Add new audio feature',2021-10-15T16:43:17Z,2021-10-17T21:01:03Z,,,
14026,b'[CLIP] minor fixes',2021-10-15T16:21:02Z,2021-10-16T00:08:58Z,,,
14025,b'DeBERTa checkpoints contain `config` keys',2021-10-15T15:48:35Z,2021-12-19T15:01:58Z,,,
14024,b'minor issues in modeling_clip.py',2021-10-15T15:35:37Z,2021-10-16T00:08:58Z,,,
14023,"b""Don't duplicate the elements in dir""",2021-10-15T13:53:20Z,2021-10-16T00:09:54Z,,,
14022,b'[FX] Fix passing None as concrete args when tracing',2021-10-15T12:44:58Z,2021-10-19T08:56:17Z,,,
14021,b'hf wav2vec2 to fairseq',2021-10-15T11:56:35Z,2021-11-28T15:02:02Z,,,
14020,b'FlauBERT cannot perform MLM with customized tokenizer (added tokens to vocabulary)',2021-10-15T11:44:18Z,2021-12-21T15:02:06Z,,"nored, RuntimeError","nored: special_tokens_mask.                                                                                                                                                          RuntimeError: shape '[-1, 68729]' is invalid for input of size 45651992"
14019,b'Add SegFormer',2021-10-15T11:43:57Z,2021-10-28T12:23:52Z,,,
14018,b'Replace assertions with ValueError exceptions',2021-10-15T08:09:49Z,2021-10-16T00:28:13Z,,,
14017,"b'[Bug?] Warning about weights not being initialized, even though not part of the model'",2021-10-15T07:49:15Z,2021-11-22T15:06:11Z,,,
14016,b'Fix weight loading issue',2021-10-15T06:41:55Z,2021-11-15T16:48:40Z,,,
14015,b'Translate README.md to Korean',2021-10-15T04:51:21Z,2021-10-22T11:42:31Z,,,
14014,"b'[Typo] Replace ""Masked"" with ""Causal"" in TF CLM script'",2021-10-15T04:27:26Z,2021-10-21T15:19:30Z,,,
14013,b'[ONNX] Add symbolic function for XSoftmax op for exporting to ONNX.',2021-10-15T04:27:21Z,2021-10-26T19:25:02Z,,,
14012,b'Allow user to choose DDP backends: nccl or gloo (#13441)',2021-10-15T03:01:59Z,2021-11-22T15:06:12Z,,,
14011,b'Allow user to choose DDP backends: nccl or gloo (#13441)',2021-10-15T02:51:14Z,2021-10-15T02:57:02Z,,,
14010,b'Bert: relative_key position embedding causes error in encoder-decoder setup',2021-10-14T20:57:59Z,2021-11-09T19:26:59Z,,RuntimeError,"RuntimeError: einsum(): operands do not broadcast with remapped shapes [original->remapped]: [1, 8, 2, 64]->[1, 8, 1, 2, 64] [3, 3, 64]->[1, 1, 3, 3, 64]"
14009,b'TF Model train and eval step metrics for seq2seq models.',2021-10-14T14:50:42Z,2021-10-19T11:14:22Z,,,
14008,b'[Testing] Move speech datasets to `hf-internal` testing ...',2021-10-14T13:38:12Z,2021-10-14T13:46:22Z,,,
14007,b'FX helper functions and quick fixes',2021-10-14T12:42:03Z,2021-11-16T08:41:46Z,,,
14006,b'Replace assertion with ValueError exception',2021-10-14T11:14:49Z,2021-10-14T12:57:12Z,,,
14005,b'typo',2021-10-14T10:54:39Z,2021-11-21T15:02:09Z,,,
14004,b'typo',2021-10-14T10:48:32Z,2021-11-21T15:02:10Z,,,
14003,b'Integration.py',2021-10-14T10:17:58Z,2021-11-21T15:02:11Z,,,
14002,b'TFEncoderDecoderModel loading TF weights issue',2021-10-14T08:17:09Z,2021-11-15T16:48:40Z,,,
14001,b'How should I try to feed new memory states to the next segments',2021-10-14T03:37:09Z,2021-10-14T15:34:55Z,,,
14000,b'Add strong test for configuration attributes',2021-10-14T01:23:18Z,2021-10-14T13:07:09Z,,,
13999,"b""Discrepancy in tokenization results using HF's AlbertTokenizer and sentencepiece library""",2021-10-13T23:47:16Z,2021-10-18T22:22:25Z,,,
13998,b'Saving/Reloading the Flax T5 model',2021-10-13T23:46:49Z,2021-10-14T15:21:00Z,,,
13997,b'[performance/precision] adding `jit.script` to activation functions',2021-10-13T22:27:20Z,,WIP,AssertionError,"AssertionError: Tensors are not equal!"
13996,b'Scatter dummies + skip pipeline tests',2021-10-13T21:26:37Z,2021-10-14T19:30:27Z,,,
13995,b'Fix FNet tokenizer tests',2021-10-13T21:25:13Z,2021-10-14T13:07:52Z,,,
13994,b'Conversational pipeline: pop instead of get to avoir duplicate kwargs',2021-10-13T21:21:34Z,2021-11-15T11:02:42Z,,,
13993,b'Use different data collator for train and eval dataset in trainer',2021-10-13T21:01:51Z,2021-10-14T02:10:29Z,,,
13992,b'Attributes explicitly defined in model configurations are now overridden by the default type.',2021-10-13T20:45:40Z,2021-10-14T13:07:09Z,,,
13991,b'CLIPProcessor using only single core',2021-10-13T19:56:32Z,2021-11-21T15:02:12Z,,,
13990,b'Logit explosion in MobileBertForNextSentencePrediction example from documentation (and all others tried)',2021-10-13T18:47:18Z,2021-11-28T15:02:03Z,,,
13989,b'Add an API to register objects to Auto classes',2021-10-13T18:33:16Z,2021-10-18T14:22:46Z,,,
13988,b'Allow single byte decoding',2021-10-13T18:02:20Z,2021-10-14T08:54:21Z,,,
13987,b'Intel OpenVINO inference backend',2021-10-13T14:00:11Z,2021-12-04T15:02:12Z,,,
13986,b'Trainer: Cannot train with 3+ GPUs / Uneven Memory Consumption',2021-10-13T13:56:53Z,2021-10-15T12:09:03Z,,,
13985,b'Tokenizers integration into onnx models \xf0\x9f\xa4\x97',2021-10-13T09:36:56Z,2021-11-21T15:02:13Z,,,
13984,b'Intel OpenVINO backend',2021-10-13T08:35:43Z,2021-10-13T08:36:09Z,,,
13983,b'Parameter max_new_tokens is always overshadow by max_length in model.generate()',2021-10-12T23:10:11Z,2021-10-13T19:19:37Z,,,
13982,b'ConversationalPipeline Not Compatible With HuggingFace SageMaker Deploy',2021-10-12T20:56:26Z,2021-10-14T21:05:00Z,,,
13981,"b""ModuleNotFoundError: No module named 'transformers.models.fnet.configuration_fnet""",2021-10-12T20:05:53Z,2021-11-21T15:02:14Z,,ModuleNotFoundError,"ModuleNotFoundError: No module named 'transformers.models.fnet.configuration_fnet'"
13980,b'[parallel doc] dealing with layers larger than one gpu',2021-10-12T19:20:50Z,2021-10-12T22:37:55Z,,,
13979,b'input params in RobertaForQuestionAnswering',2021-10-12T18:21:00Z,2021-10-17T17:29:39Z,,,
13978,b'```input_embeds``` keyword not working properly (GPT2)',2021-10-12T16:53:34Z,2021-11-21T15:02:15Z,,TypeError,"TypeError: forward() got an unexpected keyword argument 'inputs_embeds'"
13977,b'[Wav2Vec2] Make sure tensors are always bool for mask_indices',2021-10-12T16:01:02Z,2021-10-12T16:17:06Z,,,
13976,b'Fixing the lecture values by making sure defaults are not changed',2021-10-12T15:53:04Z,2021-10-12T16:18:20Z,,,
13975,b'Issues with new LayoutLMv2 ',2021-10-12T13:36:22Z,2021-10-26T12:37:55Z,,RuntimeError,"RuntimeError: The size of tensor a (44) must match the size of tensor b (49) at non-singleton dimension 1"
13974,b'Specify im-seg mask greyscole mode',2021-10-12T13:07:20Z,2021-10-12T14:26:19Z,,,
13973,"b""examples/legacy/token-classification\xe3\x80\x80doesn't work well""",2021-10-12T09:25:16Z,2021-11-19T15:06:17Z,,,
13972,b'LayoutLMv2Processor does not accept the XLMRobertaTokenizerFast',2021-10-12T08:09:22Z,,Good First Issue,ValueError,"ValueError: `tokenizer` has to be of type <class 'type'> or <class 'type'>, but is <class 'transformers.models.xlm_roberta.tokenization_xlm_roberta_fast.XLMRobertaTokenizerFast'>"
13971,b'Bug?/Question? Vocab of RoBERTa different from GPT2',2021-10-12T06:42:02Z,2021-10-12T06:53:14Z,,,
13970,b'Weird implementation of GPT2',2021-10-12T04:53:43Z,2021-10-12T05:17:04Z,,,
13969,b'How to load a fine-tuned model and do predictions?',2021-10-12T04:16:24Z,2021-11-19T15:06:19Z,,,
13968,b'Fix missing tpu variable in benchmark_args_tf.py',2021-10-12T02:25:43Z,2021-10-12T03:30:04Z,,,
13967,b'Add TFCLIPModel',2021-10-11T20:22:28Z,2021-12-23T16:19:45Z,,,
13966,"b""Feature request : add leave=True to dataset.map to enable tqdm nested bars (and whilst we're at it couldn't we get a way to access directly tqdm underneath?)""",2021-10-11T20:16:31Z,2021-10-11T20:48:54Z,,,
13965,b'In pre-training a model how can I resume from last saved model?',2021-10-11T19:41:28Z,2021-11-19T15:06:20Z,,,
13964,b'[wav2vec2] fix --gradient_checkpointing',2021-10-11T17:40:42Z,2021-11-11T16:50:21Z,,,
13963,b'Add Unispeech & Unispeech-SAT',2021-10-11T16:21:20Z,2021-10-26T16:59:59Z,,,
13962,b'Add the SEW and SEW-D speech models',2021-10-11T13:41:24Z,2021-10-15T15:26:27Z,,,
13961,b'[Gradient checkpoining] Correct disabling `find_unused_parameters` in Trainer when gradient checkpointing is enabled',2021-10-11T10:35:38Z,2021-10-11T13:34:02Z,,,
13960,b'BartEnocder add set_input_embeddings',2021-10-11T07:54:18Z,2021-10-25T11:58:30Z,,,
13959,"b""Error In Fine-Tuning MarianMT's opus-mt model ValueError: The two structures don't have the same sequence length. Input structure has length 4, while shallow structure has length 3""",2021-10-11T07:51:09Z,2022-01-14T17:19:33Z,,,
13958,b'Add support for `push_to_hub` for `AutoFeatureExtractor`',2021-10-11T07:08:30Z,2022-01-24T15:02:42Z,,,
13957,b'Replace assert with unittest assertions',2021-10-10T22:30:49Z,2021-10-11T14:21:47Z,,,
13956,b'Training on Tpu got stuck at 0%',2021-10-10T20:08:40Z,2021-11-19T15:06:21Z,,,
13955,"b'Replace assert by ValueError of src/transformers/models/electra/modeling_{electra,tf_electra}.py and all other models that had copies'",2021-10-10T19:47:11Z,2021-10-11T17:58:09Z,,,
13954,b'log_softmax runtimeerror when utilizing generation gradients in beam_search ',2021-10-10T19:06:12Z,2021-11-21T15:02:16Z,,,
13953,b'BART with inputs_embeds crashes with shift_tokens_right',2021-10-10T18:53:30Z,2021-12-21T10:57:42Z,,,
13952,b'Documentation for exporting custom architecture to ONNX',2021-10-10T18:14:12Z,2021-11-19T15:06:22Z,,,
13951,b'Raise ValueError instead of asserts in src/transformers/benchmark/benchmark.py',2021-10-10T17:00:34Z,2021-10-11T08:59:16Z,,,
13950,b'Few-Shot Learning Attempt Failure on HF Inference API with GPT-J',2021-10-10T13:38:36Z,2021-12-13T15:07:14Z,,,
13949,b'Change DataCollatorForSeq2Seq to pad labels to a multiple of `pad_to_multiple_of`',2021-10-10T13:17:42Z,2021-10-11T13:35:20Z,,,
13948,b'bert obtained different results on mac and linux',2021-10-10T11:04:12Z,2021-10-10T11:22:50Z,,,
13947,b'The tftrainer custom_datasets colab does not work with t5model',2021-10-10T04:50:59Z,2021-11-18T15:02:12Z,,,
13946,b'Incosistent vocab sizes in t5-base model & tokenizer',2021-10-09T20:59:30Z,2021-11-18T15:02:13Z,,,
13945,b'Raise exceptions instead of asserts in  src/transformers/data/processors/xnli.py',2021-10-09T15:44:02Z,2021-10-11T14:22:35Z,,,
13944,b'The first inference timing for pure PyTorch is unexpectedly fast and should be ignored',2021-10-09T15:35:11Z,2021-11-15T15:53:48Z,,,
13943,b'Raise exception instead of assert benchmark_args_utils.py',2021-10-09T14:12:28Z,2021-10-10T00:14:14Z,,,
13942,b'fix issue #13904 -attribute does not exist- ',2021-10-09T12:01:15Z,2021-10-09T13:07:39Z,,,
13941,b'Pretrained model download slowly\xef\xbc\x8cIt will speed up when I delete the folder $USER/.cache/huggingface',2021-10-09T01:12:54Z,2021-11-09T01:05:25Z,,,
13940,b'Make username optional in hub_model_id',2021-10-08T21:51:11Z,2021-10-11T16:03:58Z,,,
13939,"b'Raise exceptions instead of asserts in src/transformers/models/bart/modeling_flax_[bart, marian, mbart, pegasus].py'",2021-10-08T19:39:06Z,2021-10-14T14:12:33Z,,,
13938,b'Raise exceptions instead of asserts in src/transformers/data/processors/utils.py',2021-10-08T19:30:31Z,2021-10-11T16:21:50Z,,,
13937,b'Remove wrong model_args supplied',2021-10-08T15:06:26Z,2021-10-14T01:28:11Z,,,
13936,b'Fixed typo: herBERT -> HerBERT',2021-10-08T14:14:13Z,2021-10-08T14:27:33Z,,,
13935,b'Add vim undo files to .gitignore',2021-10-08T13:17:01Z,2021-11-15T15:07:17Z,,,
13934,b'Update bug-report.md',2021-10-08T12:22:06Z,2021-10-08T18:41:51Z,,,
13933,b'Wrong decoder_hidden_states from generate() function of BartForConditionalGeneration',2021-10-08T10:29:00Z,2021-11-15T15:07:18Z,,,
13932,b'Licensing of LayoutLM models',2021-10-08T09:39:00Z,2021-11-08T11:05:53Z,,,
13931,"b""'BertTokenizer' object has no attribute 'tokens_trie'""",2021-10-08T09:09:58Z,2021-11-24T15:03:07Z,,AttributeError,"AttributeError: 'BertTokenizer' object has no attribute 'tokens_trie'"
13930,b'Fix fast tokenization problems',2021-10-08T08:54:25Z,2021-11-10T10:16:45Z,,,
13929,"b""AttributeError: 'SequenceClassifierOutput' object has no attribute 'pooler_output'""",2021-10-08T04:58:56Z,2021-10-08T12:17:26Z,,**AttributeError,"**AttributeError: 'SequenceClassifierOutput' object has no attribute 'pooler_output'**"
13928,"b'[megatron_gpt2] dynamic gelu, add tokenizer, save config'",2021-10-08T00:33:13Z,2021-10-26T16:09:54Z,,,
13927,"b""valhalla/distilbart-mnli-12-3' is a correct model identifier listed on 'https://huggingface.co/models'""",2021-10-07T22:53:05Z,2021-11-15T15:07:20Z,,,
13926,b'Honor existing attention mask in tokenzier.pad',2021-10-07T21:57:06Z,2021-10-11T13:12:10Z,,,
13925,b'[performance doc] add tables with DDP/DP/no-DP benchmarks',2021-10-07T21:24:57Z,,WIP,,
13924,b'rgwwfq',2021-10-07T20:59:54Z,2021-10-08T06:09:27Z,New model,,
13923,b'Rewrite guides for fine-tuning with Datasets',2021-10-07T20:23:27Z,2021-11-09T19:12:50Z,Documentation,,
13922,b'Fixes a minor doc issue (missing character)',2021-10-07T15:59:35Z,2021-10-07T16:10:19Z,,,
13921,b'[Wav2Vec2] Fix mask_feature_prob',2021-10-07T15:24:54Z,2021-10-07T16:07:33Z,,,
13920,"b""'BertEncoder' object has no attribute 'gradient_checkpointing'""",2021-10-07T14:08:33Z,2021-11-21T15:02:19Z,,AttributeError,"AttributeError: 'BertEncoder' object has no attribute 'gradient_checkpointing'"
13919,b'[Generation] Fix max_new_tokens',2021-10-07T10:27:12Z,2021-10-08T15:28:19Z,,,
13918,b'Adding support for tokens being suffixes or part of each other.',2021-10-07T07:51:37Z,2021-10-08T08:10:39Z,,,
13917,b'input decoder embedding for model.generate()',2021-10-07T07:50:03Z,2021-11-24T15:03:08Z,,,
13916,b'Add missing whitespace to multiline strings',2021-10-07T05:46:52Z,2021-10-07T13:22:11Z,,,
13915,b'[trainer] memory metrics: add memory at the start report',2021-10-07T04:40:24Z,2021-10-07T17:29:02Z,,,
13914,b'Data collator ignores the input attention_mask',2021-10-06T22:43:22Z,2021-10-11T13:12:09Z,,,
13913,b'Getting out of vocabulary tokens for pretrained models.',2021-10-06T21:24:58Z,2021-11-14T15:01:51Z,,,
13912,"b""`from_pretrained` doesn't validate its kwargs""",2021-10-06T20:13:29Z,2021-11-14T15:01:52Z,,,
13911,b'[Trainer] Fix nan-loss condition',2021-10-06T16:30:46Z,2021-10-06T16:40:52Z,,,
13910,b'tensorflow LED global_attention_mask shape mismatch error',2021-10-06T15:53:35Z,2021-12-19T15:02:01Z,,"tensorflow.python.framework.errors_impl.InvalidArgumentError, #NOTE","tensorflow.python.framework.errors_impl.InvalidArgumentError:  Incompatible shapes: [1,2048,12,1045] vs. [1,2048,12,1025]#NOTE: same inputs as in the TensorFlow example!"
13909,b'#12789 Replace assert statements with exceptions',2021-10-06T15:46:43Z,2021-10-07T13:09:01Z,,,
13908,b'How to export to ONNX distilbert with question-answering head',2021-10-06T15:28:10Z,2021-11-14T15:01:53Z,,,
13907,b'Raise exceptions instead of asserts in utils/download_glue_data',2021-10-06T15:27:44Z,2021-10-07T07:14:24Z,,,
13906,b'Wrong isinstance for AutoTokenizers with unspecified tokenizer_class',2021-10-06T14:52:20Z,2021-10-06T23:50:56Z,,,
13905,b'`min_length` in `generate` method',2021-10-06T14:45:07Z,2021-10-07T06:55:49Z,,,
13904,b'Error: Attribute does not exist',2021-10-06T14:08:04Z,2021-10-09T13:07:52Z,,,
13903,b'mBART50 finetuned many-to-many model failed to generate translation',2021-10-06T12:31:11Z,2021-10-06T12:34:42Z,,,
13902,b'Best model not loaded in Trainer.hyperparameter_search',2021-10-06T12:06:10Z,2021-10-18T08:09:42Z,,,
13901,b'Replace assert statements with exceptions (#13871)',2021-10-06T10:36:06Z,2021-10-22T11:11:40Z,,,
13900,b'Context managers',2021-10-06T10:26:17Z,2021-10-20T12:15:47Z,,,
13899,b'The Pegasus model generator function ignores the temperature parameter.',2021-10-06T04:29:14Z,2021-11-14T15:01:54Z,,,
13898,b'Cannot run regression with DebertaV2ForSequenceClassification and DebertaForSequenceClassification',2021-10-06T03:16:10Z,2021-10-06T18:08:19Z,,RuntimeError,"RuntimeError: ""mse_cpu"" not implemented for 'Long' (CPU)"
13897,b'Fix hp search for non sigopt backends',2021-10-06T01:30:49Z,2021-10-06T15:52:29Z,,,
13896,b'Fix trainer logging_nan_inf_filter in torch_xla mode',2021-10-06T01:28:24Z,2021-10-06T11:54:54Z,,,
13895,"b""modeling utils don't respect `subfolder` setting when looking up model""",2021-10-06T00:07:17Z,2021-11-14T15:01:55Z,,"requests.exceptions.HTTPError, OSError","requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/bigscience/tr3d-1B3-oscar-checkpoints/resolve/global_step117000/config.jsonOSError: Can't load config for 'bigscience/tr3d-1B3-oscar-checkpoints'. Make sure that:"
13894,b'fix: replace asserts by value error',2021-10-05T21:55:19Z,2021-10-05T22:08:48Z,,,
13893,b'Inference of ONNX exported BART',2021-10-05T21:48:19Z,2021-10-07T08:36:36Z,,,
13892,b'Update parallelism.md',2021-10-05T21:46:24Z,2021-10-06T00:42:12Z,,,
13891,"b'minimal fixes to run DataCollatorForWholeWordMask with return_tensors=""np"" and return_tensors=""tf""'",2021-10-05T21:00:23Z,2021-11-03T14:36:42Z,,,
13890,"b'DataCollatorForWholeWordMask does not work with return_tensors = ""np"" and return_tensors = ""tf""'",2021-10-05T20:03:08Z,2021-11-03T14:36:42Z,,"```TypeError, ```AttributeError","```TypeError: numpy_mask_tokens() got an unexpected keyword argument 'special_tokens_mask'``````AttributeError: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'clone'```"
13889,b'Update Kwargs for EncoderDecoderModel',2021-10-05T19:51:10Z,2021-11-14T15:01:56Z,,,
13888,b'fix(integrations): consider test metrics',2021-10-05T19:45:34Z,2021-10-05T20:27:23Z,,,
13887,b'Encoder Decoder Model Generation Issue',2021-10-05T19:29:57Z,2021-12-07T12:07:55Z,,,
13886,b'Fixed horizon_length for PPLM',2021-10-05T18:59:59Z,2021-10-15T01:46:09Z,,,
13885,b'pre-training roberta from scratch',2021-10-05T18:05:26Z,2021-10-06T14:30:57Z,,,
13884,b'Autodocument the list of ONNX-supported models',2021-10-05T18:05:12Z,2021-10-06T02:43:16Z,,,
13883,b'Fix typo in README.md',2021-10-05T17:36:23Z,2021-10-08T18:25:33Z,,,
13882,b'Fix LED',2021-10-05T16:31:21Z,2021-10-07T16:30:15Z,,,
13881,"b'""test"" prefix for rewrite_logs() function of WandbCallback'",2021-10-05T16:14:54Z,2021-10-05T20:27:23Z,,,
13880,b'GPT-J float16 model output stopping after first word',2021-10-05T15:43:10Z,2021-10-08T15:28:19Z,,,
13879,b'[Feature] Add an option to use custom pad token_id in `tokenizer.pad()`',2021-10-05T15:07:48Z,2021-11-14T15:01:57Z,,,
13878,"b'Model isn\'t saved on ""save_steps"" '",2021-10-05T14:40:57Z,2021-10-09T05:34:12Z,,,
13877,b'[Speech Examples] Add pytorch speech pretraining',2021-10-05T14:14:00Z,2021-10-11T22:46:32Z,,,
13876,b'Fix list index out of range when padding nested empty lists',2021-10-05T13:44:39Z,2021-11-10T20:34:52Z,,IndexError,"IndexError: list index out of range"
13875,b'Hyperparameter tuning example code is not working either with Ray or Optuna backend. ',2021-10-05T12:40:30Z,2021-10-06T15:52:28Z,,"ray.exceptions.RayTaskError(TuneError), ray.tune.error.TuneError, AttributeError, TuneError","ray.exceptions.RayTaskError(TuneError): ray::ImplicitFunc.train_buffered() (pid=823, ip=172.28.0.2, repr=<ray.tune.function_runner.ImplicitFunc object at 0x7f6b715d4990>)ray.tune.error.TuneError: Trial raised an exception. Traceback:AttributeError: 'dict' object has no attribute 'assignments'TuneError: ('Trials did not complete', [_objective_86e23_00000, _objective_86e23_00001, _objective_86e23_00002, _objective_86e23_00003, _objective_86e23_00004, _objective_86e23_00005, _objective_86e23_00006, _objective_86e23_00007, _objective_86e23_00008, _objective_86e23_00009])"
13874,b'Add TrOCR + VisionEncoderDecoderModel',2021-10-05T12:01:01Z,2021-10-13T08:28:56Z,,,
13873,b'Fixing question-answering with long contexts ',2021-10-05T10:26:12Z,2021-10-05T14:08:58Z,,,
13872,b'Fix flax summarization example: save checkpoint after each epoch and push checkpoint to the hub',2021-10-05T10:10:57Z,2021-10-05T11:00:13Z,,,
13871,b'Replace assert statements with exceptions',2021-10-05T09:16:41Z,2021-10-06T03:02:45Z,,,
13870,b'Computer shuts down when multiple gpus are used',2021-10-05T02:15:20Z,2021-11-13T15:02:11Z,,,
13869,"b""'list' object has no attribute 'tolist error while using 'deepset/xlm-roberta-large-squad2'""",2021-10-05T00:05:20Z,2021-11-04T15:40:42Z,,AttributeError,"AttributeError: 'list' object has no attribute 'tolist'"
13868,"b""Can't use add_prefix_space in RobertaTokenizerFast""",2021-10-04T20:56:28Z,2021-10-04T23:50:08Z,,,
13867,b'Add TF notebooks',2021-10-04T20:05:39Z,2021-10-04T20:07:13Z,,,
13866,b'How to export BertForMaskedLM to onxx using onxx export script?',2021-10-04T19:46:57Z,2021-11-13T15:02:12Z,,,
13865,b'Update no_* argument (HfArgumentParser)',2021-10-04T15:07:51Z,2021-10-04T20:28:53Z,,,
13864,b'Update FSNER code in examples->research_projects->fsner',2021-10-04T15:03:37Z,2021-10-06T02:47:12Z,,,
13863,b'overflow error when initialising transformerXL model (transfo-xl-wt103)',2021-10-04T14:39:13Z,2021-11-11T15:01:46Z,,RuntimeError,"RuntimeError: $ Torch: invalid memory size -- maybe an overflow? at /pytorch/aten/src/TH/THGeneral.cpp:188"
13862,b'Fixing 1-length special tokens cut.',2021-10-04T13:22:37Z,2021-10-05T10:26:54Z,,,
13861,"b'Can I train an Rnd2GPT model through HuggingFace ""Encoder-Decoder Model""?'",2021-10-04T12:11:49Z,2021-10-04T13:02:16Z,,,
13860,b'Speech2Text2 training support',2021-10-04T11:02:24Z,2022-02-01T15:07:12Z,,,
13859,b'Fixing empty prompts for text-generation when BOS exists.',2021-10-04T10:31:31Z,2021-10-05T11:46:10Z,,,
13858,"b'there is not more toolkit and model  architecture about tabular format in hugginggface, tabular text in NLP = language + layout'",2021-10-04T10:14:13Z,2021-11-11T15:01:48Z,,,
13857,b'Update run_qa.py - CorrectTypo',2021-10-04T08:56:57Z,2021-10-06T03:10:25Z,,,
13856,b'Fixing GPU for token-classification in a better way.',2021-10-04T07:59:58Z,2021-10-06T02:44:32Z,,,
13855,b'Fixing Backward compatiblity for zero-shot',2021-10-04T07:34:30Z,2021-10-06T03:06:47Z,,,
13854,b'[WIP] Add MarianMT to models exportable with ONNX',2021-10-04T07:08:12Z,2021-12-07T15:02:17Z,,,
13853,b'Issues with FNet',2021-10-04T06:01:35Z,2022-01-11T15:07:11Z,,,
13852,b'Delete MultiBERTs conversion script',2021-10-04T05:12:39Z,2021-10-04T10:30:22Z,,,
13851,b'Remove a duplicated bullet point in the GPT-J doc',2021-10-04T04:15:11Z,2021-10-04T10:30:51Z,,,
13850,b'Update examples/documentation for torch.distributed.run',2021-10-03T19:02:25Z,2021-12-15T15:01:58Z,,,
13849,b'Unexpected behaviour with past_key_values in GPT2',2021-10-03T18:55:50Z,2021-12-13T15:07:16Z,,,
13848,b'Fix broken link to distill models in docs',2021-10-03T12:45:26Z,2021-10-04T15:57:55Z,,,
13847,b'Default arguments of clm example are confusing',2021-10-02T18:04:07Z,2021-10-04T20:28:53Z,,,
13846,b'Zero-shot classification pipeline change in 4.11.2',2021-10-02T17:59:46Z,2021-10-06T03:06:47Z,,,
13845,b'[RFC] Add `modeling_xxx_fusion.py` to support kernel fusion',2021-10-02T17:06:10Z,,"Performance, WIP",,
13844,b'How to tokenize big dataset ',2021-10-02T12:58:11Z,,Feature request,,
13843,b'Question: Is that possible to embed a tokenizer into the model for tensorflow serving?',2021-10-02T12:15:21Z,2021-10-05T15:45:14Z,,,
13842,b'Multi-task learning for MLM and token classification',2021-10-02T11:34:19Z,2021-10-05T13:47:14Z,,,
13841,b'How can one use newly added GPTJ6B model for finetuning?',2021-10-02T11:16:04Z,2021-11-19T15:06:25Z,,,
13840,b'Multilingual T5 is not adding new tokens',2021-10-02T06:21:22Z,2021-11-09T15:07:40Z,,ValueError,"ValueError: Attempt to convert a value (None) with an unsupported type (<class 'NoneType'>) to a Tensor."
13839,"b""TF mT5 model is not adding new tokens into it's vocabulary.""",2021-10-02T05:52:49Z,2022-02-11T17:35:10Z,,ValueError,"ValueError: Attempt to convert a value (None) with an unsupported type (<class 'NoneType'>) to a Tensor."
13838,"b""AttributeError: 'TFDistilBertForSequenceClassification' object has no attribute 'to'""",2021-10-02T04:04:00Z,2021-11-09T15:07:41Z,,,
13837,b'Onnx support for Zero Shot Classification Pipeline',2021-10-02T02:29:37Z,2021-11-09T15:07:42Z,,,
13836,b'Improve error message when loading models from Hub',2021-10-01T22:07:17Z,2021-10-05T12:09:10Z,,,
13835,b'Caching strategy for tokenizers to help achieve 1.46 ms inference on BERT QA on CPU',2021-10-01T20:46:58Z,2021-12-04T15:02:20Z,,,
13834,b'include megatron_gpt2 in installed modules',2021-10-01T18:35:40Z,2021-10-01T18:42:08Z,,,
13833,b'Updating CITATION.cff to fix GitHub citation prompt BibTeX output.',2021-10-01T14:34:49Z,2021-10-01T14:41:27Z,,,
13832,"b""AttributeError: type object 'EnglishDefaults' has no attribute 'create_tokenizer'""",2021-10-01T13:52:33Z,2021-10-31T15:16:06Z,,AttributeError,"AttributeError: type object 'EnglishDefaults' has no attribute 'create_tokenizer'"
13831,b'Add Tensorflow handling of ONNX conversion',2021-10-01T13:10:08Z,2022-02-10T10:18:41Z,,,
13830,b'Add on option to output a checkpoint every x minutes',2021-10-01T12:19:58Z,2021-12-28T15:02:10Z,,,
13829,"b'Fix warning situation: UserWarning: max_length is ignored when padding=True""'",2021-10-01T09:38:41Z,2021-10-01T13:29:09Z,,,
13828,b'Image Segmentation pipeline',2021-10-01T09:23:48Z,2021-10-08T07:59:54Z,,,
13827,b'Bort always predict wrong words',2021-10-01T09:03:35Z,2021-12-12T15:02:00Z,,,
13826,"b'Tokenizer - Raises wrong ""UserWarning: `max_length` is ignored when `padding`=`True`""'",2021-10-01T08:07:31Z,2021-10-01T13:29:08Z,,,
13825,b'Consistent speech model input names for the Seq2SeqTrainer generate function',2021-10-01T07:23:26Z,2021-12-23T18:43:37Z,,,
13824,b':sparkles: update image classification example',2021-10-01T07:21:21Z,2021-10-04T18:49:52Z,,,
13823,b'Add MarianMT to models exportable with ONNX',2021-10-01T07:21:04Z,2021-12-23T12:35:57Z,,,
13822,b'Could not load from pretrain even with the same code',2021-10-01T03:26:29Z,2021-11-08T15:02:01Z,,TypeError,"TypeError: init_weights() missing 1 required positional argument: 'module'"
13821,b'mT5 TensorFlow error - Attempt to convert a value (None) with an unsupported type',2021-09-30T23:26:00Z,2021-11-10T18:06:03Z,,"`ValueError, ValueError","`ValueError: Attempt to convert a value (None) with an unsupported type (<class 'NoneType'>) to a Tensor.`ValueError: Attempt to convert a value (None) with an unsupported type (<class 'NoneType'>) to a Tensor."
13820,b'Allow dataset to be an optional argument for (Distributed)LengthGroupedSampler',2021-09-30T23:10:47Z,2021-10-05T13:04:39Z,,,
13819,b'[Fix]: Send model output to cpu before numpy cast in token_clf_pipeline',2021-09-30T21:38:57Z,2021-10-06T03:06:21Z,,,
13818,b'Weird behavior of BertLMHeadModel and RobertaForCausalLM',2021-09-30T21:11:04Z,2021-10-01T19:05:21Z,,,
13817,b'Adds `PreTrainedModel.framework` attribute',2021-09-30T19:16:44Z,2021-10-08T14:07:09Z,,,
13816,b'Device error on TokenClassificationPipeline',2021-09-30T18:31:36Z,2021-10-05T12:24:33Z,,,
13815,b'[FLAX] glue training example refactor',2021-09-30T16:32:21Z,2022-01-19T11:04:51Z,WIP,,
13814,b'Update Loss calculation in prediction_step',2021-09-30T15:27:58Z,2021-11-08T15:02:03Z,,,
13813,b'Fix gather for TPU',2021-09-30T14:51:01Z,2021-09-30T15:32:41Z,,,
13812,"b""TypeError: forward() got an unexpected keyword argument 'attention_mask'""",2021-09-30T14:13:06Z,2022-02-09T15:07:53Z,,TypeError,"TypeError: forward() got an unexpected keyword argument 'attention_mask'"
13811,b'AttributeError when running question answering models for result',2021-09-30T12:26:39Z,2021-10-05T14:08:58Z,,,
13810,b'[Examples] Improve mapping in accelerate examples',2021-09-30T12:13:40Z,2021-09-30T16:52:53Z,,,
13809,b'skip gptj slow generate tests',2021-09-30T11:28:32Z,2021-09-30T19:44:33Z,,,
13808,b'Some weights of BeitModel were not initialized from the model checkpoint',2021-09-30T10:33:34Z,2021-09-30T12:21:44Z,,,
13807,"b""[Don't merge now] Add cross attention to TFGPT2""",2021-09-30T08:43:19Z,2021-10-17T10:15:04Z,,,
13806,b'Loading wav2vec2 pre-trained models',2021-09-30T08:18:04Z,2021-10-01T11:48:27Z,,,
13805,b'T5ForConditionalGeneration: enabling using past_key_values and labels in training',2021-09-30T06:19:26Z,2021-10-06T07:20:42Z,,,
13804,b'How to use LayoutLM Tensorflow version on Google Colab?',2021-09-30T05:04:05Z,2021-11-07T15:02:02Z,,,
13803,b'[testing] auto-replay captured streams',2021-09-29T23:51:22Z,2021-09-30T16:26:49Z,,,
13802,b'[examples/pytorch/image-classification] why does it require `torch>=1.9.0`',2021-09-29T21:01:55Z,2021-10-04T18:49:52Z,,,
13801,b'Transformer Hosted API Broken',2021-09-29T20:39:02Z,2021-10-07T07:48:59Z,,,
13800,b'Bart: check if decoder_inputs_embeds is set',2021-09-29T20:19:00Z,2021-10-01T17:36:58Z,,,
13799,b'Bart: check if decoder_inputs_embeds is set',2021-09-29T20:03:54Z,2021-09-29T20:15:06Z,,,
13798,"b'transformers seems to have recently been ""bricked""'",2021-09-29T19:54:18Z,2021-09-30T15:32:41Z,,"RuntimeError, torch.multiprocessing.spawn.ProcessExitedException","RuntimeError: zero-dimensional tensor (at position 0) cannot be concatenatedtorch.multiprocessing.spawn.ProcessExitedException: process 0 terminated with exit code 17"
13797,b'(Distributed)LengthGroupedSampler: allow only providing lengths but not a dataset',2021-09-29T18:06:48Z,2021-10-05T13:04:39Z,,,
13796,b'[DPR] Correct init',2021-09-29T17:51:54Z,2021-09-30T16:55:21Z,,,
13795,b'[docs/gpt-j] addd instructions for how minimize CPU RAM usage',2021-09-29T17:51:39Z,2021-09-29T18:13:46Z,,,
13794,b'LengthGroupedSampler: why longest examples go first?',2021-09-29T17:49:13Z,2021-09-30T22:52:26Z,,,
13793,b'Add TF notebooks',2021-09-29T15:43:41Z,2021-09-29T16:07:11Z,,,
13792,b'Fix length of IterableDatasetShard and add test',2021-09-29T15:20:11Z,2021-09-29T15:48:49Z,,,
13791,b'Make from_pretrained support parameters defined in the forward pass',2021-09-29T13:40:57Z,2021-11-07T15:02:03Z,,,
13790,b'DetrFeatureExtractor fails if do_normalize set to false',2021-09-29T13:08:51Z,2021-11-07T15:02:04Z,,AttributeError,"AttributeError: shape"
13789,b'is the prediction_logits.size() is correct?',2021-09-29T11:06:04Z,2021-09-30T09:04:27Z,,,
13788,b'Add BARTpho: Pre-trained Sequence-to-Sequence Models for Vietnamese',2021-09-29T10:56:01Z,2021-10-18T14:16:46Z,,,
13787,b'problem when loading local model',2021-09-29T10:43:55Z,2021-11-10T15:06:20Z,,requests.exceptions.HTTPError,"requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/api/models/chinese-roberta-wwm-ext"
13786,"b'GPT2 (117M) results mismatch on PTB, enwik8 and text8 metrics'",2021-09-29T10:16:06Z,2021-11-08T15:02:05Z,,,
13785,b'Enable readme link synchronization',2021-09-29T07:34:41Z,2021-09-29T15:19:00Z,,,
13784,b'Feature: Tail Free Sampling',2021-09-29T07:26:20Z,,WIP,,
13783,b'[WIP] Make import simple',2021-09-29T06:38:00Z,2021-09-29T08:50:38Z,,,
13782,"b""NotebookProgressBar can't display in Pycharm jupyter""",2021-09-29T02:51:14Z,2021-12-01T15:02:56Z,,,
13781,b'DDP BERT-Base on SQuaD2.0',2021-09-28T23:15:29Z,2021-11-09T15:07:45Z,,,
13780,b'Implement len in IterableDatasetShard',2021-09-28T21:59:28Z,2021-09-28T22:22:38Z,,,
13779,b'ByT5: problem with tokenizer.decode()',2021-09-28T18:31:31Z,2021-10-14T08:54:21Z,,UnicodeDecodeError,"UnicodeDecodeError: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte"
13778,b'Add TFViTModel',2021-09-28T17:02:26Z,2021-11-09T12:54:37Z,,,
13777,b'[Wav2Vec2] Better error message',2021-09-28T14:33:47Z,2021-09-29T08:30:01Z,,,
13776,b'LED for Seq2Seq output shape mismatch between tensorflow and pytorch',2021-09-28T13:58:41Z,2021-10-07T16:30:15Z,,#NOTE,"#NOTE: same inputs as in the tensorflow example!"
13775,b'Pretrained Wav2Vec2 Model large robust does not load',2021-09-28T13:54:30Z,2021-09-29T08:30:01Z,,TypeError,"TypeError: new() received an invalid combination of arguments - got (NoneType, int), but expected one of:"
13774,b'Empty prompts failing in dev sources',2021-09-28T13:02:46Z,2021-10-05T11:46:10Z,,RuntimeError,"RuntimeError: cannot reshape tensor of 0 elements into shape [-1, 0] because the unspecified dimension size -1 can be any value and is ambiguous"
13773,"b'Keras callback to push to hub each epoch, or after N steps'",2021-09-28T12:46:00Z,2021-09-29T11:47:35Z,,,
13772,b'HuggingFace Model Hub (summarisation) - models not working locally (404 not found)',2021-09-28T11:27:23Z,2021-09-28T13:01:32Z,,ValueError,"ValueError: Could not load model lidiya/bart-large-xsum-samsum with any of the following classes: (<class 'transformers.models.auto.modeling_tf_auto.TFAutoModelForSeq2SeqLM'>, <class 'transformers.models.bart.modeling_tf_bart.TFBartForConditionalGeneration'>)."
13771,"b'Connection error, and we cannot find the requested files in the cached path'",2021-09-28T06:46:26Z,2021-11-06T15:01:59Z,,,
13770,b'Link is broken in Huggingface model zoo',2021-09-28T06:06:05Z,2021-09-28T08:01:32Z,,,
13769,b'Size mismatch error while changing the  T5Config class default parameters',2021-09-28T05:16:14Z,2021-11-06T15:02:00Z,Migration,RuntimeError,"RuntimeError: Error(s) in loading state_dict for T5ForConditionalGeneration:"
13768,"b'[examples `run_glue.py`] missing requirements `scipy`, `sklearn`'",2021-09-28T01:18:10Z,2021-09-29T20:45:19Z,,,
13767,b'Fix warning for gradient_checkpointing',2021-09-27T19:27:53Z,2021-09-28T18:21:17Z,,,
13766,b'Fix filtering in test fetcher utils',2021-09-27T18:58:12Z,2021-09-27T19:26:55Z,,,
13765,b'Add an example of exporting BartModel + BeamSearch to ONNX module.',2021-09-27T17:21:57Z,2021-10-07T10:07:02Z,,,
13764,b'Add attention-mask support for ViTModel',2021-09-27T16:28:01Z,2021-11-13T15:02:17Z,,,
13763,b'Hidden states not available in S2T (and small typo in S2T documentation)',2021-09-27T15:26:50Z,2021-10-05T13:28:51Z,,,
13762,b'ByT5 tokenizer gives indices of chars instead of bytes',2021-09-27T12:06:52Z,2021-09-27T15:01:42Z,,,
13761,"b'Unable to match GPT-2 reported perplexity results on CBT, Wikitext-103, and 1BW datasets and doubt on LAMBADA accuracy'",2021-09-27T11:20:43Z,2021-11-05T15:07:09Z,,,
13760,b'Fix loss computation in Trainer',2021-09-27T10:47:59Z,2021-09-27T11:33:08Z,,,
13759,b'A version of Trainer that calculates the eval metrics (e.g. accuracy) on the training set as well',2021-09-27T08:59:02Z,2021-11-07T15:02:07Z,,,
13758,b'incorrect loss calculation',2021-09-27T08:54:24Z,2021-09-27T11:42:25Z,,,
13757,b'Update Tatoeba conversion',2021-09-27T08:40:14Z,2021-10-05T09:15:19Z,,,
13756,"b""TypeError: 'LayerNorm' object does not support indexing""",2021-09-27T08:25:37Z,2021-11-06T15:02:03Z,,TypeError,"TypeError: 'LayerNorm' object does not support indexing"
13755,b'[Tests] Cast Hubert model tests to fp16',2021-09-26T19:38:15Z,2021-09-26T19:58:23Z,,,
13754,b'This problem happend when I train the model',2021-09-26T19:24:13Z,2021-11-05T15:07:11Z,,,
13753,b'[WIP] Add Flax FNet',2021-09-26T19:02:07Z,2021-12-09T15:06:42Z,,,
13752,b'Update FNet Fourier Transform',2021-09-26T18:54:27Z,,WIP,,
13751,b'Scrolling through the docs has become very slow',2021-09-26T16:38:08Z,2021-12-04T16:59:57Z,,,
13750,"b""wav2vec 2.0 loaded from a fairseq checkpoint isn't strictly equivalent""",2021-09-26T16:34:26Z,2021-11-05T15:07:12Z,,,
13749,b'Add a method for substring(tokens / ids) checking to tokenizers.',2021-09-26T12:35:24Z,2021-11-04T15:06:04Z,,,
13748,b'FNet model card',2021-09-26T08:52:24Z,2021-09-26T09:41:21Z,model card,,
13747,b'I want to understand the source code of transformers. Where should I start? Is there a tutorial link? thank you very much!',2021-09-26T08:27:24Z,2021-11-04T15:06:05Z,Migration,,
13746,b'Fix type annotations for `distributed_concat()`',2021-09-26T08:00:24Z,2021-09-27T10:29:12Z,,,
13745,b'Update requirements for speech example',2021-09-26T02:36:37Z,2021-09-26T07:02:46Z,,,
13744,"b""using translate notebook for my dataset I get this error : ValueError: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length.""",2021-09-26T01:53:17Z,2021-10-01T13:41:57Z,,,
13743,b'[Tests] Add decorator to FlaxBeit',2021-09-25T17:34:32Z,2021-09-25T19:20:21Z,,,
13742,"b""Can't save model in saved_model format when finetune bert in tensorflow2""",2021-09-25T16:12:41Z,2021-11-08T15:02:08Z,,TypeError,"TypeError: in user code:"
13741,b'Adding target language token in mBART model.',2021-09-25T15:47:08Z,2021-09-26T06:32:28Z,,,
13740,b'Fix bug in DebertaForMaskedLM',2021-09-25T15:27:59Z,2021-11-04T15:06:06Z,,,
13739,b'Model inside docker gives different results',2021-09-25T11:08:50Z,2021-11-03T15:01:52Z,,,
13738,b'Update test dependence for torch examples',2021-09-25T02:17:27Z,2021-09-25T16:47:39Z,,,
13737,b'Add failing test for ProphetBet batching',2021-09-24T22:00:00Z,2021-11-04T15:06:07Z,,,
13736,b'Obfuscated text classification error when using CANINE Transformers',2021-09-24T21:29:03Z,2021-09-25T08:31:42Z,,ValueError,"ValueError: Target size (torch.Size([13])) must be the same as input size (torch.Size([13, 12]))"
13735,b'[megatron gpt checkpoint conversion] causal mask requires pos_embed dimension',2021-09-24T21:28:29Z,2021-09-26T16:51:40Z,,,
13734,"b""Silence warning in gradient checkpointing when it's False""",2021-09-24T19:02:31Z,2021-09-27T11:43:38Z,,,
13733,b'[Examples] speech recognition - remove gradient checkpointing',2021-09-24T16:28:11Z,2021-09-24T16:32:36Z,,,
13732,b'TypeError in tensorflow/run_summarization.py',2021-09-24T16:08:58Z,2022-01-08T15:02:36Z,,TypeError,"TypeError: prepare_inputs_for_generation() got multiple values for argument 'decoder_input_ids'"
13731,b'[TMP] Do not merge.',2021-09-24T12:53:48Z,2021-10-11T07:08:12Z,,,
13730,b'Add model card creation snippet to example scripts',2021-09-24T10:20:57Z,2021-09-24T13:51:47Z,,,
13729,b'[Tests] FNetTokenizer',2021-09-24T10:10:13Z,2021-09-24T12:57:49Z,,,
13728,"b""Error when loading weights with a different 'projection_dim' in DPRQuestionEncoder""",2021-09-24T09:44:57Z,2021-09-30T07:23:26Z,,NotImplementedError,"NotImplementedError: Make sure `_init_weigths` is implemented for <class 'transformers.models.dpr.modeling_dpr.DPRQuestionEncoder'>"
13727,b'Add support for XLM-R XL and XXL models by modeling_xlm_roberta_xl.py',2021-09-24T09:28:44Z,2022-01-29T12:42:37Z,,,
13726,b'[WIP] Tensor model parallelism for GPTNeo model. ',2021-09-24T09:09:06Z,2021-12-02T21:48:16Z,"Model Parallel, Tensor Parallel, WIP",,
13725,b'Fixing zero-shot backward compatiblity',2021-09-24T07:59:30Z,2021-09-24T11:38:17Z,,,
13724,b'Adding `batch_size` support for (almost) all pipelines',2021-09-24T07:33:06Z,2021-10-29T09:34:18Z,,RuntimeError,"RuntimeError: CUDA out of memory. Tried to allocate 472.00 MiB (GPU 0; 3.95 GiB total capacity; 2.13 GiB already allocated; 266.75 MiB free; 2.49 GiB reserved in total by PyTorch)"
13723,"b""PIL and soundfile shouldn't be required to run `transformers-cli env`""",2021-09-24T00:01:54Z,2021-11-01T15:07:45Z,,,
13722,b'[Examples] Add an official audio classification example',2021-09-23T23:46:26Z,2021-10-01T16:52:45Z,,,
13721,b'latest TPU VM dies on import of TrainingArguments?',2021-09-23T20:46:17Z,2021-12-12T03:49:13Z,,,
13720,b'Add `BlenderbotTokenizerFast`',2021-09-23T17:20:43Z,2021-10-29T13:19:02Z,,,
13719,b'How can i Use transformers to classify a obfuscated text (multinomial classification)',2021-09-23T17:14:13Z,2021-09-25T08:32:04Z,New model,,
13718,b'Fix SpeechEncoderDecoderModel',2021-09-23T16:10:02Z,2021-10-31T17:23:12Z,,,
13717,b'Handle `UnicodeDecodeError` when loading config file',2021-09-23T16:06:38Z,2021-09-23T20:56:34Z,,,
13716,b'SpeechEncoderDecoderModel does not return a loss regardless of labels',2021-09-23T13:44:50Z,2021-10-31T15:01:58Z,,,
13715,b' Unable to load weights .from_pretrained() for XLMRoberta Model',2021-09-23T13:30:30Z,2021-10-31T15:01:59Z,,,
13714,b'Align attention_mask dtype to the attention_scores for BERT to speedup mixed precision inference and training',2021-09-23T13:16:21Z,2021-10-31T15:02:00Z,,,
13713,b'Documentation Mistake: no_multi_processing for Benchmarks',2021-09-23T11:00:34Z,2021-10-31T15:02:01Z,,,
13712,b'Add FSNER example in research_projects',2021-09-23T09:12:37Z,2021-09-23T21:04:15Z,,,
13711,b'problem with using past_key_values in T5ForConditionalGeneration',2021-09-23T08:58:27Z,2021-10-06T07:20:42Z,,,
13710,b'Fix LayoutLM ONNX test error',2021-09-23T08:07:05Z,2021-09-29T13:50:16Z,,,
13709,b'Unable to call .from_pretrained() for Roberta Model',2021-09-23T07:45:01Z,2021-09-23T11:54:27Z,,TypeError,"TypeError: __init__() missing 1 required positional argument: 'num_labels'"
13708,b'Unable to execute RobertaModel from pretrained',2021-09-23T07:13:00Z,2021-09-23T07:42:20Z,Migration,TypeError,"TypeError: __init__() missing 1 required positional argument: 'num_labels'"
13707,b'RunTimeError when using prefix_allowed_tokens_fn and top-k/top-p sampling in model.generate',2021-09-23T06:00:57Z,2021-10-31T15:02:02Z,,,
13706,b'how to finetune huggingface MarianMT transformer model',2021-09-23T04:58:16Z,2021-10-31T15:02:03Z,,,
13705,b'update run_translation.py',2021-09-23T03:50:28Z,2021-10-31T15:02:03Z,,,
13704,"b'Error while running GPT-J 6B with revision=""float16""'",2021-09-23T03:50:27Z,2021-09-23T06:18:02Z,,,
13703,b'Replace torch.set_grad_enabled by torch.no_grad',2021-09-22T23:20:13Z,2021-09-23T21:08:30Z,,,
13702,b'Skip ONNX LayoutLM test',2021-09-22T23:19:46Z,2021-10-31T15:02:04Z,,,
13701,b'Fix typo in torchscript tests',2021-09-22T23:02:49Z,2021-09-22T23:02:54Z,,,
13700,b'Patch training arguments issue',2021-09-22T19:20:54Z,2021-09-22T19:33:18Z,,,
13699,b'Patch training arguments issue',2021-09-22T19:16:01Z,2021-09-22T19:33:14Z,,,
13698,b'Fine-tuning GPT-J 6B with 16Gb of VRAM',2021-09-22T18:17:44Z,2021-10-31T15:02:05Z,,,
13697,b'Pipeline \xe2\x80\x9czero-shot-classification\xe2\x80\x9d gives \xe2\x80\x9cTypeError: __call__() takes 2 positional arguments but 3 were given.\xe2\x80\x9d',2021-09-22T18:15:38Z,2021-09-24T11:38:17Z,,TypeError,"TypeError: __call__() takes 2 positional arguments but 3 were given"
13696,b'[docs/gpt-j] add a note about tokenizer',2021-09-22T15:46:17Z,2021-09-22T21:18:13Z,,,
13695,b'Wav2vec2 with different tokenizer ?',2021-09-22T15:00:57Z,2021-11-01T15:07:46Z,,,
13694,b'Limit number of checkpoint on `examples/pytorch/summarization/run_summarization.py` same as `save_total_limit` on `Trainer`',2021-09-22T08:51:07Z,2021-09-22T13:05:26Z,,,
13693,b'[Wav2Vec2FeatureExtractor] Fix `extractor.pad()` dtype backwards compatibility',2021-09-22T08:05:39Z,2021-09-22T09:02:54Z,,,
13692,b'Assertions to exceptions',2021-09-22T07:19:20Z,2021-09-22T13:14:29Z,,,
13691,b'Raise exceptions instead of using assertions for control flow #12789',2021-09-22T06:39:21Z,2021-09-22T06:50:58Z,,,
13690,b'[RFC] adding Tensor and Pipeline Parallelism to transformers',2021-09-22T01:23:51Z,,"Pipeline Parallel, Performance, Tensor Parallel, WIP",,
13689,b'New Wav2Vec2 padding has slightly backward breaking changes',2021-09-21T23:38:16Z,2021-09-22T09:02:54Z,,,
13688,b'[FlaxWav2Vec2] Revive Test',2021-09-21T22:14:28Z,2021-09-21T22:28:43Z,,,
13687,b'Allow only textual inputs to VisualBert',2021-09-21T20:53:36Z,2021-09-22T15:51:54Z,,,
13686,b'Fix FNet reference to tpu short seq length',2021-09-21T20:25:59Z,2021-09-22T22:36:24Z,,,
13685,b'Wav2vec2 pretraining',2021-09-21T19:34:55Z,,,,
13684,b'modeling_fnet.py config option ',2021-09-21T19:30:22Z,2021-09-22T22:36:24Z,,,
13683,b'pipeline fill_mask.py - needs to convert input_ids to cpu before calling numpy',2021-09-21T19:15:04Z,2021-11-04T15:10:45Z,,TypeError,TypeError: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.
13682,b'bug in movement-pruning',2021-09-21T18:34:38Z,2021-10-31T15:02:07Z,,,
13681,b'[Trainer] Make sure shown loss in distributed training is correctly averaged over all workers',2021-09-21T16:45:32Z,2021-09-26T07:03:46Z,,,
13680,b'Update modeling_flax_wav2vec2.py',2021-09-21T16:37:20Z,2021-09-21T21:36:13Z,,,
13679,b'Fix non-negligible difference between GPT2 and TFGP2',2021-09-21T16:33:40Z,2021-09-22T13:14:55Z,,,
13678,b'Modified TF train_step',2021-09-21T14:45:53Z,2021-09-27T13:47:08Z,,,
13677,b'CUDA out of memory even for GPT-NEO-125M',2021-09-21T14:36:33Z,2021-09-21T14:46:07Z,,,
13676,b'[GPT-J] Use the `float16` checkpoints in integration tests',2021-09-21T14:26:00Z,2021-09-22T20:17:57Z,,,
13675,"b'Typo ""UNKWOWN"" -> ""UNKNOWN""'",2021-09-21T12:20:53Z,2021-09-21T13:11:26Z,,,
13674,b'UnicodeDecodeError while loading pretrained model from AutoModel.from_pretrained()',2021-09-21T11:57:50Z,2021-10-29T15:07:07Z,,,
13673,b'retain_graph=True required when using a custom BigBird-based model ',2021-09-21T11:47:44Z,2021-10-29T15:07:07Z,,RuntimeError,"RuntimeError: Trying to backward through the graph a second time (or directly access saved variables after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved variables after calling backward."
13672,b'[examples/flax] use Repository API for push_to_hub',2021-09-21T11:22:01Z,2021-09-30T11:08:08Z,,,
13671,"b""AttributeError: 'T5ForConditionalGeneration' object has no attribute 'linear'""",2021-09-21T11:13:41Z,2021-10-29T15:07:08Z,,AttributeError,"AttributeError: 'T5ForConditionalGeneration' object has no attribute 'linear'"
13670,b'DPR AutoModel loading incorrect architecture for DPRContextEncoders',2021-09-21T11:10:29Z,2021-11-03T15:01:55Z,,NotImplementedError,"NotImplementedError: Make sure `_init_weigths` is implemented for <class 'transformers.models.dpr.modeling_dpr.DPRQuestionEncoder'>"
13669,b'LEDForSequenceClassification example throws a ValueError on missing decoder_input_ids/embeds',2021-09-21T09:52:59Z,2021-11-10T10:28:03Z,,ValueError,"ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds"
13668,b'[AutoTokenizer] Allow creation of tokenizers by tokenizer type',2021-09-21T08:59:10Z,2021-09-21T22:29:38Z,,,
13667,b'switch to inference_mode from no_gard',2021-09-21T06:26:38Z,2021-10-26T22:02:58Z,,,
13666,b'non-negligible difference between GPT2 and TFGP2',2021-09-21T05:31:28Z,2021-09-22T13:14:55Z,,,
13665,b'[SinusoidalPositionalEmbedding] incorrect dtype when resizing in `forward`',2021-09-21T03:10:02Z,2021-09-21T16:05:05Z,,RuntimeError,"RuntimeError: expected scalar type Float but found Half"
13664,b'T5ForConditionalGeneration.from_pretrained load pytorch *.pt checkpoint fails',2021-09-21T01:12:33Z,2021-10-29T15:07:10Z,,,
13663,b'Image classification script overrides preprocessing configuration with script defaults',2021-09-21T00:38:44Z,2021-10-04T18:49:51Z,,ValueError,"ValueError: Input image size (224*224) doesn't match model (30*30)."
13662,b'Add ESM to huggingface',2021-09-21T00:15:52Z,2022-02-17T15:07:15Z,,,
13661,b'Finetuning BART on a  multi-input sequence to sequence task',2021-09-21T00:15:27Z,2021-09-21T12:58:11Z,,,
13660,b'Fine-Tuning Wav2Vec2 with PyTorch DDP',2021-09-20T23:39:50Z,2021-10-21T16:46:01Z,,,
13659,b'Add push_to_hub to no_trainer examples',2021-09-20T22:08:28Z,2021-09-21T17:13:30Z,,,
13658,b'Syntax for from_pretrained proxies (downloading model behind corp proxy)',2021-09-20T21:50:43Z,2021-10-29T15:07:11Z,,,
13657,b'Make gradient_checkpointing a training argument',2021-09-20T19:28:58Z,2021-09-22T11:51:38Z,,,
13656,b'[WIP] Model type tokenizer',2021-09-20T16:28:16Z,2021-09-20T17:44:16Z,,,
13655,b'Add Speech AutoModels',2021-09-20T14:56:36Z,2021-09-21T06:50:33Z,,,
13654,b'Update modeling_tf_deberta.py',2021-09-20T14:39:26Z,2021-09-20T15:11:05Z,,,
13653,b'DeepSpeed and HF trainer return hugely different losses and perplexity',2021-09-20T14:21:14Z,2021-10-29T15:07:12Z,DeepSpeed,,
13652,b'Use `transformers` models as Spark estimators ',2021-09-20T12:26:50Z,2021-10-28T15:07:35Z,,,
13651,b'some error when I finetune wav2vec2 by rum_common_voice.py',2021-09-20T12:25:41Z,2022-01-18T15:07:39Z,,AttributeError,AttributeError: 'DatasetDict' object has no attibute 'select'` 
13650,b'[SequenceFeatureExtractor] Rewrite padding logic from pure python to numpy',2021-09-20T12:16:58Z,2021-09-21T14:10:14Z,,,
13649,b'[FLAX] Question Answering Example ',2021-09-20T10:29:14Z,2021-09-21T13:04:49Z,,,
13648,b'Auto model for conditional generation',2021-09-20T10:00:38Z,2021-10-21T17:29:02Z,,,
13647,b'[run_summarization] fix typo',2021-09-20T07:41:56Z,2021-09-20T07:52:26Z,,,
13646,b'fix research_projects/mlm_wwm readme.md examples',2021-09-20T05:13:52Z,2021-09-20T19:01:35Z,,,
13645,"b'lmetric = load_metric(""sacrebleu"") exception'",2021-09-20T03:19:31Z,2021-09-26T10:15:01Z,,ValueError,"ValueError: check_hostname requires server_hostname"
13644,b'Change https:/ to https:// to dataset GitHub repo',2021-09-19T21:38:00Z,2021-09-20T16:31:46Z,,,
13643,b'Fix typo distilbert doc to code link',2021-09-19T21:29:15Z,2021-09-20T19:10:33Z,,,
13642,b'Report test durations in scheduled CI tests',2021-09-19T18:33:52Z,2021-09-21T18:59:28Z,,,
13639,b'Fix mT5 documentation',2021-09-19T16:01:59Z,2021-09-20T11:53:32Z,,,
13638,b'Dead link in docs',2021-09-19T15:49:06Z,2021-09-20T19:14:26Z,,,
13637,b'Use torch.unique_consecutive to check elements are same',2021-09-19T09:35:36Z,2021-09-24T08:31:23Z,,,
13636,b'[Fix]Make sure the args tb_writer passed to the TensorBoardCallback works',2021-09-18T17:07:38Z,2021-09-20T11:50:03Z,,,
13635,b'[typo] invalid URL',2021-09-18T12:32:28Z,2021-09-20T16:32:04Z,,,
13634,b'Add the fast implementation of `BlenderbotTokenizer`',2021-09-17T20:50:59Z,2021-10-29T13:19:02Z,,,
13633,b'[Flax] Add FlaxBlenderbot',2021-09-17T20:44:16Z,2021-11-30T12:06:55Z,,,
13632,b'bug in transformers notebook (training from scratch)?',2021-09-17T19:47:38Z,2021-12-25T15:02:12Z,,,
13631,b'Updated tiny distilbert models',2021-09-17T19:35:23Z,2021-09-17T19:35:34Z,,,
13630,b'Fix GPT2Config parameters in GPT2ModelTester',2021-09-17T18:52:25Z,2021-09-17T19:36:23Z,,,
13629,b'ResourceExhaustedError: Failed to allocate request for 64.00MiB (67108864B) on device ordinal 0',2021-09-17T18:09:20Z,2021-09-20T15:41:25Z,,<ResourceExhaustedError,"<ResourceExhaustedError: Failed to allocate request for 64.00MiB (67108864B) on device ordinal 0>"
13628,b'Modified TF train_step',2021-09-17T18:04:52Z,2021-09-21T14:43:56Z,,,
13627,b'TF weights for xlm-roberta-base?',2021-09-17T16:35:14Z,2021-10-26T15:07:24Z,,,
13626,b'Huggingface master',2021-09-17T14:49:28Z,2021-09-17T14:50:28Z,,,
13625,b'Removed console spam from misfiring warnings',2021-09-17T12:52:09Z,2021-09-17T14:44:33Z,,,
13624,b'CPU memory (VRAM) not released after loading model in GPU',2021-09-17T11:00:23Z,2021-11-07T15:02:14Z,,,
13623,b'AutoTokenizer - add `from_model_name` method',2021-09-17T10:45:43Z,2021-09-21T09:00:16Z,,,
13622,b'Add `obj-det` pipeline support for `LayoutLMV2`',2021-09-17T10:35:11Z,2021-12-13T15:07:22Z,,,
13621,b'How to freeze a few layers in t5 model  during fine-tuning',2021-09-17T09:41:17Z,2021-10-25T15:06:40Z,,,
13620,b'[ASR] Add official ASR CTC example to `examples/pytorch/speech-recognition`',2021-09-17T09:33:05Z,2021-09-24T05:01:11Z,,,
13619,b'[Trainer] Add nan/inf logging filter',2021-09-17T09:23:19Z,2021-09-17T14:21:59Z,,,
13618,b'How to change type_vocab_size?',2021-09-17T08:00:01Z,2021-10-25T15:06:41Z,,ValueError,"ValueError: cannot reshape array of size 1536 into shape (3,768)"
13617,b'Correct GPT-J voab_size',2021-09-17T05:23:35Z,2021-09-22T15:48:05Z,,,
13616,b'DeepSpeed: There is no obvious benefit from increasing the number of GPU nodes',2021-09-17T03:30:51Z,2021-11-15T15:07:30Z,DeepSpeed,,
13615,b'Why we use truncated normal initializer instead of the default glorot_uniform in the tfbert?',2021-09-17T03:08:37Z,2021-10-25T15:06:42Z,,,
13614,b'Use `config_dict_or_path` for deepspeed.zero.Init',2021-09-17T02:10:12Z,2021-09-17T14:57:28Z,DeepSpeed,,
13613,b'Fixes issues with backward pass in LED/Longformer Self-attention',2021-09-16T23:59:58Z,2021-09-17T15:05:50Z,,,
13612,b'ProphetNet generation inconsistent with batch size?',2021-09-16T23:14:08Z,2021-10-25T15:06:43Z,,,
13611,b'fix some docstring in encoder-decoder models',2021-09-16T19:09:22Z,2021-09-17T15:39:36Z,,,
13610,b'How to use model.save() in tf2 when using TFBertModel',2021-09-16T16:25:55Z,2021-09-17T07:56:23Z,,IndexError,"IndexError: list index out of range"
13609,b'DataCollatorForTokenClassification numpy fix',2021-09-16T15:59:44Z,2021-09-16T17:00:59Z,,,
13608,b'Fix a pipeline test with the newly updated weights',2021-09-16T15:02:31Z,2021-09-16T15:33:09Z,,,
13607,b'`Trainer` loads model weights twice when resuming from checkpoint',2021-09-16T14:45:02Z,2021-10-24T15:02:29Z,,,
13606,b'Bugfix to implement floor division (I replaced / with //)',2021-09-16T14:39:46Z,2021-11-22T15:06:22Z,,RuntimeError,"RuntimeError: Integer division of tensors using div or / is no longer supported, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead."
13605,b'Manual download of a pytorch_model.bin results in a zip file',2021-09-16T14:38:13Z,2021-09-16T21:47:15Z,,,
13604,b'Properly use test_fetcher for examples',2021-09-16T14:33:59Z,2021-09-16T19:13:01Z,,,
13603,b'Adding license file to some of the fine-tuned models ',2021-09-16T14:22:51Z,2021-10-24T15:02:31Z,,,
13602,b'how to get 8 hidden layer in bertonnx',2021-09-16T13:48:55Z,2021-10-24T15:02:32Z,,,
13601,"b'Push to hub fails to ""update ref"" for GLUE example script'",2021-09-16T13:41:06Z,2021-10-24T15:02:33Z,,"subprocess.CalledProcessError, OSError","subprocess.CalledProcessError: Command '['git', 'push', '--set-upstream', 'origin', 'main']' returned non-zero exit status 1.OSError: remote: error: cannot lock ref 'refs/heads/main': is at 7e59d1c65a4cf5010f2445a0bad6199ea4dea34f but expected 31f6a4614c202fa2af7d677bca993b0bf4c1cab9        "
13600,b'Feature Extractor: Wav2Vec2 & Speech2Text - Allow truncation + padding=longest',2021-09-16T11:44:57Z,2021-09-16T18:02:55Z,,,
13599,b'Installing transformer on docker',2021-09-16T11:16:31Z,2021-11-14T15:02:07Z,,,
13598,b'Make assertions only if actually chunking forward',2021-09-16T10:22:40Z,2021-09-24T06:52:16Z,,,
13597,b'seq2seq trainer gives OOM error while evaluating',2021-09-16T10:18:18Z,2021-09-17T02:24:56Z,,,
13596,b'Inconsistency in  class naming',2021-09-16T10:10:24Z,2021-10-16T18:34:42Z,,,
13595,b'run_summarization.py freezes ',2021-09-16T09:48:27Z,2021-10-24T15:02:35Z,,,
13594,b'Improve tokenizer tests',2021-09-16T08:31:06Z,2021-12-03T07:39:10Z,,,
13593,b'Correct device when resizing position embeddings',2021-09-16T07:30:15Z,2021-09-16T08:07:48Z,,,
13592,b'distributed training not starting',2021-09-16T04:39:23Z,2021-09-16T05:46:50Z,,,
13591,"b""ImportError: cannot import name 'auto_class_factory'""",2021-09-16T04:02:37Z,2021-10-24T15:02:37Z,,ImportError,"ImportError: cannot import name 'auto_class_factory' from 'transformers.models.auto.modeling_auto' "
13590,"b"" Unrecognized configuration class <class 'transformers.models.distilbert.configuration_distilbert.DistilBertConfig'> for this kind of AutoModel: AutoModelForSeq2SeqLM.""",2021-09-16T01:41:31Z,2021-09-16T18:42:27Z,,,
13589,b'[ci] nightly: add deepspeed master',2021-09-16T00:10:51Z,2021-09-16T00:18:34Z,,,
13588,"b""Add system-wide requirements to 'transformers-cli env' documentation""",2021-09-15T23:24:14Z,2021-09-24T13:24:09Z,,,
13587,b'[deepspeed] replaced deprecated init arg',2021-09-15T23:11:39Z,2021-09-16T19:12:17Z,,,
13586,b'Fix make fix-copies with type annotations',2021-09-15T22:58:35Z,2021-09-16T15:55:38Z,,,
13585,b'[Tests] Disable flaky s2t test ',2021-09-15T21:46:20Z,2021-09-16T07:07:20Z,,,
13584,b'Option to serialize tokenizers in memory instead of to a directory',2021-09-15T20:22:06Z,2021-11-10T15:06:26Z,,,
13583,"b""fix-copies doesn't work well in some cases""",2021-09-15T19:46:24Z,2021-09-16T15:55:38Z,,,
13582,b'Fix DataCollatorForSeq2Seq when labels are supplied as Numpy array instead of list',2021-09-15T18:03:02Z,2021-09-16T14:35:57Z,,,
13581,b'gpt-j input shape after finetuning',2021-09-15T17:00:49Z,2021-11-08T16:15:47Z,,,
13580,"b""Trainer error when finetuning Pegasus - cannot import name 'amp' from 'apex' """,2021-09-15T16:12:25Z,2021-09-15T17:47:07Z,,`ImportError,"`ImportError: cannot import name 'amp' from 'apex' (unknown location) `"
13579,b'Initial support for symbolic tracing with torch.fx allowing dynamic axes',2021-09-15T16:00:32Z,2021-10-05T12:19:48Z,,,
13578,b'Example of exporting BartModel + Beam Search.',2021-09-15T15:32:34Z,2021-10-27T08:52:19Z,,,
13577,b'XLMR tokenizer is fully picklable',2021-09-15T11:28:22Z,2021-09-16T20:30:05Z,,,
13576,"b""Custom GPT2 Model won't load after training""",2021-09-15T11:00:57Z,2021-10-23T15:02:02Z,,TypeError,"TypeError: __init__() missing 1 required positional argument: 'config'"
13575,b'Q About BART bart-large-cnn',2021-09-15T10:10:12Z,2021-09-16T14:22:26Z,,,
13574,b'Add cpu distributed fine-tuning support for transformers Trainer API',2021-09-15T04:34:46Z,2021-09-23T16:15:28Z,,,
13573,b'Add Mistral GPT-2 Stability Tweaks',2021-09-15T04:32:03Z,2021-10-04T11:37:09Z,,,
13572,b'Add SigOpt HPO to transformers trainer api',2021-09-15T04:20:17Z,2021-09-23T15:01:51Z,,,
13571,b'single_word option of new tokens are disabled by save_pretrained when we save and reload a tokenizer twice',2021-09-15T02:47:17Z,2021-10-23T15:02:03Z,,,
13570,b'BART',2021-09-15T02:38:17Z,2021-09-15T19:24:05Z,,,
13569,b'Log metrics during training',2021-09-14T22:04:55Z,2021-10-23T15:02:04Z,,,
13568,"b""Marian Encoder's last hidden states from MarianMT and TFMarianMTModel don't match""",2021-09-14T19:57:25Z,2021-11-01T15:07:52Z,,,
13567,b'CANINE model in huggingface transformers performs worse than mBERT?',2021-09-14T17:34:54Z,2021-09-25T13:36:45Z,,,
13566,b'Fix test_fetcher when setup is updated',2021-09-14T17:26:01Z,2021-09-14T17:33:41Z,,,
13565,b'[Flax] Fixes typo in Bart based Flax Models',2021-09-14T16:29:23Z,2021-09-15T05:33:53Z,,,
13564,b'upgrade sentencepiece version',2021-09-14T15:39:53Z,2021-09-15T13:25:03Z,"Installation, dependencies",,
13563,b'sentencepiece version need upgrade',2021-09-14T15:32:49Z,2021-09-15T13:25:03Z,,,
13562,b'Layoutlm onnx support (Issue #13300)',2021-09-14T12:39:29Z,2021-09-21T19:39:38Z,,,
13561,b'm2m100 conversion failed from fairseq to hf format',2021-09-14T12:10:05Z,2021-10-24T15:02:39Z,,RuntimeError,"RuntimeError: Error(s) in loading state_dict for M2M100Model:"
13560,b'add flax mbart in auto seq2seq lm',2021-09-14T10:30:36Z,2021-09-14T13:36:41Z,,,
13559,b'[Pretrained Model] Add resize_position_embeddings',2021-09-14T10:11:17Z,2021-09-15T17:03:57Z,,,
13558,b'Internal links in README.md tables are broken',2021-09-14T09:33:18Z,2021-09-29T07:45:57Z,,,
13557,b'How to use transformers pipeline with multi-gpu?',2021-09-14T06:23:30Z,2021-11-13T15:02:23Z,,,
13556,"b'Mixture of non-prefixed and prefixed (B-, I-)'",2021-09-14T05:09:42Z,2021-10-22T15:06:19Z,,,
13555,b'Mismatch of implementations of attention mask in transformers and tokenizers ',2021-09-14T03:54:10Z,2021-10-22T15:06:20Z,,,
13554,"b""LayoutLMv2 processing doesn't handle tokenizer overflow""",2021-09-14T01:44:44Z,,Good First Issue,,
13553,b'`prediction_loss_only` = False returns `float.detatch()` error witrh HF trainer',2021-09-13T21:25:43Z,2021-10-22T15:06:21Z,,AttributeError,"AttributeError: 'float' object has no attribute 'detach'"
13552,b'[WIP] Add TFSpeech2Text',2021-09-13T20:04:27Z,2021-11-19T00:42:11Z,,,
13551,b'QUESTION: How to perform 2D interpolation of pre-trained position embeddings for fine-tuning on Vision Transformers?',2021-09-13T19:54:18Z,2021-09-14T16:30:06Z,,,
13550,b'Nightly torch ci',2021-09-13T19:41:19Z,2021-09-13T20:17:29Z,,,
13549,b'Nightly ci torch',2021-09-13T19:39:28Z,2021-09-13T19:40:46Z,,,
13548,b'RFC: split checkpoint load/save for huge models',2021-09-13T18:09:24Z,,WIP,,
13547,b'train loss is not decreasing using TFBertModel',2021-09-13T15:17:22Z,2021-09-13T15:58:40Z,,,
13546,b'RAG issue',2021-09-13T14:28:26Z,2021-10-21T15:06:47Z,,ValueError,"ValueError: unable to parse C:\My_Projects\code-repo\retriever\models\tokenizer_config.json as a URL or as a local path."
13545,b'Load BERT as DPRQuestionEncoder using from_pretrained method',2021-09-13T12:56:02Z,2021-10-21T15:06:48Z,,NotImplementedError,"NotImplementedError: Make sure `_init_weigths` is implemented for <class 'transformers.models.dpr.modeling_dpr.DPRQuestionEncoder'>"
13544,b'Can i covert a MarianMT Model to tensorflow lite model?',2021-09-13T11:36:40Z,2021-09-15T08:15:03Z,,,
13543,b'[Feature Extractors] Return attention mask always in int32',2021-09-13T11:06:32Z,2021-09-13T12:02:23Z,,,
13542,b'Add checks to build cleaner model cards',2021-09-13T10:59:23Z,2021-09-14T15:27:33Z,,,
13541,b'Small changes in `perplexity.rst`to make the notebook executable on google collaboratory',2021-09-13T10:38:26Z,2021-09-13T11:32:32Z,,,
13540,"b""fixing BC in `fill-mask` (wasn't tested in theses test suites apparently).""",2021-09-13T10:31:09Z,2021-09-13T10:48:55Z,,,
13539,b'[SequenceFeatureExtraction] Move padding logic from pure python to numpy',2021-09-13T10:02:47Z,2021-09-21T14:10:13Z,,,
13538,b'[Speech2Text] Give feature extraction higher tolerance',2021-09-13T09:54:46Z,2021-09-13T11:07:59Z,,,
13537,b'[Bart] fix slow bart mask-infilling tests',2021-09-13T09:43:29Z,2021-09-13T10:49:09Z,,,
13536,b'[Speech2Text2] Skip newly added tokenizer test',2021-09-13T09:18:31Z,2021-09-13T09:30:10Z,,,
13535,b'Fix attention mask size checking for CLIP',2021-09-13T07:55:57Z,2021-09-13T08:08:38Z,,,
13534,b'Package transformers.onnx should handle Tensorflow',2021-09-12T19:32:39Z,2022-02-23T12:33:49Z,WIP,,
13533,b'Joint Sequence and Token Classification',2021-09-12T19:17:23Z,2021-09-22T13:25:28Z,,,
13532,"b""fix PhophetNet 'use_cache' assignment of no effect""",2021-09-12T10:45:26Z,2021-09-13T05:48:50Z,,,
13528,"b""Trainer's create_model_card creates an invalid yaml metadata `datasets: - null`""",2021-09-11T11:16:03Z,2021-09-14T15:27:33Z,"model card, Should Fix",,
13527,b'T5 support for text classification demo code',2021-09-11T07:33:19Z,2021-10-19T15:02:29Z,,ValueError,"ValueError: Unrecognized configuration class <class 'transformers.models.t5.configuration_t5.T5Config'> for this kind of AutoModel: AutoModelForSequenceClassification."
13526,b'ONNXRuntimeError]RUNTIME_EXCEPTION : Non-zero status code returned while running Reshape node.',2021-09-11T07:25:43Z,2021-11-03T15:02:00Z,,RuntimeException,"RuntimeException: [ONNXRuntimeError] : 6 : RUNTIME_EXCEPTION : Non-zero status code returned while running Reshape node. Name:'Reshape_501' Status Message: D:\a_work\1\s\onnxruntime\core\providers\cpu\tensor\reshape_helper.h:42 onnxruntime::ReshapeHelper::ReshapeHelper gsl::narrow_cast<int64_t>(input_shape.Size()) == size was false. The input tensor cannot be reshaped to the requested shape. Input shape:{1,1,1,4096}, requested shape:{1,1,1,16,128}"
13525,b'FlaxCLIPModel memory leak due to JAX `jit` function cache',2021-09-11T06:47:13Z,2021-09-11T07:27:43Z,,,
13524,b'Fix GPTNeo onnx export',2021-09-11T05:38:31Z,2021-09-15T11:08:42Z,,,
13523,b'[tokenizer] use use_auth_token for config',2021-09-11T03:48:17Z,2021-09-13T11:31:35Z,,,
13522,b'The new impl for CONFIG_MAPPING prevents users from adding any custom models',2021-09-11T00:16:44Z,2021-10-18T14:22:46Z,,,
13521,b'Ignore `past_key_values` during GPT-Neo inference',2021-09-10T22:14:44Z,2021-09-13T07:06:08Z,,,
13520,b'[WIP] Wav2vec2 pretraining 2',2021-09-10T22:07:06Z,2021-11-08T15:02:18Z,,,
13519,b'CodeT5',2021-09-10T19:31:02Z,2021-09-11T09:02:40Z,New model,,
13518,b'updated setup.py',2021-09-10T17:37:06Z,2021-10-19T15:02:31Z,,,
13517,b'[Wav2Vec2] Fix dtype 64 bug',2021-09-10T16:07:16Z,2021-09-10T16:19:11Z,,,
13516,b'Loading SentenceTransformers (DistilBertModel) model using from_pretrained(...) HF function into a DPRQuestionEncoder model',2021-09-10T15:23:16Z,2021-10-26T15:07:28Z,,NotImplementedError,"NotImplementedError: Make sure `_init_weigths` is implemented for <class 'transformers.models.dpr.modeling_dpr.DPRQuestionEncoder'>"
13515,b'beit-flax',2021-09-10T13:58:26Z,2021-09-21T11:34:20Z,,,
13514,b'separate model card git push from the rest',2021-09-10T13:54:55Z,2021-09-14T16:07:36Z,"model card, work in progress, trainer",,
13513,b'TF multiple choice loss fix',2021-09-10T13:04:17Z,2021-09-10T13:49:17Z,,,
13512,b'[Wav2Vec2] Fix normalization for non-padded tensors',2021-09-10T08:37:43Z,2021-09-10T13:27:16Z,,,
13511,b'VisionTextDualEncoder',2021-09-10T06:06:48Z,2021-11-30T16:51:48Z,,,
13510,b'Huge bug. TF saved model running in nvidia-docker dose not use GPU.',2021-09-10T05:19:52Z,2021-10-19T15:02:32Z,,,
13509,b'Warn for unexpected argument combinations',2021-09-10T04:02:34Z,2021-09-24T13:14:24Z,,,
13508,b'[megatron_gpt2] checkpoint v3',2021-09-10T03:42:22Z,2021-09-20T15:50:54Z,,,
13507,"b""[Benchmark]Why 'step' consume most time?""",2021-09-10T03:02:58Z,2021-10-18T15:06:37Z,,,
13506,b'Error in code',2021-09-10T02:40:06Z,2021-10-18T15:06:38Z,,,
13505,b'Insufficient memory occurs during finetune',2021-09-10T01:42:30Z,2021-10-17T12:13:46Z,Migration,,
13504,b'Wav2vec2Processor normalization issues on transformers 4.10.0',2021-09-09T21:58:59Z,2021-09-10T13:27:16Z,,,
13503,b'Push to hub when saving checkpoints',2021-09-09T19:48:05Z,2021-09-14T12:02:15Z,,,
13502,b'examples: minor fixes in flax example readme',2021-09-09T19:44:02Z,2021-09-10T06:15:57Z,,Exception,"Exception: Is a directory (os error 21)"
13501,b'Add long overdue link to the Google TRC project',2021-09-09T16:50:27Z,2021-09-14T08:11:55Z,,,
13500,b'Loading mt5-xxl rasies error related to Pytorch/TF incompatiblity',2021-09-09T14:26:49Z,2021-10-25T15:06:49Z,,OSError,"OSError: Unable to load weights from pytorch checkpoint file for 'google/mt5-xxl' at '/net/people/plgapohl/.cache/huggingface/transformers/b36655ddd18a5fda6384477b693eb51ddc8d5bfd2e9a91ed202317f660041716.c24311abc84f0f3a6095195722be4735840971f245dfb6ea3a407c9bed537390'If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True."
13499,b'Torch size missmatch in GPT-J model (Error)',2021-09-09T13:31:21Z,2021-09-23T21:09:06Z,,RuntimeError,"RuntimeError: Error(s) in loading state_dict for GPTJForCausalLM:"
13498,b'Fix obj det image size',2021-09-09T13:24:42Z,2021-09-10T12:59:51Z,,,
13497,b'early_stopping_patience_counter increasing + 2 per epoch in distributed mode',2021-09-09T13:14:24Z,2021-10-17T15:02:10Z,,,
13496,b'MarianMT int dtype fix',2021-09-09T12:48:23Z,2021-09-09T14:54:09Z,,,
13495,b'Correct order of overflowing tokens for LayoutLmV2 tokenizer',2021-09-09T12:43:38Z,2021-11-09T12:49:54Z,,,
13494,b'Fix typo in documentation',2021-09-09T10:08:56Z,2021-09-09T12:00:06Z,,,
13493,"b'Fixing backward compatiblity for non prefixed tokens (B-, I-).'",2021-09-09T09:52:56Z,2021-09-09T17:36:10Z,,,
13492,"b'FlaxCLIPModel is 10x faster than CLIPModel during inference, and 100x slower getting gradients?'",2021-09-09T08:30:00Z,2021-10-10T04:58:12Z,,,
13491,b'[GPT-Neo] Simplify local attention',2021-09-09T07:31:52Z,2021-09-10T17:22:20Z,,,
13490,b'Rouge Metric Evaluation in Training/After Training T5 run_summarization.py ',2021-09-09T05:47:17Z,2021-10-17T15:02:11Z,,,
13489,b'Fix special tokens not correctly tokenized',2021-09-09T04:47:12Z,2021-09-17T14:28:28Z,,,
13488,b'add non auto regressive model?',2021-09-09T02:01:47Z,,New model,,
13487,b'Cannot run Movement pruning on GLUE',2021-09-09T01:52:32Z,2021-09-09T02:06:10Z,,ImportError,"ImportError: cannot import name 'add_start_docstrings_to_model_forward' from 'transformers.file_utils' (/root/transformers/examples/research_projects/movement-pruning/src/transformers/src/transformers/file_utils.py)"
13486,b'Refactor internals for Trainer push_to_hub',2021-09-08T20:36:27Z,2021-09-09T17:04:38Z,,,
13485,b'Enable passing config directly to PretrainedConfig.from_pretrained()',2021-09-08T18:55:17Z,2021-10-17T15:02:12Z,,,
13484,b'push to hub issue',2021-09-08T17:31:53Z,2021-11-14T15:02:09Z,,,
13483,"b""`BertTokenizer` splits tokens added via `add_tokens(...', special_tokens=False)` to a few subtokens """,2021-09-08T17:27:26Z,2021-09-17T14:28:28Z,,,
13482,b'Fix typo in deepspeed documentation',2021-09-08T17:09:30Z,2021-09-08T18:24:11Z,,,
13481,b'Difference in BART position embeddings with fairseq implementation',2021-09-08T16:23:58Z,2021-09-20T22:37:07Z,,,
13480,b'Fix integration tests for `TFWav2Vec2` and `TFHubert`',2021-09-08T15:28:41Z,2021-09-08T16:51:51Z,,,
13479,b'Fix Tensorflow T5 with int64 input',2021-09-08T13:13:35Z,2021-09-08T14:06:05Z,,,
13478,b'Throw ValueError for mirror downloads',2021-09-08T12:25:26Z,2021-09-08T13:09:22Z,,,
13477,"b'Typo in ""end_of_word_suffix""'",2021-09-08T11:09:24Z,2021-09-08T15:26:07Z,,,
13476,b'T5',2021-09-08T10:27:56Z,2021-10-16T15:01:51Z,,,
13475,b'replacing model head failure',2021-09-08T09:33:32Z,2021-10-16T15:01:52Z,,,
13474,b'fix CLIP conversion script.',2021-09-08T07:18:36Z,2021-09-08T07:27:19Z,,,
13473,b'LayoutLMv2 forward pass fails during pytorch.jit.trace forward pass',2021-09-08T06:42:17Z,2021-09-09T16:22:22Z,,RuntimeError,"RuntimeError: The expanded size of the tensor (561) must match the existing size (512) at non-singleton dimension 1.  Target sizes: [8, 561].  Tensor sizes: [1, 512]"
13472,"b""[Errno 13] Permission denied: '/.cache'""",2021-09-08T06:40:22Z,2021-09-09T05:20:22Z,,,
13471,b'BertTokenizerFast can not tokenize [MASK] to 4 id',2021-09-08T06:30:11Z,2021-09-23T14:18:31Z,,,
13470,b'Deprecate Mirror',2021-09-08T04:39:19Z,2021-09-08T08:09:44Z,,,
13469,b'Non-BERT dpr tokenizers',2021-09-07T23:43:09Z,2021-10-29T15:07:18Z,,,
13468,b'Use powers of 2 in download size calculations',2021-09-07T20:29:38Z,2021-09-07T20:47:53Z,,,
13467,b'Dynamically load model code from the Hub',2021-09-07T18:57:32Z,2021-09-20T17:59:21Z,,,
13466,b'1x model size CPU memory usage for `from_pretrained` ',2021-09-07T18:25:40Z,2021-09-23T02:33:09Z,,,
13465,b'Enable automated model list copying for localized READMEs',2021-09-07T18:04:39Z,2021-09-08T12:03:36Z,,,
13464,"b""Don't modify labels inplace in `LabelSmoother`""",2021-09-07T17:00:07Z,2021-09-08T11:45:36Z,,,
13463,b'Upcasting of attention computation for reliable pretraining of GPT-2 models',2021-09-07T16:14:27Z,2021-10-04T11:37:09Z,,,
13462,b'A Space Always Prefixes The First Token of `xlm-roberta-large` Encoding Results',2021-09-07T09:41:04Z,2021-10-16T15:01:53Z,,,
13461,"b'In ViT model,  last_hidden_state is not equal to hidden_states[-1]'",2021-09-07T08:25:18Z,2021-09-07T09:02:27Z,,,
13460,"b'when I overwrite a model,I meet some bugs like this'",2021-09-07T08:00:02Z,2021-09-07T09:33:25Z,,AttributeError,"AttributeError: 'NoneType' object has no attribute 'from_pretrained'"
13459,b'Add `tokenizer_max_length` to `cardiffnlp/twitter-roberta-base-sentiment`',2021-09-07T07:56:27Z,2021-10-16T15:01:53Z,,,
13458,b'Translation takes too long - from fine-tuned mbart-large-50 model',2021-09-07T05:04:58Z,2021-10-17T15:02:13Z,,,
13457,b'Can I use input embeds to generate text without input ids',2021-09-07T01:38:03Z,2021-10-16T15:01:54Z,,,
13456,b'Fix img classification tests',2021-09-07T01:31:05Z,2021-09-07T09:58:45Z,,,
13455,b'[docs] update dead quickstart link on resuing past for GPT2',2021-09-06T20:22:29Z,2021-09-07T20:57:58Z,,,
13454,b'Update version of `packaging` package',2021-09-06T19:12:07Z,2021-09-06T21:19:02Z,,AttributeError,"AttributeError: 'Version' object has no attribute 'major'"
13453,b'Multi-GPU memory usage',2021-09-06T17:13:37Z,2021-10-16T15:01:55Z,,,
13452,b'New debug wav2vec2 flax',2021-09-06T16:53:06Z,2021-10-16T15:01:56Z,,,
13451,b'skip image classification example test',2021-09-06T15:57:16Z,2021-09-06T16:16:26Z,,,
13450,"b'T5Tokenizer.from_pretrained(""t5-small"") returning NoneType'",2021-09-06T15:34:33Z,2021-09-07T02:26:34Z,,,
13449,b'Making it raise real errors on ByT5.',2021-09-06T14:22:57Z,2021-09-07T14:45:45Z,,,
13448,b'[EncoderDecoder] Fix torch device in tests',2021-09-06T13:23:17Z,2021-09-06T14:09:24Z,,,
13447,b'Adding a test for multibytes unicode.',2021-09-06T13:20:34Z,2021-09-06T14:11:24Z,,,
13446,b'[PyTorch Tests] Fix torchvision test',2021-09-06T13:16:18Z,2021-09-06T13:18:06Z,,,
13445,b'[WIP] Fix CLIPTokenizerFast',2021-09-06T11:59:40Z,2021-10-15T15:01:48Z,,,
13444,b'Adding GPT2 for translation/summarization',2021-09-06T11:11:28Z,2021-10-19T15:02:35Z,,,
13443,b'Error while saving a variation of roberta-base fast tokenizer vocabulary',2021-09-06T09:56:05Z,2021-10-15T15:01:49Z,,pyo3_runtime.PanicException,"pyo3_runtime.PanicException: no entry found for key"
13442,b'Unexpected result when tokenizing a single token rather than a sentence with multilingual tokenizer',2021-09-06T09:39:20Z,2021-09-11T03:09:24Z,,,
13441,"b""Could you support other distributed training backends from command line, I'm using GLOO.""",2021-09-06T09:20:02Z,2021-11-16T15:06:47Z,,,
13440,b'add the implementation of RecAdam optimizer',2021-09-06T08:20:38Z,2021-10-17T15:02:14Z,,,
13439,b'MisconfigurationException: Found invalid type for plugin None. Expected a precision or training type plugin.',2021-09-06T08:03:08Z,2021-09-24T08:04:58Z,,pytorch_lightning.utilities.exceptions.MisconfigurationException,"pytorch_lightning.utilities.exceptions.MisconfigurationException: Found invalid type for plugin None. Expected a precision or training type plugin."
13438,b'add torchvision in example test requirements',2021-09-06T06:31:09Z,2021-09-06T13:17:54Z,,,
13437,"b""using TFOpenAIGPTLMHeadModel load pytorch model doesn't work well""",2021-09-06T06:23:25Z,2021-10-17T15:02:15Z,,,
13436,b'[CLIP] fix logit_scale init',2021-09-06T05:51:46Z,2021-09-08T08:51:13Z,,,
13435,"b'Official example not working, are you serious?'",2021-09-06T03:26:52Z,2021-10-15T15:01:50Z,,KeyError,"KeyError: 'size'"
13434,b'A dead link in GPT2 description',2021-09-05T12:55:01Z,2021-09-07T20:57:58Z,,,
13433,b'Optimized bad word ids',2021-09-05T12:06:34Z,2021-09-07T14:51:04Z,,,
13432,b'Supporting Seq2Seq model for question answering task',2021-09-05T10:02:44Z,2021-10-25T11:42:53Z,,,
13431,b'Bb',2021-09-05T09:56:30Z,2021-09-06T08:13:01Z,,,
13430,b'Difference between `logit_scale` initialisation in Transformers CLIP and the original OpenAI implementation.',2021-09-05T08:27:58Z,2021-09-08T08:51:13Z,,,
13429,b'Illegal Instruction Error on  `prepare_inputs_for_generation`  -> gpt neo/ j',2021-09-05T07:16:27Z,2021-10-14T15:03:15Z,,,
13428,b'Please add GPT Jaaye model ine right with Transformers website',2021-09-05T05:16:35Z,2021-10-06T03:04:28Z,,,
13427,b'where processor should i put in a training code?',2021-09-05T04:52:11Z,2021-10-14T15:03:16Z,,,
13426,b'convert pytorch checkpoints to TF1.x checkpoints (reverse of transformers-cli convert)',2021-09-05T04:40:11Z,2021-10-17T15:02:16Z,,,
13425,b'[Benchmark]',2021-09-04T23:05:41Z,2021-10-05T20:04:53Z,,,
13424,b'Error with T5 model: Output is always getting truncated with 20 tokens',2021-09-04T21:15:18Z,2021-09-06T08:23:00Z,,,
13423,b'Huggingface Inference API',2021-09-04T21:02:54Z,2021-09-06T20:35:40Z,,,
13422,b'Fix scheduled tests for `SpeechEncoderDecoderModel`',2021-09-04T17:26:43Z,2021-09-06T12:55:14Z,,,
13421,b'Update setup.py',2021-09-04T16:02:28Z,2021-09-06T21:32:24Z,,,
13420,b'[Flax] Addition of FlaxPegasus',2021-09-04T15:02:47Z,2021-09-14T15:15:19Z,,,
13419,b'JAX/Flax models should be `jax.jit`ed by default? Or code examples should use jax.jit (~200x speedup)',2021-09-04T10:29:01Z,2021-10-05T04:09:09Z,,,
13418,b'Cannot Replicate xlm-roberta-large-xnli Results',2021-09-04T10:27:11Z,2021-10-13T15:06:52Z,,,
13417,b'No log output to console',2021-09-04T08:39:24Z,2021-10-13T15:06:53Z,,,
13416,b'RuntimeError: Unknown: CUDNN_STATUS_EXECUTION_FAILED',2021-09-04T07:34:25Z,2021-09-09T02:19:25Z,,RuntimeError,"RuntimeError: Unknown: CUDNN_STATUS_EXECUTION_FAILED"
13415,b'13134',2021-09-04T00:47:51Z,2021-09-04T00:48:14Z,,,
13414,"b'Fixed the MultilabelTrainer document, which would cause a potential bug when executing the code originally documented.'",2021-09-04T00:13:43Z,2021-09-08T15:48:01Z,,AttributeError,"AttributeError: 'NoneType' object has no attribute 'detach'"
13413,b'Possibility of disabling add_pooling_layer that works for all models',2021-09-04T00:02:13Z,2021-12-03T15:02:28Z,,,
13412,b'Sentencepiece Unigram tokenizer add tokens',2021-09-03T21:22:19Z,2021-10-13T15:06:54Z,,,
13411,"b""AttributeError: type object 'Wav2Vec2ForCTC' has no attribute 'from_pretrained'""",2021-09-03T18:26:53Z,2021-09-14T21:33:14Z,,AttributeError,"AttributeError: type object 'Wav2Vec2ForCTC' has no attribute 'from_pretrained'"
13410,b'Make data shuffling in `run_clm_flax.py` respect global seed',2021-09-03T17:12:12Z,2021-12-14T10:04:43Z,,,
13409,b'git.exc.InvalidGitRepositoryError when running finetune_rag.py',2021-09-03T16:05:12Z,2021-09-17T08:18:52Z,,,
13408,b'Add TAPAS MLM-only models',2021-09-03T14:36:21Z,2021-09-06T17:19:31Z,,,
13407,b'How to use multiple PreTrainedModel models in a custom model?',2021-09-03T13:19:21Z,2021-10-11T15:01:56Z,,,
13406,b'Fix tests without any real effect in EncoderDecoderMixin',2021-09-03T12:56:34Z,2021-09-06T12:51:46Z,,,
13405,"b""about 'text-generation': how can I generate sentences with multiply words? """,2021-09-03T12:36:55Z,2021-10-11T15:01:57Z,,,
13403,b'Fix scheduled TF Speech tests',2021-09-03T12:23:48Z,2021-09-06T21:12:43Z,,,
13402,"b'TrainingArguments default parameters throw error (evaluation_strategy, save_strategy)'",2021-09-03T11:23:34Z,2021-10-11T15:01:58Z,,,
13401,b'Better error raised when cloned without lfs',2021-09-03T10:26:02Z,2021-09-08T12:28:22Z,,,
13400,b'Fixing #13381',2021-09-03T10:11:58Z,2021-09-09T18:23:53Z,,,
13399,b'Unified freezing interface',2021-09-03T07:13:01Z,,"Good Second Issue, WIP",,
13398,b'PreTrainedTokenizerFast to BertTokenizer',2021-09-03T06:57:28Z,2021-09-03T14:09:34Z,,,
13397,b'T5: relative position embeddings',2021-09-03T03:37:20Z,2021-09-04T05:17:37Z,,,
13396,b'[SpeechEncoderDecoder] Fix final test',2021-09-02T16:46:56Z,2021-09-02T16:47:09Z,,,
13395,b'[Tests] Fix SpeechEncoderDecoder tests',2021-09-02T16:09:51Z,2021-09-02T16:11:27Z,,,
13394,b'LayoutLMv2Processor padding/truncation issues',2021-09-02T16:09:35Z,2021-09-03T10:40:32Z,,ValueError,"ValueError: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length."
13393,b'Tapas tf',2021-09-02T15:51:54Z,2021-11-30T10:07:56Z,,,
13392,b'TorchScript warning',2021-09-02T14:24:47Z,2021-10-11T15:01:59Z,Migration,,
13391,b'How to build a custom dataset for LayoutLMv2ForSequenceClassification?',2021-09-02T12:55:47Z,2021-09-03T08:28:09Z,,RuntimeError,"RuntimeError: stack expects each tensor to be equal size, but got [1, 20] at entry 0 and [1, 266] at entry 1"
13390,b'Transformers crashes mypy',2021-09-02T10:14:40Z,2021-11-07T15:02:22Z,,,
13389,b'How can I convert fairseq checkpoint to huggingface for `XLMProphetModel`?',2021-09-02T09:49:49Z,2021-09-11T03:09:33Z,,,
13388,b'Hard time installing huggingface for python3.8 cuda10.1',2021-09-02T09:18:21Z,2021-09-02T10:57:19Z,,ImportError,"ImportError: cannot import name 'TFPreTrainedModel' from 'transformers' "
13387,b'[doc] fix mBART example',2021-09-02T07:10:07Z,2021-09-02T09:32:19Z,,,
13386,b'[docs] Update perplexity.rst to use negative log likelihood',2021-09-02T05:17:45Z,2021-09-02T11:49:13Z,,,
13385,b'Fix name and get_class method in AutoFeatureExtractor',2021-09-02T00:45:09Z,2021-09-02T00:54:50Z,,,
13384,b'not support Pytorch 1.8.2',2021-09-01T21:19:31Z,2021-10-11T15:02:01Z,,,
13383,b'[GPU Tests] Fix SpeechEncoderDecoder GPU tests',2021-09-01T21:11:24Z,2021-09-01T21:12:01Z,,,
13382,b'Small typo',2021-09-01T20:50:04Z,2021-10-06T03:10:25Z,,,
13381,b'Zero-shot classification pipeline truncation support',2021-09-01T16:16:36Z,2021-09-09T18:23:53Z,,,
13380,b'[Flax] Fix BigBird',2021-09-01T16:02:53Z,2021-09-01T16:33:54Z,,,
13379,b'Error using SpecAugment feature masking in Wav2Vec 2.0',2021-09-01T15:49:26Z,2021-10-07T16:07:33Z,,RuntimeError,"RuntimeError: The size of tensor a (299) must match the size of tensor b (1024) at non-singleton dimension 1"
13378,b'TRAINING CUSTOM MODEL USING LAYOUTLMv2!',2021-09-01T15:39:10Z,2021-10-11T15:02:02Z,,,
13377,"b""AttributeError: '_LazyAutoMapping' object has no attribute '_mapping'""",2021-09-01T15:26:02Z,2021-09-03T11:48:43Z,,,
13376,b'Enabling automatic loading of tokenizer with `pipeline` for `audio-classification`.',2021-09-01T15:07:14Z,2021-09-02T09:37:42Z,,,
13375,b'Fix RemBERT tokenizer initialization',2021-09-01T14:54:11Z,2021-09-01T15:11:32Z,,,
13374,b'Add missing feature extractors',2021-09-01T14:53:11Z,2021-09-01T15:10:49Z,,,
13373,b'Add LayoutXLM tokenizer docs',2021-09-01T14:49:24Z,2021-09-02T07:46:06Z,,,
13372,b'Properly register missing submodules in main init',2021-09-01T14:32:38Z,2021-09-01T14:57:44Z,,,
13371,b'wav2vec2-large-xlsr-53 Tokenizer unable to load',2021-09-01T14:11:48Z,2021-09-02T09:24:30Z,,,
13370,b'[Consistency] Make sure all xxxForSequenceClassification models support problem_type',2021-09-01T13:31:23Z,2021-10-10T15:01:35Z,,,
13369,b'Fix DINO',2021-09-01T12:12:20Z,2021-09-01T14:43:00Z,,,
13368,b'Fix GPT-J _CHECKPOINT_FOR_DOC typo',2021-09-01T10:57:39Z,2021-09-01T10:57:43Z,,,
13367,b'Add BlenderBot small tokenizer to the init',2021-09-01T10:46:30Z,2021-09-22T23:00:47Z,,,
13366,b'Add `Hubert` to the `AutoFeatureExtractor`',2021-09-01T10:12:18Z,2021-09-01T15:09:02Z,,,
13365,b'flax ner example',2021-09-01T08:29:28Z,2021-09-09T04:42:57Z,,,
13364,b'Move Flax self-push to test machine',2021-09-01T07:36:49Z,2021-09-01T07:37:51Z,,,
13363,b'Which files are essential when customize and modify a specific pre-trained model\xef\xbc\x9f',2021-09-01T05:16:37Z,2021-09-01T15:39:55Z,,,
13362,b'Hyperparameter search function not working with Trainer and mlflow',2021-09-01T04:10:18Z,2021-10-10T15:01:36Z,,,
13361,b'Fixes for the documentation',2021-08-31T21:09:10Z,2021-09-01T11:54:28Z,,,
13360,"b""CTRL's `config.json` on HF Hub is missing a `model_type`""",2021-08-31T19:19:48Z,2021-09-01T08:27:29Z,,ValueError,"ValueError: Unrecognized model in .. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: layoutlmv2, beit, rembert, visual_bert, canine, roformer, clip, bigbird_pegasus, deit, luke, detr, gpt_neo, big_bird, speech_to_text, vit, wav2vec2, m2m_100, convbert, led, blenderbot-small, retribert, ibert, mt5, t5, mobilebert, distilbert, albert, bert-generation, camembert, xlm-roberta, pegasus, marian, mbart, megatron-bert, mpnet, bart, blenderbot, reformer, longformer, roberta, deberta-v2, deberta, flaubert, fsmt, squeezebert, hubert, bert, openai-gpt, gpt2, transfo-xl, xlnet, xlm-prophetnet, prophetnet, xlm, ctrl, electra, encoder-decoder, funnel, lxmert, dpr, layoutlm, rag, tapas, splinter"
13359,b'Add FlaxVisionEncoderDecoderModel',2021-08-31T17:14:17Z,2021-11-09T09:44:29Z,,,
13358,b'Unexpected weights were not initialized from the model checkpoint error ',2021-08-31T12:57:21Z,2021-08-31T16:35:14Z,,,
13357,b'[GitHub Runner] Fix flax runner',2021-08-31T12:55:40Z,2021-08-31T13:01:35Z,,,
13356,b'Import of transformers package throwing value_error',2021-08-31T11:15:42Z,2021-12-01T15:03:05Z,,,
13355,b'Dependency parsing head for pretrained models',2021-08-31T10:04:39Z,,Feature request,,
13354,b'Does Bart Model can fill <mask> with variable length?',2021-08-31T09:17:25Z,2021-09-02T08:50:58Z,,,
13353,b'Torchscript test for Flaubert',2021-08-31T08:38:45Z,2021-09-01T08:44:31Z,,,
13352,b'Torchscript test for ConvBERT',2021-08-31T08:38:28Z,2021-09-01T08:43:10Z,,,
13351,b'Torchscript test for DistilBERT',2021-08-31T08:38:04Z,2021-09-01T08:42:22Z,,,
13350,b'Torchscript test',2021-08-31T08:24:22Z,2021-09-01T08:41:47Z,,,
13349,b'Layoutlm onnx support (Issue #13300)',2021-08-31T07:49:26Z,2021-09-14T11:52:09Z,,,
13348,b'Cannot run grid search using Trainer API and Ray Tune',2021-08-31T07:35:05Z,2021-09-06T18:16:40Z,,"ray.exceptions.RayTaskError(TuneError), ray.tune.error.TuneError, RuntimeError","ray.exceptions.RayTaskError(TuneError): ray::ImplicitFunc.train_buffered() (pid=1182, ip=172.28.0.2, repr=<types.ImplicitFunc object at 0x7f67395b5ed0>)ray.tune.error.TuneError: Trial raised an exception. Traceback:RuntimeError: CUDA out of memory. Tried to allocate 384.00 MiB (GPU 0; 15.90 GiB total capacity; 13.14 GiB already allocated; 312.75 MiB free; 13.29 GiB reserved in total by PyTorch)"
13347,b'Predicted Start_index < Predicted End_index in BertForQuestionAnswering',2021-08-31T06:33:51Z,2021-10-09T15:01:46Z,,,
13346,"b'Bert (sentence classification) output is non-deterministic(have checked previous issue, SET model.eval() )'",2021-08-31T03:40:58Z,2021-10-09T15:01:46Z,,,
13345,b'Doc mismatch fixed',2021-08-31T03:19:42Z,2021-08-31T10:28:37Z,,,
13344,b'How to use BertForSequenceClassification for the Apect Based Sentiment Analysis',2021-08-31T02:49:39Z,2021-10-09T15:01:47Z,,,
13343,b'OverflowError: out of range integral type conversion attempted for run_summarization.py script using t5-small',2021-08-30T23:23:11Z,2021-10-09T15:01:48Z,,OverflowError,"OverflowError: out of range integral type conversion attempted```"
13342,b'Add the `AudioClassificationPipeline`',2021-08-30T21:01:24Z,2021-09-01T08:03:48Z,,,
13341,b'Padding labels is wrong when using `pad_to_multiple_of`',2021-08-30T20:38:20Z,2021-08-31T16:28:51Z,,,
13340,b'Tests fetcher tests',2021-08-30T20:09:57Z,2021-08-31T07:57:02Z,,,
13339,b'Add generate kwargs to Seq2SeqTrainingArguments',2021-08-30T19:11:58Z,2021-08-31T12:42:00Z,,,
13338,b'Handle nested dict/lists of tensors as inputs in the Trainer',2021-08-30T18:46:51Z,2021-08-31T10:34:32Z,,,
13337,b'Fix release utils',2021-08-30T15:51:26Z,2021-08-30T16:09:14Z,,,
13336,b'Fix AutoTokenizer when no fast tokenizer is available',2021-08-30T15:43:16Z,2021-08-30T15:55:18Z,,,
13335,b'T5 - Flax - Decreasing performance on pretraining',2021-08-30T15:36:47Z,2021-12-04T15:02:35Z,,,
13334,b'Update label2id in the model config for run_glue',2021-08-30T14:29:07Z,2021-08-30T14:35:10Z,,,
13333,b'Use existing functionality for #13251',2021-08-30T13:24:44Z,2021-08-30T13:43:23Z,,,
13332,b'bug in gpt2 notebook (in tensorflow)',2021-08-30T13:03:38Z,2021-10-07T15:06:01Z,,AttributeError,"AttributeError: 'DatasetDict' object has no attribute 'cardinality'"
13331,b'bert:What is the tf version corresponding to tensformers?',2021-08-30T11:42:36Z,2021-08-30T15:45:55Z,,,
13330,b'model(**batch) returns loss dictionary that cant be divided by gradient_accu',2021-08-30T10:14:56Z,2021-09-01T07:00:31Z,,TypeError,"TypeError: unsupported operand type(s) for /: 'dict' and 'int'"
13329,b'GPT-J-6B in run_clm.py',2021-08-30T10:03:43Z,2021-10-07T15:06:03Z,,KeyError,"KeyError: 'gptj'"
13328,b'Licenses for Helsinki-NLP models',2021-08-30T09:49:59Z,2021-09-28T12:03:54Z,model card,,
13327,b'Wrong weight initialization for TF t5 model',2021-08-30T07:50:28Z,2021-10-07T15:06:04Z,,,
13326,b'Wav2Vec2ForCTC is not BaseModelOutput',2021-08-30T07:20:28Z,2021-10-07T15:06:05Z,,,
13325,b'Handling tag with no prefix for aggregation_strategy in TokenClassificationPipeline',2021-08-29T21:09:09Z,2021-09-09T17:36:09Z,,,
13324,b'distilbert-flax',2021-08-29T11:09:54Z,2021-08-30T12:16:18Z,,,
13323,b'Documentation mismatch in Preprocessing data',2021-08-29T04:46:39Z,2021-08-31T10:28:37Z,,,
13322,"b'DestilGTP2 code from pytorch-transformers does not work in transformers, I made a basic example'",2021-08-29T02:38:20Z,2021-10-06T15:06:26Z,Migration,,
13321,b'Add missing module __spec__',2021-08-28T18:18:49Z,2021-08-30T16:39:06Z,,,
13320,b'examples: only use keep_linebreaks when reading TXT files',2021-08-28T10:20:16Z,2021-08-28T14:22:29Z,,,
13319,b'neptune.ai logger: add ability to connect to a neptune.ai run',2021-08-28T10:18:25Z,2021-08-30T13:59:17Z,,,
13318,b'Errors when fine-tuning RAG on cloud env',2021-08-28T09:41:37Z,2021-10-05T15:12:10Z,,,
13317,b'How to use the pretraining task of ProphetNet',2021-08-28T09:02:04Z,2021-10-05T15:12:11Z,,,
13316,b'Squeeze and Excitation Network',2021-08-28T05:38:51Z,2021-11-08T15:02:24Z,,,
13315,"b""Current trainer.py doesn't support beam search""",2021-08-28T01:00:24Z,2021-10-05T15:12:13Z,,,
13314,b'neptune.ai logger: utilize `rewrite_logs` in `NeptuneCallback` as in `WandbCallback`',2021-08-27T23:50:49Z,2021-08-28T12:04:39Z,,,
13313,"b'[Testing] Add Flax Tests on GPU, Add Speech and Vision to Flax & TF tests'",2021-08-27T22:25:45Z,2021-08-31T09:08:22Z,,,
13312,b'Having problem Pre-training GPT models',2021-08-27T20:37:02Z,2021-08-28T09:01:26Z,,"TypeError, notebook","TypeError: __init__() got an unexpected keyword argument 'keep_linebreaks'notebook: https://colab.research.google.com/drive/1bk8teH0Egu-gAmBC_zlvUifMHS7y_SyM?usp=sharing"
13311,b'[Feature request] Introduce GenericTransformer to ease deployment of custom models to the Hub',2021-08-27T19:43:26Z,2021-10-05T15:12:13Z,,,
13310,b':bug: fix small model card bugs',2021-08-27T17:46:33Z,2021-08-30T14:45:57Z,,,
13309,b'Fixing a typo in the data_collator documentation',2021-08-27T16:49:45Z,2021-08-31T10:01:12Z,,,
13308,b'[Large PR] Entire rework of pipelines.',2021-08-27T16:21:17Z,2021-09-10T12:47:49Z,,,
13307,b'[Flax] Correct all return tensors to numpy',2021-08-27T15:28:40Z,2021-08-27T15:38:34Z,,,
13306,b'Missing on_predict event in TrainerCallback',2021-08-27T15:22:01Z,2021-08-31T08:31:57Z,,,
13305,b'Layoutlm onnx support',2021-08-27T15:20:23Z,2021-08-31T07:43:28Z,,,
13304,b'Slow tests - run rag token in half precision',2021-08-27T14:48:03Z,2021-08-30T10:02:08Z,,,
13303,b'[Slow tests] Disable Wav2Vec2 pretraining test for now',2021-08-27T14:30:20Z,2021-08-30T10:03:03Z,,,
13302,b'Fix loading for newer m2m models',2021-08-27T14:07:19Z,2021-10-31T15:02:14Z,,,
13301,b'Fixing mbart50 with `return_tensors` argument too.',2021-08-27T14:05:55Z,2021-08-27T15:22:06Z,,,
13300,b'Support for converting LayoutLM to ONNX',2021-08-27T12:33:50Z,2021-10-04T15:06:29Z,,,
13299,b'Moving `zero-shot-classification` pipeline to new testing.',2021-08-27T10:52:57Z,2021-08-27T13:46:12Z,,,
13298,b'Examples: label mapping for text classication tasks are not written into configuration',2021-08-27T10:19:44Z,2021-08-30T14:35:10Z,,,
13297,b'Moving `translation` pipeline to new testing scheme.',2021-08-27T09:43:19Z,2021-08-27T10:26:17Z,,,
13296,b'__version__ attribute missing in mode config for sentence-transformers/paraphrase-mpnet-base-v2',2021-08-27T09:38:55Z,2021-09-26T15:04:48Z,,,
13295,b'GPT2 model state dictionary Tensor types are not matching with pytorch',2021-08-27T08:37:47Z,2021-10-04T15:06:30Z,,,
13294,b'albert flax',2021-08-27T08:36:49Z,2021-08-30T15:29:28Z,,,
13293,b'DistilBertTokenizer for distilbert-base-multilingual-cased is unable to encode / decode Japanese characters properly adding unnecessary characters in between',2021-08-27T07:37:24Z,2021-10-04T15:06:31Z,,,
13292,b'Add REALM',2021-08-27T07:31:40Z,2022-01-18T12:24:14Z,WIP,,
13291,b'torch longformer to tf longformer',2021-08-27T03:41:19Z,2021-08-30T12:23:38Z,,,
13290,b'Add GPT2ForTokenClassification',2021-08-27T03:13:21Z,2021-08-31T10:19:04Z,,,
13289,b'Fix minor typo in parallelism doc',2021-08-27T03:04:00Z,2021-08-31T10:49:05Z,,,
13288,b'GPT2 for classification - Errors encountered while running run_glue.py and (possible) fixes',2021-08-26T22:47:19Z,2021-10-16T15:01:59Z,,ValueError,"ValueError: Gradient clipping in the optimizer (by setting clipnorm or clipvalue) is currently unsupported when using a distribution strategy."
13287,b'Pretraining T5-v1_1 on Flax',2021-08-26T18:18:16Z,2021-10-04T15:06:32Z,,,
13286,b'Moving `token-classification` pipeline to new testing.',2021-08-26T15:32:00Z,2021-08-27T09:24:57Z,,,
13285,b'Moving `text-generation` pipeline to new testing framework.',2021-08-26T14:44:32Z,2021-08-26T15:30:04Z,,,
13284,b'Question about bart-base model',2021-08-26T14:23:38Z,2021-08-27T11:29:07Z,,,
13283,b'Moving `text2text-generation` to new pipeline testing mecanism',2021-08-26T14:12:24Z,2021-08-26T14:26:59Z,,,
13282,b'Hotfixing master tests.',2021-08-26T14:02:15Z,2021-08-26T14:09:53Z,,,
13281,b'Moving `table-question-answering` pipeline to new testing',2021-08-26T13:43:09Z,2021-08-26T14:09:48Z,,,
13280,b'Moving `table-question-answering` pipeline to new testing.',2021-08-26T12:47:17Z,2021-08-26T13:09:58Z,,,
13279,b'Moving `summarization` pipeline to new testing format.',2021-08-26T10:56:24Z,2021-08-26T12:47:11Z,,,
13278,b'[Hotfix] Fixing the test (warnings was incorrect.)',2021-08-26T10:10:52Z,2021-08-26T10:13:48Z,,,
13277,b'Moving question_answering tests to the new testing scheme. Had to tweak a little some ModelTesterConfig for pipelines.',2021-08-26T10:05:32Z,2021-08-26T10:37:55Z,,,
13276,b'Announcing the default model used by the pipeline (with a link).',2021-08-26T09:44:34Z,2021-08-30T10:04:31Z,,,
13275,b'Fix BeitForMaskedImageModeling',2021-08-26T09:41:58Z,2021-08-27T13:09:57Z,,,
13274,b'`pipeline` backed with ONNX Runtime and quantization for faster inference',2021-08-26T09:08:58Z,2021-10-09T15:01:49Z,,,
13273,b'Docs: TrainingArguments call incorrect',2021-08-26T08:59:27Z,2021-10-03T15:02:02Z,,,
13272,b'Move `image-classification` pipeline to new testing',2021-08-26T08:47:19Z,2021-08-26T09:52:49Z,,,
13271,"b'Global transformers package imports, render local changes to the transformer src code useless for example scripts '",2021-08-26T01:23:24Z,2021-10-03T15:02:03Z,,,
13270,"b'Commit v4.9.2 release appears as v4.5.1 in ""transformers-cli env""'",2021-08-26T00:01:53Z,2021-10-05T15:12:15Z,,,
13269,b'Add PLBart',2021-08-25T19:24:45Z,2022-02-18T13:17:09Z,WIP,,
13268,b'additional global attended token in bigbird-roberta',2021-08-25T18:22:35Z,2021-09-08T12:23:30Z,,,
13267,b'Better notification service',2021-08-25T16:06:55Z,2021-08-25T16:14:44Z,,,
13266,b'Add error message concerning revision',2021-08-25T15:53:37Z,2021-08-26T08:32:58Z,,,
13265,b'Add DINO conversion script',2021-08-25T14:38:23Z,2021-08-26T15:25:21Z,,,
13264,b'Revisions not working as expected',2021-08-25T14:19:40Z,2021-10-21T15:48:49Z,,OSError,"OSError: Can't load config for 'GroNLP/bert-base-dutch-cased'. Make sure that:"
13263,b'Replace assert statement with if condition and raise ValueError',2021-08-25T13:29:32Z,2021-08-25T16:14:03Z,,,
13262,b'Printing weights of a pre-trained model ',2021-08-25T11:18:43Z,2021-08-25T12:27:06Z,,,
13261,b'Fix failing Hubert test',2021-08-25T10:40:31Z,2021-08-25T16:41:27Z,,,
13260,b'Add require flax to MT5 Flax test',2021-08-25T10:04:31Z,2021-08-25T16:56:25Z,,,
13259,b'Some `model_type`s cannot be in the mapping',2021-08-25T09:59:38Z,2021-08-25T16:56:16Z,,,
13258,b'Add CLIP tokenizer to AutoTokenizer',2021-08-25T09:40:26Z,2021-08-25T16:56:08Z,,,
13257,b'Remove side effects of disabling gradient computaiton',2021-08-25T09:32:45Z,2021-08-25T09:32:51Z,,,
13256,b'ingore_mismatched_sizes Wav2Vec2 unknown argument',2021-08-25T09:07:52Z,2021-08-25T12:20:56Z,,TypeError,"TypeError: __init__() got an unexpected keyword argument 'ingore_mismatched_sizes'"
13255,b'Label Smoothing for Question Answering task',2021-08-25T08:58:08Z,2021-10-03T15:02:05Z,,,
13254,b'\xe8\xaf\xb7\xe9\x97\xae\xe4\xb8\x80\xe4\xb8\x8b\xef\xbc\x8c\xe6\x83\x85\xe6\x84\x9f\xe5\x88\x86\xe6\x9e\x90\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\xe5\x8f\xaa\xe6\x98\xaf\xe9\x92\x88\xe5\xaf\xb9\xe8\x8b\xb1\xe6\x96\x87\xe7\x9a\x84\xe5\x90\x97',2021-08-25T08:22:14Z,2021-08-25T08:45:10Z,,,
13253,b'Cannot use RemBert',2021-08-25T05:59:32Z,2021-08-25T06:49:37Z,,,
13252,b'Add `--max_length` argument in seq2seq trainer.',2021-08-25T05:40:14Z,2021-08-30T19:35:24Z,,,
13251,b'fix `tokenizer_class_from_name` for models with `-` in the name',2021-08-25T04:15:47Z,2021-08-26T08:29:15Z,,ModuleNotFoundError,"ModuleNotFoundError: No module named 'transformers.models.xlm-roberta'"
13250,b'Check None before going through iteration',2021-08-25T04:14:33Z,2021-08-30T12:18:51Z,,,
13249,b'how to finetune mT5 on XGLUE-NTG task',2021-08-25T03:48:57Z,2021-10-03T15:02:05Z,Migration,,
13248,b'[doc] correct TP implementation resources',2021-08-25T03:10:32Z,2021-08-31T10:47:23Z,,,
13247,b'Why do we need to use `Loss.repeat(eval_batch_size)` in accelerator gather loop? ',2021-08-25T00:25:09Z,2021-10-03T15:02:06Z,,,
13246,b'[model loading] framework-agnostic dtype parameter',2021-08-24T22:37:45Z,,WIP,,
13245,b'Add ability to include additional model card info within Trainer',2021-08-24T22:19:55Z,2021-09-02T19:20:18Z,,,
13244,b'Tapas tokenization Different from Tensorflow Code',2021-08-24T20:19:40Z,,Good First Issue,,
13243,b'Validate onnx model',2021-08-24T20:07:55Z,2021-10-29T15:07:22Z,,,
13242,b'[Tentative] Adds support for exporting TransformerXL-based models to ONNX',2021-08-24T19:29:02Z,2021-11-29T18:14:55Z,WIP,,
13241,b'Can we use trainer api with custom model layer?',2021-08-24T17:16:04Z,2021-09-06T15:44:34Z,,,
13240,b'Improve T5 docs ',2021-08-24T13:18:11Z,2021-09-01T13:05:40Z,,,
13239,b'BERT finetuning \xe2\x80\x9cindex out of range in self\xe2\x80\x9d',2021-08-24T13:09:19Z,2021-10-01T15:02:15Z,Migration,IndexError,"IndexError: index out of range in self"
13238,b'Correct way to use pre-trained models - Any document on this?',2021-08-24T12:57:48Z,2021-10-01T15:02:15Z,,,
13237,b'Fix broken links in Splinter documentation',2021-08-24T11:27:25Z,2021-08-24T11:55:22Z,,,
13236,b'Upgrade `os.path` to use `pathlib.Path` API for `from_pretrained` internals',2021-08-24T11:26:16Z,2021-08-28T12:42:29Z,,,
13235,b'BeitForMaskedImageModeling forward not using bool_masked_pos',2021-08-24T10:52:49Z,2021-08-27T13:09:57Z,,,
13234,b'Error while trying to run run_wwm_mlm.py using my saved model: TypeError: \xe2\x80\x98NoneType\xe2\x80\x99 object is not iterable',2021-08-24T08:04:57Z,2021-10-02T15:02:02Z,,TypeError,"TypeError: NoneType object is not iterable"
13233,"b""Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at 'yyy' and are newly initialized""",2021-08-24T07:20:26Z,2021-10-02T15:02:03Z,,,
13232,"b""run_translation_no_trainer with MBart: unsupported operand type(s) for /: 'dict' and 'int'""",2021-08-24T07:06:21Z,2021-08-30T16:41:05Z,,,
13231,b'codecarbon plugin issues',2021-08-24T02:52:57Z,2021-12-04T15:02:37Z,,"urllib3.exceptions.MaxRetryError, requests.exceptions.ConnectionError","urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='get.geojs.io', port=443): Max retries exceeded with url: /v1/ip/geo.json (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fcd95312700>: Failed to establish a new connection: [Errno 111] Connection refused'))requests.exceptions.ConnectionError: HTTPSConnectionPool(host='get.geojs.io', port=443): Max retries exceeded with url: /v1/ip/geo.json (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fcd95312700>: Failed to establish a new connection: [Errno 111] Connection refused'))"
13230,b'Bert Loses Patience - Batch Inference Doubt',2021-08-24T00:54:20Z,2021-10-01T15:02:17Z,,,
13229,"b""Can't install transformers in Conda environment with python 3.9""",2021-08-24T00:03:22Z,2022-01-09T15:02:13Z,,conda.exceptions.UnsatisfiableError,"conda.exceptions.UnsatisfiableError: "
13228,b'Improve documentation of pooler_output in ModelOutput',2021-08-23T23:28:59Z,2021-08-30T12:08:16Z,,,
13227,b'make test failing',2021-08-23T21:02:48Z,2021-10-15T15:01:56Z,,,
13226,b'Bump notebook from 6.1.5 to 6.4.1 in /examples/research_projects/lxmert',2021-08-23T20:55:38Z,2021-08-24T13:52:39Z,dependencies,,
13225,b'Allow local_files_only for fast pretrained tokenizers',2021-08-23T17:20:37Z,2021-08-24T07:05:34Z,,,
13224,b'Add RemBert to AutoTokenizer',2021-08-23T17:15:22Z,2021-08-23T17:16:49Z,,,
13223,"b""Unable to load 'rembert' checkpoint """,2021-08-23T15:52:07Z,2021-08-24T07:49:03Z,,HTTPError,"HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/api/models/rembert"
13222,b'Add TFEncoderDecoderModel + Add cross-attention to some TF models',2021-08-23T12:40:17Z,2021-10-12T22:10:35Z,,,
13221,"b'Typo in M2M100 1.2B model card page, strange translation results and new M2M100 615M model'",2021-08-23T12:10:17Z,2021-10-04T15:06:37Z,,,
13220,b'[Tentative] Moving slow tokenizer to the Trie world.',2021-08-23T09:52:52Z,2021-09-09T15:26:16Z,,,
13219,"b'""Resource exhausted"" when loading Flax GPT-Neo 2.7B'",2021-08-22T15:12:49Z,,WIP,RuntimeError,"RuntimeError: Resource exhausted: Failed to allocate request for 100.00MiB (104857600B) on device ordinal 0"
13218,b'How to run GLUE tasks on my model?',2021-08-22T12:59:21Z,2021-09-29T15:02:13Z,,,
13217,b'Update clip loss calculation',2021-08-21T23:01:53Z,2021-09-02T06:45:57Z,,,
13216,b'Use DS callable API to allow hf_scheduler + ds_optimizer',2021-08-21T19:19:33Z,2021-08-30T17:01:07Z,DeepSpeed,,
13215,b'Input to a Tensorflow model where a dictionary cannot be used',2021-08-21T19:15:41Z,2021-10-24T15:02:51Z,,**ValueError,"**ValueError: Failed to find data adapter that can handle input: (<class 'list'> containing values of types {'(<class \'list\'> containing values of types {""<class \'tensorflow.python.framework.ops.EagerTensor\'>""})'}), (<class 'list'> containing values of types {""<class 'int'>""})**"
13214,b'\xe2\x9c\xa8 add citation file',2021-08-21T18:03:31Z,2021-08-30T11:46:55Z,,,
13213,b'Questions on generating using encoder-decoder models',2021-08-21T15:14:39Z,2021-08-23T13:16:42Z,,,
13212,b'fix: typo spelling grammar',2021-08-21T12:53:15Z,2021-08-30T12:09:15Z,,,
13211,b'correcting group beam search function output score bug',2021-08-21T04:08:58Z,2021-08-23T11:27:25Z,,,
13210,b'Add support for XLM-R XL and XXL models ',2021-08-21T02:41:24Z,2021-09-24T17:11:59Z,,,
13209,"b'fix `AutoModel.from_pretrained(..., torch_dtype=...)`'",2021-08-21T01:48:06Z,2021-08-24T09:43:42Z,,"TypeError, (note","TypeError: Object of type dtype is not JSON serializable(note: 2 separate issues were reported there but it looks like only this is the real issue, so linking to close it with this PR)"
13208,"b'Loading of a model takes much RAM, passing to CUDA doesn\xe2\x80\x99t free RAM'",2021-08-21T01:26:20Z,2021-11-07T15:02:27Z,,,
13207,b'Support for Training with BF16',2021-08-20T21:55:11Z,2021-12-01T02:00:48Z,WIP,,
13206,b'CausalLM vs HeadModel',2021-08-20T15:42:07Z,2021-09-29T15:02:16Z,,,
13205,b'Fixes #12941 where use_auth_token not been set up early enough',2021-08-20T14:37:38Z,2021-08-30T15:19:50Z,,,
13204,b'[Optimization] AdaFactor not working on TPU but works on GPU.',2021-08-20T13:20:04Z,2021-08-30T18:44:45Z,,,
13203,b'How do i get the CLS token from the model output?',2021-08-20T12:56:13Z,2021-08-20T14:40:28Z,,,
13202,b'-100 when calculating perplexity of a model..',2021-08-20T10:53:14Z,2021-09-27T15:02:20Z,,,
13201,"b'Train Bart model only use one cpu core, Any solutions to use more cores?'",2021-08-20T10:05:26Z,2021-09-27T15:02:21Z,,,
13200,b'Some tokenizers are not really picklable',2021-08-20T09:05:37Z,2021-09-19T15:22:00Z,,,
13199,b'How to use transformers for batch inference ',2021-08-20T07:41:02Z,2021-08-20T09:55:56Z,,tensorflow.python.framework.errors_impl.InvalidArgumentError,"tensorflow.python.framework.errors_impl.InvalidArgumentError: Value for attr Tindices of string is not in the list of allowed values: int32, int64"
13198,b'Correct wrong function signatures on the docs website',2021-08-20T07:24:10Z,2021-08-30T15:40:25Z,,,
13197,b'Training DetrForObjectDetection failed in a multiple-GPU environment.',2021-08-20T05:36:04Z,2021-09-29T15:02:17Z,,IndexError,"IndexError: Caught IndexError in replica 0 on device 0."
13196,b'check torch_dtype in config as well',2021-08-20T04:05:52Z,2021-08-25T23:27:02Z,,,
13195,"b""'torch_dtype' keyword not working with 'AutoModel'""",2021-08-20T04:00:04Z,2021-09-19T16:52:02Z,,,
13194,b'use float 16 in causal mask and masked bias',2021-08-20T02:56:05Z,2021-08-30T10:09:25Z,,,
13193,b'Megatron conversion code converts some weights in fp16 to fp32(or uint8).',2021-08-20T02:37:15Z,2021-08-30T10:09:25Z,,,
13192,b'Incosistent behaviour between fast and slow RoBERTa tokenizers',2021-08-20T02:06:56Z,2021-09-27T15:02:23Z,,,
13191,b'Why repeat initializing loss modules in every forward?',2021-08-20T01:27:58Z,2021-09-27T15:02:24Z,,,
13190,b'[Documentation] PLEASE HELP with very simple tasks!!!',2021-08-19T18:42:06Z,2021-09-27T15:02:25Z,,AttributeError,"AttributeError: 'BaseModelOutputWithPoolingAndCrossAttentions' object has no attribute 'logits'"
13189,b'Question about xla_spawn.py script and torch_xla.distributed.xla_multiprocessing',2021-08-19T18:22:02Z,2021-09-05T06:47:39Z,,,
13188,b'Fall back to `observed_batch_size` when the `dataloader` does not know the `batch_size`.',2021-08-19T17:34:07Z,2021-08-30T15:12:35Z,,TypeError,"TypeError: repeat(): argument 'repeats' (position 1) must be tuple of ints, not NoneType"
13187,"b""Unable to load model by ignoring size mismatch; TypeError: __init__() got an unexpected keyword argument 'ignore_mismatched_sizes'""",2021-08-19T16:00:23Z,2021-09-27T15:02:26Z,,"RuntimeError, TypeError","RuntimeError: Error(s) in loading state_dict for BertForSequenceClassification:TypeError: __init__() got an unexpected keyword argument 'ignore_mismatched_sizes'"
13186,b'Add SpeechEncoderDecoder & Speech2Text2',2021-08-19T15:09:23Z,2021-09-01T11:33:31Z,,,
13185,b'Adding CvT Model : Convolution based Image Transformers',2021-08-19T15:08:45Z,2022-03-07T15:05:26Z,,,
13184,b'Custom errors and BatchSizeError',2021-08-19T15:06:04Z,2021-08-24T13:01:02Z,,,
13183,b'Fix LUKE tests',2021-08-19T13:14:26Z,2021-08-23T07:41:36Z,,,
13182,b'T5TokenizerFast not reversible when text contains special tokens',2021-08-19T12:20:20Z,2021-10-01T15:02:20Z,,,
13181,b'SageMaker: Fix sagemaker DDP & metric logs',2021-08-19T12:02:27Z,2021-08-23T08:18:07Z,,,
13180,b'Conversion of Wav2vec2 model to TFWav2vec2 model',2021-08-19T09:33:41Z,2021-08-27T14:11:48Z,,,
13179,b'Correct order of overflowing_tokens for slow tokenizer',2021-08-19T08:33:28Z,2021-09-02T09:58:24Z,,,
13178,b'how to finetune based huggingface: run_glue.py',2021-08-19T06:51:17Z,2021-09-26T15:02:19Z,,,
13177,b'Bug of PyTorch group_beam_search function',2021-08-19T05:24:51Z,2021-08-23T11:27:24Z,,,
13176,b'GPT2 error when we try to run torch.jit.script ',2021-08-18T21:44:44Z,2021-09-20T15:48:28Z,,torch.jit.frontend.UnsupportedNodeError,"torch.jit.frontend.UnsupportedNodeError: GeneratorExp aren't supported:"
13175,b'GPT-Neo ONNX Inference with past is broken',2021-08-18T21:13:02Z,2021-10-16T15:02:04Z,,onnxruntime.capi.onnxruntime_pybind11_state.RuntimeException,"onnxruntime.capi.onnxruntime_pybind11_state.RuntimeException: [ONNXRuntimeError] : 6 : RUNTIME_EXCEPTION : Non-zero status code returned while running Reshape node. Name:'Reshape_501' Status Message: /onnxruntime_src/onnxruntime/core/providers/cpu/tensor/reshape_helper.h:42 onnxruntime::ReshapeHelper::ReshapeHelper(const onnxruntime::TensorShape&, std::vector<long int>&, bool) gsl::narrow_cast<int64_t>(input_shape.Size()) == size was false. The input tensor cannot be reshaped to the requested shape. Input shape:{1,1,1, 4096}, requested shape:{1,1,1,16,128}"
13174,b'[Benchmark]',2021-08-18T21:09:54Z,2021-08-19T09:58:39Z,,,
13173,b'enable mixed precision for Tensorflow training benchmarks',2021-08-18T20:42:42Z,2021-10-01T15:02:21Z,,,
13172,b'No module named: Regex while importing GPT2Tokenizer',2021-08-18T19:53:30Z,2021-08-19T10:26:16Z,,,
13171,b'[Docs] Function signatures on website not correctly reflecting current code.',2021-08-18T19:15:21Z,2021-08-30T17:18:33Z,,,
13170,b'Using `bf16` instead of `fp16`',2021-08-18T18:39:55Z,2021-12-01T02:00:47Z,WIP,,
13169,"b""RobertaTokenizerFast object has no attribute '_convert_token_to_id'""",2021-08-18T15:51:46Z,2021-09-26T15:02:20Z,,,
13168,b'Issue with `Speech2TextFeatureExtractor` method `from_pretrained` and `from_dict`',2021-08-18T15:34:55Z,2021-08-20T07:15:06Z,,AttributeError,"AttributeError: type object 'Speech2TextFeatureExtractor' has no attribute 'from_dict'"
13167,b'Update namespaces inside torch.utils.data to the latest.',2021-08-18T14:07:23Z,2021-08-19T12:29:52Z,,,
13166,b'[AutoFeatureExtractor] Fix loading of local folders if config.json exists',2021-08-18T13:52:59Z,2021-08-18T14:18:14Z,,,
13165,b' Performance issues in the program',2021-08-18T13:08:05Z,2021-09-26T15:02:21Z,,,
13164,b'Missing weight in pretrained model `pegasus-xsum`',2021-08-18T09:34:43Z,2021-08-18T14:17:52Z,,,
13163,b'is there any <SOS> or <EOS> token in reformer-enwik8?',2021-08-18T08:14:30Z,2021-10-24T15:02:54Z,,,
13162,b'Fine-tuned Robust Wav2Vec 2.0 models',2021-08-18T07:47:28Z,2021-09-10T09:53:07Z,New model,,
13161,b'Cannot run  run_mlm.py on a Japanese dataset - AttributeError: module transformers.models.mbart50 has no attribute BertJapaneseTokenizerFast',2021-08-18T07:41:28Z,2021-08-30T15:55:18Z,,AttributeError,"AttributeError: module transformers.models.mbart50 has no attribute BertJapaneseTokenizerFast"
13160,b'Advice needed: Adding more FSMT models',2021-08-18T06:26:00Z,,New model,,
13159,b'Fix load_tf_weights alias.',2021-08-18T04:47:40Z,2021-08-23T16:08:33Z,,,
13158,b'CvT: Convolution based Image Transformers',2021-08-18T04:42:34Z,,New model,,
13157,b'export BART model to ONNX failed with [Segmentation fault (core dumped)]',2021-08-18T03:49:39Z,2021-08-25T02:13:47Z,,,
13156,b'\xf0\x9f\x90\x9b: skip_special_tokens in tokenization_utils.py',2021-08-18T03:49:13Z,2021-09-26T15:02:23Z,,,
13155,b'Add FSNER example in research_projects',2021-08-17T20:01:11Z,2021-09-23T07:52:00Z,,,
13154,"b""AttributeError: 'AlbertModel' object has no attribute 'bias' -Transforms 4.9.2""",2021-08-17T18:48:42Z,2021-09-26T15:02:24Z,,AttributeError,"AttributeError: 'AlbertModel' object has no attribute 'bias'"
13153,b'Add Wav2Vec2 & Hubert ForSequenceClassification',2021-08-17T13:28:06Z,2021-08-27T17:52:51Z,,,
13152,b'Set missing seq_length variable when using inputs_embeds with ALBERT & Remove code duplication',2021-08-17T13:01:30Z,2021-08-31T10:51:25Z,,,
13151,b'Unhashable type : dict for visualbert example code.',2021-08-17T10:47:20Z,2021-08-17T17:30:53Z,,TypeError,"TypeError: unhashable type: 'dict'"
13150,b'examples: add keep_linebreaks option to CLM examples',2021-08-17T09:21:11Z,2021-08-27T09:35:45Z,,,
13149,b'Autoregressive differentiable decoding? (no teacher forcing nor self-reconstruction)',2021-08-17T08:31:49Z,2021-09-24T15:02:16Z,,,
13148,b'Slow tokenizers return overflowing tokens in reversed order',2021-08-17T08:21:54Z,2021-09-02T09:58:23Z,,,
13147,b'Support OpenNMT models',2021-08-17T07:55:48Z,,New model,,
13146,b'Runtime error when training DetForObjectDetection using HFTrainer with GPU.',2021-08-17T07:33:52Z,2021-08-31T10:34:32Z,,RuntimeError,"RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking arugment for argument x2 in method wrapper__cdist_forward)"
13145,b'remove unwanted control-flow code from DeBERTa-V2',2021-08-17T04:11:55Z,2021-08-23T16:07:42Z,,,
13144,b'[Benchmark]',2021-08-17T01:09:03Z,2021-08-18T14:12:26Z,,,
13143,"b""fix wrong 'cls' masking for bigbird qa model output""",2021-08-16T19:42:28Z,2021-09-01T12:03:17Z,,,
13142,b'Pretrain BART MNLI model on Financial Phrasebank',2021-08-16T17:32:28Z,2021-09-24T15:02:17Z,,,
13141,b'Implement a `batch_size` parameter in the `pipeline` object',2021-08-16T16:05:56Z,2021-11-08T16:35:52Z,,,
13140,b'Ci continue through smi failure',2021-08-16T15:38:05Z,2021-08-16T15:40:38Z,,,
13139,b'[WIP][Wav2Vec2] Fix Wav2Vec2 Pretraining',2021-08-16T13:39:23Z,2021-10-12T15:02:42Z,,,
13138,b'Fix classifier dropout in RobertaForMultipleChoice',2021-08-16T12:14:38Z,2021-09-23T15:01:59Z,,,
13137,b'how to finetune or test XLM-ProphetNet on XGLUE-NTG task',2021-08-16T09:37:02Z,2021-09-23T15:02:00Z,Migration,,
13136,b'Correct & simplify check_dummies regex',2021-08-16T05:57:22Z,2021-10-01T15:02:24Z,,,
13135,b'dtype',2021-08-15T22:24:37Z,2021-08-15T22:24:56Z,,,
13134,b'\xe2\x9c\xa8 Add PyTorch image classification example',2021-08-15T20:22:40Z,2021-09-02T19:29:42Z,,,
13133,b'[WIP] Add Few Shot Named Entity Recognition (FSNER) model',2021-08-15T14:32:58Z,2021-08-17T19:36:00Z,,,
13132,b'Fix the loss calculation of ProphetNet',2021-08-15T14:12:05Z,2021-08-20T09:01:54Z,,,
13131,b'[WIP] Add Few Shot Named Entity Recognition (FSNER) model',2021-08-15T13:18:47Z,2021-08-15T13:46:59Z,,,
13130,"b'[Flax] Add logging steps, eval steps, and save steps for hybrid CLIP example'",2021-08-15T07:45:27Z,2021-10-31T15:02:19Z,,,
13129,b'Fix classifier dropout in bertForMultipleChoice',2021-08-15T07:00:04Z,2021-08-16T08:17:37Z,,,
13128,b'Fix missing `seq_len` in `electra` model when `inputs_embeds` is used. ',2021-08-14T22:09:04Z,2021-08-16T16:36:08Z,,,
13127,b'RuntimeError: Error(s) in loading state_dict for BeitForImageClassification: \tsize mismatch for classifier.weight',2021-08-14T17:00:45Z,2021-09-22T15:02:04Z,,RuntimeError,"RuntimeError: Error(s) in loading state_dict for BeitForImageClassification:"
13126,b'torch.jit.trace quantized bigbird leads to 0INTERNAL ASSERT FAILED runtime error ',2021-08-14T02:18:55Z,2021-09-21T15:02:11Z,,RuntimeError,"RuntimeError: 0INTERNAL ASSERT FAILED at ""/pytorch/torch/csrc/jit/ir/alias_analysis.cpp"":532, please report a bug to PyTorch. We don't have an op for aten::constant_pad_nd but it isn't a special case.  Argument types: Tensor, int[], bool, "
13125,"b""type object 'AutoModelForSequenceClassification' has no attribute 'from_config'""",2021-08-14T01:31:32Z,2021-09-21T15:02:12Z,,,
13124,"b' You must login to the Hugging Face hub on this computer by typing `transformers-cli login` and entering your credentials to use `use_auth_token=True`. Alternatively, you can pass your own token as the `use_auth_token` argument in the translation notebook.'",2021-08-14T00:46:39Z,2021-08-17T00:35:58Z,,,
13123,b'Value error while running run_glue.py example with gpt2 ',2021-08-13T23:01:24Z,2021-09-26T15:02:26Z,,,
13122,"b""Electra raises UnboundLocalError: local variable 'seq_length' referenced before assignment when inputs are pre-computed embeddings""",2021-08-13T22:28:39Z,2021-09-21T15:02:13Z,,,
13121,"b""AutoModel KeyError: 'layoutlmv2'""",2021-08-13T18:04:33Z,2021-08-16T17:49:51Z,,,
13120,b'Deberta_v2 tf',2021-08-13T16:16:37Z,2021-08-31T10:32:47Z,,,
13119,b'Optimizes ByT5 tokenizer',2021-08-13T15:40:00Z,2021-08-17T08:11:58Z,,,
13118,"b""Fix frameworks table so it's alphabetical""",2021-08-13T15:35:36Z,2021-08-16T13:45:19Z,,,
13117,"b""Can we directly replace gpt2LMHeadModel with BertLMHeadModel to see bert's performance? #7""",2021-08-13T15:03:56Z,2021-09-21T15:02:14Z,,,
13116,b'Problem about using mBART50 for Russian to Chinese translation',2021-08-13T13:58:34Z,,WIP,,
13114,b'Migrating conversational pipeline tests to new testing format',2021-08-13T10:18:48Z,2021-08-26T07:50:45Z,,,
13113,b'Fix CircleCI nightly tests',2021-08-13T06:49:30Z,2021-08-13T06:57:30Z,,,
13115,b'typeerror: textinputsequence must be str',2021-08-13T06:08:53Z,2021-09-20T15:02:14Z,,TypeError,"TypeError: TextInputSequence must be str"
13112,b'modified roberta source code',2021-08-12T18:47:13Z,2021-08-12T18:50:21Z,,,
13111,b'`ModelError` when calling SageMaker Endpoint for prediction using the official notebooks',2021-08-12T18:39:07Z,2021-09-20T15:02:15Z,,ModelError,"ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (400) from model with message ""{"
13110,b'adding modified roberta',2021-08-12T18:28:54Z,2021-08-12T18:31:05Z,,,
13109,b'Fix flax gpt2 hidden states',2021-08-12T17:11:17Z,2021-08-13T08:45:54Z,,,
13108,b'Multi Lang Marian Translator not working (opus_mt_mul_en)',2021-08-12T16:58:55Z,2021-10-22T15:06:35Z,,,
13107,b'[WIP] Add TFSpeech2Text',2021-08-12T16:27:04Z,2021-09-09T00:11:34Z,,,
13106,b'Fix VisualBERT docs',2021-08-12T14:50:13Z,2021-08-13T06:14:04Z,,,
13105,b'TF/Numpy variants for all DataCollator classes',2021-08-12T14:31:13Z,2021-08-31T12:06:49Z,,,
13104,b'Fix VisualBERT docs',2021-08-12T14:30:18Z,2021-08-12T14:34:04Z,,,
13103,b'Ci last fix',2021-08-12T14:19:37Z,2021-08-12T14:45:06Z,,,
13102,b'inconsistency of the last element in hidden_states between PyTorch/Flax GPT2(Neo)',2021-08-12T14:17:02Z,2021-08-13T08:45:54Z,,,
13101,b'[To Show] Required changes for general multi-modal models',2021-08-12T12:35:23Z,2021-08-12T21:42:24Z,,,
13100,b'Rely on huggingface_hub for common tools',2021-08-12T12:28:24Z,2021-08-12T12:59:02Z,,,
13099,b'[FlaxCLIP] allow passing params to image and text feature methods',2021-08-12T12:09:08Z,2021-08-12T13:05:01Z,,,
13098,b'Fix Flax params dtype',2021-08-12T11:29:05Z,2021-11-11T09:15:20Z,WIP,,
13097,b'Reactive test fecthers on scheduled test with proper git install',2021-08-12T09:33:31Z,2021-08-12T09:38:14Z,,,
13096,b'Optimize Token Classification models for TPU',2021-08-12T08:25:23Z,2021-09-17T14:07:52Z,,,
13095,b'Memory accumulation when using hybrid clip script',2021-08-11T23:45:06Z,2021-09-22T15:02:06Z,,,
13094,b'Improve type checker performance',2021-08-11T22:04:57Z,2021-08-12T16:45:54Z,,,
13093,"b"" AttributeError: module 'sacrebleu' has no attribute 'DEFAULT_TOKENIZER'  in translation.ipynb notebook""",2021-08-11T18:50:31Z,2021-08-13T02:55:22Z,,,
13092,b'[Benchmark]',2021-08-11T16:16:33Z,2021-09-22T15:02:07Z,,,
13091,b'Install git',2021-08-11T15:44:35Z,2021-08-11T16:09:42Z,,,
13090,b'[Flax/JAX] Run jitted tests at every commit',2021-08-11T15:39:46Z,2021-08-12T12:49:46Z,,,
13089,"b'can""t connect ther online datasets.the issue:ConnectionError: Couldn\'t reach https://raw.githubusercontent.com/huggingface/datasets/1.7.0/datasets/glue/glue.py'",2021-08-11T15:30:20Z,2021-09-20T15:02:17Z,,,
13088,b'Doctests job',2021-08-11T15:13:12Z,2021-08-12T07:42:25Z,,,
13087,b'Fix classifier dropout in AlbertForMultipleChoice',2021-08-11T14:48:56Z,2021-08-12T07:37:32Z,,,
13086,b'Missing `lm_head` parameter in FlaxGPT2LMHeadModel.params',2021-08-11T14:36:31Z,2021-09-19T15:01:47Z,,,
13085,b'Proper import for unittest.mock.patch',2021-08-11T14:08:16Z,2021-08-12T09:23:01Z,,,
13084,b'\xe3\x82\x84\xe3\x81\x8b\xe3\x82\x89\xe3\x82\x93',2021-08-11T13:18:12Z,2021-09-19T15:01:48Z,,,
13082,b'I modified https://github.com/huggingface/transformers/blob/master/examples/language-modeling/run_language_modeling.py script few days ago for training electra from scratch. But there were some problems(maybe bugs) i had to solve for this task.',2021-08-11T12:02:15Z,2021-09-19T15:01:49Z,,,
13081,b'location is currently not available...please share the exact locationDetailed Explanation',2021-08-11T12:01:32Z,2021-09-19T15:01:49Z,,,
13080,b'[Vision Transformers] examples and pretraining',2021-08-11T11:53:55Z,2021-09-19T15:01:50Z,,,
13079,b'fix: keep separate hypothesis for different beam group',2021-08-11T11:12:24Z,2021-09-29T15:02:22Z,,,
13078,"b'[Doctest] Setup, quicktour and task_summary'",2021-08-11T08:57:24Z,2021-08-11T11:45:25Z,,,
13077,b'Add MultiBERTs conversion script',2021-08-11T07:58:54Z,2021-09-30T16:48:56Z,,,
13076,b'respect dtype of the the model when instiating not working',2021-08-11T05:45:41Z,2021-08-24T09:43:42Z,,TypeError,"TypeError: Object of type dtype is not JSON serializable"
13075,b'Custom Seq2Seq translation model training exits with error',2021-08-10T21:19:21Z,2021-09-29T15:02:24Z,,TypeError,"TypeError: forward() got an unexpected keyword argument 'encoder_hidden_states'"
13074,b'Change a parameter name in FlaxBartForConditionalGeneration.decode()',2021-08-10T19:56:24Z,2021-08-12T12:19:48Z,,,
13073,b't5 base not found.',2021-08-10T17:45:26Z,2021-09-20T15:02:20Z,,,
13072,"b""Revert to all tests whil we debug what's wrong""",2021-08-10T16:35:42Z,2021-08-10T16:37:01Z,,,
13071,b'Fix fallback of test_fetcher',2021-08-10T14:13:50Z,2021-08-10T14:17:06Z,,,
13070,b'top-k sampling for Flax models',2021-08-10T14:10:49Z,2021-09-17T15:01:59Z,,,
13069,b'MultiBerts in Huggingface',2021-08-10T13:14:59Z,2021-09-30T16:48:56Z,New model,,
13068,b'Can not instantiate `PreTrainedTokenizerFast` from instantiated tokenizer object',2021-08-10T13:02:12Z,2021-08-12T12:41:22Z,,,
13067,b'Fix ModelOutput instantiation form dictionaries',2021-08-10T09:44:58Z,2021-08-10T10:20:04Z,,,
13066,b'Model output dict',2021-08-10T09:40:01Z,2021-08-10T09:40:31Z,,,
13065,b'[WIP] Add Japanese RoBERTa Model',2021-08-10T08:08:55Z,2021-11-01T13:32:42Z,,,
13064,b'How to extract the encoded data of feed & forward layer in tfbertmodel?',2021-08-10T06:40:56Z,2021-10-07T15:06:14Z,,,
13063,b'[WIP] Correct wav2vec2 flax',2021-08-09T23:49:36Z,2021-08-17T15:42:10Z,,,
13062,"b""Cannot import name 'BEiTForImageClassification' from 'transformers' """,2021-08-09T22:36:49Z,2021-08-11T02:45:03Z,,,
13061,b'Fix small typo in M2M100 doc',2021-08-09T17:01:55Z,2021-08-09T17:17:06Z,,,
13060,b'Exporting Fine tuned T5ForConditionalGeneration model to TF-Serving using ONNX',2021-08-09T16:48:02Z,2021-10-05T10:07:07Z,,,
13059,b'RAG: building my own dataset',2021-08-09T16:43:33Z,2021-09-18T15:01:47Z,,,
13058,b'Non-English characters not fully supported by GPT-2 HF model',2021-08-09T16:32:46Z,2021-09-17T15:02:02Z,,,
13057,b'Use original key for label in DataCollatorForTokenClassification',2021-08-09T16:30:23Z,2021-08-10T16:39:48Z,,,
13056,"b'Change how ""additional_special_tokens"" argument in the "".from_pretrained"" method of the tokenizer is taken into account'",2021-08-09T16:26:33Z,2021-08-23T12:35:19Z,,,
13055,b'Roll out the test fetcher on push tests',2021-08-09T14:58:53Z,2021-08-10T12:54:52Z,,,
13054,b'Is there any convenient way to train a transformer from scratch ?',2021-08-09T14:51:44Z,2021-08-11T14:32:54Z,,,
13053,b'Is there any way to train a transformer model from scrat',2021-08-09T14:44:43Z,2021-08-09T14:45:35Z,,,
13052,b'Fix omitted lazy import for xlm-prophetnet',2021-08-09T12:09:53Z,2021-08-13T10:24:53Z,,,
13051,b'[Feature Processing Sequence] Remove duplicated code',2021-08-09T11:27:31Z,2021-08-16T14:02:35Z,,,
13050,b'docs: add HuggingArtists to community notebooks',2021-08-09T10:27:53Z,2021-08-10T07:36:45Z,,,
13049,b'Add MBART to models exportable with ONNX',2021-08-09T10:21:42Z,2021-08-09T12:56:05Z,,,
13048,b'Add to ONNX docs',2021-08-09T10:05:54Z,2021-08-09T13:51:49Z,,,
13047,b'How do i pre-train Bert_mlm model [Discussion]',2021-08-09T09:55:38Z,2021-08-11T09:07:39Z,,,
13046,b'TFBertPreTrainingLoss has something wrong',2021-08-09T09:13:17Z,2021-09-17T15:02:03Z,,,
13045,b'Add FNet',2021-08-08T20:38:33Z,2021-09-20T11:24:31Z,,**Note**,"**Note**: I am trying to make this model as similar to Bert is possible. The original implementation has slightly different layers. For example, `FNetIntermediate` and `FNetOutput` equivalents are combined into a single layer in original FNet code, but I keep them separate. Hope this is okay?"
13044,b'MLM example not able to run_mlm_flax.py',2021-08-08T20:19:59Z,2021-08-10T20:33:27Z,,,
13043,b'[DeepSpeed] DeepSpeed 0.4.4 does not run with Wav2Vec2 pretraining script',2021-08-08T15:05:22Z,2021-09-14T15:12:22Z,,"/usr/include/c++/10/chrono, /usr/include/c++/10/chrono:473:154, /usr/include/c++/10/chrono:428:27, subprocess.CalledProcessError, RuntimeError, ImportError, AttributeError","/usr/include/c++/10/chrono: In substitution of template<class _Rep, class _Period> template<class _Period2> using __is_harmonic = std::__bool_constant<(std::ratio<((_Period2::num / std::chrono::duration<_Rep, _Period>::_S_gcd(_Period2::num, _Period::num)) * (_Period::den / std::chrono::duration<_Rep, _Pe/usr/include/c++/10/chrono:473:154:   required from here                                                                                                                                                                                                                                                          /usr/include/c++/10/chrono:428:27: internal compiler error: Segmentation fault                                                                                                                                                                                                                                    subprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.                                                                                                                                                                                                                         RuntimeError: Error building extension 'cpu_adam' ImportError: No module named 'cpu_adam'AttributeError: 'DeepSpeedCPUAdam' object has no attribute 'ds_opt_adam'"
13042,b'Squad bert',2021-08-08T14:04:06Z,2021-08-08T14:13:43Z,,,
13041,b'Script to convert the bart model from pytorch checkpoint to tensorflow checkpoint',2021-08-08T12:34:46Z,2021-08-09T07:00:02Z,,,
13040,b'Add try-except for torch_scatter',2021-08-08T10:18:04Z,2021-08-10T07:29:35Z,,,
13039,b'Remove usage of local variables related with model parallel and move \xe2\x80\xa6',2021-08-08T08:10:08Z,2021-08-09T23:16:06Z,,,
13038,b'Check in PreTrainedTokenizer can cause incorrect tokenization',2021-08-08T06:01:39Z,2021-09-20T15:02:24Z,,,
13037,b'Spanish NER bad extraction',2021-08-08T00:44:54Z,2021-08-10T19:48:24Z,,,
13036,b'Do the Trainer docs need an update?',2021-08-07T21:43:58Z,2021-09-16T15:01:51Z,,,
13035,b'Rotate checkpoint `shutil.rmtree(checkpoint)` fails',2021-08-07T05:46:03Z,2021-08-11T08:29:21Z,,OSError,"OSError: [Errno 39] Directory not empty: '/en-google_mt5-xl-1e-5-1234/checkpoint-320'"
13034,b'transformers-cli depends on torchaudio optional deps',2021-08-06T23:52:38Z,2021-08-09T17:00:17Z,,OSError,"OSError: sndfile library not found"
13033,"b'Getting near constant training loss, T5 not learning anything?'",2021-08-06T20:13:38Z,2021-08-10T13:47:55Z,,,
13032,b'Masked word prediction in new language with mBERT/XLM',2021-08-06T19:51:32Z,2021-10-23T15:02:18Z,,,
13031,b'How can I convert a `checkpoint.pth` (a model trained with pytorch-pretrained-bert) to huggingface model with `config.json` and `pytorch_model.bin` file?',2021-08-06T18:37:07Z,2021-09-14T15:02:03Z,Migration,,
13030,b'Tpu tie weights',2021-08-06T15:43:09Z,2021-08-06T18:41:39Z,,,
13029,b'supporting t5 for question answering',2021-08-06T13:55:36Z,2021-10-23T15:02:19Z,,,
13028,b'Fix ONNX test: Put smaller ALBERT model',2021-08-06T12:44:45Z,2021-08-06T16:41:34Z,,,
13027,b'Get multiple results from Hugging face pipeline library',2021-08-06T12:42:49Z,2021-08-07T17:50:59Z,,,
13026,b'Update model configs - Allow setters for common properties ',2021-08-06T09:37:03Z,2021-09-06T14:30:13Z,,,
13025,b'MT5-large model on hub has wrong config',2021-08-06T09:00:40Z,2021-08-15T13:57:36Z,,,
13024,b'[Flax] Refactor gpt2 & bert example docs',2021-08-06T08:43:16Z,2021-08-09T11:37:50Z,,,
13023,b'Disentangle auto modules from other modeling files',2021-08-06T07:18:30Z,2021-08-06T11:12:30Z,,,
13022,b'GPT-J-6B',2021-08-06T05:18:32Z,2021-08-31T15:53:02Z,,,
13021,"b""TypeError: __init__() got an unexpected keyword argument 'save_strategy'""",2021-08-06T02:36:02Z,2021-09-13T15:02:10Z,,,
13020,b'RobertaForMaskedLM loss calculated wrong(?)',2021-08-05T23:18:32Z,2021-09-13T15:02:11Z,,,
13019,b'GPU Out of Memory when repeatedly running large models (`hyperparameter_search`)',2021-08-05T18:05:26Z,2021-09-13T15:02:12Z,,RuntimeError,"RuntimeError: Caught RuntimeError in replica 0 on device 0."
13018,b'Unable to resume training from checkpoint on TPU v3-8',2021-08-05T14:59:12Z,2021-08-06T23:11:31Z,,ValueError,"ValueError: loaded state dict contains a parameter group that doesn't match the size of optimizer's group"
13017,b'Fix VisualBert Embeddings',2021-08-05T14:50:30Z,2021-08-12T07:57:34Z,,,
13016,b'FX submodule naming fix',2021-08-05T13:55:34Z,2021-08-06T13:37:29Z,,,
13015,b'Fix TYPE_CHECKING not imported',2021-08-05T13:46:38Z,2021-08-09T11:55:51Z,,,
13014,b'T5 with past ONNX export',2021-08-05T12:33:33Z,2021-08-06T13:46:26Z,,,
13013,b'Update generate method - Fix floor_divide warning',2021-08-05T12:04:57Z,2021-08-05T13:55:14Z,,,
13012,b'[Flax T5] Speed up t5 training',2021-08-05T10:45:57Z,2021-08-06T09:21:38Z,,,
13011,b'The traced Encoder of LEDForConditionalGeneration does not allow dynamic batching',2021-08-05T09:11:56Z,2021-09-12T15:02:07Z,,RuntimeError,"RuntimeError: The following operation failed in the TorchScript interpreter."
13010,b'GPT-J',2021-08-05T08:40:34Z,2021-08-06T06:41:03Z,,,
13009,b'Problem saving tf wav2vec in savedmodel format',2021-08-05T07:29:29Z,2021-09-20T15:02:26Z,,,
13008,b'[Flax Encoder Decoder] Make Flax GPT2 working with cross attention',2021-08-05T07:05:19Z,2021-08-23T15:57:30Z,,,
13007,b'Importing hides underlying error',2021-08-04T22:12:29Z,2021-09-12T15:02:09Z,,"ImportError, AttributeError","ImportError: cannot import name 'AutoModelForCausalLM' from 'transformers' (venv/lib/python3.7/site-packages/transformers/__init__.py)AttributeError: 'NoneType' object has no attribute 'origin'"
13006,b'[Flax] Correct pt to flax conversion if from base to head',2021-08-04T21:13:17Z,2021-08-05T16:38:50Z,,,
13005,b'HyperParameter search in sagemaker',2021-08-04T21:10:49Z,2021-08-05T18:03:06Z,,TypeError,"TypeError: can't pickle _thread.RLock objects"
13004,b'Create perplexity.rst',2021-08-04T19:59:07Z,2021-08-05T06:56:13Z,,,
13003,b'Not getting the same results with run_qa and run_qa_no_trainer scripts',2021-08-04T19:53:27Z,2021-08-04T20:25:33Z,,,
13002,b'TF CLM example fix typo',2021-08-04T18:00:28Z,2021-08-31T12:21:39Z,,,
13001,b'VisualBERT - ModuleAttributeError',2021-08-04T14:00:27Z,2021-08-12T08:10:04Z,,torch.nn.modules.module.ModuleAttributeError,"torch.nn.modules.module.ModuleAttributeError: 'VisualBertEmbeddings' object has no attribute 'input_embeds"
13000,b'Newly trained tokenizers not adding [CLS] and [SEP] tokens',2021-08-04T12:04:40Z,2021-08-04T22:58:43Z,,ValueError,"ValueError: 2 is not in list"
12999,b'pad_to_multiple_of added to DataCollatorForWholeWordMask',2021-08-04T11:57:06Z,2021-08-04T13:49:21Z,,,
12998,b'DataCollatorForWholeWordMask does not return attention_mask',2021-08-04T11:06:27Z,2021-08-04T13:54:11Z,,,
12997,b'how to user class_weight in transformers.trainer',2021-08-04T10:15:48Z,2021-08-04T11:40:52Z,,,
12996,b'Perceiver IO',2021-08-04T03:43:43Z,2021-12-08T13:20:35Z,New model,,
12995,b'Option for `(Distributed)LengthGroupedSampler` to treat groups as a hard constraint',2021-08-03T19:02:42Z,2021-09-20T15:02:28Z,,,
12994,b'Add BEiT',2021-08-03T15:32:37Z,2021-08-04T16:29:23Z,,,
12993,b'Gloabl attention not recognised in longformer pretrained MLM model to get sentence vector?',2021-08-03T15:02:36Z,2021-09-11T15:02:03Z,,,
12992,b'I met an error when I use EncoderDecoderModel.',2021-08-03T14:45:28Z,2021-08-04T03:16:45Z,,RuntimeError,"RuntimeError: mat1 dim 1 must match mat2 dim 0"
12991,b'How is Bert fine-tuned on STS-B task?',2021-08-03T14:05:12Z,2021-08-04T07:27:36Z,,,
12990,"b'kindly adding some documentations on t5-v1_1-base""""'",2021-08-03T12:31:33Z,2021-09-02T15:23:45Z,,,
12989,b'Training hangs at the very start while using deepspeed',2021-08-03T12:09:30Z,2021-10-25T15:07:02Z,DeepSpeed,,
12988,b'[Flax] Correctly Add MT5',2021-08-03T12:02:25Z,2021-08-04T14:03:14Z,,,
12987,b'[Flax] Align jax flax device name',2021-08-03T11:31:24Z,2021-08-04T14:00:09Z,,,
12986,b'pylint error when using `transformers.AutoModelForSequenceClassification.from_pretrained(path)`',2021-08-03T11:13:26Z,2021-08-04T10:49:03Z,,,
12985,b'The transferred onnx model is much bigger than the origin pytorch model',2021-08-03T10:50:08Z,2021-09-11T15:02:04Z,,,
12984,b'convert_graph_to_onnx.convert broken for gpt-neo-x.xB since 4.5.0.dev0',2021-08-03T10:28:23Z,2021-09-11T15:02:05Z,,RuntimeError,"RuntimeError: ONNX export failed: Couldn't export operator aten::unfold"
12983,b'subclassing a torch.utils.data.Dataset object for a T5 model',2021-08-03T09:41:32Z,2021-08-04T09:05:06Z,,,
12982,"b""Fine-Tune Wav2Vec2 for English ASR with \xf0\x9f\xa4\x97 Transformers, loading fine-tune models from local isn't working""",2021-08-03T03:23:27Z,2021-08-04T03:13:56Z,,,
12981,b'fix `Trainer.train(resume_from_checkpoint=False)` is causing an exception',2021-08-02T16:23:41Z,2021-08-03T08:10:34Z,,,
12980,b'tapas-base model is not predicting answers well.',2021-08-02T15:05:40Z,2021-09-11T15:02:06Z,,,
12979,b'Documentation: Dataset to Model interface examples',2021-08-02T14:30:32Z,2021-09-10T15:02:16Z,,,
12978,b'Validation and Evaluation not cumputed in run_qa.py',2021-08-02T11:43:32Z,2021-09-10T15:02:17Z,,,
12977,b'Control sequence length for Token Classification with Trainer',2021-08-02T09:12:00Z,2021-08-03T07:50:13Z,,,
12976,b'Fix template for inputs docstrings',2021-08-02T06:56:46Z,2021-08-03T06:28:25Z,,,
12975,b'Place BigBirdTokenizer in sentencepiece-only objects',2021-08-02T06:01:49Z,2021-08-02T06:26:38Z,,,
12974,b'fix typo in example/text-classification README',2021-08-01T20:52:53Z,2021-08-02T10:58:43Z,,,
12973,b'Add retrieval model config',2021-08-01T18:04:28Z,2021-08-01T18:04:44Z,,,
12972,b'Deberta tf',2021-08-01T12:03:21Z,2021-08-12T09:01:26Z,,,
12971,b'[FLAX] Potential bug in CLM script when using text files',2021-08-01T09:35:10Z,2021-08-31T15:33:08Z,,,
12970,b'`Trainer.train(resume_from_checkpoint=False)` is causing an exception',2021-07-31T19:07:31Z,2021-08-03T08:10:36Z,,,
12969,b'Add tokenizer method to convert ids to tokens',2021-07-31T19:05:59Z,2021-08-01T09:06:38Z,,,
12968,b'403 error in colab to download tokenizer',2021-07-31T18:50:12Z,2021-08-02T06:04:34Z,,HTTPError,"HTTPError: HTTP Error 403: rate limit exceeded"
12967,b'Unable to convert output to interpretable format',2021-07-31T18:46:13Z,2021-08-01T09:06:49Z,,,
12966,b'Workaround for training models with really big text files',2021-07-31T13:45:02Z,2021-08-03T12:59:50Z,,,
12965,b'Bugs when fine tuning the gpt2',2021-07-31T07:44:03Z,2021-08-31T08:20:21Z,,,
12964,b'Using `model.sample()` and increasing the `max_length` leads to CUDA OOM crash',2021-07-31T07:25:52Z,2021-09-08T15:02:03Z,,,
12963,b'Prevent `Trainer.evaluate()` crash when using only tensorboardX',2021-07-31T02:53:48Z,2021-08-01T06:35:47Z,,,
12962,b'`Trainer.evaluate()` crashes when using only tensorboardX',2021-07-31T02:53:15Z,2021-08-01T06:35:47Z,,AttributeError,"AttributeError: 'NoneType' object has no attribute 'log_metric'"
12961,b'Use min version for huggingface-hub dependency',2021-07-31T01:12:57Z,2021-08-08T14:06:06Z,,,
12960,b'[Very WIP] Migrating ALL pipelines to new testing + fixes',2021-07-30T20:25:42Z,2021-09-13T14:24:39Z,,,
12959,b'huggingface-hub version conflict',2021-07-30T19:18:36Z,2021-09-08T15:02:04Z,,,
12958,b'Weird behavior with mBART-50 and Spanish',2021-07-30T18:43:24Z,,WIP,,
12957,"b'404 Error when loading pretrained model, after finetuning'",2021-07-30T18:16:15Z,2021-08-02T10:27:41Z,,"HTTPError, OSError","HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/textgen_models%5Ctextgen_model_shuffle_e10/resolve/main/config.jsonOSError: Can't load config for 'textgen_models\textgen_model_shuffle_e10'. Make sure that:"
12956,b'Wav2Vec2 WER remains 1.00 and return blank transcriptions.',2021-07-30T17:37:09Z,2021-09-08T15:02:05Z,,,
12955,b'Add splinter',2021-07-30T16:03:58Z,2021-08-17T12:29:02Z,,,
12954,b'Fix typo in example of DPRReader',2021-07-30T15:05:04Z,2021-08-02T06:08:57Z,,,
12953,b'Fix division by zero in NotebookProgressPar',2021-07-30T13:18:50Z,2021-07-30T13:31:29Z,,,
12952,b'Add multilingual documentation support',2021-07-30T11:46:36Z,2021-07-30T12:56:14Z,,,
12951,b'Add substep end callback method',2021-07-30T11:14:56Z,2021-07-30T12:20:39Z,,,
12950,b'ZeroDivisionError in NotebookProgressBar.update with small dataset',2021-07-30T09:19:52Z,2021-07-30T13:31:29Z,,,
12949,b'[end2end rag] Slow speed when extending the external KB',2021-07-30T09:13:39Z,2021-09-06T15:01:42Z,,,
12948,b'BertForQuestionAnswering result not match when multiple run in same input',2021-07-30T09:12:32Z,2021-07-30T14:43:30Z,,,
12947,b'[FLAX] Minor fixes in LM example',2021-07-30T09:05:25Z,2021-07-30T16:27:53Z,,,
12946,"b""ImportError: cannot import name 'BigBirdTokenizer' from 'transformers'""",2021-07-30T07:53:52Z,2021-08-02T06:34:18Z,,,
12945,b'Transformers tokenizer pickling issue using hydra and submitit_slurm',2021-07-29T18:23:21Z,2021-09-06T15:01:43Z,,TypeError,"TypeError: cannot pickle '_LazyModule' object"
12944,b'rum_mlm crashes with bookcorpus and --preprocessing_num_workers',2021-07-29T17:42:45Z,2021-09-06T15:01:44Z,,`BrokenPipeError,"`BrokenPipeError: [Errno 32] Broken pipe`"
12943,b'Moving fill-mask pipeline to new testing scheme',2021-07-29T17:39:47Z,2021-08-13T10:04:18Z,,,
12942,b'trainer is not reproducible ',2021-07-29T15:46:39Z,2021-09-06T15:01:45Z,,,
12941,"b""OSError: Can't load config for 'bert-base-uncased""",2021-07-29T15:27:27Z,2021-08-30T15:19:50Z,,"HTTPError, OSError","HTTPError: 403 Client Error: Forbidden for url: https://huggingface.co/bert-base-uncased/resolve/main/config.jsonOSError: Can't load config for 'bert-base-uncased'. Make sure that:"
12940,"b'Starting today, I get an error downloading pre-trained models'",2021-07-29T15:18:19Z,2021-07-29T23:13:14Z,,OSError,"OSError: Can't load config for 'bert-base-uncased'. Make sure that:"
12939,b'Fix from_pretrained with corrupted state_dict',2021-07-29T15:13:24Z,2021-08-04T09:48:40Z,,,
12938,b'Add CpmTokenizerFast',2021-07-29T14:03:59Z,2021-07-29T19:05:16Z,,,
12937,b'Not able use TF Dataset on TPU when created via generator in Summarization example',2021-07-29T10:56:37Z,2021-10-01T15:02:27Z,,"UnavailableError, NotFoundError","UnavailableError: 9 root error(s) found.NotFoundError: Op type not registered 'XlaSetDynamicDimensionSize' in binary running on n-f62ff7a1-w-0. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed."
12936,b'`PretrainedTokenizer.return_special_tokens` returns incorrect mask',2021-07-29T06:06:03Z,2022-01-06T09:58:08Z,,,
12935,b'Better error message? `CUDA error: CUBLAS_STATUS_ALLOC_FAILED`',2021-07-29T04:26:47Z,2021-07-29T13:45:45Z,,"RuntimeError, IndexError","RuntimeError: CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`IndexError: index out of range in self"
12934,b'[Wav2vec Pretrain] KeyError: \xe2\x80\x98attention_mask\xe2\x80\x99',2021-07-29T01:30:36Z,2021-07-30T01:49:45Z,,KeyError,"KeyError: 'attention_mask'"
12933,b'ONNX v2 raises an Exception when using PyTorch < 1.8.0 ',2021-07-28T21:48:13Z,2021-07-29T16:02:29Z,,,
12932,b'Error when trying `push_to_hub` for a fine-tuned model on Colab',2021-07-28T19:01:29Z,2021-07-28T19:58:13Z,,ValueError,"ValueError: If not specifying `clone_from`, you need to pass Repository a valid git clone."
12931,b'How to fuse copy mechnism into the GenerationMixin\xef\xbc\x9f',2021-07-28T16:31:55Z,2021-09-06T15:01:46Z,,,
12930,b'Print defaults when using --help for scripts',2021-07-28T14:39:34Z,2021-07-28T15:37:21Z,,,
12929,b'Add option to set max_len in run_ner',2021-07-28T13:29:32Z,2021-07-28T13:38:12Z,,,
12928,b'Fix QA examples for roberta tokenizer',2021-07-28T13:18:57Z,2021-07-28T13:47:49Z,,,
12927,b'Add missing @classmethod decorators',2021-07-28T12:49:14Z,2021-07-28T17:01:38Z,,,
12926,b'Misleading warning when using DPRContextEncoderTokenizer',2021-07-28T08:26:59Z,2021-09-05T15:01:58Z,,,
12925,b'How to reproduce XLNet correctly And What is the config for finetuning XLNet?',2021-07-28T01:16:19Z,2021-07-29T05:50:07Z,Migration,,
12924,b'Feature request: Show command line argument defaults',2021-07-28T00:31:59Z,2021-07-28T15:37:21Z,,,
12923,b'Transformers onnx export error',2021-07-27T23:31:40Z,2021-07-30T07:35:44Z,"External, Should Fix",AttributeError,"AttributeError: 'dict' object has no attribute 'size'"
12922,b'GPT2 Layers',2021-07-27T20:51:42Z,2021-07-27T21:09:39Z,,,
12921,"b""LEDForSequenceClassification and LEDForQuestionAnswering example codes don't work.""",2021-07-27T20:39:45Z,2021-09-06T15:01:47Z,,,
12920,b'Add callback method for substeps during gradient accumulation.',2021-07-27T18:20:36Z,2021-07-30T17:51:54Z,,,
12919,b'Fix typo in the example of MobileBertForPreTraining',2021-07-27T16:57:49Z,2021-07-28T11:45:30Z,,,
12918,b'Fix StoppingCriteria ABC signature',2021-07-27T16:20:52Z,2021-07-28T16:47:16Z,,,
12917,b'Tokenizer from tokenizers library cannot be used in Trainer',2021-07-27T15:42:58Z,2021-07-27T21:09:13Z,,,
12916,b'fill-mask pipeline with tables (TapasForMaskedLM) fails DataFrame type assertion',2021-07-27T15:40:36Z,2021-09-06T13:51:49Z,,,
12915,b'saved checkpoint for best model and last model needs to be different ',2021-07-27T14:46:34Z,2021-07-27T15:05:01Z,,,
12914,b'[FLAX] Minor fixes in CLM example',2021-07-27T14:00:15Z,2021-07-27T14:18:04Z,,,
12913,b'Add truncation_side option to tokenizers',2021-07-27T13:46:29Z,2022-01-20T08:11:35Z,WIP,,
12912,b'memory crash with large dataset',2021-07-27T13:28:24Z,2021-09-03T15:07:04Z,,,
12911,b'GPT-Neo ONNX export',2021-07-27T12:55:48Z,2021-08-05T08:12:13Z,,,
12910,b'fix distiller.py',2021-07-27T12:28:46Z,2021-07-28T18:11:39Z,,,
12909,b'Truncating the prefix of a sequence rather than the suffix',2021-07-27T10:59:55Z,,WIP,,
12908,b'Training Transformer XL from scratch for CLM',2021-07-27T10:30:59Z,2021-09-03T15:07:06Z,,KeyError,"KeyError: 'loss'"
12907,"b""Can't set attention_probs_dropout_prob in LEDConfig""",2021-07-27T09:08:01Z,2021-09-03T15:07:07Z,,,
12906,b'AttributeError in BERT-Tokenizer',2021-07-27T08:41:53Z,2021-09-03T15:07:08Z,,AttributeError,"AttributeError: 'BasicTokenizer' object has no attribute 'strip_accents'"
12905,"b""The Unsupervised denoising training example in T5's doc""",2021-07-27T08:13:23Z,2021-07-28T11:45:54Z,,,
12904,b'transformers.__spec__ returning None. Causing downstream import errors',2021-07-27T07:44:27Z,2021-07-27T13:50:51Z,,ValueError,"ValueError: transformers.__spec__ is None"
12903,"b""ValueError: Outputs values doesn't match between reference model and ONNX exported model""",2021-07-27T07:37:16Z,2021-09-03T15:07:09Z,,,
12902,b'pipeline does not load a (local) model',2021-07-27T07:22:07Z,2021-09-03T15:07:10Z,,ValueError,"ValueError: unable to parse C:\Users\me\mymodel\modelcard.json as a URL or as a local path"
12901,b'Update generation_logits_process.py',2021-07-27T03:02:59Z,2021-07-28T18:16:35Z,,,
12900,b'Update generation_logits_process.py',2021-07-27T01:49:20Z,2021-07-28T18:17:20Z,,,
12899,b'`Seq2SeqTrainer` set max_length and num_beams only when non None ',2021-07-27T01:31:25Z,2021-07-27T12:37:46Z,,,
12898,b'Tensorflow Mixed Precision Training',2021-07-27T01:12:03Z,2021-09-03T15:07:11Z,,,
12897,b'Correct validation_split_percentage argument from int (ex:5) to float (0.05)',2021-07-27T00:30:08Z,2021-07-28T01:01:41Z,,,
12896,b'Update tokenization_auto.py',2021-07-26T23:13:11Z,2021-07-28T18:17:57Z,,,
12895,b'Fix push_to_hub for TPUs',2021-07-26T21:10:13Z,2021-07-26T21:10:34Z,,,
12894,b'tokenizers add_token bug',2021-07-26T20:47:23Z,2021-09-06T15:01:48Z,,,
12893,b'Create py.typed',2021-07-26T17:32:23Z,2021-08-13T08:12:59Z,,,
12892,b'CANINE pre-training',2021-07-26T16:28:48Z,2021-09-03T15:07:12Z,,,
12891,b'Fix docstring typo in tokenization_auto.py',2021-07-26T15:25:59Z,2021-07-28T18:19:34Z,,,
12890,b'Multi-GPU fails ',2021-07-26T14:34:04Z,2021-09-03T15:07:13Z,,,
12889,b'Fix documentation of BigBird tokenizer',2021-07-26T13:55:36Z,2021-07-26T14:11:25Z,,,
12888,b'Add accelerate to examples requirements',2021-07-26T13:52:49Z,2021-07-26T13:57:34Z,,,
12887,b'Add config option to skip 1-D position embeddings in LayoutLM',2021-07-26T10:29:44Z,2021-08-25T16:19:33Z,,,
12886,b'Object detection pipeline',2021-07-26T08:44:23Z,2021-09-08T15:17:32Z,Core: Pipeline,,
12885,"b""an unexpected keyword argument 'output_signature'""",2021-07-26T08:31:34Z,2021-07-26T10:27:02Z,,TypeError,"TypeError: from_generator() got an unexpected keyword argument 'output_signature'"
12884,b'Super slow ByT5 Tokenizer',2021-07-26T07:09:04Z,2021-08-17T08:11:58Z,,,
12883,b'Distributed TPU training with run_mlm duplicate data ',2021-07-26T01:53:59Z,2021-09-03T15:07:14Z,,,
12882,b'loss sudden increase',2021-07-26T01:05:20Z,2021-07-26T10:58:56Z,,,
12881,b'Tensorflow GPT-2 model incapable of freezing layers',2021-07-25T21:48:08Z,2021-09-03T15:07:15Z,,,
12880,b'RoBERTa: Truncation error: Sequence to truncate too short to respect the provided max_length',2021-07-25T17:38:24Z,2021-08-01T19:08:52Z,,Exception,"Exception: Truncation error: Sequence to truncate too short to respect the provided max_length"
12879,b'Feature Request: Add support for --do_train/eval/predict arguments in the TF examples script for token classification',2021-07-25T17:19:49Z,2021-10-01T15:02:28Z,,,
12878,b'Trainer accumulates logits',2021-07-25T13:39:30Z,2021-07-28T14:22:37Z,,,
12877,b'run_mlm.py errors when running validation only',2021-07-25T07:12:32Z,2021-07-26T22:51:15Z,,,
12876,b'New transformers.onnx CLI does not support ONNX quantization',2021-07-24T20:53:46Z,2021-07-27T21:48:13Z,,,
12875,b'Model card updated/deleted',2021-07-24T20:47:47Z,2021-07-25T07:21:19Z,,,
12874,b'Finetuning GPT-2 on small datasets',2021-07-24T19:48:42Z,2021-09-02T15:06:00Z,,,
12873,b'Possibly wrong API documentation for BigBirdTokenizerFast ',2021-07-24T18:59:27Z,2021-07-26T14:11:25Z,,,
12872,b'Allow multilabel classification mode for widgets in the models repo',2021-07-24T16:15:48Z,2021-07-24T17:15:59Z,,,
12870,b'Bart Generation',2021-07-24T16:13:03Z,2021-09-10T15:02:19Z,,,
12869,"b""I donnot want print trainer's logging info""",2021-07-24T07:20:17Z,2021-07-27T01:17:22Z,,,
12868,"b""MT5-base tokenizer can't decode to target language after decoding""",2021-07-24T06:59:06Z,2021-08-09T18:53:53Z,,,
12867,b'Possible bug in spm-based tokenizers',2021-07-23T18:18:14Z,2022-01-16T15:02:35Z,,,
12866,b'[MPNet] example of fine-tuning MPNet language model on domain specific corpus',2021-07-23T18:12:12Z,2021-09-01T15:02:19Z,,,
12865,b'Add TF multiple choice example',2021-07-23T16:35:05Z,2021-07-26T14:15:52Z,,,
12864,b'[Speech2Text] Slow tests are failing on master',2021-07-23T15:28:54Z,2021-09-01T15:02:20Z,,,
12863,b'[Wav2Vec2] Slow pretraining tests are failing on CPU',2021-07-23T15:21:10Z,2021-09-01T15:02:21Z,,,
12862,b'BatchFeature should cast to `np.float32` by default',2021-07-23T15:00:38Z,,WIP,,
12861,b'Asking for consent to publish `_LazyModule` as a standalone PyPI package on GitHub',2021-07-23T14:59:52Z,2021-07-26T19:40:44Z,,,
12860,b'[tests] fix logging_steps requirements',2021-07-23T14:44:47Z,2021-07-23T15:05:48Z,,,
12859,b'Cannot import pipeline after installation',2021-07-23T12:24:05Z,2021-08-30T15:07:35Z,,,
12858,b'Pin git python to <3.1.19',2021-07-23T10:48:14Z,2021-07-23T12:16:04Z,,,
12857,b'wav2vec pretrain and fine-tune with huge data',2021-07-23T10:17:40Z,2021-08-23T08:27:35Z,,,
12856,"b""TypeError: '>' not supported between instances of 'NoneType' and 'int'""",2021-07-23T09:50:39Z,2021-08-30T15:07:36Z,,,
12855,b'fix typo in gradient_checkpointing arg',2021-07-23T08:02:44Z,2021-07-30T07:06:34Z,,,
12854,b'How could I convert output tensor from transformer to text generation?',2021-07-23T06:46:45Z,2021-08-30T15:07:37Z,,,
12853,b'Fix barrier for SM distributed',2021-07-23T04:46:07Z,2021-07-26T12:30:53Z,,,
12852,b'How to ignore PAD tokens for NER',2021-07-23T04:00:36Z,2021-07-23T18:42:40Z,,,
12851,b'Got `ONNXRuntimeError` when try to run BART in ONNX format',2021-07-23T00:02:26Z,,WIP,RuntimeException,"RuntimeException: [ONNXRuntimeError] : 6 : RUNTIME_EXCEPTION : Non-zero status code returned while running Reshape node. Name:'Reshape_109' Status Message: /onnxruntime_src/onnxruntime/core/providers/cpu/tensor/reshape_helper.h:42 onnxruntime::ReshapeHelper::ReshapeHelper(const onnxruntime::TensorShape&, std::vector<long int>&, bool) gsl::narrow_cast<int64_t>(input_shape.Size()) == size was false. The input tensor cannot be reshaped to the requested shape. Input shape:{2}, requested shape:{1,1}"
12850,b'run_mlm_no_trainer.py requires --model_name_or_path',2021-07-22T23:40:40Z,2021-08-30T15:07:37Z,,,
12849,b'run_mlm_no_trainer.py requires accelerate but not in requirements.txt',2021-07-22T23:30:33Z,2021-08-23T08:25:54Z,,,
12848,b'legacy finetune with t5 issues',2021-07-22T20:00:00Z,2021-08-30T15:07:38Z,DeepSpeed,,
12847,b'Default process group has not been initialized while using sagemaker data parallel',2021-07-22T18:48:23Z,2021-07-26T12:30:53Z,,"[1,4]<stdout>:RuntimeError","[1,4]<stdout>:RuntimeError: Default process group has not been initialized, please make sure to call init_process_group."
12846,b'T5: Create position related tensors directly on device instead of CPU',2021-07-22T18:37:42Z,2021-08-04T15:58:30Z,,,
12845,b'Add Model Details for Pipeline API',2021-07-22T18:01:39Z,2021-08-30T10:04:31Z,,,
12844,b'Generate text from inputs_embeds for seq2seq models like BART or T5',2021-07-22T15:47:55Z,2021-08-30T15:07:40Z,,,
12843,b'Moving feature-extraction pipeline to new testing scheme',2021-07-22T13:53:33Z,2021-07-29T17:35:55Z,,,
12842,"b""FlaxGPT2LMHeadModel doesn't fail on mismatch between tokenizer and model vocab size""",2021-07-22T12:15:59Z,2021-09-26T15:02:32Z,,,
12841,b'Set Experiment Name in `MLflowCallback`',2021-07-22T11:40:15Z,2021-08-30T15:07:41Z,,,
12840,b'How to use the parameters of a certain layer',2021-07-22T11:10:48Z,2021-10-19T15:02:51Z,,,
12838,b'`transformers-cli` fails out of the box',2021-07-22T05:28:45Z,2021-07-22T14:42:04Z,,OSError,"OSError: dlopen(/Users/stellabiderman/opt/anaconda3/lib/python3.8/site-packages/torchaudio/_torchaudio.so, 6): Symbol not found: __ZN2at6detail10noopDeleteEPv"
12837,b'Fix CpmTokenizer for training/finetuning CPM model',2021-07-22T03:59:37Z,2021-07-29T19:05:16Z,,,
12836,b'[parallelism doc] document Deepspeed-Inference and parallelformers',2021-07-21T19:39:35Z,2021-07-21T22:11:02Z,,,
12835,b'Add support for T5 models in Zero Shot Classification pipeline',2021-07-21T19:20:28Z,2021-10-03T15:02:17Z,,,
12834,b'Add ESM to hugging face',2021-07-21T18:36:40Z,2021-08-30T15:07:42Z,,,
12833,b'unable to load cache when network is unavailable',2021-07-21T18:13:46Z,2021-09-12T15:02:16Z,,ValueError,"ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on."
12832,b'Fix type of max_seq_length arg in run_swag.py',2021-07-21T16:41:58Z,2021-07-22T06:14:15Z,,,
12831,b'Raise warning in HP search when hp is not in args',2021-07-21T16:41:38Z,2021-07-21T16:44:42Z,,,
12830,b'[Deepspeed] warmup_ratio docs',2021-07-21T16:33:43Z,2021-07-21T17:49:29Z,,,
12829,b'Flaky tests',2021-07-21T16:15:57Z,2021-08-30T15:07:43Z,,,
12828,b'Incorrect Tokenization behavior when working with Hindi using RobertaTokenizer',2021-07-21T14:20:29Z,2021-08-28T15:02:10Z,,,
12839,b'Error when doing `push_to_hub` two times in a row',2021-07-21T13:01:29Z,2021-08-30T15:07:44Z,,"CalledProcessError, OSError","CalledProcessError: Command '['git', 'commit', '-m', 'add tokenizer']' returned non-zero exit status 1.OSError: Sur la branche main"
12827,"b""VisualBert ValueError: visual_embeds can not be of class 'NoneType' when running on text only""",2021-07-21T10:55:19Z,2021-09-22T15:51:53Z,,ValueError,"ValueError: `visual_embeds` can not be of type <class 'NoneType'> when using a VisualBert Model."
12826,b'Allow the use of tensorflow datasets having UNKNOWN_CARDINALTY for TFTrainer',2021-07-21T09:43:19Z,2021-08-28T15:02:11Z,,,
12825,b'TFBertModel much slower on GPU than BertModel',2021-07-21T09:04:23Z,2021-08-28T15:02:12Z,,,
12824,"b""Can't use padding in Wav2Vec2Tokenizer. TypeError: '<' not supported between instances of 'NoneType' and 'int'.""",2021-07-21T09:03:14Z,2021-09-20T15:02:32Z,,TypeError,"TypeError: '<' not supported between instances of 'NoneType' and 'int'"
12823,b'Fix generation docstrings regarding input_ids=None',2021-07-21T08:57:22Z,2021-08-18T14:51:54Z,,,
12822,b'label list in MNLI dataset',2021-07-21T08:46:48Z,2021-08-28T15:02:13Z,,,
12821,b'Any example to accelerate BART/MBART model with onnx runtime\xef\xbc\x9f',2021-07-21T08:43:59Z,2021-08-09T13:51:49Z,,,
12820,b'fix typo in gradient_checkpointing metadata',2021-07-21T08:29:09Z,2021-07-23T06:22:57Z,,,
12819,"b""Converting a tensor to a python boolean might cause the trace to be incorrect. We can't record the data flow of python values""",2021-07-21T06:38:33Z,2021-08-28T15:02:14Z,,,
12818,b'Refer warmup_ratio when setting warmup_num_steps.',2021-07-21T04:19:28Z,2021-07-21T10:37:49Z,,,
12817,b'tensor size mismatch in NER.py',2021-07-21T02:57:35Z,2021-07-27T22:55:24Z,,RuntimeError,"RuntimeError: The size of tensor a (607) must match the size of tensor b (512) at non-singleton dimension 1"
12816,"b""[debug] DebugUnderflowOverflow doesn't work with DP""",2021-07-21T01:18:58Z,2021-07-21T16:36:03Z,,,
12815,b'DebugUnderflowOverflow crashes with Multi-GPU training',2021-07-20T21:59:35Z,2021-07-21T16:36:03Z,,KeyError,"KeyError: Caught KeyError in replica 0 on device 0."
12814,b'minor mistake in the documentation of XLMTokenizer',2021-07-20T21:10:38Z,2021-08-28T15:02:15Z,,,
12813,b'Update pyproject.toml',2021-07-20T20:12:19Z,2021-07-21T11:47:30Z,,,
12812,b'Expose get_config() on ModelTesters',2021-07-20T13:57:39Z,2021-07-21T08:13:11Z,,,
12811,b'Add _CHECKPOINT_FOR_DOC to all models',2021-07-20T13:46:30Z,2021-07-21T12:29:43Z,,,
12810,b'[CLIP/docs] add and fix examples',2021-07-20T12:14:03Z,2021-07-20T13:28:50Z,,,
12809,b'[Longformer] Correct longformer docs',2021-07-20T11:36:01Z,2021-07-20T12:17:21Z,,,
12808,b'https://huggingface.co/facebook/detr-resnet-101-panoptic model has 250 classes ?',2021-07-20T11:28:56Z,2021-08-27T15:07:17Z,,,
12807,b'Model Request: Blenderbot 2.0',2021-07-20T11:21:52Z,,New model,,
12806,b'Fix tokenizer saving during training with `Trainer`',2021-07-20T11:20:46Z,2021-09-01T14:32:57Z,,,
12805,b'What is the data format of transformers language modeling run_clm.py fine-tuning?',2021-07-20T09:43:30Z,2021-08-27T15:07:19Z,,IndexError,"IndexError: index out of bounds"
12804,b'[Sequence Feature Extraction] Add truncation',2021-07-20T09:36:59Z,2021-07-23T15:53:31Z,,,
12803,b'Add min and max question length options to TapasTokenizer',2021-07-20T08:16:07Z,2021-08-23T07:44:43Z,,,
12802,b'Very strange Training Data Loss Pattern when fitting MT5 for Summarization',2021-07-20T07:58:30Z,2021-10-01T15:02:30Z,,,
12801,b'Add possibility to ignore imports in test_fecther',2021-07-20T07:56:34Z,2021-07-26T13:48:19Z,,,
12800,b' I can not find transformers v4.9.0',2021-07-20T07:36:42Z,2021-08-27T15:07:20Z,,,
12799,b'Jax/Flax Text-Classification Examples are not working...',2021-07-20T04:13:54Z,2021-08-27T15:07:21Z,,,
12798,b'Error while converting a RoBERTa TF checkpoint to Pytorch',2021-07-20T02:26:23Z,2021-07-26T20:48:15Z,,AttributeError,"AttributeError: type object 'RobertaForMaskedLM' has no attribute 'load_tf_weights'"
12797,b'[Documentation] Improve docs for hyper-parameter search',2021-07-19T23:49:23Z,2021-08-27T15:07:21Z,,,
12796,b'[trainer] sanity checks for `save_steps=0|None` and `logging_steps=0`',2021-07-19T18:49:15Z,2021-07-20T16:05:27Z,,ZeroDivisionError,"ZeroDivisionError: integer division or modulo by zero"
12795,b'The network does not change if I resume training from checkpoint while also providing custom optimizer and scheduler',2021-07-19T17:20:09Z,2021-07-20T13:39:38Z,,,
12794,b'add `classifier_dropout` to classification heads',2021-07-19T16:37:41Z,2021-07-26T12:30:05Z,,,
12793,b'Feature Request: El-Attention',2021-07-19T16:35:45Z,,New model,,
12792,b'Declaring `classifier_dropout` in model config but not using it',2021-07-19T16:29:22Z,2021-08-01T15:08:13Z,,,
12791,b'[CIs] add troubleshooting docs',2021-07-19T15:54:43Z,2021-07-20T07:32:02Z,,,
12790,b'Tapas tokenizer',2021-07-19T15:54:25Z,2021-08-23T07:44:42Z,,ValueError,"ValueError: Could not encode the query and table header given the maximum length. Encoding the query and tableheader results in a length of 1235 which is higher than the max_length of 512"
12789,b'Raise exceptions instead of using assertions for control flow',2021-07-19T14:28:44Z,,Good First Issue,,
12788,b'Add ONNX export for gpt_neo models',2021-07-19T13:28:00Z,2021-08-26T15:06:28Z,,,
12787,b'How to use past_key_values in RAG model?',2021-07-19T13:12:46Z,2021-08-18T16:55:22Z,,,
12786,b'Enforce eval and save strategies are compatible when --load_best_model_at_end',2021-07-19T12:14:08Z,2021-07-19T17:50:48Z,,,
12785,"b'Pre training problem, please help me out'",2021-07-19T11:35:41Z,2021-08-14T08:41:14Z,,SyntaxError,"SyntaxError: invalid syntax`"
12784,b'Improving pipeline tests',2021-07-19T10:17:37Z,2021-07-22T13:19:35Z,,,
12783,b'How to solve the CUDA out of memory',2021-07-19T10:12:18Z,2021-08-26T15:06:29Z,,,
12782,b'[Flax] Correct flax docs',2021-07-19T10:04:07Z,2021-08-04T14:31:23Z,,,
12781,b'Set dropout for ClassificationHead',2021-07-19T09:42:24Z,2021-08-05T14:05:21Z,,,
12780,b'Documentation of longformer model is confusinng',2021-07-19T09:12:56Z,2021-07-20T12:17:21Z,,,
12779,b'Longer timeout for slow tests',2021-07-19T08:55:28Z,2021-07-19T08:55:41Z,,,
12778,b'Unable to finetune RAG end2end due to error in finetune_rag.py file',2021-07-19T08:28:45Z,2021-08-26T15:06:29Z,,"UnboundLocalError, ValueError","UnboundLocalError: local variable 'retriever' referenced before assignmentValueError: Failed to look up actor with name 'retrieval_worker_0'. You are either trying to look up a named actor you didn't create, the named actor died, or the actor hasn't been created because named actor creation is asynchronous."
12777,b'Error in the model card page',2021-07-19T07:34:04Z,2021-08-09T08:26:03Z,,,
12776,b'How to use the transformers pre-training model to calculate the probability of a sentence instead of PPL?',2021-07-19T06:28:17Z,2021-08-26T15:06:30Z,,,
12775,b'Adding m2m100 12B',2021-07-19T06:12:49Z,2022-03-13T22:30:47Z,New model,,
12774,"b""max_length parameter in Wav2Vec2FeatureExtractor doesn't affect""",2021-07-18T20:54:41Z,2021-07-23T15:53:31Z,,,
12773,"b""Can't load config for [community model] : DeepESP/gpt2-spanish """,2021-07-18T15:13:30Z,2021-07-18T15:56:41Z,,OSError,"OSError: Can't load config for 'DeepESP/gpt2-spanish'. Make sure that:"
12772,b'We made a toolkit can parallelize almost all the Hugging Face models. But we have some question !',2021-07-18T04:00:58Z,2021-09-22T07:43:57Z,,,
12771,b'GPTNeo Flax - crashes - n> sizes_size',2021-07-17T16:24:26Z,2021-10-26T15:07:41Z,,,
12770,b'Fix push_to_hub docstring and make it appear in doc',2021-07-17T09:03:53Z,2021-07-17T13:52:33Z,,,
12769,b'Seq2SeqTrainer Model parallelism with AWS Sagemaker - not enough values to unpack error',2021-07-17T08:50:30Z,2021-08-24T15:06:29Z,,,
12768,b'Alphafold 2.0',2021-07-17T07:55:01Z,,New model,,
12767,b'How to override model.generate() function in GenerationMixin class?',2021-07-17T05:13:08Z,2021-07-19T07:27:22Z,,,
12766,b'Log Azure ML metrics only for rank 0',2021-07-17T01:01:48Z,2021-07-30T07:11:31Z,,,
12765,"b'Error in HuggingFace Course ""Fine-tuning a pretrained model""'",2021-07-16T20:09:20Z,2021-07-19T15:05:45Z,,ValueError,"ValueError: too many dimensions 'str'"
12764,b'[Wav2Vec2] Padded vectors should not allowed to be sampled',2021-07-16T16:55:42Z,2021-07-16T17:07:09Z,,,
12763,b'IndexError: index out of range in self',2021-07-16T16:19:23Z,2021-08-24T15:06:31Z,,IndexError,"IndexError: index out of range in self"
12762,b't5 fast tokenizer save_vocabulary fails without sentencepiece file',2021-07-16T14:37:42Z,2021-09-02T09:56:08Z,,TypeError,"TypeError: expected str, bytes or os.PathLike object, not NoneType"
12761,b'Unable to run model parallel training using jax on TPU-VM',2021-07-16T14:29:58Z,2021-10-14T15:03:32Z,,,
12760,b'Embedding layer Pruning implementation',2021-07-16T14:16:52Z,2021-08-23T15:06:14Z,,,
12759,b'Preserve `list` type of `additional_special_tokens` in `special_token_map`',2021-07-16T13:20:51Z,2021-07-16T16:26:55Z,,,
12758,b'Turn on eval mode when exporting to ONNX',2021-07-16T12:48:44Z,2021-07-16T13:09:15Z,,,
12757,b'[flax/model_parallel] fix typos',2021-07-16T11:14:25Z,2021-07-16T11:14:59Z,,,
12756,b'unk_id is missing for SentencepieceTokenizer',2021-07-16T09:30:30Z,2021-07-20T05:02:04Z,,`Exception,"`Exception: Encountered an unknown token but `unk_id` is missing`"
12755,b'ValueError: cannot reshape array of size ... in run_t5_mlm_flax.py data_collator',2021-07-16T08:15:08Z,2022-03-04T16:04:43Z,,ValueError,"ValueError: cannot reshape array of size 98111 into shape (192,newaxis)"
12754,b'Add EncodedInput as an alternative input type of _batch_encode_plus in tokenization_utils_fast',2021-07-16T07:41:56Z,2021-08-23T15:06:15Z,,,
12753,b'Getting Word Embeddings for Sentences using long-former model?',2021-07-16T06:46:20Z,2021-07-17T08:30:56Z,,,
12752,"b'[Flax/run_hybrid_clip] Fix duplicating images when captions_per_image exceeds the number of captions, enable truncation'",2021-07-16T05:23:41Z,2021-09-02T05:49:49Z,,,
12751,b'add intel-tensorflow-avx512 to the candidates',2021-07-15T22:21:45Z,2021-07-16T09:54:49Z,,,
12750,b'hyperparameter search requirements/gpt2 metric',2021-07-15T22:09:40Z,2021-08-18T17:27:50Z,,,
12749,b'[ray] Fix `datasets_modules` ImportError with Ray Tune',2021-07-15T20:45:53Z,2021-07-19T08:32:41Z,,,
12748,b'[Wav2Vec2] Correctly pad mask indices for PreTraining',2021-07-15T20:22:38Z,2021-07-15T20:40:25Z,,,
12747,b'[WIP] Add classification head for T5 and MT5',2021-07-15T20:15:39Z,2021-09-19T15:01:58Z,,,
12746,b'How to finetune mT5',2021-07-15T17:04:01Z,2021-08-23T15:06:17Z,,,
12745,b'Replace specific tokenizer in log message by AutoTokenizer',2021-07-15T16:35:37Z,2021-07-15T16:59:48Z,,,
12744,b'Blenderbot output logits dimensions mismatch',2021-07-15T14:21:24Z,2021-07-16T07:10:15Z,,,
12743,b'[Debug] wav2vec2 pretraining',2021-07-15T13:27:10Z,2021-09-18T15:01:55Z,,,
12742,b'Patch T5 device test',2021-07-15T13:18:53Z,2021-07-15T15:40:17Z,,,
12741,b'Change create_model_card to use best eval_results when args.load_best_model_at_end==True',2021-07-15T13:14:57Z,2021-07-16T10:05:15Z,,,
12740,b'Skip test while the model is not available',2021-07-15T13:14:06Z,2021-07-15T13:14:12Z,,,
12739,b'Skip test while the model is not available',2021-07-15T13:06:41Z,2021-07-15T13:06:48Z,,,
12738,b'Doctest Integration',2021-07-15T12:58:24Z,2021-07-15T13:12:20Z,,,
12737,b'Fix MBart failing test',2021-07-15T12:51:58Z,2021-07-15T15:39:36Z,,,
12736,b'LXMERT integration test typo',2021-07-15T12:29:42Z,2021-07-15T12:29:50Z,,,
12735,b'Fix led torchscript',2021-07-15T12:26:10Z,2021-07-15T15:48:50Z,,"RuntimeError, AssertionError","RuntimeError: 0INTERNAL ASSERT FAILED at ""/pytorch/torch/csrc/jit/ir/alias_analysis.cpp"":532, please report a bug to PyTorch. We don't have an op for aten::constant_pad_nd but it isn't a special case.  Argument types: Tensor, int[], bool, AssertionError: Couldn't trace module."
12734,b'Fix DETR integration test',2021-07-15T12:22:57Z,2021-07-15T15:48:37Z,,,
12733,b'Fix AutoModel tests',2021-07-15T12:22:24Z,2021-07-15T13:06:12Z,,,
12732,b'Not able to load the custom model after training in Hugging Face',2021-07-15T11:21:19Z,2021-07-16T07:16:16Z,,,
12731,b'Remove framework mention',2021-07-15T09:16:15Z,2021-07-15T15:49:02Z,,,
12730,b'Adding a Wav2Vec2ForSpeechClassification class',2021-07-15T07:30:13Z,2021-09-01T13:27:59Z,,,
12729,b'Checkpoints are saved multiple times during hyperparameter tuning / How to get the best model?',2021-07-15T07:14:07Z,2021-08-22T15:02:07Z,,,
12728,b'How can I generate sentencepiece file or vocabulary from tokenizers?',2021-07-15T04:09:43Z,2021-09-20T15:02:35Z,,,
12727,b'Getting incompatible shapes when using global_attention_mask in TFLongformerModel',2021-07-15T04:04:55Z,2021-10-12T15:02:51Z,,InvalidArgumentError,"InvalidArgumentError:  Incompatible shapes: [2,1024,12,513] vs. [2,1024,12,522]"
12726,b'Unrecognized configuration class GPT2Config for AutoModelForSeq2SeqLM | Microsoft DialoGPT no longer working',2021-07-15T03:01:56Z,2021-07-17T03:38:05Z,,{'error',"{'error': ""Unrecognized configuration class <class 'transformers.models.gpt2.configuration_gpt2.GPT2Config'> "
12725,b'[doc] performance: batch sizes',2021-07-15T00:29:40Z,2021-07-15T16:39:34Z,,,
12724,b'[doc] testing: how to trigger a self-push workflow',2021-07-14T23:45:45Z,2021-07-15T23:18:57Z,,,
12723,b'[deepspeed] nvme test hanging experiment: take4',2021-07-14T23:26:34Z,2021-07-20T15:30:27Z,,,
12722,b'[deepspeed] nvme test hanging experiment: take3',2021-07-14T23:22:44Z,2021-07-14T23:27:25Z,,,
12721,b'[WIP] [deepspeed] nvme test hanging experiment: take2',2021-07-14T23:13:35Z,2021-07-14T23:23:09Z,,,
12720,b'[Flax] Correct shift labels for seq2seq models in Flax',2021-07-14T22:10:37Z,2021-07-15T06:42:37Z,,,
12719,b'[Flax] Change all `shift_tokens_right` to numpy code',2021-07-14T22:00:19Z,2021-07-15T06:42:37Z,,,
12718,b'[trainer] release tmp memory in checkpoint load',2021-07-14T21:25:55Z,2021-07-14T22:18:02Z,,,
12717,b'[wip] [deepspeed] nvme test hanging experiment',2021-07-14T21:17:07Z,2021-07-14T23:14:32Z,,,
12716,b'Fix typo in Speech2TextForConditionalGeneration example',2021-07-14T21:12:44Z,2021-07-15T06:44:03Z,,,
12715,b'[testing] failing tests/deepspeed/test_deepspeed.py::TrainerIntegrationDeepSpeed::test_stage3_nvme_offload',2021-07-14T21:12:31Z,2021-08-14T16:24:15Z,DeepSpeed,,
12714,b'layoutlm TokenClassificationPipeline',2021-07-14T21:07:23Z,2021-07-15T15:06:35Z,,,
12713,b'Add versioning system to fast tokenizer files',2021-07-14T21:06:39Z,2021-07-21T12:24:37Z,,,
12712,b'[doc] parallelism: Which Strategy To Use When',2021-07-14T20:32:29Z,2021-07-15T16:38:52Z,,,
12711,b'Error while performing eval on clm using gpt2 in flax',2021-07-14T20:05:36Z,2021-07-18T19:58:31Z,,TypeError,"TypeError: tree_map() missing 1 required positional argument: 'tree'"
12710,b'[test] split test into 4 sub-tests to avoid timeout',2021-07-14T18:55:27Z,2021-07-14T20:04:58Z,,,
12709,b'Init adds its own files as impacted',2021-07-14T18:00:35Z,2021-07-15T08:17:47Z,,,
12708,b'[Bug?] question answering - end position of each input is weird',2021-07-14T17:35:18Z,2021-07-15T16:35:28Z,,,
12707,b'Convert model from flax to TF',2021-07-14T14:50:45Z,2021-08-26T15:06:33Z,,,
12706,b'Deprecate TFTrainer',2021-07-14T14:17:00Z,2021-07-14T14:59:14Z,,,
12705,b'Fix uninitialized variables when `config.mask_feature_prob > 0`',2021-07-14T13:48:22Z,2021-07-14T14:30:19Z,,,
12704,b'Where is the casual mask when using BertLMHeadModel and set config.is_decoder = True?',2021-07-14T13:15:50Z,2021-07-24T06:41:55Z,,,
12703,b'Update TF examples README',2021-07-14T12:32:37Z,2021-07-14T14:15:25Z,,,
12702,b'Examples/flax/run_clm_flax.py showing error file extension error for train_file attribute even though file has the correct extension',2021-07-14T11:45:26Z,2021-07-14T18:44:37Z,,AssertionError,"AssertionError: `train_file` should be a csv, a json or a txt file."
12701,b'Translate README.md to Traditional Chinese',2021-07-14T11:30:07Z,2021-07-15T15:35:39Z,,,
12700,b'Doc - expecting `push_to_hub` method for Tokenizers to be also in the Tokenizer class doc pages',2021-07-14T11:24:33Z,2021-07-17T13:52:33Z,,,
12699,b'Add a custom timeout for log replica test',2021-07-14T09:16:44Z,2021-07-14T20:05:57Z,,,
12698,b'[Examples]Flax Seq2Seq example fails when doing only eval or predict',2021-07-14T08:37:47Z,2021-08-22T15:02:08Z,,,
12697,b'SystemError: <built-in method run_backward of torch._C._EngineBase object at 0x7f06bfae6b30> returned NULL without setting an error> ```',2021-07-14T07:34:46Z,2021-09-16T15:01:58Z,,SystemError,"SystemError: <built-in method run_backward of torch._C._EngineBase object at 0x7f06bfae6b30> returned NULL without setting an error"
12696,b'Refactored code to improve performance.',2021-07-14T04:36:27Z,2021-07-18T22:38:39Z,,,
12695,b'[WIP] [Deepspeed] model zoo continued',2021-07-14T04:20:18Z,,"DeepSpeed, WIP",,
12694,b'Refactored code to improve performance',2021-07-14T02:01:40Z,2021-07-14T02:03:42Z,,,
12693,b'Strange output from summarization models ',2021-07-14T01:08:00Z,2021-08-22T15:02:10Z,,,
12692,b'Provide mask_time_indices to `_mask_hidden_states` to avoid double masking',2021-07-14T00:04:03Z,2021-07-14T11:17:34Z,,,
12691,"b'OSError: Not found: ""/root/.cache/huggingface/transformers/5ec31591d9130cc9be0872e6b3dc0b276e514ab96e68404ac4a876ff03cb413b.dbd4bc2544d5c9f8f0d109844726c1600fa95cf0ba770b54c146f702be6e55dc"": No such file or directory Error #2'",2021-07-13T23:16:48Z,2021-08-22T15:02:10Z,,,
12690,b'[Deepspeed] non-native optimizers are mostly ok with zero-offload',2021-07-13T23:15:47Z,2021-07-14T03:18:52Z,,,
12689,b'Flax MLM: Allow validation split when loading dataset from local file',2021-07-13T21:27:18Z,2021-07-20T11:38:26Z,,,
12688,b'[doc] parallelism - when to use which mode',2021-07-13T21:22:27Z,2021-07-15T16:38:51Z,"Documentation, Performance",,
12687,b'Assert evaluation_strategy not no when load_best_model_at_end',2021-07-13T20:54:35Z,2021-07-19T17:51:23Z,,,
12686,b'No docs for v2.3.0',2021-07-13T20:31:56Z,2021-08-22T15:02:11Z,,,
12685,b'[trainer] `--load_best_model_at_end` silently turns of `--save_steps` settings',2021-07-13T18:42:08Z,2021-08-30T11:06:55Z,,,
12684,b'Add timeout to CI.',2021-07-13T17:00:52Z,2021-07-13T19:13:18Z,,,
12683,b'confusing description in prepare_seq2seq_batch of MBart',2021-07-13T16:40:09Z,2021-08-22T15:02:12Z,,,
12682,b'Fix minor docstring typos.',2021-07-13T16:01:32Z,2021-07-13T16:08:16Z,,,
12681,b'Flax - Loading pretrained model overwrites weights of different shapes',2021-07-13T13:49:24Z,2021-07-14T19:48:57Z,,,
12680,b'Running out of memory when resume training.',2021-07-13T13:08:07Z,2021-07-15T09:56:37Z,DeepSpeed,,
12679,b'Fix multiple choice doc examples',2021-07-13T12:39:57Z,2021-07-14T07:35:18Z,,,
12678,b'Mask prediction does not work with whitespace  before mask token',2021-07-13T12:08:43Z,2021-07-13T12:30:49Z,,,
12677,"b'Processing custom wikipedia data with clm training script throws error when ""blockifying"" data'",2021-07-13T11:32:55Z,2021-07-13T11:56:37Z,,TypeError,"TypeError: can only concatenate list (not ""str"") to list"
12676,"b'Wrong model is used in example, should be character instead of subword model'",2021-07-13T10:40:11Z,2021-07-13T12:40:27Z,,,
12675,b'[WIP][examples/flax] add gradient accumulation',2021-07-13T10:34:29Z,,WIP,,
12674,b'Nothing',2021-07-13T07:17:37Z,2021-07-13T07:18:59Z,,,
12673,b'Too Many kernels and embeddings were randomly initialized when loading Hugging Face GPT-2 Model',2021-07-13T05:17:20Z,2021-07-13T13:34:02Z,,,
12672,b'[doc] fix distil* example link',2021-07-13T03:38:51Z,2021-08-21T15:02:24Z,,,
12671,b'Update generation_logits_process.py',2021-07-12T23:58:29Z,2021-08-24T18:34:05Z,,,
12670,"b""Converting fairseq roberta to transformer throws ModuleAttributeError: 'RobertaHubInterface' object has no attribute 'args'""",2021-07-12T22:48:18Z,2021-08-21T15:02:25Z,,,
12669,b'[tokenizer.prepare_seq2seq_batch] change deprecation to be easily actionable',2021-07-12T22:42:03Z,2021-07-13T16:19:04Z,,,
12668,b'Vocab size difference between tokenizer and config for XLMR.',2021-07-12T20:35:01Z,2021-08-21T15:02:26Z,,,
12667,b'Adding TF translation example',2021-07-12T19:37:17Z,2021-07-13T18:08:25Z,,,
12666,"b'translation with identical source and target language, for text normalization'",2021-07-12T19:35:00Z,2021-08-21T15:02:27Z,,,
12665,"b""word_ids() returned by RoBERTa Tokenizer behaves inconsistently for alphanumeric tokens like '18th'""",2021-07-12T18:55:12Z,2021-08-30T15:07:48Z,,,
12664,b'Add option to load a pretrained model with mismatched shapes',2021-07-12T18:53:13Z,2021-07-13T14:15:16Z,,,
12663,b'Fix typo in README_zh-hans.md',2021-07-12T17:49:48Z,2021-07-12T17:50:12Z,,,
12662,b'[Flax Generation] Correct inconsistencies PyTorch/Flax',2021-07-12T17:13:35Z,2021-07-13T17:53:30Z,,,
12661,"b""'TransfoXLLMHeadModelOutput' object has no attribute 'loss'""",2021-07-12T17:10:00Z,2021-08-21T15:02:29Z,,AttributeError,"AttributeError: 'TransfoXLLMHeadModelOutput' object has no attribute 'loss'"
12660,b'Updates timeline for project evaluation',2021-07-12T16:39:45Z,2021-07-12T19:24:59Z,,,
12659,"b""Can't load pretrained model when working in virtual environment""",2021-07-12T16:03:34Z,2021-08-21T15:02:30Z,,"OSError, urllib3.exceptions.MaxRetryError, requests.exceptions.ProxyError","OSError: [Errno 0] Errorurllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /bert-base-uncased/resolve/main/config.json (Caused by ProxyError('Cannot connect to proxy.', OSError(0, 'Error')))requests.exceptions.ProxyError: HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /bert-base-uncased/resolve/main/config.json (Caused by ProxyError('Cannot connect to proxy.', OSError(0, 'Error')))"
12658,"b'Autotokenizer error ""Already borrowed"" when used on thread pool'",2021-07-12T15:14:55Z,2021-08-21T15:02:31Z,,,
12657,b'Remove SageMaker documentation',2021-07-12T15:07:21Z,2021-07-12T16:02:52Z,,,
12656,b'Pipeline should be agnostic',2021-07-12T14:52:23Z,2021-07-12T15:42:59Z,,,
12655,"b""**encode_plus() shouldn't run for  W2V2CTC""",2021-07-12T14:33:56Z,2021-07-13T10:31:56Z,,,
12654,b'Pickle auto models',2021-07-12T14:21:29Z,2021-07-12T15:15:55Z,,,
12653,b'[WIP] Patch BigBird tokenization test',2021-07-12T13:58:55Z,2021-07-13T06:53:06Z,,,
12652,b'Fix transfo xl integration test',2021-07-12T13:33:16Z,2021-07-12T15:51:35Z,,,
12651,"b""TF TransfoXL doesn't work with the `generate` method""",2021-07-12T13:27:34Z,2021-08-20T15:02:09Z,,,
12650,b'The extended trainer tests should require torch',2021-07-12T13:21:38Z,2021-07-12T13:47:06Z,,,
12649,b'Skip TestMarian_MT_EN',2021-07-12T12:58:53Z,2021-07-12T13:11:32Z,,,
12648,b'Inconsistency between the tokenization of `CLIPTokenizer` and `CLIPTokenizerFast` with `openai/clip-vit-base-patch32`',2021-07-12T12:34:17Z,,WIP,,
12647,b'`TestMarian_MT_EN::test_batch_generation_mt_en` Failing due to randomly generated tokens',2021-07-12T12:22:56Z,,WIP,,
12646,b'Fixed docs',2021-07-12T12:11:26Z,2021-07-12T16:03:13Z,,,
12645,b'`TFHubertModelTest.test_model_from_pretrained` is failing',2021-07-12T12:05:43Z,2021-07-12T12:20:45Z,,,
12644,b'Only test the files impacted by changes in the diff',2021-07-12T11:39:44Z,2021-07-14T14:56:55Z,,,
12643,b'Adding an argument to exclude some states (pretrained weights) from being loaded.',2021-07-12T10:17:48Z,2021-07-13T16:28:25Z,,,
12642,"b'""token_type_ids"" is discarded when using GenerationMixin in \xe2\x80\x9dgeneration_utils.py\xe2\x80\x9c'",2021-07-12T09:55:14Z,2021-09-05T15:02:05Z,,,
12641,b'USE_TORCH while import transformers forever true',2021-07-12T08:06:05Z,2021-07-16T09:58:10Z,,AttributeError,"AttributeError: 'Version' object has no attribute 'major'"
12640,b'fix typo in modeling_t5.py docstring',2021-07-12T07:35:40Z,2021-07-12T16:24:32Z,,,
12639,b'Refactored code to improve performance/employ best practices.',2021-07-12T07:21:17Z,2021-07-12T21:25:59Z,,,
12638,b'[flax]fix jax array type check',2021-07-12T06:55:15Z,2021-07-12T09:48:43Z,,,
12637,b'Slower training speed under DeepSpeed',2021-07-12T04:45:46Z,2021-08-20T15:02:10Z,DeepSpeed,,
12636,b'Error on training XLNet. RuntimeError: CUDA error: device-side assert triggered',2021-07-12T04:30:02Z,2021-07-13T00:53:56Z,,RuntimeError,"RuntimeError: CUDA error: device-side assert triggered"
12635,b'Long-Short Transformer',2021-07-12T01:47:30Z,,New model,,
12634,b'Add ByT5 option to example run_t5_mlm_flax.py',2021-07-11T21:46:55Z,2021-07-13T12:39:57Z,,,
12633,b'Error pushing GPT2 flax training model to hub',2021-07-11T21:03:52Z,2021-07-12T11:11:13Z,,TypeError,"TypeError: '>' not supported between instances of 'int' and 'NoneType'"
12632,b'Vocab Size does not change when adding new tokens',2021-07-11T19:08:16Z,2021-07-13T00:50:25Z,,,
12631,"b""TypeError: forward() got an unexpected keyword argument 'label' in main tutorial""",2021-07-11T11:31:03Z,2021-07-11T15:30:39Z,,TypeError,"TypeError: forward() got an unexpected keyword argument 'label'"
12630,b'[Examples][Flax] added test file in summarization example ',2021-07-11T05:30:58Z,2021-07-12T06:45:15Z,,,
12629,b'How much of an improvement is DistilGPT-2 over an equivalent model trained without distilation?',2021-07-10T21:17:03Z,2021-08-18T15:06:21Z,,,
12628,b'GPTNeo Error Attempting to Generate Text',2021-07-10T18:50:08Z,,WIP,TypeError,"TypeError: dynamic_update_slice update shape must be smaller than operand shape, got update shape (1, 45) for operand shape (1, 20)."
12627,b'Add Flax Models to Pipelines',2021-07-10T18:26:54Z,,Feature request,,
12626,"b""can't load flax weights in PyTorch if flax model is saved with dtype `bfloat16`""",2021-07-10T12:37:26Z,2021-08-18T15:06:23Z,,TypeError,"TypeError: can't convert np.ndarray of type bfloat16. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool."
12625,b'Flax Wav2Vec2 - Add venv section and fix training script',2021-07-10T10:11:22Z,2021-09-07T15:02:27Z,,,
12624,b'Add tokenizer_file parameter to PreTrainedTokenizerFast docstring',2021-07-10T09:27:35Z,2021-07-12T11:51:59Z,,,
12623,b'Inconsistent shapes between value and initializer for parameter: FlaxGPT2LMHeadModel',2021-07-10T08:19:25Z,2021-07-11T05:04:23Z,,ScopeParamShapeError,"ScopeParamShapeError: Inconsistent shapes between value and initializer for parameter ""scale"" in ""/transformer/ln_f"": (1024,), (0,). (https://flax.readthedocs.io/en/latest/flax.errors.html#flax.errors.ScopeParamShapeError)"
12622,b'unclear `prepare_seq2seq_batch` deprecation',2021-07-10T04:43:14Z,2021-07-13T16:19:04Z,,,
12621,"b""can't pickle <class 'types.AutoModelForCausalLM'>""",2021-07-10T02:06:06Z,2021-07-12T15:15:54Z,,_pickle.PicklingError,"_pickle.PicklingError: Can't pickle <class 'types.AutoModelForCausalLM'>: it's notfound as types.AutoModelForCausalLM"
12620,b'[doc] fix anchor',2021-07-10T01:04:50Z,2021-07-10T01:48:28Z,,,
12619,b'Add tokenizers class mismatch detection between `cls` and checkpoint',2021-07-09T21:35:25Z,2021-07-17T13:52:22Z,,,
12618,b'validation metrics not being logged by Trainer',2021-07-09T19:27:34Z,2021-08-18T15:06:24Z,,,
12617,b'TF summarization example',2021-07-09T18:33:11Z,2021-07-12T14:58:38Z,,,
12616,b'Weird outputs by `opus-mt-en-es`',2021-07-09T17:30:15Z,2021-08-09T15:41:29Z,,,
12615,b'[FLax] Fix marian docs 2',2021-07-09T17:13:35Z,2021-07-09T17:28:57Z,,,
12614,b'[Flax Marian] Add marian flax example',2021-07-09T16:34:04Z,2021-07-09T17:01:58Z,,,
12613,"b'RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.HalfTensor [12, 4096, 1]], which is output 0 of ViewBackward, is at version 1; expected version 0 instead.'",2021-07-09T15:44:02Z,2021-09-17T15:05:50Z,,RuntimeError,"RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.HalfTensor [12, 4096, 1]], which is output 0 of ViewBackward, is at version 1; expected version 0 instead."
12612,b'[Flax] Fix mt5 auto',2021-07-09T15:26:27Z,2021-07-09T16:33:04Z,,,
12611,b'Better heuristic for token-classification pipeline.',2021-07-09T15:00:40Z,2021-07-26T14:21:27Z,,,
12610,b'Unable to load mT5 with FlaxAutoModelForSeq2SeqLM',2021-07-09T14:02:45Z,2021-07-09T16:33:04Z,,ValueError,"ValueError: Unrecognized configuration class <class 'transformers.models.mt5.configuration_mt5.MT5Config'> for this kind of AutoModel: FlaxAutoModelForSeq2SeqLM."
12609,b'Fix arg count for partial functions',2021-07-09T13:08:06Z,2021-07-09T13:24:43Z,,,
12608,b'[Flax] Fix cur step flax examples',2021-07-09T12:51:03Z,2021-07-09T12:51:28Z,,,
12607,b'T5 mlm Flax streaming example',2021-07-09T11:35:06Z,2021-09-19T15:02:01Z,,,
12606,b'Remote process received SIGTERM on 96 core  tpu-vm during group_text map on datasets',2021-07-09T11:01:44Z,2021-08-16T15:02:00Z,,,
12605,b'\xf0\x9f\x90\x9b `model_init` fails when its a partially evaluated funtion.',2021-07-09T11:00:06Z,2021-07-12T06:30:44Z,,,
12604,b'Add LayoutLMv2 + LayoutXLM',2021-07-09T07:17:15Z,2021-08-30T10:35:43Z,,Notes,"Notes: "
12603,b'Facing Issue while loading pytorch model as flax model',2021-07-09T06:06:54Z,2021-07-09T06:38:23Z,,ValueError,"ValueError: Unrecognized configuration class <class 'transformers.models.t5.configuration_t5.T5Config'> for this kind of AutoModel: FlaxAutoModelForMaskedLM."
12602,b'How to transfer fine-tuned model from python to rust?',2021-07-09T05:19:42Z,2021-07-09T06:02:25Z,,,
12601,b'Cannot load .pt model using Transformers',2021-07-09T02:18:01Z,2021-08-30T15:07:51Z,,,
12600,b'Custom tokenizer from Tokenizers library',2021-07-09T00:56:24Z,2021-07-12T00:56:16Z,,,
12599,b'Point to the right file for hybrid CLIP',2021-07-09T00:28:16Z,2021-07-12T06:46:23Z,,,
12598,"b'`tokenizer.special_tokens_map` has stringified list for ""additional_special_tokens"" value.'",2021-07-08T22:49:47Z,2021-07-16T16:26:54Z,,,
12597,b'[doc] fix broken ref',2021-07-08T20:46:07Z,2021-07-08T21:11:01Z,,,
12596,b'Translate README.md to Simplified Chinese',2021-07-08T19:48:39Z,2021-07-12T17:19:54Z,,,
12595,b'[Flax] Add flax marian',2021-07-08T19:23:50Z,2021-07-09T10:42:13Z,,,
12594,b'GPT-2 asking for Padding Token',2021-07-08T18:11:35Z,2021-07-09T22:24:32Z,,ValueError,"ValueError: Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`."
12593,b'XLM-RoBERTa NER extraction breaks/splitting the words !',2021-07-08T17:35:11Z,2021-07-26T14:21:27Z,,,
12592,b'Add Flax sprint project evaluation section',2021-07-08T16:08:02Z,2021-07-09T06:52:30Z,,,
12591,b'Fix MT5 init',2021-07-08T14:59:09Z,2021-07-08T15:12:18Z,,,
12590,b'flax model parallel training',2021-07-08T14:46:22Z,2021-07-14T17:25:44Z,,,
12589,b'Git LFS bug when uploading to hub',2021-07-08T14:19:22Z,2021-07-08T15:13:49Z,,,
12588,"b""'MT5Tokenizer' is not defined (on Google colab) """,2021-07-08T14:11:26Z,2021-07-08T15:12:18Z,,NameError,"NameError: name 'MT5Tokenizer' is not defined"
12587,b'OOM during saving step',2021-07-08T14:07:40Z,2021-07-13T10:25:02Z,DeepSpeed,,
12586,b'Fix caching issue #12536',2021-07-08T13:47:42Z,2021-08-15T15:01:41Z,,,
12585,b'Error when running wav2vec2 embeddings',2021-07-08T13:14:54Z,2021-08-09T11:07:41Z,,"ValueError, RuntimeError","ValueError: text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).RuntimeError: expected scalar type Long but found Float"
12584,"b""[Flax]Not able to Run Hugging Face GPT2 model for jax on TPU's""",2021-07-08T13:13:09Z,2021-07-12T09:48:43Z,,,
12583,b'AttributeError for DataCollatorForLanguageModelling with tokenizers.Tokenizer',2021-07-08T12:45:37Z,2021-07-08T16:58:40Z,,AttributeError,"AttributeError: 'tokenizers.Tokenizer' object has no attribute 'mask_token'"
12582,b'Simplify unk token',2021-07-08T12:26:30Z,2021-07-09T13:02:34Z,,,
12581,"b'ViT doesnt use tokenizer, yet shown as example transformer website'",2021-07-08T11:47:53Z,2021-08-15T15:01:41Z,,ValueError,"ValueError: Unrecognized configuration class <class 'transformers.models.vit.configuration_vit.ViTConfig'> to build an AutoTokenizer."
12580,"b""Unable to quantize Google's LaBSE model using convert_graph_to_onnx.py""",2021-07-08T10:21:02Z,2021-08-15T15:01:42Z,,,
12579,"b""ImportError: cannot import name 'LineByLineTextDataset' from 'transformers' (unknown location)""",2021-07-08T07:27:51Z,2021-08-15T15:01:43Z,,ImportError,"ImportError: cannot import name 'LineByLineTextDataset' from 'transformers' (unknown location)"
12578,b'tuple index out of range for FlaxMBartForConditionalGeneration',2021-07-08T06:56:16Z,2021-07-12T09:54:02Z,,IndexError,"IndexError: tuple index out of range"
12577,b'[Work In Progress] SentenceTransformer implementation based on CLIP',2021-07-08T06:36:09Z,2021-08-15T15:01:43Z,,,
12576,"b'Summarization failure ""All images are copyrighted"" for certain text inputs'",2021-07-08T06:06:45Z,2021-08-15T15:01:44Z,,,
12575,"b""HTTPSConnectionPool(host='cdn-lfs.huggingface.co', port=443): Max retries exceeded""",2021-07-08T02:29:38Z,2021-08-15T15:01:45Z,,socket.gaierror,"socket.gaierror: [Errno -2] Name or service not known"
12574,b'[model.from_pretrained] raise exception early on failed load',2021-07-08T01:15:53Z,2021-07-08T15:17:51Z,,,
12573,b'PEGASUS using ONNX',2021-07-08T01:13:48Z,2021-08-15T15:01:46Z,,,
12572,b'push_to_hub related issues (from Google Colab)',2021-07-08T00:18:59Z,2021-08-15T15:01:46Z,,,
12571,b'AutoTokenizer not loading gpt2 model on instance without internet connection even after caching model',2021-07-08T00:14:08Z,2021-08-24T07:05:33Z,,ValueError,"ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on."
12570,"b""Can't Select Specific GPU by TrainingArguments""",2021-07-07T23:51:13Z,2021-08-15T15:01:47Z,,,
12569,b'Remove logging of GPU count etc from run_t5_mlm_flax.py',2021-07-07T20:57:14Z,2021-07-07T22:05:47Z,,,
12568,b'Pegasus from Pytorch to tensorflow',2021-07-07T19:30:33Z,2021-08-15T15:01:48Z,Migration,,
12567,b'Init pickle',2021-07-07T17:49:46Z,2021-07-08T11:20:46Z,,,
12566,b'[examples/hybrid_clip] fix loading clip vision model',2021-07-07T17:20:09Z,2021-07-07T17:20:28Z,,,
12565,b'tfhub.de -> tfhub.dev',2021-07-07T16:10:33Z,2021-08-09T06:11:17Z,,,
12564,b'Added fsck_etags to verify cache consistency.',2021-07-07T15:51:02Z,2021-08-15T15:01:48Z,,,
12563,b'Issue in terms of accuracy of onnx converted models of SQUAD based ROBERTA  on legal domain.',2021-07-07T14:50:44Z,2021-08-15T15:01:49Z,,,
12562,b'Double check for attribute num_examples',2021-07-07T14:35:05Z,2021-07-07T16:50:42Z,,,
12561,"b""Don't stop at num_epochs when using IterableDataset""",2021-07-07T14:18:06Z,2021-07-08T11:24:47Z,,,
12560,b'Adding prepare_decoder_input_ids_from_labels methods to all TF ConditionalGeneration models',2021-07-07T14:14:20Z,2021-07-07T14:30:47Z,,,
12559,b'[Flax] Allow retraining from save checkpoint',2021-07-07T13:40:33Z,2021-07-07T13:43:44Z,,,
12558,b'Fix group_lengths for short datasets',2021-07-07T13:14:58Z,2021-07-08T11:23:42Z,,,
12557,b'Cached data not checked for integrity',2021-07-07T13:06:50Z,2021-08-15T15:01:50Z,,,
12556,b'Slow gpt2 training on TPU with run_clm_flax.py',2021-07-07T12:46:55Z,2021-08-15T15:01:50Z,,,
12555,b'Display error message for pipeline loading failures',2021-07-07T12:25:19Z,2021-09-11T15:02:16Z,,,
12554,b'Issue converting Flax model to Pytorch',2021-07-07T11:58:51Z,2021-07-07T13:17:43Z,,,
12553,b'`model_name_or_path` does not seem to load in previously trained checkpoints',2021-07-07T10:57:45Z,2021-07-07T13:38:33Z,,,
12552,b'Make LazyModule picklable',2021-07-07T09:54:28Z,2021-08-06T16:34:29Z,,,
12551,b'[trainer] add option to ignore keys for the train function too (#11719)',2021-07-07T09:31:24Z,2021-07-07T12:07:46Z,,,
12550,"b'This will reduce ""Already borrowed error"":'",2021-07-07T09:09:25Z,2021-07-09T07:36:05Z,,,
12549,"b""TypeError: cannot pickle '_LazyModule' object""",2021-07-07T03:53:55Z,2021-07-08T11:21:13Z,,TypeError,"TypeError: cannot pickle '_LazyModule' object"
12548,b'raise exception when arguments to pipeline are incomplete',2021-07-07T03:48:33Z,2021-07-08T08:17:34Z,,,
12547,b'Get Start with CamembertForSequenceClassification',2021-07-07T00:58:18Z,2021-08-15T15:01:51Z,,,
12546,b'Necessary resources for training a (small/tiny) LM from scratch?',2021-07-06T21:43:28Z,2021-07-07T08:29:16Z,,,
12545,b'[Flax] Error converting model to PyTorch from Flax',2021-07-06T19:21:16Z,2021-07-07T15:37:01Z,,TypeError,"TypeError: can't convert np.ndarray of type bfloat16. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool."
12544,b'[examples/flax] add adafactor optimizer',2021-07-06T19:01:07Z,2021-07-07T06:20:31Z,,,
12543,b'[Flax] Adapt examples to be able to use eval_steps and save_steps',2021-07-06T17:37:59Z,2021-07-06T18:41:51Z,,,
12542,"b""ModuleNotFoundError: No module named 'transformers'""",2021-07-06T15:31:41Z,2021-07-06T17:29:04Z,,ModuleNotFoundError,"ModuleNotFoundError: No module named 'transformers'"
12541,b'Edit readme',2021-07-06T15:27:10Z,2021-07-06T15:31:45Z,,,
12540,b'Updated README',2021-07-06T13:59:39Z,2021-07-06T14:12:17Z,,,
12539,b'How to make BART infill unmasked deletions (not masked tokens)?',2021-07-06T13:58:52Z,2021-08-14T15:02:19Z,,,
12538,b'[WIP] Extend the testing of tokenizers that just have a legacy version',2021-07-06T13:53:56Z,2021-08-25T15:06:26Z,,,
12537,b'[WIP] flax gradient checkpointing',2021-07-06T13:47:28Z,,WIP,,
12536,b'Attempting to load non-existent vocab files from cache leads to a breaking behavior offline (while non-breaking online)',2021-07-06T13:20:26Z,2021-08-14T15:02:22Z,,,
12535,b'Extend the testing of tokenizers that just have a legacy version',2021-07-06T13:08:04Z,2021-08-14T15:02:22Z,,,
12534,b'[Flax] from_pretrained does not consider the passed dtype',2021-07-06T12:51:26Z,,WIP,,
12533,"b'The ""additional_special_tokens"" argument in the "".from_pretrained"" method of the tokenizer is not necessarily taken into account.'",2021-07-06T12:50:11Z,2021-09-07T15:02:29Z,,,
12532,b'Flax Save/Load from base model with different name',2021-07-06T11:44:46Z,2021-08-16T15:02:03Z,,,
12531,b' run_clm_no_trainer.py  ModuleNotFoundError',2021-07-06T10:55:01Z,2021-07-08T13:43:09Z,,ModuleNotFoundError,"ModuleNotFoundError: No module named 'datasets_modules.datasets.mimic_string'"
12530,b'Is there a Bert version of the OpenAIGPTLMHeadModel?',2021-07-06T08:34:23Z,2021-08-16T15:02:04Z,,,
12529,b'Load Trainer state',2021-07-06T08:16:33Z,2021-07-07T13:39:43Z,,,
12528,b'FlaxRobertaModel.from_pretrained does not load weights correctly',2021-07-06T08:06:21Z,2021-07-06T10:36:56Z,,,
12527,"b""[Examples][Flax] AttributeError: 'DataTrainingArguments' object has no attribute 'test_file'""",2021-07-06T04:33:27Z,2021-07-12T06:45:14Z,,AttributeError,"AttributeError: 'DataTrainingArguments' object has no attribute 'test_file'"
12526,b'Really long training time on bert fine-tuning for classification',2021-07-06T01:32:30Z,2021-07-11T23:49:54Z,,,
12525,b'[debugging utils] minor doc improvements',2021-07-06T00:03:50Z,2021-07-10T00:38:28Z,,,
12524,b'[doc] DP/PP/TP/etc parallelism',2021-07-06T00:00:56Z,2021-07-10T00:39:09Z,,,
12523,b'How much GPU memory is required for DistilBERT?',2021-07-05T22:52:46Z,2021-07-09T18:12:10Z,,,
12522,b'Keep getting an OOM when doing a evaluation',2021-07-05T22:45:44Z,2021-07-06T13:51:28Z,,,
12521,b'Improve documentation of pooler_output in ModelOutput',2021-07-05T21:35:43Z,2021-08-16T15:02:05Z,,,
12520,b'[Wav2Vec2] Flax - Adapt wav2vec2 script',2021-07-05T20:12:33Z,2021-07-05T22:49:47Z,,,
12519,b'[Flax] Fix hybrid clip',2021-07-05T19:00:11Z,2021-07-06T05:42:48Z,,,
12518,b'Can hidden states be passed instead of input_ids or inputs_embeds in Transformers OpenAI GPT2?',2021-07-05T18:27:00Z,2021-08-14T15:02:24Z,,,
12517,b'MLM training fails with no validation file(same as #12406 for pytorch now)',2021-07-05T17:37:34Z,2021-07-07T13:05:44Z,,,
12516,b'[Flax] Fix another bug in logging steps',2021-07-05T17:34:45Z,2021-07-05T17:35:22Z,,,
12515,b'[Flax] Correct logging steps flax',2021-07-05T17:20:36Z,2021-07-05T17:21:00Z,,,
12514,b'[Flax] Correct flax training scripts',2021-07-05T16:35:53Z,2021-07-05T17:14:50Z,,**Note**,"**Note**: By default `logging_steps` is set to 500. "
12513,b'Loading FlaxHybridCLIP trained model ',2021-07-05T14:49:41Z,2021-07-06T05:42:48Z,,KeyError,"KeyError: 'hybrid-clip'"
12512,b'Remove tf.roll wherever not needed',2021-07-05T14:49:22Z,2021-07-07T15:17:31Z,,,
12511,b'Add a warning for broken ProphetNet fine-tuning',2021-07-05T13:27:27Z,2021-07-07T08:32:49Z,,,
12510,b'Fix order of state and input in Flax Quickstart README',2021-07-05T13:07:30Z,2021-07-05T14:03:14Z,,,
12509,"b""`transformers-cli env` doesn't work out-of-the-box on v3-8 TPU""",2021-07-05T13:03:53Z,2021-08-13T15:07:39Z,,OSError,"OSError: sndfile library not found"
12508,b'Our Model works well in the local ! But when we upload that it doesnt work showing irrelevent performance ! ',2021-07-05T12:29:11Z,2021-07-05T18:26:34Z,,,
12506,b'[Flax] Non working model when exporting to Huggingface',2021-07-05T10:36:28Z,2021-08-13T15:07:40Z,,,
12505,b'Error while running rn_clm_flax.py training script',2021-07-05T09:50:26Z,2021-07-05T17:14:50Z,,RuntimeError,"RuntimeError: Resource exhausted: Attempting to allocate 121.12M. That was not possible. There are 127.34M free. Due to fragmentation, the largest contiguous region of free memory is 120.12M.; (0x0x0_HBM0): while running replica 0 and partition 0 of a replicated computation (other replicas may have failed as well)."
12504,b'Creating Flax VisualBert based on Flax Bert',2021-07-05T09:17:49Z,2021-07-06T22:27:00Z,,TypeError,"TypeError: __call__() missing 4 required positional arguments: 'visual_inputs_embeds', 'visual_attention_mask', 'visual_token_type_ids', and 'visual_position_ids'"
12503,b'model.generate does not work when using a FlaxGPTNeoForCausalLM model in PT (flax-community-event)',2021-07-05T09:14:25Z,2021-07-06T06:17:47Z,,"KeyError, AttributeError","KeyError: 'new_ones'AttributeError: "
12502,b'jax hybrip_clip error ( numpy  is not a valid JAX type )',2021-07-05T08:37:48Z,2021-07-22T01:33:42Z,,,
12501,b'Add `Repository` import to the FLAX example script',2021-07-05T07:50:47Z,2021-07-05T07:51:12Z,,,
12500,b'text-classification example broken; version issue / inconsistency issues',2021-07-05T04:29:25Z,2021-07-08T21:12:53Z,,,
12499,b'`max_steps` would not override `num_train_epochs` when training with IterableDataset',2021-07-05T03:36:23Z,2021-07-08T11:24:46Z,,,
12498,b'[Flax] Fix wav2vec2 pretrain arguments',2021-07-05T03:01:18Z,2021-07-05T12:35:21Z,,,
12497,b'implementing tflxmertmodel integration test',2021-07-04T21:50:19Z,2021-07-06T15:44:48Z,,,
12496,b'Feature Request: Flax Encoder-Decoder Model',2021-07-04T18:46:49Z,2021-08-13T15:07:40Z,,,
12495,b'Flax MLM example script has PyTorch dependency',2021-07-04T11:22:34Z,2021-07-05T17:14:50Z,,ImportError,"ImportError: Method `device` requires PyTorch."
12494,b'Add padding for decoder inputs for flax t5 example',2021-07-04T06:29:16Z,2021-09-07T15:02:30Z,,,
12493,b'FlaxGPTNeo',2021-07-04T05:28:26Z,2021-07-06T13:25:19Z,,,
12492,b'Bug in MLM script not parsing arguments properly + lack of warning on incorrect args',2021-07-03T23:06:58Z,2021-07-08T11:52:01Z,,,
12491,b'[examples/flax] clip style image-text training example',2021-07-03T15:04:03Z,2021-07-05T07:56:44Z,,,
12490,b'non-identical position_ids in an input batch',2021-07-03T09:46:04Z,2021-08-11T15:06:09Z,,,
12489,b'Get core dump after import FlaxRobertaModel',2021-07-03T05:59:10Z,2021-07-03T10:54:41Z,,,
12488,"b""Fine-tuning t5-large: greedy predictions don't match teacher-forcing results""",2021-07-02T20:34:47Z,2021-09-07T15:02:32Z,,,
12487,b'Fix Padded Batch Error 12282',2021-07-02T19:00:25Z,2021-07-20T11:36:47Z,,,
12486,b'GPT2: discrepancy between `inputs_embeds` and `input_ids` when the input sentence has length = 1 ',2021-07-02T17:44:50Z,2021-07-12T15:51:56Z,,,
12485,b'Tensorboard error while running mlm_flax TPU example script on TPU',2021-07-02T15:41:05Z,2021-08-11T15:06:10Z,,,
12484,b'checkpoints are not saved after implementing a custom loss ',2021-07-02T15:26:31Z,2021-08-12T15:06:27Z,,,
12483,b'Text Classification on GLUE on TPU using Jax/Flax : BigBird',2021-07-02T15:22:27Z,2021-08-12T15:06:27Z,,,
12482,b'Sources of randomness for Longformer',2021-07-02T14:40:11Z,2022-02-16T15:07:17Z,,,
12481,b'How to construct a pretrain by myself using Tensorflow2 +Keras?',2021-07-02T10:34:06Z,2021-08-09T15:07:36Z,,,
12480,b'Fix TAPAS test uncovered by #12446',2021-07-02T08:22:08Z,2021-07-02T08:35:10Z,,,
12479,b'AttributeError when using custom IterableDataset with set_epoch method',2021-07-02T07:11:37Z,2021-07-07T16:50:42Z,,,
12478,b'Mismatch between tokenizer and model in pipeline',2021-07-02T05:56:44Z,2021-07-08T08:17:34Z,,,
12477,"b'[Deepspeed] adapt multiple models, add zero_to_fp32 tests'",2021-07-02T04:54:53Z,2021-07-13T19:07:33Z,,,
12476,b'How to fine-tune a model with my custom tokenizer?',2021-07-02T03:02:02Z,2021-08-09T15:07:37Z,,,
12475,b'BartForConditionalGeneration: `decoder_input_ids` should not be computed if `decoder_inputs_embeds` is set',2021-07-01T22:06:34Z,2021-08-09T15:07:37Z,,,
12474,"b""[Pegasus][tokenizer] pegasus tokenizer doesn't have any BOS token?""",2021-07-01T19:39:37Z,2021-08-09T15:07:38Z,,,
12473,b'Release `utils/style_doc.py` as a python package',2021-07-01T19:37:30Z,2021-09-07T15:02:33Z,,,
12472,b'DistilBERT - Operation reordering for compatibility with TensorRT',2021-07-01T19:07:11Z,2021-08-23T15:06:23Z,,,
12471,b'Rework notebooks and move them to the Notebooks repo',2021-07-01T17:54:06Z,2021-07-02T06:29:52Z,,,
12470,b'[Flax] Dataset streaming example',2021-07-01T17:53:09Z,2021-07-05T14:13:10Z,,,
12469,b'NER example for Tensorflow',2021-07-01T17:16:38Z,2021-07-05T14:42:18Z,,,
12468,b'Add guide on how to build demos for the Flax sprint',2021-07-01T16:30:55Z,2021-07-02T18:35:18Z,,,
12467,b'Import check_inits handling of duplicate definitions.',2021-07-01T16:22:26Z,2021-07-01T16:52:00Z,,,
12466,b'fixed typo in flax-projects readme',2021-07-01T14:39:21Z,2021-07-02T06:57:40Z,,,
12465,b'Added talk details',2021-07-01T14:05:22Z,2021-07-01T14:19:23Z,,,
12464,b'Fix training_args.py barrier for torch_xla',2021-07-01T14:03:01Z,2021-07-01T14:17:38Z,,,
12463,b'Add TPU README',2021-07-01T13:30:35Z,2021-07-01T16:11:54Z,,,
12462,b'Skip ProphetNet test',2021-07-01T13:30:34Z,2021-08-04T16:24:54Z,,,
12461,b'Fixing bug with param count without embeddings',2021-07-01T13:29:25Z,2021-07-01T17:25:40Z,,,
12460,b'Extractive summarization pipeline',2021-07-01T13:02:14Z,,Feature request,,
12459,"b""Fix to keep up with the changes made in fairseq's RobertaModel""",2021-07-01T12:57:49Z,2021-08-08T15:01:48Z,,AttributeError,"AttributeError: 'RobertaHubInterface' object has no attribute 'args'"
12458,"b'[Wav2Vec2, Hubert] Fix ctc loss test'",2021-07-01T11:35:54Z,2021-07-01T12:59:32Z,,,
12457,b'gpt2 causal mask to skip learning context input? (beginner question)',2021-07-01T11:32:24Z,2021-07-01T21:01:37Z,,,
12456,b'convert_graph_to_onnx.py failing to run on Wav2Vec2 models',2021-07-01T10:48:30Z,2021-07-05T08:57:57Z,,,
12455,b'Setting global tokens in BigBirdModel',2021-07-01T10:34:22Z,2021-07-23T12:30:45Z,,,
12454,b'(WIP) Add FNet with flax template',2021-07-01T10:08:23Z,2021-08-08T15:01:49Z,,,
12453,b'Update CANINE test',2021-07-01T07:34:58Z,2021-08-04T07:29:35Z,,,
12452,b'Comment fast GPU TF tests',2021-07-01T07:19:05Z,2021-07-01T13:26:46Z,,,
12451,"b'Wiki content part : XLM-RoBERTa,   ""xlm-roberta-base""'",2021-07-01T06:58:36Z,2021-08-08T15:01:49Z,,,
12450,b'Finetuned model generates incomprehensible text when used while in memory but works fine when loaded via saved checkpoints.',2021-07-01T06:00:08Z,2021-08-08T15:01:50Z,,,
12449,b'Pass `model_kwargs` when loading a model in `pipeline()`',2021-07-01T02:31:32Z,2021-07-09T13:24:56Z,,,
12448,b'Instantiating a model from `pipeline()` ignores `model_kwargs` parameter',2021-07-01T02:30:54Z,2021-07-09T13:24:55Z,,,
12447,b'[Flax community event] How to use hub during training',2021-06-30T18:43:34Z,2021-07-01T10:41:22Z,,,
12446,b'[roberta] fix lm_head.decoder.weight ignore_key handling',2021-06-30T18:34:37Z,2021-07-01T17:31:19Z,,,
12445,b'Deit',2021-06-30T18:11:51Z,2021-08-09T15:07:40Z,,,
12444,b'Cannot load model saved with AutoModelForMaskedLM.from_pretrained if state_dict = True',2021-06-30T16:44:49Z,2021-08-10T15:03:00Z,,AttributeError,"AttributeError: 'bool' object has no attribute 'keys'"
12443,b'[Wav2Vec2] Better names for internal classes',2021-06-30T16:28:39Z,,WIP,,
12442,b'Add to talks section',2021-06-30T14:40:10Z,2021-06-30T14:58:03Z,,,
12441,b'Add template for adding flax models',2021-06-30T13:13:55Z,2021-09-01T07:49:04Z,,,
12440,b'cookiecutter template for adding flax model',2021-06-30T12:42:41Z,2021-09-01T07:49:04Z,,,
12439,"b'Expand text-generation pipeline support for other causal models e.g., BigBirdForCausalLM'",2021-06-30T12:31:45Z,,Feature request,,
12438,"b'IndexError: index out of bound, MLM+XLA (pre-training)'",2021-06-30T12:22:31Z,2021-07-08T11:23:41Z,,"IndexError, RuntimeError","IndexError: index out of boundsRuntimeError: Default process group has not been initialized, please make sure to call init_process_group."
12437,b'Add test for a WordLevel tokenizer model',2021-06-30T11:26:15Z,2021-07-01T10:37:07Z,,,
12436,b'Add DEBERTA-base model for usage in EncoderDecoderModel.',2021-06-30T11:18:45Z,,Feature request,,
12435,b'Using huggingface Pipeline in industry',2021-06-30T11:16:36Z,2021-07-06T07:39:26Z,,,
12434,b'TPU not initialized when running official `run_mlm_flax.py` example.',2021-06-30T10:19:15Z,2021-07-13T21:27:05Z,,,
12433,b'Added to talks section',2021-06-30T09:42:01Z,2021-06-30T11:14:12Z,,,
12432,b'fix typo in mt5 configuration docstring',2021-06-30T09:37:30Z,2021-06-30T14:24:06Z,,,
12431,b'how to continue pre-train custom data',2021-06-30T08:51:08Z,2021-08-08T15:01:52Z,,,
12430,b'Distilling zero-shot classification: Assertion `srcIndex < srcSelectDimSize` failed.',2021-06-30T08:37:19Z,2021-08-17T15:01:59Z,,,
12429,b'Add default bos_token and eos_token for tokenizer of deberta_v2',2021-06-30T07:52:35Z,2021-06-30T12:03:58Z,,,
12428,b'[DeBerta V2] The vocab size of DeBerta V2 is incorrect',2021-06-30T07:01:08Z,2021-07-07T11:17:50Z,,,
12427,b'[DeepSpeed] Convert from fp16 to fp32 issue zero_to_fp32.py',2021-06-30T06:47:50Z,2021-08-08T15:01:53Z,,RuntimeError,"RuntimeError: start (2499259392) + length (16777216) exceeds dimension size (2499738752)."
12426,b'[roberta] lm_head.decoder save/load needs fixing',2021-06-30T05:09:43Z,2021-07-01T17:31:19Z,,,
12425,b'Loading custom model',2021-06-30T01:10:14Z,2021-08-08T15:01:54Z,,,
12424,b'Fix default bool in argparser',2021-06-29T21:58:34Z,2021-06-30T11:57:06Z,,,
12423,b'HfArgumentParser defaults booleans to on',2021-06-29T21:43:04Z,2021-06-30T11:57:05Z,,,
12422,b'[modelcard] fix',2021-06-29T21:20:10Z,2021-06-29T21:59:03Z,,,
12421,b'Add option to save on each training node',2021-06-29T19:03:25Z,2021-06-30T06:41:47Z,,,
12420,b'Easily train a new fast tokenizer from a given one - tackle the special tokens format (str or AddedToken)',2021-06-29T17:13:26Z,2021-06-29T18:31:23Z,,,
12419,b'[JAX/Flax readme] add philosophy doc',2021-06-29T17:07:47Z,2021-06-30T16:10:12Z,,,
12418,b'DeepSpeed gets stuck when training',2021-06-29T15:56:13Z,2021-08-18T15:06:28Z,,,
12417,b'Raises an error when BertTokenizer is initialized from BertJapaneseTokenizer',2021-06-29T15:08:20Z,2021-07-09T21:23:35Z,,,
12416,b'BertTokenizer with BertJapaneseTokenizer pretrained model generates unintended tokenization.',2021-06-29T14:57:00Z,2021-07-17T13:52:22Z,,,
12415,b'Added talks',2021-06-29T14:46:04Z,2021-06-29T15:01:17Z,,,
12414,b'Streaming mode in training examples',2021-06-29T14:33:37Z,2021-07-09T08:40:23Z,,,
12413,b'Benchmark Colab does not work',2021-06-29T13:49:24Z,2021-08-06T15:11:54Z,,ValueError,"ValueError: too many values to unpack (expected 2)"
12412,b'fix ids_to_tokens naming error in tokenizer of deberta v2',2021-06-29T11:55:12Z,2021-06-29T12:15:36Z,,,
12411,b'\xf0\x9f\x8c\x9f New model addition: FNet',2021-06-29T11:53:40Z,,New model,,
12410,b'New Model: Charformer: Fast Character Transformers via Gradient-based Subword Tokenization ',2021-06-29T11:15:45Z,,New model,,
12409,b'[Flax] Example scripts - correct weight decay ',2021-06-29T08:36:42Z,2021-06-29T11:01:08Z,,,
12408,b'Wrong logical operation',2021-06-29T05:49:49Z,2021-07-01T03:44:40Z,,,
12407,"b'Validation split added: custom data files @sgugger, @patil-suraj'",2021-06-29T04:48:38Z,2021-07-01T17:22:42Z,,,
12406,b'MLM training fails with no validation file',2021-06-29T04:35:29Z,2021-07-02T01:53:57Z,,,
12405,b'Fix for the issue of device-id getting hardcoded for token_type-ids during Tracing for iBert ',2021-06-29T00:12:49Z,2021-11-03T15:02:20Z,,,
12404,b'[FLAX] Core dump using example code',2021-06-28T21:06:46Z,2021-08-15T15:01:53Z,,,
12403,b'[Deepspeed][initialization] pegasus: unable to load/init the weights',2021-06-28T19:01:55Z,2021-07-10T00:01:05Z,DeepSpeed,ValueError,"ValueError: not enough values to unpack (expected 2, got 1)"
12402,b'[Flax][WIP] added Flax Pegasus Models',2021-06-28T18:52:05Z,2021-09-11T14:40:41Z,WIP,,
12401,b'[Deepspeed] match the trainer log level ',2021-06-28T18:29:27Z,2021-06-28T18:43:25Z,,,
12400,b'[WIP] train tokenizer like test',2021-06-28T16:53:10Z,2021-06-29T19:00:25Z,,,
12399,b'Reference postprocess_qa_predictions score method',2021-06-28T15:57:05Z,2021-08-06T15:11:57Z,,,
12398,b'[Flax community event] Add more description to readme',2021-06-28T15:17:02Z,2021-06-28T16:18:42Z,,,
12397,b'[RoFormer] Fix some issues',2021-06-28T14:52:46Z,2021-07-06T07:31:57Z,,,
12396,b'getting error with BertForMaskedLM',2021-06-28T14:46:35Z,2021-08-06T15:11:58Z,,RuntimeError,"RuntimeError: Caught RuntimeError in replica 0 on device 0."
12395,b'Minor fixes in original RAG training script',2021-06-28T14:18:04Z,2021-06-29T12:39:48Z,,,
12394,"b""Remove the need for `einsum` in Albert's attention computation""",2021-06-28T13:40:29Z,2021-06-28T16:30:06Z,,,
12393,b'[example/flax] add summarization readme',2021-06-28T11:51:42Z,2021-06-29T08:32:33Z,,,
12392,b'GLM Model implementation [WIP]',2021-06-28T11:03:59Z,,WIP,,
12391,b'[Flax] Adapt flax examples to include `push_to_hub`',2021-06-28T11:01:43Z,2021-06-28T18:23:35Z,,,
12390,b'`fill-mask` pipeline provides `<mask>` token among predictions',2021-06-28T09:36:10Z,2021-08-05T15:14:06Z,,,
12389,b'GPT2-large for sequence classification default num_labels differs from the default for GPT2-small and GPT2-medium',2021-06-28T09:19:32Z,2021-09-08T15:45:42Z,,,
12388,b'Onnx export v2 fixes',2021-06-28T07:26:07Z,2021-06-28T07:39:58Z,,,
12387,b'Connot correctly fine-tune Bert for generation',2021-06-28T04:51:41Z,2021-09-07T15:02:37Z,Migration,,
12386,"b""A model or a config like 'transformer_iwslt_de_en' for machine translation""",2021-06-28T04:19:32Z,,New model,,
12385,b'Rework LongFormer to make it compatible with ONNX',2021-06-27T22:41:29Z,,WIP,,
12384,b'Request: New LM Adapted checkpoints for T5',2021-06-27T14:16:37Z,,New model,,
12383,b'Size of tensors not matching even though using tweets (all same length)',2021-06-27T14:11:15Z,2021-08-04T15:06:46Z,,`RuntimeError,"`RuntimeError: The expanded size of the tensor (601) must match the existing size (514) at non-singleton dimension 1.  Target sizes: [1, 601].  Tensor sizes: [1, 514]`"
12382,b'About a error in retrain a xlm model',2021-06-27T08:45:09Z,2021-08-04T15:06:47Z,,,
12381,b'A fast tokenizer for BertJapaneseTokenizer',2021-06-27T08:29:37Z,,Feature request,,
12380,b'Module version identification problem',2021-06-27T02:54:55Z,2021-09-05T15:02:12Z,,,
12379,b'Tracking variables other than loss during training',2021-06-27T00:38:27Z,2021-08-04T15:06:48Z,,,
12378,"b""TypeError: new(): invalid data type 'numpy.str_'""",2021-06-26T21:45:38Z,2021-08-04T15:06:49Z,,,
12377,b'conversion wav2vec2 model from fairseq to huggingface',2021-06-26T16:46:30Z,2021-08-04T15:06:50Z,Migration,,
12376,b'Issue in layer-drop implementation in TensorFlow models in graph mode',2021-06-26T14:21:17Z,2021-08-23T15:06:28Z,,ValueError,"ValueError: in user code:"
12375,b'model.generate occurs error:  generation_beam_search',2021-06-26T14:12:06Z,2021-08-03T15:06:47Z,,"/opt/conda/conda-bld/pytorch_1587428207430/work/aten/src/ATen/native/cuda/MultinomialKernel.cu:87, RuntimeError","/opt/conda/conda-bld/pytorch_1587428207430/work/aten/src/ATen/native/cuda/MultinomialKernel.cu:87: binarySearchForMultinomial: block: [39,0,0], thread: [0,3,0] AssRuntimeError: CUDA error: device-side assert triggered"
12374,"b""ImportError: cannot import name 'BertEncoder' from 'transformers'""",2021-06-26T12:17:48Z,2021-06-28T08:03:48Z,,ImportError,"ImportError: cannot import name 'BertEncoder' from 'transformers' (unknown location)"
12373,b'Added .lower() method to label',2021-06-26T12:05:06Z,2021-06-28T18:07:03Z,,,
12372,b'Wav2vec2 Dataset',2021-06-26T11:17:56Z,2021-08-03T15:06:48Z,,,
12371,b'[Documentation] Warn that DataCollatorForWholeWordMask is limited to BertTokenizer-like tokenizers',2021-06-26T10:46:08Z,2021-06-28T11:39:56Z,,,
12370,b'[WIP] DataCollatorForTextInfilling',2021-06-26T10:02:58Z,2021-08-30T15:07:57Z,,,
12369,b'[Trainer.py] when --load_best_model_at_end is set in Distributed Training',2021-06-26T08:37:39Z,2021-06-26T13:32:04Z,,,
12368,b'[Examples] Replace `print` statement with `logger.info` in QA example utils',2021-06-26T08:05:41Z,2021-06-26T16:31:25Z,,,
12367,b'[Examples] Added context manager to datasets map',2021-06-26T06:54:44Z,2021-06-28T16:14:00Z,,,
12366,b'Tokens Jumbling',2021-06-26T06:27:47Z,2021-08-03T15:06:48Z,,,
12365,b'[Examples] Update Example Template for `--log_level` feature',2021-06-26T03:38:04Z,2021-06-26T03:50:30Z,,,
12364,b'[CI] add dependency table sync verification',2021-06-25T23:42:19Z,2021-06-28T15:55:59Z,,,
12363,b'[examples] add `main_process_first` context manager to datasets map calls',2021-06-25T22:29:26Z,2021-06-28T16:14:00Z,Good First Issue,,
12362,b'fixed multiplechoice tokenization',2021-06-25T20:48:02Z,2021-06-25T21:41:08Z,,,
12361,b'Easily train a new fast tokenizer from a given one',2021-06-25T20:22:36Z,2021-06-29T19:00:08Z,,,
12360,b'[examples] remove extra white space from log format',2021-06-25T19:49:15Z,2021-06-25T20:20:16Z,,,
12359,b'[Examples] Replicates the new --log_level feature to all trainer-based pytorch',2021-06-25T19:01:59Z,2021-06-25T21:58:42Z,,,
12358,b'Tensorflow LM examples',2021-06-25T18:13:12Z,2021-06-28T18:31:44Z,,,
12357,b'Replace NotebookProgressReporter by ProgressReporter in Ray Tune run',2021-06-25T16:34:04Z,2021-06-25T18:12:06Z,,,
12356,b'Fixed a typo in readme',2021-06-25T11:40:51Z,2021-06-25T11:49:30Z,,,
12355,b'[Flax] Add T5 pretraining script',2021-06-25T11:07:50Z,2021-06-28T19:11:29Z,,,
12354,b'Input structure has type class tuple while shallow structure has type class transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput',2021-06-25T08:17:03Z,2021-08-02T15:06:35Z,,TypeError,"TypeError: The two structures don't have the same sequence type. Input structure has type <class 'tuple'>, while shallow structure has type <class 'transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput'>."
12353,b'ERROR: Failed building wheel for tokenizers',2021-06-25T06:32:42Z,2021-06-28T11:42:03Z,,`ERROR,"`ERROR: Could not build wheels for tokenizers which use PEP 517 and cannot be installed directly`"
12352,b'[WIP][FIX] Prevent output some config files when using fast tokenizer',2021-06-25T03:49:54Z,2021-07-26T15:00:47Z,,,
12351,b'[trainer] add main_process_first context manager',2021-06-25T03:13:04Z,2021-06-25T21:58:04Z,,,
12350,b'Fix exception in prediction loop occurring for certain batch sizes',2021-06-25T00:17:17Z,2021-06-25T14:55:15Z,,,
12349,b'Prediction fails for certain batch sizes',2021-06-25T00:14:26Z,2021-06-25T14:55:15Z,,RuntimeError,"RuntimeError: zero-dimensional tensor (at position 0) cannot be concatenated"
12348,b'Generate text until condition',2021-06-24T23:36:15Z,,WIP,,
12347,"b""TypeError: __init__() got an unexpected keyword argument 'report_to'""",2021-06-24T22:54:00Z,2021-06-25T13:29:15Z,,TypeError,"TypeError: __init__() got an unexpected keyword argument 'report_to'"
12346,"b'All evaluation processes overload one GPU, when other 7 are available. While Training process fine and is distributed across all 8 cards'",2021-06-24T22:37:36Z,2021-08-02T15:06:36Z,,RuntimeError,"RuntimeError: CUDA out of memory. "
12345,b'[examples] [distributed] process datasets.map only on main process in ',2021-06-24T19:57:07Z,2021-06-25T21:58:03Z,,,
12344,b'Update run_mlm.py',2021-06-24T19:56:05Z,2021-06-28T11:49:22Z,,,
12343,b'[trainer] fix label smoothing for default compute_loss',2021-06-24T11:59:59Z,2021-06-24T13:01:09Z,,,
12342,b'Add flax/jax quickstart',2021-06-24T10:40:00Z,2021-06-24T16:04:18Z,,,
12341,b'[examples/Flax] move the examples table up',2021-06-24T10:33:05Z,2021-06-24T10:33:37Z,,,
12340,b'[Flax] Move up examples',2021-06-24T10:32:20Z,2021-06-24T10:35:12Z,,,
12339,b'How to get offset mapping then decoding wav2vec?',2021-06-24T10:15:09Z,2021-08-01T15:02:01Z,,,
12338,b'[ray] try fixing import error',2021-06-24T07:51:42Z,2021-06-24T08:13:18Z,,,
12337,b'ValueError: expected sequence of length 133 at dim 1 (got 80)',2021-06-24T06:53:01Z,2021-08-01T15:02:02Z,,ValueError,"ValueError: expected sequence of length 133 at dim 1 (got 80)"
12336,b'Fix torchscript tests',2021-06-24T05:31:22Z,2021-06-24T13:52:28Z,,,
12335,b'[WIP] FNet',2021-06-24T03:18:14Z,2021-08-06T15:12:01Z,,,
12334,b'Add additional variables without shape',2021-06-24T00:56:07Z,2021-08-01T15:02:03Z,,,
12333,b'Missing tokenizer_class for `mbart-large-50-many-to-one-mmt` model',2021-06-24T00:38:53Z,2021-06-24T07:39:56Z,,,
12332,b'Cast logits from bf16 to fp32 at the end of TF_T5',2021-06-23T21:56:49Z,2021-08-03T19:02:59Z,,tensorflow.python.framework.errors_impl.InvalidArgumentError,"tensorflow.python.framework.errors_impl.InvalidArgumentError: cannot compute Mul as input #1(zero-based) was expected to be a bfloat16 tensor but is a float tensor [Op:Mul]"
12331,b'Default Parameters for training DistillBERT and DistillGPT2',2021-06-23T21:50:43Z,2021-08-01T15:02:05Z,,,
12330,b'Fixing the pipeline optimization by reindexing targets (V2)',2021-06-23T16:51:40Z,2021-07-08T14:58:16Z,,,
12329,b'Fixing the pipeline optimization by rescaling the logits first.',2021-06-23T16:39:13Z,2021-07-07T13:25:57Z,,,
12328,b'UpdateDescription of TrainingArgs param save_strategy',2021-06-23T16:26:17Z,2021-06-23T17:39:44Z,,,
12327,b'[Flax T5] Fix weight initialization and fix docs',2021-06-23T16:01:27Z,2021-06-23T16:39:22Z,,,
12326,b'Changed modeling_fx_utils.py to utils/fx.py for clarity',2021-06-23T15:54:07Z,2021-06-23T16:16:25Z,,,
12325,b'How to assign gpu when using run_language_modeling.py',2021-06-23T14:37:59Z,2021-08-01T15:02:06Z,,,
12324,b'fill-mask pipeline: fix handling topk() indices',2021-06-23T14:23:51Z,2021-07-08T15:00:59Z,,,
12323,b'Conda build',2021-06-23T13:35:21Z,2021-06-23T15:07:08Z,,,
12322,b'Generate text with `model.generate` on TPU does not work',2021-06-23T13:01:37Z,2021-09-22T15:02:25Z,WIP,,
12321,b'[Proposal] Image segmentation pipeline',2021-06-23T12:16:19Z,2021-09-03T15:07:27Z,Core: Pipeline,,
12320,b'Add mention of the huggingface_hub methods for offline mode',2021-06-23T08:53:29Z,2021-06-23T13:45:30Z,,,
12319,"b""`fill-mask` pipeline cannot load tokenizer's `config.json` (fixed in 4.8.0)""",2021-06-23T07:44:17Z,2021-06-23T18:18:27Z,,,
12318,b'Downloading the models is getting slower than before',2021-06-23T06:52:25Z,2021-08-02T15:06:38Z,,,
12317,"b""Tokenizer's normalization preprocessor cause misalignment in return_offsets_mapping for tokenizer classification task""",2021-06-23T05:05:38Z,2021-08-02T15:06:38Z,,ValueError,"ValueError: NumPy boolean array indexing assignment cannot assign 88 input values to the 87 output values where the mask is true"
12316,b'[models] respect dtype of the model when instantiating it',2021-06-22T23:45:01Z,2021-06-29T03:11:21Z,,RuntimeError,"RuntimeError: ""LayerNormKernelImpl"" not implemented for 'Half'"
12315,b'Model is saved every eval_steps steps if eval_steps < save_steps. Is this expected behavior?',2021-06-22T21:47:15Z,2021-06-23T17:39:44Z,,,
12314,b'Add all XxxPreTrainedModel to the main init',2021-06-22T21:27:41Z,2021-06-23T14:40:55Z,,,
12313,b'FlaxBartPretrainedModel -> FlaxBartPreTrainedModel',2021-06-22T20:16:17Z,2021-06-22T20:37:05Z,,,
12312,b'Add possibility to maintain full copies of files',2021-06-22T19:01:31Z,2021-06-28T14:02:54Z,,,
12311,b'[Flax/JAX] Add how to propose projects markdown',2021-06-22T17:54:13Z,2021-06-23T13:50:36Z,,,
12310,"b""New `--log_level` feature introduces failures using 'passive' mode""",2021-06-22T17:43:16Z,2021-06-22T18:13:23Z,,ValueError,"ValueError: Unknown level: 'passive'"
12309,b'[trainer] 2 bug fixes and a rename',2021-06-22T16:27:19Z,2021-06-22T18:13:24Z,,,
12308,b'odd whitespace handling with imported sentencepiece models',2021-06-22T16:07:04Z,2021-08-16T15:02:11Z,Core: Tokenization,,
12307,b'Tokenizing in the dataset and padding manually using tokenizer.pad in the collator',2021-06-22T15:44:24Z,2021-09-09T15:02:25Z,,">ValueError, ValueError",">ValueError: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length.ValueError: Caught ValueError in DataLoader worker process 0."
12306,b'Dimensional weight error',2021-06-22T14:16:57Z,2021-06-22T15:37:42Z,,RuntimeError,"RuntimeError: Expected 3-dimensional input for 3-dimensional weight [512, 1, 10], but got 4-dimensional input of size [1, 1, 1, 43200] instead"
12305,b'[Flax] Main doc for event orga',2021-06-22T13:59:31Z,2021-06-22T17:02:52Z,,,
12304,b'Add CodeCarbon Integration',2021-06-22T12:39:56Z,2021-06-23T06:53:09Z,,,
12303,b'Fix and improve documentation for LEDForConditionalGeneration',2021-06-22T12:26:27Z,2021-06-22T13:58:14Z,,,
12302,b'electra SequenceClassification layer change',2021-06-22T11:27:53Z,2021-07-30T15:07:14Z,,,
12301,b'Are there any examples showing how to use the `metric_for_best_model` of class `TrainingArguments`?',2021-06-22T09:50:42Z,2021-06-23T01:55:43Z,,,
12300,b'[Flax] ViT training example',2021-06-22T09:23:25Z,2021-07-05T12:53:03Z,,,
12299,b'T ** 2 in distillation process',2021-06-22T08:04:39Z,2021-07-30T15:07:15Z,,,
12298,b'add FlaxAutoModelForImageClassification in main init',2021-06-22T06:35:59Z,2021-06-22T12:56:05Z,,,
12297,b'Error: while executing run_qa.py from examples/pytorch/question-answering/  directory',2021-06-22T06:00:35Z,2021-06-22T13:55:54Z,,,
12296,b'BART infilling example? ',2021-06-22T05:39:19Z,2021-07-30T15:07:16Z,,,
12295,b'[examples] replicate the new `--log_level` feature to all trainer-based pytorch examples',2021-06-22T02:51:20Z,2021-06-25T21:58:42Z,Good First Issue,,
12294,b'[tests] multiple improvements',2021-06-22T00:09:18Z,2021-06-22T02:51:37Z,,,
12293,"b'[tests] reset report_to to none, avoid deprecation warning'",2021-06-21T23:22:24Z,2021-06-21T23:50:12Z,,,
12292,b'Fix for the issue of device-id getting hardcoded for position-ids during Tracing for Flaubert',2021-06-21T21:09:33Z,2021-09-01T08:46:58Z,WIP,,
12291,b'Trainer: adjust wandb installation example',2021-06-21T20:52:48Z,2021-06-22T12:47:31Z,,,
12290,b'Fix for the issue of device-id getting hardcoded for position-ids during Tracing for Distillbert',2021-06-21T19:21:35Z,2021-09-01T08:47:25Z,WIP,,
12289,b'Fix TFWav2Vec2 SpecAugment',2021-06-21T18:18:39Z,2021-06-29T08:15:57Z,,,
12288,b'Add out of vocabulary error to ASR models',2021-06-21T18:12:21Z,2021-06-29T07:57:46Z,,,
12287,b'Fix for the issue of device-id getting hardcoded for token_type_ids during Tracing for ConvBert',2021-06-21T18:07:44Z,2021-09-01T08:47:58Z,WIP,,
12286,b'Memory leak when using DistilBert for inference to extract [CLS] hidden state',2021-06-21T17:10:32Z,2021-06-22T21:12:39Z,,,
12285,b'[WIP][Flax] CLIP training example',2021-06-21T14:53:09Z,2021-07-03T15:04:51Z,,,
12284,b'[FlaxClip] fix test from/save pretrained test',2021-06-21T13:13:39Z,2021-06-21T14:54:34Z,,,
12283,b'[TFWav2Vec2] Fix docs',2021-06-21T10:08:10Z,2021-06-23T13:51:31Z,,,
12282,b'TFWav2Vec2ForCTC: Error when using padded batch and attention mask',2021-06-21T09:41:08Z,2021-07-23T09:50:37Z,,InvalidArgumentError,"InvalidArgumentError: Incompatible shapes: [1,547,768] vs. [1,544,1] [Op:Mul]"
12281,b'[WIP] Cogview',2021-06-21T07:26:40Z,2021-07-29T15:07:13Z,,,
12280,b'Rename detr targets to labels',2021-06-21T07:21:31Z,2021-06-29T07:07:47Z,,,
12279,b'Better CI feedback',2021-06-21T06:51:56Z,2021-06-21T06:52:12Z,,,
12278,"b'ViTFeatureExtractor.save_pretrained() generate ""preprocessor_config.json"" but not ""config.json""'",2021-06-21T03:35:04Z,2021-07-29T15:07:14Z,,,
12277,b'Update feature_extraction_utils.py',2021-06-21T03:14:03Z,2021-08-25T15:06:34Z,,,
12276,b'[trainer + examples] set log level from CLI',2021-06-21T02:36:43Z,2021-06-22T02:30:50Z,,,
12275,b'Transformers-CLI not saving pytorch model after conversion',2021-06-20T23:51:46Z,2021-07-31T15:01:53Z,,FileNotFoundError,"FileNotFoundError: [Errno 2] No such file or directory: 'pytorch/pytorch_model.bin'"
12274,b'[performance] module init w/ `from_pretrained` skip storage allocation',2021-06-20T15:52:49Z,,WIP,,
12273,b'[Deepspeed] [performance] inefficient load with `from_pretrained` w/ zero3',2021-06-20T15:49:30Z,,"DeepSpeed, WIP",,
12272,b'[Deepspeed zero3] lazy weights init ',2021-06-20T15:45:38Z,,"DeepSpeed, WIP",,
12271,b'[Flax] Add wav2vec2',2021-06-20T11:13:41Z,2021-06-30T17:44:23Z,,,
12270,b'Add error message to Wav2Vec2 & Hubert if labels > vocab_size',2021-06-20T10:49:02Z,2021-09-20T16:26:19Z,Good First Issue,,
12269,b'Add TFSpeech2Text',2021-06-20T10:48:58Z,,WIP,,
12268,b'[Documentation] Example for LEDForConditionalGeneration does not work',2021-06-20T09:43:02Z,2021-06-22T13:58:14Z,,,
12267,b'[WIP] Enable GPT2Model to handle 3d attention_mask',2021-06-20T07:58:21Z,2021-08-09T15:07:51Z,,,
12266,b'Causal Mask in BertGeneration',2021-06-20T06:44:39Z,2021-08-23T15:06:32Z,,,
12265,b'Mbart continue training with same training task on a specific language',2021-06-20T01:52:28Z,2021-06-21T08:18:24Z,,,
12264,b'TFWav2Vec2ForCTC & Wav2Vec2ForCTC gives different loss values',2021-06-19T20:01:39Z,2021-07-14T10:24:02Z,,,
12263,b'Add VisualBERT demo notebook',2021-06-19T19:24:22Z,2021-08-11T14:10:59Z,WIP,,
12262,b'[WIP] SMITH',2021-06-19T19:12:17Z,,WIP,,
12261,b'GPT2Model cannot handle 3D attention_mask',2021-06-19T07:57:40Z,2021-07-28T15:07:11Z,,,
12260,b'353 duplicate tokens in GPT-2?',2021-06-18T22:43:31Z,2021-06-18T23:07:17Z,,,
12259,b'`ValueError: Expected input batch_size to match target batch_size` occurs when training GPT2 with `Seq2SeqTrainer`',2021-06-18T19:08:57Z,2021-06-21T17:28:26Z,,ValueError,"ValueError: Caught ValueError in replica 0 on device 0."
12258,b'[docs]  performance ',2021-06-18T18:49:21Z,2021-06-22T22:34:19Z,,,
12257,"b""[DeepSpeed] don't ignore --adafactor""",2021-06-18T18:01:35Z,2021-06-21T15:17:01Z,,,
12256,b'[Flax] Fix flax test save pretrained',2021-06-18T17:48:50Z,2021-06-21T15:37:14Z,,,
12255,b'[Flax] [WIP] allow loading head model with base model weights',2021-06-18T17:31:49Z,2021-06-21T14:56:43Z,Flax,,
12254,b'[Documentation Example] Task Summary - Start/End of Span in QA Example',2021-06-18T16:06:24Z,2021-07-28T15:07:11Z,,,
12253,b'First hidden state of the last layer of Bert (french version : FlauBert) only prints vectors of 0 or -0 after using it !!',2021-06-18T15:36:46Z,2021-07-28T15:07:12Z,,,
12252,b'Tensorflow QA example',2021-06-18T14:22:44Z,2021-06-21T15:37:28Z,,,
12251,b'[Flax] Add jax flax to env command',2021-06-18T14:15:21Z,2021-06-21T16:12:12Z,,,
12250,b'RAG with T5 in a multitask setting',2021-06-18T14:14:34Z,2021-07-30T15:07:19Z,,,
12249,b'Got unexpected result when using BertTokenizer in Chinese',2021-06-18T12:50:29Z,2021-07-26T15:06:25Z,,,
12248,b'finding a bug in training the code of /src/transformers/models/detr/modeling_detr.py',2021-06-18T08:36:55Z,2021-06-29T07:07:47Z,,,
12247,b'[FlaxBart] few small fixes',2021-06-18T08:31:45Z,2021-06-18T09:29:42Z,,,
12246,b'Different Weights between google-bert (uncased_L-12_H-768_A-12) and Huggingface-bert (bert-base-uncased)',2021-06-18T08:02:46Z,2021-06-22T08:27:56Z,,,
12245,"b""TFBertForMaskedLM won't reload from saved checkpoint, shape mismatch issue""",2021-06-18T06:50:51Z,2021-07-26T15:06:26Z,,ValueError,"ValueError: Tensor's shape (512, 768) is not compatible with supplied shape [2, 768]"
12244,"b'RoFormerTokenizerFast has a wrong result when setting ""return_offsets_mapping=True""'",2021-06-18T06:01:08Z,2021-07-06T07:31:57Z,,,
12243,b'GPT-J',2021-06-18T05:16:15Z,2021-08-31T01:37:42Z,,,
12242,"b""Can't load tokenizer for 'imxly/t5-pegasus'.""",2021-06-18T03:11:24Z,2021-07-27T15:02:04Z,,,
12241,b'Modify BERT encoder layers?',2021-06-18T02:40:01Z,,New model,,
12240,b'Depreciate pythonic Mish and support PyTorch 1.9 version of Mish',2021-06-18T00:39:54Z,2021-06-18T13:13:46Z,,,
12239,b'[t5 doc] make the example work out of the box',2021-06-17T23:22:45Z,2021-06-18T17:00:20Z,,,
12238,b'[doc] t5 incomplete example',2021-06-17T23:12:50Z,2021-06-18T17:00:20Z,,,
12237,"b""BART fine-tuning doesn't work and produces a fixed output for each input""",2021-06-17T23:06:48Z,2021-08-08T15:02:01Z,,,
12236,b'[Flax] Add FlaxMBart',2021-06-17T21:55:07Z,2021-07-07T06:50:38Z,,,
12235,b'can predict_with_generate (do_eval) work with sharded_ddp fairscale in 4.6.1+?',2021-06-17T20:24:35Z,2021-08-12T15:06:37Z,,,
12234,b'Reconstructing Tokens from Bert Embedding?',2021-06-17T19:08:33Z,2021-06-24T16:23:55Z,,,
12233,b'Add FlaxBigBird QuestionAnswering script',2021-06-17T18:45:13Z,2021-06-25T17:05:49Z,,,
12232,b'RobertaForMaskedLM.from_pretrained throwing some weights not initialized error when loading same model type',2021-06-17T18:20:23Z,2021-06-18T16:30:29Z,,,
12231,b'Batch inference runtime slows down for inputs with different length sentences',2021-06-17T17:25:08Z,2021-09-21T15:02:31Z,,,
12230,b'Flax summarization script ',2021-06-17T16:45:10Z,2021-06-23T10:19:31Z,Flax,,
12229,b'Add link to the course',2021-06-17T15:12:05Z,2021-06-17T15:14:53Z,,,
12228,b'[Flax] FlaxAutoModelForSeq2SeqLM',2021-06-17T14:56:36Z,2021-06-18T07:50:10Z,,,
12227,b'[Blenderbot] Fix docs',2021-06-17T12:07:35Z,2021-07-13T13:17:31Z,,,
12226,b'update desc for map in all examples',2021-06-17T11:57:19Z,2021-06-17T19:37:32Z,,,
12225,b'Pegasus pretraining in fp16 results in NaN loss',2021-06-17T10:27:02Z,2021-07-14T07:51:08Z,,,
12224,b'Support for torch 1.9.0',2021-06-17T10:00:30Z,2021-06-17T15:29:01Z,,ImportError,"ImportError: Found an incompatible version of torch. Found version 1.9.0, but only version 1.8 is supported."
12223,b'Argument `never_split` not working on `AutoTokenizer`',2021-06-17T09:53:29Z,2021-07-25T15:01:56Z,,,
12222,b'[WIP] enabling `inference_mode` for pipelines for potentially improved perf.',2021-06-17T08:55:25Z,2021-08-08T15:02:02Z,,,
12221,b'Tokenizer encoding skips \xef\xbf\xbd character',2021-06-17T08:48:39Z,2021-06-17T15:33:18Z,,,
12220,b'[Trainer.py] tr_loss in trainer with distributed training',2021-06-17T08:29:27Z,2021-06-17T11:55:02Z,,,
12219,b'Enabling users to provide their own `stopping_criteria` + `logits_processor` to `generate`.',2021-06-17T07:27:46Z,2021-12-27T10:11:45Z,,,
12218,b'T5 model seq2seq text generation using word embeddings instead of token_ids does not work',2021-06-17T06:27:27Z,2021-07-25T15:01:57Z,,,
12217,b'fix pt-1.9.0 `add_` deprecation',2021-06-17T03:39:41Z,2021-06-17T15:54:00Z,,,
12216,b'Fix blenderbot checkpoint convert codes.',2021-06-17T00:18:16Z,2021-06-17T00:23:49Z,,,
12215,b'Missing PredictionHeadTransform for BertGenerationDecoder',2021-06-16T23:37:41Z,2021-09-16T15:02:10Z,,,
12214,b'Getting 404 Client Error when loading BaptisteDoyen/camembert-base-xnli',2021-06-16T22:39:13Z,2021-07-30T15:07:21Z,,,
12213,"b'[Question] When pretraining a language model, can I choose to mask specific words?'",2021-06-16T22:37:44Z,2021-07-25T15:01:59Z,,,
12212,b'Clearer indication for overridden method in generation',2021-06-16T21:31:49Z,,WIP,,
12211,b'[WIP] tweak model repo saving',2021-06-16T21:28:45Z,2021-07-25T15:01:59Z,,,
12210,b'Better documentation for generation parameter defaults',2021-06-16T21:09:14Z,,WIP,,
12209,b'The kernel appears to have died. It will restart automatically. from transformers import pipeline',2021-06-16T20:19:17Z,2021-07-25T15:02:00Z,,,
12208,b'AutoTokenizer: infer the class from the tokenizer config if possible',2021-06-16T20:09:22Z,2021-06-17T16:39:22Z,,,
12207,b'Pipeline update & tests',2021-06-16T19:59:32Z,2021-06-17T07:41:16Z,,,
12206,b'Add TFHubertModel',2021-06-16T19:43:56Z,2021-07-09T17:55:26Z,,,
12205,b'[Docs] fixed broken link',2021-06-16T17:41:30Z,2021-06-16T19:14:53Z,,,
12204,b'(#12203) Fix blenderbot checkpoint convert codes.',2021-06-16T17:03:04Z,2021-06-17T00:17:52Z,,,
12203,b'blenderbot checkpoint convert script has bug.',2021-06-16T17:01:53Z,2021-06-17T01:09:28Z,,,
12202,b'Training in google colab with TPU using TFTrainer fails with ',2021-06-16T14:43:58Z,2021-08-23T15:06:34Z,,,
12201,b'ValueError: char_to_token() is not available when using Python based tokenizers ; XLNetTokenizer and encodings.char_to_token bug ;',2021-06-16T14:23:32Z,2021-06-26T12:39:09Z,,ValueError,"ValueError: char_to_token() is not available when using Python based tokenizers"
12200,b'[Docs] Broken Link in the Benchmarks.rst',2021-06-16T14:18:25Z,2021-06-16T19:14:53Z,,,
12199,b'[WIP] TensorFlow variant of DataCollatorForLanguageModeling.',2021-06-16T13:00:58Z,2021-09-20T15:02:49Z,,,
12198,b'Enabling AutoTokenizer for HubertConfig.',2021-06-16T12:47:38Z,2021-06-16T14:28:46Z,,,
12197,"b""XLM-RoBERTa MLM Trainer not saving 'sentencepiece.bpe.model' file""",2021-06-16T12:38:24Z,2021-06-17T02:18:23Z,,,
12196,b'Where I can find official pretrained weights of SOP in Albert and NSP in Bert?',2021-06-16T12:05:56Z,2021-09-13T15:02:29Z,,,
12195,b'Batched pipeline for NER',2021-06-16T11:49:40Z,2021-06-16T12:43:10Z,,,
12194,b'LayoutXLM not loaded',2021-06-16T10:59:52Z,2021-07-24T15:02:07Z,,KeyError,"KeyError: 'layoutxlm'"
12193,b'Cannot import RobertaPreTrainedModel',2021-06-16T10:49:04Z,2021-06-23T14:40:54Z,,,
12192,b'Marian tatoeba conversion update',2021-06-16T08:21:33Z,2021-09-19T15:02:09Z,,,
12191,b'updated DLC images and sample notebooks',2021-06-16T08:17:41Z,2021-06-16T11:24:00Z,,,
12190,b'How to figure out which pretrained tokenizers support emojis?',2021-06-16T06:55:21Z,2021-07-14T14:45:32Z,,,
12189,b'T5 Generate from Encoder Output',2021-06-16T05:11:33Z,2021-07-24T15:02:08Z,,,
12188,b'TextDatasetForNextSentencePrediction does not seem to contain truncate function unlike LineByLineWithSOPTextDataset',2021-06-16T01:25:58Z,2021-07-24T15:02:09Z,,RuntimeError,"RuntimeError: The size of tensor a (555) must match the size of tensor b (512) at non-singleton dimension 1"
12187,b'Clean push to hub API',2021-06-16T00:08:05Z,2021-06-23T14:11:19Z,,,
12186,b'[WIP] Flax XLM',2021-06-15T21:09:03Z,2021-07-24T15:02:09Z,,,
12185,b'Use yaml to create metadata',2021-06-15T21:08:13Z,2021-06-16T17:17:45Z,,,
12184,b'Temporarily deactivate torchhub test',2021-06-15T20:06:48Z,2021-06-15T20:16:51Z,,,
12183,b'Inconsistency between GPTNeo and GPT2 config classes',2021-06-15T19:50:59Z,2021-09-16T15:45:29Z,WIP,,
12182,"b""KeyError: 'labels' during Distilling Zero Shot Classification""",2021-06-15T17:36:01Z,,Good Second Issue,KeyError,"KeyError: 'labels'"
12181,b'Temporarily deactivate torch-scatter while we wait for new release',2021-06-15T17:01:15Z,2021-06-15T20:03:58Z,,,
12180,"b""Can't run 124M using transformers""",2021-06-15T16:29:10Z,2021-07-24T15:02:11Z,,,
12179,b'Tensorflow variant of DataCollatorForLanguageModeling.',2021-06-15T15:00:13Z,2021-06-16T12:23:47Z,,,
12178,b'Update AutoModel classes in summarization example',2021-06-15T14:23:04Z,2021-06-15T14:36:11Z,,,
12177,b'Exception during hyperparameter search with Ray and transformers library starting from version 4.5.0',2021-06-15T14:02:20Z,2021-06-15T18:53:20Z,,TypeError,"TypeError: cannot pickle '_thread.RLock' object"
12176,b'Update conversion of Tatoeba marian models',2021-06-15T12:25:46Z,2021-06-15T12:47:14Z,,,
12175,b'TPU training is stuck using T5 with PyTorch Lightning',2021-06-15T11:50:04Z,2021-07-26T15:06:30Z,,,
12174,b'Pretrained XLM model with TLM objective generates nonsensical predictions',2021-06-15T11:49:18Z,2021-07-23T15:02:46Z,,,
12173,b'Use a released version of optax rather than installing from Git.',2021-06-15T10:50:58Z,2021-06-15T11:12:51Z,,,
12172,b'How can we modify the MM-IMDB model for sequence to sequence generation tasks?',2021-06-15T10:31:08Z,2021-07-23T15:02:47Z,,,
12171,b'[Flax generate] Add params to generate',2021-06-15T09:56:47Z,2021-06-15T10:50:12Z,,,
12170,b'Vision Transformer (ViT) feature vector example (not classification)',2021-06-15T09:33:36Z,2021-06-15T10:07:40Z,,,
12169,b'Allow setting permissions of downloaded models (via envvar)',2021-06-15T08:38:06Z,2021-06-16T07:42:30Z,Feature request,,
12168,b'Special tokens not tokenized properly',2021-06-15T07:32:25Z,2021-07-05T19:25:39Z,,,
12167,b'ViT for resolution beyond 224x224 support',2021-06-15T07:17:13Z,2021-06-15T10:45:51Z,,,
12166,b'[testing] ensure concurrent pytest workers use a unique port for torch.dist',2021-06-15T04:22:03Z,2021-06-15T18:13:00Z,,,
12165,b'Documentation for tiny-gpt2 in transformers/examples/pytorch',2021-06-15T01:43:36Z,2021-06-15T01:49:25Z,,,
12164,b'[testing] concurrent dist tests fail when using the same master_port',2021-06-15T01:12:25Z,2021-06-15T18:13:00Z,,,
12163,b'Missing code for predicting custom labels in Bert',2021-06-14T22:22:07Z,2021-07-24T15:02:12Z,,,
12162,b'Add video links to the documentation',2021-06-14T20:50:04Z,2021-06-15T10:37:37Z,,,
12161,b'consistent nn. and nn.functional: part 5 docs',2021-06-14T19:27:41Z,2021-06-14T20:34:33Z,,,
12160,"b""[Jax Slow Circle CI] Don't close PR""",2021-06-14T19:20:53Z,2021-07-13T13:18:04Z,,,
12159,"b""Can't run QA fine-tune for bert/albert in distributed way""",2021-06-14T19:15:39Z,2021-06-14T23:15:28Z,,"RuntimeError, subprocess.CalledProcessError","RuntimeError:     Tensors must be non-overlapping and densereturn type(tensor)(distributed_concat(t, num_total_examples) for t in tensor)subprocess.CalledProcessError: Command '['/opt/conda/bin/python', '-u', 'run_qa.py', '--local_rank=7', '--model_name_or_path', 'bert-large-uncased-whole-word-masking', '--dataset_name', 'squad', '--do_train', '--do_eval', '--learning_rate', '3e-5', '--num_train_epochs', '1', '--max_seq_length', '384', '--doc_stride', '128', '--output_dir', './new_out', '--max_steps', '100', '--per_device_eval_batch_size=3', '--per_device_train_batch_size=3', '--cache_dir', '.']' returned non-zero exit status 1."
12158,b'Pretraining for TFWav2Vec2',2021-06-14T19:10:54Z,2021-06-21T11:38:27Z,,,
12157,b'Add course banner',2021-06-14T18:40:49Z,2021-06-15T13:25:50Z,,,
12156,b'[style] consistent nn. and nn.functional: part 4 `examples`',2021-06-14T18:17:22Z,2021-06-14T19:28:25Z,,,
12155,b'[style] consistent nn. and nn.functional: part 3 `tests`',2021-06-14T18:09:10Z,2021-06-14T19:18:23Z,,,
12154,b'[Flax] Fix flax pt equivalence tests',2021-06-14T18:08:07Z,2021-06-14T18:19:10Z,,,
12153,b'[style] consistent nn. and nn.functional: part2: templates',2021-06-14T17:39:47Z,2021-06-14T18:41:24Z,,,
12152,b'\xf0\x9f\xa4\x97 The Hugging Face Course is out!',2021-06-14T16:29:27Z,2021-07-23T15:02:48Z,,,
12151,b'do_normalize set to True by default for WAV2VEC tokenizer',2021-06-14T14:28:53Z,2021-07-22T15:06:35Z,,,
12150,b'Flax T5',2021-06-14T13:36:01Z,2021-06-23T12:13:32Z,,,
12149,b'Feature request for encoding more than one pair of texts',2021-06-14T11:39:25Z,,Feature request,,
12148,b'[Flax] fix error message',2021-06-14T10:31:52Z,2021-06-14T13:12:18Z,,,
12147,b'Improve detr',2021-06-14T10:05:04Z,2021-06-17T14:37:54Z,,,
12146,b'[Flax] Add links to google colabs',2021-06-14T09:59:29Z,2021-06-14T10:00:29Z,,,
12145,b'Have dummy processors have a `from_pretrained` method',2021-06-14T09:10:35Z,2021-06-15T12:39:06Z,,"AttributeError, ImportError","AttributeError: type object 'Speech2TextProcessor' has no attribute 'from_pretrained'ImportError: "
12144,b'How to train the new wav2vec unsupervised model using hugging face ?',2021-06-14T04:53:39Z,,Feature request,,
12143,b'Ouput Includes Input',2021-06-14T01:57:42Z,2021-06-15T03:26:12Z,,,
12142,b'CLIP tokenizer inconsistent with OpenAI release',2021-06-13T18:40:54Z,2021-08-16T15:02:19Z,,,
12141,b'RuntimeError: Could not infer dtype of numpy.int64 on Squad T5',2021-06-13T14:48:41Z,2021-07-21T15:06:40Z,,RuntimeError,"RuntimeError: Could not infer dtype of numpy.int64"
12140,b'[FLAX] port GPTNeo to Flax',2021-06-13T14:05:16Z,2022-03-11T14:45:08Z,New model,,
12139,b'Add output in a dictionary for TF `generate` method',2021-06-13T11:43:44Z,2021-06-23T09:52:11Z,,,
12138,b'Using checkpoints in gpt neo xl',2021-06-13T10:21:32Z,2021-07-21T15:06:41Z,,,
12137,b'wav2vec2 not converging when finetuning',2021-06-13T10:16:57Z,2021-06-13T11:54:59Z,,,
12136,b'Fix t5 error message',2021-06-13T09:00:41Z,2021-06-13T11:02:57Z,,,
12135,b'[lm examples] Replicate --config_overrides addition to other LM examples',2021-06-13T04:36:41Z,2021-06-14T12:12:23Z,,,
12134,b'Ray Tune Integration Updates',2021-06-13T03:13:27Z,2021-06-15T18:11:30Z,,,
12133,b'Adding fastseq support to more recent version of HF transformers',2021-06-12T20:53:10Z,2021-07-21T15:06:41Z,,,
12132,"b'Use text_column_name variable instead of ""text""'",2021-06-12T17:23:09Z,2021-06-14T12:11:13Z,,,
12131,b'[Flax] Add Beam Search',2021-06-12T14:39:26Z,2021-06-16T08:43:54Z,,,
12130,b'Fix for making student ProphetNet for Seq2Seq Distillation',2021-06-12T08:46:48Z,2021-06-21T13:36:45Z,,,
12129,b'TypeError when trying to load pretrained ALBERT model in BertTokenizer ',2021-06-12T08:10:33Z,2021-07-24T15:02:14Z,,TypeError,"TypeError: stat: path should be string, bytes, os.PathLike or integer, not NoneType"
12128,"b""got multiple values for argument 'input_shape'""",2021-06-12T05:50:58Z,2021-07-20T15:07:20Z,,TypeError,"TypeError: __init__() got multiple values for argument 'input_shape'"
12127,b'Multi-GPU training has literally no GPU-Utilization (0%)',2021-06-12T05:00:10Z,2021-06-13T08:03:59Z,,,
12126,b'[Performance] Tracking open Issues and PRs (pytorch transformers)',2021-06-12T03:45:57Z,,"Good First Issue, Performance, Good Difficult Issue",,
12125,b'Correct typo in summary of tasks doc',2021-06-12T00:33:25Z,2021-06-28T03:10:58Z,,,
12124,b'[style] consistent nn. and nn.functional',2021-06-11T23:45:05Z,2021-06-14T16:44:29Z,,,
12123,b'[optim] implement AdafactorSchedule',2021-06-11T21:36:35Z,2021-06-14T16:43:49Z,,,
12122,b'Model card defaults',2021-06-11T21:25:12Z,2021-06-15T20:01:37Z,,,
12121,"b""Don't log anything before logging is setup in examples""",2021-06-11T20:51:57Z,2021-06-14T12:03:33Z,,,
12120,b'ValueError in predict function for ClassificationModel',2021-06-11T20:10:52Z,2021-07-20T15:07:22Z,,ValueError,"ValueError: could not broadcast input array from shape (16,2) into shape (4,2)"
12119,b'Adding ZeroShotImageClassificationPipeline',2021-06-11T19:49:29Z,2022-02-23T08:41:42Z,,,
12118,b'Passing a custom stopping_criteria list to model.generate() yields a multiple value error for that keyword arg',2021-06-11T19:30:57Z,2021-08-13T15:07:56Z,,,
12117,"b""GPT Neo Tokenizers can't change BOS or EOS token""",2021-06-11T18:22:49Z,2021-08-23T15:06:37Z,,,
12116,b'Enable add_prefix_space on run_ner if necessary',2021-06-11T16:37:43Z,2021-06-15T13:33:21Z,,,
12115,b'Hosted inference api keeps returning 400 error',2021-06-11T13:58:43Z,2021-06-21T19:37:44Z,,,
12114,b'Get the loss in LongformerForQuestionAnswering for fine-tuning',2021-06-11T13:28:21Z,2021-07-19T15:07:20Z,,,
12113,b'Optimizing away the `fill-mask` pipeline.',2021-06-11T12:07:35Z,2021-06-23T08:38:05Z,,,
12112,b'How to pass `past_key_values` to GPTNeo model?',2021-06-11T10:48:20Z,2021-06-13T12:25:30Z,,RuntimeError,"RuntimeError: Tensors must have same number of dimensions: got 3 and 4"
12111,b'add readme for flax clm',2021-06-11T10:15:41Z,2021-06-14T09:33:55Z,,,
12110,b'Fix head masking generate tests',2021-06-11T08:02:35Z,2021-06-11T08:04:07Z,,,
12109,b'Why attention mask is -10000 but not * 0?',2021-06-11T06:48:47Z,2021-07-20T15:07:25Z,,,
12108,b'How to access training loss in TrainerCallback?',2021-06-11T04:27:46Z,2021-07-19T15:07:21Z,,,
12107,b'How can I add a CNN layer on top of bert model?',2021-06-11T01:44:01Z,2021-07-19T15:07:22Z,,,
12106,b'Add GPT-J 6B support to the gpt-neo implementation',2021-06-11T00:47:00Z,2021-06-18T07:05:46Z,,,
12105,b'What is the correct way to pass labels to DetrForSegmentation?',2021-06-10T22:15:23Z,2021-06-17T14:37:54Z,,,
12104,b'Issue with mBART50 es-en translation ',2021-06-10T18:19:34Z,,WIP,,
12103,b'ViT tensorflow Implementation',2021-06-10T15:43:36Z,,Feature request,,
12102,b'Appending label2id and id2label to models for inference',2021-06-10T14:13:12Z,2021-06-10T14:25:05Z,,,
12101,b'GPT2 medium config n_ctx is wrong I guess?',2021-06-10T09:46:33Z,2021-06-10T14:06:28Z,,,
12100,"b"" 'Speech2TextProcessor' has no attribute 'from_pretrained'`""",2021-06-10T09:34:12Z,2021-06-15T12:39:06Z,,`AttributeError,"`AttributeError: type object 'Speech2TextProcessor' has no attribute 'from_pretrained'`"
12099,b'FillMaskPipeline very slow when provided with a large `targets`',2021-06-10T09:16:27Z,2021-06-23T08:38:05Z,,,
12098,b'\xf0\x9f\x8c\x9f New model addition - GPT-J-6B',2021-06-10T07:26:01Z,2021-08-31T15:53:02Z,New model,,
12097,b'Add from_pretrained to dummy timm objects',2021-06-10T07:22:38Z,2021-06-11T16:27:10Z,,,
12096,b'DetrFeatureExtractor post_process not rescaling bboxes as expected',2021-06-10T06:11:22Z,2021-06-10T06:57:43Z,,,
12095,b'Continuous training on Fine-tuned Model',2021-06-10T04:59:40Z,2021-06-10T07:29:08Z,,,
12094,b'Create a torchscript version of Tokenizer in Bert',2021-06-10T01:50:54Z,2021-07-18T15:02:04Z,,,
12093,b'Speedup batch matmul in pytorch',2021-06-10T01:35:41Z,2021-06-13T17:34:26Z,,,
12092,b'Replicating PEGASUS results on a benchmark dataset',2021-06-09T23:07:31Z,2021-07-18T15:02:04Z,,,
12091,b'Provide more useful error message in Detr from_pretrained when timm not installed',2021-06-09T22:15:38Z,2021-06-11T16:27:10Z,,AttributeError,"AttributeError: type object 'DetrForObjectDetection' has no attribute 'from_pretrained'"
12090,b'Checkpoint detected info log in run_clm.py',2021-06-09T19:20:32Z,2021-06-14T12:03:33Z,,,
12089,b'[Wav2Vec2ForPretraining] Correct checkpoints wav2vec2 & fix tests',2021-06-09T19:17:51Z,2021-06-09T19:41:59Z,,,
12088,b'[versions] rm require_version_examples',2021-06-09T17:27:57Z,2021-06-09T18:02:53Z,,,
12087,b'[examples/flax] pass decay_mask fn to optimizer',2021-06-09T17:19:10Z,2021-06-09T17:49:27Z,,,
12086,"b""examples requirements isn't in sync with `require_version_examples`""",2021-06-09T17:01:29Z,2021-06-09T18:02:53Z,,,
12085,b'PyTorch MLM - Dummy Script',2021-06-09T13:47:31Z,2021-06-09T17:49:42Z,,,
12084,b'Memory Efficient FP 16 Training',2021-06-09T13:35:45Z,2021-07-18T15:02:05Z,,,
12083,b'Add text_column_name and label_column_name to run_ner and run_ner_no_trainer args',2021-06-09T12:51:47Z,2021-06-10T12:03:20Z,,,
12082,b'Add support for XLM-R XL and XXL models',2021-06-09T10:43:20Z,2021-07-27T13:41:25Z,,,
12081,"b'GPT2 Flax ""TypeError: JAX only supports number and bool dtypes, got dtype object in array""'",2021-06-09T09:48:11Z,2021-06-10T09:39:27Z,,TypeError,"TypeError: JAX only supports number and bool dtypes, got dtype object in array"
12080,b'Fix missing id2label and label2id in run_ner.py',2021-06-09T09:28:13Z,2021-06-09T16:01:58Z,,,
12079,"b'Use Distilbert to run language model, encounter error  ""Unrecognized configuration class ""  '",2021-06-09T09:22:31Z,2021-06-09T14:33:04Z,,ValueError,"ValueError: Unrecognized configuration class <class 'transformers.models.distilbert.configuration_distilbert.DistilBertConfig'> for this kind of AutoModel: AutoModelForCausalLM."
12078,b'OSError: Unable to open file (file signature not found)',2021-06-09T02:54:58Z,2021-06-09T07:39:10Z,,OSError,"OSError: Unable to open file (file signature not found)"
12077,b'[Deepspeed] new docs',2021-06-09T01:30:46Z,2021-06-23T18:07:38Z,,,
12076,b'[wav2vec2 / Deepspeed] sync LayerDrop for Wav2Vec2Encoder + tests',2021-06-09T00:55:09Z,2021-06-09T12:21:03Z,,,
12075,b' Using whitespace tokenizer for training models ',2021-06-08T21:12:21Z,2021-06-09T11:52:28Z,,,
12074,b'[test] support more than 2 gpus',2021-06-08T19:29:53Z,2021-06-09T16:23:47Z,,,
12073,b'src_lang/tgt_lang missing in mbart example',2021-06-08T18:58:01Z,2021-09-02T09:32:19Z,WIP,,
12072,b'Inconsistent behavior on CPU vs. GPU ',2021-06-08T17:04:56Z,2021-07-19T15:07:25Z,,,
12071,b'XLM-R XL/XXL',2021-06-08T14:52:16Z,,New model,,
12070,b'Properly indent block_size',2021-06-08T13:15:14Z,2021-06-08T14:27:03Z,,,
12069,b'[WIP] Add helper function to align labels between datasets and model config',2021-06-08T13:13:19Z,2021-06-08T13:54:29Z,,,
12068,b'grads is None when using GPT2 transformers in tensorflow',2021-06-08T12:06:26Z,2021-07-17T15:01:51Z,,,
12067,b'Selecting specific GPU CUDA devices',2021-06-08T10:01:41Z,2021-06-10T13:46:38Z,,,
12066,b'Fix LUKE integration tests',2021-06-08T09:01:16Z,2021-06-08T09:21:38Z,,,
12065,b'How can we predict story future based on past events?',2021-06-08T08:34:30Z,2021-07-17T15:01:51Z,,,
12064,"b""ImportError: cannot ipmort name 'TFAutoModel'""",2021-06-08T07:26:47Z,2021-06-08T08:37:46Z,,,
12063,b'Fix tapas issue',2021-06-08T07:13:33Z,2021-06-08T09:22:32Z,,,
12062,b'fp16 models getting auto converted to fp32 in .from_pretrained() ',2021-06-08T06:43:32Z,2021-06-29T03:11:21Z,,,
12061,b'[testing] making network tests more reliable',2021-06-08T03:31:56Z,,WIP,,
12060,b'[skipped test] to fix',2021-06-08T03:23:45Z,2021-06-08T09:22:32Z,,,
12059,b'[CI] skip failing test',2021-06-08T03:22:15Z,2021-06-08T03:48:41Z,,,
12058,b'[Deepspeed] various fixes',2021-06-08T00:41:43Z,2021-06-08T15:36:15Z,DeepSpeed,,
12057,b'adds metric prefix.',2021-06-07T23:36:59Z,2021-06-08T02:34:10Z,,,
12056,b'[testing] set tests to not rebuild datasets',2021-06-07T18:50:52Z,2021-06-08T18:05:14Z,,,
12055,b'Settings for perfect Story writing based on the input text?',2021-06-07T13:20:08Z,2021-10-19T15:03:14Z,,,
12054,b'How to update the GPT2 with loss which are provided from another separate module?',2021-06-07T11:20:15Z,2021-06-08T10:18:20Z,,,
12053,b'[JAX] Bump jax lib',2021-06-07T10:55:24Z,2021-06-07T12:04:18Z,,,
12052,b'No max_length set on huawei-noah/TinyBERT_General_4L_312D/config.json',2021-06-07T10:22:55Z,2021-08-01T15:02:15Z,,,
12051,b'Add early stopping args to TrainingArguments',2021-06-07T09:26:07Z,2021-07-16T15:02:10Z,,,
12050,"b""[end2end RAG]  AttributeError: module 'pickle' has no attribute 'PickleBuffer'""",2021-06-07T08:32:15Z,2021-07-16T15:02:11Z,,AttributeError,"AttributeError: module 'pickle' has no attribute 'PickleBuffer'"
12049,b'fix  past_key_values docs',2021-06-07T06:16:19Z,2021-06-07T09:54:03Z,,,
12048,"b'OpenAI GPT language modeling shape mismatch: 512 position embeddings, 1024 input emebddings'",2021-06-07T06:15:05Z,2021-06-08T14:27:02Z,,RuntimeError,"RuntimeError: The size of tensor a (1024) must match the size of tensor b (512) at non-singleton dimension 1"
12047,b'Question: Masked Loss for LukeForEntitySpanClassification',2021-06-07T03:14:31Z,2021-07-16T15:02:12Z,,,
12046,"b""ImportError: cannot import name 'AutoTokenizer' from 'transformers'""",2021-06-07T02:07:49Z,2021-07-16T15:02:13Z,,ImportError,"ImportError: cannot import name 'AutoTokenizer' from 'transformers' (D:\Environment\Anaconda3\envs\huggingface\lib\site-packages\transformers\__init__.py)"
12045,"b'[wip] [deps] data_collator fails with older numpy, update numpy>=1.20.0'",2021-06-06T23:39:04Z,2021-06-07T04:06:29Z,,RuntimeError,"RuntimeError: Could not infer dtype of numpy.float32"
12044,b'Electra model vocabulary',2021-06-06T16:50:10Z,2021-06-07T04:54:27Z,,,
12043,b'[Draft] Wav2Vec2 - Save intermediate PR verifying  that implementation matches fairseq ones',2021-06-06T14:52:22Z,2021-07-15T15:01:54Z,,,
12042,b'Add optional grouped parsers description to HfArgumentParser',2021-06-06T14:00:47Z,2021-06-07T15:47:13Z,,,
12041,b'Why my simple Bert model for text classification could not learn anything?',2021-06-06T08:27:24Z,2021-07-15T15:01:55Z,,,
12040,b'Add torch to requirements.txt in language-modeling',2021-06-05T17:29:16Z,2021-06-08T13:02:35Z,,,
12039,b'pipelines should allow passing in tokenizer arguments',2021-06-05T15:01:13Z,2021-06-06T01:29:59Z,,,
12038,b'Support for pointer-generator architectures.',2021-06-05T13:30:06Z,2021-07-14T15:02:21Z,,,
12037,b'Using the  latest DPR checkpoint available with HuggingFace DPR class',2021-06-05T13:01:49Z,2021-06-07T11:14:18Z,,,
12036,b'I cannot import deepsepped',2021-06-05T10:11:54Z,2021-06-05T11:44:37Z,,,
12035,b'Fixed Typo in modeling_bart.py',2021-06-05T09:25:32Z,2021-06-07T06:14:25Z,,,
12034,"b'After loading fine-tuned model from local and use it for prediction, it continue training from scratch again!'",2021-06-05T03:09:54Z,2021-07-14T15:02:22Z,,,
12033,b'A bug of modeling_wav2vec2.py:1033 line',2021-06-05T02:11:42Z,2021-06-25T08:53:33Z,,,
12032,b'Documents of `past_key_values` in input and output for `PegasusModel` are not aligned',2021-06-05T00:52:04Z,2021-06-07T09:54:03Z,,AttributeError,"AttributeError: 'tuple' object has no attribute 'shape'"
12031,b'Layoutlmv2 port with testing',2021-06-05T00:24:00Z,2021-08-08T15:02:10Z,,,
12030,b'xla_spawn.py: Cannot load large (~1GB) optimizer.pt from checkpoint',2021-06-04T18:00:01Z,2021-07-15T15:01:56Z,,,
12029,b'Seq2SeqTrainer: cannot set max length when we evaluate/(generate) during training',2021-06-04T17:17:40Z,2021-06-04T18:19:12Z,,,
12028,b'New TF GLUE example',2021-06-04T17:16:32Z,2021-06-10T13:14:37Z,,,
12027,b'Replace legacy tensor.Tensor with torch.tensor/torch.empty',2021-06-04T16:55:24Z,2021-06-08T12:58:39Z,,,
12026,b'Fixes bug that appears when using QA bert and distilation.',2021-06-04T13:07:04Z,2021-06-07T15:22:00Z,,RuntimeError,"RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation"
12025,b'Extend pipelines for automodel tupels',2021-06-04T07:35:16Z,2021-06-07T15:41:27Z,,,
12024,b'Add CANINE',2021-06-04T07:34:13Z,2021-06-30T12:05:44Z,,,
12023,b'Flax CLM script',2021-06-04T07:01:57Z,2021-06-11T09:46:21Z,"Examples, Flax",,
12022,b'Getting IndexError: index out of range in self while finetuning GPTNeo on Text Classification',2021-06-04T05:45:01Z,2021-07-12T15:07:16Z,,IndexError,"IndexError: index out of range in self"
12021,b'[Deepspeed] Assert on mismatches between ds and hf args',2021-06-04T02:07:18Z,2021-06-04T15:58:24Z,,,
12020,"b'Remove ""Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.""'",2021-06-04T01:37:30Z,2021-09-16T15:02:15Z,,,
12019,b'Allow registerable Components',2021-06-04T01:08:39Z,2021-07-12T15:07:17Z,,,
12018,"b'[TrainerArguments] format and sort __repr__, add __str__'",2021-06-04T00:08:12Z,2021-06-04T16:39:39Z,,,
12017,b'Fix deberta 2 Tokenizer Integration Test',2021-06-03T19:18:33Z,2021-06-07T08:55:55Z,,,
12016,b'FAILED tests/test_tokenization_deberta_v2.py::DebertaV2TokenizationTest::test_tokenizer_integration',2021-06-03T19:05:10Z,2021-06-07T08:55:55Z,,,
12015,b'Fix problem_type to match with the applied loss function for distillbert sequence classification ',2021-06-03T16:59:35Z,2021-06-07T13:36:16Z,,,
12014,b'MIssmatch between problem_type and loss functions in DistillBert for sequence classification',2021-06-03T16:52:21Z,2021-06-11T19:22:49Z,,,
12013,b'[Flax] Refactor MLM ',2021-06-03T15:19:55Z,2021-06-03T15:31:32Z,,,
12012,b'RAG end to end with RAY throws pickling error',2021-06-03T13:39:04Z,2021-06-03T15:04:35Z,,,
12011,b'Add mlm pretraining xla torch readme',2021-06-03T12:48:46Z,2021-06-14T09:31:21Z,,,
12010,b'Translation example generates the same input',2021-06-03T12:43:01Z,2021-07-17T15:01:53Z,,,
12009,b'some issue in FlaxBertForMultipleChoice',2021-06-03T11:24:43Z,2021-06-03T16:51:33Z,,jax.core.InconclusiveDimensionOperation,"jax.core.InconclusiveDimensionOperation: Cannot divide evenly the sizes of shapes (1, 1) and (-1, 4)"
12008,b'Issue while using DPR with tensorflow and py torch',2021-06-03T09:51:24Z,2021-07-17T15:01:54Z,,AttributeError,"AttributeError: 'DPRReaderOutput' object has no attribute 'stat_logits' "
12007,"b""Fix megatron_gpt2 attention block's causal mask""",2021-06-03T09:37:32Z,2021-06-14T08:57:56Z,,,
12006,b'Fluctuating embedding given by different random seed during inference',2021-06-03T09:32:16Z,2021-07-11T15:02:06Z,,,
12005,"b'where is the code for  DetrFeatureExtractor, DetrForObjectDetection'",2021-06-03T09:28:27Z,2021-06-10T07:06:59Z,,,
12004,b'Megatron GPT2 not compatible with transformers',2021-06-03T08:51:12Z,2021-06-07T04:42:50Z,,,
12003,b'Fast Tokenization fail for the pretrained model',2021-06-03T06:51:27Z,2021-07-11T15:02:07Z,,Exception,"Exception: No such file or directory (os error 2)"
12002,"b""ImportError: cannot import name 'MarianMTModel'""",2021-06-03T06:37:18Z,2021-07-11T15:02:08Z,,,
12001,b'Update run_ner.py with id2label config',2021-06-03T05:21:59Z,2021-06-09T11:27:06Z,,,
12000,b'Exporting the operator repeat_interleave to ONNX opset version (<=12) is not supported!',2021-06-03T05:07:15Z,2021-06-24T06:55:49Z,,,
11999,"b'Unable to find examples on using DPR for transfer learning,request you to provide examples'",2021-06-03T04:06:27Z,2021-07-18T15:02:07Z,,,
11998,b'Add SENet Blocks in Encoding Layers',2021-06-03T02:44:47Z,,Feature request,,
11997,b'[deepspeed] add nvme test skip rule',2021-06-02T18:12:43Z,2021-06-02T19:06:38Z,DeepSpeed,,
11996,"b""None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.""",2021-06-02T15:31:24Z,2021-07-11T15:02:09Z,,,
11995,b'tensorflow has no attribute swish',2021-06-02T15:30:37Z,2021-06-03T07:46:49Z,,`AttributeError,`AttributeError: module 'tensorflow_core.python.keras.api._v2.keras.activations' has no attribute 'swish'`
11994,b'CLIPFeatureExtractor should resize images with kept aspect ratio',2021-06-02T15:02:22Z,2021-06-10T13:10:42Z,,,
11993,b'XLNET on SQuAD2 evaluation error',2021-06-02T14:54:41Z,2021-06-17T18:12:24Z,,RuntimeError,"RuntimeError: Input tensor at index 3 has invalid shape [384, 1, 1024], but expected [384, 2, 1024]"
11992,b'CLIPFeatureExtractor should resize images with kept aspect ratio',2021-06-02T14:36:54Z,2021-06-10T13:10:41Z,,,
11991,b'Trainer API',2021-06-02T14:17:13Z,2021-07-11T15:02:10Z,,wandb.errors.UsageError,"wandb.errors.UsageError: api_key not configured (no-tty). call wandb.login(key=[your_api_key])"
11990,b'Fix examples in VisualBERT docs',2021-06-02T13:57:34Z,2021-06-02T14:12:53Z,,,
11989,"b'    EOFError(""No valid references for a sentence!"") for run_translation example'",2021-06-02T10:26:16Z,2021-07-12T15:07:19Z,,EOFError,"EOFError: No valid references for a sentence!"
11988,b'Add loss reduction parameter in forward() method',2021-06-02T08:47:40Z,2021-07-12T15:07:19Z,,,
11987,b'Movement Pruning does not achieve expected results',2021-06-02T08:06:30Z,2021-07-12T15:07:20Z,,,
11986,b'[WIP] Add ViLBERT',2021-06-02T06:57:58Z,,WIP,,
11985,b'Changed the hidden_size to d_model for XLNET docs',2021-06-02T04:46:37Z,2021-07-12T15:07:21Z,,,
11984,b'[deepspeed] Move code and doc into standalone files',2021-06-02T03:59:42Z,2021-06-02T16:56:00Z,DeepSpeed,,
11983,b'Bump urllib3 from 1.25.8 to 1.26.5 in /examples/research_projects/lxmert',2021-06-02T03:46:58Z,2021-06-02T07:40:20Z,dependencies,<p><strong>NOTE,<p><strong>NOTE: urllib3 v2.0 will drop support for Python 2</strong>.
11982,"b"" AttributeError: 'GPT2LMHeadModel' object has no attribute 'get_encoder'""",2021-06-02T02:42:55Z,2021-06-02T13:23:48Z,,AttributeError,"AttributeError: 'GPT2LMHeadModel' object has no attribute 'get_encoder'"
11981,b'Rewrite ProphetNet to adapt converting ONNX friendly',2021-06-02T00:02:41Z,2021-06-23T10:34:18Z,,,
11980,b'[Trainer] add train loss and flops metrics reports',2021-06-01T19:46:48Z,2021-06-01T22:58:31Z,,,
11979,"b'Typo in usage example, changed to device instead of torch_device'",2021-06-01T18:42:13Z,2021-06-01T18:58:49Z,,,
11978,b'No package metadata found for tqdm while generating exe',2021-06-01T18:03:43Z,2021-06-02T15:31:59Z,,,
11977,b'T5-Training Arguments',2021-06-01T18:02:51Z,2021-07-11T15:02:11Z,,,
11976,b'Update return introduction of `forward` method',2021-06-01T18:00:40Z,2021-06-02T16:53:09Z,,,
11975,"b'It seems not able to add the args ""repetition_penalty"" when running the code run_summarization.py for prediction.'",2021-06-01T17:50:30Z,2021-07-11T15:02:11Z,,,
11974,b'Fix loss reporting with deepspeed',2021-06-01T14:04:15Z,2021-06-04T11:57:09Z,,,
11973,b'typo correction',2021-06-01T12:59:17Z,2021-06-01T16:24:59Z,,,
11972,b'RuntimeError: Overflow when unpacking long during training the model',2021-06-01T12:35:36Z,2021-08-08T15:02:11Z,,RuntimeError,"RuntimeError: Overflow when unpacking long"
11971,b'ByT5 model',2021-06-01T10:31:45Z,2021-06-01T18:07:37Z,New model,,
11970,b'Saving and loading a model does not work',2021-06-01T06:43:01Z,2021-06-02T15:44:08Z,,,
11969,"b""run_qa.py for Question and answering doesn't work for SQUAD2 """,2021-06-01T06:27:10Z,2021-07-10T15:01:51Z,,,
11968,b'[Pipelines] Extend pipelines to handle multiple possible AutoModel classes',2021-05-31T22:21:14Z,2021-06-04T09:53:22Z,,,
11967,b'Flax Big Bird',2021-05-31T20:07:54Z,2021-06-14T19:01:03Z,Flax,,
11966,b'[DeepSpeed] decouple `DeepSpeedConfigHF` from `Trainer`',2021-05-31T18:51:45Z,2021-06-01T20:24:53Z,,,
11965,b'Reproducibility Questions',2021-05-31T18:32:25Z,2021-07-10T15:01:52Z,,,
11964,b'Fix weight decay masking in `run_flax_glue.py`',2021-05-31T17:03:11Z,2021-06-03T10:35:26Z,Flax,,
11963,b'How to achive character lvl tokenization? (cant convert from huggingface/tokenizers)',2021-05-31T16:52:26Z,2021-07-10T15:01:53Z,,IndexError,"IndexError: list index out of range"
11962,b'[RAG] Fix rag from pretrained question encoder generator behavior',2021-05-31T14:47:03Z,2021-06-02T08:17:15Z,,,
11961,b'Add MT5ForConditionalGeneration as supported arch. to summarization README',2021-05-31T12:08:15Z,2021-05-31T15:54:33Z,,,
11960,b'Summarization also supports MT5ForConditionalGeneration',2021-05-31T11:51:23Z,2021-05-31T16:28:51Z,,,
11959,b'Add new token to pretrained GPT2 tokenizer',2021-05-31T09:01:56Z,2021-06-01T08:03:30Z,,,
11958,"b'Issue: IndexError: ""Index out of range in self"" when generating translations with MarianMTModel'",2021-05-31T08:49:26Z,2021-05-31T11:38:31Z,,IndexError,"IndexError: index out of range in self"
11957,b'Byt5',2021-05-31T08:38:37Z,2021-06-01T18:18:59Z,New model,,
11956,b'Authorize args when instantiating an AutoModel',2021-05-31T07:49:26Z,2021-06-01T13:27:54Z,,"TypeError, OSError","TypeError: __init__() takes 1 positional argument but 2 were givenOSError: AutoModel is designed to be instantiated using the `AutoModel.from_pretrained(pretrained_model_name_or_path)` or `AutoModel.from_config(config)` methods."
11955,b'Killed Message',2021-05-31T05:43:24Z,2021-07-08T15:06:25Z,,,
11954,b'Uncoupling ZeRO-3 weak ref bridge b/w Trainer and modeling_utils',2021-05-31T04:08:17Z,2021-06-01T20:24:53Z,,,
11953,b'AutoModel abstraction fails for pre-training initialization',2021-05-31T00:20:15Z,2021-06-03T01:47:05Z,,TypeError,"TypeError: __init__() takes 1 positional argument but 2 were given"
11952,"b""TypeError: __init__() got an unexpected keyword argument 'force_bos_token_to_be_generated'""",2021-05-30T17:03:50Z,2021-06-01T04:21:42Z,,TypeError,"TypeError: __init__() got an unexpected keyword argument 'force_bos_token_to_be_generated'"
11951,b'[Flax] Adding Visual-Transformer',2021-05-30T15:03:19Z,2021-06-10T15:47:13Z,Flax,,
11950,b'ValueError: You have to specify either decoder_inputs or decoder_inputs_embeds',2021-05-30T14:49:57Z,2021-07-08T15:06:25Z,,,
11949,b'report_to flag does not work with TFTrainer',2021-05-30T13:42:40Z,2021-06-02T12:50:47Z,,ImportError,"ImportError: You must import Comet before these modules: tensorflow, torch, tensorboard"
11948,b'Flax port vision Transformer to flax',2021-05-30T12:41:25Z,2021-06-10T15:47:13Z,New model,,
11947,b'Encoding/decoding NLP model in tensorflow lite (fine-tuned GPT2)',2021-05-30T09:44:46Z,2021-07-08T15:06:26Z,,,
11946,b'Loading mbart-large-50-one-to-many-mmt is very slow',2021-05-30T08:08:29Z,2021-07-08T15:06:27Z,,,
11945,b'reinitialize wandb config for each hyperparameter search run',2021-05-30T08:01:58Z,2021-06-01T13:18:33Z,,,
11944,b'wandb integration gags during hyperparameter search',2021-05-30T07:55:31Z,2021-06-01T13:18:33Z,,wandb.errors.Error,"wandb.errors.Error: You must call wandb.init() before wandb.log()                                                                                                                             "
11943,b'RuntimeError: CUDA error: device-side assert triggered',2021-05-30T07:49:41Z,2021-06-02T10:02:31Z,,RuntimeError,"RuntimeError: CUDA error: device-side assert triggered"
11942,b'Typo in Pegasus model usage example',2021-05-30T07:32:04Z,2021-06-01T18:59:31Z,,NameError,"NameError: name 'torch_device' is not defined"
11941,b'position_ids version changed during training',2021-05-30T02:32:45Z,2021-07-08T15:06:28Z,,RuntimeError,"RuntimeError: RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.LongTensor [1, 128]] is at version 4; expected version 3 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!"
11940,b'[deepspeed] docs',2021-05-30T02:24:38Z,2021-06-01T16:21:21Z,,,
11939,b'XLM tokenizer lang2id attribute is None',2021-05-29T18:07:55Z,2021-08-09T15:08:01Z,,,
11938,b'[docs] XLNETModel forward returns last_hidden_state 3rd dim should be d_model instead of hidden_size ',2021-05-29T16:19:55Z,2021-07-08T15:06:29Z,,,
11937,b'Neptune.ai integration',2021-05-29T09:13:19Z,2021-06-01T13:40:53Z,,,
11936,b'Flax text-classification multi-optimizer incorrect',2021-05-29T09:07:04Z,2021-06-03T10:35:26Z,,,
11935,b'Use `self.assertEqual` instead of `assert` in deberta v2 test.',2021-05-29T07:33:05Z,2021-05-31T08:02:11Z,,,
11934,b'Predict masked word at the beginning of the sentence',2021-05-29T06:57:43Z,2021-07-08T15:06:30Z,,,
11933,b'Porting Layoutlmv2 to Huggingface',2021-05-29T05:43:18Z,2021-07-07T15:03:06Z,,,
11932,b'LayoutLMv2 Model',2021-05-29T05:30:10Z,2021-08-30T10:35:42Z,New model,,
11931,b'How to load the best performance checkpoint after  training\xef\xbc\x9f',2021-05-29T02:13:58Z,2021-07-10T15:01:54Z,,,
11930,b'Ray pickling issue when running hp search',2021-05-28T18:41:28Z,2021-05-28T20:34:39Z,,TypeError,"TypeError: can't pickle _thread.RLock objects"
11929,b'Use `self.assertEqual` instead of `assert` in tests.',2021-05-28T16:20:59Z,2021-05-31T08:02:11Z,,,
11928,b'[Flax][WIP] Speed up pretraining',2021-05-28T14:22:22Z,2021-06-03T14:17:08Z,,,
11927,b'add relevant description to tqdm in examples',2021-05-28T14:07:36Z,2021-06-10T19:59:55Z,,,
11926,b'Modifying the distill bert architecture',2021-05-28T13:19:04Z,2021-05-28T15:00:43Z,,,
11925,b'BERT pretraining: [SEP] vs. Segment Embeddings?',2021-05-28T12:10:21Z,2021-07-05T15:07:18Z,,,
11924,b'Test optuna and ray',2021-05-28T11:51:57Z,2021-05-28T11:52:01Z,,,
11923,b'Trainer.predict using customized model.predict function?',2021-05-28T11:22:22Z,2021-07-05T15:07:19Z,,,
11922,b'get_ordinal(local=True) replaced with get_local_ordinal() in training_args.py',2021-05-28T09:50:49Z,2021-06-01T13:04:51Z,,,
11921,"b""ProphetNetForConditionalGeneration model isn't returning all objects properly""",2021-05-28T08:34:09Z,2021-05-28T13:39:20Z,,,
11920,b'Remove redundant `nn.log_softmax` in `run_flax_glue.py`',2021-05-28T07:34:51Z,2021-05-31T14:29:04Z,,,
11919,b'Trainer reported loss is wrong when using DeepSpeed and gradient_accumulation_steps > 1',2021-05-28T05:54:56Z,2021-06-16T06:21:37Z,DeepSpeed,,
11918,"b'[Flax] Return Attention from BERT, ELECTRA, RoBERTa and GPT2'",2021-05-28T05:06:06Z,2021-05-28T10:46:56Z,,,
11917,b'[Flax][WIP] Addition of Flax-Wav2Vec Model',2021-05-28T04:51:35Z,2021-06-29T21:53:22Z,,,
11916,b'Wrong perplexity when evaluate the megatron-gpt2.',2021-05-28T04:23:15Z,2021-06-07T05:10:23Z,,,
11915,b'RuntimeError: The size of tensor a (716) must match the size of tensor b (512) at non-singleton dimension 1',2021-05-28T03:43:43Z,2021-07-05T15:07:20Z,,RuntimeError,"RuntimeError: The size of tensor a (716) must match the size of tensor b (512) at non-singleton dimension 1"
11914,b'How to get back the identified words from LayoutLMForTokenClassification?',2021-05-28T03:16:30Z,2021-07-05T15:07:21Z,,,
11913,b'Inference for pinned model keeps loading',2021-05-28T01:22:25Z,2021-05-28T15:02:15Z,,,
11912,b'Distillation of Pegasus using Pseudo labeling',2021-05-27T23:06:27Z,2021-05-28T19:26:13Z,,,
11911,b'Fix a condition in test_generate_with_head_masking',2021-05-27T20:48:53Z,2021-06-10T14:28:07Z,,,
11910,"b""xla_spawn.py:  xm.get_ordinal() got an unexpected keyword argument 'local'""",2021-05-27T20:06:19Z,2021-06-01T13:05:09Z,,TypeError,"TypeError: get_ordinal() got an unexpected keyword argument 'local'"
11909,b'FlaxGPTNeo Draft PR',2021-05-27T20:00:18Z,2021-07-07T18:32:48Z,Flax,,
11908,b'Fine tuning with transformer models for Regression tasks',2021-05-27T19:40:44Z,2021-06-09T21:37:13Z,,,
11907,b'Add conversion from TF to PT for Tapas retrieval models',2021-05-27T17:35:11Z,2021-07-07T15:03:08Z,,,
11906,b'Added Sequence Classification class in GPTNeo',2021-05-27T15:10:21Z,2021-05-28T10:27:03Z,,,
11905,b'Customize pretrained model for model hub',2021-05-27T14:04:14Z,2021-05-31T11:37:27Z,,,
11904,"b""'error': 'Model Matthieu/stsb-xlm-r-multilingual is currently loading'""",2021-05-27T13:59:55Z,2021-07-05T15:07:22Z,,`{'error',"`{'error': 'Model Matthieu/stsb-xlm-r-multilingual is currently loading', 'estimated_time': 44.49033436}`"
11903,b'Problem when freezing all GPT2 model except the LM head',2021-05-27T13:21:26Z,2021-05-31T01:36:55Z,,,
11902,b'[Flax] return attentions',2021-05-27T12:08:52Z,2021-05-31T07:47:29Z,,,
11901,b'[Flax] Add attention weights outputs to all models',2021-05-27T11:11:26Z,2021-06-13T20:11:43Z,Good First Issue,,
11900,b'[Community Notebooks] Add Emotion Speech Noteboook',2021-05-27T09:44:33Z,2021-05-27T09:46:10Z,,,
11899,b' Provides an option to select the parallel mode of the Trainer.',2021-05-27T09:03:05Z,2021-07-05T15:07:23Z,,,
11898,b'mutil gpu errors',2021-05-27T08:02:08Z,2021-07-05T15:07:24Z,,torch.nn.modules.module.ModuleAttributeError,"torch.nn.modules.module.ModuleAttributeError: 'DataParallel' object has no attribute 'train_model'"
11897,b'Fix Tensorflow Bart-like positional encoding',2021-05-27T04:45:16Z,2021-07-13T21:18:55Z,,,
11896,b'Update deepspeed config to reflect hyperparameter search parameters',2021-05-27T04:43:22Z,2021-05-27T11:53:33Z,,,
11895,b'Small error in documentation / Typo',2021-05-27T03:40:12Z,2021-06-07T06:14:25Z,,,
11894,b'Deepspeed integration ignores Optuna trial parameters in hyperparameter_search',2021-05-27T02:37:47Z,2021-05-27T11:53:33Z,DeepSpeed,,
11893,b'RAG-2nd2end-revamp',2021-05-27T00:14:16Z,2021-06-01T06:32:27Z,,,
11892,b'Link official Cloud TPU JAX docs',2021-05-26T19:20:09Z,2021-05-26T19:44:40Z,,,
11891,b'GPT2 saved pb file cannot handle dynamic sequence length',2021-05-26T18:58:13Z,2021-05-27T16:05:21Z,,,
11890,b'changing find_batch_size to work with tokenizer outputs',2021-05-26T15:35:23Z,2021-05-26T15:59:06Z,,,
11889,b'Hubert',2021-05-26T14:57:25Z,2021-06-16T11:14:13Z,New model,,
11888,b'Add a new pipeline for the Relation Extraction task.',2021-05-26T14:46:59Z,2021-05-28T08:25:38Z,,,
11887,b'Wrong subword aggregation when using aggregation_strategy',2021-05-26T13:10:58Z,2021-07-26T14:21:26Z,,,
11886,b'[Flax] Allow dataclasses to be jitted',2021-05-26T11:52:37Z,2021-05-26T14:01:13Z,,,
11885,b'Find the requested files in the cached path without the internet',2021-05-26T11:41:52Z,2021-07-04T15:02:00Z,,,
11884,b'Mask token mismatch with the model on hosted inference API of Model Hub',2021-05-26T10:30:38Z,2021-05-31T16:36:55Z,,,
11883,b'Add FlaxCLIP',2021-05-26T10:06:27Z,2021-06-01T04:14:31Z,Flax,,
11882,b'BertForMaskedLM training fails when using iterable eval_dataset and DataCollatorForLanguageModeling collator.',2021-05-26T09:58:51Z,2021-06-07T16:09:20Z,,,
11881,b'Adding new Jax Models.',2021-05-26T02:41:35Z,2021-06-13T14:04:19Z,New model,,
11880,"b""KeyError: 'labels' in distill_classifier.py""",2021-05-25T22:52:51Z,2021-06-18T12:50:33Z,,KeyError,"KeyError: 'labels'"
11879,"b""Trainer : AttributeError: 'str' object has no attribute '_memory_tracker'""",2021-05-25T22:41:46Z,2021-05-26T03:41:19Z,,AttributeError,"AttributeError: 'str' object has no attribute '_memory_tracker'"
11878,b'[Wav2Vec2ForCTC] example typo fixed',2021-05-25T20:52:26Z,2021-05-25T21:06:14Z,,,
11877,"b""basic_tokenizer don't preserve token encoding/format""",2021-05-25T20:49:38Z,2021-07-04T15:02:01Z,,,
11876,b'Cannot add tokenizer to model repo',2021-05-25T18:45:42Z,2021-05-25T21:51:47Z,,"CalledProcessError, OSError","CalledProcessError: Command '['git', 'push']' returned non-zero exit status 1.OSError: error: RPC failed; HTTP 403 curl 22 The requested URL returned error: 403 Forbidden"
11875,b'[lm examples] replicate --config_overrides addition to other LM examples',2021-05-25T18:12:51Z,2021-06-14T12:12:23Z,Good First Issue,,
11874,b'[AutomaticSpeechRecognitionPipeline] Ensure input tensors are on device',2021-05-25T17:44:55Z,2021-05-26T08:19:38Z,,,
11873,b'Errors in Quickstart  Documentation related to GPT-2',2021-05-25T15:15:39Z,2021-07-04T15:02:02Z,,,
11872,b'modify qa-trainer',2021-05-25T15:02:15Z,2021-06-01T12:28:42Z,,RuntimeError,"RuntimeError: Tensors must be non-overlapping and dense"
11871,b'Want to use bert-base-uncased model without internet connection',2021-05-25T14:42:30Z,2021-05-28T10:47:08Z,,,
11870,b'Issue: BART does not learn during fine-tuning for abstractive text summarization',2021-05-25T12:52:12Z,2021-08-03T15:07:04Z,,,
11869,b'Custom train file not supported in run_qa.py',2021-05-25T12:04:38Z,2021-05-25T15:24:29Z,,IndexError,"IndexError: list index out of range"
11868,b'Wrong BlenderbotConfig description (max_position_embeddings)',2021-05-25T11:45:44Z,2021-07-02T15:05:07Z,,,
11867,b'Fix incorrect TPU pricing in Flax GLUE README.md',2021-05-25T10:44:11Z,2021-07-02T15:05:08Z,,,
11866,b'# \xf0\x9f\x96\xa5 Benchmarking `transformers`',2021-05-25T10:33:27Z,2021-05-25T12:48:58Z,,,
11865,b'[Benchmark]',2021-05-25T10:32:56Z,2021-05-25T12:48:43Z,,,
11864,b'Bart tokenizer and bart model for conditional generation have different vocab size ',2021-05-25T09:48:00Z,2021-05-31T07:17:24Z,,,
11863,b'[Proposal] Adding infinite generation as an option to generate',2021-05-25T09:31:42Z,2021-07-02T15:05:09Z,,,
11862,"b""'SequenceClassifierOutput' object has no attribute 'log_softmax'""",2021-05-25T09:09:04Z,2021-05-25T11:16:57Z,,,
11861,b'ONNX model conversion',2021-05-25T08:58:54Z,2021-07-02T15:05:11Z,,,
11860,b'Add some tests to the slow suite ',2021-05-25T08:05:52Z,2021-05-25T08:06:06Z,,,
11859,b'Enable memory metrics in tests that need it',2021-05-25T08:03:41Z,2021-05-25T08:06:20Z,,,
11858,b'typo',2021-05-25T07:07:04Z,2021-05-25T08:23:46Z,,,
11857,b'[WIP] Fix cross attentions for TF T5',2021-05-25T05:56:24Z,2021-06-21T17:31:35Z,,,
11856,b'fixed a small typo in the CONTRIBUTING doc',2021-05-25T05:33:36Z,2021-05-25T08:18:55Z,,,
11855,b'[lm examples] fix overflow in perplexity calc',2021-05-25T01:04:28Z,2021-05-25T15:11:26Z,,,
11854,b'Permission denied for cardiffnlp/twitter-roberta-base-emotion',2021-05-24T21:57:03Z,2021-06-01T18:24:26Z,,,
11853,b'Multi-node training for casual language modeling example does not work',2021-05-24T18:40:25Z,2021-05-25T13:24:26Z,,,
11852,b'Fix two typos in docs',2021-05-24T18:12:20Z,2021-05-24T18:26:03Z,,,
11851,b'Switch mem metrics flag',2021-05-24T16:22:23Z,2021-05-24T17:30:40Z,,,
11850,b'Gradient is None in after deepspeed backward ',2021-05-24T16:15:57Z,2021-05-25T01:10:36Z,DeepSpeed,,
11849,b'Add simple ByteTokenizer for Reformer',2021-05-24T15:59:18Z,2021-07-10T15:01:55Z,,,
11848,b'Token Classification OOM',2021-05-24T15:36:06Z,2021-05-27T15:23:47Z,,RuntimeError,"RuntimeError: CUDA out of memory. Tried to allocate 2.12 GiB (GPU 0; 7.80 GiB total capacity; 3.81 GiB already allocated; 2.18 GiB free; 3.97 GiB reserved in total by PyTorch)"
11847,"b""Request addition of 'GPT2ForwardBackward' models""",2021-05-24T14:39:21Z,,New model,,
11846,b'Fix reference to XLNet',2021-05-24T13:20:44Z,2021-05-24T13:26:40Z,,,
11845,b'Regression in training speed since 4.4.0',2021-05-24T12:50:03Z,2021-05-24T17:30:40Z,,,
11844,b'Fix flos single node',2021-05-24T12:29:58Z,2021-05-24T18:15:52Z,,,
11843,b'Issues loading finetuned BERT',2021-05-24T11:43:43Z,2021-05-24T20:05:40Z,,RuntimeError,"RuntimeError: Error(s) in loading state_dict for BertModel:"
11842,b'Fix bug in Masked Language Modeling example scripts (#11840))',2021-05-24T10:27:35Z,2021-07-01T15:07:13Z,,,
11841,"b'Generate Function call throughs error when ""inputs_embeds"" argument passed'",2021-05-24T10:16:42Z,2021-07-19T15:07:29Z,,AttributeError,"AttributeError: 'NoneType' object has no attribute 'new_ones'"
11840,b'Bug in MLM example scripts',2021-05-24T09:43:09Z,2021-07-01T15:07:14Z,,,
11839,b'[Flax] Fix PyTorch import error',2021-05-24T09:35:52Z,2021-05-24T09:41:11Z,,,
11838,b'Is 10% in annotation different from 0.5 in code\xef\xbc\x9f',2021-05-24T08:55:58Z,2021-07-01T15:07:15Z,,,
11837,b'Module torch has no attribute minimum for modeling_big_bird.py',2021-05-24T06:31:07Z,2021-07-11T15:02:14Z,,,
11836,b'Not able to fine tune language model',2021-05-24T06:27:15Z,2021-07-01T15:07:16Z,,ValueError,"ValueError: Need either a dataset name or a training/validation file.**"
11835,b'Tiny fix in README.md of run_flax_mlm',2021-05-24T06:01:28Z,2021-07-01T15:07:17Z,,,
11834,b'convert_pytorch_checkpoint_to_tf2.py AttributeError: embeddings.word_embeddings.weight not found in PyTorch model',2021-05-24T02:29:59Z,2021-07-12T00:09:42Z,,AttributeError,"AttributeError: embeddings.word_embeddings.weight not found in PyTorch model"
11833,b'[BUG] Trainer predict bug under DDP model.',2021-05-23T22:56:43Z,2021-07-01T15:07:17Z,,RuntimeError,"RuntimeError: Gather got an input of invalid size: got [1024, 7, 768], but expected [1024, 8, 768]"
11832,b'Seq2seq-based model running slowly on TPU',2021-05-23T15:37:33Z,2021-05-25T11:18:10Z,,,
11831,b'[docs] XLnet reference link bug in description of past_index Parameter of TrainingArguments ',2021-05-23T05:30:08Z,2021-05-24T13:26:40Z,,,
11830,b'Delete key or set to `None` in __getstate__ impl.',2021-05-23T04:49:50Z,2021-05-23T10:44:59Z,,,
11829,b'[AutomaticSpeechRecognitionPipeline] CUDA support',2021-05-23T02:14:13Z,2021-05-26T08:19:37Z,,`RuntimeError,"`RuntimeError: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same`"
11828,b'possible bug in `TokenizerFast` when setting `return_offset_mapping=True`',2021-05-22T19:41:39Z,2021-05-25T08:27:02Z,,TypeError,"TypeError: forward() got an unexpected keyword argument 'offset_mapping'"
11827,b'My modified `run_glue.py` works well with v4.1.1 but not good with v4.6.0',2021-05-22T17:07:16Z,2021-05-24T15:44:31Z,,,
11826,b'feat: add contributor over time graph to README',2021-05-22T08:34:33Z,2021-06-30T15:06:24Z,,,
11825,b'Faster list concat for trainer_pt_utils.get_length_grouped_indices()',2021-05-21T22:15:06Z,2021-05-22T14:27:20Z,,,
11824,b'Add flax text class colab',2021-05-21T22:10:16Z,2021-05-21T22:11:58Z,,,
11823,b'Hugging Face model Bio_ClinicalBERT producing 404 error',2021-05-21T18:22:56Z,2021-05-25T09:21:58Z,,,
11822,b'Training Transformer XL from scratch',2021-05-21T16:57:46Z,2021-07-06T15:03:00Z,,TypeError,"TypeError: forward() got an unexpected keyword argument 'attention_mask'"
11821,b'[run_clm.py] restore caching',2021-05-21T15:58:14Z,2021-05-21T16:15:03Z,,,
11820,b'[Flax] Small fixes in `run_flax_glue.py`',2021-05-21T15:40:51Z,2021-05-21T15:52:23Z,,,
11819,b'Add option to log only once in multinode training',2021-05-21T13:57:18Z,2021-05-25T12:03:43Z,,,
11818,b'[Trainer] Report both steps and num samples per second',2021-05-21T13:50:16Z,2021-05-24T23:51:43Z,,,
11817,b'same sentence different padding length result different embedding.',2021-05-21T13:06:41Z,2021-06-28T15:07:02Z,,,
11816,b'ValueError batch-size mismatch when redefining classifier layer on BertForSequenceClassification',2021-05-21T12:57:36Z,2021-05-21T20:10:58Z,,ValueError,"ValueError: Expected input batch_size (24) to match target batch_size (16)."
11815,b'How get sentenses embbedings from TFBertForMaskedLM',2021-05-21T11:51:40Z,2021-05-28T12:23:01Z,,,
11814,b'Permission error for cardiffnlp/twitter-roberta-base-emotion',2021-05-21T11:50:52Z,2021-06-01T18:22:40Z,,,
11813,b'fix roformer config doc',2021-05-21T11:37:12Z,2021-05-21T12:06:11Z,,,
11812,b'Patch recursive import',2021-05-21T10:21:16Z,2021-05-21T10:50:01Z,,,
11811,b'GPT Neo for Sequence Classification',2021-05-21T09:42:27Z,2021-05-28T10:27:03Z,Good First Issue,,
11810,b'Feature to use the PreTrainedTokenizerFast class as a stand-alone tokenizer',2021-05-21T09:28:00Z,2021-06-14T09:58:45Z,,,
11809,"b'Wrong LayerNorm weight names in ""bert-base-uncased"" checkpoint ?'",2021-05-21T09:19:18Z,2021-06-28T15:07:03Z,,RuntimeError,"RuntimeError: Error(s) in loading state_dict for BertForPreTraining:"
11808,b'How to save and load model from local path in pipeline api ?',2021-05-21T07:12:08Z,2021-05-26T01:16:42Z,,,
11807,b'version of T5 is not reported in HuggingFace models ',2021-05-21T06:55:02Z,2021-06-28T15:07:04Z,,,
11806,b'updated the original RAG implementation to be compatible with latest Pytorch-Lightning',2021-05-21T04:40:27Z,2021-06-08T12:42:49Z,,,
11805,b'[Deepspeed] support `zero.Init` in `from_config`',2021-05-21T04:08:42Z,2021-05-21T16:07:46Z,,,
11804,b'Index out of range when doing manual testing for TFBertModel',2021-05-21T03:54:20Z,2021-05-25T07:01:50Z,,IndexError,"IndexError: list index out of range"
11803,b'bert model (bert-base-chinese) consumed too much memory',2021-05-21T03:48:14Z,2021-06-28T15:07:05Z,,,
11802,"b'Text Generation, adding random words, weird linebreaks & symbols at random.'",2021-05-21T03:12:02Z,2021-06-28T15:07:06Z,,,
11801,b'[examples] run_clm re-processes dataset on every run',2021-05-20T22:49:10Z,2021-06-11T22:45:49Z,,,
11800,"b""CamemBert Tokenizer AttributeError: 'NoneType' object has no attribute 'tokenize'""",2021-05-20T20:52:09Z,2021-05-21T13:45:55Z,,,
11799,"b'ImportError: tokenizers>=0.10.1,<0.11 is required for a normal functioning of this module, but found tokenizers==0.8.1rc1.'",2021-05-20T20:37:44Z,2021-06-01T06:52:24Z,,ImportError,"ImportError: tokenizers>=0.10.1,<0.11 is required for a normal functioning of this module, but found tokenizers==0.8.1rc1."
11798,b'[Examples] create model with custom config on the fly',2021-05-20T20:26:56Z,2021-05-25T17:40:50Z,,,
11797,b'[examples] add desc to `dataset.map` to improve tqdm bars',2021-05-20T19:27:57Z,2021-06-10T19:59:55Z,,,
11796,b'[trainer] multi-node tweaks',2021-05-20T19:03:14Z,2021-05-25T12:03:43Z,,,
11795,b'get_length_grouped_indices() uses slow list concat',2021-05-20T18:25:28Z,2021-05-22T14:27:20Z,,,
11794,b'Bug in TokenClassificationPipeline',2021-05-20T18:21:38Z,2021-06-28T15:07:07Z,,,
11793,b'[trainer] the noisy tensorflow loaded when asked explicitly not to load it',2021-05-20T17:58:55Z,2021-06-11T22:51:59Z,,,
11792,b'T5EncoderModel slower in half-precision',2021-05-20T16:51:49Z,2021-05-27T06:40:38Z,,,
11791,b'LongformerForSequenceClassification: global_attention_mask=None',2021-05-20T15:42:50Z,2021-06-28T15:07:07Z,,,
11790,b'facebook/mbart-large-50-one-to-many-mmt fails on Swahili',2021-05-20T14:30:24Z,2021-07-26T15:06:36Z,,,
11789,b'PegasusTokenizer returning None',2021-05-20T14:22:47Z,2021-07-05T15:07:25Z,,,
11788,b'EncoderDecoder Cross Attention Generation Output Shape does not match Documentation',2021-05-20T13:48:32Z,2021-06-28T15:07:09Z,,,
11787,b'GPT Neo past_key_values unexpected behaviour',2021-05-20T13:47:24Z,2021-09-10T17:22:20Z,WIP,,
11786,b'[RFC] Laying down building stone for more flexible ONNX export capabilities',2021-05-20T13:35:44Z,2021-07-08T14:54:43Z,,,
11785,b'Fix regression in regression',2021-05-20T13:23:19Z,2021-05-20T13:55:14Z,,,
11784,b'Fix release utilpattern in conf.py',2021-05-20T13:05:25Z,2021-05-20T13:30:31Z,,,
11783,b'PyInstaller Transformers runtime import error',2021-05-20T10:57:24Z,2021-06-28T15:07:10Z,,"SyntaxError, AttributeError","SyntaxError: invalid syntaxAttributeError: 'NoneType' object has no attribute 'get_filename'_"
11782,b'[WIP] Expand `past_key_values` also during beam search in EncoderDecoder models',2021-05-20T05:44:40Z,2021-05-31T14:46:00Z,,,
11781,b'`generate` with `num_beam` > 1 does not work in EncoderDecoder models when `past` is supplied.',2021-05-20T05:40:48Z,2021-06-25T04:39:45Z,,,
11780,"b""Unintentional(?) interface change on loss function in models didn't work well for single-column regression""",2021-05-19T23:21:35Z,2021-05-20T13:55:13Z,,,
11779,b'Deprecate commands from the transformers-cli that are in the hf-cli',2021-05-19T19:19:54Z,2021-05-20T07:16:04Z,,,
11778,b'[Flax] Align GLUE training script with mlm training script',2021-05-19T17:25:04Z,2021-05-21T08:36:56Z,,,
11777,b'Flax Generate',2021-05-19T17:21:03Z,2021-05-26T23:18:18Z,,,
11776,b'uplaod',2021-05-19T17:14:43Z,2021-05-19T17:22:42Z,,,
11775,"b""Fix usage of head masks by TF encoder-decoder models' `generate()` function""",2021-05-19T15:14:24Z,2021-05-26T13:02:44Z,,,
11774,b'Finetune - Helsinki-NLP/opus-mt-fr-en ',2021-05-19T14:30:35Z,2021-06-28T15:07:11Z,,"_pickle.UnpicklingError, OSError","_pickle.UnpicklingError: invalid load key, 'v'.OSError: Unable to load weights from pytorch checkpoint file for '/marian/examples/test' at '/marian/examples/test/"
11773,b'[Demo] Slow down in TPU training',2021-05-19T11:22:30Z,2021-05-25T08:12:59Z,,,
11772,b'Different performance when training different transformers version',2021-05-19T10:37:13Z,2021-06-28T15:07:11Z,,,
11771,b'Add DOI badge to README',2021-05-19T10:23:09Z,2021-05-19T13:48:57Z,,,
11770,b'[T5 failing CI] Fix generate test',2021-05-19T09:09:47Z,2021-05-19T09:31:18Z,,,
11769,"b'Trainer removes newer checkpoints, not older.'",2021-05-19T08:04:55Z,2021-06-28T15:07:12Z,,,
11768,"b'DataCollatorForWholeWordMask only works for BERT, and nothing is said in the docstring.'",2021-05-19T07:53:48Z,2021-06-28T11:39:56Z,,,
11767,b'AttributeError when using EncoderDecoderModel.forward() with encoder_outputs and return_dict=True',2021-05-19T07:35:08Z,2021-06-28T15:07:13Z,,,
11766,b'Error when using IterableDataset as train_dataset for Trainer',2021-05-19T07:10:15Z,2021-05-20T04:45:37Z,,TypeError,"TypeError: vars() argument must have __dict__ attribute"
11765,b'Unable to use fill-mask pipeline on gpt-neo model',2021-05-19T05:37:06Z,2021-06-28T15:07:14Z,,,
11764,b'[Wav2Vec2] SpecAugment Fast',2021-05-19T00:59:27Z,2021-05-25T12:59:52Z,,,
11763,b'A cleaner and more scalable implementation of symbolic tracing',2021-05-18T18:00:21Z,2021-05-20T16:02:29Z,,,
11762,b'Fix a bug in summarization example which did not load model from config properly',2021-05-18T17:46:54Z,2021-05-18T18:38:36Z,,,
11761,b'Add batching to pipelines',2021-05-18T13:05:37Z,2021-05-19T08:51:41Z,,,
11760,b'add `dataset_name` to data_args and added accuracy metric',2021-05-18T12:21:37Z,2021-05-18T14:27:29Z,,,
11759,b'error in load of tokenizer with add_token',2021-05-18T11:15:20Z,2021-07-02T15:05:13Z,,AssertionError,"AssertionError: Non-consecutive added token '#' found. Should have index 100005 but has index 100006 in saved vocabulary."
11758,b'Add more subsections to main doc',2021-05-18T10:47:57Z,2021-05-18T13:38:57Z,,,
11757,b'Fix incorrect newline in #11650',2021-05-18T10:28:21Z,2021-05-18T13:28:13Z,,,
11756,b'word_to_tokens method of XLNetTokenizerFast not behaving correctly',2021-05-18T09:23:20Z,2021-06-26T15:02:15Z,,,
11755,b'A problem of Ibert IntSoftmax',2021-05-18T05:57:00Z,2021-06-26T15:02:16Z,,,
11754,b'Trainer accumulates GPU usage at the beginning of each step',2021-05-17T23:12:09Z,2021-05-23T14:13:58Z,,RuntimeError,"RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 15.90 GiB total capacity; 14.96 GiB already allocated; 21.75 MiB free; 15.00 GiB reserved in total by PyTorch)"
11753,b'Add Flax Examples and Cloud TPU README',2021-05-17T20:49:04Z,2021-05-18T16:45:16Z,,,
11752,"b""Fixed: Better names for nlp variables in pipelines' tests and docs.""",2021-05-17T19:26:59Z,2021-05-18T13:47:29Z,,,
11751,b'parallelize and deparallelize method for GPT-Neo series model',2021-05-17T18:42:12Z,2021-05-25T20:05:47Z,,,
11750,b'Flax BERT fix token type init',2021-05-17T18:37:15Z,2021-05-17T18:54:34Z,,,
11749,b'[deepspeed] supporting `--adafactor` ',2021-05-17T16:56:51Z,2021-06-21T15:17:00Z,"DeepSpeed, WIP",,
11748,b'Fix checkpoint deletion',2021-05-17T16:17:56Z,2021-05-18T11:42:39Z,,,
11747,b'mbart-large-cc25 tokenization_utils_fast.py TypeError ',2021-05-17T14:52:50Z,2021-05-19T12:45:36Z,,TypeError,"TypeError: int() argument must be a string, a bytes-like object or a number, not 'NoneType'"
11746,b'Use new evaluation loop in TrainerQA',2021-05-17T13:56:35Z,2021-05-17T14:10:14Z,,,
11745,b'[Flax MLM] Refactor run mlm with optax',2021-05-17T12:49:40Z,2021-05-19T11:00:58Z,,,
11744,b'[BigBird Pegasus] Make tests faster',2021-05-17T10:10:33Z,2021-05-17T10:30:54Z,,,
11743,b'Wrong output used by RobertaForSequenceClassification classification head',2021-05-17T09:58:46Z,2021-06-18T13:29:36Z,,,
11742,b'Issue with symbolic tracing for T5',2021-05-17T09:55:32Z,2021-05-17T10:17:31Z,,,
11741,b'Convert blenderbot checkpoint to tensorflow (TF)',2021-05-17T09:26:50Z,2021-06-16T15:26:37Z,,,
11740,b'Add visual + link to Premium Support webpage',2021-05-17T09:08:18Z,2021-05-17T09:28:57Z,,,
11739,b'Remove tapas model card',2021-05-17T07:53:02Z,2021-05-17T08:42:37Z,,,
11738,b'Remove extra self from _save_checkpoint call',2021-05-16T20:41:57Z,2021-06-16T23:33:36Z,,TypeError,"TypeError: _save_checkpoint() got multiple values for argument 'metrics'"
11737,b'Add regression tests for slow sentencepiece tokenizers. ',2021-05-16T19:22:27Z,2021-06-01T13:24:39Z,,,
11736,b'Support for running Gpt-Neo 2.7B with 6 GB vram for inference',2021-05-16T12:05:51Z,2021-06-24T15:01:46Z,,,
11735,b'Problem with mT5 and the official Summarization notebook',2021-05-16T08:56:42Z,2021-06-24T15:01:47Z,,,
11734,b'Cant load google/reformer-enwik8',2021-05-15T12:29:48Z,2021-05-16T19:08:18Z,,OSError,"OSError: Can't load tokenizer for 'google/reformer-enwik8'. Make sure that:"
11733,b'CPU Memory Leak when using RoBERTa for just word vector representation',2021-05-15T07:47:04Z,2021-06-23T15:01:47Z,,,
11732,b'Import `SPIECE_UNDERLINE` from `file_utils` instead of WET definition',2021-05-15T06:29:51Z,,WIP,,
11731,b'`ci/circleci: run_tests_torch` reaches 10 min. time limit',2021-05-15T05:38:20Z,2021-05-29T18:53:27Z,,,
11730,b'Bert2bert on Swag with very low accuracy',2021-05-14T21:30:00Z,2021-06-15T12:15:29Z,,,
11729,b'[Benchmark]',2021-05-14T17:51:34Z,2021-05-14T18:25:28Z,,,
11728,"b""ImportError: cannot import name 'load_dataset' from 'datasets'""",2021-05-14T16:27:23Z,2021-06-24T15:01:48Z,,ImportError,"ImportError: cannot import name 'load_dataset' from 'datasets' (C:\Users\bookw\Dropbox\Equity-Analyst-Project\equity-analysts-sentiment\datasets.py) """"""'"""
11727,b'Improvements to Flax finetuning script',2021-05-14T14:22:58Z,2021-05-17T08:26:33Z,,,
11726,b'[Flax] Correct example script',2021-05-14T11:00:46Z,2021-05-14T11:02:57Z,,,
11725,b'Fix#11724',2021-05-14T10:37:04Z,2021-05-14T10:37:21Z,,,
11724,b'A bug in modeling_tf_marian.py and modeling_tf_pegasus.py SinusoidalPositionalEmbedding _init_weight',2021-05-14T10:27:47Z,2021-07-13T21:18:55Z,,,
11723,b'Warnings about some weights that were not initialized in Greek BERT',2021-05-14T09:43:21Z,2021-06-21T15:06:11Z,,,
11722,b'Plug a custom tokenizer into PreTrainedTokenizer',2021-05-14T08:55:30Z,2021-05-14T12:42:40Z,,TypeError,"TypeError: __init__() takes 1 positional argument but 2 were given"
11721,"b'ValueError: could not broadcast input array from shape (2816,384) into shape (2698,384)'",2021-05-14T07:21:59Z,2021-05-17T14:10:13Z,,ValueError,"ValueError: could not broadcast input array from shape (2816,384) into shape (2698,384)"""
11720,b'RagRetriever fails to find faiss-gpu installed with pip not conda',2021-05-14T01:41:32Z,2021-05-17T08:49:01Z,,ImportError,"ImportError: "
11719,b'parameter `ignore_keys` of `trainer.predict` not accessible in `Trainer` or `TrainingArguments`',2021-05-14T00:42:46Z,2021-07-07T12:07:46Z,,,
11718,b'Fix loading the best model on the last stage of training',2021-05-13T18:49:13Z,2021-05-13T20:11:12Z,,,
11717,b'Fix T5 beam search when using parallelize',2021-05-13T18:05:54Z,2021-05-14T09:44:03Z,,RuntimeError,"RuntimeError: Input, output and indices must be on the current device"
11716,b'Refactor slow sentencepiece tokenizers.',2021-05-13T13:25:45Z,2021-07-21T08:28:31Z,,,
11715,b'Request for feature for setting batch size in pipeline when inference',2021-05-13T10:48:39Z,2021-06-20T15:01:43Z,,,
11714,b'Blender 9B model',2021-05-13T08:39:31Z,2021-06-16T15:44:55Z,,,
11713,"b'Unable to import transformers: ImportError: numpy>=1.17 is required for a normal functioning of this module, but found numpy==1.16.3'",2021-05-13T08:14:25Z,2021-05-19T09:03:28Z,,ImportError,"ImportError: numpy>=1.17 is required for a normal functioning of this module, but found numpy==1.16.3."
11712,b'Reformer for questions answering(squad)',2021-05-13T07:57:12Z,2021-06-20T15:01:44Z,,,
11711,b'How to accelerate the inference speed when using pipeline',2021-05-13T07:51:16Z,2021-06-20T15:01:45Z,,,
11710,b'AssertionError: internal model should be a reference to self.model',2021-05-13T07:49:34Z,2021-06-20T15:01:45Z,,AssertionError,"AssertionError: internal model should be a reference to self.model"
11709,b'Fix gpt-2 warnings',2021-05-13T07:29:23Z,2021-05-13T07:35:45Z,,,
11708,b'stop at load tokenizer_config.json when run barthez for mrpc ',2021-05-13T02:17:42Z,2021-05-13T02:20:37Z,,,
11707,"b""Loading Basic GPT-2  model gives warning that attention layers weren't loaded from pre-trained weights""",2021-05-12T23:50:28Z,2021-05-13T07:35:45Z,,,
11706,b'Add Cloud details to README',2021-05-12T15:07:29Z,2021-05-14T13:51:25Z,,,
11705,b'[Lazy init] Force fall back to slow init for composite models',2021-05-12T14:20:22Z,2021-05-12T14:52:54Z,,,
11704,b'[RAG] official facebook example code for RAG is not working anymore.',2021-05-12T13:14:35Z,2021-05-12T14:52:54Z,,AttributeError,"AttributeError: 'RagSequenceForGeneration' object has no attribute '_init_weights'"
11703,b'remove defaults to None if optional',2021-05-12T12:30:51Z,2021-05-12T13:11:11Z,,,
11702,b'channel_len specified but not used',2021-05-12T12:15:19Z,2021-07-18T15:02:11Z,,,
11701,b'[Flax] Updates README and fixes bug',2021-05-12T11:49:14Z,2021-05-12T12:52:53Z,Flax,,
11700,b'Offline installation of the transformers repo (error message)',2021-05-12T11:13:57Z,2021-06-20T15:01:46Z,,,
11699,b'Mixed precision training : link broken',2021-05-12T11:02:06Z,2021-05-12T11:45:20Z,,,
11698,b'Support ViT model in EncoderDecoder',2021-05-12T10:33:08Z,,WIP,,
11697,b'add the chinese ref in place to tackle the memory issue',2021-05-12T10:25:46Z,2021-06-20T15:01:47Z,,,
11696,b'[CLIP] fix example in config doc',2021-05-12T10:24:22Z,2021-05-12T13:48:53Z,,,
11695,b'[Flax] Fix BERT initialization & token_type_ids default',2021-05-12T09:41:05Z,2021-05-13T09:58:19Z,,,
11694,b'Fix clip docs',2021-05-12T09:17:24Z,2021-05-12T09:58:30Z,,,
11693,b'Flag to disable shuffling for data loader',2021-05-12T09:06:54Z,2021-05-13T12:20:00Z,,,
11692,b'fix url for CLIP doc',2021-05-12T08:36:18Z,2021-05-12T09:17:10Z,,,
11691,b'BertForSemanticSimilarity',2021-05-12T07:40:37Z,2021-06-20T15:01:48Z,,,
11690,b'add --validation_split_percentage for custom dataset',2021-05-12T03:04:08Z,2021-06-20T15:01:48Z,,,
11689,b'DeBERTa pretraining data preparation',2021-05-12T00:17:45Z,2021-06-20T15:01:49Z,,,
11688,b'Trainer skips training when continuing training with model.from_pretrained()',2021-05-11T20:17:08Z,2021-05-13T08:28:17Z,,,
11687,"b'Remove ""`optional`, defaults to :obj:`None`""'",2021-05-11T18:51:11Z,2021-05-12T13:11:11Z,,,
11686,b'Routing Transformers / Add Google PG-19 Models',2021-05-11T16:51:34Z,,New model,,
11685,b'[WIP] Add flax generate',2021-05-11T16:32:31Z,2021-05-19T17:25:53Z,,,
11684,b'Add new model RoFormer (use rotary position embedding )',2021-05-11T16:25:42Z,2021-05-20T12:00:34Z,,,
11683,b'Issue getting prediction_scores from TransfoXLHeadLM model when labels are provided',2021-05-11T16:09:49Z,2021-06-22T15:06:09Z,,,
11682,b'Test checkpointing',2021-05-11T15:48:05Z,2021-05-11T16:02:48Z,,,
11681,b'Cannot reproduce results from zeroshot demo app',2021-05-11T15:28:03Z,2021-07-19T15:07:31Z,,,
11680,b'[TokenClassification] Label realignment for subword aggregation',2021-05-11T14:26:54Z,2021-05-18T07:53:21Z,,,
11679,b'Grammar and style edits for the frontpage README',2021-05-11T13:58:14Z,2021-05-11T14:49:34Z,,,
11678,"b'Zeroshot pipeline performance worse on CPU when processing multiple texts as ""batch""'",2021-05-11T12:22:00Z,2021-05-12T16:51:24Z,,,
11677,b'Identify issue in slow torch tests',2021-05-11T10:12:16Z,,"WIP, Testing",,
11676,b'Merge strings that are being concatenated in the same line',2021-05-11T09:54:07Z,2021-06-19T15:01:56Z,,,
11675,b'Fix TF Roberta for mixed precision training',2021-05-11T08:49:36Z,2021-05-11T16:01:04Z,,,
11674,b'Add MacOS TF version',2021-05-11T08:13:03Z,2021-05-11T09:42:21Z,,,
11673,b'Add --text_column to run_summarization_no_trainer',2021-05-11T06:50:00Z,2021-05-11T11:58:39Z,,,
11672,b'Fix docstring of description about input_ids',2021-05-11T06:36:43Z,2021-05-11T12:12:03Z,,,
11671,b'Why run_translation.py automatically runs on CPU?',2021-05-11T05:44:02Z,2021-05-14T06:13:12Z,,,
11670,"b'license missing for xlm-roberta-large, and bert-base-spanish-wwm models'",2021-05-11T04:16:59Z,2021-06-19T15:01:57Z,,,
11669,b'[Question]  How to serialize and load a trained RoBERTa model?',2021-05-11T00:12:20Z,2021-06-19T15:01:57Z,,,
11668,"b""KeyError: 'bigbird_pegasus'""",2021-05-10T17:17:31Z,2021-05-11T07:04:15Z,,"KeyError, ImportError","KeyError: 'bigbird_pegasus'ImportError: cannot import name 'BigBirdPegasusForConditionalGeneration' from 'transformers' (unknown location)"
11667,b'[BigBird Pegasus] Add config to auto tokenizer',2021-05-10T16:22:10Z,2021-05-10T16:38:17Z,,,
11666,"b'GPTNeoForCausalLM: resuming Trainer from checkpoint causes Missing key(s) in state_dict: ""lm_head.weight""'",2021-05-10T16:19:20Z,2021-05-11T16:02:48Z,,RuntimeError,"RuntimeError: Error(s) in loading state_dict for GPTNeoForCausalLM:"
11665,b'[Question] How to move and reuse preprocessed dataset?',2021-05-10T15:53:18Z,2021-06-11T04:39:35Z,,,
11664,b'RuntimeError: Error(s) in loading state_dict for Wav2Vec2ForCTC',2021-05-10T15:43:02Z,2021-06-19T15:01:58Z,,RuntimeError,"RuntimeError: Error(s) in loading state_dict for Wav2Vec2ForCTC:"
11663,b'Save scaler state dict when checkpointing',2021-05-10T14:44:28Z,2021-05-10T14:58:31Z,,,
11662,b'IBERT: Testing the speedup',2021-05-10T14:36:21Z,2021-06-17T15:07:19Z,,,
11661,b'Update pretrained_models.rst',2021-05-10T13:03:16Z,2021-06-17T15:07:20Z,,,
11660,b'run_text_classification.py fix',2021-05-10T12:09:43Z,2021-05-10T13:28:05Z,,,
11659,"b""[Doc] Something wrong in description of 'DistilBertForSequenceClassification' in doc""",2021-05-10T12:02:31Z,2021-05-11T12:12:02Z,,,
11658,b'NCLL No space left on device Error while training with deepspeed',2021-05-10T11:15:39Z,2021-05-10T12:52:30Z,,"RuntimeError, subprocess.CalledProcessError, common_utils.errors.InternalError","RuntimeError: NCCL error in: /pytorch/torch/lib/c10d/ProcessGroupNCCL.cpp:784, unhandled system error, NCCL version 2.7.8subprocess.CalledProcessError: Command '['/usr/bin/python', '-u', 'nlp_ner_layoutlm/train_pipeline/training_step/training_script.py', '--local_rank=1', '--local_example_folder', '/f8a83c0e-1438-4a6c-a2d1-f5ed8cf76b0a/layoutlm_data', '--model_dir', '/mnt/pipeline/f8a83c0e-1438-4a6c-a2d1-f5ed8cf76b0a/pytorch_model', '--window_length', '512', '--batch_size', '8', '--weight_decay', '0.0', '--adam_epsilon', '1e-08', '--learning_rate', '2e-05', '--epochs', '200', '--seed', '11046060', '--bit_precision_fp16', '1', '--tagging_scheme', 'BILOU', '--profile_logs', '/mnt/pipeline/f8a83c0e-1438-4a6c-a2d1-f5ed8cf76b0a/tensorboard_logs', '--patience', '50', '--gradient_accumulation_steps', '2', '--warmup_steps', '300', '--composite', '0', '--n_transformer_layers', '1', '--composite_loss_weight', '0.5', '--self_training', '0', '--base_model', '/mnt/pipeline/LAYOUTLM_PRE_TRAINED_MODEL/base-uncased-huggingface']' returned non-zero exit status 1.common_utils.errors.InternalError: Something went wrong while training in distributed mode. Process finished with Exit Code 1"
11657,b'Memory Leak in Deberta (v1) Base ',2021-05-10T10:27:58Z,2021-08-30T15:08:12Z,,,
11656,b'DISTILBERT: run_squad.py not working',2021-05-10T10:20:19Z,2021-06-17T15:07:21Z,,**TypeError,"**TypeError: forward() got an unexpected keyword argument 'token_type_ids'**"
11655,b'Fine-tuning the Entire RAG Architecture (including DPR retriever)',2021-05-10T09:28:10Z,2021-05-31T15:10:26Z,,,
11654,b'add bigbird-pegasus evaluation notebook',2021-05-10T08:43:25Z,2021-05-10T08:48:21Z,,,
11653,b'Add DETR',2021-05-10T07:33:06Z,2021-06-09T15:51:13Z,,,
11652,b'[DOC] Fine-Tuning NER Custom Dataset Clarification',2021-05-10T06:54:43Z,2021-07-07T15:03:13Z,,`ValueError,"`ValueError: NumPy boolean array indexing assignment cannot assign 544 input values to the 464 output values where the mask is true`."
11651,b'BigBird on TPU',2021-05-10T06:44:15Z,2021-05-13T10:51:30Z,,,
11650,b'[Examples] Fix invalid links after reorg',2021-05-09T20:04:04Z,2021-05-10T05:46:48Z,,,
11649,b'Reformer inference widget is broken',2021-05-09T17:42:04Z,2021-06-17T15:07:22Z,,,
11648,b'Very large difference between the results after resume',2021-05-09T17:17:18Z,2021-05-09T18:26:54Z,,,
11647,"b""Key Error: 'pre-processing' during conversion from tatoeba to Marian model""",2021-05-09T09:28:22Z,,WIP,KeyError,"KeyError: 'pre-processing'"
11646,b'Strange implementation of `convert_tokens_to_string` in albert tokenizer.',2021-05-09T04:49:29Z,2021-07-21T08:47:49Z,WIP,,
11645,b'Bad result in fine-tuning XLNet for SQuAD',2021-05-08T12:34:09Z,2021-06-16T15:06:30Z,,,
11644,b'Cannot load studio-ousia/luke-base for AutoModelForTokenClassification',2021-05-08T12:18:47Z,2021-07-14T15:02:33Z,,ValueError,"ValueError: Unrecognized configuration class <class 'transformers.models.luke.configuration_luke.LukeConfig'> for this kind of AutoModel: AutoModelForTokenClassification."
11643,b'How to train TFBertForTokenClassification without padding mechanism ',2021-05-08T11:16:16Z,2021-06-16T15:06:31Z,,,
11642,"b'IDE cannot correctly navigate to references, It will navigate all object to `transformers/utils/dummy_pt_objects.py`.'",2021-05-08T08:59:18Z,2021-07-26T15:06:39Z,,,
11641,b'How to change training epochs when using run_summarization.py',2021-05-08T06:56:15Z,2021-06-16T15:06:31Z,,,
11640,b'Multilingual MobileBERT',2021-05-08T03:16:21Z,2021-05-25T01:24:48Z,New model,,
11639,b'I-BERT tokenizer not loading; example code not working.',2021-05-07T23:39:04Z,2021-05-10T15:52:59Z,,TypeError,"TypeError: expected str, bytes or os.PathLike object, not NoneType"
11638,b'[Deepspeed Wav2vec2] integration',2021-05-07T22:29:53Z,2021-06-08T19:32:04Z,,,
11637,b'[self-push CI] sync with self-scheduled',2021-05-07T20:55:09Z,2021-05-07T21:06:33Z,,,
11636,b'[examples] fix sys.path in conftest.py',2021-05-07T19:59:45Z,2021-05-07T21:44:22Z,,,
11635,b'Add visual + link to Premium Support webpage to README',2021-05-07T18:27:23Z,2021-05-17T09:28:57Z,,,
11634,b'Add missing git dependency for RAG example',2021-05-07T17:17:30Z,2021-05-10T05:49:53Z,,,
11633,b'Reduce to 1 worker and set timeout for GPU TF tests',2021-05-07T15:53:14Z,2021-05-07T15:55:21Z,,,
11632,b'Felix',2021-05-07T13:25:07Z,,New model,,
11631,b'Update code example',2021-05-07T12:05:03Z,2021-05-10T05:48:44Z,,,
11630,b'Simplify GPT-Neo local attention implementation',2021-05-07T11:36:33Z,2021-09-09T15:19:06Z,WIP,,
11629,b'LukeForEntitySpanClassification - ValueError: only one element tensors can be converted to Python scalars',2021-05-07T10:46:37Z,2021-05-07T12:11:59Z,,ValueError,"ValueError: only one element tensors can be converted to Python scalars"
11628,b'Fixes NoneType exception when topk is larger than one coupled with a small context in the Question-Answering pipeline',2021-05-07T10:26:03Z,2021-05-10T17:28:11Z,,,
11627,b'make fix copy',2021-05-07T09:00:45Z,2021-05-07T11:48:52Z,,,
11626,b'NN_pruning module for Question Answering',2021-05-07T08:46:56Z,2021-05-10T11:26:11Z,,RuntimeError,"RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.LongTensor [16]] is at version 3; expected version 2 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True)."
11625,b'CUDA error: an illegal memory access was encountered',2021-05-07T08:08:43Z,2021-05-08T05:42:55Z,,RuntimeError,"RuntimeError: CUDA error: an illegal memory access was encountered"
11624,b'Fix comment in run_clm_no_trainer.py',2021-05-07T05:28:31Z,2021-05-07T07:02:30Z,,,
11623,"b'When I use run_ner.py to fine-tune the model based on Bert, I cannot predict any entities'",2021-05-07T01:51:04Z,2021-05-28T02:00:04Z,,,
11622,b'[TokenClassification] Label realignment for subword aggregation',2021-05-07T01:27:40Z,2021-06-06T15:23:22Z,,,
11621,"b""Fix usage of head masks by PT encoder-decoder models' `generate()` function""",2021-05-06T21:48:02Z,2021-05-18T23:44:54Z,,,
11620,b'Fix RNG saves in distributed mode.',2021-05-06T20:48:09Z,2021-05-06T21:14:13Z,,,
11619,b'[cuda ext tests] fixing tests',2021-05-06T20:00:51Z,2021-05-06T20:35:28Z,,,
11618,b'[fairscale] rng states saving fails in an extended multi-gpu test',2021-05-06T19:58:24Z,2021-05-06T21:14:13Z,,,
11617,b'Adding TFWav2Vec2Model',2021-05-06T19:57:14Z,2021-06-14T17:58:55Z,,,
11616,"b'wrong default value of argument ""ignore_index"" in CrossEntropyLoss for loss calculation in models forward method'",2021-05-06T19:18:04Z,2021-06-16T15:06:33Z,,,
11615,b'[Lazy init] Fix edge cases',2021-05-06T18:07:02Z,2021-05-06T18:42:51Z,,,
11614,b'Vectorized Numpy to Torch based Functions for SpecAugment',2021-05-06T14:52:46Z,2021-05-25T13:05:17Z,,,
11613,b'Re-styling in seq2seq attention',2021-05-06T14:26:07Z,2021-05-06T18:24:19Z,,,
11612,b'Error when using Adafactor without learn rate',2021-05-06T11:45:46Z,2021-06-14T16:43:49Z,,,
11611,b'Fix typo in docstring',2021-05-06T11:17:41Z,2021-05-06T11:39:28Z,,,
11610,"b'In ""Question Answering"" separate context from question'",2021-05-06T09:59:08Z,2021-07-11T15:02:20Z,,,
11609,"b""[RAG] ModuleNotFoundError: No module named 'git' when finetuning the model""",2021-05-06T09:55:45Z,2021-05-10T08:35:42Z,,ModuleNotFoundError,"ModuleNotFoundError: No module named 'git'"
11608,b'Is it correct to load weights from task A to train task B',2021-05-06T07:42:49Z,2021-05-06T09:18:10Z,,,
11607,b'Fix Python version',2021-05-06T06:50:06Z,2021-05-06T06:50:11Z,,,
11606,b'Added Feature: Prefix decoding for wav2vec2 models',2021-05-06T06:21:17Z,2021-09-08T15:02:35Z,,,
11605,b'fix typo in command',2021-05-06T04:29:43Z,2021-05-06T07:02:54Z,,,
11604,b'Model type to AutoModelForQuestionAnswering incorrect',2021-05-06T03:40:04Z,2021-06-14T15:07:08Z,,ValueError,"ValueError: Unrecognized configuration class <class 'transformers.models.gpt2.configuration_gpt2.GPT2Config'> for this kind of AutoModel: AutoModelForQuestionAnswering."
11603,b'TFWav2Vec2Model',2021-05-06T01:29:15Z,2021-06-14T19:04:21Z,New model,,
11602,"b""Seq2SeqTrainer not working for a list of inputs: TypeError: can't convert np.ndarray of type numpy.object_""",2021-05-06T00:43:00Z,2021-05-16T13:05:00Z,Migration,,
11601,b'Using `TFGPT2LMHeadModel.generate` with tf.distribute.TPUStrategy and tf.function',2021-05-06T00:12:08Z,2021-06-14T15:07:09Z,,"NotImplementedError, OperatorNotAllowedInGraphError","NotImplementedError: TPUStrategy.run(fn, ...) does not support pure eager execution. please make sure the function passed into `strategy.run` is a `tf.function` or `strategy.run` is called inside a `tf.function` if eager behavior is enabled.OperatorNotAllowedInGraphError: in user code:"
11600,b'[consistent use] `F` vs. `nn.functional`',2021-05-05T19:27:46Z,2021-06-14T19:47:00Z,WIP,,
11599,b'Auto modelcard',2021-05-05T19:06:15Z,2021-05-11T15:30:34Z,,,
11598,b'Add the ImageClassificationPipeline',2021-05-05T18:36:13Z,2021-05-07T12:08:41Z,,,
11597,"b'Is the ""dummy inputs and standard lengths"" implication, when using tracing for exporting a model, still true?'",2021-05-05T12:49:22Z,2021-06-13T15:01:46Z,,,
11596,b'fix head_mask for albert encoder part(`AlbertTransformer`)',2021-05-05T12:47:14Z,2021-05-06T06:18:02Z,,,
11595,b'Accept tensorflow-rocm package when checking TF availability',2021-05-05T12:42:12Z,2021-05-05T18:44:30Z,,,
11594,b'Distil BART for text simplification',2021-05-05T09:49:51Z,,New model,,
11593,b'[WIP] HF style example flax version',2021-05-05T09:44:46Z,2021-06-07T13:48:29Z,,,
11592,b'Cannot use multiple GPUs to finetune RAG using sample code with customized knowledge',2021-05-05T09:38:24Z,2021-06-14T15:07:11Z,,,
11591,b'add importlib_metadata and huggingface_hub as dependency in the conda recipe',2021-05-05T06:45:56Z,2021-05-05T07:36:18Z,,,
11590,b'evaluation in TFTrainer does not run on GPU',2021-05-04T22:38:17Z,2021-06-10T21:18:59Z,,,
11589,b'Fix failing test on Windows Platform',2021-05-04T22:31:36Z,2021-05-20T23:54:23Z,,,
11588,b'[trainer] document resume randomness',2021-05-04T20:16:01Z,2021-05-04T21:17:12Z,,,
11587,b'Removes SageMakerTrainer code but keeps class as wrapper',2021-05-04T17:12:33Z,2021-05-04T18:31:19Z,,,
11586,b'Windows Test Errors ',2021-05-04T17:02:03Z,2021-06-13T15:01:47Z,,`RuntimeError,"`RuntimeError: 0INTERNAL ASSERT FAILED at ""..\\torch\\csrc\\jit\\ir\\alias_analysis.cpp"":532, please report a bug to PyTorch. We don't have an op for aten::constant_pad_nd but it isn't a special case.  Argument types: Tensor, int[], bool,`"
11585,b'[template runner CI] copies need to be fixed too',2021-05-04T16:47:07Z,2021-05-05T07:35:15Z,,,
11584,b'Punctuation in Wav2Vec2',2021-05-04T16:29:28Z,2021-06-17T15:07:24Z,,,
11583,b'Wrong results for GLUE task STS-B',2021-05-04T15:42:13Z,2021-05-20T13:55:14Z,,,
11582,b'Reproducible checkpoint',2021-05-04T14:57:12Z,2021-05-04T20:20:57Z,,,
11581,b'unable to save model1.h5 .I am using huggingface distilbert',2021-05-04T13:56:58Z,2021-06-12T15:01:49Z,,NotImplementedError,NotImplementedError: 
11580,"b""ValueError: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length""",2021-05-04T12:28:47Z,2021-05-04T13:31:34Z,,,
11579,b'Support saving (and loading) models to a remote bucket',2021-05-04T12:22:42Z,2021-06-20T15:01:51Z,,,
11578,b'How to use `model.generate` with custom model with additional parameters?',2021-05-04T12:16:43Z,2021-05-05T08:43:36Z,,,
11577,b'potential mismatch between `save_pretrained` and `from_pretrained` for `AutoTokenizer`',2021-05-04T10:15:39Z,,WIP,OSError,"OSError: Can't load config for 'bert-base-multilingual-cased-good'. Make sure that:"
11576,b'no connection error',2021-05-04T09:57:40Z,2021-06-12T15:01:49Z,,,
11575,b'longform',2021-05-04T06:39:50Z,2021-05-04T07:50:57Z,,,
11574,b'Number of BART layers is confusing in the Pretrained models page',2021-05-04T04:10:14Z,2021-06-12T15:01:50Z,,,
11573,b'Make quality scripts work when one backend is missing.',2021-05-04T00:09:18Z,2021-05-04T13:53:45Z,,,
11572,b'[Deepspeed] fix resize_token_embeddings',2021-05-03T19:15:07Z,2021-05-03T20:12:06Z,,,
11571,b'reformer-enwik8 Fine-tuning',2021-05-03T18:08:23Z,2021-06-12T15:01:51Z,,,
11570,"b""[fixup/style] requires TF but doesn't say that cleanly""",2021-05-03T16:48:59Z,2021-05-04T13:53:44Z,,AttributeError,"AttributeError: module transformers.models.auto has no attribute modeling_tf_auto"
11569,b'Fix metric computation in `run_glue_no_trainer`',2021-05-03T15:21:36Z,2021-05-03T15:42:56Z,,,
11568,b'`TypeError: TextInputSequence must be str` from Fast Tokenizer',2021-05-03T14:57:11Z,2021-06-11T15:03:37Z,,TypeError,"TypeError: TextInputSequence must be str"
11567,b'Temporary files from an interrupted download litter the disk',2021-05-03T14:10:53Z,2021-06-11T15:03:38Z,,,
11566,b'Fixes a useless warning in `generate`.',2021-05-03T12:43:16Z,2021-05-03T16:48:14Z,,,
11565,"b""hyperparameter_search raytune: ModuleNotFoundError: No module named 'datasets_modules'""",2021-05-03T11:17:14Z,2021-07-19T08:32:41Z,,"ray.exceptions.RayTaskError(TuneError), ray.tune.error.TuneError, ModuleNotFoundError, CondaError","ray.exceptions.RayTaskError(TuneError): ray::ImplicitFunc.train_buffered() (pid=4311, ip=172.16.9.2)ray.tune.error.TuneError: Trial raised an exception. Traceback:ModuleNotFoundError: No module named 'datasets_modules'CondaError: KeyboardInterrupt"
11564,b'Adds Flax BERT finetuning example on GLUE',2021-05-03T11:03:03Z,2021-05-11T18:03:02Z,,,
11563,b'Remove `datasets` submodule.',2021-05-03T10:02:21Z,2021-05-03T10:02:33Z,,,
11562,b'[Wav2Vec2] Fix convert',2021-05-03T09:38:51Z,2021-05-03T09:53:30Z,,,
11561,"b""AttributeError: 'TrainingArguments' object has no attribute 'resume_from_checkpoint' in training GPT2""",2021-05-03T09:14:32Z,2021-05-03T12:34:25Z,,AttributeError,"AttributeError: 'TrainingArguments' object has no attribute 'resume_from_checkpoint'"
11560,"b""Error In Fine-Tuning Transformer XL ValueError: The two structures don't have the same sequence length. Input structure has length 3, while shallow structure has length 2.""",2021-05-03T08:22:45Z,2021-07-12T15:07:30Z,,ValueError,"ValueError: in user code:"
11559,b'fix the mlm longformer example by changing [MASK] to <mask>',2021-05-03T07:49:14Z,2021-05-03T11:43:30Z,,,
11558,b'[Flax BERT/Roberta] few small fixes',2021-05-03T07:21:35Z,2021-05-03T08:35:06Z,,,
11557,b'DPR with ELECTRA models',2021-05-03T07:03:36Z,2021-06-11T15:03:39Z,,,
11556,b'FlaxGPT2',2021-05-03T04:54:06Z,2021-05-18T21:50:51Z,,,
11555,b'NaN Person/Spearman corr. when fine-tuning BERT with example code and commands',2021-05-03T04:50:11Z,2021-05-03T15:42:56Z,,,
11549,b'Bugs when trying to train a T5model from scratch in the run_summarization.py script',2021-05-03T00:41:23Z,2021-06-11T15:03:40Z,,ValueError,"ValueError: You have to specify either decoder_inputs or decoder_inputs_embeds"
11548,b'why run_translation.py automatically is running on cpu?',2021-05-02T18:41:18Z,2021-06-12T15:01:52Z,,,
11547,b'tokenizer not padding input_ids',2021-05-02T13:33:47Z,2021-05-03T13:32:50Z,,,
11546,b'T5 fp16 crashes on the CPU (but works on CUDA)',2021-05-02T08:07:03Z,2021-07-08T15:06:36Z,,RuntimeError,"RuntimeError: ""baddbmm__mkl"" not implemented for 'Half'"
11545,b'Strugle to change `num_labels` of roberta-large-mnli',2021-05-02T06:28:58Z,2021-06-14T15:07:13Z,,`RuntimeError,"`RuntimeError: Error(s) in loading state_dict for RobertaForSequenceClassification: size mismatch for classifier.out_proj.weight: copying a param with shape torch.Size([3, 1024]) from checkpoint, the shape in current model is torch.Size([2, 1024])."
11544,"b""'return_dict_in_generate' and 'output_scores' argument in BartForConditionalGeneration.generate()""",2021-05-02T01:47:54Z,2021-05-07T06:08:24Z,,,
11543,b'Error when trying to use GPTNeo model',2021-05-02T00:23:10Z,2021-05-03T20:03:56Z,,AttributeError,"AttributeError: type object 'GPTNeoForCausalLM' has no attribute 'from_pretrained"
11541,"b'Pegasus tokenizer does not have bos token, cannot pretrain'",2021-05-01T21:34:47Z,2021-07-10T15:02:00Z,,ValueError,"ValueError: Expected input batch_size (10) to match target batch_size (7)."
11540,b'Fix examples in M2M100 docstrings',2021-05-01T20:12:54Z,2021-05-03T05:26:31Z,,,
11539,b'Unable to load DistilBertModel during training ',2021-05-01T19:42:21Z,2021-05-05T05:06:53Z,,,
11538,b'[Wav2vec2] Fixed tokenization mistakes while adding single-char tokens to tokenizer',2021-05-01T16:43:35Z,2021-05-03T15:19:13Z,,,
11537,b'[Flax] Add FlaxBart models',2021-05-01T08:58:32Z,2021-06-14T09:46:09Z,,,
11536,b'Adafactor gives RuntimeError: tensors must be 2-D',2021-05-01T07:22:16Z,2021-12-12T12:31:46Z,,RuntimeError,"RuntimeError: tensors must be 2-D"
11535,b'Vectorized Numpy based functions to Torch based Functions for SpecAugment.',2021-05-01T07:07:53Z,2021-05-06T14:53:14Z,,,
11534,"b'How to run transformer model like t5-small, facebook/bart-large-cnn without loading pretrained weights?'",2021-04-30T17:35:28Z,2021-06-09T15:06:55Z,,,
11533,b'Update training tutorial',2021-04-30T16:43:28Z,2021-05-03T17:18:46Z,,,
11532,b'Files not accessible via IPv6',2021-04-30T15:58:51Z,2021-08-18T15:52:30Z,,,
11531,b'Adding custom tokens makes the T5Tokenizer always strip spaces',2021-04-30T15:55:42Z,,WIP,,
11530,b'generate text with inputs_embeds (instead of input_ids) for T5.',2021-04-30T15:53:04Z,2021-05-23T15:50:00Z,,,
11529,b'Deberta v2 Fast Tokenizer',2021-04-30T15:39:18Z,2021-06-17T15:07:26Z,,,
11528,b'Adds Flax BERT finetuning example on GLUE',2021-04-30T13:03:17Z,2021-05-03T10:59:54Z,,,
11527,b'Run model templates on master',2021-04-30T12:41:23Z,2021-04-30T12:47:13Z,,,
11526,b'Add Stas and Suraj as authors',2021-04-30T12:31:56Z,2021-04-30T13:03:13Z,,,
11525,"b'Adding support for `pipeline(""automatic-speech-recognition"")`.'",2021-04-30T11:04:21Z,2021-07-07T14:06:48Z,,,
11524,"b'[examples, translation/summerization] resize token embeds'",2021-04-30T10:00:30Z,2021-04-30T12:47:01Z,,,
11523,b'Distributed multi-node support for CPU cluster',2021-04-30T09:14:26Z,2021-06-07T15:18:19Z,,,
11522,b'Compute probability of target sentences given an input',2021-04-30T08:48:15Z,2021-06-07T15:18:20Z,,,
11521,b'How to set up a custom tokenizer for distilbart',2021-04-30T08:44:31Z,2021-05-03T13:17:01Z,,,
11520,b'[Master] Make style',2021-04-30T07:54:42Z,2021-04-30T07:54:58Z,,,
11519,b'RoBERTa adds two sep tokens',2021-04-30T07:13:23Z,2021-04-30T08:35:32Z,,,
11518,"b'BART summarization, tokenizer not working'",2021-04-30T02:49:51Z,2021-04-30T12:47:01Z,,,
11517,b'rag import not on windows',2021-04-29T18:10:56Z,2021-04-30T10:30:05Z,,,
11516,b'Run_summarization not working for mbart50',2021-04-29T17:24:47Z,2021-06-07T15:18:21Z,,TypeError,"TypeError: int() argument must be a string, a bytes-like object or a number, not 'NoneType'"
11515,b'Issues with TFGPT2ForSequenceClassification',2021-04-29T17:08:09Z,2021-06-07T15:18:22Z,,ValueError,"ValueError: in user code:"
11514,b'solved coefficient issue for the TF version of gelu_fast',2021-04-29T16:22:08Z,2021-04-29T19:47:26Z,,,
11513,b'Improve task summary docs',2021-04-29T16:14:11Z,2021-04-30T13:06:47Z,,,
11512,b'Piece A',2021-04-29T16:12:50Z,2021-06-07T15:18:23Z,,,
11511,b'Fix do_eval default value in training_args.py',2021-04-29T15:57:16Z,2021-04-30T12:35:12Z,,,
11510,b'[Examples] Added support for test-file in QA examples with no trainer',2021-04-29T15:48:46Z,2021-04-30T13:02:50Z,,,
11509,"b'I-BERT: expected str, bytes or os.PathLike object, not NoneType'",2021-04-29T14:06:34Z,2021-05-07T08:38:16Z,,TypeError,"TypeError: expected str, bytes or os.PathLike object, not NoneType"
11508,b'Help understanding how to build a dataset for language as with the old TextDataset',2021-04-29T13:03:37Z,2021-04-29T13:16:21Z,,,
11507,b'Fine-Tuning TFGPT2LMHeadModel / What to pass to fit',2021-04-29T11:56:06Z,2021-06-07T15:18:24Z,,,
11506,b'[WIP] Adding DETR',2021-04-29T11:55:08Z,2021-05-10T07:30:41Z,,,
11505,b'encoder decoder in transformers',2021-04-29T11:54:47Z,2021-06-07T15:18:25Z,,,
11504,b'Issue in checkpointing',2021-04-29T09:24:25Z,2021-05-04T20:20:57Z,,,
11503,b'[Examples] Check key exists in datasets first',2021-04-29T08:50:44Z,2021-05-09T19:42:38Z,,,
11502,b'Pin HuggingFace Hub dependency',2021-04-29T08:45:03Z,2021-04-30T06:57:51Z,,,
11501,b'Penalise n-gram repetition in generated sequences',2021-04-29T08:43:14Z,2021-06-07T15:18:26Z,,,
11500,b'not able load model from pipeline NotFound error',2021-04-29T06:35:41Z,2021-06-07T15:18:27Z,,"requests.exceptions.HTTPError, OSError","requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/textattack/bert-base-uncased-yelp-polarity/resolve/main/tf_model.h5OSError: Can't load weights for 'textattack/bert-base-uncased-yelp-polarity'. Make sure that:"
11499,b'[DeepSpeed] fp32 support',2021-04-28T22:41:36Z,2021-04-30T19:51:49Z,DeepSpeed,,
11498,b'[Flax] Add docstrings & model outputs',2021-04-28T19:39:46Z,2021-04-29T10:04:51Z,,,
11497,b'Reformat to make code clearer in tokenizer call',2021-04-28T19:29:42Z,2021-04-29T11:51:10Z,,,
11496,b'Update TF text classification example',2021-04-28T18:43:14Z,2021-04-30T12:45:33Z,,,
11495,b'mbart encoder decoder model',2021-04-28T17:01:13Z,2021-06-07T15:18:28Z,,,
11494,b'correct incorrect dimension comment in Longformer model',2021-04-28T15:32:38Z,2021-04-30T07:42:13Z,,,
11493,b'[Docs] remove paragraph on CI from installation instructions',2021-04-28T14:20:25Z,2021-04-28T15:16:42Z,,,
11492,b'Split checkpoint from model_name_or_path in examples',2021-04-28T13:28:32Z,2021-04-29T22:33:48Z,,,
11491,b'Tensorflow \xe2\x80\x9cIndex out of bound\xe2\x80\x9d error when trying to use the TF Longformer transformer in a custom TF network',2021-04-28T13:13:30Z,2021-06-06T15:06:12Z,,tensorflow.python.framework.errors_impl.InvalidArgumentError,"tensorflow.python.framework.errors_impl.InvalidArgumentError: slice index -63 of dimension 0 out of bounds. [Op:StridedSlice] name: model/tf.__operators__.getitem/strided_slice/"
11490,b'add importlib_metadata as dependency as it is required for py<3.8',2021-04-28T12:38:08Z,2021-05-04T07:45:13Z,,,
11489,b'Update README.md',2021-04-28T11:24:41Z,2021-04-30T08:29:59Z,,,
11488,"b'TFLongformerForMaskedMLM example throws ValueError ""shapes are incompatible""'",2021-04-28T10:30:31Z,2021-05-03T11:43:30Z,,ValueError,"ValueError: Shapes (11,) and (9,) are incompatible"
11487,b'Importing problem ',2021-04-28T08:37:54Z,2021-06-06T15:06:13Z,,,
11486,b'Update `PreTrainedTokenizerBase` to check/handle batch length for `text_pair` parameter',2021-04-28T07:54:34Z,2021-04-28T14:11:18Z,,,
11485,b'run_mlm.py : Missing key(s) in state_dict & Unexpected key(s) in state_dict',2021-04-28T07:29:22Z,2021-04-29T22:33:48Z,,RuntimeError,"RuntimeError: Error(s) in loading state_dict for RobertaForMaskedLM:"
11484,"b'MBartForConditionalGeneration.from_pretrained(""facebook/mbart-large-50"") Not working'",2021-04-28T03:56:36Z,2021-06-06T15:06:14Z,,,
11483,b'The performance of the huggingface QA model depend on the order in which it loads',2021-04-28T02:37:03Z,2021-04-28T05:30:54Z,,,
11482,b'[Docs] Clarify Subphrase classification?',2021-04-28T01:05:23Z,2021-04-28T02:18:11Z,,,
11481,b'Fix checkpointing in SageMaker MP',2021-04-28T00:01:56Z,2021-05-03T17:18:27Z,,,
11480,b'Error In Running Predictions for run_text_classification.py',2021-04-27T23:10:55Z,2021-07-11T15:02:22Z,,UnicodeDecodeError,"UnicodeDecodeError: 'utf-8' codec can't decode byte 0x89 in position 0: invalid start byte"
11479,b'[Docs] Add Caching Example For CI? ',2021-04-27T22:27:54Z,2021-04-28T15:16:42Z,,,
11478,b'[Flax] Add FlaxBart model',2021-04-27T16:48:29Z,2021-06-14T09:46:09Z,,,
11477,b'Move integrations imports before any ML framework imports',2021-04-27T16:10:59Z,2021-06-06T15:06:15Z,,,
11476,b'Adding new argument `max_new_tokens` for generate.',2021-04-27T16:04:35Z,2021-05-27T12:22:58Z,,,
11475,"b'Experimental symbolic tracing feature with torch.fx for BERT, ELECTRA and T5'",2021-04-27T16:02:47Z,2021-05-14T18:57:30Z,,,
11474,b'can not import mbart and mT5 modeling file',2021-04-27T14:52:31Z,2021-04-30T13:14:06Z,,ImportError,"ImportError: cannot import name 'modeling_mbart'"
11473,b'Can not import modeling_mbart',2021-04-27T14:49:28Z,2021-04-28T14:26:20Z,,,
11472,b'Update min versions in README and add Flax',2021-04-27T14:01:49Z,2021-04-28T13:10:06Z,,,
11471,b'Pytorch - Lazy initialization of models',2021-04-27T13:22:46Z,2021-05-05T15:22:21Z,,,
11470,b'[FlaxRoberta] Add FlaxRobertaModels & adapt run_mlm_flax.py',2021-04-27T12:00:00Z,2021-05-04T17:57:59Z,,,
11469,b'Train GPT2 with Trainer & TrainingArguments using/specifying attention_mask',2021-04-27T11:42:08Z,2021-06-04T15:18:47Z,,,
11468,b'binary classification does not work with a large amount of data',2021-04-27T09:57:11Z,2021-06-04T15:18:48Z,,,
11467,b'Finish Making Quick Tour respect the model object',2021-04-27T05:37:36Z,2021-04-27T14:04:12Z,,,
11466,b'fix docs for decoder_input_ids',2021-04-27T05:24:50Z,2021-04-27T14:06:37Z,,,
11465,b'[resume optimization] skip loading pretrained weights on resume',2021-04-27T01:57:38Z,,WIP,,
11464,b'[DeepSpeed] ZeRO-Infinity integration: getting started and issues',2021-04-27T01:29:59Z,2021-06-05T15:04:44Z,DeepSpeed,,
11463,"b""[model loading] don't init weights for pretrained models""",2021-04-27T01:01:27Z,2021-04-28T03:26:19Z,,,
11462,b'update QuickTour docs to reflect model output object',2021-04-27T00:27:00Z,2021-04-27T02:18:38Z,,,
11461,b'T5-large FP16 produces nan in loss',2021-04-27T00:14:01Z,2021-06-05T15:04:46Z,,,
11460,b'support batch-sampler in trainer ',2021-04-26T21:18:30Z,2021-06-04T15:18:49Z,,,
11459,b'extending metric_for_best_model to a list of strings ',2021-04-26T20:49:26Z,2021-06-04T15:18:50Z,,,
11458,"b'""Is next sentence"" pre-training task availability for Language Modeling scripts'",2021-04-26T20:23:36Z,2021-06-04T15:18:50Z,,,
11457,b'Can this `@slow` annotation be removed at barthez tokenizer test=',2021-04-26T19:47:48Z,2021-04-26T19:54:30Z,,,
11456,b'Perturb Hidden-State in Encoder-Decoder Models',2021-04-26T17:43:44Z,2021-04-28T05:25:03Z,,,
11455,"b""Unable to use custom dataset: AttributeError: 'list' object has no attribute 'keys'""",2021-04-26T16:22:27Z,2021-05-03T16:25:16Z,,AttributeError,"AttributeError: 'list' object has no attribute 'keys'"
11454,"b""cannot import name 'set_seed' from 'transformers'""",2021-04-26T16:11:09Z,2021-06-04T15:18:51Z,,,
11453,b'Give each hub test a different repo name',2021-04-26T15:38:17Z,2021-04-26T15:52:23Z,,,
11452,"b""wav2vec2 doesn't work with torch.distributed.launch & multi GPU""",2021-04-26T12:45:05Z,2021-07-05T15:07:36Z,,"RuntimeError, subprocess.CalledProcessError","RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by (1) passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`; (2) making sure all `forward` function outputs participate in calculating loss. If you already have done the above two steps, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).subprocess.CalledProcessError: Command '['/home/aidealab/.conda/envs/hf/bin/python', '-u', 'run_common_voice.py', '--local_rank=3', '--model_name_or_path=facebook/wav2vec2-large-xlsr-53', '--dataset_config_name=tr', '--output_dir=/home/aidealab/workspace/transformers/examples/research_projects/wav2vec2/outputs', '--overwrite_output_dir', '--num_train_epochs=5', '--per_device_train_batch_size=16', '--learning_rate=3e-4', '--warmup_steps=500', '--evaluation_strategy=steps', '--save_steps=400', '--eval_steps=400', '--logging_steps=400', '--save_total_limit=3', '--freeze_feature_extractor', '--feat_proj_dropout=0.0', '--layerdrop=0.1', '--gradient_checkpointing', '--fp16', '--group_by_length', '--do_train', '--do_eval']' returned non-zero exit status 1."
11451,b'mBART and DataCollatorForLanguageModeling: index -1 is out of bounds for dimension 1 with size N',2021-04-26T11:46:39Z,2021-06-03T15:11:38Z,,RuntimeError,"RuntimeError: index -1 is out of bounds for dimension 1 with size 309"
11450,b'[Black] Pin Version',2021-04-26T11:39:31Z,2021-04-26T12:09:05Z,,,
11449,b'Clarify description of the is_split_into_words argument',2021-04-26T10:33:28Z,2021-04-26T15:29:36Z,,,
11448,b'Activating gradient checkpointing',2021-04-26T09:48:12Z,2021-04-26T21:26:46Z,,,
11447,"b'Google Colab TypeError: expected str, bytes or os.PathLike object, not NoneType'",2021-04-26T09:43:01Z,2021-04-30T07:46:33Z,,TypeError,"TypeError: expected str, bytes or os.PathLike object, not NoneType"
11446,b'[wav2vec] deepspeed eval bug in the case of >1 gpus',2021-04-26T09:41:44Z,2021-06-08T19:32:04Z,DeepSpeed,,
11445,b'CLIP',2021-04-26T09:06:55Z,2021-05-12T08:18:16Z,PR for Model Addition,,
11444,b'Variable Correction for Consistency in Distillation Example',2021-04-26T08:42:31Z,2021-04-26T17:30:48Z,,,
11443,b'BERT model gets fairly random results',2021-04-26T08:15:47Z,2021-04-27T08:53:54Z,,,
11442,b'Upgrade Black to version 21.4b0',2021-04-26T07:07:59Z,2021-04-26T11:50:34Z,,,
11441,b'Minor error on example distillation script',2021-04-26T06:50:07Z,2021-04-27T02:32:36Z,,AttributeError,"AttributeError: 'Namespace' object has no attribute 'n_gpu'"
11440,b'Feedback whilst resuming',2021-04-26T06:39:54Z,2021-04-28T14:29:02Z,,,
11439,b'[BigBird] enable BigBirdForQuestionAnswering to return pooler output',2021-04-26T05:15:08Z,2021-04-26T07:05:53Z,,,
11438,b'[docs] fix invalid class name',2021-04-26T05:00:26Z,2021-04-26T15:37:32Z,,,
11437,b'[Makefile] make sure to test against the local checkout',2021-04-26T04:59:56Z,2021-04-26T15:42:43Z,,ImportError,"ImportError: cannot import name 'PushToHubMixin' from 'transformers.file_utils' (/mnt/nvme1/code/huggingface/transformers-master/src/transformers/file_utils.py)"
11436,b'\xe6\xa2\xaf\xe5\xba\xa6\xe7\x88\x86\xe7\x82\xb8\xe9\x97\xae\xe9\xa2\x98',2021-04-26T00:53:48Z,2021-06-04T15:18:52Z,,,
11435,b'convert gpt2 from tensorflow to pytorch',2021-04-25T23:04:46Z,2021-06-03T15:11:39Z,,AttributeError,"AttributeError: 'GPT2Model' object has no attribute '_step'"
11434,b'Updating checkpoint for GPT2ForSequenceClassification #11334',2021-04-25T21:26:24Z,2021-04-26T04:58:51Z,,,
11433,b'tensorflow version is not able to pick the trained model from local directory in an air gapped system',2021-04-25T18:09:07Z,2021-06-03T15:11:40Z,,,
11432,b'Typo fixes',2021-04-25T17:41:07Z,2021-04-26T13:14:25Z,,,
11431,b'Accepts BatchEncoding in LengthGroupedSampler',2021-04-25T13:33:02Z,2021-04-30T12:27:46Z,,,
11430,b'Fix `sp_model_kwargs` param missing at unpickle in `XLMRobertaTokenizer`',2021-04-25T13:14:05Z,2021-04-30T07:44:59Z,,,
11429,b'`sp_model_kwargs` param missing at unpickle in `XLMRobertaTokenizer`',2021-04-25T13:08:10Z,2021-04-30T07:45:18Z,,,
11428,"b""RoBERTa: ValueError: The two structures don't have the same sequence length. Input structure has length 5, while shallow structure has length 4.""",2021-04-25T12:06:10Z,2021-06-02T15:19:05Z,,"InvalidArgumentError, ValueError, TypeError","InvalidArgumentError: TypeError: `generator` yielded an element that did not match the expected structure. The expected structure was ({'input_ids': tf.int32, 'attention_mask': tf.int32, 'feature_index': tf.int64, 'qas_id': tf.string}, {'start_positions': tf.int64, 'end_positions': tf.int64, 'cls_index': tf.int64, 'p_mask': tf.int32, 'is_impossible': tf.int32}), but the yielded element was ({'input_ids': [0, 1779, 222, 12674, 1755, 386, 1959, 1406, 116, 2, 2, 12674, 12695, 272, 354, 6591, 10690, 1634, 12, 43732, 48229, 5605, 43621, 16948, 49066, 267, 35423, 10659, 282, 1090, 35423, 10278, 73, 19417, 12, 975, 2191, 12, 28357, 43, 36, 5400, 772, 204, 6, 14130, 43, 16, 41, 470, 3250, 6, 2214, 9408, 6, 638, 3436, 8, 3390, 4, 8912, 8, 1179, 11, 2499, 6, 1184, 6, 79, 3744, 11, 1337, 6970, 8, 7950, 9150, 25, 10, 920, 6, 8, 1458, 7, 9444, 11, 5, 628, 4525, 29, 25, 483, 3250, 9, 248, 947, 387, 1816, 12, 13839, 23313, 18, 7442, 4, 1554, 4628, 30, 69, 1150, 6, 4101, 16152, 10690, 1634, 6, 5, 333, 1059, 65, 9, 5, 232, 18, 275, 12, 11393, 1816, 1134, 9, 70, 86, 4, 2667, 25224, 794, 5, 800, 9, 12674, 12695, 18, 2453, 2642, 6, 34880, 9412, 11, 3437, 36, 35153, 238, 61, 2885, 69, 25, 10, 5540, 3025, 3612, 6, 2208, 292, 12727, 4229, 8, 3520, 5, 18919, 6003, 727, 346, 12, 1264, 7695, 22, 347, 36616, 11, 3437, 113, 8, 22, 30047, 5637, 845, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...ValueError: The two structures don't have the same sequence length. Input structure has length 5, while shallow structure has length 4.TypeError: `generator` yielded an element that did not match the expected structure. The expected structure was ({'input_ids': tf.int32, 'attention_mask': tf.int32, 'feature_index': tf.int64, 'qas_id': tf.string}, {'start_positions': tf.int64, 'end_positions': tf.int64, 'cls_index': tf.int64, 'p_mask': tf.int32, 'is_impossible': tf.int32}), but the yielded element was ({'input_ids': [0, 1779, 222, 12674, 1755, 386, 1959, 1406, 116, 2, 2, 12674, 12695, 272, 354, 6591, 10690, 1634, 12, 43732, 48229, 5605, 43621, 16948, 49066, 267, 35423, 10659, 282, 1090, 35423, 10278, 73, 19417, 12, 975, 2191, 12, 28357, 43, 36, 5400, 772, 204, 6, 14130, 43, 16, 41, 470, 3250, 6, 2214, 9408, 6, 638, 3436, 8, 3390, 4, 8912, 8, 1179, 11, 2499, 6, 1184, 6, 79, 3744, 11, 1337, 6970, 8, 7950, 9150, 25, 10, 920, 6, 8, 1458, 7, 9444, 11, 5, 628, 4525, 29, 25, 483, 3250, 9, 248, 947, 387, 1816, 12, 13839, 23313, 18, 7442, 4, 1554, 4628, 30, 69, 1150, 6, 4101, 16152, 10690, 1634, 6, 5, 333, 1059, 65, 9, 5, 232, 18, 275, 12, 11393, 1816, 1134, 9, 70, 86, 4, 2667, 25224, 794, 5, 800, 9, 12674, 12695, 18, 2453, 2642, 6, 34880, 9412, 11, 3437, 36, 35153, 238, 61, 2885, 69, 25, 10, 5540, 3025, 3612, 6, 2208, 292, 12727, 4229, 8, 3520, 5, 18919, 6003, 727, 346, 12, 1264, 7695, 22, 347, 36616, 11, 3437, 113, 8, 22, 30047, 5637, 845, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,..."
11427,b'Fix link to the TPU launcher script in the pytorch examples',2021-04-25T11:12:50Z,2021-04-26T13:08:43Z,,,
11426,b'[Flax] Add Electra models',2021-04-25T10:48:13Z,2021-05-04T18:56:10Z,,,
11425,"b""ALBERT: The following keyword arguments are not supported by this model: ['cls_index', 'p_mask', 'is_impossible'].""",2021-04-25T08:09:51Z,2021-06-03T15:11:41Z,,ValueError,"ValueError: in user code:"
11424,b'Simple questions about EncoderDecoderModel',2021-04-25T08:09:45Z,2021-06-03T15:11:42Z,,,
11423,b'IBert: What would be the possible reason `IntLayerNorm` does not decrease the loss?',2021-04-25T05:01:59Z,2021-04-25T08:44:29Z,,,
11422,"b""Transformers 4.1.1 & Tensorflow 2.0, AttributeError: module'tensorflow_core.keras.activations' has no attribute'swish' """,2021-04-25T03:56:29Z,2021-04-28T13:10:06Z,,AttributeError,"AttributeError: module'tensorflow_core.keras.activations' has no attribute'swish' "
11421,"b'Race condition when using --save_total_limit, --load_best_model_at_end and deepspeed zero2+cpu_offload'",2021-04-25T00:38:17Z,2021-06-19T06:39:43Z,,OSError,"OSError: [Errno 39] Directory not empty: '/mnt/experiments/roberta-large-mlm/checkpoint-23000'"
11420,b'[Question] Implementing character based tokenizer',2021-04-24T21:53:18Z,2021-06-03T15:11:43Z,,,
11419,b'Parameter in `DebertaV2Tokenizer.__init__()` without documentation: `split_by_punct`',2021-04-24T21:33:12Z,2021-06-28T15:07:19Z,,,
11418,b'[Deepspeed] ZeRO-Infinity integration plus config revamp',2021-04-24T21:07:34Z,2021-04-26T17:40:33Z,DeepSpeed,,
11417,b'Enable option for subword regularization in more tokenizers.',2021-04-24T19:43:14Z,2021-05-13T06:44:55Z,,,
11416,b'Transformers Pegasus - how do I fine tune another language?',2021-04-24T18:18:06Z,2021-06-03T15:11:43Z,,,
11415,b'Roberta Tokenizer cannot handle inputs with `<mask>` token',2021-04-24T16:21:14Z,2021-06-03T15:11:44Z,,,
11414,b'checkpointing is not still covering all cases ',2021-04-24T15:04:18Z,2021-04-26T08:06:56Z,,,
11413,b'Allow adding custom logits processors in the `generate` method',2021-04-24T12:07:11Z,2021-06-17T15:07:28Z,,,
11412,b'Small bug while converting wav2vec2 model trained using fairseq to huggingface',2021-04-24T07:11:56Z,2021-06-08T15:06:43Z,,,
11411,b'What do these model parameters mean?',2021-04-24T05:58:45Z,2021-06-01T15:20:19Z,,,
11410,b'wrong parentclass in documentation',2021-04-24T00:04:35Z,2021-04-24T00:19:15Z,,,
11409,b'How to use GPU when running run_summarization.py',2021-04-23T23:53:35Z,2021-06-01T15:20:20Z,,,
11408,b'[CI] solving the pytest crashing and hanging CI job',2021-04-23T22:32:39Z,,"WIP, Testing",,
11407,b'Add basic support for FP16 in SageMaker model parallelism',2021-04-23T21:44:45Z,2021-04-26T12:55:14Z,,,
11406,b'Pass along seed to DistributedSampler',2021-04-23T21:35:35Z,2021-04-26T14:26:53Z,,,
11405,b'Default to accuracy metric in run_glue_no_trainer',2021-04-23T17:57:26Z,2021-04-23T18:50:00Z,,,
11404,b'Documentation 404 error',2021-04-23T17:48:06Z,2021-06-01T15:20:21Z,,,
11403,b'metric is uninitialized when csv data is supplied to example/pytorch/text-classification/run_glue_no_trainer.py',2021-04-23T17:09:35Z,2021-04-23T18:50:00Z,,UnboundLocalError,"UnboundLocalError: local variable 'metric' referenced before assignment"
11402,b'Positional embeddings are not applied when input embeddings are passed in for Pytorch DistilBert model',2021-04-23T16:23:17Z,2021-06-03T15:11:45Z,,,
11401,"b'Download offile HuggingFace Models in other format than "".bin"" Format'",2021-04-23T15:31:07Z,2021-06-01T15:20:21Z,,,
11400,b'[Wav2Vec2] Correct conversion script',2021-04-23T13:20:58Z,2021-04-23T13:36:27Z,,,
11399,b'unable to import transformers in Python <3.8',2021-04-23T12:55:36Z,2021-05-05T07:36:18Z,,`ModuleNotFoundError,"`ModuleNotFoundError: No module named 'importlib_metadata'`"
11398,"b""RuntimeError: [enforce fail at CPUAllocator.cpp:64] . DefaultCPUAllocator: can't allocate memory: you tried to allocate 237414383616 bytes. Error code 12 (Cannot allocate memory)""",2021-04-23T12:30:04Z,2021-04-26T12:58:52Z,,,
11397,b'PreTrainedTokenizerFast.save_pretrained() ERROR',2021-04-23T11:31:47Z,2021-04-24T15:38:44Z,,TypeError,"TypeError: PyModel.save() got an unexpected keyword argument: name"
11396,b'Fix  small typo in text',2021-04-23T11:26:27Z,2021-04-23T11:37:20Z,,,
11395,b'[Blenderbot] Integration Test should be slow',2021-04-23T10:00:08Z,2021-04-23T11:49:10Z,,,
11394,b'[Flax] Correct Flax <=> PyTorch conversion',2021-04-23T09:24:37Z,2021-04-23T09:59:34Z,Flax,,
11393,b'[Flax] Typo',2021-04-23T09:21:43Z,2021-04-23T09:34:59Z,Flax,,
11392,b'MayBe There is a bug with class DebertaV2PredictionHeadTransform',2021-04-23T07:27:00Z,2021-05-31T15:11:51Z,,"`RuntimeError, RuntimeError","`RuntimeError: mat1 dim 1 must match mat2 dim` when Use official Debertav2 mlm . RuntimeError: Caught RuntimeError in replica 0 on device 0."
11391,b'Fix typos in README for text-classification',2021-04-23T07:07:54Z,2021-04-23T11:48:42Z,,,
11390,b'S3 checkpoints not working with distributed training on sagemaker ',2021-04-23T06:13:59Z,2021-04-26T06:24:08Z,,InternalServerError,"InternalServerError: We encountered an internal error. Please try again."
11389,b'Distributed DataSampler has fixed data order despite random seeds.',2021-04-23T05:14:38Z,2021-04-26T14:26:53Z,,,
11388,b'CUDA OOM in the middle of training when the training data is large',2021-04-23T03:55:54Z,2021-05-31T15:11:52Z,,RuntimeError,"RuntimeError: Caught RuntimeError in replica 0 on device 0."
11387,b'Implement Fast Tokenization for Deberta',2021-04-23T03:40:41Z,2021-04-30T12:08:16Z,,,
11386,b'[Seq2seq] Add Support for TensorFlow',2021-04-22T21:25:00Z,2021-05-31T15:11:53Z,,,
11385,"b'[docs]Incorrect way of input encoding for ""multiple choice"" models in documentation?'",2021-04-22T20:48:41Z,2021-07-12T15:07:32Z,,,
11384,b'some issue in loading local txt file as Dataset for run_mlm.py',2021-04-22T19:34:54Z,2021-06-03T15:11:46Z,,FileNotFoundError,"FileNotFoundError: Couldn't find file at https://raw.githubusercontent.com/huggingface/datasets/master/datasets/dataset/dataset.py"
11383,b'Fixed trainer total_flos relaoding in distributed mode',2021-04-22T15:50:06Z,2021-04-23T11:53:33Z,,,
11382,b'Fix Trainer with remove_unused_columns=False',2021-04-22T14:43:08Z,2021-04-22T15:16:24Z,,,
11381,b'Trainer._remove_unused_columns() returns None',2021-04-22T14:35:49Z,2021-04-22T15:16:24Z,,TypeError,"TypeError: object of type 'NoneType' has no len()"
11380,b'[Examples] Fixes inconsistency around eval vs val and predict vs test',2021-04-22T12:24:06Z,2021-04-26T16:24:31Z,,,
11379,b'Correctly cast num_train_epochs to int',2021-04-22T12:21:56Z,2021-04-22T12:50:00Z,,,
11378,b'Remove max length beam scorer',2021-04-22T11:51:54Z,2021-04-26T22:28:40Z,,,
11377,b'new call for model addition ',2021-04-22T11:44:54Z,,WIP,,
11376,b'Wav2vec2: comparison to original implementation',2021-04-22T11:19:28Z,2021-05-24T15:26:21Z,,,
11375,b'Output probability from `model.generate` for TF models',2021-04-22T10:58:55Z,2021-05-30T15:08:14Z,,,
11374,b'[Flax] Correct typo',2021-04-22T10:38:36Z,2021-04-22T11:11:44Z,Flax,,
11373,b'Add space',2021-04-22T09:08:29Z,2021-04-22T12:18:59Z,,,
11372,b'[run_translation.py] fix typo',2021-04-22T08:10:10Z,2021-04-22T12:17:11Z,,,
11371,b'[examples] UserWarning: `max_length` is deprecated',2021-04-22T04:28:45Z,2021-05-03T16:48:13Z,,,
11370,b'ERRORS: run_mlm_performer.py',2021-04-22T03:57:27Z,2021-05-30T15:08:15Z,,,
11369,b'Fix typo',2021-04-22T02:44:50Z,2021-04-22T14:10:17Z,,,
11368,"b""Megatron fused CUDA kernels to improve Hugging Face model classes' scalability""",2021-04-22T01:03:49Z,,"Performance, WIP",,
11367,b'Replace double occurrences as the last step',2021-04-21T22:03:27Z,2021-05-24T07:38:59Z,,,
11366,b'RuntimeError: CUDA error: device-side assert triggered',2021-04-21T20:29:20Z,2021-05-30T15:08:16Z,,,
11365,b'Index out of range in self with fine-tuned DPR Context Encoder',2021-04-21T19:48:00Z,2021-06-25T15:02:19Z,,IndexError,"IndexError: index out of range in self"
11364,b'[Flax] Big FlaxBert Refactor',2021-04-21T17:43:09Z,2021-04-23T07:53:09Z,Flax,,
11363,"b'torch_xla/csrc/tensor_methods.cpp:880 : Check failed: xla::ShapeUtil::Compatible(shapes.back(), tensor_shape) '",2021-04-21T17:15:10Z,2021-05-13T10:51:30Z,,RuntimeError,"RuntimeError: Error while lowering: s64[1,2368]{1,0} aten::copysign, pad=(0, 19, 0, 0), value=0"
11362,b'Training a TimeSFormer for video classification ',2021-04-21T16:29:25Z,2021-05-30T15:08:17Z,,,
11361,b'Move old TF text classification script to legacy',2021-04-21T16:13:28Z,2021-04-21T16:36:18Z,,,
11360,b'Merge new TF example script',2021-04-21T15:43:09Z,2021-04-21T16:04:55Z,,,
11359,b'[testing doc] bring doc up to date',2021-04-21T15:38:09Z,2021-04-21T15:51:01Z,,,
11358,b'Different results between `AlbertTokenizer` and `AlbertTokenizerFast` modules with a new `spiece.model` file',2021-04-21T14:23:49Z,2021-05-20T13:55:55Z,,,
11357,b'possible mistake in documentation',2021-04-21T13:20:53Z,2021-04-27T14:06:37Z,,,
11356,b'Whyhttps://github.com/huggingface/transformers/tree/master/examples/pplm',2021-04-21T11:09:14Z,2021-05-29T15:08:24Z,,,
11355,b'Fix token_type_ids error for big_bird model.',2021-04-21T11:07:59Z,2021-04-21T17:37:57Z,,,
11354,b'Question-answering pipeline failing with Nonetype exception when selecting spans with tokens outside of the context',2021-04-21T09:42:19Z,2021-05-10T17:28:10Z,,TypeError,"TypeError: 'NoneType' object cannot be interpreted as an integer"
11353,b' T5 Gradient Checkpointing',2021-04-21T04:49:25Z,2021-04-30T08:43:56Z,,,
11352,b'[deepspeed] fix resume from checkpoint',2021-04-21T03:55:55Z,2021-04-21T14:48:16Z,,,
11351,b'fine tuning encoder decoder for custom language translation',2021-04-21T01:14:45Z,2021-05-29T15:08:25Z,,,
11350,b'Examples reorg',2021-04-21T01:13:55Z,2021-04-21T15:11:21Z,,,
11349,b'[Wav2Vec2] Fix special tokens for Wav2Vec2 tokenizer',2021-04-20T21:29:55Z,2021-04-22T10:23:09Z,,,
11348,"b""'Tensor' object has no attribute 'size'""",2021-04-20T21:05:35Z,2021-05-29T15:08:26Z,,,
11347,b'Extract metric_key_prefix during NotebookProgressCallback.on_evaluate',2021-04-20T20:34:08Z,2021-04-21T15:12:09Z,,,
11346,b'[contributing doc] explain/link to good first issue',2021-04-20T19:09:35Z,2021-04-21T17:10:11Z,,,
11345,b'absolute embeddings in Deberta',2021-04-20T18:58:40Z,2021-06-28T15:07:22Z,,,
11344,b'[run_summarization.py] wrong dataset leads to CUDA error:s',2021-04-20T16:53:34Z,2021-09-15T17:03:57Z,,RuntimeError,"RuntimeError: CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`"
11343,b'Update to use datasets remove_cloumns method',2021-04-20T15:56:43Z,2021-04-20T18:12:02Z,,,
11342,b'mlflow parameter overflow when training a language adapter ',2021-04-20T15:29:22Z,2021-04-25T13:59:20Z,,mlflow.exceptions.MlflowException,"mlflow.exceptions.MlflowException: Param value '{'adapters': {'de': (text_lang, 'bb1c8efb82510bed')}, 'config_map': {text_lang: AdapterConfig(original_ln_before=True, original_ln_after=True, residual_before_ln=True, adapter_residual_before_ln=False, ln_before=False, ln_after=False, mh_adapter=Fals' had length 786, which exceeded length limit of 250"
11341,b'Warpping model.generate before exporting into tensorflow savedmodel format',2021-04-20T15:20:49Z,2021-05-29T15:08:27Z,,,
11340,b'Remove boiler plate code',2021-04-20T15:17:04Z,2021-04-21T16:34:39Z,Flax,,
11339,b'Perform max_input_tokens truncation with Summarization Pipeline',2021-04-20T14:04:14Z,2021-05-05T08:47:48Z,,,
11338,b' tf generate compatible with tf.function',2021-04-20T13:52:18Z,2021-05-29T15:08:28Z,,,
11337,b'Adding `AutomaticSpeechRecognitionPipeline`.',2021-04-20T13:04:24Z,2021-04-30T09:54:08Z,,,
11336,b'M2M-100 SentencePiece model produces tokens that are missing on the fixed dictionary',2021-04-20T12:05:15Z,2021-05-29T15:08:29Z,,,
11335,b'[GPTNeo] create local attention mask ones',2021-04-20T11:49:16Z,2021-04-20T13:07:44Z,,,
11334,b'Bug in GPT2ForSequenceClassification',2021-04-20T10:11:33Z,2021-05-29T15:08:30Z,,,
11333,"b""Potential bug: Tokens with punctuation are re-tokenized although I've set `is_split_into_words=True`""",2021-04-20T09:52:54Z,2021-04-26T15:29:36Z,,,
11332,b'batch_encode_plus set a sort parameter',2021-04-20T08:23:07Z,2021-05-29T15:08:31Z,,,
11331,b'[Generate] Remove outdated code',2021-04-20T08:12:28Z,2021-04-20T12:16:02Z,,,
11330,b'Correcting comments in T5Stack to reflect correct tuple order ',2021-04-20T06:49:08Z,2021-05-26T13:07:23Z,,,
11329,b'Honor contributors to models',2021-04-20T02:53:38Z,2021-04-21T13:47:27Z,,,
11328,b'Trainer push to hub',2021-04-20T01:16:48Z,2021-04-23T13:17:38Z,,,
11327,b'run_ner.py example MobileBERT FP16 returns nan loss',2021-04-19T22:01:17Z,2021-04-20T00:42:21Z,,,
11326,b'Parameter missing from state_dict of optimizer when loading from checkpoint',2021-04-19T21:36:35Z,2021-05-29T15:08:32Z,,,
11325,b'Enable added tokens',2021-04-19T20:59:19Z,2021-05-04T12:13:58Z,,,
11324,b'[Trainer] Add a progress bar for batches skipped',2021-04-19T20:38:03Z,2021-04-19T23:04:52Z,,,
11323,b'Bug in trainer: substantially different results from restarting from a checkpoint and without',2021-04-19T19:18:04Z,2021-05-10T14:58:31Z,,,
11322,b'[Trainer] fix the placement on device with fp16_full_eval',2021-04-19T17:45:30Z,2021-04-19T18:55:34Z,,,
11321,"b""EncoderDecoderModel's decoder gets unexpected use_cache argument""",2021-04-19T17:01:03Z,2021-04-22T09:42:05Z,,,
11320,b'Irregular VRAM usage with gpt-neo inference with sequences longer than 250 tokens',2021-04-19T16:22:05Z,2021-09-10T17:22:20Z,WIP,RuntimeError,"RuntimeError: CUDA out of memory. Tried to allocate 4.59 GiB (GPU 0; 15.90 GiB total capacity; 9.75 GiB already allocated; 4.60 GiB free; 10.42 GiB reserved in total by PyTorch)"
11319,"b""Error in loading model tokenizer ('Helsinki-NLP/opus-mt-en-fr' actually loads 'Helsinki-NLP/opus-mt-en-de')""",2021-04-19T15:51:08Z,2021-05-03T12:19:15Z,,,
11318,b'Load checkpoint without re-creating the model',2021-04-19T14:42:54Z,2021-04-20T00:31:30Z,,,
11317,b'large memory usage when resuming training from a checkpoint',2021-04-19T12:53:16Z,2021-05-27T15:09:52Z,,RuntimeError,"RuntimeError: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 0; 23.70 GiB total capacity; 21.38 GiB already allocated; 41.69 MiB free; 22.18 GiB reserved in total by PyTorch)"
11316,b'Added BERT pretraining example running on Graphcore IPUs to research projects',2021-04-19T12:26:32Z,2021-06-19T15:02:06Z,,,
11315,b'T5Model crashes when trained with multiple GPUs',2021-04-19T12:20:10Z,2021-05-27T15:09:53Z,,RuntimeError,"RuntimeError: Caught RuntimeError in replica 1 on device 1."
11314,b'Removed `max_length` from being mandatory within `generate`.',2021-04-19T09:57:19Z,2021-04-21T09:56:46Z,,,
11313,b'[WIP] Add PiT',2021-04-19T09:56:10Z,,WIP,,
11312,b'The output of IBERT is float32. Am I doing wrong?',2021-04-19T09:40:39Z,2021-06-13T15:01:52Z,,,
11311,b'Update hf_argparser.py',2021-04-19T08:32:28Z,2021-05-27T15:09:55Z,,,
11310,b'[Benchmark] GPT2LMHeadModel (gpt2-medium) forward pass inference became 9% slower compared to 2.8.0 release',2021-04-19T07:54:34Z,2021-05-27T15:09:56Z,,,
11309,b'Vit deit fixes',2021-04-19T07:41:45Z,2021-05-12T15:46:03Z,,,
11308,b'RAG with RAY implementation: Ray workers memory slowly increase over time.',2021-04-19T06:53:13Z,2021-04-20T06:45:52Z,,,
11307,b'Getting time offsets of beginning and end of each word in Wav2Vec2',2021-04-19T03:57:57Z,,"Good First Issue, Good Second Issue",,
11306,b'Wav2Vec2 Pretraining',2021-04-18T21:44:30Z,2021-06-09T17:40:57Z,,,
11305,"b'invalid multinomial distribution (with replacement=False, not enough non-negative category to sample)'",2021-04-18T10:17:14Z,2021-05-27T15:09:58Z,,,
11304,b'env about run longformer model downloaded from https://github.com/allenai/longformer ',2021-04-18T08:44:10Z,2021-04-18T11:49:25Z,,,
11303,b'small bug in RAG model',2021-04-18T06:21:54Z,2021-06-02T08:17:14Z,,,
11302,b'Problems with webbased editing of model cards ',2021-04-18T05:54:39Z,2021-05-13T12:50:19Z,,,
11301,b'Longformer model with weight(model.encoder.embed_positions.weight) error',2021-04-18T01:56:52Z,2021-05-27T15:09:59Z,,RuntimeError,"RuntimeError: Error(s) in loading state_dict for BartModel:"
11300,b'EncoderDecoderConfigs should not create new objects',2021-04-17T22:13:20Z,2021-04-25T09:45:47Z,,,
11299,b'Pr2keep encoder decoder synced',2021-04-17T21:40:01Z,2021-04-17T21:40:43Z,,,
11297,b'Fixing bug in generation',2021-04-17T17:39:44Z,2021-04-23T16:24:26Z,,,
11296,b'Cannot save GPT2 model with signature',2021-04-17T17:31:29Z,2021-04-23T18:28:48Z,,ValueError,"ValueError: Got a non-Tensor value (<tf.Tensor 'StatefulPartitionedCall:1' shape=(2, None, 12, 384, 64) dtype=float32>, <tf.Tensor 'StatefulPartitionedCall:2' shape=(2, None, 12, 384, 64) dtype=float32>, <tf.Tensor 'StatefulPartitionedCall:3' shape=(2, None, 12, 384, 64) dtype=float32>, <tf.Tensor 'StatefulPartitionedCall:4' shape=(2, None, 12, 384, 64) dtype=float32>, <tf.Tensor 'StatefulPartitionedCall:5' shape=(2, None, 12, 384, 64) dtype=float32>, <tf.Tensor 'StatefulPartitionedCall:6' shape=(2, None, 12, 384, 64) dtype=float32>, <tf.Tensor 'StatefulPartitionedCall:7' shape=(2, None, 12, 384, 64) dtype=float32>, <tf.Tensor 'StatefulPartitionedCall:8' shape=(2, None, 12, 384, 64) dtype=float32>, <tf.Tensor 'StatefulPartitionedCall:9' shape=(2, None, 12, 384, 64) dtype=float32>, <tf.Tensor 'StatefulPartitionedCall:10' shape=(2, None, 12, 384, 64) dtype=float32>, <tf.Tensor 'StatefulPartitionedCall:11' shape=(2, None, 12, 384, 64) dtype=float32>, <tf.Tensor 'StatefulPartitionedCall:12' shape=(2, None, 12, 384, 64) dtype=float32>) for key 'past_key_values' in the output of the function __inference_call_90110 used to generate the SavedModel signature 'serving_default'. Outputs for functions used as signatures must be a single Tensor, a sequence of Tensors, or a dictionary from string to Tensor."
11295,"b'Improve ""infer_framework_from_model"" func readability'",2021-04-17T10:55:25Z,2021-05-26T15:08:45Z,,,
11294,b'serious bug with trainer.py when restarting the training from a checkpoint',2021-04-17T10:33:00Z,2021-04-20T00:31:30Z,,,
11293,b'OSError: Unable to load weights from pytorch checkpoint file',2021-04-17T08:23:58Z,2021-05-26T15:08:46Z,,"RuntimeError, OSError","RuntimeError: version_ <= kMaxSupportedFileFormatVersion INTERNAL ASSERT FAILED at /tmp/pip-req-build-66hwoyb6/caffe2/serialize/inline_container.cc:132, please report a bug to PyTorch. Attempted to read a PyTorch file with version 3, but the maximum supported version for reading is 2. Your PyTorch installation may be too old. (init at /tmp/pip-req-build-66hwoyb6/caffe2/serialize/inline_container.cc:132)OSError: Unable to load weights from pytorch checkpoint file for 'google/mt5-small' at '/home/notooth/.cache/huggingface/transformers/8e7b2a80ddcb5611b27d8c89e1e8e33a947e105415051402a22b9c8d7d1caeb0.e22331f3a065b885b30ae3dd1ff11ccaf7fbc444485f6eb07ef5e0138bca8b70'If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True."
11292,b'move device statements outside if statements',2021-04-17T06:43:49Z,2021-04-19T12:25:40Z,,,
11290,b'Python crashes when loading Bert model from pretrained',2021-04-16T22:00:59Z,2021-05-26T15:08:47Z,,,
11289,b'google/pegasus-cnn_dailymail generates blank file',2021-04-16T21:58:41Z,2021-04-30T21:56:55Z,,(Note,"(Note: here the batch size is changed due to memory limitation. Although experiments are performed on the same devices, CNN/DM requires more spaces considering the unique feature of dataset itself.)"
11288,b'Question about T5-11b model weights',2021-04-16T21:06:23Z,2021-04-17T14:34:34Z,,,
11287,b'Zero-shot pipeline feature extraction',2021-04-16T18:38:38Z,2021-04-19T03:50:50Z,,,
11286,b'Trainer support for IterableDataset for evaluation and predict',2021-04-16T16:40:55Z,2021-04-16T20:01:58Z,,,
11285,b'`resize_token_embeddings` not taken into account in `save_pretrained` for `EncoderDecoderModel`',2021-04-16T15:24:52Z,2021-04-25T09:45:47Z,,,
11284,b'Loading from checkpoint seems to hang indefinitely for Roberta',2021-04-16T15:23:34Z,2021-05-26T15:08:48Z,,,
11283,b'Beam search decoding and language model integration for Wav2Vec2ForCTC models',2021-04-16T14:43:10Z,2021-07-02T15:05:24Z,,,
11282,b'tf.function and half precision fails with Roberta models',2021-04-16T14:12:31Z,2021-05-24T15:01:48Z,,TypeError,"TypeError: in user code:"
11281,b'Adding and consequently removing tokens leads to incorrect number of input embeddings',2021-04-16T13:47:20Z,2021-05-27T15:10:01Z,,,
11280,b'failed to import BertModel',2021-04-16T13:10:46Z,2021-04-16T13:42:10Z,Migration,,
11279,b'fp16 compatibility',2021-04-16T12:43:08Z,2021-05-24T15:01:49Z,,,
11278,b'[Benchmark]',2021-04-16T12:05:20Z,2021-04-16T12:06:54Z,,,
11277,b'We should make an eco freindly phone and it should be affordable for everyone',2021-04-16T11:39:15Z,2021-04-19T12:20:09Z,,,
11276,b'Running gpt-neo 2.7B with less than 13GB of system memory like Colab',2021-04-16T10:41:54Z,,Feature request,,
11275,b'modify double considering special tokens in `language_modeling.py`',2021-04-16T05:57:16Z,2021-04-19T15:24:43Z,,,
11274,b'[debug utils] activation/weights underflow/overflow detector',2021-04-16T02:29:31Z,2021-04-30T18:15:47Z,,,
11273,b'update dependency_versions_table',2021-04-16T02:03:23Z,2021-04-16T02:10:29Z,,,
11272,b'squad_convert_example_to_features is broken',2021-04-16T00:05:27Z,2021-04-16T00:32:46Z,,,
11271,"b'gpt-neo 2.7 crashes, 1.3 runs fine'",2021-04-15T23:02:35Z,2021-05-24T15:01:51Z,,,
11270,b'Workflow fixes',2021-04-15T22:04:34Z,2021-04-16T03:21:18Z,,,
11268,b'DataCollatorForSOP marked as deprecated but DataCollatorForLanguageModeling does not offer the same functionality',2021-04-15T13:53:35Z,2021-04-16T09:52:00Z,,,
11267,b'inf/nan in generate (beam_sample) with small temperature values',2021-04-15T13:39:19Z,2022-02-13T15:02:36Z,,RuntimeError,"RuntimeError: probability tensor contains either `inf`, `nan` or element < 0"
11266,b'chunk of words for input token',2021-04-15T11:27:31Z,2021-05-23T15:02:01Z,,,
11265,"b'TensorFlow ""predict"" returns empty output with MirroredStrategy'",2021-04-15T10:50:20Z,2021-09-14T17:54:25Z,,,
11264,b'Multi-Workers distributed training',2021-04-15T10:23:54Z,2021-04-24T15:05:53Z,,,
11263,b'TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect',2021-04-15T07:44:42Z,2021-05-23T15:02:02Z,,,
11262,b'Failed to import transformers',2021-04-15T06:16:06Z,2021-06-21T15:06:23Z,,ImportError,"ImportError: /lib/x86_64-linux-gnu/libm.so.6: version `GLIBC_2.29' not found (required by /home/notooth/anaconda3/lib/python3.8/site-packages/tokenizers/tokenizers.cpython-38-x86_64-linux-gnu.so)"
11261,"b'--sharded_ddp ""zero_dp_3 offload"" fails with AssertionError '",2021-04-15T05:43:16Z,2021-05-23T15:02:04Z,,,
11260,b'About pre-trained model : facebook/wav2vec2-large-xlsr-53 & facebook/wav2vec2-base',2021-04-15T02:39:05Z,2021-05-23T15:02:05Z,,,
11259,b'[Benchmark]',2021-04-15T01:49:00Z,2021-05-23T15:02:05Z,,,
11258,b'Support for set_epoch in IterableDataset',2021-04-14T21:18:33Z,2021-04-15T11:36:33Z,,,
11257,b'[Benchmark]',2021-04-14T21:16:19Z,2021-04-15T01:50:18Z,,,
11256,"b""Getting KeyError: 'loss' when fine-tuning model on a pre-trained MLM""",2021-04-14T20:05:24Z,2021-05-23T15:02:06Z,,KeyError,"KeyError: 'loss'"
11255,"b'Big Bird generate() ""local variable \'next_tokens\' referenced before assignment""'",2021-04-14T18:10:21Z,2021-05-23T15:02:07Z,,UnboundLocalError,"UnboundLocalError: local variable 'next_tokens' referenced before assignment"
11254,b'Trainer iterable dataset',2021-04-14T18:09:20Z,2021-04-14T21:02:27Z,,,
11253,b'New TF examples',2021-04-14T18:05:42Z,2021-04-21T15:46:33Z,,,
11252,b'Fix for the issue of device-id getting hardcoded for token_type_ids during Tracing [WIP]',2021-04-14T17:45:20Z,2021-06-22T09:21:31Z,,,
11251,b'Add batching in TokenClassificationPipeline',2021-04-14T16:53:46Z,2021-05-13T18:44:20Z,,,
11250,b'[Benchmark]',2021-04-14T16:10:20Z,2021-04-14T19:28:38Z,,,
11249,"b""TypeError: can't pickle _thread.RLock objects hyperparameter_search raytune""",2021-04-14T15:52:08Z,2021-06-15T18:11:29Z,,TypeError,"TypeError: can't pickle _thread.RLock objects"
11248,b'Fix #10128',2021-04-14T15:12:19Z,2021-04-14T15:47:54Z,,,
11247,b'Adding pipeline task aliases.',2021-04-14T14:34:17Z,2021-04-15T07:51:24Z,,,
11246,b'Enable Wav2Vec2 Pretraining',2021-04-14T13:38:37Z,2021-06-09T17:40:56Z,"Good First Issue, Good Second Issue",,
11245,b'RuntimeError: leaf variable has been moved into the graph interior',2021-04-14T12:40:50Z,2021-05-23T15:02:09Z,,RuntimeError,"RuntimeError: leaf variable has been moved into the graph interior"
11244,b'Batching in NER pipeline',2021-04-14T11:24:11Z,2021-05-23T15:02:10Z,,,
11243,b'Cant load tokenizer locally after downloading it',2021-04-14T11:02:04Z,2021-04-27T08:45:58Z,,OSError,"OSError: Can't load tokenizer for '/mnt/kingston/github/MIARFID/ALC/cardiffnlp/twitter-roberta-base-sentiment'. Make sure that:"
11242,b'position_ids generated from Roberta',2021-04-14T10:22:49Z,2021-04-15T00:05:53Z,,,
11241,b'add new token to Bert',2021-04-14T04:48:47Z,2021-05-30T15:08:21Z,,,
11240,b'Close open files to suppress ResourceWarning',2021-04-14T03:20:46Z,2021-04-14T14:31:04Z,,,
11239,"b""Getting `NameError: name 'BertOnlyMLMHead' is not defined` error when upgrading to latest transformers """,2021-04-14T02:04:55Z,2021-05-23T15:02:11Z,Migration,NameError,"NameError: name 'BertOnlyMLMHead' is not defined"
11238,b'Fix dimention misspellings.',2021-04-13T23:56:25Z,2021-04-14T14:39:38Z,,,
11237,b' [deepspeed] test on one node 2 gpus max',2021-04-13T22:21:08Z,2021-04-14T18:06:59Z,,,
11236,b'[troubleshooting] add 2 points of reference to the offline mode',2021-04-13T20:23:33Z,2021-04-14T15:39:24Z,,,
11235,b'[Deepspeed] zero3 tests band aid',2021-04-13T19:54:37Z,2021-04-13T21:58:10Z,,,
11234,b'Tokenizer fast save',2021-04-13T19:08:56Z,2021-04-15T13:32:33Z,,,
11233,b'Indent code block in the documentation',2021-04-13T17:52:56Z,2021-04-13T19:36:37Z,,,
11232,b'BigBird Causal Attention',2021-04-13T17:32:56Z,2021-05-27T15:10:04Z,,,
11231,"b'""Connection error, and we cannot find the requested files in the cached path."" ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.'",2021-04-13T17:29:38Z,2021-05-23T15:02:12Z,,ValueError,"ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on."
11230,b'run_qa.py fails evaluating on Squad2',2021-04-13T17:18:50Z,2021-05-23T15:02:13Z,,ValueError,"ValueError: max() arg is an empty sequence"
11229,b'Avoid using no_sync on SageMaker DP',2021-04-13T16:55:45Z,2021-04-13T19:34:01Z,,,
11228,"b'Make ""embeddings"" plural in warning message within tokenization_utils_base'",2021-04-13T16:26:24Z,2021-04-14T14:13:25Z,,,
11227,b'Make sure code blocks are indented with four spaces',2021-04-13T15:01:08Z,2021-04-13T17:50:11Z,,,
11226,b'Add prefix to examples in model_doc rst',2021-04-13T14:49:07Z,2021-04-14T14:58:55Z,,,
11225,b'Refactor GPT2',2021-04-13T14:41:16Z,2021-04-13T15:45:24Z,,,
11224,b'Doc check: a bit of clean up',2021-04-13T14:02:51Z,2021-04-13T16:14:25Z,,,
11223,b'Add LUKE',2021-04-13T13:25:16Z,2021-05-03T13:07:29Z,,,
11222,b'Weird issue with OOM on exported save_pretrained models',2021-04-13T10:56:04Z,2021-05-22T19:57:38Z,,json.decoder.JSONDecodeError,"json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)"
11221,b'fix docs for decoder_input_ids',2021-04-13T09:50:03Z,2021-04-13T12:58:08Z,,,
11220,b'added cache_dir=model_args.cache_dir to all example with cache_dir arg',2021-04-13T09:39:07Z,2021-04-13T16:35:19Z,,,
11219,b'Add documentation for BertJapanese',2021-04-13T09:08:31Z,2021-04-13T13:49:16Z,,,
11218,b'[WIP] FSMT bart-like refactor',2021-04-13T08:45:28Z,,WIP,,
11217,b'Question about validation_set',2021-04-13T07:24:36Z,2021-05-21T15:02:11Z,,,
11216,b'Load BART-base error',2021-04-12T23:43:29Z,2021-05-21T15:02:12Z,,OSError,"OSError: Unable to load weights from pytorch checkpoint file for '/home/ahmad2/.cache/huggingface/transformers/bart-base' at '/home/ahmad2/.cache/huggingface/transformers/bart-base/pytorch_model.bin'If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True.`"
11215,"b""It don't find simple logic sequences """,2021-04-12T23:17:57Z,2021-05-21T15:02:13Z,,,
11214,b'Import torch.utils.checkpoint in ProphetNet',2021-04-12T22:47:10Z,2021-04-12T22:56:18Z,,,
11213,b'Fix GPT-2 warnings',2021-04-12T22:01:54Z,2021-04-13T12:53:05Z,,,
11212,b'Add Matt as the TensorFlow reference',2021-04-12T20:59:42Z,2021-04-13T12:52:30Z,,,
11211,b'Beam search on BART seq2seq',2021-04-12T20:08:53Z,2021-05-21T15:02:14Z,,,
11210,b'Documentation enhancement - model_type',2021-04-12T19:33:54Z,2021-05-21T15:02:15Z,,,
11209,b'[RFC] introduce `config.trained_precision`',2021-04-12T16:54:09Z,,WIP,,
11208,b'Issue: Trainer error on `evaluate()` in multithreaded/distributed context (shape mismatch)',2021-04-12T16:34:38Z,2021-04-12T20:02:25Z,,ValueError,"ValueError: could not broadcast input array from shape (104,) into shape (96,)"
11207,b'Replace error by warning when loading an architecture in another',2021-04-12T15:01:25Z,2021-04-13T14:33:52Z,,,
11206,b'Sagemaker test docs update for framework upgrade',2021-04-12T14:59:09Z,2021-04-12T23:08:33Z,,,
11205,b'Rework examples/ to overwrite cache_dir for datasets too. ',2021-04-12T13:34:24Z,2021-04-13T16:35:19Z,,,
11204,"b""ModuleNotFoundError: No module named 'transformers.modeling_camembert'""",2021-04-12T12:20:03Z,2021-05-20T15:06:35Z,,,
11203,"b'How to extract the specific output using the method ""encoder_output[0]""'",2021-04-12T12:17:40Z,2021-04-12T20:22:10Z,,,
11202,b'Fix TFBert embedding tf variables with the same name - Fixes problems with checkpoints under tf.distribute.Strategy',2021-04-12T12:03:42Z,2021-06-21T15:06:25Z,,,
11201,b'Issue: List index out of range when using Seq2SeqTrainer',2021-04-12T11:29:47Z,2021-04-13T09:29:49Z,,IndexError,"IndexError: list index out of range"
11200,b'Issue: Adding new tokens to bert tokenizer in QA',2021-04-12T10:23:12Z,2021-04-19T18:55:34Z,,,
11199,b'Add  examples/bert-loses-patience who can help',2021-04-12T10:02:00Z,2021-05-20T15:06:35Z,,,
11198,b'trainer.evaluate() expects batch_size to match target batch_size',2021-04-12T09:50:06Z,2021-04-12T12:58:34Z,,ValueError,"ValueError: Expected input batch_size (18) to match target batch_size (6)."
11197,b'[T5] Add 3D attention mask to T5 model (2) (#9643)',2021-04-12T07:45:59Z,2021-05-13T11:02:27Z,,,
11196,b'Added translation example script ',2021-04-12T05:13:03Z,2021-04-20T11:18:47Z,,,
11195,"b""Getting no attribute 'output_attentions' error when upgrading to latest huggingface transformers """,2021-04-12T03:33:34Z,2021-05-20T15:06:36Z,Migration,torch.nn.modules.module.ModuleAttributeError,"torch.nn.modules.module.ModuleAttributeError: 'CaptionBertSelfAttention' object has no attribute 'output_attentions'"
11194,b'Transfer learning on bert',2021-04-12T02:12:36Z,2021-04-12T22:08:05Z,,,
11193,"b""ProphetNet with AttributeError: module 'torch.utils' has no attribute 'checkpoint'""",2021-04-12T02:05:28Z,2021-04-12T22:56:17Z,,AttributeError,"AttributeError: module 'torch.utils' has no attribute 'checkpoint'"
11192,"b'Loading a model saved with `TFGPT2LMHeadModel.save_pretrained` with `GPT2LMHeadModel.from_pretrained(..., from_tf=True)`'",2021-04-12T01:38:15Z,2021-04-13T12:53:04Z,,,
11191,b'Decoding throws Segmentation Fault ',2021-04-11T20:42:49Z,2021-04-26T17:01:35Z,,,
11190,"b""wav2vec 2.0 doesn't appear to do vector quantization""",2021-04-11T20:13:16Z,2021-05-20T15:06:37Z,,,
11189,b'correct the input_ids value and batch_sentences value.',2021-04-11T19:08:34Z,2021-05-20T15:06:38Z,,,
11188,b'Fix typo',2021-04-11T13:21:09Z,2021-04-12T21:35:32Z,,,
11187,b'ELECTRA-large-discriminator results are not stable',2021-04-11T12:27:28Z,2021-05-18T04:03:05Z,,,
11186,b'strange memory usage for t5 models',2021-04-11T11:34:34Z,2021-04-17T13:01:08Z,,RuntimeError,"RuntimeError: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 0; 23.70 GiB total capacity; 21.14 GiB already allocated; 1.69 MiB free; 22.36 GiB reserved in total by PyTorch)"
11185,b'Loading pretrained mBART model always generate the same output',2021-04-11T11:06:24Z,2021-04-12T10:20:55Z,,,
11184,b'Can not  instantiate BertGenerationEncoder or  BertGenerationDecoder from bert model',2021-04-11T01:43:29Z,2021-04-13T14:33:52Z,,AssertionError,"AssertionError: You tried to initiate a model of type 'bert-generation' with a pretrained model of type 'bert'"
11183,b'Replaced `which` with `who`',2021-04-10T23:50:53Z,2021-04-12T22:08:28Z,,,
11182,b'Minor typos fixed',2021-04-10T23:47:57Z,2021-04-12T11:55:40Z,,,
11181,b'How to kill bad starts when pre-training from scratch',2021-04-10T15:00:47Z,2021-05-19T15:08:24Z,,,
11180,b'Sequential constraints?',2021-04-10T14:46:16Z,,WIP,,
11179,"b""Why couldn't I use encoder_hidden_states when position_ids is not None? GPT2Model.foward()""",2021-04-10T12:44:13Z,2021-04-19T12:36:32Z,,,
11178,b'Use MSELoss with single class label in (M)BartForSequenceClassification',2021-04-10T12:05:21Z,2021-04-13T09:54:46Z,,,
11177,"b'TypeError: expected str, bytes or os.PathLike object, not NoneType'",2021-04-10T07:00:08Z,2021-05-19T15:08:27Z,,,
11176,b'bug fix',2021-04-10T06:12:59Z,2021-05-19T15:08:28Z,,,
11175,b'MemoryError: when we run_language_model.py to train an English Adapter',2021-04-10T04:25:08Z,2021-05-19T15:08:29Z,,,
11174,b'Using BART for Mask Infilling makes all the first tokens missing  ',2021-04-10T02:36:31Z,2021-05-11T00:49:54Z,,,
11173,"b""Encoder-Decoder Models Can't Generate using Apex""",2021-04-09T23:20:45Z,2021-05-19T15:08:31Z,,KeyError,"KeyError: 0"
11172,b'Run CI on deepspeed and fairscale',2021-04-09T20:48:42Z,2021-04-13T19:47:07Z,,,
11171,b'Error in running run_tf_text_classification.py',2021-04-09T19:35:45Z,2021-05-24T15:01:53Z,,ValueError,"ValueError: Input is not valid. Should be a string, a list/tuple of strings or a list/tuple of integers."
11170,b'[examples/translation] support mBART-50 and M2M100 fine-tuning',2021-04-09T18:14:21Z,2021-04-09T18:28:43Z,,,
11169,b'Unable to resume checkpoints with TFBertModel using tf.distribute.Strategy and a custom LM head that shares the underlying TFBertEmbeddings layer',2021-04-09T17:35:30Z,2021-05-19T15:08:32Z,,`ValueError,"`ValueError: in user code:    /opt/ml/code/unsilo_ml/python/supervisors/TF2custom_keras_loop_supervisor.py:395 train_epoch  *        loss, local_global_step = distributed_train_step(x)    /opt/ml/code/unsilo_ml/python/supervisors/TF2custom_keras_loop_supervisor.py:359 distributed_train_step  *        per_replica_losses, per_replica_global_step = self.dist_strategy.run(    /opt/ml/code/unsilo_ml/python/supervisors/TF2custom_keras_loop_supervisor.py:313 train_step  *        predictions = keras_model(features, training=True)    /opt/ml/code/unsilo_ml/python/models/base_models/base_model.py:72 call  *        return self.build_forward_pass(training=training, inputs=inputs)    /opt/ml/code/unsilo_ml/python/models/multitask_model.py:103 build_forward_pass  *        inputs_with_encoder_output = self.prepare_inputs_with_encoder_output(    /opt/ml/code/unsilo_ml/python/models/multitask_model.py:147 prepare_inputs_with_encoder_output  *        encoder_outputs = self.encoder(inputs, training=training)    /opt/ml/code/unsilo_ml/python/modules/encoders/util_encoders/pipe_encoder.py:18 call  *        encoder_output = self.resolve_tensor_dict(    /opt/ml/code/unsilo_ml/python/modules/encoders/bert_encoder.py:83 call  *        hidden_states = self.bert_model(    /opt/conda/lib/python3.7/site-packages/transformers/models/bert/modeling_tf_bert.py:887 call  *        outputs = self.bert(    /opt/conda/lib/python3.7/site-packages/transformers/models/bert/modeling_tf_bert.py:645 call  *        embedding_output = self.embeddings(    /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py:1008 __call__  **        self._maybe_build(inputs)    /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py:2710 _maybe_build        self.build(input_shapes)  # pylint:disable=not-callable    /opt/conda/lib/python3.7/site-packages/transformers/models/bert/modeling_tf_bert.py:159 build        initializer=get_initializer(self.initializer_range),    /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py:639 add_weight        caching_device=caching_device)    /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/tracking/base.py:810 _add_variable_with_custom_getter        **kwargs_for_getter)    /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer_utils.py:142 make_variable        shape=variable_shape if variable_shape else None)    /opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/variables.py:260 __call__        return cls._variable_v1_call(*args, **kwargs)    /opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/variables.py:221 _variable_v1_call        shape=shape)    /opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/variables.py:67 getter        return captured_getter(captured_previous, **kwargs)    /opt/conda/lib/python3.7/site-packages/tensorflow/python/distribute/shared_variable_creator.py:69 create_new_variable        v = next_creator(**kwargs)    /opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/variables.py:67 getter        return captured_getter(captured_previous, **kwargs)    /opt/conda/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:2083 creator_with_resource_vars        created = self._create_variable(next_creator, **kwargs)    /opt/conda/lib/python3.7/site-packages/tensorflow/python/distribute/mirrored_strategy.py:489 _create_variable        distribute_utils.VARIABLE_POLICY_MAPPING, **kwargs)    /opt/conda/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_utils.py:311 create_mirrored_variable        value_list = real_mirrored_creator(**kwargs)    /opt/conda/lib/python3.7/site-packages/tensorflow/python/distribute/mirrored_strategy.py:481 _real_mirrored_creator        v = next_creator(**kwargs)    /opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/variables.py:67 getter        return captured_getter(captured_previous, **kwargs)    /opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py:714 variable_capturing_scope        lifted_initializer_graph=lifted_initializer_graph, **kwds)    /opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/variables.py:264 __call__        return super(VariableMetaclass, cls).__call__(*args, **kwargs)    /opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py:227 __init__        initial_value = initial_value()    /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/tracking/base.py:82 __call__        self._checkpoint_position, shape, shard_info=shard_info)    /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/tracking/base.py:117 __init__        self.wrapped_value.set_shape(shape)    /opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/ops.py:1217 set_shape        (self.shape, shape))    ValueError: Tensor's shape (512, 768) is not compatible with supplied shape [2, 768] | ValueError: in user code: /opt/ml/code/unsilo_ml/python/supervisors/TF2custom_keras_loop_supervisor.py:395 train_epoch * loss, local_global_step = distributed_train_step(x) /opt/ml/code/unsilo_ml/python/supervisors/TF2custom_keras_loop_supervisor.py:359 distributed_train_step * per_replica_losses, per_replica_global_step = self.dist_strategy.run( /opt/ml/code/unsilo_ml/python/supervisors/TF2custom_keras_loop_supervisor.py:313 train_step * predictions = keras_model(features, training=True) /opt/ml/code/unsilo_ml/python/models/base_models/base_model.py:72 call * return self.build_forward_pass(training=training, inputs=inputs) /opt/ml/code/unsilo_ml/python/models/multitask_model.py:103 build_forward_pass * inputs_with_encoder_output = self.prepare_inputs_with_encoder_output( /opt/ml/code/unsilo_ml/python/models/multitask_model.py:147 prepare_inputs_with_encoder_output * encoder_outputs = self.encoder(inputs, training=training) /opt/ml/code/unsilo_ml/python/modules/encoders/util_encoders/pipe_encoder.py:18 call * encoder_output = self.resolve_tensor_dict( /opt/ml/code/unsilo_ml/python/modules/encoders/bert_encoder.py:83 call * hidden_states = self.bert_model( /opt/conda/lib/python3.7/site-packages/transformers/models/bert/modeling_tf_bert.py:887 call * outputs = self.bert( /opt/conda/lib/python3.7/site-packages/transformers/models/bert/modeling_tf_bert.py:645 call * embedding_output = self.embeddings( /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py:1008 __call__ ** self._maybe_build(inputs) /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py:2710 _maybe_build self.build(input_shapes) # pylint:disable=not-callable /opt/conda/lib/python3.7/site-packages/transformers/models/bert/modeling_tf_bert.py:159 build initializer=get_initializer(self.initializer_range), /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py:639 add_weight caching_device=caching_device) /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/tracking/base.py:810 _add_variable_with_custom_getter **kwargs_for_getter) /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer_utils.py:142 make_variable shape=variable_shape if variable_shape else None) /opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/variables.py:260 __call__ return cls._variable_v1_call(*args, **kwargs) /opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/variables.py:221 _variable_v1_call shape=shape) /opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/variables.py:67 getter return captured_getter(captured_previous, **kwargs) /opt/conda/lib/python3.7/site-packages/tensorflow/python/distribute/shared_variable_creator.py:69 create_new_variable v = next_creator(**kwargs) /opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/variables.py:67 getter return captured_getter(captured_previous, **kwargs) /opt/conda/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:2083 creator_with_resource_vars created = self._create_variable(next_creator, **kwargs) /opt/conda/lib/python3.7/site-packages/tensorflow/python/distribute/mirrored_strategy.py:489 _create_variable distribute_utils.VARIABLE_POLICY_MAPPING, **kwargs) /opt/conda/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_utils.py:311 create_mirrored_variable value_list = real_mirrored_creator(**kwargs) /opt/conda/lib/python3.7/site-packages/tensorflow/python/distribute/mirrored_strategy.py:481 _real_mirrored_creator v = next_creator(**kwargs) /opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/variables.py:67 getter return captured_getter(captured_previous, **kwargs) /opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py:714 variable_capturing_scope lifted_initializer_graph=lifted_initializer_graph, **kwds) /opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/variables.py:264 __call__ return super(VariableMetaclass, cls).__call__(*args, **kwargs) /opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py:227 __init__ initial_value = initial_value() /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/tracking/base.py:82 __call__ self._checkpoint_position, shape, shard_info=shard_info) /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/tracking/base.py:117 __init__ self.wrapped_value.set_shape(shape) /opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/ops.py:1217 set_shape (self.shape, shape)) ValueError: Tensor's shape (512, 768) is not compatible with supplied shape [2, 768]`"
11168,b'[examples run_clm] fix _LazyModule hasher error',2021-04-09T16:53:33Z,2021-04-09T18:39:12Z,,TypeError,"TypeError: cannot pickle '_LazyModule' object"
11167,b'added json dump and extraction of train run time',2021-04-09T16:52:02Z,2021-04-09T19:18:00Z,,,
11166,b'[run_clm] tokenize_function clarification makes it non-hashable => no-reusing cache',2021-04-09T16:01:37Z,2021-04-09T18:12:10Z,,,
11165,b'tokenizer.encode_plus returns torch.tensors loaded on the desired device',2021-04-09T14:39:33Z,2021-04-09T16:23:43Z,,,
11164,b'error while training wave2vec on arabic text',2021-04-09T14:05:44Z,2021-05-17T15:03:01Z,,IndexError,"IndexError: list index out of range"
11163,b'Make `get_special_tokens_mask` consider all tokens',2021-04-09T14:01:10Z,2021-04-09T15:57:44Z,,,
11162,b'ZeroDivisionError: float division by zero after some epochs while training using run_mmimdb.py',2021-04-09T13:53:14Z,2021-05-17T15:03:02Z,,"""ZeroDivisionError","""ZeroDivisionError: float division by zero"""
11161,b'Correct typographical error in README.md',2021-04-09T12:20:41Z,2021-04-09T15:52:22Z,,,
11160,b'Why optimizer need split parameter group?',2021-04-09T10:16:24Z,2021-05-17T15:03:03Z,,,
11159,b'LM finetuning on domain specific unlabelled data',2021-04-09T10:14:24Z,2021-04-09T19:27:03Z,,,
11158,b'Why padding tokens can be masked in albert model? Is it bug or right?',2021-04-09T08:09:00Z,2021-04-09T18:14:10Z,,,
11157,b'model_path should be ignored as the checkpoint path',2021-04-09T06:36:12Z,2021-04-12T13:06:41Z,,RuntimeError,"RuntimeError: Caught RuntimeError in replica 1 on device 1."
11156,b'Multi-`train_dataset` in Huggingface Trainer',2021-04-09T05:36:52Z,2021-04-12T11:56:26Z,,,
11155,b'[BUG] padding tokens are also masked in DataCollatorForLanguageModeling',2021-04-09T03:27:36Z,2021-04-09T15:57:44Z,,,
11154,b'Using run_language_modeling.py to train an English adapter',2021-04-09T03:23:18Z,2021-05-17T15:03:04Z,Migration,,
11153,"b""cannot import name 'BigBirdModel' from 'transformers'""",2021-04-09T03:02:04Z,2021-05-17T15:03:04Z,,,
11152,b'typo',2021-04-09T02:23:55Z,2021-04-09T02:47:32Z,,,
11151,b'[setup] make fairscale and deepspeed setup extras',2021-04-08T21:44:27Z,2021-04-08T22:46:54Z,,,
11150,b'Add support for multiple models for one config in auto classes',2021-04-08T19:51:11Z,2021-04-08T22:41:36Z,,,
11149,b'Enable option for subword regularization in `XLMRobertaTokenizer`',2021-04-08T19:21:49Z,2021-04-23T21:52:32Z,,,
11148,"b""[setup] extras[docs] must include 'all'""",2021-04-08T19:12:04Z,2021-04-08T22:10:44Z,,,
11147,b'Add fairscale and deepspeed back to the CI',2021-04-08T17:52:41Z,2021-04-08T18:36:45Z,,,
11146,b'[tests] relocate core integration tests',2021-04-08T17:39:26Z,2021-04-08T20:13:17Z,,,
11145,b'[run_clm] clarify why we get the tokenizer warning on long input',2021-04-08T15:59:56Z,2021-04-08T16:46:29Z,,,
11144,"b'[trainer] solve ""scheduler before optimizer step"" warning'",2021-04-08T15:53:00Z,2021-04-08T18:28:49Z,,,
11143,b'Training loss is not logged correctly when doing evaluation with Trainer',2021-04-08T15:24:21Z,2021-04-08T16:51:57Z,,,
11142,b'[Community notebooks] Add Wav2Vec notebook for creating captions for YT Clips',2021-04-08T15:05:13Z,2021-04-09T06:40:37Z,,,
11141,"b""Don't duplicate logs in TensorBoard and handle --use_env""",2021-04-08T13:36:54Z,2021-04-08T20:12:36Z,,,
11140,b'Updates SageMaker docs for updating DLCs',2021-04-08T12:32:01Z,2021-04-08T20:05:53Z,,,
11139,b'OOM issue with prediction',2021-04-08T09:44:47Z,2021-05-17T15:03:05Z,,RuntimeError,"RuntimeError: CUDA out of memory. Tried to allocate 932.00 MiB (GPU 0; 15.78 GiB total capacity; 12.89 GiB already allocated; 913.69 MiB free; 13.79 GiB reserved in total by PyTorch)"
11138,b'Fix typing error in Trainer class (prediction_step)',2021-04-08T09:40:35Z,2021-04-08T12:22:26Z,,,
11137,"b'Inference time got very high, very low CUDA activity'",2021-04-08T08:49:58Z,2021-06-21T15:06:27Z,,,
11136,b'Trainer callbacks such as on_epoch_end do not pass in the documented eval dataloader',2021-04-08T08:25:57Z,2021-05-16T15:01:54Z,,,
11135,b'Adding FastSpeech2',2021-04-08T07:33:38Z,2021-04-29T17:18:48Z,,,
11134,b'Problem with data download',2021-04-08T06:59:24Z,2021-05-16T15:01:54Z,,,
11133,b'Typo fix of the name of BertLMHeadModel in BERT doc',2021-04-08T05:21:04Z,2021-04-08T12:22:59Z,,,
11132,b'Clear add labels to token classification example',2021-04-08T02:01:09Z,2021-05-16T15:01:55Z,,,
11131,b'Update training.rst',2021-04-08T01:41:57Z,2021-05-16T15:01:56Z,,,
11130,b'Fix LogitsProcessor documentation',2021-04-08T01:21:39Z,2021-04-09T07:09:44Z,,,
11129,"b'denoising with sentence permutation, and language sampling'",2021-04-07T22:08:57Z,,Feature request,,
11128,b'Run mlm pad to multiple for fp16',2021-04-07T21:19:53Z,2021-04-08T20:12:49Z,,,
11127,b'Fix and refactor check_repo',2021-04-07T19:35:28Z,2021-04-07T21:56:21Z,,,
11126,b'Create embeddings vectors for the context parameter of QuestionAnsweringPipeline for reusability.',2021-04-07T18:45:21Z,2021-05-16T15:01:57Z,,,
11125,b'Not very good answers',2021-04-07T17:34:12Z,2021-05-16T15:01:58Z,,,
11124,b'ALBERT pretrained tokenizer loading failed on Google Colab',2021-04-07T17:32:31Z,2021-04-07T18:44:01Z,,TypeError,"TypeError: 'NoneType' object is not callable"
11123,b'Adds use_auth_token with pipelines',2021-04-07T16:11:46Z,2021-04-07T18:33:00Z,,,
11122,b'fixed max_length in beam_search() and group_beam_search() to use beam\xe2\x80\xa6',2021-04-07T14:41:32Z,2021-05-26T15:08:53Z,,,
11121,b'Errors in inference API',2021-04-07T14:18:32Z,2021-04-08T17:05:01Z,,,
11120,b'Adds a note to resize the token embedding matrix when adding special \xe2\x80\xa6',2021-04-07T13:31:39Z,2021-04-07T14:06:46Z,,,
11119,b'updated user permissions based on umask',2021-04-07T13:14:19Z,2021-05-10T06:45:29Z,,,
11118,b'Some styling of the training table in Notebooks',2021-04-07T13:10:34Z,2021-04-07T14:00:34Z,,,
11117,b'Add an error message for Reformer w/ .backward()',2021-04-07T12:36:29Z,2021-04-20T22:23:37Z,,,
11116,b'Wrong num_label configuration in Fine Tuning NER when model_name_or_path is specified',2021-04-07T12:25:40Z,2021-05-15T15:01:58Z,,,
11115,b'Nested MLflow logging with cross-validation',2021-04-07T10:19:05Z,2021-05-15T15:01:59Z,,`Exception,"`Exception: Run with UUID d2bf3cf7cc7b4e359f4c4db098604350 is already active. To start a new run, first end the current run with mlflow.end_run(). To start a nested run, call start_run with nested=True`"
11114,"b'Add CodeTrans, a model for source code generation, documentation generation and similar subtasks.'",2021-04-07T09:13:04Z,2021-09-17T13:08:25Z,New model,,
11113,b'How to resume_from_checkpoint for Seq2SeqTrainer of EncoderDecoderLM',2021-04-07T08:21:48Z,2021-04-15T05:34:39Z,,,
11112,b'XLNET tokenization changes after saving and loading',2021-04-07T07:35:31Z,2021-04-07T09:37:40Z,,,
11111,"b""Where to add truncation=True for warning Truncation was not explicitely activated but max_length is provided a specific value, please use truncation=True to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.""",2021-04-07T06:44:23Z,2021-05-15T15:01:59Z,,,
11110,b'[versions] handle version requirement ranges',2021-04-07T06:43:54Z,2021-04-07T16:09:38Z,,AttributeError,"AttributeError: module 'numpy' has no attribute '__version__'"
11109,b'[BigBird] fix bigbird slow tests',2021-04-07T06:26:10Z,2021-04-07T14:07:27Z,,,
11108,b'[run_clm] handling large inputs',2021-04-07T05:40:30Z,2021-04-08T16:46:28Z,,,
11107,b'Dependency version check fails for tokenizers',2021-04-07T05:30:45Z,2021-04-07T20:19:47Z,,TypeError,"TypeError: expected string or bytes-like object"
11106,b'lr scheduler before optimizer step warning',2021-04-07T05:25:23Z,2021-04-08T18:28:49Z,,,
11105,"b""fix: The 'warn' method is deprecated""",2021-04-07T04:37:42Z,2021-04-07T13:20:06Z,,,
11104,b'Model config is logged twice on startup',2021-04-07T04:26:08Z,,WIP,,
11103,b'dead link fixed',2021-04-07T04:12:09Z,2021-04-07T11:50:48Z,,,
11102,b'GPT2 IndexError: index out of range in functional.py by running run_clm.py when adding any special tokens (even eos and bos only)',2021-04-07T01:09:45Z,2021-04-07T14:06:45Z,,IndexError,"IndexError: index out of range in self"
11101,b'Confusion',2021-04-07T00:36:05Z,2021-05-15T15:02:00Z,Migration,,
11100,b'Dummies multi backend',2021-04-06T23:22:27Z,2021-04-07T13:56:40Z,,,
11099,b'[examples] fix white space',2021-04-06T20:57:18Z,2021-04-07T13:20:58Z,,,
11098,b'[doc] gpt-neo',2021-04-06T20:22:05Z,2021-04-06T20:42:06Z,,,
11097,b'Auto feature extractor',2021-04-06T20:10:49Z,2021-04-06T23:20:08Z,,,
11096,b'GPTNeo: RuntimeError: shape mismatch when using past_key_values to go forward more than one token',2021-04-06T18:58:36Z,2021-06-10T15:02:30Z,,RuntimeError,"RuntimeError: shape '[1, 1, 1, 2048]' is invalid for input of size 4096"
11095,b'XLMRobertaTokenizerFast gives incorrect offset mappings when loaded from disk',2021-04-06T17:50:34Z,2021-04-08T21:45:42Z,,,
11094,b'Using BERTModel for learning a siamese encoder',2021-04-06T16:42:10Z,2021-04-07T11:17:00Z,,,
11093,b'Cannot get test logits after training for TFSequenceClassifier on TF 2',2021-04-06T16:02:04Z,2021-05-15T15:02:01Z,,`ValueError,"`ValueError: Attempt to convert a value (TFSequenceClassifierOutput(loss=None, logits=None, hidden_states=None, attentions=None)) with an unsupported type (<class 'transformers.modeling_tf_outputs.TFSequenceClassifierOutput'>) to a Tensor.`"
11092,b'[question/help] T5 cross-attention shows inconsistent results',2021-04-06T13:43:20Z,2021-04-14T09:13:33Z,,,
11091,b'accelerate question answering examples with no trainer',2021-04-06T13:02:48Z,2021-04-06T23:35:21Z,,,
11090,b'added new merged Trainer test',2021-04-06T12:57:59Z,2021-04-06T13:12:22Z,,,
11089,b'[Possible Bug] Getting IndexError: list index out of range when fine-tuning custom LM model',2021-04-06T12:22:50Z,2021-05-17T15:03:07Z,,IndexError,"IndexError: list index out of range"
11088,b'CTRL model can not work',2021-04-06T11:10:48Z,2021-05-15T15:02:02Z,,,
11087,b'using RAG with local documents',2021-04-06T09:17:07Z,2021-04-06T09:17:32Z,,,
11086,b'using RAG with local documents',2021-04-06T09:09:30Z,2021-04-06T11:57:04Z,,,
11085,b'Add DistilBertForCausalLM',2021-04-06T08:59:34Z,,WIP,,
11084,b'How to use tensorboard with Trainer?',2021-04-06T08:52:14Z,2021-04-06T13:42:38Z,,,
11083,b'added social thumbnail for docs',2021-04-06T08:50:22Z,2021-04-06T12:56:18Z,,,
11082,b'performance drop after using bert',2021-04-06T07:32:57Z,2021-05-16T15:02:00Z,,,
11081,"b""HF emoji unicode doesn't work in console""",2021-04-06T05:52:59Z,2021-04-06T12:03:00Z,,,
11080,"b""'BertTokenizer' object has no attribute 'decode'""",2021-04-06T05:26:14Z,2021-04-06T18:02:02Z,,AttributeError,"AttributeError: 'BertTokenizer' object has no attribute 'decode'"
11079,b'GPTNeo: handle padded wte (#11078)',2021-04-06T05:13:15Z,2021-04-07T12:05:21Z,,,
11078,b'GPTNeo: importing model with padded vocab size should truncate wte',2021-04-06T04:56:48Z,2021-04-07T12:05:20Z,,,
11077,b'bug in quantization on albert ',2021-04-06T03:11:50Z,2021-04-06T03:47:14Z,,AttributeError,"AttributeError: 'function' object has no attribute 't'"
11076,b'FP16 overflow with GPT-Neo when using sequence lengths of 2048.',2021-04-06T02:28:56Z,2021-05-27T15:10:07Z,,,
11075,b'Big Bird Fast Tokenizer implementation',2021-04-05T22:48:20Z,2021-05-10T07:01:24Z,,,
11074,b'Make a base init in FeatureExtractionMixin',2021-04-05T21:51:49Z,2021-04-05T22:02:29Z,,,
11073,b'Add Readme for language modeling scripts with custom training loop and accelerate',2021-04-05T21:00:38Z,2021-04-06T00:56:12Z,,,
11072,b'[WIP] Pad to multiple of 8 for run mlm example',2021-04-05T20:12:19Z,2021-04-07T21:28:18Z,,,
11071,b'Fix distributed gather for tuples of tensors of varying sizes',2021-04-05T19:44:24Z,2021-04-05T20:21:49Z,,,
11070,b'Document common config attributes',2021-04-05T18:41:54Z,2021-04-05T19:29:02Z,,,
11069,b'[docs] [sphinx] need to resolve cross-references for inherited/mixin methods',2021-04-05T18:13:30Z,2021-06-02T16:22:47Z,"Good First Issue, Good Second Issue",,
11068,b'Add a special tokenizer for CPM model',2021-04-05T18:09:11Z,2021-04-09T18:07:47Z,,,
11067,b'Inconsistent ProphetNet Tokenization',2021-04-05T17:33:01Z,2021-05-17T15:03:09Z,,,
11066,b'Add center_crop to ImageFeatureExtractionMixin',2021-04-05T16:40:24Z,2021-04-05T19:28:52Z,,,
11065,b'Token indices sequence length is longer than the specified maximum sequence length for this model (651 > 512) with Hugging face sentiment classifier',2021-04-05T14:37:42Z,2021-05-13T15:02:03Z,,IndexError,"IndexError: index out of range in self"
11064,b'Some models have no tokenizers',2021-04-05T13:33:39Z,2021-04-05T13:37:49Z,,,
11063,"b'save_strategy=""no"" but checkpoints are created after each evaulation'",2021-04-05T13:25:53Z,2021-05-17T15:03:10Z,,,
11062,b'Pin docutils',2021-04-05T13:21:17Z,2021-04-05T13:35:22Z,,,
11061,b'Replace pkg_resources with importlib_metadata',2021-04-05T13:10:09Z,2021-04-05T19:12:19Z,,,
11060,b'Remove unnecessary space',2021-04-05T13:01:50Z,2021-04-05T13:36:20Z,,,
11059,"b'run_summarization: fine tuning Pegasus large, CUDA out of memory error '",2021-04-05T11:33:47Z,2021-05-24T15:01:56Z,,RuntimeError,"RuntimeError: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 0; 15.78 GiB total capacity; 14.02 GiB already allocated; 21.75 MiB free; 14.16 GiB reserved in total by PyTorch)"
11058,b'MemoryError when computing metrics on Wav2Vec2',2021-04-05T10:54:55Z,2021-05-13T15:02:05Z,,,
11057,b'Difference in tokenizer output depending on where `add_prefix_space` is set. ',2021-04-05T10:30:25Z,2021-06-07T15:18:36Z,,,
11056,b'Add DeiT (PyTorch)',2021-04-05T09:46:25Z,2021-04-12T22:07:10Z,,,
11055,b'Got ValueError when `output_hidden_states=True` with `eval_accumulation_steps`',2021-04-05T06:54:10Z,2021-04-05T20:21:49Z,,ValueError,"ValueError: could not broadcast input array from shape (16,22,768) into shape (16,19,768)"
11054,b'Add parallelize method to GPT-neo models',2021-04-04T23:11:36Z,2021-05-13T15:02:06Z,,,
11053,b'[doc] fix code-block rendering',2021-04-04T20:39:51Z,2021-04-05T13:06:07Z,,,
11052,b'Implement fast tokenizer for Big Bird models',2021-04-04T19:24:34Z,2021-05-10T07:01:24Z,,,
11051,b'Training mask language model using multiple files',2021-04-04T16:14:30Z,2021-05-13T15:02:07Z,,,
11050,b'accelerate scripts for question answering with no trainer',2021-04-04T12:39:27Z,2021-04-06T12:33:23Z,,,
11049,b'[docs] fix xref to `PreTrainedModel.generate`',2021-04-03T20:49:56Z,2021-06-02T16:21:06Z,WIP,,
11048,b'fix incorrect case for s|Pretrained|PreTrained|',2021-04-03T20:33:30Z,2021-04-05T01:08:42Z,,,
11047,b'Use Bert model without pretrained weights',2021-04-03T07:55:13Z,2021-04-03T12:07:52Z,,,
11046,b'Potential incorrect application of layer norm in BlenderbotSmallDecoder',2021-04-03T03:37:32Z,2021-05-26T15:08:55Z,,,
11045,b'Multi-GPU seq2seq example evaluation significantly slower than legacy example evaluation',2021-04-03T00:52:24Z,2021-05-11T15:01:57Z,,,
11044,b'[DeepSpeed] ZeRO stage 3 integration: getting started and issues',2021-04-02T23:40:42Z,2021-04-27T01:47:03Z,DeepSpeed,,
11043,"b""Can't load model estimater after training""",2021-04-02T21:51:44Z,2021-06-09T15:07:05Z,,,
11042,b'[LXMERT] Unclear what img_tensorize does with color spaces',2021-04-02T15:12:57Z,2021-05-11T15:01:58Z,,,
11041,b'wav2vec2 converter: create the proper vocab.json while converting fairseq wav2vec2 finetuned model',2021-04-02T15:04:14Z,2021-04-13T10:24:34Z,,,
11040,b'max_length in beam_search() and  group_beam_search() does not consider beam_scorer.max_length',2021-04-02T14:56:32Z,2021-04-26T22:28:40Z,,RuntimeError,"RuntimeError: The expanded size of the tensor (5) must match the existing size (6) at non-singleton dimension 0.  Target sizes: [5].  Tensor sizes: [6]"
11039,b'Trainer not logging into Tensorboard',2021-04-02T14:17:54Z,2021-04-08T13:53:55Z,,,
11038,b'DeBERTa xlarge v2 throwing runtime error',2021-04-02T12:05:53Z,2021-05-10T15:02:16Z,,RuntimeError,"RuntimeError: Error(s) in loading state_dict for DebertaForSequenceClassification:"
11037,b'Was bert-large-uncased-whole-word-masking-finetuned-squad fine tuned or not.',2021-04-02T09:35:13Z,2021-04-03T05:52:37Z,,,
11036,b'BertForTokenClassification class ignores long tokens when making predictions',2021-04-02T06:22:15Z,2021-05-10T15:02:17Z,,,
11035,b'404 Client Error: Not Found for url: https://huggingface.co/%5CHuggingface-Sentiment-Pipeline/resolve/main/config.json',2021-04-02T05:39:04Z,2021-05-10T15:02:18Z,,"HTTPError, OSError","HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/%5CHuggingface-Sentiment-Pipeline/resolve/main/config.jsonOSError: Can't load config for '\Huggingface-Sentiment-Pipeline'. Make sure that:"
11034,b'GPT-2 example is broken?',2021-04-02T03:56:40Z,2021-04-05T13:36:20Z,,,
11033,b'RuntimeError: The size of tensor a (1024) must match the size of tensor b (1025) at non-singleton dimension 3',2021-04-02T03:45:57Z,2021-05-11T15:01:58Z,,,
11032,b'How to get masked word prediction for other languages',2021-04-02T02:59:40Z,2021-05-11T15:01:59Z,,,
11031,b'Roberta and XLNet sentence pair training example',2021-04-02T02:58:21Z,2021-04-02T14:24:53Z,,,
11030,b'pipeline.from_pretrained',2021-04-01T22:16:56Z,2021-05-02T17:20:00Z,,,
11029,b'Documentation about loading a fast tokenizer within Transformers',2021-04-01T21:22:48Z,2021-04-05T14:51:16Z,,,
11028,b'Fine Tune GPT-NEO 2.7B',2021-04-01T20:31:26Z,2021-05-11T15:02:00Z,,,
11027,b'Refactor AutoModel classes and add Flax Auto classes',2021-04-01T20:29:34Z,2021-04-05T14:11:29Z,,,
11026,b'Add `examples/language_modeling/run_clm_no_trainer.py`',2021-04-01T20:08:29Z,2021-04-05T16:27:53Z,,,
11025,b'fixed typo: logging instead of logger',2021-04-01T19:23:37Z,2021-04-02T13:22:22Z,,,
11024,b'Add a script to check inits are consistent',2021-04-01T19:07:12Z,2021-04-05T00:41:34Z,,,
11023,b'Strange ValueError with GPT-2',2021-04-01T18:09:12Z,2021-05-10T15:02:19Z,,ValueError,"ValueError: in user code:"
11022,"b""cannot import name 'AutoModelForSequenceClassification' from 'transformers'""",2021-04-01T18:07:36Z,2021-05-10T15:02:20Z,,,
11021,b'Module Not found: datasets_modules.datasets.output',2021-04-01T17:23:48Z,2021-04-08T12:32:35Z,,ModuleNotFoundError,"ModuleNotFoundError: No module named 'datasets_modules.datasets.output'"
11020,b'Trainer API crashes GPUs',2021-04-01T15:25:04Z,2021-06-08T15:06:50Z,,RuntimeError,"RuntimeError: CUDA error: unspecified launch failure"
11019,b'Enable multiple `eval_dataset` in `Trainer` API',2021-04-01T14:57:11Z,,Feature request,,
11018,b'T5 documentation for computing pretraining loss seems to have a mistake ',2021-04-01T14:49:55Z,2021-05-23T15:02:16Z,,,
11017,b'Cannot run the gpt neo 2.7B example',2021-04-01T14:34:18Z,2021-04-01T15:20:14Z,,KeyError,"KeyError: 'gpt_neo'"
11016,b'Add new CANINE model',2021-04-01T13:53:21Z,2021-06-30T12:05:44Z,New model,,
11015,b'added new notebook and merge of trainer',2021-04-01T11:46:29Z,2021-04-01T21:13:47Z,,,
11014,"b""OSError: Can't load config for '/content/wav2vec2-large-xlsr-asr-demo'. Make sure that:""",2021-04-01T11:19:17Z,2021-05-10T15:02:21Z,,OSError,"OSError: Can't load config for '/content/wav2vec2-large-xlsr-asr-demo'. Make sure that:"
11013,b'use `BaseModelOutput` as common interface for all different `BaseModelOutputWith*`?',2021-04-01T10:41:02Z,,Feature request,,
11012,"b'Add multi-class, multi-label and regression to transformers'",2021-04-01T09:06:59Z,2021-05-04T06:23:40Z,,,
11011,b'a memory leak in evaluation',2021-04-01T06:52:18Z,2021-04-06T10:03:15Z,,,
11010,b'run_seq2seq.py meet bug in using huggingface datasets billsum',2021-04-01T06:20:47Z,2021-05-10T15:02:22Z,,KeyError,"KeyError: 'validation'"
11009,b'How to load weights from a private server?',2021-04-01T05:35:52Z,2021-04-15T02:04:51Z,,,
11008,b'error: fine-tunes language model with added_tokens',2021-04-01T02:53:17Z,2021-04-06T03:04:54Z,,RuntimeError,"RuntimeError: CUDA error: CUBLAS_STATUS_INTERNAL_ERROR when calling `cublasSgemm( handle, opa, opb, m, n, k, &alpha, a, lda, b, ldb, &beta, c, ldc)`"
11007,b'about .py file',2021-04-01T02:47:40Z,2021-05-10T15:02:23Z,,,
11006,"b'""Converting Tensorflow Checkpoints"" meets  (\'Pointer shape torch.Size([312]) and array shape (128,) mismatched\', torch.Size([312]), (128,))'",2021-04-01T02:35:58Z,2021-05-10T15:02:24Z,,AssertionError,"AssertionError: ('Pointer shape torch.Size([312]) and array shape (128,) mismatched', torch.Size([312]), (128,)), and the pretrainmodel comes from https://github.com/ZhuiyiTechnology/pretrained-models"
11005,b'ReduceLROnPlateau-like functionality?',2021-03-31T21:50:33Z,,Feature request,,
11004,b'Getting `raise NotImplementedError` for base_model.get_input_embeddings() when upgrading from pytorch-transformers',2021-03-31T21:46:00Z,2021-05-11T15:02:03Z,Migration,,
11003,b'conda install transformers (not working) behaving differently from pip install transformers (working) for CentOS 7.9',2021-03-31T20:37:07Z,2021-03-31T23:13:57Z,,,
11002,"b""KeyError: 'gpt_neo' with EleutherAI/gpt-neo-1.3B""",2021-03-31T19:54:34Z,2021-03-31T20:27:41Z,,KeyError,"KeyError: 'gpt_neo'"
11001,b'Add `examples/language_modeling/run_mlm_no_trainer.py`',2021-03-31T18:27:20Z,2021-03-31T22:49:46Z,,,
11000,b'In the group by length documentation length is misspelled as legnth',2021-03-31T16:35:46Z,2021-03-31T22:28:07Z,,,
10999,b'ROUGE Multiple References',2021-03-31T16:27:19Z,2021-05-10T15:02:26Z,,,
10998,"b""Get following error with EncoderDecoder model: TypeError: forward() got an unexpected keyword argument 'use_cache'""",2021-03-31T15:42:11Z,2021-05-19T15:08:42Z,,TypeError,"TypeError: forward() got an unexpected keyword argument 'use_cache'."
10997,b'[Docs] Add blog to BigBird docs',2021-03-31T15:24:58Z,2021-03-31T15:36:00Z,,,
10996,"b'GPT Neo, Print Most Probable Next Word: String Indices Must Be Integers'",2021-03-31T14:25:29Z,2021-05-09T15:01:48Z,,`TypeError,`TypeError: string indices must be integers`
10995,b'[Notebook] add BigBird trivia qa notebook',2021-03-31T13:57:58Z,2021-03-31T14:00:57Z,,,
10994,b'Fix the checkpoint for I-BERT',2021-03-31T12:02:12Z,2021-03-31T12:02:52Z,,,
10993,b'[GPT Neo] fix example in config',2021-03-31T11:57:13Z,2021-03-31T12:08:57Z,,,
10992,b'GPT Neo configuration needs to be set to use GPT2 tokenizer',2021-03-31T11:55:02Z,2021-03-31T12:03:20Z,,,
10991,b'Add BigBirdPegasus',2021-03-31T11:49:26Z,2021-05-07T07:27:43Z,,,
10990,"b""Can't find ibert-roberta-base model""",2021-03-31T11:49:25Z,2021-03-31T12:02:52Z,,OSError,"OSError: Can't load tokenizer for 'ibert-roberta-base'. Make sure that:"
10989,b'Fixed some typos and removed legacy url',2021-03-31T10:21:42Z,2021-03-31T11:23:15Z,,,
10988,b'unable to use multiple GPUs with HF integration of DeepSpeed on Jupyter notebooks',2021-03-31T10:06:30Z,2021-03-31T18:38:19Z,DeepSpeed,,
10987,b'Sagemaker test fix',2021-03-31T09:41:26Z,2021-03-31T11:44:22Z,,,
10986,b'BART : Cannot run trainer.evaluate() after trainer.train()',2021-03-31T08:15:00Z,2021-03-31T15:04:15Z,,KeyError,"KeyError: 0"
10985,b'[WIP] GPT Neo cleanup',2021-03-31T07:29:25Z,2021-04-06T16:24:15Z,,,
10984,b'AttributeError due to multi-processing using PyTorchBenchmark',2021-03-31T07:21:59Z,2021-06-21T15:06:30Z,,"AttributeError, EOFError","AttributeError: Can't pickle local object 'separate_process_wrapper_fn.<locals>.multi_process_func.<locals>.wrapper_func'EOFError: Ran out of input"
10983,b'FineTune XLSR-Wav2Vec2 on New Langauge WER still 1 ',2021-03-31T06:53:27Z,2021-05-09T15:01:49Z,,,
10982,b'Update setup.py',2021-03-31T05:41:00Z,2021-03-31T05:41:52Z,,,
10981,b'support passing path to a `config` variable in AutoClass',2021-03-31T05:28:32Z,2021-05-09T15:01:49Z,,,
10980,b'Enforce string-formatting with f-strings',2021-03-31T02:28:58Z,2021-03-31T14:00:27Z,,,
10979,b'Tagged Model Version Not Working',2021-03-31T00:37:45Z,2021-03-31T18:21:35Z,,requests.exceptions.HTTPError,"requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/roberta-large/resolve/v3.5.0/config.json"
10978,b'Add GPT Neo models to Write With Transformer',2021-03-31T00:10:23Z,,Feature request,,
10977,b'[Flax] Add other BERT classes',2021-03-30T22:22:38Z,2021-03-31T06:45:59Z,Flax,,
10976,b'Transformers QA Online Demo is not working',2021-03-30T21:15:21Z,2021-03-31T13:42:49Z,,,
10975,b'Merge trainers',2021-03-30T20:06:25Z,2021-03-31T14:01:30Z,,,
10974,b'Reproducing DistilRoBERTa',2021-03-30T18:49:48Z,2021-03-30T19:42:39Z,,,
10973,b'accelerate scripts for question answering and qa with beam search',2021-03-30T18:38:18Z,2021-04-04T12:04:15Z,,,
10972,b'Add more metadata to the user agent',2021-03-30T17:45:10Z,2021-03-31T13:36:07Z,,,
10971,b'added py7zr',2021-03-30T17:32:31Z,2021-03-30T17:47:12Z,,"[1,14]<stdout>:ImportError","[1,14]<stdout>:ImportError: To be able to use this dataset, you need to install the following dependencies['py7zr'] using 'pip install py7zr' for instance'"
10970,b'Fixed a bug where the `pipeline.framework` would actually contain a fully qualified model.',2021-03-30T17:17:47Z,2021-03-30T17:26:35Z,,,
10969,b'[GPT Neo] defaults for max length and sampling',2021-03-30T16:47:56Z,2021-03-30T16:50:41Z,,,
10968,b'GPT Neo few fixes',2021-03-30T14:59:21Z,2021-03-30T15:15:55Z,,,
10967,b'[BigBird] Fix big bird gpu test',2021-03-30T13:51:47Z,2021-03-30T14:03:49Z,,,
10966,b'improved sagemaker documentation for git_config and examples',2021-03-30T13:45:39Z,2021-03-30T16:00:52Z,,,
10965,b'Gradient checkpointing in Wav2Vec2',2021-03-30T13:31:38Z,2021-05-08T15:01:49Z,,,
10964,"b""pkg_resources' working_set caching breaks transformers import on google colab""",2021-03-30T11:34:47Z,2021-04-05T19:12:19Z,,FileNotFoundError,"FileNotFoundError: [Errno 2] No such file or directory: '/usr/local/lib/python3.7/dist-packages/tqdm-4.41.1.dist-info/METADATA'"
10963,b'compute perplexity using a custom metric function',2021-03-30T11:14:18Z,2021-03-30T16:34:07Z,,,
10962,b'fix md file to avoid evaluation crash',2021-03-30T10:28:12Z,2021-03-30T18:26:23Z,,,
10961,b'Supporting `config_path` for `AutoModel`',2021-03-30T09:42:33Z,2021-05-08T15:01:50Z,,,
10960,b'What is the score of trainer.predict()?',2021-03-30T07:53:13Z,2021-03-30T23:41:38Z,,,
10959,b'Fix summarization notebook link',2021-03-30T06:33:39Z,2021-03-30T12:28:58Z,,,
10958,b'Returning Confidence Score For Extractive QA Task When Using Non-Pipeline Approach ',2021-03-30T05:54:35Z,,Feature request,,
10957,b'check_version not valid',2021-03-29T23:13:06Z,2021-04-01T16:37:25Z,,,
10956,b'[T5/MT5] resolve inf/nan under amp (mixed precision) ',2021-03-29T22:43:37Z,,WIP,,
10955,b'Input gets lost when converting mBART decoder to onnx',2021-03-29T21:21:16Z,2021-03-31T17:33:15Z,,,
10954,b'[vulnerability] dep fix',2021-03-29T21:09:19Z,2021-03-29T21:25:47Z,,,
10953,"b'Use pre-computed lengths, if available, when grouping by length'",2021-03-29T19:17:21Z,2021-03-29T19:44:20Z,,,
10952,b'[Trainer] possible DDP memory regression',2021-03-29T17:25:25Z,2021-03-29T18:32:50Z,,,
10951,b'Fixes in the templates',2021-03-29T16:45:09Z,2021-03-29T21:36:14Z,,,
10950,b'Add Vision Transformer and ViTFeatureExtractor',2021-03-29T12:45:10Z,2021-04-01T15:16:05Z,,,
10949,b'How to freeze Camembert model for Classification tasks? ',2021-03-29T12:11:04Z,2021-05-07T15:01:57Z,Migration,,
10948,"b""[MarianMTModel] 'list' object has no attribute 'size'""",2021-03-29T10:46:22Z,2021-06-06T15:06:24Z,,AttributeError,"AttributeError: 'list' object has no attribute 'size'"
10947,b'Save model error: list index out of range after pass input_processing call',2021-03-29T09:36:16Z,2021-05-07T15:01:58Z,,IndexError,"IndexError: list index out of range"
10946,b'[Feature] Add a new tiny feature for self-attention analysis',2021-03-29T08:55:12Z,2021-05-07T15:02:00Z,,,
10945,b'Are there memory leaks when using DeepSpeed on training T5?',2021-03-29T08:47:41Z,2021-05-08T15:01:51Z,DeepSpeed,,
10944,b'Please implement DUMA: Reading Comprehension with Transposition Thinking',2021-03-29T08:42:13Z,,New model,,
10943,b'Converting marian tatoeba models',2021-03-29T08:31:28Z,2021-10-15T11:55:53Z,,ValueError,"ValueError: Length mismatch: Expected axis has 7 elements, new values have 9 elements"
10942,b'Wav2Vec2CTCTokenizer does not take the vocabulary into account when identifying tokens in a sentence',2021-03-29T08:31:05Z,2021-04-22T10:23:09Z,,,
10941,b'Added documentation for data collator.',2021-03-28T21:37:32Z,2021-04-12T15:59:47Z,,,
10940,b'Addition of  SequenceClassification config specific documentation to XModelForSequenceClassification.',2021-03-28T20:39:50Z,2021-03-29T15:54:45Z,,,
10939,b'[Example] Fixed finename for Saving null_odds in the evaluation stage in QA Examples ',2021-03-28T13:32:20Z,2021-03-28T16:48:12Z,,,
10938,b'saving pretrained models that were obtained from another model',2021-03-28T11:02:26Z,2021-05-05T15:02:09Z,,,
10937,b'[trainer metrics] fix cpu mem metrics; reformat runtime metric',2021-03-28T05:48:46Z,2021-03-29T20:47:02Z,,,
10936,b'Fix initializing BertJapaneseTokenizer with AutoTokenizers',2021-03-28T05:42:00Z,2021-03-29T14:26:16Z,,AttributeError,"AttributeError: 'NoneType' object has no attribute '__name__'"
10935,b'Add DALL-E: Zero-Shot Text-to-Image Generation',2021-03-28T04:13:41Z,,New model,,
10934,b'Add `examples/multiple-choice/run_swag_no_trainer.py`',2021-03-27T23:16:33Z,2021-03-29T20:41:10Z,,,
10933,"b""Can't download the facebook/bart-large-mnli tensorflow model""",2021-03-27T20:30:21Z,2021-03-27T20:47:38Z,,"HTTPError, OSError","HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/facebook/bart-large-mnli/resolve/main/tf_model.h5OSError: Can't load weights for 'facebook/bart-large-mnli'. Make sure that:"
10932,b'Updated colab links in readme of examples',2021-03-27T18:22:28Z,2021-03-29T12:47:09Z,,,
10931,"b'Another way to express masked_index = torch.nonzero(input_ids == self.tokenizer.mask_token_id, as_tuple=False)'",2021-03-27T16:10:14Z,2021-05-05T15:02:11Z,,`TypeError,"`TypeError: nonzero() got an unexpected keyword argument 'as_tuple'`"
10930,b'Error while predicting on single sentence for token classification task',2021-03-27T15:57:23Z,2021-03-29T00:44:03Z,,IndexError,"IndexError: list index out of range"
10929,b'Training with DeepSpeed takes more GPU memory than without DeepSpeed',2021-03-27T14:06:38Z,2021-05-04T15:02:16Z,DeepSpeed,,
10928,b'Add example for registering callbacks with trainers',2021-03-27T03:55:28Z,2021-04-05T16:27:23Z,,,
10927,b'Add Pooler to DistilBERT',2021-03-26T20:24:04Z,2021-05-04T15:02:17Z,,,
10926,b'Typo in examples/text-classification README',2021-03-26T19:53:56Z,2021-05-04T15:02:18Z,,,
10925,b'Sagemaker test',2021-03-26T17:51:28Z,2021-03-30T06:28:02Z,,,
10924,b'Models not able to run when packed with PyInstaller',2021-03-26T17:45:42Z,2021-05-04T15:02:19Z,,OSError,"OSError: could not get source code"
10923,b'/pytorch/xla/torch_xla/csrc/helpers.h:100 : Check failed: scalar_value.isIntegral()',2021-03-26T17:27:52Z,2021-06-21T15:06:32Z,,RuntimeError,"RuntimeError: /pytorch/xla/torch_xla/csrc/helpers.h:100 : Check failed: scalar_value.isIntegral() "
10922,b'Use reformer in down stream task meet problem',2021-03-26T12:15:55Z,2021-05-11T15:02:07Z,,"ValueError, AssertionError","ValueError: Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`.AssertionError: If training, make sure that config.axial_pos_shape factors: (512, 1024) multiply to sequence length. Got prod((512, 1024)) != sequence_length: 1024. You might want to consider padding your sequence length to 524288 or changing config.axial_pos_shape."
10921,b'Tokenizer is adding ## to every word from the second.',2021-03-26T06:42:46Z,2021-05-04T15:02:21Z,,,
10920,b'Rename NLP library to Datasets library',2021-03-26T06:34:00Z,2021-03-26T12:07:59Z,,,
10919,"b'GPT2 on TPU, training is so slow.'",2021-03-26T06:04:49Z,2021-05-04T15:02:22Z,,,
10918,b'OSError: file bert-base-uncased/config.json not found',2021-03-26T05:16:10Z,2021-08-22T15:02:48Z,,OSError,"OSError: file bert-base-uncased/config.json not found"
10917,b' longformer speed compared to bert model',2021-03-26T03:31:22Z,2021-03-26T22:28:43Z,,,
10916,"b""AttributeError: 'Trainer' object has no attribute 'log_metrics'""",2021-03-26T01:33:30Z,2021-05-04T15:02:23Z,,,
10915,b'Bump pyyaml from 5.3.1 to 5.4 in /examples/research_projects/lxmert',2021-03-26T00:39:22Z,2021-03-26T13:07:24Z,dependencies,,
10914,b'[vulnerability] fix dependency',2021-03-25T23:36:43Z,2021-03-26T13:06:12Z,,,
10913,"b""pegasus xsum won't train on xsum dataset""",2021-03-25T23:34:37Z,2021-05-04T15:02:24Z,,,
10912,"b'Summarization length not controlled by max_length, min_length'",2021-03-25T21:41:56Z,,WIP,,
10911,b'Add nvidia megatron models',2021-03-25T20:59:42Z,2021-04-08T18:09:12Z,,,
10910,b'Wav2Vec2 CommonVoice training - Save the processor before training starts',2021-03-25T20:27:40Z,2021-04-14T11:52:06Z,,,
10909,b'LengthGroupedSampler slowly iterates over dataset',2021-03-25T19:56:55Z,2021-03-29T19:44:41Z,,,
10908,"b'Improve the documentation for TrainingArguments.label_names, and if possible raise an error if users misinterpret this attribute like I did'",2021-03-25T19:00:13Z,2021-05-04T15:02:26Z,,,
10907,"b""Exception: cannot import name 'Regex' from 'tokenizers' """,2021-03-25T18:35:09Z,2021-05-04T15:02:27Z,,,
10906,b'Return global attentions (see #7514)',2021-03-25T16:53:22Z,2021-03-29T12:00:23Z,,,
10905,b'Add ImageFeatureExtractionMixin',2021-03-25T16:29:33Z,2021-03-26T15:23:57Z,,,
10904,b'ONNX export: move sample input to same device as model when inferring shapes',2021-03-25T16:01:17Z,2021-04-26T05:54:43Z,,,
10903,b'Add 3D attention mask to T5 model (#9643)',2021-03-25T15:28:59Z,2021-04-12T07:47:27Z,,,
10902,b'Add `examples/run_ner_no_trainer.py`',2021-03-25T15:06:06Z,2021-03-29T19:11:23Z,,,
10901,b'Error with detecting cached files when running without Internet connection (related to #10067)',2021-03-25T13:58:37Z,2021-05-02T15:02:05Z,,,
10900,b'Getting a model to work on a system with no internet access',2021-03-25T11:06:33Z,2021-04-24T15:12:33Z,,,
10899,b'updates sagemaker documentation',2021-03-25T09:57:52Z,2021-03-25T13:01:31Z,,,
10898,b'run_glue_no_trainer: datasets -> raw_datasets',2021-03-25T07:36:54Z,2021-03-25T12:28:17Z,,,
10897,b'[doc] Custom datasets page reference dataset library as NLP library',2021-03-25T07:01:29Z,2021-03-26T12:07:59Z,,,
10896,b'save only the best performing checkpoint',2021-03-25T06:14:31Z,2021-05-02T15:02:06Z,,,
10895,b'Add missing global_attentions into the return_dict of Longformer models',2021-03-25T02:59:25Z,2021-03-29T12:02:47Z,,,
10894,"b' Invalid argument:  Incompatible shapes: [24,1536,12,514] vs. [24,1536,12,513]'",2021-03-25T01:14:06Z,2021-05-02T15:02:07Z,,InvalidArgumentError,"InvalidArgumentError: 2 root error(s) found."
10893,b'[trainer] large scale models support',2021-03-24T23:54:02Z,2021-05-19T15:08:47Z,,,
10892,"b""ImportError: cannot import name 'BertLayerNorm' when upgrading to latest transformers""",2021-03-24T22:10:16Z,2021-03-31T17:52:19Z,Migration,ImportError,"ImportError: cannot import name 'BertLayerNorm'"
10891,b'Update Training Arguments Documentation: ignore_skip_data -> ignore_data_skip',2021-03-24T20:08:59Z,2021-03-24T20:44:51Z,,,
10890,b'Remove version warning in pretrained BART models',2021-03-24T19:00:42Z,2021-03-24T19:21:40Z,,,
10889,b'Fix overflowing bad word ids',2021-03-24T18:56:29Z,2021-03-24T19:13:57Z,,RuntimeError,"RuntimeError: size is inconsistent with indices: for dim 1, size is 30000 but found index 30001"
10888,b'Instantiate model only once in pipeline',2021-03-24T18:47:17Z,2021-03-29T14:39:14Z,,,
10887,b'Error Loading a Hub Model (Multilingual-MiniLM)',2021-03-24T17:55:59Z,2021-03-25T04:52:37Z,,TypeError,"TypeError: stat: path should be string, bytes, os.PathLike or integer, not NoneType"
10886,b'Fix comment in modeling_t5.py',2021-03-24T16:06:23Z,2021-03-25T18:23:56Z,,,
10885,b'Memory accumulates when training in a loop',2021-03-24T14:41:48Z,2021-03-25T12:59:50Z,,,
10884,b'Wav2vec2 Training Loss not decreasing',2021-03-24T12:51:13Z,2021-05-15T15:02:09Z,,,
10883,b'[Community notebooks] Add notebook for fine-tuning Bart with Trainer in two langs',2021-03-24T09:58:01Z,2021-03-24T15:03:38Z,,,
10882,"b""AttributeError: 'RobertaConfig' object has no attribute 'attn_type'""",2021-03-24T09:26:42Z,2021-03-24T10:23:54Z,,AttributeError,"AttributeError: 'RobertaConfig' object has no attribute 'attn_type'`"
10881,b'MlFlow log artefacts',2021-03-24T08:33:58Z,2021-05-01T15:02:34Z,,,
10880,b'Scheduler Not Pickleable',2021-03-24T07:11:24Z,2021-05-01T15:02:35Z,,AttributeError,"AttributeError: Can't pickle local object 'get_linear_schedule_with_warmup.<locals>.lr_lambda'"
10879,b'error type of tokenizer in __init__ definition',2021-03-24T06:15:44Z,2021-03-24T15:00:14Z,,,
10878,b'RuntimeError: while running run_common_voice.py (XLSR wav2vec finetuning week)',2021-03-24T04:08:48Z,2021-05-01T15:02:36Z,,`RuntimeError,"`RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by (1) passing the keyword argument 'find_unused_parameters=True' to 'torch.nn.parallel.DistributedDataParallel'; (2) making sure all 'forward' function outputs participate in calculating loss. If you already have done the above two steps, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's 'forward' function. Please include the loss function and the structure of the return value of 'forward' of your module when reporting this issue (e.g. list, dict, iterable).`"
10877,b'`XLMRobertaTokenizer` `encode_plus` api producing `<unk>` for a valid token',2021-03-24T02:54:00Z,2021-04-05T17:42:00Z,,,
10876,b'Add new notebook links in the docs',2021-03-23T23:46:22Z,2021-03-24T13:45:08Z,,,
10875,b'Fix test_trainer_distributed',2021-03-23T22:56:46Z,2021-03-23T23:03:06Z,,,
10874,b'transformers.models.auto.tokenization_auto',2021-03-23T20:12:11Z,2021-05-01T15:02:37Z,,,
10873,b'Wav2Vec2/XLRS-Wav2Vec2 Pre-Training',2021-03-23T19:32:07Z,2021-06-09T17:40:57Z,,,
10872,b'Training GPT2 does not use GPU',2021-03-23T17:47:17Z,2021-05-01T15:02:38Z,,,
10871,b'not created config.json in Wav2Vec2ForCTC for ASR',2021-03-23T17:45:12Z,2021-05-01T15:02:39Z,,,
10870,b'Sm trainer smp init fix',2021-03-23T17:14:58Z,2021-03-23T19:07:55Z,,,
10869,b'Camembert-base MaskedLM has different config settings that actual camambert-base',2021-03-23T17:07:04Z,2021-06-01T15:20:33Z,,RuntimeError,"RuntimeError: Error(s) in loading state_dict for CamembertForMaskedLM:"
10868,b'[Examples] Added predict stage and Updated Example Template',2021-03-23T15:58:09Z,2021-03-23T17:38:00Z,,,
10867,b'Amazon SageMaker Documentation',2021-03-23T14:56:00Z,2021-03-23T14:56:44Z,,,
10866,"b'add processing ""cache"" and augmentation'",2021-03-23T14:11:03Z,2021-04-22T15:40:38Z,,,
10865,b'Update the example template for a no Trainer option',2021-03-23T13:31:05Z,2021-03-23T14:02:40Z,,,
10864,b'transformers import error',2021-03-23T12:08:00Z,2021-04-30T15:02:10Z,,,
10863,b'Fix p_mask cls token masking in question-answering pipeline',2021-03-23T11:01:00Z,2021-03-23T13:08:40Z,,,
10862,b'Fixed confusing order of args in generate() docstring',2021-03-23T08:31:42Z,2021-03-23T17:48:22Z,,,
10861,b'[trainer] Fixes Typo in Predict Method of Trainer',2021-03-23T04:57:22Z,2021-03-23T12:15:28Z,,,
10860,"b""The exact English pretraining data and Chinese pretraining data that are exact same to the BERT paper's pretraining data.""",2021-03-23T03:43:44Z,2021-04-23T01:12:03Z,,,
10859,b'[file_utils] import refactor',2021-03-23T02:30:05Z,2021-03-23T16:41:41Z,,,
10858,"b'If run trainer._maybe_log_save_evaluate() twice continuously, it will appear \xe2\x80\x9cZeroDivisionError: float division by zero\xe2\x80\x9d'",2021-03-23T01:57:52Z,2021-03-23T13:44:30Z,,,
10857,b'Make convert_to_onnx runable as script again',2021-03-22T21:30:53Z,2021-03-23T02:16:39Z,,,
10856,b'Use DataCollatorForSeq2Seq in run_summarization in all cases',2021-03-22T18:39:04Z,2021-03-22T19:05:40Z,,,
10855,b'm2m_100 finetuning not working (KeyError: none)',2021-03-22T16:54:23Z,2021-05-16T15:02:07Z,,KeyError,"KeyError: None"
10854,b'Run summarization always use data collator for seq2 seq',2021-03-22T14:49:50Z,2021-03-22T19:05:19Z,,,
10853,"b""Error building extension 'fused_adam'""",2021-03-22T12:59:45Z,2021-03-25T03:25:24Z,DeepSpeed,**RuntimeError,"**RuntimeError: Error building extension 'fused_adam'**"
10852,b'Longformer training : CUDA error: device-side assert triggered',2021-03-22T12:11:28Z,2021-04-29T15:07:12Z,,RuntimeError,"RuntimeError: CUDA error: device-side assert triggered"
10851,b'Small inconsistency in tokenization_utils for special tokens retrieval',2021-03-22T11:41:08Z,2021-04-30T15:02:11Z,,,
10850,b'How to train encoder decoder for explicit negation generation',2021-03-22T10:02:22Z,2021-03-29T08:26:10Z,,,
10849,b'Fix: typo in FINE_TUNE_XLSR_WAV2VEC2.md',2021-03-22T09:26:31Z,2021-03-22T11:58:59Z,,,
10848,b'GPT Neo',2021-03-22T08:38:41Z,2021-03-30T13:42:30Z,,,
10847,b'fix code quality issues',2021-03-22T08:03:56Z,2021-04-06T12:00:43Z,,,
10846,b'[Wav2Vec2] Small tab fix',2021-03-22T07:32:09Z,2021-03-22T07:32:21Z,,,
10845,b'Option to change loss function for fine tuning',2021-03-22T07:09:30Z,2021-03-22T09:17:17Z,,,
10844,b'Add GPT-Neo',2021-03-22T06:22:48Z,2021-04-28T05:49:00Z,New model,,
10843,b'Is there a `DataCollator` cat mask n-gram words for LM?',2021-03-22T05:17:59Z,2021-04-29T15:07:14Z,Migration,,
10842,b'How to fine-tune RAG on MS-MARCO dataset?',2021-03-22T03:04:11Z,2021-03-22T13:19:36Z,,,
10841,b'issue of run_mlm.py ',2021-03-22T02:48:12Z,2021-04-29T15:07:15Z,,IndexError,"IndexError: index out of bounds"
10840,"b""why My Albert pretrain loss can't decrease?""",2021-03-22T01:55:44Z,2021-04-29T15:07:16Z,Migration,,
10839,b'Fix on_step_begin and on_step_end Callback Sequencing',2021-03-21T22:47:22Z,2021-03-22T13:15:39Z,,,
10838,b'Can\xe2\x80\x99t download the pre-trained pegasus-large model',2021-03-21T21:33:50Z,2021-04-29T15:07:16Z,,OSError,"OSError: Can't load weights for 'google/pegasus-large'. Make sure that:"
10837,b'pegasus-xsum summarized a story of Eiffel Tower into one on the World Trade Center',2021-03-21T21:27:39Z,2021-04-29T15:07:17Z,,,
10836,b'Generating text with MBart Large 50 on GPU with Tensorflow is significantly slower than with Pytorch',2021-03-21T20:04:48Z,2021-04-29T15:07:18Z,,,
10835,b'Issues finetuning MBART 50 many to many',2021-03-21T18:05:17Z,2021-06-28T15:07:31Z,,,
10834,b'Local Attention for GPT2',2021-03-21T16:35:54Z,2021-03-30T17:32:43Z,,,
10833,b'weird large memory usage of mbert model ',2021-03-21T16:10:15Z,2021-04-29T15:07:19Z,,,
10832,"b'run_mlm.py: CUDA error: device-side assert triggered, THCTensorIndex'",2021-03-21T15:44:53Z,2021-03-23T08:06:27Z,,RuntimeError,"RuntimeError: CUDA error: device-side assert triggered"
10831,"b""Encoder Decoder Model didn't return a reasonable result""",2021-03-21T09:01:21Z,2021-04-28T15:02:11Z,,,
10830,b'getting nans with t5-large + fix',2021-03-21T08:52:41Z,2021-07-23T15:03:09Z,,,
10829,b'[Wav2Vec2] Small improvements for wav2vec2 info script',2021-03-21T08:41:16Z,2021-03-21T08:41:44Z,,,
10828,b'[wav2vec sprint doc] add doc for Local machine',2021-03-21T07:40:39Z,2021-03-21T07:55:34Z,,,
10827,b'Log continuously models with wandb',2021-03-21T00:37:32Z,2021-06-28T15:07:32Z,,,
10826,b'feat(wandb): logging and configuration improvements',2021-03-20T21:12:09Z,2021-03-22T14:45:17Z,,,
10825,b'ReformerEmbedding unclear behavior',2021-03-20T18:51:58Z,2021-04-30T15:02:12Z,,,
10824,"b'Running ""convert_graph_to_onnx.py"" doesn\'t work.'",2021-03-20T17:47:09Z,2021-03-20T17:50:32Z,,ModuleNotFoundError,"ModuleNotFoundError: No module named '__main__.file_utils'; '__main__' is not a package"
10823,b'Modify the Trainer class to handle simultaneous execution of Ray Tune and Weights & Biases',2021-03-20T17:27:09Z,2021-03-22T18:04:52Z,,,
10822,b'Correct AutoConfig call docstrings',2021-03-20T16:52:48Z,2021-03-22T13:12:45Z,,,
10821,b'checkpoint breaks with deepspeed ',2021-03-20T14:57:47Z,2021-04-19T08:40:16Z,DeepSpeed,"RuntimeError, subprocess.CalledProcessError","RuntimeError: The size of tensor a (302612288) must match the size of tensor b (129296512) at non-singleton dimension 0subprocess.CalledProcessError: Command '['/users/dara/anaconda3/envs/deepspeed/bin/python', '-u', 'run_seq2seq.py', '--local_rank=0', 'configs/test.json']' returned non-zero exit status 1."
10820,b'JSONLINES support on examples/seq2seq/run_translation.py ',2021-03-20T09:05:07Z,2021-03-22T02:34:18Z,,FileNotFoundError,"FileNotFoundError: Couldn't find file at https://raw.githubusercontent.com/huggingface/datasets/master/datasets/jsonl/jsonl.py"
10819,b'mt5 getting nans with fp16',2021-03-20T06:44:36Z,2021-06-07T15:18:43Z,,,
10818,b'Bump jinja2 from 2.11.2 to 2.11.3 in /examples/research_projects/lxmert',2021-03-20T05:36:07Z,2021-03-22T12:54:51Z,dependencies,,
10817,b'[vulnerability] in example deps fix',2021-03-20T04:44:21Z,2021-03-22T13:05:24Z,,,
10816,b'[trainer] figuring out why eval with `--fp16_full_eval` is 25% slower',2021-03-20T04:30:07Z,,"Good First Issue, Good Second Issue",,
10815,b'[trainer] fix nan in full-fp16 label_smoothing eval',2021-03-20T04:17:07Z,2021-03-23T02:23:24Z,,,
10814,b'[makefile] autogenerate target',2021-03-19T20:20:13Z,2021-03-22T13:14:22Z,,,
10813,b'Example code for ReformerForMaskedLM',2021-03-19T19:28:44Z,2021-04-28T15:02:13Z,,```AssertionError,"```AssertionError: If you want to use `ReformerForMaskedLM` make sure `config.is_decoder=False` for bi-directional self-attention.```"
10812,b'Domain adaptation ',2021-03-19T17:13:22Z,2021-03-22T12:24:09Z,,,
10811,b'Add transformers id to hub requests',2021-03-19T14:49:23Z,2021-03-19T15:26:32Z,,,
10810,b'handle_impossible_answer not working in the question answering pipeline for ROBERTa model',2021-03-19T14:42:59Z,2021-03-23T13:08:40Z,,,
10809,b'[Flax] Add general conversion script',2021-03-19T13:25:34Z,2021-03-30T09:13:59Z,Flax,,
10808,b'wav2vec doc tweaks',2021-03-19T12:27:29Z,2021-03-19T16:48:54Z,,,
10807,"b'I am finetuning mBART for summarization using finetune_trainer.py on custom dataset, but I keep getting this error. '",2021-03-19T09:59:48Z,2021-04-26T15:02:04Z,,,
10806,b'[XLSR-Wav2Vec2 Info doc] Add a couple of lines',2021-03-19T09:47:22Z,2021-03-19T09:52:54Z,,,
10805,b'ONNX export outputs many warnings',2021-03-19T09:26:42Z,2021-04-26T15:02:05Z,,,
10804,b'Initializing ddp is extremely slow when finetuning RAG',2021-03-19T06:20:36Z,2021-04-26T15:02:06Z,,,
10803,b'How much vRAM should I have for fine tuning DeBERTa v2 xxlarge?',2021-03-19T04:38:33Z,2021-03-28T14:47:27Z,,,
10802,b'addressing vulnerability report in research project deps',2021-03-19T01:10:28Z,2021-03-19T02:02:11Z,,,
10801,b'Sort init import',2021-03-19T00:24:25Z,2021-03-19T20:17:13Z,,,
10800,"b'How to get a probability for the result of t5_tokenizer.decode(output,...)?'",2021-03-18T23:04:46Z,2021-03-19T14:31:37Z,,,
10799,b'Expand a bit the presentation of examples',2021-03-18T21:03:18Z,2021-03-19T14:06:10Z,,,
10798,b'Truncated words on GPT-2 output',2021-03-18T19:28:43Z,2021-03-19T02:34:00Z,,,
10797,b'Pretrained XLNetTokenizer not returning tokenizer',2021-03-18T17:50:39Z,2021-03-19T13:43:56Z,,,
10796,b'[Example] Fix a NaN bug in the flax mlm example',2021-03-18T17:23:45Z,2021-04-24T21:41:34Z,,,
10795,b'Fix distributed evaluation',2021-03-18T17:05:34Z,2021-03-18T17:12:04Z,,,
10794,b'Add new community notebook - wav2vec2 with GPT',2021-03-18T17:02:26Z,2021-03-21T07:59:53Z,,,
10793,b'[doc] no more bucket',2021-03-18T16:36:24Z,2021-04-01T18:25:47Z,,,
10792,b'[Example] Updating Question Answering examples for Predict Stage',2021-03-18T15:23:12Z,2021-03-19T13:42:17Z,,,
10791,b'run_summarization script breaks with label_smoothing_factor and pad_to_max_length true',2021-03-18T13:42:43Z,2021-03-22T19:05:40Z,,RuntimeError,"RuntimeError: The expanded size of the tensor (128) must match the existing size (64) at non-singleton dimension 1.  Target sizes: [4, 128, 1].  Tensor sizes: [4, 64, 1]"
10790,"b""HerbertTokenizer doesn't work on version 3.5.1""",2021-03-18T11:20:34Z,2021-03-18T12:07:41Z,,`OSError,"`OSError: Can't load tokenizer for 'allegro/herbert-base-cased'. Make sure that:"
10789,b'[Deepspeed ZeRO-3] Broken model save on fresh Transformers branch',2021-03-18T11:18:10Z,2021-04-25T15:02:45Z,DeepSpeed,`ValueError,"`ValueError: [deepspeed] failed to resume from checkpoint ./templates/siamese-t5-small-v1_1-template`"
10788,"b""TypeError: __init__() got an unexpected keyword argument 'filepath' when using RAG model""",2021-03-18T08:14:03Z,2021-04-25T15:02:46Z,,TypeError,"TypeError: __init__() got an unexpected keyword argument 'filepath'"
10787,b'Can DeepSpeed ZeRO-3 be applied for training? ',2021-03-18T08:12:28Z,2021-10-19T15:03:32Z,,,
10786,b'Add XLSR-Wav2Vec2 Fine-Tuning README.md',2021-03-18T08:01:06Z,2021-03-18T21:22:43Z,,,
10785,b'Typo in M2M100 model page',2021-03-18T03:57:29Z,2021-03-18T17:01:49Z,,,
10784,b'How to interpret fine-tuned model results and use model',2021-03-18T02:48:27Z,2021-03-18T16:48:05Z,,,
10783,b'Fix bug in input check for LengthGroupSampler',2021-03-18T02:18:31Z,2021-03-18T14:25:57Z,,,
10782,b'add dockerfile for zero optimzier',2021-03-18T01:24:48Z,2021-03-18T01:25:13Z,,,
10781,b'Add support for detecting intel-tensorflow version',2021-03-17T20:49:44Z,2021-03-18T00:25:47Z,,,
10780,b'Improve the speed of adding tokens from added_tokens.json',2021-03-17T20:24:14Z,2021-04-01T12:56:12Z,,,
10779,b'EncoderDecoderModel with different model dimensions',2021-03-17T18:40:33Z,,WIP,,
10778,b'Smmp batch not divisible by microbatches fix',2021-03-17T18:24:13Z,2021-03-17T23:18:12Z,,,
10777,b'[trainer] make failure to find a resume checkpoint fatal + tests',2021-03-17T17:48:00Z,2021-03-17T18:16:37Z,,,
10776,b'[examples] document resuming ',2021-03-17T17:00:47Z,2021-03-17T19:48:36Z,,,
10775,b'Check copies blackify',2021-03-17T16:59:09Z,2021-03-17T22:11:21Z,,,
10774,"b""torch.nn.modules.module.ModuleAttributeError: 'AlbertEmbeddings' object has no attribute 'bias'""",2021-03-17T16:02:41Z,2021-04-25T15:02:49Z,,"""layer_norm_eps"", torch.nn.modules.module.ModuleAttributeError","""layer_norm_eps"": 1e-12,torch.nn.modules.module.ModuleAttributeError: 'AlbertEmbeddings' object has no attribute 'bias'"
10773,b'Wav2Vec2 - fix flaky test',2021-03-17T13:45:00Z,2021-03-17T15:10:17Z,,,
10772,b'Differences between S2T and Wav2Vec2',2021-03-17T12:57:15Z,2021-04-25T15:02:50Z,,,
10771,b'Fix ProphetNet Flaky Test',2021-03-17T12:18:47Z,2021-03-17T13:15:14Z,,,
10770,b'TAPAS for Question Generation',2021-03-17T10:37:06Z,2021-05-05T15:02:15Z,,,
10769,b'[Generate] Add save mode logits processor to remove nans and infs if necessary',2021-03-17T10:24:20Z,2021-03-22T22:00:05Z,,,
10768,b'Bug in multi-gpu training setting max_iters ',2021-03-17T09:28:37Z,2021-04-28T15:02:14Z,,,
10767,b'add run_common_voice script',2021-03-17T07:46:46Z,2021-03-18T11:51:16Z,,,
10766,b'auto model encodings for a text snippet returns different floating values across different batch sizes',2021-03-17T07:36:42Z,2021-04-25T15:02:52Z,,,
10765,b'Cannot import name swish from transformers.activations',2021-03-17T06:24:03Z,2021-03-18T06:14:19Z,,`ImportError,"`ImportError: cannot import name 'swish' from 'transformers.activations' (/Users/array/opt/miniconda3/lib/python3.7/site-packages/transformers/activations.py)`"
10764,b'TokenClassificationPipeline: top-k predictions',2021-03-17T06:19:00Z,2021-04-25T15:02:54Z,,,
10763,b'TokenClassificationPipeline: ignoring subwords',2021-03-17T05:50:21Z,2021-04-27T15:01:55Z,,,
10762,b'[DeepSpeed] simplify init',2021-03-17T05:28:48Z,2021-03-17T17:21:03Z,DeepSpeed,,
10761,b'[doc] [testing] extend the pytest -k section with more examples',2021-03-17T05:10:57Z,2021-03-17T13:23:38Z,,,
10760,b'[DeepSpeed] improve checkpoint loading code plus tests',2021-03-17T04:25:50Z,2021-03-17T17:22:58Z,DeepSpeed,,
10759,b'AlbertForMaskedLM always has bad results',2021-03-17T03:58:16Z,2021-04-26T15:02:08Z,,,
10758,b'Even slower when using multiple gpus with sharded_ddp',2021-03-17T02:44:27Z,2021-04-26T15:02:09Z,,,
10757,b'BERT for Regression predicts constant',2021-03-17T01:53:57Z,2021-03-19T16:31:40Z,,,
10756,"b'Google Colab TypeError: expected str, bytes or os.PathLike object, not NoneType'",2021-03-16T22:14:00Z,2021-03-17T18:03:55Z,,TypeError,"TypeError: expected str, bytes or os.PathLike object, not NoneType"
10755,b'Online decoding for ASR',2021-03-16T21:40:27Z,2021-03-17T00:20:04Z,,,
10754,"b'run_clm.py gpt-2 training example in documentation runs out of memory on a 32gb v100, should be verified and/or modified '",2021-03-16T20:58:14Z,2021-04-26T15:02:10Z,,`RuntimeError,"`RuntimeError: CUDA out of memory. Tried to allocate 512.00 MiB (GPU 0; 31.72 GiB total capacity; 30.32 GiB already allocated; 187.88 MiB free; 30.38 GiB reserved in total by PyTorch)`"
10753,b'[DeepSpeed] ZeRO Stage 3',2021-03-16T20:51:56Z,2021-04-08T16:53:01Z,DeepSpeed,,
10752,b'Patches full import failure when sentencepiece is not installed',2021-03-16T19:53:34Z,2021-03-16T19:58:20Z,,,
10751,b'Tensorflow Keras model.loads_weights() breaks on TFElectraModel trained with v4.3.0',2021-03-16T19:49:34Z,2021-04-28T15:02:15Z,,ValueError,"ValueError: Cannot assign to variable tf_electra_model_1/electra/embeddings/token_type_embeddings/embeddings:0 due to variable shape (2, 128) and value shape (512, 128) are incompatible"
10750,b'Patches the full import failure and adds a test',2021-03-16T19:13:57Z,2021-03-16T19:37:52Z,,,
10749,b'bug in new version 4.4.0 sentencepiece is not available',2021-03-16T16:58:06Z,2021-03-16T19:37:52Z,,"`ModuleNotFoundError, `AttributeError","`ModuleNotFoundError: No module named 'sentencepiece'``AttributeError: module transformers.models.ibert has no attribute IBertLayer`"
10748,b'Fix URLs from #10744',2021-03-16T15:27:23Z,2021-03-16T15:31:29Z,,,
10747,"b'Issues with MODEL_FOR_MASKED_LM_MAPPING.keys(), and transformer.utils.check_min_version()'",2021-03-16T15:18:53Z,2021-03-17T08:57:52Z,,"ImportError, AttributeError","ImportError: cannot import name 'check_min_version' from 'transformers.utils' (/PATH/TO/site-packages/transformers/utils/__init__.py)AttributeError: 'NoneType' object has no attribute 'keys'"
10746,b'Add DistributedSamplerWithLoop',2021-03-16T14:42:44Z,2021-03-16T15:22:39Z,,,
10745,b'fix M2M100 example',2021-03-16T14:39:51Z,2021-03-16T14:50:01Z,,,
10744,b'Remove old links to CDN',2021-03-16T14:39:41Z,2021-03-16T14:48:54Z,,,
10743,b'Fix DeBERTa + Conversational pipeline slow tests',2021-03-16T14:33:59Z,2021-03-16T15:18:20Z,,,
10742,b'DialoGPT- cannot increase number of conversation turns',2021-03-16T13:54:10Z,2021-04-08T17:44:20Z,,,
10741,b'Fix S2T example',2021-03-16T12:38:34Z,2021-03-16T12:55:07Z,,,
10740,b'BigBird',2021-03-16T10:56:24Z,2021-04-24T15:01:56Z,,KeyError,"KeyError: 'big_bird'"
10739,b'Tokenizer becomes very slow after adding new tokens',2021-03-16T10:48:50Z,2021-05-15T15:02:12Z,,,
10738,b'load wav2vec model from local path',2021-03-16T10:41:36Z,2021-04-24T15:01:57Z,,,
10737,b'`group_texts` duplicates special tokens',2021-03-16T09:19:24Z,2021-03-18T11:30:27Z,,,
10736,b'Position ids in RoBERTa',2021-03-16T07:01:13Z,2021-04-24T15:01:57Z,,,
10735,b'Release utils',2021-03-16T02:27:20Z,2021-03-16T12:41:47Z,,,
10734,b'[examples/seq2seq/README.md] fix t5 examples',2021-03-16T01:44:22Z,2021-03-18T16:55:39Z,,,
10733,"b'[examples run_summarization.py] t5 worse score w/ --source_prefix ""summarize: "" than w/o'",2021-03-16T01:35:05Z,2021-03-16T22:55:40Z,,,
10732,b'run_clm.py does not work with any other block_size other than 1024',2021-03-16T00:58:31Z,2021-04-24T15:01:58Z,,RuntimeError,"RuntimeError: CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`"
10731,b'Fix log message for training from checkpoint with global step',2021-03-16T00:25:10Z,2021-04-25T15:02:55Z,,,
10730,b'Stacked Roberta run_mlm.py',2021-03-15T22:36:15Z,2021-03-21T15:55:11Z,,,
10729,b'Multi-node training with the latest transformers/examples code',2021-03-15T21:36:33Z,2021-03-16T19:24:37Z,,,
10728,b'[Issue template] need to update/extend who to tag',2021-03-15T21:11:23Z,2021-03-17T18:33:14Z,,,
10727,b'Rename zero-shot pipeline multi_class argument',2021-03-15T20:37:15Z,2021-03-15T22:02:47Z,,,
10726,b'broken models on the hub',2021-03-15T19:46:28Z,2021-04-25T15:02:57Z,,TypeError,"TypeError: expected str, bytes or os.PathLike object, not NoneType"
10725,b'Flax testing should not run the full torch test suite',2021-03-15T19:16:44Z,2021-03-16T05:05:37Z,Flax,,
10724,b'Add minimum version check in examples',2021-03-15T18:19:53Z,2021-03-15T23:29:55Z,,,
10723,b'Train tokenizer for Deberta',2021-03-15T16:58:02Z,2021-04-25T15:02:58Z,,,
10722,b'iterative evaluation in Trainer to save memory',2021-03-15T15:59:33Z,2021-03-16T12:54:36Z,,MemoryError,"MemoryError: Unable to allocate 2.77 TiB for an array with shape (200206, 77, 49408) and data type float32"
10721,"b'Run Time Error: RuntimeError: Expected hidden[0] size (2, 1, 512), got [2, 128, 512] - Seq2Seq Model with PreTrained BERT Model'",2021-03-15T15:36:04Z,2021-06-14T15:07:25Z,,RuntimeError,"RuntimeError: Expected hidden[0] size (2, 1, 512), got [2, 128, 512]"
10720,b'Cannot use custom roberta tokenizer with run_mlm_wwm.py',2021-03-15T15:28:25Z,2021-04-24T15:01:59Z,,Exception,"Exception: data did not match any variant of untagged enum ModelWrapper at line 1 column 1138661"
10719,b'[WIP] Extend LayoutLMTokenizer to handle bounding boxes',2021-03-15T14:18:45Z,2021-05-15T15:02:13Z,,,
10718,b'Fix backward compatibility with EvaluationStrategy',2021-03-15T13:58:01Z,2021-03-15T14:20:38Z,,,
10717,b'How can I get the exact position von answers?',2021-03-15T12:52:07Z,2021-03-15T13:16:10Z,,,
10716,b'Language model for wav2vec2.0 decoding',2021-03-15T12:51:47Z,2021-03-16T16:56:46Z,,,
10715,b'Pegasus-Large Question',2021-03-15T09:48:33Z,2021-04-22T15:01:59Z,,,
10714,b'[Wav2Vec2] Make wav2vec2 test deterministic',2021-03-15T08:03:36Z,2021-03-15T13:50:05Z,,,
10713,b'Is any possible with pipeline for using local model?',2021-03-15T07:23:13Z,2021-03-15T08:04:26Z,,,
10712,b'Update modeling_tf_pytorch_utils.py',2021-03-15T07:12:52Z,2021-05-11T04:46:14Z,,,
10711,"b""'Trainer' object has no attribute 'log_metrics'""",2021-03-15T03:12:28Z,2021-03-15T12:08:20Z,,AttributeError,"AttributeError: 'Trainer' object has no attribute 'log_metrics'"
10710,b'independent training / eval with local files',2021-03-14T23:11:11Z,2021-03-15T23:35:27Z,,,
10709,b'Wrong link to super class',2021-03-14T15:43:57Z,2021-03-15T11:39:10Z,,,
10708,b'ValueError: Unsupported value type BatchEncoding returned by IteratorSpec._serialize',2021-03-14T15:02:19Z,2021-03-14T15:05:30Z,,ValueError,"ValueError: Unsupported value type BatchEncoding returned by IteratorSpec._serialize"
10707,b'Inheriting from BartForConditionalGeneration into a new class - weight  not initializing',2021-03-14T13:37:39Z,2021-03-14T14:28:07Z,,,
10706,b'Trainer crashes when saving checkpoint',2021-03-14T11:35:33Z,2021-03-15T14:24:43Z,,"TypeError, AttributeError","TypeError: cannot pickle '_thread.lock' objectAttributeError: 'Accuracy' object has no attribute 'startswith'"
10705,b'Please provide format of the dataset to finetuning wav2vec using run_asr.py script',2021-03-14T09:12:11Z,2021-04-14T17:34:55Z,,,
10704,b'How to generate texts in huggingface in a batch way?',2021-03-14T07:59:57Z,2021-04-22T15:02:00Z,,,
10703,b'DebertaTokenizer Rework closes #10258',2021-03-14T06:05:35Z,2021-04-01T17:53:54Z,,,
10702,b'Performance Issue in doing inferencing hugging face models',2021-03-14T05:35:55Z,2021-03-15T12:47:09Z,,,
10701,"b'Seq2Seq Model with PreTrained BERT Model is Throwing Error During Training: ValueError(""You cannot specify both input_ids and inputs_embeds at the same time"")'",2021-03-14T04:43:44Z,2021-03-15T14:20:52Z,,AttributeError,"AttributeError: 'Field' object has no attribute 'size'"
10700,"b'Trying to implement ""nielsr/luke-large"" gives ""KeyError: \'luke\'""'",2021-03-13T16:04:49Z,2021-03-14T10:31:22Z,,KeyError,"KeyError: 'luke'"
10699,b'TF BART models - Add `cross_attentions` to model output and fix cross-attention head masking',2021-03-13T11:48:19Z,2021-04-26T12:16:21Z,,,
10698,b'Add `cross_attentions` to the output of TensorFlow encoder-decoder models',2021-03-13T09:13:32Z,2021-04-22T15:02:02Z,,,
10697,b'Fix Wav2Vec2 classes imports',2021-03-13T07:11:48Z,2021-03-13T17:28:15Z,,,
10696,"b""OSerror, when loading 'wav2vec2-large-xlsr-53' Model of Wav2vec2""",2021-03-13T04:23:04Z,2021-03-15T09:08:43Z,,OSError,"OSError: "
10695,b'Merge from huggingface/transformer master ',2021-03-13T01:27:18Z,2021-03-13T01:27:37Z,,,
10694,b'[Wav2Vec2] Fix documentation inaccuracy',2021-03-13T00:59:02Z,2021-03-15T17:11:17Z,,,
10693,b'mBART Large-50 MMT provides incorrect translation when the source and target language are the same',2021-03-13T00:45:50Z,2021-04-15T04:38:56Z,,,
10692,b'Add RemBERT model code to huggingface',2021-03-12T23:50:29Z,2021-07-24T15:31:43Z,WIP,,
10691,b'Naming convention for (pytorch) checkpoints broken?',2021-03-12T19:12:51Z,2021-03-16T19:59:28Z,,,
10690,b'enable loading Mbart50Tokenizer with AutoTokenizer ',2021-03-12T17:40:11Z,2021-03-15T10:50:37Z,,,
10689,b'Fix mixed precision for TFGPT2LMHeadModel',2021-03-12T14:53:10Z,2021-07-04T15:02:20Z,,,
10688,b'Adding required flags to non-default arguments in hf_argparser',2021-03-12T14:43:00Z,2021-03-15T13:27:56Z,,,
10687,b'Multiple fixes in SageMakerTrainer',2021-03-12T14:05:27Z,2021-03-15T13:28:15Z,,,
10686,b'fix backend tokenizer args override: key mismatch',2021-03-12T13:41:54Z,2021-03-19T02:13:45Z,,,
10685,b'Distributed barrier before loading model',2021-03-12T13:35:55Z,2021-03-15T12:28:15Z,,,
10684,b'Question answering: a couple of things after fine-tuning a model',2021-03-12T12:42:43Z,2021-03-15T13:32:41Z,,,
10683,b'Add util for deleting cached models programmatically ',2021-03-12T12:35:40Z,2021-04-23T15:02:11Z,,,
10682,b'Token Classification: How to tokenize and align labels with overflow and stride?',2021-03-12T12:10:03Z,2021-03-12T23:09:45Z,,,
10681,b'Tests run on Docker',2021-03-12T11:51:53Z,2021-03-15T21:28:01Z,,,
10680,b'[TFMarian] Slow integration tests are failing',2021-03-12T11:32:06Z,2021-06-13T15:02:04Z,,,
10679,b'[Tests] RAG',2021-03-12T11:23:09Z,2021-03-15T07:07:12Z,,,
10678,"b'T5-base out of memory on one 2080 GPU with batchsize 4, sequence length 100'",2021-03-12T06:45:12Z,2021-03-12T12:26:01Z,,,
10677,"b""hf_argparser doesn't set the required flag on non-defaulted enums""",2021-03-12T02:47:41Z,2021-03-15T13:27:56Z,,,
10676,b'Improve the speed of adding tokens from added_tokens.json',2021-03-11T22:47:40Z,2021-04-01T12:56:12Z,,,
10675,b'Reformer _pad_to_mult_of_chunk_length seems incorrect',2021-03-11T22:32:41Z,2021-05-15T15:02:14Z,,,
10674,b'[trainer] loss = NaN with label_smoothing and full-fp16 eval',2021-03-11T20:43:13Z,2021-03-23T02:23:24Z,"Good First Issue, Good Second Issue",,
10673,b'Add auto_wrap option in fairscale integration',2021-03-11T20:14:22Z,2021-03-12T12:50:20Z,,,
10672,b'fix typing error for HfArgumentParser for Optional[bool]',2021-03-11T18:48:31Z,2021-03-11T22:42:54Z,,,
10671,b'Fixes Pegasus tokenization tests',2021-03-11T18:35:16Z,2021-03-11T18:35:50Z,,,
10670,b'Fix integration slow tests',2021-03-11T18:33:25Z,2021-03-11T18:43:53Z,,,
10669,b'MT5 integration test: adjust loss difference',2021-03-11T18:15:34Z,2021-03-12T06:09:46Z,,,
10668,b'Add DeBERTa to MODEL_FOR_PRETRAINING_MAPPING',2021-03-11T17:33:52Z,2021-03-11T18:56:47Z,,,
10667,b'[S2T] fix example in docs',2021-03-11T16:50:46Z,2021-03-11T17:13:38Z,,,
10666,b'training LayouttLM 1 epoch in distributed more results in error',2021-03-11T16:32:28Z,2021-03-15T12:28:15Z,,,
10665,b'W2v2 test require torch',2021-03-11T16:31:18Z,2021-03-11T17:56:13Z,,,
10664,b'TensorFlow tests: having from_pt set to True requires torch to be installed.',2021-03-11T16:23:21Z,2021-03-12T11:16:41Z,,,
10663,b'Onnx fix test',2021-03-11T16:21:57Z,2021-03-11T18:38:30Z,,,
10662,b'Specify minimum version for sacrebleu',2021-03-11T16:16:00Z,2021-03-11T18:45:07Z,,,
10661,b'Fix Marian/TFMarian tokenization tests',2021-03-11T15:29:24Z,2021-03-11T17:58:15Z,,,
10660,b'fix: #10628 expanduser path in TrainingArguments',2021-03-11T15:21:33Z,2021-03-12T14:18:19Z,,,
10659,b'How to use deepspeed finetune RAG?',2021-03-11T15:14:08Z,2021-03-11T17:25:02Z,,,
10658,b'GPT2DoubleHeadsModel made parallelizable',2021-03-11T14:41:49Z,2021-03-15T13:10:44Z,,,
10657,b'S2S + M2M100 should be available in tokenization_auto',2021-03-11T14:35:25Z,2021-03-11T14:53:36Z,,,
10656,b'Fix broken link',2021-03-11T14:18:48Z,2021-03-11T19:29:03Z,,,
10655,"b""MarianMT - tokenizer.supported_language_codes -> 'NoneType' object has no attribute 'supported_language_codes'""",2021-03-11T14:00:39Z,2021-03-11T18:16:20Z,,AttributeError,"AttributeError: 'NoneType' object has no attribute 'supported_language_codes'"
10654,b'Allow private model hosting and resolution ',2021-03-11T13:35:40Z,2021-04-22T15:02:04Z,,,
10653,b'Fix Longformer tokenizer filename',2021-03-11T13:33:23Z,2021-03-11T13:34:09Z,,,
10652,b'Infernal tokenizer loading trained ',2021-03-11T13:05:15Z,2021-04-22T15:02:05Z,,,
10651,b'added support for exporting of T5 models to onnx with past_key_values.',2021-03-11T12:53:34Z,2021-04-23T16:14:20Z,,RuntimeError,"RuntimeError: output with shape [5, 8, 1, 2] doesn't match the broadcast shape [5, 8, 2, 2]"
10650,"b'DistilBertTokenizerFast ignores ""do_lower_case=False"" parameter'",2021-03-11T12:39:43Z,2021-03-11T13:45:38Z,,,
10649,b'[Question] How do I prevent a lack of VRAM halfway through training a (Pegasus) model?',2021-03-11T12:32:52Z,2021-03-11T12:41:28Z,,,
10648,b'[XLSR-Wav2Vec2] Add multi-lingual Wav2Vec2 models',2021-03-11T12:17:33Z,2021-03-11T14:44:18Z,,,
10647,b'Update README.md',2021-03-11T11:44:34Z,2021-03-11T13:58:05Z,,,
10646,"b'seq2seq BertGeneration model failed ""ValueError: You have to specify either input_ids or inputs_embeds""'",2021-03-11T11:32:52Z,2021-03-25T06:23:12Z,,ValueError,"ValueError: You have to specify either input_ids or inputs_embeds"
10645,b'export T5 model to onnx with past_key_values',2021-03-11T10:24:44Z,2021-04-22T15:02:06Z,,RuntimeError,"RuntimeError: output with shape [5, 8, 1, 2] doesn't match the broadcast shape [5, 8, 2, 2]"
10644,b'Numeracy',2021-03-11T10:17:48Z,2021-03-11T10:18:56Z,,,
10643,b'Space token cannot be add when is_split_into_words = True',2021-03-11T09:14:48Z,2021-04-22T15:02:07Z,,,
10642,"b""Unable To Load Pretrained Longformer Models' Tokenizers""",2021-03-11T07:42:06Z,2021-03-11T13:34:09Z,,,
10641,b'Unable to reduce time in summarization!',2021-03-11T07:33:09Z,2021-03-19T14:48:45Z,,,
10640,b'Nonetype when using deepspeed ',2021-03-11T06:49:59Z,2021-04-22T15:02:08Z,DeepSpeed,"RuntimeError, TypeError","RuntimeError: [enforce fail at CPUAllocator.cpp:67] . DefaultCPUAllocator: can't allocate memory: you tried to allocate 2329531392 bytes. Error code 12 (Cannot allocate memory)TypeError: 'NoneType' object is not callable"
10639,b'Support Quantization Aware Fine-tuning in all models (pytorch)',2021-03-11T06:03:47Z,2021-04-22T15:02:09Z,,,
10638,b'Fix beam search when using logits processors',2021-03-11T03:11:23Z,,WIP,,
10637,b'Remove special treatment for custom vocab files',2021-03-10T20:49:18Z,2021-03-11T16:11:57Z,,,
10636,b'Layout lm tf 2',2021-03-10T19:50:58Z,2021-03-25T16:32:39Z,,,
10635,b'Document Trainer limitation on custom models',2021-03-10T19:37:31Z,2021-03-10T19:58:22Z,,,
10634,b'Issues with Multi-GPU',2021-03-10T19:25:01Z,2021-03-11T02:30:28Z,,RuntimeError,"RuntimeError: Caught RuntimeError in replica 0 on device 0."
10633,b'Extend trainer logging for sm',2021-03-10T18:31:49Z,2021-03-10T19:53:49Z,,,
10632,b'Ensure metric results are JSON-serializable',2021-03-10T17:52:35Z,2021-03-11T14:00:24Z,,,
10631,b'Help using Speech2Text',2021-03-10T17:05:30Z,2021-03-11T12:12:11Z,,AssertionError,"AssertionError: choose a window size 400 that is [2, 2]"
10630,b'I get different results everytime I run run_squad.py',2021-03-10T14:50:23Z,2021-04-22T15:02:10Z,,,
10629,b'Using `label` in Trainer leads to TypeError',2021-03-10T10:54:48Z,2021-03-10T18:59:46Z,,TypeError,"TypeError: forward() got an unexpected keyword argument 'labels'"
10628,b'expanduser path in Trainer',2021-03-10T09:46:45Z,2021-03-12T14:18:23Z,,,
10627,b'considering `pad_to_multiple_of` for run_mlm.py',2021-03-10T09:44:40Z,2021-04-08T20:12:49Z,,,
10626,b'Average checkpoints',2021-03-10T08:52:01Z,2021-03-10T13:50:56Z,,,
10625,"b'Model ""deberta-v2--xxlarge-mnli"" doesn\'t work!!!'",2021-03-10T08:09:37Z,2021-03-10T12:40:26Z,,KeyError,"KeyError: 'deberta-v2'"
10624,b'Copy tokenizer files in each of their repo',2021-03-10T04:10:56Z,2021-03-10T16:26:23Z,,,
10623,b'Invalid pytorch_model.bin for TAPAS-large',2021-03-10T03:14:09Z,2021-03-10T17:43:27Z,,,
10622,b'wav2vec2: adding single-char tokens to tokenizer causes tokenization mistakes',2021-03-10T03:05:16Z,2021-05-03T15:19:12Z,,,
10621,b'Fixes an issue in `text-classification` where MNLI eval/test datasets are not being preprocessed.',2021-03-10T01:08:52Z,2021-03-10T03:13:46Z,,ValueError,"ValueError: You have to specify either input_ids or inputs_embeds"
10620,b'MNLI eval/test dataset is not being preprocessed in `run_glue.py`',2021-03-10T01:08:12Z,2021-03-11T19:20:49Z,,ValueError,"ValueError: You have to specify either input_ids or inputs_embeds"
10619,b'wav2vec2: `convert_tokens_to_string` contracts legitimately repeated characters',2021-03-10T01:05:25Z,2021-03-10T17:02:10Z,,,
10618,"b'Run_qa  crashes because of parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))  '",2021-03-09T22:59:26Z,2021-03-12T17:19:11Z,,TypeError,"TypeError: issubclass() arg 1 must be a clas "
10617,b'Request: Ignore Dataset transforms when iterating to the most recent checkpoint when resuming training',2021-03-09T22:22:27Z,2021-04-22T15:02:12Z,,,
10616,"b'changing "".view()"" to "".reshape()"" for pytorch'",2021-03-09T22:18:18Z,2021-04-22T15:02:13Z,,,
10615,b'Fix tests of TrainerCallback',2021-03-09T21:24:31Z,2021-03-09T21:25:32Z,,,
10614,b'Not able to convert T5 tf checkpoints',2021-03-09T19:50:45Z,2021-04-22T15:02:13Z,,ValueError,"ValueError: --model_type should be selected in the list [bert, gpt, gpt2, transfo_xl, xlnet, xlm]"
10613,b'OOM issues with save_pretrained models',2021-03-09T17:14:06Z,2021-04-22T15:02:14Z,,json.decoder.JSONDecodeError,"json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)"
10612,b'Implementing efficient self attention in T5',2021-03-09T16:29:30Z,,New model,,
10611,b'split seq2seq script into summarization & translation',2021-03-09T16:01:41Z,2021-03-15T13:11:43Z,,,
10610,b'Trigger add sm information',2021-03-09T15:38:34Z,2021-03-09T16:31:46Z,,,
10609,b'SortedDL for contiguous LM',2021-03-09T13:13:25Z,2021-03-26T15:52:04Z,,,
10608,b'Image feature extractor design',2021-03-09T12:56:18Z,2021-03-16T10:49:57Z,,Notes,"Notes: "
10607,"b""Can't load config for hosted model, works when downloaded""",2021-03-09T10:52:26Z,2021-03-11T09:20:58Z,,OSError,"OSError: Can't load config for 'EMBEDDIA/sloberta'. Make sure that:"
10606,b'[M2M100] remove final_logits_bias',2021-03-09T09:02:03Z,2021-03-10T04:22:32Z,,,
10605,b'Fix cross-attention head mask for Torch encoder-decoder models',2021-03-09T08:57:45Z,2021-04-23T16:58:07Z,,,
10604,b'fix flaky m2m100 test',2021-03-09T07:30:22Z,2021-03-09T14:35:08Z,,,
10603,b'AlbertForSequenceClassification random output',2021-03-09T06:37:09Z,2021-04-22T15:02:16Z,,,
10602,b'[examples template] added max_sample args and metrics changes',2021-03-09T05:22:31Z,2021-03-09T17:06:56Z,,,
10601,b'Speedup tf tests',2021-03-09T02:43:55Z,2021-03-09T02:44:08Z,,,
10600,"b'[docs] How to solve ""Title level inconsistent"" sphinx error'",2021-03-09T01:41:58Z,2021-03-09T04:16:34Z,,,
10599,b'Pass encoder outputs into GenerationMixin',2021-03-08T23:40:18Z,2021-03-12T16:13:12Z,,,
10598,b'Check layer types for Optimizer construction',2021-03-08T21:11:35Z,2021-03-08T21:40:12Z,,,
10597,b'No model card for  roberta-large-finetuned-wsc',2021-03-08T21:03:20Z,2021-03-10T18:47:50Z,,,
10596,b'Fairscale FSDP fix model save',2021-03-08T20:03:12Z,2021-03-09T19:42:07Z,fairscale,,
10595,b'Fix version control with anchors',2021-03-08T15:16:14Z,2021-03-08T15:19:23Z,,,
10594,b'[FeatureExtractorSavingUtils] Refactor PretrainedFeatureExtractor',2021-03-08T13:35:49Z,2021-03-09T09:16:59Z,,,
10593,b'Enable torch 1.8.0 on GPU CI',2021-03-08T12:11:28Z,2021-03-08T12:11:43Z,,,
10592,b'CUBLAS_STATUS_INTERNAL_ERROR at examples/question-answering/run_qa.py',2021-03-08T09:02:00Z,2021-04-14T15:25:25Z,,RuntimeError,"RuntimeError: Caught RuntimeError in replica 0 on device 0."
10591,b'Fix typo in docstring for pipeline',2021-03-08T09:01:58Z,2021-03-08T10:10:04Z,,,
10590,b'[M2M100] fix positional embeddings',2021-03-08T08:58:11Z,2021-03-08T10:36:19Z,,,
10589,b'Small question about BertForMaskedLM usage on TF model',2021-03-08T08:41:02Z,2021-04-22T15:02:16Z,,,
10588,"b""Can't reproduce xlm-roberta-large finetuned result on XNLI""",2021-03-08T08:39:29Z,2021-03-08T13:13:37Z,,,
10587,b'[WIP] Add Linformer',2021-03-08T08:15:22Z,,WIP,,
10586,b' from_pretrained: check that the pretrained model is for the right model architecture',2021-03-08T06:40:11Z,2021-03-18T16:51:43Z,,,
10585,b'[run_seq2seq] fix nltk lookup',2021-03-08T06:00:18Z,2021-03-08T06:09:59Z,,,
10584,b'[examples tests] various fixes',2021-03-08T05:53:57Z,2021-03-08T18:28:45Z,,,
10583,b'[trainer] fix double wrapping + test',2021-03-08T05:31:10Z,2021-03-08T15:15:56Z,,,
10582,b'wrong model used for BART Summarization example',2021-03-08T03:37:27Z,2021-03-08T10:15:06Z,,,
10581,b'wav2vec2: support datasets other than LibriSpeech',2021-03-08T01:01:33Z,2021-03-18T07:20:26Z,,,
10580,b'Issue when customizing loss in Trainer',2021-03-07T21:59:42Z,2021-03-12T08:24:06Z,,,
10579,b'request about deepspeed tutorial ',2021-03-07T19:51:17Z,2021-04-22T15:02:17Z,DeepSpeed,,
10578,b'Why HFArgumentParser.parse_dict(TrainerArguments) return tuple instead of dict?',2021-03-07T18:40:52Z,2021-03-08T12:42:03Z,,AttributeError,"AttributeError: 'tuple' object has no attribute 'seed'"
10577,b'seq2seq example with T5 does not run due to issue with loading tokernizer',2021-03-07T18:34:51Z,2021-03-07T18:55:21Z,,ValueError,"ValueError: Couldn't instantiate the backend tokenizer from one of: (1) a `tokenizers` library serialization file, (2) a slow tokenizer instance to convert or (3) an equivalent slow tokenizer class to instantiate and convert. You need to have sentencepiece installed to convert a slow tokenizer to a fast one."
10576,"b'Movement pruning for DistilGPT2 - pre_trained model, issue while using dynamic_quantization'",2021-03-07T18:29:29Z,2021-04-22T15:02:18Z,,```AttributeError,"```AttributeError: 'NoneType' object has no attribute 'is_quantized'```"
10575,b'bug in run_finetune',2021-03-07T18:05:39Z,2021-03-07T18:09:57Z,,ImportError,"ImportError: cannot import name 'is_offline_mode' from 'transformers.file_utils'"
10574,b'The dimension of Feature extraction',2021-03-07T14:42:24Z,2021-04-22T15:02:19Z,,,
10573,b'Update data_collator.py',2021-03-07T08:33:52Z,2021-03-08T12:56:54Z,,,
10572,"b'Import error for class Speech2TextProcessor, Speech2TextTransformerForConditionalGeneration'",2021-03-07T07:13:36Z,2021-03-11T12:15:21Z,,ImportError,"ImportError: cannot import name 'Speech2TextProcessor' from 'transformers' (unknown location)"
10571,b'Advice on creating/wrapping `PreTrainedModel` to be compatible with the codebase?',2021-03-07T03:45:34Z,2021-03-09T13:37:44Z,,,
10570,b'fix tf doc bug',2021-03-07T02:53:17Z,2021-03-08T03:31:50Z,,,
10569,b'offline mode for firewalled envs (part 2)',2021-03-07T00:12:37Z,2021-03-08T16:52:20Z,,,
10568,b'Ner label re alignment',2021-03-06T21:50:05Z,2021-05-07T06:11:16Z,,,
10567,b'XLSR-53',2021-03-06T21:15:31Z,2021-09-17T13:06:17Z,New model,,
10566,b'from_pretrained() - some model weights not initialized message',2021-03-06T19:46:11Z,2021-03-11T17:16:21Z,,,
10565,b'Mismatch between input and target batch_sizes while training FSMT model',2021-03-06T17:10:56Z,2021-04-22T15:02:20Z,,ValueError,"ValueError: Expected input batch_size (1440) to match target batch_size (1600)."
10564,b'[Causal Language Modeling] seems not as expected',2021-03-06T15:36:34Z,2021-03-12T07:02:13Z,,,
10563,"b'I have trained Bert on my own data which has been converted to IDs by using BertForMaskedLM, but when I use the model for the further fine-tuned, I found this error '",2021-03-06T15:16:50Z,2021-03-06T22:31:10Z,,,
10562,b'Stale bot updated',2021-03-06T06:49:58Z,2021-04-14T14:24:33Z,,,
10561,b'[examples tests on multigpu] resolving require_torch_non_multi_gpu_but_fix_me',2021-03-06T04:16:55Z,2021-03-08T19:11:40Z,,,
10560,b'[examples] run_glue_deebert.py distrbuted fails',2021-03-06T04:01:00Z,2021-03-08T17:01:55Z,,"RuntimeError, subprocess.CalledProcessError","RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. subprocess.CalledProcessError: Command '['/home/stas/anaconda3/envs/main-38/bin/python', '-u', 'examples/research_projects/deebert/run_glue_deebert.py', '--local_rank=1', '--model_type', 'roberta', '--model_name_or_path', 'roberta-base', '--task_name', 'MRPC', '--do_train', '--do_eval', '--do_lower_case', '--data_dir', './tests/fixtures/tests_samples/MRPC/', '--max_seq_length', '128', '--per_gpu_eval_batch_size=1', '--per_gpu_train_batch_size=8', '--learning_rate', '2e-4', '--num_train_epochs', '3', '--overwrite_output_dir', '--seed', '42', '--output_dir', './examples/deebert/saved_models/roberta-base/MRPC/two_stage', '--plot_data_dir', './examples/deebert/results/', '--save_steps', '0', '--overwrite_cache', '--eval_after_first_stage']' returned non-zero exit status 1."
10559,b'[website] installation doc blues',2021-03-06T03:31:17Z,2021-03-08T15:19:23Z,,,
10558,"b'Dear developer, does transformers have the support to translate Chinese text into English?'",2021-03-06T02:35:07Z,2021-04-22T15:02:21Z,,,
10557,b'[RAG] Expected RAG output after fine tuning',2021-03-06T00:07:39Z,2021-03-22T14:20:40Z,,,
10556,b'Layoutlm tf',2021-03-05T23:59:57Z,2021-03-09T19:14:07Z,,,
10555,b'Add new GLUE example with no Trainer.',2021-03-05T22:19:11Z,2021-03-10T14:29:19Z,,,
10554,b'Fixed dead link in Trainer documentation',2021-03-05T19:44:35Z,2021-03-05T19:56:37Z,,,
10553,b'Transformers upgrade',2021-03-05T17:37:49Z,2021-03-05T18:46:07Z,,,
10552,b'Handle padding in decoder_inputs_id when using generate',2021-03-05T17:10:55Z,2021-03-17T16:55:55Z,,,
10551,b'Added max_sample_ arguments',2021-03-05T16:38:57Z,2021-03-08T18:57:10Z,Examples,,
10550,b'How to get best model from hyperparameter search easily',2021-03-05T16:36:06Z,2021-04-22T15:02:22Z,,,
10549,b'Fix embeddings for PyTorch 1.8',2021-03-05T16:34:47Z,2021-03-05T21:18:48Z,,,
10548,b'Dead link to optuna.create_study under hyperparamter_search in Trainer',2021-03-05T16:01:56Z,2021-03-05T19:56:37Z,,,
10547,b'[Wav2Vec2 Example Script] Typo',2021-03-05T15:16:42Z,2021-03-05T15:17:12Z,,,
10546,b'Fix torch 1.8.0 segmentation fault',2021-03-05T14:46:16Z,2021-03-05T17:10:19Z,,,
10545,b'Fixing conversation test for torch 1.8',2021-03-05T13:36:50Z,2021-03-05T14:24:14Z,,,
10544,b'Handle padding in decoder_inputs_id when using generate',2021-03-05T12:50:05Z,2021-03-05T17:09:42Z,,,
10543,b'Similar issue like #1091 in Blenderbot',2021-03-05T12:04:45Z,2021-04-22T15:02:23Z,,,
10542,"b""OSError: Can't load weights for 'facebook/mbart-large-cc25' when using TFMBartModel""",2021-03-05T11:50:04Z,2021-04-22T15:02:24Z,,"HTTPError, OSError","HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/facebook/mbart-large-cc25/resolve/main/tf_model.h5OSError: Can't load weights for 'facebook/mbart-large-cc25'. Make sure that:"
10541,b'Facing Issue while running `run_tf_multiple_choice.py` from examples',2021-03-05T10:25:29Z,2021-03-06T05:30:42Z,,ValueError,"ValueError: `generator` yielded an element of shape (4, 1, 80) where an element of shape (None, None) was expected."
10540,b'\xf0\x9f\x90\x9b Bug in attention head mask for cross-attention module in encoder-decoder models',2021-03-05T10:00:33Z,2021-04-23T16:58:07Z,,,
10539,b'Wave2vec custom training tokenizer bug',2021-03-05T09:30:02Z,2021-03-13T09:55:54Z,,,
10538,b'Transfomer-xl  padding token',2021-03-05T09:20:03Z,2021-03-05T20:31:13Z,,,
10537,b'Fix example of custom Trainer to reflect signature of compute_loss',2021-03-05T09:19:27Z,2021-03-05T12:44:53Z,,,
10536,b'Enabling multilingual models for translation pipelines.',2021-03-05T08:39:20Z,2021-04-16T09:31:36Z,,,
10535,b'tensorflow model convert onnx',2021-03-05T07:27:32Z,2021-04-23T15:02:14Z,,,
10534,b'VisualBERT',2021-03-05T04:42:25Z,2021-06-02T12:43:08Z,WIP,,
10533,b'RAG with RAY workers keep repetitive copies  of knowledge base as  .nfs files until the process is done. ',2021-03-05T04:33:01Z,2021-03-31T11:02:05Z,,,
10532,b'Calling Inference API returns input text',2021-03-05T04:17:48Z,2021-04-23T15:02:15Z,,,
10531,b'Typo correction.',2021-03-05T03:58:52Z,2021-03-05T20:27:10Z,,,
10530,b'Test/Predict on summarization task',2021-03-05T03:41:56Z,2021-04-23T15:02:16Z,,,
10529,b'Typo in deberta_v2/__init__.py',2021-03-05T02:36:33Z,2021-03-05T20:27:10Z,,,
10528,b'Different vocab_size between model and tokenizer of mT5',2021-03-05T02:24:38Z,2021-04-23T15:02:17Z,,,
10527,b'Refactoring checkpoint names for multiple models',2021-03-05T01:40:49Z,2021-03-05T23:06:55Z,,,
10526,b'Fix Adafactor documentation (recommend correct settings)',2021-03-04T23:03:39Z,2021-04-01T04:03:39Z,,,
10525,b'fine-tune Pegasus with xsum using Colab but generation results have no difference',2021-03-04T20:43:02Z,2021-03-05T16:39:22Z,,,
10524,b'Change/remove default maximum length in run_glue.py',2021-03-04T19:18:52Z,2021-04-14T15:08:39Z,,,
10523,b'BERT as encoder - position ids',2021-03-04T18:56:21Z,2021-03-04T22:28:49Z,,,
10522,b'Inconsistent API output for Q&A models between eager mode and torchscripted ',2021-03-04T18:28:43Z,2021-04-23T15:02:18Z,,,
10521,b'Removes overwrites for output_dir ',2021-03-04T15:39:02Z,2021-03-04T16:12:38Z,,,
10520,b'Unable to translate Arabic to many other languages in MBart-50',2021-03-04T15:34:22Z,2021-05-17T15:03:18Z,,,
10519,"b'Adding option to truncation from beginning instead of end, for both longest_first and longest_second'",2021-03-04T15:27:07Z,,Feature request,,
10518,b'Converting models for tensoflowjs (node)',2021-03-04T15:13:07Z,2021-03-09T10:38:09Z,External,,
10517,b'Not always consider a local model a checkpoint in run_glue',2021-03-04T14:49:36Z,2021-03-04T16:11:39Z,,,
10514,b'Bug in Hosted inference API',2021-03-04T12:48:40Z,2021-04-23T15:02:19Z,,,
10513,b'Add Vision Transformer + ViTFeatureExtractor',2021-03-04T12:21:22Z,2021-03-29T12:41:46Z,,,
10512,b'Dynamic batch size for Seq2SeqTrainer',2021-03-04T10:35:24Z,2021-04-23T15:02:20Z,,,
10511,b'Why the positional embeddings in bert are not inplemented by sin/cos as the original paper said? Are these embeddings trainable?',2021-03-04T05:45:13Z,2021-04-23T15:02:20Z,,,
10510,b'Error in run_squad.py with BartForQuestionAnswering model',2021-03-04T05:11:50Z,2021-04-23T15:02:21Z,,AttributeError,"AttributeError: 'tuple' object has no attribute 'detach'`"
10509,b'Stale Bot',2021-03-04T02:06:30Z,2021-03-05T21:41:50Z,,,
10508,b'Loading tapas model into pipeline from directory gives different result',2021-03-03T23:48:08Z,2021-03-04T16:29:00Z,,,
10507,"b""'Trainer' object has no attribute 'log_metrics'""",2021-03-03T20:04:51Z,2021-04-23T15:02:22Z,,AttributeError,"AttributeError: 'Trainer' object has no attribute 'log_metrics'"
10506,b'[WIP][BIGBIRD] Add new conversion',2021-03-03T19:59:34Z,2021-03-04T11:12:47Z,,,
10505,b'Remove unsupported methods from ModelOutput doc',2021-03-03T19:09:26Z,2021-03-03T19:55:18Z,,,
10504,b'Rework TPU checkpointing in Trainer',2021-03-03T18:52:35Z,2021-03-04T16:46:11Z,,,
10503,b'Fine tuning a pipeline',2021-03-03T18:50:05Z,2021-04-23T15:02:23Z,,,
10502,b'GLUE benchmark crashes with MNLI and STSB',2021-03-03T17:33:43Z,2021-03-04T16:11:39Z,,,
10501,b'[ProphetNet] Bart-like Refactor',2021-03-03T16:29:35Z,2021-03-04T20:27:13Z,,,
10500,b'Fine tune of speaker embeddings model',2021-03-03T15:46:36Z,2021-04-23T15:02:23Z,,,
10499,"b'f""The model \'{self.model.__class__.__name__}\' is not supported for {self.task}. Supported models are {supported_models}"",'",2021-03-03T15:45:01Z,2021-03-03T16:23:14Z,,transformers.pipelines.base.PipelineException,"transformers.pipelines.base.PipelineException: The model 'BertModel' is not supported for question-answering. Supported models are ['ConvBertForQuestionAnswering', 'LEDForQuestionAnswering', 'DistilBertForQuestionAnswering', 'AlbertForQuestionAnswering', 'CamembertForQuestionAnswering', 'BartForQuestionAnswering', 'MBartForQuestionAnswering', 'LongformerForQuestionAnswering', 'XLMRobertaForQuestionAnswering', 'RobertaForQuestionAnswering', 'SqueezeBertForQuestionAnswering', 'BertForQuestionAnswering', 'XLNetForQuestionAnsweringSimple', 'FlaubertForQuestionAnsweringSimple', 'MobileBertForQuestionAnswering', 'XLMForQuestionAnsweringSimple', 'ElectraForQuestionAnswering', 'ReformerForQuestionAnswering', 'FunnelForQuestionAnswering', 'LxmertForQuestionAnswering', 'MPNetForQuestionAnswering', 'DebertaForQuestionAnswering', 'DebertaV2ForQuestionAnswering', 'IBertForQuestionAnswering']"
10498,b'DeBERTa Fast Tokenizer',2021-03-03T10:46:22Z,2021-04-30T12:08:16Z,"Good First Issue, Good Second Issue",,
10497,b'Wav2Vec fine code',2021-03-03T09:31:08Z,2021-03-03T11:00:37Z,,TypeError,"TypeError: expected Tensor as element 0 in argument 0, but got numpy.ndarray"
10496,b'[T5] Fix speed degradation bug t5',2021-03-03T08:48:54Z,2021-03-03T09:42:41Z,,,
10495,b'Albert quantized',2021-03-03T08:07:59Z,2021-04-24T15:02:04Z,,,
10494,b'[Wav2Vec2] Improve SpecAugment function by converting numpy based fun\xe2\x80\xa6',2021-03-03T05:44:59Z,2021-04-14T12:07:44Z,,,
10493,b'Generate can return cross-attention weights too',2021-03-03T05:39:27Z,2021-03-03T08:27:03Z,,,
10492,b'Model Weights Fail to Load from Pre-Trained Model when Using `tf.name_scope` ',2021-03-03T05:29:32Z,2021-04-24T15:02:04Z,,,
10491,b'ONNX Training for Transformers',2021-03-03T03:06:27Z,2021-05-23T15:02:25Z,,,
10490,"b""Pipeline's QnA and run_qa predictions do not match""",2021-03-02T23:45:02Z,2021-04-26T15:02:14Z,,,
10489,b'Fix typos',2021-03-02T21:48:09Z,2021-03-03T08:47:25Z,,,
10488,b'Smp grad accum',2021-03-02T21:25:17Z,2021-03-03T17:13:30Z,,,
10487,b'remap MODEL_FOR_QUESTION_ANSWERING_MAPPING classes to names auto-generated file',2021-03-02T21:11:56Z,2021-03-03T16:54:01Z,,,
10486,b'Trainer not logging to WandB in SageMaker',2021-03-02T19:55:30Z,2021-05-10T15:02:34Z,,,
10485,b'Constrained decoding?',2021-03-02T19:38:02Z,2021-03-05T14:58:20Z,,,
10484,b'Corrupted Relative Attention in T5 Decoder',2021-03-02T19:21:22Z,2021-06-08T15:06:59Z,,Notes,"Notes: 1525->""answer"", 10->"":"", 2024->""correct"", 2062->""restaurant"""
10483,b'add shift on BartForCausalLM',2021-03-02T18:37:36Z,2021-03-08T13:26:11Z,,,
10482,b'[examples] should all examples support the predict stage?',2021-03-02T18:06:49Z,2021-03-19T13:42:17Z,Examples,,
10481,b'feat(docs): navigate with left/right arrow keys',2021-03-02T15:17:59Z,2021-03-03T16:17:13Z,,,
10480,b'Different result in AutoModelForCausalLM',2021-03-02T12:37:38Z,2021-03-12T15:25:18Z,,,
10479,b'Question regarding training of BartForConditionalGeneration',2021-03-02T11:26:09Z,2021-05-07T15:02:10Z,,,
10478,b'generate() decoder_input_ids padding',2021-03-02T11:08:27Z,2021-03-19T12:40:50Z,,,
10477,b'Facing NCCL error on Multi-GPU training(on single machine) using run_glue.py script',2021-03-02T10:22:31Z,2021-03-04T19:46:50Z,,RuntimeError,"RuntimeError: NCCL error in: /opt/conda/conda-bld/pytorch_1603729138878/work/torch/lib/c10d/ProcessGroupNCCL.cpp:784, unhandled system error, NCCL version 2.7.8"
10476,b'The size of CoNLL-2003 is not consistant with the official release.',2021-03-02T09:03:19Z,2021-03-02T15:39:13Z,,,
10475,b'Fixes compatibility bug when using grouped beam search and constrained decoding together',2021-03-02T06:53:24Z,2021-03-02T07:41:54Z,,,
10474,"b'Continue pre-training using the example code ""run_mlm.py""'",2021-03-02T06:49:39Z,2021-03-06T15:08:53Z,,AssertionError,"AssertionError: text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized exampl"
10473,b'Issue with converting my own BERT TF2 checkpoint to PyTorch and loading the converted PyTorch checkpoint for training',2021-03-02T05:56:39Z,2021-03-09T07:37:17Z,,,
10472,b'Needed a feature to convert facebook mbart many - many model to ONNX runtime inorder to reduce the inference time',2021-03-02T05:05:30Z,2021-04-24T15:02:06Z,,,
10471,b'Question: change location of cache datasets',2021-03-02T04:55:09Z,2021-03-02T05:27:16Z,,,
10470,b'(Sorry I can not visit the forum) BORT question: pre-training-using-knowledge-distillation is better than pre-training-only for downstream tasks?',2021-03-02T03:01:23Z,2021-03-03T06:47:51Z,,,
10469,b'The described function in docs was not implemented in source code',2021-03-02T01:54:03Z,2021-03-03T19:55:18Z,,,
10468,b'run_ner.py training data file format',2021-03-01T23:45:47Z,2021-03-03T23:25:31Z,,,
10467,"b""modeling files loaded when they aren't being asked to be loaded""",2021-03-01T23:21:16Z,2021-03-03T16:54:00Z,,,
10466,b'Fix the bug in constructing the all_hidden_states of DeBERTa v2',2021-03-01T22:43:10Z,2021-03-03T17:05:21Z,,,
10465,b'Tflite conversion error for TFMT5ForConditionalGeneration model ',2021-03-01T22:36:16Z,2021-04-24T15:02:06Z,,Exception,"Exception: /home/python_user/.pyenv/versions/3.7.2/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py:1047:0: error: 'tf.Reshape' op requires 'shape' to have at most one dynamic dimension, but got multiple dynamic dimensions at indices 0 and 3"
10464,b'[Deepspeed] Allow HF optimizer and scheduler to be passed to deepspeed ',2021-03-01T18:53:23Z,2021-03-16T22:51:09Z,DeepSpeed,,
10463,b'Script for squad_v2 for custom data not working',2021-03-01T18:53:00Z,2021-04-24T15:02:07Z,,ValueError,"ValueError: External features info don't match the dataset:"
10462,b'Add I-BERT to README',2021-03-01T17:12:22Z,2021-03-01T17:12:31Z,,,
10461,b'pass correct head mask to cross-attention layer',2021-03-01T15:59:46Z,2021-03-05T08:40:35Z,,,
10460,b'How to Reduce the inference time of Facebook/many to many model?',2021-03-01T14:32:50Z,2021-04-24T15:02:08Z,,,
10459,b'[Wav2Vec2] Improve SpecAugment function by converting numpy based function to pytorch based function',2021-03-01T14:28:44Z,2021-05-25T14:02:51Z,Good First Issue,,
10458,b'Work towards fixing Flax tests',2021-03-01T10:52:40Z,2021-04-21T08:02:13Z,,,
10457,b'[Wav2Vec2] Remove unused config',2021-03-01T09:15:00Z,2021-03-01T09:30:12Z,,,
10456,b'How to Improve inference time of facebook/mbart many to many model?',2021-03-01T08:21:18Z,2021-03-01T16:08:29Z,,,
10455,b'[Wav2Vec2FeatureExtractor] smal fixes',2021-03-01T07:51:14Z,2021-03-01T14:49:52Z,,,
10454,b'How can I make the logging utils log to a file as well? ',2021-03-01T07:37:48Z,2021-03-01T07:43:37Z,,,
10453,b'pytorch Albert quantization error',2021-03-01T07:26:03Z,2021-04-23T15:02:25Z,,AttributeError,AttributeError: 'function' object has no attribute 't'
10452,b'BartForConditionalGeneration breaks with label smoothing loss',2021-03-01T07:15:07Z,2021-03-02T07:06:12Z,,RuntimeError,"RuntimeError: Size does not match at dimension 1 expected index [1, 7, 1] to be smaller than src [1, 5, 50265] apart from dimension 2"
10451,b'BART for generating sequence of length more than 1024 tokens',2021-03-01T04:41:31Z,2021-03-01T08:12:41Z,,,
10450,"b""OSError: Error no file named ['pytorch_model.bin', 'tf_model.h5'] When I try to use my model""",2021-02-28T18:19:47Z,2021-04-15T05:01:44Z,,"UnpicklingError, OSError","UnpicklingError: invalid load key, 'v'.OSError: Unable to load weights from pytorch checkpoint file for '/content/beta-kvantorium-simple-small' at '/content/beta-kvantorium-simple-small/pytorch_model.bin'If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True. "
10449,"b'pytorch/aten/src/THCUNN/ClassNLLCriterion.cu:108: cunn_ClassNLLCriterion_updateOutput_kernel: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.'",2021-02-28T18:16:04Z,2021-03-01T16:09:53Z,,RuntimeError,"RuntimeError: CUDA error: device-side assert triggered"
10448,"b'When I try to import my model I run into an error ""TypeError: PyMetaspace.__new__() got an unexpected keyword argument: str_rep""'",2021-02-28T17:37:49Z,2021-04-08T19:45:10Z,,TypeError,"TypeError: PyMetaspace.__new__() got an unexpected keyword argument: str_rep"
10447,b'changing the way checkpoint is done in the new release ',2021-02-28T17:09:13Z,2021-04-23T15:02:26Z,,,
10446,"b""AttributeError: 'Trainer' object has no attribute 'log_metrics'""",2021-02-28T16:39:59Z,2021-03-01T20:53:47Z,,AttributeError,"AttributeError: 'Trainer' object has no attribute 'log_metrics'"
10445,b'[IBert] Correct link to paper',2021-02-28T15:51:45Z,2021-02-28T16:03:50Z,,,
10444,"b'TypeError: can only concatenate str (not ""int"") to str'",2021-02-28T11:18:10Z,2021-04-23T15:02:26Z,,TypeError,"TypeError: can only concatenate str (not ""int"") to str"
10443,b'Adds terms to Glossary',2021-02-28T10:02:56Z,2021-02-28T13:27:55Z,,,
10442,b'Bug in Electra Example',2021-02-28T08:00:47Z,2021-03-01T15:09:26Z,,,
10441,"b""TypeError: __init__() got an unexpected keyword argument 'model' in `run_seq2seq.py` example when using on our own files""",2021-02-28T05:33:02Z,2021-03-01T08:30:20Z,,TypeError,"TypeError: __init__() got an unexpected keyword argument 'model'"
10440,b'Checkpoint refactoring for Multiple Models',2021-02-28T03:50:07Z,2021-03-03T20:54:02Z,,,
10439,"b'Option to output ""test_preds_seq2seq.txt"" text file with each checkpoint generated in ""run_seq2seq.py""'",2021-02-27T23:41:32Z,2021-04-23T15:02:27Z,,,
10438,b'Setting max_length for model training produces error',2021-02-27T22:27:22Z,2021-02-28T00:35:14Z,,"RuntimeError, IndexError","RuntimeError: CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`IndexError: index out of range in self"
10437,b'[Trainer] add --max_train_samples  --max_val_samples  --max_test_samples ',2021-02-27T17:23:58Z,2021-03-08T18:57:10Z,trainer,,
10436,b'updated logging and saving metrics',2021-02-27T16:47:46Z,2021-02-27T17:53:44Z,,,
10435,b'Confused about the time of forword',2021-02-27T13:13:51Z,2021-04-23T15:02:28Z,,,
10434,b'TF Dataset Pipeline throws `RuntimeError: Already borrowed` when tokenizing',2021-02-27T10:06:27Z,2021-03-02T14:05:15Z,,"UnknownError, RuntimeError","UnknownError: RuntimeError: Already borrowedRuntimeError: Already borrowed"
10433,b'About the speed when return_dict is set to True',2021-02-27T04:24:41Z,2021-04-23T15:02:28Z,,,
10432,b'Adding Longformer Encoder Decoder support for T5',2021-02-27T02:57:42Z,2021-04-23T15:02:29Z,,,
10431,b'Fix conda-build',2021-02-27T01:20:25Z,2021-02-27T01:20:30Z,,,
10430,b'Inference with Finetuned BERT Model outputting odd results',2021-02-26T21:56:30Z,2021-04-23T15:02:30Z,,,
10429,"b""Trainer's load_best_model_at_end argument results in error with DistributedDataParallel""",2021-02-26T21:53:31Z,2021-04-23T15:02:31Z,,OSError,"OSError: Can't load config for 'checkpoint-115'. Make sure that: - 'checkpoint-115' is a correct model identifier listed on 'https://huggingface.co/models' - or 'checkpoint-115' is the correct path to a directory containing a config.json file"
10428,b'[run_seq2seq.py] restore functionality: saving to test_generations.txt',2021-02-26T21:51:55Z,2021-02-27T16:21:50Z,,,
10427,b'[examples] better model example',2021-02-26T21:33:09Z,2021-02-27T01:01:02Z,,,
10426,b'[WIP] CLIP',2021-02-26T21:29:49Z,2021-04-26T09:08:26Z,WIP,,
10425,b'RAG and retrieved documents',2021-02-26T20:09:30Z,2021-02-26T20:25:06Z,,,
10424,b'Refactor checkpoint name in BERT and MobileBERT',2021-02-26T18:24:27Z,2021-03-03T16:21:17Z,,,
10423,b'[examples] add --max_train_samples  --max_val_samples  --max_test_samples cl args to all scripts',2021-02-26T18:18:01Z,2021-03-09T17:06:56Z,Good First Issue,,
10422,b'Layoutlm tf',2021-02-26T17:09:11Z,2021-03-05T23:58:13Z,,,
10421,b'updated metrics saving and logging',2021-02-26T16:58:42Z,2021-02-27T16:50:32Z,,,
10420,b'Unable to convert Facebook/mbart-many-to-many model to onxx',2021-02-26T14:33:48Z,2021-04-23T15:02:32Z,,,
10419,b'[LED] Correct Docs',2021-02-26T14:29:28Z,2021-02-26T14:53:28Z,,,
10418,b'Slow evaluation using Trainer with TPUs in Colab',2021-02-26T14:21:36Z,2021-02-26T16:19:15Z,,,
10417,b'Dont use sigmoid when num_labels==1',2021-02-26T14:06:34Z,2021-04-23T15:02:33Z,,,
10416,b'Add BERTForMultiLabel Classification or Regression',2021-02-26T13:09:20Z,2021-03-31T12:56:28Z,,,
10415,b'Bug when combining grouped beam search and constrained prefix decoding',2021-02-26T11:48:47Z,2021-03-02T07:41:54Z,,RuntimeError,"RuntimeError: shape '[-1, 2, 1]' is invalid for input of size 1"
10414,b'Add Ray Tune hyperparameter search integration test',2021-02-26T11:38:22Z,2021-02-26T15:18:33Z,,,
10413,b'Update run_mlm.py',2021-02-26T10:56:49Z,2021-04-23T15:02:33Z,,,
10412,b'Trainer: Make `best_model_checkpoint` path in `trainer_state.json` relative to `args.output_dir`',2021-02-26T10:36:41Z,2021-04-23T15:02:34Z,,,
10411,b'Problem using add_special_tokens',2021-02-26T10:28:51Z,2021-04-23T15:02:35Z,,AssertionError,"AssertionError: Key extra_id_0 is not a special token"
10410,b'[WIP] RAG end-to-end retriever training (with ray workers)',2021-02-26T09:41:41Z,2021-05-10T09:29:58Z,,,
10409,"b'[ci, flax] non-existing models are unlikely to pass tests'",2021-02-26T08:46:08Z,2021-02-26T09:35:36Z,,,
10408,b'Question about the `decoder_input_ids`  in `LEDForConditionalGeneration` forward method',2021-02-26T07:18:00Z,2021-02-26T14:53:28Z,,,
10407,b'offline mode for firewalled envs',2021-02-26T02:46:33Z,2021-03-06T01:27:49Z,,,
10406,b'Ray Tune Integration Bug Fixes',2021-02-26T01:12:56Z,2021-02-27T00:06:08Z,,,
10405,b'Problem running T5 (configuration) with text classification',2021-02-25T22:14:47Z,2021-02-26T17:13:24Z,,ValueError,"ValueError: Unrecognized configuration class <class 'transformers.models.t5.configuration_t5.T5Config'> for this kind of AutoModel: AutoModelForSequenceClassification."
10404,b'Model Hub: Search by model size',2021-02-25T21:18:15Z,2021-04-23T15:02:36Z,,,
10403,b'Sagemaker Model Parallel tensoboard writing fix',2021-02-25T21:05:39Z,2021-02-26T13:04:55Z,,,
10402,b'SageMaker Model Parallel: cluttered tensorboard plots',2021-02-25T20:02:32Z,2021-02-26T13:05:07Z,,,
10401,b'Fix run_glue evaluation when model has a label correspondence',2021-02-25T19:02:46Z,2021-02-25T20:30:38Z,,,
10400,b'[Deepspeed] getting multiple prints of: Avoid using `tokenizers` before the fork if possible',2021-02-25T18:40:59Z,2021-02-25T19:07:13Z,DeepSpeed,,
10399,b'Make Barthez tokenizer tests a bit faster',2021-02-25T16:14:01Z,2021-02-25T16:42:25Z,,,
10398,b'Does the synonym replacement tasks need Transformer?',2021-02-25T15:39:49Z,2021-02-25T15:45:15Z,,,
10397,b'Ignore unexpected weights from PT conversion',2021-02-25T14:56:02Z,2021-02-25T15:42:27Z,,,
10396,b' how to freeze specific layers of TFbert model and just train a classifier? ',2021-02-25T14:31:02Z,2021-02-25T21:52:05Z,,,
10395,b'RobertaTokenizerFast does not add special tokens',2021-02-25T13:28:36Z,2021-03-01T08:55:16Z,,,
10394,"b""DeepSpeedEngine object has no attribute 'no_sync'""",2021-02-25T13:19:38Z,2021-02-26T13:41:21Z,,torch.nn.modules.module.ModuleAttributeError,"torch.nn.modules.module.ModuleAttributeError: 'DeepSpeedEngine' object has no attribute 'no_sync'"
10393,b'NER Pipeline not working',2021-02-25T11:16:09Z,2021-02-25T16:08:46Z,,ValueError,"ValueError: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length."
10392,b'Remove unused variable in example for Q&A',2021-02-25T10:50:25Z,2021-02-25T14:18:47Z,,,
10391,b'some bugs about mbart50 for spanish',2021-02-25T10:02:29Z,2021-05-01T15:02:49Z,,,
10390,b'Tokenizer not working',2021-02-25T09:32:00Z,2021-05-19T15:08:59Z,Should Fix,TypeError,"TypeError: PyBertNormalizer.__new__() got an unexpected keyword argument: do_lower_case"
10389,b'GA: only run model templates once - from fork',2021-02-25T00:33:34Z,2021-02-25T00:36:41Z,,,
10388,b'GA: only run model templates once',2021-02-25T00:29:26Z,2021-02-25T00:48:01Z,,,
10387,b'loss.backward() TypeError seed issue for pretrained reformer model',2021-02-24T22:57:03Z,2021-03-01T16:58:20Z,,TypeError,"TypeError: int() argument must be a string, a bytes-like object or a number, not 'NoneType'"
10386,b'MNLI evaluation on pretrained models',2021-02-24T22:12:23Z,2021-02-25T16:43:16Z,,,
10385,b'DDP performing slightly worse in terms of loss and metrics than DP',2021-02-24T21:19:33Z,2021-03-16T15:16:36Z,,,
10384,b'Fine-tune pretrained Wav2Vec2 on a small custom dataset ',2021-02-24T20:47:39Z,2021-04-23T15:02:37Z,,,
10383,b'Run GA on every push even on forks',2021-02-24T20:46:49Z,2021-02-25T00:23:39Z,,,
10382,b'Run GA on forks (Attempt #2)',2021-02-24T20:40:29Z,2021-02-24T20:45:26Z,,,
10381,"b'Option to output ""test predictions"" text file with each checkpoint in run_seq2seq.py'",2021-02-24T18:32:26Z,2021-02-27T16:21:50Z,,,
10380,b'Trainer.train() gets stuck when executed on K8 pods',2021-02-24T17:51:36Z,2021-04-23T15:02:38Z,,,
10379,b'[firewalled env] OFFLINE mode',2021-02-24T17:24:07Z,2021-03-07T00:18:40Z,,,
10378,"b""AttributeError: 'QAModel' object has no attribute 'automatic_optimization'""",2021-02-24T15:21:06Z,2021-02-24T16:26:39Z,,,
10377,b'Training LongformerForQuestionAnswering on TriviaQA',2021-02-24T14:56:47Z,2021-04-23T15:02:39Z,,,
10376,"b""UnicodeDecodeError: 'utf-8' codec can't decode byte 0x80 in position 64: invalid start byte""",2021-02-24T14:33:10Z,2021-03-02T11:57:06Z,,UnicodeDecodeError,"UnicodeDecodeError: 'utf-8' codec can't decode byte 0x80 in position 64: invalid start byte"
10375,b'DPR decode_best_spans include spans from title',2021-02-24T13:43:08Z,2021-04-23T15:02:40Z,,,
10374,b'Fix None in add_token_positions - issue #10210',2021-02-24T13:36:43Z,2021-02-25T16:18:33Z,,,
10373,b'[Documentation issue] Sequence to sequence models',2021-02-24T13:33:41Z,2021-05-17T15:03:20Z,,,
10372,b'deprecated reference `tokenizer.max_len` in glue.py (PR #10220)',2021-02-24T10:49:09Z,2021-02-24T14:02:02Z,,,
10371,b'Load pretrained model except the head layer for a specific downstream task',2021-02-24T09:50:17Z,,Feature request,,
10370,"b""ReformerForQuestionAnswering : int() argument must be a string, a bytes-like object or a number, not 'NoneType'""",2021-02-24T08:29:58Z,2021-04-20T22:23:37Z,,TypeError,"TypeError: int() argument must be a string, a bytes-like object or a number, not 'NoneType'"
10369,b'Why should `attn_weights` be reshaped twice in BartAttention ?',2021-02-24T08:19:06Z,2021-02-24T17:01:16Z,,,
10368,"b""TFMarianModel from_pretrained can't load weights""",2021-02-24T08:03:29Z,2021-09-01T15:03:06Z,,,
10367,b'device-side assert triggered Error while doing inference on Distilbert and Bert',2021-02-24T07:06:39Z,2021-02-24T15:08:30Z,,,
10366,"b""can't allocate memory error with wav2vec2""",2021-02-24T07:04:57Z,2021-04-23T15:02:42Z,,RuntimeError,"RuntimeError: [enforce fail at CPUAllocator.cpp:65] . DefaultCPUAllocator: can't allocate memory: you tried to allocate 24373495488 bytes. Error code 12 (Cannot allocate memory)"
10365,b'Knowledge Retrieval missing from BlenderBot Implementation',2021-02-24T05:44:08Z,,New model,,
10364,b'Loading mBART Large 50 MMT (many-to-many) is slow',2021-02-24T02:54:51Z,2021-05-17T15:03:22Z,,,
10363,b'[trainer] move secondary methods into a separate file',2021-02-24T01:54:28Z,2021-02-24T16:32:52Z,,,
10362,b'[Trainer/Deepspeed] handle get_last_lr() before first step()',2021-02-23T21:01:32Z,2021-02-24T01:42:25Z,,,
10361,b'denoising objective for pretraining',2021-02-23T21:00:01Z,2021-04-23T15:02:43Z,,,
10360,b'Rag Use Your Knowledge dataset',2021-02-23T20:22:32Z,2021-04-24T15:02:10Z,,,
10359,b'Security Bug found - looking for contact for responsible disclosure',2021-02-23T20:15:34Z,2021-02-24T20:26:08Z,,,
10358,b'BART Summarization : Torchscript Export / Inference Triton Server',2021-02-23T18:47:54Z,2021-05-17T15:03:23Z,Migration,**Error**,"**Error**: ModuleAttributeError: 'RecursiveScriptModule' object has no attribute 'generate'"
10357,b'tokenization_marian.py: use current_spm for decoding',2021-02-23T17:59:42Z,2021-03-08T13:14:31Z,,,
10356,b'Fine-tuning bart-base on XSum and got 34.0 as ROUGE1 (40.61 with higher lr)',2021-02-23T17:07:21Z,2021-05-10T15:02:37Z,,,
10355,b'ProphetNet Positional Embeddings Index Issue',2021-02-23T16:55:40Z,2021-05-26T15:09:07Z,,,
10354,b'Add support for ZeRO-2/3 and ZeRO-offload in fairscale',2021-02-23T16:35:26Z,2021-02-25T16:07:54Z,,,
10353,"b'[bert-base-german-cased] use model repo, not external bucket'",2021-02-23T15:51:47Z,2021-02-23T17:30:47Z,,,
10351,b'Can every line in the input CSV file contain more than one sentence when pertraining BERT for MLM Loss?',2021-02-23T14:21:14Z,2021-02-23T15:48:00Z,,,
10350,"b'Got ""RuntimeError: CUDA error: device-side assert triggered"" with Seq2SeqTrainer'",2021-02-23T13:41:44Z,2021-02-23T15:57:39Z,,RuntimeError,"RuntimeError: CUDA error: device-side assert triggered`"
10349,b'Padding of bbox input in LayoutLM',2021-02-23T12:49:05Z,2021-05-12T15:02:50Z,,ValueError,"ValueError: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length."
10348,b'BertForMaskedLM cannot be initialized from BERT checkpoints',2021-02-23T12:36:09Z,2021-02-25T15:42:27Z,,,
10347,b'[Benchmark]',2021-02-23T12:09:58Z,2021-02-23T16:10:51Z,,,
10346,b'Custom tokenizer with run_mlm script',2021-02-23T12:07:11Z,2021-04-24T15:02:11Z,,ValueError,"ValueError: Unrecognized model in ../Data/tokenizer. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: wav2vec2, convbert, led, blenderbot-small, retribert, mt5, t5, mobilebert, distilbert, albert, bert-generation, camembert, xlm-roberta, pegasus, marian, mbart, mpnet, bart, blenderbot, reformer, longformer, roberta, deberta, flaubert, fsmt, squeezebert, bert, openai-gpt, gpt2, transfo-xl, xlnet, xlm-prophetnet, prophetnet, xlm, ctrl, electra, encoder-decoder, funnel, lxmert, dpr, layoutlm, rag, tapas`"
10345,b'MarianMT - ONNX only accepts fixed input despite setting dynamic axes',2021-02-23T11:23:46Z,2021-04-24T15:02:11Z,,RuntimeException,"RuntimeException: [ONNXRuntimeError] : 6 : RUNTIME_EXCEPTION : Non-zero status code returned while running Reshape node. Name:'Reshape_62' Status Message: /onnxruntime_src/onnxruntime/core/providers/cpu/tensor/reshape_helper.h:42 onnxruntime::ReshapeHelper::ReshapeHelper(const onnxruntime::TensorShape&, std::vector<long int>&) gsl::narrow_cast<int64_t>(input_shape.Size()) == size was false. The input tensor cannot be reshaped to the requested shape. Input shape:{1,13}, requested shape:{5}"
10344,b'Fix broken examples/seq2seq/README.md markdown',2021-02-23T10:58:43Z,2021-02-23T15:49:25Z,,,
10343,b'Where can we find the `RAG` implementation?',2021-02-23T06:41:28Z,2021-02-23T15:48:19Z,,,
10342,b'DialoGPT tokenizer config issue',2021-02-23T06:36:55Z,2021-02-24T04:43:35Z,,,
10341,b'Translate English into Japanese using mbart',2021-02-23T05:12:56Z,2021-04-24T15:02:12Z,,,
10340,b'tokenizer.Tokenizer compatibility with Inference API or Auto* classes',2021-02-23T01:39:13Z,,Feature request,,
10339,b'Problem with GPT2/DistilGPT2 prediction - dimension mismatch',2021-02-22T22:35:25Z,2021-02-23T01:52:26Z,,RuntimeError,"RuntimeError: Sizes of tensors must match except in dimension 3. Got 53 and 23"
10338,b'Fix evaluation with label smoothing in Trainer',2021-02-22T21:14:14Z,2021-02-22T21:39:03Z,,,
10337,b'[trainer] port metrics logging and saving methods to all example scripts',2021-02-22T21:13:28Z,2021-02-27T17:53:44Z,"Good First Issue, trainer",,
10336,b'[Benchmark] Converting a QA distilbert model to onnx - the f1 score plummet',2021-02-22T19:08:41Z,2021-03-01T17:21:20Z,,,
10335,b'Return cross-attention weights in generation function',2021-02-22T18:58:23Z,2021-03-03T08:27:02Z,,,
10334,b'Loading from last checkpoint functionality in Trainer.train',2021-02-22T15:43:34Z,2021-02-22T20:33:00Z,,,
10333,b'Clean TF ConvBert',2021-02-22T15:03:23Z,2021-02-22T17:59:25Z,,,
10332,b'bug in bert pretraining ',2021-02-22T14:51:25Z,2021-02-22T15:49:29Z,,,
10331,b'Add note to resize token embeddings matrix when adding new tokens to voc',2021-02-22T14:09:27Z,2021-02-22T14:48:21Z,,,
10330,b'[DeepSpeed] strange learning rate schedule in linear_schedule_with_warmup',2021-02-22T13:45:34Z,2021-02-24T03:20:59Z,DeepSpeed,,
10329,b'Raise an error instead of a warning when model files are not loaded correctly',2021-02-22T13:26:25Z,2021-04-24T15:02:13Z,,,
10328,b'DeBERTa-v2 fixes',2021-02-22T12:33:29Z,2021-02-22T12:45:18Z,,,
10327,b'mBART 50 models not found in model shortcut name list',2021-02-22T11:28:59Z,2021-04-15T04:50:41Z,,KeyError,"KeyError: None"
10326,b'[DeepSpeed] unable to increase batch size from 4 for T5-3b with 2x 32GB V100 GPUs',2021-02-22T10:09:08Z,2021-03-04T03:54:32Z,,,
10325,b'Input mismatch with TFDistilBert training from scratch inspite of cross checking input dimensions',2021-02-22T09:37:21Z,2021-02-22T15:27:40Z,,,
10324,"b'[PretrainedFeatureExtractor] + Wav2Vec2FeatureExtractor, Wav2Vec2Processor, Wav2Vec2Tokenizer'",2021-02-22T08:19:10Z,2021-02-25T14:42:46Z,,,
10321,b'[Tensor Parallelism] Megatron-LM to transformers',2021-02-21T21:57:17Z,,"Tensor Parallel, WIP",,
10320,b'BERT for speech',2021-02-21T17:21:16Z,2021-02-22T13:51:51Z,,,
10319,b'[Question] Add a new token to tokenizer and bart model',2021-02-21T17:11:05Z,2021-02-22T14:48:21Z,,,
10318,b'Guidance for continued pre-training of BART with de-noising.',2021-02-21T16:15:35Z,2021-04-24T15:02:14Z,,,
10317,b'ForTokenClassification head on BART',2021-02-21T14:30:22Z,2021-04-24T15:02:15Z,,,
10316,b'fix typo in conversion script',2021-02-21T13:15:07Z,2021-02-21T15:54:27Z,,,
10315,b'Huggingface mt5 does not reach the performance of original mt5 on paws-x ',2021-02-21T11:59:18Z,2021-04-24T15:02:15Z,,,
10314,b'ConvBERT fix torch <> tf weights conversion',2021-02-21T11:37:13Z,2021-02-24T11:55:35Z,,,
10313,b'ValueError: too many values to unpack (expected 2)',2021-02-21T09:32:52Z,2021-04-24T15:02:16Z,,ValueError,"ValueError: too many values to unpack (expected 2)"
10312,b'LayoutLM Tensorflow model',2021-02-21T09:30:10Z,2021-03-25T16:44:23Z,,,
10311,b'Matrix multiplication error for ReformerModelWithLMHead when tie_word_embeddings is True',2021-02-21T08:05:00Z,2021-04-24T15:02:17Z,,RuntimeError,"RuntimeError: mat1 and mat2 shapes cannot be multiplied (4096x512 and 256x320)"
10310,b'[Trainer] implement gradient_accumulation_steps support in DeepSpeed integration',2021-02-21T07:32:38Z,2021-02-22T19:15:59Z,DeepSpeed,,
10309,b'[Example] Using label_smoothing_factor raise error when evaluating model',2021-02-20T23:51:49Z,2021-02-22T21:39:03Z,,AttributeError,"AttributeError: 'NoneType' object has no attribute 'detach'"
10308,"b""[ci] don't fail when there are no zombies""",2021-02-20T21:21:51Z,2021-02-20T21:28:43Z,,,
10307,b'pretraining objective of T5 model ',2021-02-20T20:42:01Z,2021-04-24T15:02:18Z,,,
10306,b'Issue Loading bert-based-german-cased',2021-02-20T18:59:47Z,2021-02-23T17:30:47Z,,,
10305,b'Documentation of the decode method is missing',2021-02-20T17:48:43Z,2021-02-21T05:27:42Z,,,
10304,b'fixes #10303',2021-02-20T17:34:56Z,2021-02-20T20:21:33Z,,,
10303,b'convert_tokens_to_string documentation bug',2021-02-20T17:20:11Z,2021-02-20T20:21:33Z,,,
10302,b'Tensorflow not found but i can import it',2021-02-20T16:06:53Z,2021-04-24T15:02:19Z,,,
10301,b'[WIP] Add Megatron-11B',2021-02-20T15:24:25Z,,WIP,,
10300,"b""unexpected keyword argument 'forced_bos_token_id' when using mbart-large-50-many-to-many-mmt""",2021-02-20T14:58:32Z,2021-02-22T15:36:21Z,,TypeError,"TypeError: forward() got an unexpected keyword argument 'forced_bos_token_id'"
10299,"b""Object of type 'int64' is not JSON serializable in Trainer.save_checkpoint""",2021-02-20T13:12:30Z,2021-03-11T14:00:23Z,,TypeError,"TypeError: Object of type 'int64' is not JSON serializable"
10298,b'Converting fairseq NMT to transformers misses model weight',2021-02-20T09:35:04Z,2021-02-24T12:25:52Z,fsmt,,
10297,b'AutoTokenizer from pretrained BERT throws TypeError when encoding certain input',2021-02-20T08:40:32Z,2021-02-22T15:50:29Z,,TypeError,"TypeError: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]"
10296,"b""[predict] AttributeError: 'Seq2SeqTrainer' object has no attribute 'metrics_format'""",2021-02-20T04:19:15Z,2021-02-23T02:40:29Z,,AttributeError,"AttributeError: 'Seq2SeqTrainer' object has no attribute 'metrics_format'"
10295,b'[examples/seq2seq] defensive programming + expand/correct README',2021-02-20T04:03:25Z,2021-02-22T18:58:50Z,,,
10294,b'Marian input decoding bug',2021-02-20T02:17:13Z,2021-03-08T13:14:31Z,,,
10293,"b""[pretrained] model classes aren't checking the arch of the pretrained model it loads""",2021-02-20T02:03:33Z,2021-03-18T16:51:43Z,Good First Issue,TypeError,"TypeError: expected str, bytes or os.PathLike object, not NoneType"
10292,"b""[examples s2s] AttributeError: 'MBartTokenizerFast' object has no attribute 'tgt_lang'""",2021-02-20T01:04:50Z,2021-03-15T19:55:14Z,,AttributeError,"AttributeError: 'MBartTokenizerFast' object has no attribute 'tgt_lang'"
10291,b'Fix example links in the task summary',2021-02-19T22:34:21Z,2021-02-19T23:04:15Z,,,
10290,b'Trainer train continues after resume_from_checkpoint on a checkpoint with early stop',2021-02-19T22:29:06Z,2021-04-24T15:02:19Z,,,
10289,b'Masking issues with GPT2LMHeadModel.generate()',2021-02-19T21:59:24Z,2021-04-24T15:02:20Z,,,
10288,b'Minor documentation issue',2021-02-19T21:51:10Z,2021-02-19T23:04:15Z,,,
10287,b'Deprecate prepare_seq2seq_batch',2021-02-19T20:27:54Z,2021-02-22T17:36:17Z,,,
10286,b'Introduce save_strategy training argument',2021-02-19T20:21:49Z,2021-02-28T00:34:22Z,,,
10285,b'Random Word Replacement Probability',2021-02-19T19:50:33Z,2021-02-19T19:55:46Z,,,
10284,b'Patch zero shot distillation script cuda issue',2021-02-19T19:04:13Z,2021-02-19T19:06:58Z,,,
10283,b'Clean TF BART and TF Seq2Ses template',2021-02-19T18:31:25Z,2021-02-22T17:59:43Z,,,
10282,b'[tests] tests/test_trainer_distributed.py intermittent failure',2021-02-19T18:22:27Z,2021-03-18T02:05:08Z,Tests,,
10281,b'[CI] Kill any run-away pytest processes',2021-02-19T17:22:01Z,2021-02-19T18:36:37Z,,,
10280,b'Trainer.train argument resume_from_last_checkpoint',2021-02-19T15:53:02Z,2021-02-22T20:33:00Z,,,
10279,b'Performance of mbart-large-50-many-to-many-mmt on de/fr/it',2021-02-19T15:52:24Z,2021-05-17T15:03:24Z,,,
10278,b'Improving training time for Marian MT model with the Trainer ',2021-02-19T15:12:20Z,2021-04-25T15:03:12Z,,,
10277,"b""ImportError: cannot import name 'pipeline' from 'transformers' (unknown location)""",2021-02-19T14:47:13Z,2021-02-20T01:08:32Z,,,
10276,b'Move the TF NER example',2021-02-19T13:29:45Z,2021-02-19T21:06:13Z,,,
10275,b'Fix squad processor for TF',2021-02-19T12:59:55Z,2021-04-23T15:02:44Z,,,
10274,b'Rework the AMP for TF XLNet',2021-02-19T10:47:20Z,2021-02-24T13:38:29Z,,,
10273,b'ElectraForQuestionAnswering with SQuADHead',2021-02-19T05:58:46Z,2021-04-23T15:02:45Z,,,
10272,b'Summarization of long text with T5 seems to output random memory content',2021-02-19T05:47:48Z,2021-04-23T15:02:46Z,,,
10271,b'[test] fix func signature',2021-02-18T23:23:50Z,2021-02-19T00:44:42Z,,,
10270,b'[ISSUES.md] propose using google colab to reproduce problems',2021-02-18T23:12:27Z,2021-02-19T01:15:52Z,,,
10269,b'Language Modeling Task (GPT2 / CLM) Does Not Generate Line Breaks?',2021-02-18T21:59:09Z,2021-02-26T18:15:02Z,,,
10268,b'[trainer] implement support for full fp16 in evaluation/predict',2021-02-18T21:18:30Z,2021-02-19T01:02:35Z,,,
10267,b'Introduce logging_strategy training argument',2021-02-18T20:18:31Z,2021-02-19T16:49:23Z,,,
10266,b'[trainer] add Trainer methods for metrics logging and saving ',2021-02-18T19:54:28Z,2021-02-22T21:02:53Z,,,
10265,b'Tapas Tokenizer  makes DataFrame iterrows() iterator crazy ...',2021-02-18T16:54:15Z,2021-04-23T15:02:47Z,,AttributeError,"AttributeError: 'Cell' object has no attribute 'lower'"
10264,b'Making TF TransfoXL model compliant with AMP',2021-02-18T16:04:21Z,2021-02-19T11:58:08Z,,,
10263,b'NER label re-alignment always expects B labelled first sub-words',2021-02-18T15:17:04Z,2021-05-18T07:53:21Z,"Good First Issue, Good Second Issue",,
10262,b'Making TF T5 model compliant with AMP and XLA',2021-02-18T15:13:05Z,2021-02-19T11:57:16Z,,,
10261,b'Making TF OpenAI GPT model compliant with AMP and XLA',2021-02-18T14:13:44Z,2021-02-19T14:33:25Z,,,
10260,b'Making TF MPNet model compliant with XLA',2021-02-18T13:30:23Z,2021-02-19T11:56:41Z,,,
10259,b'Making TF MobileBert model compliant with AMP ',2021-02-18T12:24:21Z,2021-02-19T11:55:25Z,,,
10258,b'Deberta Tokenizer convert_ids_to_tokens() is not giving expected results',2021-02-18T12:16:18Z,2021-04-01T17:53:54Z,,,
10257,b'Making TF Lxmert model compliant with AMP',2021-02-18T12:06:38Z,2021-02-19T11:54:15Z,,,
10256,b'[Question]: Register new Tokenizer',2021-02-18T11:02:58Z,2021-04-23T15:02:48Z,,,
10255,b'Addition of on-the-fly loading for MLM training and fix for default pad_to_max_length value for TPU',2021-02-18T10:08:48Z,2021-02-18T14:30:30Z,,,
10254,"b""ImportError: cannot import name 'MBart50TokenizerFast' from 'transformers' (unknown location)""",2021-02-18T07:58:01Z,2021-03-03T15:28:55Z,,ImportError,"ImportError: cannot import name 'MBart50TokenizerFast' from 'transformers' (unknown location)"
10253,b'Load custom models',2021-02-18T02:10:11Z,2021-02-18T16:24:05Z,,"FileNotFoundError, OSError","FileNotFoundError: Cannot find the requested files in the cached path and outgoing traffic has been disabled. To enable model look-ups and downloads online, set 'local_files_only' to False.OSError: Can't load config for './SqueezeBert/results/best_checkpoint/config.json'. Make sure that:"
10252,b'microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract not available for tensorflow',2021-02-17T23:16:51Z,2021-02-18T09:59:13Z,,,
10251,b'[ci] scheduled job test',2021-02-17T23:06:38Z,2021-02-18T05:23:26Z,,,
10250,b'[CI] force scheduled action hub re-run',2021-02-17T22:59:37Z,2021-02-17T23:07:23Z,,,
10249,b'[CI] force scheduled action hub re-run',2021-02-17T22:32:34Z,2021-02-17T22:59:56Z,,,
10248,b'[CI] 2 fixes',2021-02-17T22:08:35Z,2021-02-17T22:12:40Z,,,
10247,b'[BUG] [Ray-Tune] ValueError: checkpoint not in list',2021-02-17T21:41:05Z,2021-02-22T12:41:56Z,,"ray.exceptions.RayTaskError(TuneError), ray.tune.error.TuneError, ValueError","ray.exceptions.RayTaskError(TuneError): ray::ImplicitFunc.train_buffered() (pid=473, ip=172.28.0.2)ray.tune.error.TuneError: Trial raised an exception. Traceback:ValueError: 'results/run-34e77498/checkpoint-10' is not in list"
10246,b'TensorFlow Question-Answering example fails to run (cardinality error)',2021-02-17T20:40:46Z,2021-04-23T15:02:49Z,,AttributeError,"AttributeError: '_AssertCardinalityDataset' object has no attribute 'cardinality'"
10245,b'`compute_metrics` show better results than `generate` because target data leaks',2021-02-17T20:17:50Z,2021-04-23T15:02:50Z,,,
10244,b'Script for distilling zero-shot classifier to more efficient student',2021-02-17T20:12:36Z,2021-02-18T22:08:45Z,"Distillation, Examples",,
10243,"b'[trainer] refactor place_model_on_device logic, add deepspeed'",2021-02-17T19:52:38Z,2021-02-17T23:52:36Z,,,
10242,b'Upgrade transformers from 3.5.0 to 4.3.2 instance error',2021-02-17T18:28:47Z,2021-04-23T15:02:50Z,,ValueError,"ValueError: Input 0 of layer dense is incompatible with the layer: : expected min_ndim=2, found ndim=0. Full shape received: []"
10241,b'[Trainer] doc update',2021-02-17T18:11:42Z,2021-02-17T23:58:08Z,,,
10240,b'CUDA memory error on increasing the number of generations',2021-02-17T17:47:13Z,2021-04-23T15:02:51Z,,,
10239,"b""Question about (no_decay = ['bias', 'LayerNorm.weight']) in BERT(Transformer-based) """,2021-02-17T16:41:14Z,2021-02-18T11:16:34Z,,,
10238,b'ConvBert not compatible with torch v1.6',2021-02-17T16:18:50Z,2021-04-23T15:02:52Z,,,
10237,b'TransCoder',2021-02-17T15:55:44Z,2021-04-23T15:02:52Z,,,
10236,b'Add m2m100',2021-02-17T15:13:39Z,2021-03-06T16:44:16Z,PR for Model Addition,,
10235,b'[file_utils] do not gobble certain kinds of requests.ConnectionError',2021-02-17T15:08:00Z,2021-03-18T16:37:45Z,,,
10234,b'Request to add Switch Transformer',2021-02-17T14:57:41Z,,New model,,
10233,b'Making TF Longformer-like models compliant with AMP',2021-02-17T14:34:54Z,2021-02-22T14:41:57Z,,,
10232,b'Multilabel Sequence Classification in trainer',2021-02-17T13:02:34Z,,Feature request,,
10231,b'Wav2Vec2 finetune',2021-02-17T12:42:45Z,2021-03-01T09:13:18Z,,,
10230,b'Making TF GPT2 compliant with XLA and AMP',2021-02-17T10:04:03Z,2021-02-18T08:36:02Z,,,
10229,b'Introduce warmup_ratio training argument',2021-02-17T09:43:43Z,2021-02-18T17:23:34Z,,,
10228,b'Converting original T5 to be used in Transformers',2021-02-17T09:33:15Z,2021-02-17T13:29:00Z,,,
10227,b'Showing individual token and corresponding score during beam search',2021-02-17T08:17:19Z,2021-02-18T03:07:25Z,,,
10226,b'Trainer.train() is stuck ',2021-02-17T08:07:28Z,2021-02-22T09:49:47Z,,,
10225,b'[Trainer] memory tracker metrics',2021-02-17T01:32:36Z,2021-02-18T17:27:32Z,,,
10224,"b""No module named 'tasks'""",2021-02-16T23:34:00Z,2021-02-17T03:18:23Z,,ModuleNotFoundError,"ModuleNotFoundError: No module named 'tasks' `"
10223,b'Slow Multi-GPU DDP training with run_clm.py and GPT2',2021-02-16T23:09:12Z,2021-04-23T15:02:54Z,,,
10222,b'the change from single mask to multi mask support for pytorch',2021-02-16T22:06:26Z,,WIP,,
10221,b'T5 relative attention bias: Discrepancy to original implementation',2021-02-16T20:54:44Z,2021-02-17T10:44:59Z,,,
10220,b'fix deprecated reference `tokenizer.max_len` in glue.py',2021-02-16T20:14:20Z,2021-02-24T14:01:29Z,,,
10219,b'[trainer] fix ignored columns logger',2021-02-16T19:04:48Z,2021-02-16T21:35:39Z,,,
10218,b'discrepancy between the Huggingface T5Tokenizer and the original T5tokenizer',2021-02-16T18:37:34Z,2021-04-23T15:02:55Z,,ValueError,"ValueError: This tokenizer does not have a mask token which is necessary for masked language modeling. You should pass `mlm=False` to train on causal language modeling instead."
10217,b'Fix add_token_positions in custom datasets tutorial',2021-02-16T17:12:46Z,2021-02-16T19:00:05Z,,,
10216,b'Making TF Funnel compliant with AMP',2021-02-16T16:59:16Z,2021-02-18T11:29:44Z,,,
10215,b'Factor out methods',2021-02-16T16:34:33Z,2021-02-17T14:53:44Z,,,
10214,b'StopIteration Error when running beam search for squad 2.0',2021-02-16T16:10:05Z,2021-02-17T00:51:47Z,,,
10213,b'Store FLOS as floats to avoid overflow.',2021-02-16T15:56:17Z,2021-02-16T16:15:15Z,,,
10212,b'RuntimeError: Overflow when unpacking long',2021-02-16T14:17:14Z,2021-02-16T16:15:15Z,,RuntimeError,"RuntimeError: Overflow when unpacking long"
10211,b'Making TF XLM-like models XLA and AMP compliant',2021-02-16T13:52:19Z,2021-02-17T17:02:48Z,,,
10210,b'QA Documentation: I got error just copy and pasting documentation',2021-02-16T13:13:29Z,2021-02-17T12:58:07Z,,RuntimeError,"RuntimeError: Could not infer dtype of NoneType"
10209,b'Make TF CTRL compliant with XLA and AMP',2021-02-16T11:55:33Z,2021-02-17T17:54:15Z,,,
10208,b'different behavior for get_input_embeddings()  between 4.2.x  and 4.3.x in Tensorflow',2021-02-16T11:38:23Z,2021-02-16T17:04:36Z,,,
10207,b'Unlock XLA test for TF ConvBert',2021-02-16T11:05:59Z,2021-02-16T12:59:41Z,,,
10206,b'Tokenizer is working different from expected functionality. ',2021-02-16T06:30:02Z,2021-04-23T15:02:56Z,,,
10205,b'set tgt_lang of MBart Tokenizer for summarization',2021-02-16T06:13:18Z,2021-02-16T14:39:37Z,,,
10204,b'1.3GB dataset creates over 107GB of cache file!',2021-02-16T05:51:54Z,2021-02-18T17:02:36Z,,,
10203,b'[run_glue] Add MNLI compatible mode',2021-02-16T05:40:53Z,2021-04-13T11:22:44Z,,,
10202,b'Fast Tokenizers instantiated via vocab/merge files do not respect skip_special_tokens=True',2021-02-16T05:36:08Z,2021-04-17T23:06:58Z,,,
10201,b'Better Fine-Tuning by Reducing Representational Collapse',2021-02-16T03:51:53Z,,Feature request,,
10200,b'Bugfix: Removal of padding_idx in BartLearnedPositionalEmbedding',2021-02-16T03:32:32Z,2021-02-25T11:33:13Z,,,
10199,b'StopIteration error happened',2021-02-15T18:44:37Z,2021-04-23T15:02:57Z,,,
10198,"b""ONNX Export - cannot resolve operator 'Shape' with opsets: ai.onnx v11""",2021-02-15T18:38:22Z,2021-02-15T19:20:07Z,,,
10197,b'Fine-tuning Seq2Seq models for Machine translation',2021-02-15T18:07:41Z,2021-04-23T15:02:57Z,,,
10196,b'[CI] make the examples sub-group of tests run always',2021-02-15T18:00:36Z,2021-02-15T18:01:35Z,,,
10195,b'Specify dataset dtype',2021-02-15T17:48:22Z,2021-02-15T17:57:17Z,,,
10194,b'Uploaded a new model but is not found on the hub.',2021-02-15T16:55:03Z,2021-02-15T17:02:13Z,New model,,
10193,b'Make use of our copy-consistency script for task-specific models',2021-02-15T15:32:12Z,2021-03-05T23:06:55Z,"Good First Issue, Good Second Issue",,
10192,b'run_mlm.py not utilizing TPU',2021-02-15T15:03:24Z,2021-02-15T15:26:18Z,,,
10191,b'Making TF BART-like models XLA and AMP compliant',2021-02-15T14:44:15Z,2021-02-17T16:48:56Z,,,
10190,b'0% GPU usage when using `hyperparameter_search`',2021-02-15T13:48:23Z,2021-02-15T18:39:37Z,,,
10189,b'Fix TF template',2021-02-15T13:27:18Z,2021-02-15T14:21:57Z,,,
10188,b'Failing Multi-GPU trainer test',2021-02-15T12:27:58Z,2021-03-09T23:42:24Z,,RuntimeError,"RuntimeError: NCCL error in: /pytorch/torch/lib/c10d/ProcessGroupNCCL.cpp:784, unhandled system error, NCCL version 2.7.8"
10187,b'Add new model to labels that should not stale',2021-02-15T11:31:17Z,2021-02-15T11:31:29Z,,,
10186,b'Support for DeBERTa V2 models',2021-02-15T11:24:03Z,2021-03-02T15:55:04Z,,**RuntimeError,"**RuntimeError: Error(s) in loading state_dict for DebertaForSequenceClassification:"
10185,b'Saving HF wrapped in Keras',2021-02-15T10:49:27Z,2021-04-23T15:02:58Z,,AttributeError,"AttributeError: Must set `config_class` to use @keras_serializable "
10184,b'Fixing NER pipeline for list inputs.',2021-02-15T10:24:21Z,2021-02-15T11:22:45Z,,,
10183,b'BigBird',2021-02-15T09:22:26Z,2021-03-30T05:51:35Z,,,
10182,b'`super()` does not have `prepare_seq2seq_batch()`  in `transformers/models/rag/tokenization_rag.py` ',2021-02-15T06:45:59Z,2021-02-16T06:18:24Z,,,
10181,b'Inconsistent loss computation?',2021-02-15T01:23:51Z,2021-04-23T15:02:59Z,,,
10180,b'ONNX Export for Fine-Tuned DistilBertForTokenClassification',2021-02-14T18:09:29Z,2021-02-15T18:15:41Z,,,
10179,b'Why is the attention_mask added to the attn_weights instead of multiplying/masking?',2021-02-14T16:28:00Z,2021-02-15T07:33:12Z,,,
10178,b'Fix datasets set_format',2021-02-14T15:34:18Z,2021-02-15T10:49:08Z,,,
10177,b'Loading a model from local files achieves way too lower accuracy in comparison to model downloading',2021-02-14T14:21:05Z,2021-02-17T21:22:37Z,,,
10176,b'Conditional generation with T5',2021-02-14T09:49:34Z,2021-02-14T14:16:28Z,,,
10175,b'Speech2TextTransformer',2021-02-14T08:33:04Z,2021-03-10T16:12:05Z,,,
10174,b'How to train an MBart model from scratch for a new language pair? ',2021-02-14T03:54:31Z,2021-02-15T00:39:18Z,,,
10173,"b'What does the ""<s> token"" mean in Longformer\'s global_attention_mask?'",2021-02-13T23:53:08Z,2021-02-14T00:21:22Z,,,
10172,b'Saving PruneBERT notebook fails to run on torch > 1.5',2021-02-13T17:31:45Z,2021-04-23T15:03:00Z,,AttributeError,"AttributeError: 'tuple' object has no attribute 'is_quantized'"
10171,b'Revert propagation',2021-02-13T13:19:04Z,2021-02-13T13:19:56Z,,,
10170,b'T5 training with Keras: InvalidArgumentError:  logits and labels must have the same first dimension',2021-02-13T12:57:56Z,2021-04-23T15:03:02Z,,tensorflow.python.framework.errors_impl.InvalidArgumentError,"tensorflow.python.framework.errors_impl.InvalidArgumentError:  logits and labels must have the same first dimension, got logits shape [8192,64] and labels shape [1024]"
10169,b'run_langauge_modeling for T5',2021-02-13T11:57:31Z,2021-04-23T15:03:03Z,,,
10168,"b""NER pipeline doesn't work for a list of sequences""",2021-02-13T11:52:50Z,2021-02-15T11:22:45Z,,ValueError,"ValueError: expected sequence of length 16 at dim 1 (got 38)"
10167,b'[RAG] fix tokenizer',2021-02-13T07:02:43Z,2021-02-15T14:18:12Z,,,
10166,b'[tests] failing test only when run in a group',2021-02-13T05:47:33Z,2021-03-09T17:49:17Z,"Good First Issue, Good Second Issue",,
10165,b'[example scripts] inconsistency around eval vs val',2021-02-13T04:03:13Z,2021-04-26T16:24:31Z,"Examples, WIP",,
10164,b'[example scripts] disambiguate language specification API',2021-02-13T02:46:20Z,2021-03-18T02:01:30Z,,,
10163,b'Increasing gradient accummulation steps significantly slows down training',2021-02-13T01:42:35Z,2021-02-13T02:12:39Z,,,
10162,b'fix run_seq2seq.py; porting trainer tests to it',2021-02-13T01:29:17Z,2021-02-15T17:12:17Z,,,
10161,"b'Seq2seq now has larger memory requirements, OOM w/Deepspeed on previously runnable models '",2021-02-12T21:48:04Z,2021-04-23T15:03:05Z,DeepSpeed,RuntimeError,"RuntimeError: CUDA out of memory. Tried to allocate 256.00 MiB (GPU 2; 39.59 GiB total capacity; 37.87 GiB already allocated; 40.69 MiB free; 37.88 GiB reserved in total by PyTorch)"
10160,b'past_key_values tuple index out of range error when using text2text-generation pipeline with encoder-decoder model',2021-02-12T21:24:54Z,2021-05-17T15:03:26Z,,IndexError,"IndexError: tuple index out of range"
10159,b'[hf_api] delete deprecated methods and tests',2021-02-12T20:30:39Z,2021-02-12T20:35:06Z,,,
10158,b'Multiple Mask support in Pipeline',2021-02-12T19:26:25Z,,Feature request,,
10157,b'Fix typo in comments',2021-02-12T17:44:50Z,2021-02-13T13:26:01Z,,,
10156,b'Fix typo in comment',2021-02-12T17:43:03Z,2021-02-13T13:26:26Z,,,
10155,b'rfc: integration tests need non-example application for testing',2021-02-12T17:40:33Z,2021-03-18T02:04:20Z,"Tests, Benchmarks",,
10154,b'Add mBART-50',2021-02-12T15:31:26Z,2021-02-15T15:28:55Z,,,
10153,b'I-BERT model support',2021-02-12T15:26:19Z,2021-02-25T15:06:42Z,,,
10152,b'Reduce the time spent for the TF slow tests',2021-02-12T14:20:11Z,2021-02-18T14:52:57Z,,,
10151,b'Model Parallelism for Bert Models',2021-02-12T12:22:20Z,2021-02-22T09:49:26Z,Model Parallel,TypeError,"TypeError: forward() takes 1 positional argument but 2 were given"
10150,b'Problem with evaluation_strategy',2021-02-12T10:42:58Z,2021-02-15T09:42:37Z,,,
10149,b'Issue using num_beams parameter for T5 / DeepSpeed',2021-02-12T08:16:07Z,2021-02-12T21:53:16Z,,"ValueError, ValueErrorValueError","ValueError: Some specified arguments are not used by the HfArgumentParser: ['--num_beams', '8']ValueErrorValueError: : Some specified arguments are not used by the HfArgumentParser: ['--num_beams', '8']Some specified arguments are not used by the HfArgumentParser: ['--num_beams', '8']"
10148,b'Fix typo in GPT2DoubleHeadsModel docs',2021-02-12T05:27:17Z,2021-02-12T17:18:39Z,,,
10147,b'BERT with regression head cannot fit one datapoint',2021-02-11T20:07:01Z,2021-02-19T05:04:59Z,,,
10146,b'Model not training beyond 1st epoch',2021-02-11T19:19:08Z,2021-02-14T16:35:03Z,,,
10145,b'Add Fine-Tuning for Wav2Vec2',2021-02-11T17:36:51Z,2021-03-01T09:13:18Z,,,
10144,b'T5 Base length of Tokenizer not equal config vocab_size ',2021-02-11T16:35:31Z,2021-04-25T15:03:18Z,,,
10143,"b'context manager for seeding, or generating fixed random tensor.'",2021-02-11T16:10:42Z,2021-04-25T15:03:19Z,,,
10142,b'T5 GPU Runtime Degradation',2021-02-11T13:48:50Z,2021-03-03T09:42:41Z,,,
10141,b'Add AMP for TF Albert',2021-02-11T12:58:16Z,2021-02-15T16:18:34Z,,,
10140,b'Direct way to apply different learning rate for different group of parameters in Trainer.',2021-02-11T12:20:02Z,2021-02-16T14:58:45Z,,,
10139,"b'ValueError: `Checkpoint` was expecting a trackable object (an object derived from `TrackableBase`), got GPT2LMHeadModel'",2021-02-11T11:09:07Z,2021-02-11T11:29:22Z,,ValueError,"ValueError: `Checkpoint` was expecting a trackable object (an object derived from `TrackableBase`), got GPT2LMHeadModel("
10138,b'Back Translation',2021-02-11T10:55:30Z,2021-02-12T12:16:25Z,,,
10137,"b'Text to Speech Generalized End-To-End Loss for Speaker Verification, Real Time Voice Cloning'",2021-02-11T10:31:55Z,,"New model, Feature request",,
10136,b'[WIP][examples/seq2seq] move old s2s scripts to legacy',2021-02-11T09:18:12Z,2021-02-15T18:48:03Z,,,
10135,b'Adding end-to-end retriever training to RAG with RAY implementation. ',2021-02-11T08:50:04Z,2021-04-25T10:20:03Z,,,
10134,b'cant install from source',2021-02-11T08:46:29Z,2021-02-15T11:00:07Z,,,
10133,b'[examples/run_s2s] remove task_specific_params and update rouge computation',2021-02-11T08:17:06Z,2021-02-12T11:48:22Z,,,
10132,b'Where the helsinki models downloaded to? when using the pretrained models',2021-02-11T07:28:54Z,2021-02-11T11:46:11Z,,,
10131,b'Trainer Evaluates at every step',2021-02-11T06:02:41Z,2021-04-25T15:03:21Z,,,
10130,b'[DeepSpeed in notebooks] Jupyter + Colab',2021-02-11T05:15:06Z,2021-02-11T22:02:05Z,DeepSpeed,,
10129,b'Fix v2 model loading issue',2021-02-11T03:08:08Z,2021-02-15T10:13:04Z,,,
10128,b'Bug in numpy_pad_and_concatenate',2021-02-11T01:30:59Z,2021-04-14T15:47:54Z,,,
10127,b'XLM-R tokenizer is none',2021-02-10T23:03:59Z,2021-04-25T15:03:22Z,,,
10126,b'Add new community notebook - Blenderbot',2021-02-10T22:02:52Z,2021-02-11T09:53:40Z,,,
10125,b'Converted pytorch model to onnx does not work correctly',2021-02-10T20:38:11Z,2021-04-25T15:03:23Z,,,
10124,b'[Doc] Fix version control in internal pages',2021-02-10T20:18:10Z,2021-02-13T13:52:30Z,,,
10123,b'Help on training TFBERT to IntegerEncoded sequences',2021-02-10T19:15:22Z,2021-02-12T13:04:50Z,,,
10122,b'Add SageMakerTrainer for model paralellism',2021-02-10T16:29:21Z,2021-02-11T23:44:18Z,,,
10121,b'Allow `do_lower_case=True` for any tokenizer',2021-02-10T15:28:24Z,,Feature request,,
10120,b'Conversion from slow to fast for BPE spm vocabs contained an error.',2021-02-10T13:42:15Z,2021-02-13T13:24:53Z,,,
10119,b'Line endings should be LF across repo and not CRLF',2021-02-10T08:38:53Z,2021-02-10T15:50:01Z,,,
10118,b'Exporting transformers models in ONNX format',2021-02-10T07:33:10Z,2021-04-25T15:03:25Z,,ModuleNotFoundError,ModuleNotFoundError: No module named '__main__.file_utils'; '__main__' is not a package
10117,b'[Wav2Vec2] Improve Tokenizer & Model for batched inference',2021-02-10T07:04:51Z,2021-02-11T12:40:55Z,,,
10116,b'[scheduled github CI] add deepspeed fairscale deps',2021-02-10T06:17:53Z,2021-02-10T08:12:27Z,DeepSpeed,,
10115,b'[CI] build docs faster',2021-02-10T06:10:48Z,2021-02-10T08:02:40Z,,,
10114,b'[DeepSpeed] restore memory for evaluation',2021-02-10T05:43:36Z,2021-02-10T17:09:49Z,DeepSpeed,,
10113,b'CUDA Out of Memory After Several Epochs',2021-02-10T03:58:06Z,2021-05-19T15:09:07Z,,"max_grad_norm, RuntimeError","max_grad_norm: 1RuntimeError: CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 23.88 GiB total capacity; 22.53 GiB already allocated; 86.38 MiB free; 23.21 GiB reserved in total by PyTorch)"
10112,b'Possible bug in RAG Tokenizer',2021-02-10T02:12:21Z,2021-02-16T17:06:00Z,,AttributeError,"AttributeError: 'super' object has no attribute 'prepare_seq2seq_batch'"
10111,b'Bug in RAG Sequence generate ',2021-02-10T02:09:17Z,2021-04-25T15:03:27Z,,,
10110,b'Fix tokenizers training in notebooks',2021-02-10T02:00:20Z,2021-02-10T02:48:22Z,,,
10109,b'Git does not find the model folder and does not commit model files in the hugging face',2021-02-09T23:26:26Z,2021-04-15T04:44:07Z,,,
10108,b'Non-JSON-serializable tokenizer config with `save_pretrained` ',2021-02-09T22:43:47Z,2021-02-13T10:05:50Z,,TypeError,"TypeError: Object of type BertConfig is not JSON serializable"
10107,b'Remove speed metrics from default compute objective [WIP]',2021-02-09T21:03:49Z,2021-02-10T00:03:03Z,,,
10106,"b'Revert ""Fix TFConvBertModelIntegrationTest::test_inference_masked_lm Test""'",2021-02-09T20:39:10Z,2021-04-01T08:54:48Z,,,
10105,b'PruneTrain: Fast Neural Network Training by Dynamic Sparse Model Reconfiguration',2021-02-09T19:52:34Z,,"New model, Feature request",,
10104,b'Fix TFConvBertModelIntegrationTest::test_inference_masked_lm Test',2021-02-09T19:07:39Z,2021-02-09T19:22:54Z,,,
10103,b'Fix Faiss Import',2021-02-09T18:24:16Z,2021-02-09T18:43:42Z,,,
10102,b'Replace faiss cpu by faiss',2021-02-09T18:06:59Z,2021-02-11T09:46:35Z,,,
10101,b'Change dependency from faiss-cpu to faiss',2021-02-09T17:58:51Z,2021-02-09T18:06:57Z,,,
10100,b'Fix some edge cases in report_to and add deprecation warnings',2021-02-09T15:07:34Z,2021-02-09T15:38:13Z,,,
10099,b'Issue training Longformer ',2021-02-09T14:04:53Z,2021-04-24T15:02:23Z,,,
10098,b'Adding support for TFEncoderDecoderModel',2021-02-09T13:31:55Z,2021-04-24T15:02:24Z,,,
10097,"b'DeBERTa v2 throws ""TypeError: stat: path should be string..."", v1 not'",2021-02-09T11:19:33Z,2021-04-24T15:02:24Z,,TypeError,"TypeError: stat: path should be string, bytes, os.PathLike or integer, not NoneType"
10096,b'Fix example in Wav2Vec2 documentation',2021-02-09T10:48:51Z,2021-02-09T11:07:57Z,,,
10095,b'Fix naming in TF MobileBERT',2021-02-09T10:27:39Z,2021-02-09T11:10:32Z,,,
10094,b'[RAG] fix generate',2021-02-09T10:25:17Z,2021-02-09T18:57:38Z,,,
10093,b'Pre-Training for Question Generation',2021-02-09T10:18:28Z,2021-02-09T12:42:02Z,,,
10092,b'Logging propagation',2021-02-09T09:45:26Z,2021-02-09T15:27:50Z,,,
10091,b'How to run distributed training on multiple machines?',2021-02-09T08:37:34Z,2021-02-09T14:22:36Z,,,
10090,b'[question] Are the tensorflow bert weights same as the original repo ?',2021-02-09T08:34:43Z,2021-04-24T15:02:25Z,,,
10089,b'Deprecate Wav2Vec2ForMaskedLM and add Wav2Vec2ForCTC',2021-02-09T08:25:54Z,2021-02-09T08:49:02Z,,,
10088,b'Language modelling head has zero weights in pretrained TFMobileBertForMaskedLM',2021-02-09T08:04:41Z,2021-04-14T16:52:32Z,,,
10087,b'remove adjust_logits_during_generation method',2021-02-09T05:52:28Z,2021-02-10T17:09:10Z,,,
10086,b'doc: update W&B related doc',2021-02-09T05:28:38Z,2021-02-09T19:47:52Z,,,
10085,b'[examples/s2s] add test set predictions',2021-02-09T04:09:42Z,2021-02-09T15:11:42Z,,,
10084,b'Tapas not working with tables exceeding token limit',2021-02-08T23:41:00Z,2021-04-24T15:02:26Z,,,
10083,b'model.generate needs BART config update',2021-02-08T22:21:59Z,2021-02-12T17:46:15Z,,,
10082,b'Supporting truncation from both ends of the sequence in BertTokenizerFast',2021-02-08T21:29:32Z,,Feature request,,
10081,"b'pipeline(""sentiment-analysis\') - index out of range in self'",2021-02-08T21:07:10Z,2021-02-09T09:46:55Z,,,
10080,b'[deepspeed tests] transition to new tests dir',2021-02-08T18:56:14Z,2021-02-08T20:41:53Z,,,
10079,"b'Unclear error ""NotImplementedError:  ""while saving tokenizer. How fix it?'",2021-02-08T17:49:49Z,2021-04-24T15:02:27Z,,NotImplementedError,"NotImplementedError: "
10078,b'Replace strided slice with tf.expand_dims',2021-02-08T17:15:56Z,2021-02-09T16:48:29Z,,,
10077,b'Update tokenizers requirement',2021-02-08T17:15:02Z,2021-02-08T17:27:27Z,,,
10076,b'[tests] where to put deepspeed + fairscale tests',2021-02-08T16:51:39Z,2021-02-08T20:41:53Z,,,
10075,b'assertion failed: [predictions must be >= 0]',2021-02-08T16:41:28Z,2021-02-10T11:18:10Z,,tensorflow.python.framework.errors_impl.InvalidArgumentError,"tensorflow.python.framework.errors_impl.InvalidArgumentError: 2 root error(s) found."
10074,"b""AttributeError: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'new_ones'""",2021-02-08T15:05:35Z,2021-02-08T15:17:19Z,,AttributeError,"AttributeError: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'new_ones'"
10073,b'Added integration tests for Pytorch implementation of the ELECTRA model',2021-02-08T14:22:43Z,2021-02-08T20:42:26Z,,,
10072,b'Fixing model templates',2021-02-08T13:55:31Z,2021-02-08T14:07:02Z,,,
10071,b'Fix mlflow param overflow clean',2021-02-08T13:38:40Z,2021-02-08T16:58:03Z,,,
10070,b'remove token_type_ids from TokenizerBertGeneration output',2021-02-08T13:32:06Z,2021-02-08T18:05:33Z,,,
10069,b'Fix TF template',2021-02-08T12:27:03Z,2021-02-08T13:10:51Z,,,
10068,b'Integrating GPT-2 model with Web page',2021-02-08T11:54:17Z,2021-04-24T15:02:28Z,,,
10067,"b'""Connection error, and we cannot find the requested files in the cached path."" ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.'",2021-02-08T11:36:32Z,2021-03-09T17:02:54Z,,ValueError,"ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on."
10066,"b'Removing run_pl_glue.py from text classification docs, include run_xnli.py & run_tf_text_classification.py'",2021-02-08T11:34:17Z,2021-02-08T18:04:22Z,,,
10065,b'Model templates tests are run twice',2021-02-08T10:52:41Z,2021-02-25T00:23:39Z,,,
10064,b'Fix model template typo',2021-02-08T10:49:52Z,2021-02-08T11:02:06Z,,,
10063,b'[Finetune Seq2Seq Trainer] fix bert2bert test',2021-02-08T10:25:41Z,2021-02-08T13:04:29Z,,,
10062,b'Disable temporarily too slow tests (Longformer/LED)',2021-02-08T10:14:27Z,2021-02-08T11:32:32Z,,,
10061,b'Dimension error while finetuning longformer with roberta-large EncoderDecoderModel ',2021-02-08T10:06:59Z,2021-02-09T08:36:01Z,,RuntimeError,"RuntimeError: Caught RuntimeError in replica 0 on device 0."
10060,b'[BART Tests] Fix Bart mask filling pipeline tests',2021-02-08T10:03:21Z,2021-02-08T10:25:09Z,,,
10059,b'Fix slow dpr test',2021-02-08T09:42:11Z,2021-02-08T09:43:26Z,,,
10058,b'When encoding text to feature vectors - Would be awesome to be able to use the simplest tokenizer with a split on spaces',2021-02-08T08:27:28Z,2021-04-24T15:02:28Z,,AttributeError,"AttributeError: 'BasicTokenizer' object has no attribute 'encode'"
10057,b'Fixed docs for the shape of `scores` in `generate()`',2021-02-08T07:08:55Z,2021-05-02T08:10:47Z,,,
10056,b'Play around with mask-filling of original model',2021-02-07T20:57:31Z,2021-04-21T16:39:58Z,,,
10055,b'Cannnot train Roberta: 2 different errors',2021-02-07T20:24:24Z,2021-02-11T23:51:36Z,,"TypeError, AttributeError","TypeError: in user code:AttributeError: 'DataFrame' object has no attribute 'shuffle'"
10054,"b'Error: ""Transformers CLI tool: error: unrecognized arguments: kvantorium-small""  while deploying machine learning model to hugging face profile'",2021-02-07T18:26:36Z,2021-04-23T15:03:06Z,,,
10053,b'Add CharacterBERT model [WIP]',2021-02-07T17:50:10Z,,WIP,,
10052,b'implementing tflxmertmodel integration test',2021-02-07T16:46:54Z,2021-05-17T15:03:29Z,,,
10051,b'[example] run_ner.py raised error: IndexError: Target 3 is out of bounds.',2021-02-07T14:24:12Z,2021-04-23T15:03:08Z,,"IndexError, TypeError","IndexError: Target 3 is out of bounds.TypeError: 'NoneType' object is not callable"
10050,b'run_ner.py fails when loading a model/checkpoint from a directory',2021-02-07T13:47:41Z,2021-02-08T11:26:05Z,,IndexError,"IndexError: Target 3 is out of bounds."
10049,b'Installing tf2.0 in my env but still get ImportError in my code',2021-02-07T13:35:34Z,2021-04-23T15:03:08Z,,,
10048,"b""ImportError: cannot import name 'list_datasets'""",2021-02-07T12:44:56Z,2021-02-08T14:35:27Z,,ImportError,"ImportError: cannot import name 'list_datasets'"
10047,b'Can you give some suggestion about add features with input_ids to token-classification model ?',2021-02-07T02:30:33Z,2021-04-23T15:03:09Z,,,
10046,b'[s2s examples] Replace -100 token ids with the tokenizer pad_id for compute_metrics',2021-02-06T23:42:55Z,2021-02-08T15:08:17Z,,OverflowError,"OverflowError: out of range integral type conversion attempted"
10045,b'BertGenerationTokenizer provides an unexpected value for BertGenerationModel',2021-02-06T08:01:29Z,2021-02-08T18:05:33Z,,,
10044,b'[s2s examples] dataset porting',2021-02-06T06:53:09Z,2021-02-20T00:17:57Z,,,
10043,b'[s2s examples] README.md fixes',2021-02-06T06:13:23Z,2021-02-08T01:51:35Z,,,
10042,b'Pegasus ONNX format?',2021-02-06T06:06:01Z,2021-04-23T15:03:10Z,,,
10041,"b""[s2s] Can't mix --fp16 and --device cpu""",2021-02-06T03:58:52Z,2021-02-08T01:54:21Z,,RuntimeError,"RuntimeError: ""threshold_cpu"" not implemented for 'Half'"
10040,b'seq2seq: fail gracefully when predicting using --device cpu and --fp16',2021-02-06T03:07:33Z,2021-02-08T01:54:20Z,,,
10039,b'[trainer] deepspeed bug fixes and tests',2021-02-06T01:34:02Z,2021-02-08T17:44:03Z,DeepSpeed,,
10038,b'[examples s2s] run_seq2seq.py tweaks',2021-02-05T22:20:31Z,2021-03-18T01:26:55Z,,,
10037,b'[examples] make run scripts executable',2021-02-05T22:12:39Z,2021-02-05T23:51:19Z,,,
10036,b'[s2s examples] convert existing scripts to run_seq2seq.py from finetune_trainer.py',2021-02-05T21:35:27Z,2021-04-23T15:03:11Z,Examples,,
10035,b'Cannot import DataCollatorForSeq2Seq from Transformers library',2021-02-05T17:16:31Z,2021-02-05T17:31:00Z,,ImportError,"ImportError: cannot import name 'DataCollatorForSeq2Seq' from 'transformers' (unknown location)"
10034,b'Truncate max length if needed in all examples',2021-02-05T16:17:20Z,2021-02-08T10:03:56Z,,,
10033,b'A few fixes in the documentation',2021-02-05T15:46:53Z,2021-02-08T10:02:02Z,,,
10032,b'generation length always equal to 20 when using run_seq2seq.py script',2021-02-05T15:37:53Z,2021-02-09T09:22:45Z,,,
10031,"b""AttributeError: module 'transformers' has no attribute 'PegasusForCausalLM'""",2021-02-05T15:03:29Z,2021-02-05T15:15:22Z,,AttributeError,"AttributeError: module transformers has no attribute PegasusForCausalLM"
10030,b'Check copies match full class/function names',2021-02-05T14:53:03Z,2021-02-08T09:58:25Z,,,
10029,b'Override Default Params on QnA Pipeline',2021-02-05T14:48:36Z,2021-02-06T18:30:56Z,,,
10028,b'custom JSON data breaks run_seq2seq.py',2021-02-05T14:14:45Z,2021-02-05T15:10:02Z,,AttributeError,"AttributeError: 'list' object has no attribute 'keys'"
10027,b'Bump minimum Jax requirement to 2.8.0',2021-02-05T12:49:13Z,2021-02-05T13:20:27Z,,,
10026,"b'T5 doubling training time per iteration from save_steps to save_steps (1st 100 steps 33s/it - then, 75s/it)'",2021-02-05T12:48:59Z,2021-04-23T15:03:12Z,,,
10025,b'Check TF ops for ONNX compliance',2021-02-05T12:46:23Z,2021-02-15T12:55:10Z,,,
10024,b'Datasets library not suitable for huge text datasets',2021-02-05T12:16:03Z,2021-04-23T15:03:12Z,,,
10023,b'Accessing language modeling script checkpoint model and tokenizer for finetuning',2021-02-05T12:09:57Z,2021-04-23T15:03:13Z,,"ValueError, RuntimeError","ValueError: Unrecognized model in /output_dir/checkpoint-1000/. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: led, blenderbot-small, retribert, mt5, t5, mobilebert, distilbert, albert, bert-generation, camembert, xlm-roberta, pegasus, marian, mbart, mpnet, bart, blenderbot, reformer, longformer, roberta, deberta, flaubert, fsmt, squeezebert, bert, openai-gpt, gpt2, transfo-xl, xlnet, xlm-prophetnet, prophetnet, xlm, ctrl, electra, encoder-decoder, funnel, lxmert, dpr, layoutlm, rag, tapasRuntimeError: Error(s) in loading state_dict for XLMRobertaForSequenceClassification:"
10022,b'Added integration tests for Pytorch implementation of the FlauBert model',2021-02-05T11:29:58Z,2021-02-08T09:36:51Z,,,
10021,b'Clarify QA pipeline output based on character',2021-02-05T10:30:02Z,2021-02-05T10:40:31Z,,,
10020,b'Protobuf',2021-02-05T09:25:55Z,2021-04-23T15:03:14Z,,ImportError,"ImportError: "
10019,b'Tokenizer Batch decoding  of predictions obtained from model.generate in t5',2021-02-05T08:02:32Z,2021-02-08T10:02:02Z,,,
10018,b'Integrate DeBERTa v2(the 1.5B model surpassed human performance on Su\xe2\x80\xa6',2021-02-04T23:55:08Z,2021-02-19T23:34:45Z,,,
10017,b'python utils/check_repo.py fails',2021-02-04T22:43:20Z,2021-02-05T13:20:27Z,,ImportError,"ImportError: cannot import name 'cusolver' from 'jaxlib' (/home/stas/anaconda3/envs/main-38/lib/python3.8/site-packages/jaxlib/__init__.py)"
10016,b'Feature-extraction pipeline to return Tensor',2021-02-04T22:12:05Z,2021-04-23T15:03:15Z,,,
10015,b'Do not allow fine tuning with sequence size larger than during training',2021-02-04T21:42:37Z,2021-02-08T10:03:56Z,,,
10014,b'Update doc for pre-release',2021-02-04T21:42:34Z,2021-02-04T21:52:27Z,,,
10013,b'[Question] Pipeline QA start_index',2021-02-04T20:14:49Z,2021-02-05T10:21:50Z,,,
10012,b'return_dict scores are inconsistent between sampling and beam search',2021-02-04T19:23:40Z,2021-07-04T15:02:27Z,,,
10011,b'OOM when trying to fine tune patrickvonplaten/led-large-16384-pubmed',2021-02-04T18:28:09Z,2021-04-23T15:03:17Z,DeepSpeed,RuntimeError,"RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 15.78 GiB total capacity; 13.96 GiB already allocated; 20.00 MiB free; 14.56 GiB reserved in total by PyTorch)"
10010,b'Problem fine-tuning BERTweet',2021-02-04T18:12:36Z,2021-02-04T21:37:22Z,,,
10009,b'Why two separators?',2021-02-04T17:04:37Z,2021-02-04T19:35:36Z,,,
10008,"b""[models] why aren't .bin files compressed for faster download?""",2021-02-04T16:52:52Z,2022-01-26T16:48:42Z,WIP,,
10007,b'Fix TF LED/Longformer attentions computation',2021-02-04T16:49:36Z,2021-02-10T15:58:38Z,,,
10006,b'run_ner.py raised error',2021-02-04T16:02:04Z,2021-02-05T15:57:39Z,,ReferenceError,"ReferenceError: {'help': 'The name of the task (ner, pos...).'} does not reference a class __dict__"
10005,b'[License info] Longformer SQuAD finetuned model',2021-02-04T14:49:36Z,2021-02-10T19:56:25Z,New model,,
10004,b'Converting wav2vec2-base-960h to ONNX report an error while converting',2021-02-04T14:46:23Z,2021-04-23T15:03:18Z,,,
10003,b'Hotfixing tests',2021-02-04T14:35:26Z,2021-02-04T16:41:35Z,,,
10002,b'Cleaning up `ConversationalPipeline` to support more than DialoGPT.',2021-02-04T14:22:17Z,2021-02-08T11:29:07Z,,,
10001,b'BART CausalLM example',2021-02-04T12:58:47Z,2021-04-23T15:03:18Z,,,
10000,b'German DistilBertModel raises an issue',2021-02-04T12:54:33Z,2021-02-04T13:14:01Z,,RuntimeError,"RuntimeError: a view of a leaf Variable that requires grad is being used in an in-place operation."
9999,b'Fix model templates',2021-02-04T10:51:50Z,2021-02-04T12:47:27Z,,,
9998,b'Add DETR',2021-02-04T10:17:17Z,2021-04-23T15:03:19Z,,,
9997,"b'Remove unintentional ""double"" assignment  in TF-BART like models'",2021-02-04T10:08:04Z,2021-02-04T15:24:48Z,,,
9996,b'[DeepSpeed] [success] trained t5-11b on 1x 40GB gpu',2021-02-04T06:21:40Z,2021-02-08T19:34:16Z,DeepSpeed,,
9995,"b""Added Integration testing for Pytorch implementation of DistilBert model from issue #9948'""",2021-02-04T04:36:00Z,2021-02-04T09:25:00Z,,,
9994,b'\xf0\x9f\x9a\x80 Faster batch translation with FSMT model',2021-02-04T03:49:51Z,2021-02-04T07:50:48Z,,,
9993,b'[trainer] a few fixes',2021-02-04T03:40:19Z,2021-02-04T15:44:57Z,,,
9992,b'Adversarial/amnesic heads',2021-02-04T02:34:49Z,,Feature request,,
9991,b'[documentation] non-PR doc editing',2021-02-04T00:24:45Z,2021-03-18T02:01:51Z,,,
9990,b'Implementing the test integration of BertGeneration',2021-02-03T23:07:04Z,2021-02-08T13:22:19Z,,,
9989,b'create LxmertModelIntegrationTest Pytorch',2021-02-03T22:57:51Z,2021-07-05T09:21:25Z,,,
9988,b'Add head_mask and decoder_head_mask to TF LED',2021-02-03T21:24:26Z,2021-02-09T16:45:19Z,,,
9987,b'Add `from_slow` in fast tokenizers build and fixes some bugs',2021-02-03T21:01:47Z,2021-02-04T08:34:23Z,,,
9986,b'How to train on shards of bookcorpus + wikipedia + openwebtext on 1 TB disk.',2021-02-03T19:42:40Z,2021-02-26T12:25:39Z,,,
9985,b'Loss function inputs for DistilBertForTokenClassification-like model using DistilBertModel',2021-02-03T19:23:16Z,2021-04-23T15:03:21Z,wontfix,,
9984,b'[Proposal] Adding new `encoder_no_repeat_ngram_size` to `generate`.',2021-02-03T18:13:41Z,2021-02-04T14:00:19Z,,,
9983,b'Added integration tests for Pytorch implementation of the FlauBert model',2021-02-03T17:41:59Z,2021-02-05T11:30:16Z,,,
9982,b'Added integration tests for Pytorch implementation of the ELECTRA model',2021-02-03T17:03:35Z,2021-02-08T14:23:07Z,,,
9981,"b""Can't make sense of encoding for a downloadable AutoTokenizer""",2021-02-03T16:40:21Z,2021-03-06T00:11:59Z,wontfix,,
9980,b'Added integration tests for Pytorch implementation of the ALBERT model',2021-02-03T16:21:49Z,2021-02-03T16:41:11Z,,,
9979,b'Added integration tests for TensorFlow implementation of the MPNet model',2021-02-03T15:21:26Z,2021-02-03T16:39:41Z,,,
9978,b'Added integration tests for TensorFlow implementation of the mobileBERT',2021-02-03T14:57:50Z,2021-02-03T16:36:46Z,,,
9977,b'[run_clm.py] fix getting extention',2021-02-03T14:23:02Z,2021-02-03T14:44:44Z,,,
9976,b'Added integration tests for TensorFlow implementation of the ALBERT model',2021-02-03T14:17:01Z,2021-02-03T14:49:19Z,,,
9975,b'TF DistilBERT integration tests ',2021-02-03T14:15:02Z,2021-02-03T14:51:01Z,,,
9974,"b""Make use of attention_mask in Trainer's compute_metrics""",2021-02-03T10:15:35Z,2021-02-04T15:26:54Z,,,
9973,b'attention_mask -> encoder_attention_mask in cross attn of BERT-like models',2021-02-03T10:08:32Z,2021-07-02T15:05:41Z,wontfix,,
9972,b'Fix GroupedLinearLayer in TF ConvBERT',2021-02-03T09:34:30Z,2021-02-03T09:49:07Z,,,
9971,b'DebertaForSequenceClassification documents  examples  report RuntimeError: Index tensor must have the same number of dimensions as input tensor',2021-02-03T08:18:30Z,2021-02-22T02:40:39Z,,RuntimeError,"RuntimeError: Index tensor must have the same number of dimensions as input tensor"
9970,b'[research proj] [lxmert] remove bleach dependency',2021-02-03T05:53:00Z,2021-02-03T10:24:41Z,,,
9969,b'fix steps_in_epoch variable in trainer when using max_steps',2021-02-03T03:31:10Z,2021-02-03T14:30:38Z,,,
9968,b'Disk memory management',2021-02-02T23:33:03Z,2021-03-06T00:12:02Z,wontfix,,
9967,b'Added an integration test for the Pytorch implementation of the DistilBERT model from issue #9948',2021-02-02T23:21:42Z,2021-02-04T04:07:14Z,,,
9966,b'Bump bleach from 3.1.5 to 3.3.0 in /examples/research_projects/lxmert',2021-02-02T23:10:51Z,2021-02-03T10:25:40Z,dependencies,,
9965,b'[trainer] new in pytorch: `torch.optim._multi_tensor` faster optimizers ',2021-02-02T19:02:53Z,,"Performance, WIP",,
9964,"b'Add head_mask, decoder_head_mask, cross_head_mask to ProphetNet'",2021-02-02T18:54:19Z,2021-04-25T09:06:16Z,wontfix,,
9963,b'Model Save/Load Fails for Hadoop File Server',2021-02-02T17:50:22Z,2021-03-06T00:12:06Z,wontfix,,
9962,b'Deepseep configs keys probelm',2021-02-02T17:06:39Z,2021-02-02T18:18:37Z,DeepSpeed,,
9961,b'What is the correct way to use Adafactor?',2021-02-02T15:42:08Z,2021-03-06T00:12:07Z,wontfix,,
9960,b'How to resize RobertaLMHead with pretrained weights? ',2021-02-02T14:59:12Z,2021-02-03T12:27:23Z,,,
9959,b'Problem while initializing custom model with added tokens ',2021-02-02T14:15:53Z,2021-02-02T14:59:34Z,,,
9958,b'tokenizer is slow when adding new tokens',2021-02-02T13:39:50Z,2021-03-06T00:12:08Z,wontfix,,
9957,b'[mBART] one slow integration test is failing on master',2021-02-02T10:59:23Z,2021-05-27T15:10:28Z,,,
9956,b'[Good first issue] MPNet TensorFlow Integration tests',2021-02-02T10:18:25Z,2021-02-03T16:39:41Z,"TensorFlow, Tests, Good First Issue",,
9955,b'[Good first issue] MobileBERT TensorFlow Integration tests',2021-02-02T10:17:15Z,2021-02-03T16:36:46Z,"TensorFlow, Tests, Good First Issue",,
9954,b'[Good first issue] LXMERT TensorFlow Integration tests',2021-02-02T10:16:07Z,2021-10-26T21:44:07Z,"TensorFlow, Tests, Good First Issue",,
9953,b'[Good first issue] DistilBERT TensorFlow Integration tests',2021-02-02T10:14:54Z,2021-02-03T14:51:01Z,"TensorFlow, Tests, Good First Issue",,
9952,b'[Good first issue] MPNet PyTorch Integration tests',2021-02-02T10:11:16Z,2021-02-03T16:41:52Z,"PyTorch, Tests, Good First Issue",,
9951,b'[Good first issue] LXMERT PyTorch Integration tests',2021-02-02T10:10:26Z,,"PyTorch, Tests, Good First Issue",,
9950,b'[Good first issue] FlauBERT PyTorch Integration tests',2021-02-02T10:08:50Z,2021-02-08T09:36:51Z,"PyTorch, Tests, Good First Issue",,
9949,b'[Good first issue] ELECTRA PyTorch Integration tests',2021-02-02T10:06:09Z,2021-02-08T20:42:25Z,"PyTorch, Tests, Good First Issue",,
9948,b'[Good first issue] DistilBERT PyTorch Integration tests',2021-02-02T10:04:55Z,2021-02-04T09:25:00Z,"PyTorch, Tests, Good First Issue",,
9947,b'[Good first issue] BERT Generation PyTorch Integration tests',2021-02-02T09:59:08Z,2021-02-10T12:50:50Z,"PyTorch, Tests, Good First Issue",,
9946,b'[Good first issue] ALBERT TensorFlow Integration tests',2021-02-02T09:52:38Z,2021-02-03T14:49:19Z,"TensorFlow, Tests, Good First Issue",,
9945,b'[Good first issue] ALBERT PyTorch Integration tests',2021-02-02T09:50:22Z,2021-02-03T16:41:11Z,"PyTorch, Tests, Good First Issue",,
9944,b'[Bart models] fix typo in naming',2021-02-02T08:53:31Z,2021-02-02T09:22:42Z,,,
9943,b'ALBERT Tokenizer integration test',2021-02-02T08:46:15Z,2021-02-02T09:39:34Z,,,
9942,b'Fix Longformer and LED',2021-02-02T08:01:45Z,2021-02-03T11:26:33Z,,,
9941,b'Converting pretrained tf2 bert model to pytorch model for using FillMaskPipeline',2021-02-02T07:05:03Z,2021-03-06T00:12:10Z,wontfix,,
9940,b'[wip] [pipeline parallel] t5 - experiment #2',2021-02-02T06:28:27Z,2021-06-04T06:16:01Z,"Pipeline Parallel, WIP",,
9939,"b""Can't import pipeline""",2021-02-02T03:00:35Z,2021-02-05T16:27:45Z,,ImportError,"ImportError: cannot import name 'pipeline' from 'transformers' (unknown location)"
9938,b'trainer_seq2seq.py Question',2021-02-02T02:18:08Z,2021-02-02T09:19:58Z,,,
9937,b'ConvBERT: minor fixes for conversion script',2021-02-01T22:45:58Z,2021-02-02T11:09:25Z,,ImportError,"ImportError: attempted relative import with no known parent package"
9936,b'ConvBERT: minor fixes for conversion script',2021-02-01T22:40:37Z,2021-02-01T22:42:28Z,,ImportError,"ImportError: attempted relative import with no known parent package"
9935,b'Use compute_loss in prediction_step',2021-02-01T22:20:43Z,2021-02-02T12:00:18Z,,,
9934,b'Bump numpy',2021-02-01T21:46:53Z,2021-02-02T10:46:33Z,,,
9933,b'Possible bug in `prepare_for_model` when using fast tokenizers',2021-02-01T21:31:11Z,2021-03-06T00:12:14Z,wontfix,AssertionError,"AssertionError: You cannot use ``already_has_special_tokens=False`` with this tokenizer. Please use a slow (full python) tokenizer to activate this argument.Or set `return_special_token_mask=True` when calling the encoding method to get the special tokens mask in any tokenizer. "
9932,b'Fix 9918',2021-02-01T21:23:03Z,2021-02-02T10:22:21Z,,,
9931,b'[2D Parallelism] Tracking feasibility',2021-02-01T19:42:37Z,,"Model Parallel, DeepSpeed, Pipeline Parallel, WIP",,
9930,b'Hyperparameter search w/ RayTune BrokenPipeError: [Errno 32] Broken pipe',2021-02-01T16:43:40Z,2021-02-03T15:17:33Z,,"BrokenPipeError, redis.exceptions.ConnectionError","BrokenPipeError: [Errno 32] Broken piperedis.exceptions.ConnectionError: Error 32 while writing to socket. Broken pipe."
9929,b'Hyperparameter search w/ Optuna CUDA out of memory',2021-02-01T15:55:56Z,2021-02-01T16:35:19Z,,RuntimeError,"RuntimeError: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 14.76 GiB total capacity; 13.66 GiB already allocated; 13.75 MiB free; 13.83 GiB reserved in total by PyTorch)"
9928,b'[Tokenizer Utils Base] Make pad function more flexible',2021-02-01T15:44:19Z,2021-02-02T07:35:28Z,,,
9927,b'Missing None verification in the CLM language modeling example',2021-02-01T12:51:39Z,2021-02-03T14:44:44Z,,,
9926,b'Deploying a transformers pipeline into Google Cloud AI-Platform prediction',2021-02-01T12:11:21Z,2021-02-03T08:00:48Z,,,
9925,b'Implementing ELECTRIC training for ELECTRA',2021-02-01T12:09:42Z,,Feature request,,
9924,b'[docs] fix auto model docs',2021-02-01T11:13:41Z,2021-02-01T13:17:46Z,,,
9923,b'Fix bart conversion script',2021-02-01T11:00:31Z,2021-02-01T16:17:15Z,,,
9922,b'Tensorflow doc changes on loss output size',2021-02-01T10:55:43Z,2021-02-01T16:17:51Z,,,
9921,"b'[Templates] Add template ""call-for-model"" markdown and ""call-for-big-bird"" markdown'",2021-02-01T09:01:14Z,2021-02-05T12:47:55Z,,,
9920,b'Would you like to add convert the generator script by ConvBert',2021-02-01T08:23:52Z,2021-04-24T15:02:32Z,wontfix,,
9919,"b""AttributeError: module 'torch.utils' has no attribute 'checkpoint' for fine tune LED""",2021-02-01T02:32:06Z,2021-02-02T16:45:07Z,,AttributeError,"AttributeError: module 'torch.utils' has no attribute 'checkpoint'"
9918,"b""[doc] transformers.PreTrainedTokenizer.encode() doesn't get resolved to its doc""",2021-02-01T01:44:53Z,2021-02-02T10:22:21Z,,,
9917,b'distilbert: fix creation of sinusoidal embeddings',2021-01-31T22:08:35Z,2021-02-03T16:42:16Z,,RuntimeError,"RuntimeError: a view of a leaf Variable that requires grad is being used in an in-place operation."
9916,b'RAG + DPR model performance issues',2021-01-31T20:35:40Z,2021-03-06T00:12:18Z,wontfix,,
9915,b'prediction_step() is not using compute_loss()',2021-01-31T14:48:23Z,2021-03-06T00:12:19Z,wontfix,,
9914,"b""AttributeError: 'torch.Size' object has no attribute 'as_list'""",2021-01-31T11:39:44Z,2021-03-06T00:12:20Z,wontfix,AttributeError,"AttributeError: 'torch.Size' object has no attribute 'as_list'"
9913,b'Gradient accumulation and distributed parallelism will reduce the effect?',2021-01-31T09:04:52Z,2021-03-06T00:12:22Z,wontfix,,
9912,b'How to add more fields in TrainingArguments',2021-01-31T07:42:02Z,2021-03-06T00:12:23Z,wontfix,,
9911,b'[seq2seq] fix logger format for non-main process',2021-01-31T04:07:41Z,2021-02-01T08:08:13Z,,,
9910,b'Doc title in the template',2021-01-30T23:00:15Z,2021-02-01T08:05:32Z,,,
9909,b'run_seq2seq.py : Why we pad labels with -100?',2021-01-30T22:29:29Z,2021-01-30T23:22:20Z,,,
9908,b'[seq2seq] some logging for all processes in distributed mode',2021-01-30T21:31:11Z,,WIP,,
9907,b'Remove subclass for sortish sampler',2021-01-30T19:51:37Z,2021-02-01T13:06:33Z,,,
9906,"b'Error ""Expected input batch_size (16) to match target batch_size (1440)"" in the WNUT NER example'",2021-01-30T15:16:27Z,2021-02-01T15:34:45Z,,ValueError,"ValueError: Expected input batch_size (16) to match target batch_size (1440)."
9905,b'exe executable file',2021-01-30T14:29:10Z,2021-03-06T00:12:24Z,wontfix,,
9904,b'Tokenizer return offsets',2021-01-30T12:36:42Z,2021-04-24T15:02:33Z,,,
9903,b'Clarify definition of seed argument in TrainingArguments',2021-01-30T11:06:09Z,2021-01-31T16:09:31Z,,,
9902,b'PPLM example - AttributeError issue',2021-01-30T10:27:59Z,2021-04-24T15:02:34Z,,AttributeError,"AttributeError: 'tuple' object has no attribute 'shape'"
9901,b'Missing model license information',2021-01-30T08:55:47Z,2021-02-03T15:25:58Z,,,
9900,"b""run_seq2seq.py doesn't work after enabling sortish sampler""",2021-01-30T06:41:52Z,2021-02-01T13:06:33Z,,AttributeError,"AttributeError: 'Dataset' object has no attribute 'make_sortish_sampler'"
9899,b'Does Sortish Sampler work with multiple GPUs in seq2seq?',2021-01-30T04:15:24Z,2021-02-02T00:14:24Z,,,
9898,b'[doc] nested markup is invalid in rst',2021-01-30T03:50:04Z,2021-01-30T14:59:20Z,,,
9897,b'[t5 tokenizer] add info logs',2021-01-30T02:22:53Z,2021-02-13T14:10:22Z,,,
9896,b'[wandb] restore WANDB_DISABLED=true to disable wandb ',2021-01-30T02:10:06Z,2021-02-01T08:14:07Z,,,
9895,b'TFGPT2LMHeadModel unknown location',2021-01-30T00:17:23Z,2021-01-30T19:12:31Z,,,
9894,"b""ImportError: cannot import name 'PreTrainedEncoderDecoder' from 'transformers' (unknown location)""",2021-01-29T23:44:29Z,2021-01-31T02:26:53Z,,ImportError,"ImportError: cannot import name 'PreTrainedEncoderDecoder' from 'transformers' (unknown location)"
9893,b'rfc: new benchmark tool',2021-01-29T21:00:39Z,,"Benchmarks, WIP",,
9892,b'Seeking clarification on T5  prefix for summarization',2021-01-29T20:54:36Z,2021-02-11T16:24:54Z,,,
9891,b'Remove Token from Vocab?',2021-01-29T19:08:43Z,2021-04-25T15:03:34Z,,,
9890,b'Restore TF embeddings and attention layers to their previous version',2021-01-29T16:35:50Z,2021-02-08T11:36:31Z,,,
9889,b'm2m_100',2021-01-29T14:25:28Z,2021-02-17T15:12:13Z,New model,,
9888,b'[Quick poll] Give your opinion on the future of \xf0\x9f\xa4\x97 transformers: 40k edition!',2021-01-29T12:30:15Z,2021-04-25T15:03:35Z,Discussion,,
9887,b'Fit chinese wwm to new datasets',2021-01-29T10:54:46Z,2021-02-01T08:38:00Z,,,
9886,b'Conversion of BPE tokenizer for Marian models',2021-01-29T10:42:38Z,2021-04-25T15:03:36Z,Fast Tokenizers,,
9885,b'Finetune_Trainer Question',2021-01-29T09:14:03Z,2021-01-29T10:28:22Z,,,
9884,b'Exporting model to onnx increases the model size',2021-01-29T08:26:16Z,2021-02-19T12:01:16Z,,,
9883,"b'examples/seq2seq , where can I find the  definition for the sortish_sampler argument?'",2021-01-29T06:18:06Z,2021-01-29T06:30:05Z,,,
9882,b'Some weights of {} were not initialized from the model checkpoint',2021-01-29T03:09:07Z,2021-02-01T09:34:03Z,,,
9881,b'DeBERTa pretraining using MLM: model gradients become NAN ',2021-01-29T02:30:39Z,2021-02-07T16:46:11Z,,,
9880,b'[trainer] [deepspeed] refactor deepspeed setup devices',2021-01-29T00:46:51Z,2021-01-29T16:18:04Z,,,
9879,b'[seq2seq] correctly handle mt5',2021-01-29T00:05:29Z,2021-01-29T16:11:22Z,,,
9878,b'[DOCS] curl links go to 404 not found in NER tutorial',2021-01-28T22:03:31Z,2021-01-29T19:06:52Z,,,
9877,b'Fix head masking for TFT5 models',2021-01-28T21:47:02Z,2021-02-17T16:00:09Z,,,
9876,b'When on sagemaker use their env variables for saves',2021-01-28T21:11:19Z,2021-01-29T14:52:27Z,,,
9875,"b""Clarify use of unk_token in slow tokenizers' docstrings""",2021-01-28T20:23:16Z,2021-01-29T10:11:54Z,,,
9874,b'pin_memory -> dataloader_pin_memory',2021-01-28T18:30:25Z,2021-01-28T20:10:46Z,,,
9873,b'Strange hyperparameter warning',2021-01-28T17:04:02Z,2021-01-29T20:48:21Z,,,
9872,b'on_log event should occur *after* the current log is written',2021-01-28T16:36:01Z,2021-01-28T18:11:05Z,,,
9871,"b""Exception: You're trying to run a `Unigram` model but you're file was trained with a different algorithm""",2021-01-28T13:27:18Z,2021-04-25T15:03:37Z,,Exception,"Exception: You're trying to run a `Unigram` model but you're file was trained with a different algorithm"
9870,b'IndexError when finetuning barthez on summarization',2021-01-28T13:14:24Z,2021-02-09T09:22:25Z,,IndexError,"IndexError: piece id is out of range."
9869,b'Added do_lower_case parameters for tokenizer in mlm training.',2021-01-28T11:04:42Z,2021-01-28T16:08:10Z,,,
9868,b'Remove submodule',2021-01-28T09:01:16Z,2021-01-28T09:03:54Z,,,
9867,b'where is position_embedding_type used',2021-01-28T08:29:08Z,2021-01-28T15:33:07Z,,,
9866,b'Whole word mask in run_mlm_wwm.py',2021-01-28T07:51:11Z,2021-04-25T15:03:38Z,,,
9865,"b""[trainer] seq2seq doesn't handle mt5 correctly""",2021-01-28T07:26:55Z,2021-01-29T16:11:22Z,,torch.nn.modules.module.ModuleAttributeError,"torch.nn.modules.module.ModuleAttributeError: 'MT5ForConditionalGeneration' object has no attribute 'model'"
9864,"b'Longformer: raise TypeError(""pred must not be a Python bool"", pred)'",2021-01-28T04:16:11Z,2021-02-03T16:17:22Z,,TypeError,"TypeError: in user code:"
9863,b'Add support for tf2 encoder_decoder',2021-01-27T23:23:52Z,2021-10-12T22:10:34Z,New model,,
9862,b'AttributeError with T5Tokenizer',2021-01-27T22:14:32Z,2021-04-25T15:03:40Z,,,
9861,b'Rag modification',2021-01-27T21:29:24Z,2021-01-27T21:29:35Z,,,
9860,b'Padding tokens affect MobileBert output',2021-01-27T21:26:55Z,2021-04-19T13:36:04Z,,,
9859,b'Head masking and test_head_masking not working properly for TFT5 models.',2021-01-27T21:24:09Z,2021-02-17T16:00:09Z,,,
9858,b'Remove redundant `test_head_masking  = True` flags in test files',2021-01-27T21:17:38Z,2021-01-28T15:09:14Z,,,
9857,b'Pin memory in Trainer by default',2021-01-27T19:02:32Z,2021-01-28T07:50:47Z,,,
9856,b'Add head_mask and decoder_head_mask to PyTorch LED',2021-01-27T17:47:54Z,2021-02-02T19:06:52Z,,,
9855,b'About max_length in generation_utils.py',2021-01-27T16:17:00Z,2021-03-02T01:28:06Z,,,
9854,b'Deprecate model_path in Trainer.train',2021-01-27T15:56:11Z,2021-01-28T13:32:46Z,,,
9853,b'Fix computation of attention_probs when head_mask is provided.',2021-01-27T15:09:29Z,2021-01-28T11:11:53Z,,,
9852,b'Adding a new `return_full_text` parameter to TextGenerationPipeline.',2021-01-27T14:40:39Z,2021-01-29T09:27:33Z,,,
9851,b'[GA forks] Test on every push',2021-01-27T14:11:32Z,2021-01-27T14:11:54Z,,,
9850,b'Some model use serve previous version can not do inference in web api.',2021-01-27T13:49:59Z,2021-04-25T15:03:42Z,,,
9849,b'Labeled pull requests',2021-01-27T13:45:35Z,2021-01-27T13:45:55Z,,,
9848,b'Add XLA test',2021-01-27T13:32:47Z,2021-01-29T10:25:03Z,,,
9847,b'TFBart lables consider both pad token and -100',2021-01-27T13:27:37Z,2021-01-31T22:31:29Z,,,
9846,b'Adding new parameter to `generate`:  `max_time`.',2021-01-27T13:20:12Z,2021-03-12T09:11:50Z,,,
9845,"b""[WIP/ don't merge] T5 gradient checkpointing""",2021-01-27T13:10:41Z,2021-04-25T15:03:43Z,,,
9844,b'[examples/seq2seq] support label smoothing',2021-01-27T13:08:15Z,2021-02-05T17:51:57Z,,,
9843,b'SQUAD Question Answering example:: RuntimeError: Could not infer dtype of NoneType',2021-01-27T13:05:56Z,2021-04-25T15:03:44Z,,,
9842,b'Fix model templates',2021-01-27T13:00:41Z,2021-01-27T13:20:59Z,,,
9841,b'Multi-TPU training uses just 1 out of 8 cores.',2021-01-27T12:28:42Z,2021-01-27T15:43:11Z,,,
9840,b'Fix TF template',2021-01-27T12:14:22Z,2021-01-27T12:40:30Z,,,
9839,b'Run GA on forks',2021-01-27T11:31:07Z,2021-02-24T20:39:44Z,Tests,,
9838,b'logging_epochs argument for TrainingArguments',2021-01-27T10:23:04Z,2021-02-19T16:49:22Z,Feature request,,
9837,b'Fixing flaky conversational test + flag it as a pipeline test.',2021-01-27T09:30:29Z,2021-01-28T09:19:56Z,,,
9836,"b'[docs] use `versionadded`, `versionchanged` and `deprecated` directive'",2021-01-27T09:21:39Z,2021-04-25T15:03:45Z,,,
9835,"b""support Mixed Precision and avoid 'dtype=float32' in implementation""",2021-01-27T08:49:10Z,2021-04-23T15:03:24Z,,,
9834,b'Improved TF inputs',2021-01-27T08:46:59Z,2021-02-02T14:08:29Z,,,
9833,b'Mixed Precision support and avoid \xe2\x80\x98\xe5\x88\x86\xe5\x92\xaf\xe5\x95\x8a\xe5\xa4\xaa\xe2\x80\x99',2021-01-27T08:28:22Z,2021-01-27T11:10:22Z,,,
9832,"b""ImportError: cannot import name 'get_last_checkpoint'""",2021-01-27T08:20:38Z,2021-03-08T09:48:11Z,,ImportError,ImportError: cannot import name 'get_last_checkpoint'
9831,b'[Setup.py] update jaxlib',2021-01-27T07:58:18Z,2021-01-27T08:34:21Z,,,
9830,b'[MT5 Import init] Fix typo',2021-01-27T07:57:58Z,2021-01-27T09:09:57Z,,,
9829,b'Update run_xnli.py to use Datasets library',2021-01-27T07:32:59Z,2021-02-11T04:57:24Z,,,
9828,b'[LedFastTokenizer] Correct missing None statement',2021-01-27T07:22:47Z,2021-01-27T07:43:15Z,,,
9827,b'I am trying to Fine tune on BartForConditionalGeneration but I end up getting all <pad_tokens>. Can you please help resolve it? ',2021-01-27T06:59:25Z,2021-01-27T10:59:22Z,,,
9826,b'Delete a needless duplicate condition',2021-01-27T06:58:23Z,2021-01-27T10:15:23Z,,,
9825,b'Add tpu_zone and gcp_project in training_args_tf.py',2021-01-27T05:51:37Z,2021-01-27T13:45:10Z,,,
9824,b'[wip] [doc] Performance and Scalability notes',2021-01-27T02:19:52Z,2021-06-22T22:34:19Z,"Performance, WIP",,
9823,b'Allow --arg Value for booleans in HfArgumentParser',2021-01-27T01:56:04Z,2021-01-27T14:31:43Z,,,
9822,b'Fix auto-resume training from checkpoint',2021-01-27T00:31:55Z,2021-01-27T08:48:19Z,,,
9821,b'[trainer] renaming cl args/ trainer attributes to be clear per-gpu vs total',2021-01-26T21:45:29Z,2021-03-18T01:34:03Z,,,
9820,b'Add a flag for find_unused_parameters',2021-01-26T21:45:03Z,2021-01-27T11:18:06Z,,,
9819,b'Add head_mask and decoder_head_mask to FSMT',2021-01-26T21:39:06Z,2021-02-01T06:30:22Z,,,
9818,"b'When resuming training from checkpoint, Trainer loads model'",2021-01-26T21:17:36Z,2021-01-27T14:31:19Z,,,
9817,b'[docs] expand install instructions',2021-01-26T20:49:52Z,2021-01-28T17:36:47Z,,,
9816,b'Setup logging with a stdout handler',2021-01-26T20:24:47Z,2021-01-27T08:39:12Z,,,
9815,b'Fix a bug in run_glue.py (#9812)',2021-01-26T19:15:59Z,2021-01-26T19:32:20Z,,,
9814,b'Missing head_mask and decoder_head_mask arguments in encoder-decoder models',2021-01-26T19:06:17Z,2021-02-01T06:30:22Z,,,
9813,b'ADD BORT',2021-01-26T18:46:05Z,2021-01-27T18:25:12Z,PR for Model Addition,,
9812,b'`label_to_id` in `run_glue.py` seems to have a wrong `if` statement',2021-01-26T18:36:33Z,2021-01-26T19:32:20Z,,,
9811,b'adapt mbart and generate for Mbart50',2021-01-26T18:29:45Z,2021-02-15T06:34:07Z,,,
9810,b'Can I use a smaller base model than allenai/led-base-16384 for LED?',2021-01-26T17:55:53Z,2021-01-27T08:49:59Z,,ValueError,"ValueError: Unrecognized configuration class <class 'transformers.models.longformer.configuration_longformer.LongformerConfig'> for this kind of AutoModel: AutoModelForSeq2SeqLM."
9809,b'Fix fine-tuning translation scripts',2021-01-26T16:23:14Z,2021-01-26T16:30:32Z,,KeyError,"KeyError: 'en-XX'"
9808,b'Adding a test to prevent late failure in the Table question answering pipeline.',2021-01-26T15:41:46Z,2021-01-27T09:10:53Z,,,
9807,b'Partial local tokenizer load',2021-01-26T15:03:09Z,2021-01-28T08:29:14Z,,,
9806,b'Add a test for TF mixed precision',2021-01-26T14:26:28Z,2021-01-27T08:36:49Z,,,
9805,b'Commit the last step on world_process_zero in WandbCallback',2021-01-26T14:10:43Z,2021-01-26T18:21:27Z,,,
9804,b'Finetuning ProphetNet with Seq2SeqTrainer fails.',2021-01-26T11:59:12Z,2021-09-22T15:02:58Z,,RuntimeError,"RuntimeError: CUDA error: device-side assert triggered"
9803,b'convert_graph_to_onnx.convert broken for model bart-large / wmt19-en-de',2021-01-26T10:35:14Z,,"Good First Issue, Good Second Issue","RuntimeException, ValueError","RuntimeException: [ONNXRuntimeError] : 6 : RUNTIME_EXCEPTION : Non-zero status code returned while running Reshape node. Name:'Reshape_74' Status Message: /data/shared/packages/onnxruntime/onnxruntime/core/providers/cpu/tensor/reshape_helper.h:43 onnxruntime::ReshapeHelper::ReshapeHelper(const onnxruntime::TensorShape&, std::vector<long int>&) gsl::narrow_cast<int64_t>(input_shape.Size()) == size was false. The input tensor cannot be reshaped to the requested shape. Input shape:{1,6}, requested shape:{5}ValueError: The type of axis index is expected to be an integer"
9802,"b'[trainer] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters'",2021-01-26T06:10:05Z,2021-01-27T11:18:06Z,,,
9801,b'[trainer] a consistent way to limit the number of items',2021-01-26T05:43:51Z,2021-03-06T00:12:27Z,wontfix,,
9800,b'[traner] fix --lr_scheduler_type choices',2021-01-26T05:33:50Z,2021-01-27T15:12:15Z,,,
9799,b'Authorize last version of tokenizer',2021-01-26T01:20:21Z,2021-02-04T19:18:34Z,,,
9798,b'Smdistributed trainer',2021-01-25T22:53:29Z,2021-01-26T15:28:22Z,,,
9797,b'Conversion of Electra checkpoint from official repo TF (pretrained on custom dataset)',2021-01-25T22:51:13Z,2021-01-26T17:44:39Z,,,
9796,b'Improve pytorch examples for fp16',2021-01-25T22:04:47Z,2021-01-26T09:47:08Z,,,
9795,b'does LED use distributed training by default?',2021-01-25T21:48:42Z,2021-01-25T22:07:39Z,,,
9794,b'[Flaky Generation Tests] Make sure that no early stopping is happening for beam search',2021-01-25T21:09:25Z,2021-01-26T08:21:44Z,,,
9793,b'Add the ability to skip runtime version check',2021-01-25T20:19:43Z,2021-03-06T00:12:28Z,wontfix,,
9792,b'Strange start token in MT5 generation',2021-01-25T20:03:27Z,2021-03-06T00:12:30Z,wontfix,,
9791,b'Fix broken links in the converting tf ckpt document',2021-01-25T19:40:58Z,2021-01-26T08:37:58Z,,,
9790,b'RagTokenForGeneration: Fixed parameter name for logits_processor',2021-01-25T19:31:07Z,2021-01-26T15:44:03Z,,,
9789,b'Allow RAG to output decoder cross-attentions',2021-01-25T19:02:46Z,2021-01-26T17:32:47Z,,,
9788,b'Clean TF Bert',2021-01-25T18:01:06Z,2021-01-27T10:28:12Z,,,
9787,b'Fix model parallel definition in superclass',2021-01-25T13:29:16Z,2021-01-25T16:12:08Z,,,
9786,b'Truncated Translations with mT5 model',2021-01-25T12:21:53Z,2021-01-25T13:04:12Z,,,
9785,b'GPT2 MNLI training using run_glue.py',2021-01-25T11:18:08Z,2021-02-01T10:07:03Z,,ValueError,"ValueError: Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`."
9784,b'Translation Model in ONNX: Choosable Output Formats',2021-01-25T10:36:17Z,,Feature request,,
9783,b'Adding `skip_special_tokens=True` to FillMaskPipeline',2021-01-25T10:08:22Z,2021-01-26T09:06:29Z,,,
9782,b'Add BlenderbotSmallForCausalLM for EncoderDecoder',2021-01-25T09:30:27Z,2021-01-25T17:23:36Z,,,
9781,b'Implementation of BlenderbotForCausalLM',2021-01-25T09:26:29Z,2021-01-25T17:23:54Z,,,
9780,b'Calculating Confidence score for Question Answering Models',2021-01-25T09:15:46Z,2021-03-06T00:12:32Z,wontfix,,
9779,b'I want to train a BART model for conditional text generation. I want to train the encoder and the decoder separately for a specific task. Can anyone help with the code? I am new to this.  @patrickvonplaten',2021-01-25T09:03:17Z,2021-01-25T11:25:18Z,,,
9778,b'Link Not Working',2021-01-25T08:57:25Z,2021-01-25T12:51:43Z,,,
9777,"b""padding='max_length' allowing more than max length""",2021-01-25T07:17:57Z,2021-01-27T04:12:01Z,,,
9776,b'Auto-resume training from checkpoint',2021-01-25T01:49:39Z,2021-01-25T17:03:52Z,,,
9775,b'New API for TensorFlow saved models not compatible with T5 and MarianMT',2021-01-24T21:52:34Z,2021-01-25T14:03:41Z,,ValueError,"ValueError: in user code:"
9774,b'adding MarianForCausalLM for EncoderDecoder use',2021-01-24T21:24:27Z,2021-01-25T17:26:31Z,,,
9773,b'RagRetriever question_hidden_states shape',2021-01-24T20:05:53Z,2021-03-06T00:12:34Z,wontfix,,
9772,b'Add classes for Multi-label classification models? ',2021-01-24T17:52:31Z,,Feature request,,
9771,b'TF loss function output inconsistent with Pytorch one for multiple tasks',2021-01-24T17:04:46Z,2021-02-01T16:17:51Z,,,
9770,b'TFBartForConditionalGeneration with labels padded with -100 gives Nan loss.',2021-01-24T16:02:07Z,2021-02-03T16:17:35Z,,```tensorflow.python.framework.errors_impl.InvalidArgumentError,"```tensorflow.python.framework.errors_impl.InvalidArgumentError:  Received a label value of -100 which is outside the valid range of [0, 50265).  Label values: 0 2387 2335 16 11962 2 -100 -100 -100 -100 -100 ...........```"
9769,b'saving best model only using modelcheckpoint keras',2021-01-24T14:58:05Z,2021-03-06T00:12:37Z,wontfix,,
9768,"b'PegasusForCausalLM, analog to `ProphetNetForCausalLM`'",2021-01-24T14:16:43Z,2021-01-25T17:27:25Z,,,
9767,b'tensorflow training problem',2021-01-24T06:28:17Z,2021-01-30T15:07:31Z,,,
9766,b'[wip] [doc] Parallelism notes',2021-01-24T06:04:46Z,2021-07-10T00:39:09Z,"Model Parallel, DeepSpeed, Pipeline Parallel, WIP",,
9765,b'[wip] [pipeline parallel] t5 - experiment',2021-01-24T03:16:17Z,2021-06-04T06:15:59Z,"Pipeline Parallel, WIP",,
9764,"b'index mismatch in ""offset_mapping"" with TokenizerFast and pre-tokenized input'",2021-01-24T00:07:22Z,2021-01-26T15:36:41Z,,,
9763,b'squad_v2 crashes during evaluation',2021-01-23T19:44:26Z,2021-01-24T08:58:11Z,,ValueError,"ValueError: max() arg is an empty sequence"
9762,b'Fix a typo in `Trainer.hyperparameter_search` docstring',2021-01-23T19:40:50Z,2021-01-25T11:40:04Z,,,
9761,b'Fix broken [Open in Colab] links (#9688)',2021-01-23T09:28:49Z,2021-01-23T09:41:47Z,,,
9760,b'fix text summarization evaluation bugs when calculate rouge',2021-01-23T02:30:27Z,2021-01-25T15:58:49Z,,,
9759,b'fix a small bug',2021-01-23T02:17:01Z,2021-01-23T02:27:17Z,,,
9758,b'save tokenizer and model from fine tuned LED model',2021-01-22T20:21:24Z,2021-01-24T18:36:25Z,,,
9757,b'Extra indicators for BPE for Unicode Characters ',2021-01-22T18:34:27Z,2021-04-14T18:35:05Z,,,
9756,b'Remove a TF usage warning and rework the documentation',2021-01-22T17:31:22Z,2021-01-27T09:45:43Z,,,
9755,b'Fix a TF test',2021-01-22T16:30:50Z,2021-01-22T16:40:16Z,,,
9754,b'Improve the `run_xlni` example to use the Datasets library',2021-01-22T15:47:21Z,2021-02-11T04:57:24Z,Good Second Issue,,
9753,b'named_parameters not showing embedding matrix of RobertaLMHead (more a question than a bug)',2021-01-22T15:46:59Z,2021-01-25T09:11:07Z,,,
9752,b'Improve PyTorch examples for FP16',2021-01-22T15:44:49Z,2021-01-26T09:47:08Z,Good First Issue,,
9751,"b'AdaFactor: avoid updating group[""lr""] attributes'",2021-01-22T15:36:27Z,2021-02-01T13:07:34Z,,,
9750,"b""ValueError: Couldn't instantiate the backend tokenizer while loading model tokenizer""",2021-01-22T13:02:12Z,2021-01-25T14:40:44Z,,ValueError,"ValueError: Couldn't instantiate the backend tokenizer from one of: (1) a `tokenizers` library serialization file, (2) a slow tokenizer instance to convert or (3) an equivalent slow tokenizer class to instantiate and convert. You need to have sentencepiece installed to convert a slow tokenizer to a fast one."
9749,b'Use object store to pass trainer object to Ray Tune (makes it work with large models)',2021-01-22T11:36:45Z,2021-01-25T10:01:56Z,,,
9748,b'Trainer object empties dataset',2021-01-22T10:19:31Z,2021-01-25T10:16:03Z,,,
9747,b'mT5 additional_special_tokens seems not work',2021-01-22T09:45:31Z,2021-02-01T06:03:24Z,,,
9746,"b'Fix an efficiency related bug the ""prediction_loop"" of trainer_tf.py'",2021-01-22T09:32:53Z,2021-03-06T00:12:38Z,wontfix,,
9745,b'fine tune patrickvonplaten/longformer2roberta-cnn_dailymail-fp16 using LED updates',2021-01-22T04:30:03Z,2021-04-23T15:03:27Z,,`TypeError,"`TypeError: forward() got an unexpected keyword argument 'head_mask'` issue given that `EncoderDecoderModel` wasn't intended for longformer. So I'm now trying to see if I can use `LEDForConditionalGeneration` for it but I noticed when I try doing:"
9744,b'Wrong offsets mapping in XLMRobertaTokenizerFast',2021-01-22T03:26:44Z,2021-02-09T20:05:53Z,,,
9743,b'DistilGPT2 extremely strange model behaviour',2021-01-22T01:42:36Z,2021-03-06T00:12:39Z,wontfix,,
9742,b'--fp16 fine-tuning appears to be taking more memory (4.3.0). ',2021-01-22T00:53:44Z,2021-03-06T00:12:41Z,wontfix,,
9741,b'examples: fix XNLI url',2021-01-22T00:39:14Z,2021-01-22T12:43:53Z,,,
9740,b'RAG Model without DPR',2021-01-21T22:12:43Z,2021-01-22T14:58:40Z,,,
9739,b'Error using TFAutoModelForSequenceClassification with Tensorflow 2.2.0',2021-01-21T20:11:19Z,2021-01-25T14:42:08Z,,AttributeError,"AttributeError: module 'tensorflow.keras.layers.experimental' has no attribute 'EinsumDense'"
9738,b'[fsmt] onnx triu workaround',2021-01-21T18:25:28Z,2021-01-25T13:57:38Z,,,
9737,b'[fsmt] Exporting the operator triu to ONNX opset version 12 is not supported',2021-01-21T18:07:34Z,2021-01-25T13:57:37Z,fsmt,RuntimeError,"RuntimeError: Exporting the operator triu to ONNX opset version 12 is not supported. Please open a bug to request ONNX export support for the missing operator."
9736,"b""[fsmt] token_type_ids isn't used""",2021-01-21T18:04:47Z,2021-01-23T04:38:54Z,fsmt,RuntimeError,"RuntimeError: Exporting the operator triu to ONNX opset version 12 is not supported. Please open a bug to request ONNX export support for the missing operator"
9735,b'Add `report_to` training arguments to control the integrations used',2021-01-21T17:13:27Z,2021-01-22T15:34:34Z,,,
9734,b'Fixes to run_seq2seq and instructions',2021-01-21T16:35:58Z,2021-01-22T15:03:58Z,,,
9733,"b'[WIP] Small improvement in shape manipulation in t5, makes exporting'",2021-01-21T16:30:32Z,2021-04-23T15:03:28Z,,,
9732,b'OSError: [Errno 116] Stale file handle',2021-01-21T15:31:01Z,2021-03-06T00:12:42Z,wontfix,OSError,"OSError: [Errno 116] Stale file handle"
9731,b'Mismatch of the mask token id of BART between fairseq and huggingface',2021-01-21T13:59:02Z,2021-11-01T08:28:28Z,,,
9730,b'Docs suggest to use discriminator weights for ElectraForMaskedLM instead of generator',2021-01-21T13:47:30Z,2021-03-06T00:12:44Z,wontfix,,
9729,b'Changing model default for TableQuestionAnsweringPipeline.',2021-01-21T13:08:11Z,2021-01-21T13:31:52Z,,,
9728,b'Fix some TF slow tests',2021-01-21T12:09:22Z,2021-01-22T13:50:47Z,,,
9727,b'ERROR about using layer_past and use_cache in Attention Layer of GPT2',2021-01-21T11:23:22Z,2021-01-22T03:39:59Z,,RuntimeError,"RuntimeError: The size of tensor a (13) must match the size of tensor b (7) at non-singleton dimension 3"
9726,b'fix T5 head mask in model_parallel',2021-01-21T10:51:37Z,2021-01-21T11:16:15Z,,,
9725,"b""AutoModel doesn't work with DPRContextEncoder""",2021-01-21T10:42:24Z,2021-03-06T00:12:46Z,wontfix,,
9724,b'Run_ner.py falsely aligns prediction list',2021-01-21T10:10:34Z,2021-03-06T00:12:47Z,wontfix,,
9723,b'[LED] Reduce Slow Test required GPU RAM from 16GB to 8GB',2021-01-21T09:36:31Z,2021-01-21T10:16:15Z,,,
9722,b'convert_graph_to_onnx.convert broken for translation model facebook/wmt19-en-de',2021-01-21T09:25:00Z,2021-01-23T04:38:54Z,,TypeError,"TypeError: forward() got an unexpected keyword argument 'token_type_ids'"
9721,b'[T5] Fix T5 model parallel tests',2021-01-21T09:23:55Z,2021-01-21T10:17:13Z,,,
9720,b'Temporarily deactivate TPU tests while we work on fixing them',2021-01-21T09:14:32Z,2021-01-21T09:17:40Z,,,
9719,b'[PretrainedModel] add tie_weights to init',2021-01-21T08:49:47Z,2021-01-21T11:46:28Z,,,
9718,b'T5 Model Parallelism in 4.3.0',2021-01-21T08:39:48Z,2021-01-21T11:16:15Z,,AttributeError,"AttributeError: 'list' object has no attribute 'to'"
9717,b'ConvBERT Model',2021-01-21T08:24:27Z,2021-01-27T08:20:10Z,"Core: Modeling, PR for Model Addition",,
9716,b'CUDA out of memory error on Trainer hyperparameter_search',2021-01-21T04:11:34Z,2021-04-23T15:03:29Z,,,
9715,b'Error when passing --line_by_line to run_mlm.py',2021-01-21T01:07:00Z,2021-03-06T00:12:48Z,wontfix,ReferenceError,"ReferenceError: {'help': 'The name of the dataset to use (via the datasets library).'} does not reference a class __dict__"
9714,b'Slow BERT Tokenizer adds UNK when calling tokenize()',2021-01-21T00:22:19Z,2021-01-29T10:11:54Z,,,
9713,b'Fix memory regression in Seq2Seq example',2021-01-21T00:13:04Z,2021-01-21T17:05:48Z,,,
9712,b'[trainer] no --deepspeed and --sharded_ddp together',2021-01-20T23:29:36Z,2021-01-21T00:50:22Z,,,
9711,b'Add support for RemBERT',2021-01-20T23:16:44Z,2021-07-24T15:31:43Z,"New model, Feature request",,
9710,b'Let Trainer provide the device to perform training',2021-01-20T21:49:21Z,2021-01-20T22:32:57Z,,,
9709,b'DeepSpeed: Exits with CUDA runtime error on A100 (requires recompiling DeepSpeed for NVIDIA 8.0 Arch)',2021-01-20T21:04:14Z,2021-03-06T00:12:50Z,wontfix,RuntimeError,"RuntimeError: CUDA error: no kernel image is available for execution on the device"
9708,b'fix typo',2021-01-20T19:46:42Z,2021-01-21T08:21:02Z,,,
9707,b'Allow text generation for ProphetNetForCausalLM ',2021-01-20T18:56:34Z,2021-01-21T10:13:39Z,,,
9706,"b'[PR/Issue templates] normalize, group, sort + add myself for deepspeed'",2021-01-20T17:08:51Z,2021-01-26T05:09:01Z,,,
9705,b'[deepspeed] fix the backward for deepspeed',2021-01-20T16:55:41Z,2021-01-20T17:07:08Z,,,
9704,"b'ValueError(""The training dataset must have an asserted cardinality"") when running run_tf_ner.py'",2021-01-20T16:54:35Z,2021-03-06T00:12:51Z,wontfix,ValueError,"ValueError: The training dataset must have an asserted cardinality"
9703,b'Fix WAND_DISABLED test',2021-01-20T16:49:29Z,2021-01-20T17:30:25Z,,,
9702,b'ProphetNetForCausalLM text generation fails',2021-01-20T15:49:00Z,2021-01-21T10:13:38Z,,AttributeError,"AttributeError: 'ProphetNetForCausalLM' object has no attribute 'get_encoder'"
9701,b'how to run pegasus finetune on multiple gpus',2021-01-20T13:46:46Z,2021-01-20T17:01:33Z,,,
9700,b'NAN return from F.softmax function in pytorch implementation of BART self-attention',2021-01-20T13:28:09Z,2021-04-23T15:03:30Z,,,
9699,b'WANDB_DISABLED env variable not working as expected',2021-01-20T13:21:21Z,2021-01-20T17:30:24Z,,,
9698,b'Model Parallelism for DeBERTa',2021-01-20T12:41:35Z,2021-04-23T05:05:08Z,,,
9697,b'Fix TF template',2021-01-20T11:30:57Z,2021-01-20T14:04:53Z,,,
9696,b'Add notebook',2021-01-20T11:12:52Z,2021-01-20T15:19:26Z,,,
9695,b'The model learns nothing after 3 epochs of training',2021-01-20T10:58:44Z,2021-01-20T13:28:11Z,,,
9694,"b""ModuleAttributeError: 'GPT2LMHeadModel' object has no attribute 'backward'""",2021-01-20T10:10:59Z,2021-01-20T17:07:08Z,,,
9693,"b""ModuleAttributeError: 'GPT2LMHeadModel' object has no attribute 'backward'""",2021-01-20T10:07:31Z,2021-01-20T10:12:57Z,,,
9692,"b""input one model's output to another one""",2021-01-20T10:07:12Z,2021-01-20T14:11:27Z,,,
9691,b'Add DeBERTa head models',2021-01-20T08:57:48Z,2021-01-20T15:18:51Z,,,
9690,b' Is there a C++ interface?',2021-01-20T08:27:10Z,2021-01-20T09:17:55Z,,,
9689,b'MLM training for DeBERTa not supported: configuration class is missing',2021-01-20T05:27:39Z,2021-01-20T15:18:51Z,,ValueError,"ValueError: Unrecognized configuration class <class 'transformers.models.deberta.configuration_deberta.DebertaConfig'> for this kind of AutoModel: AutoModelForMaskedLM."
9688,b'[Open in Colab] links not working in examples/README.md',2021-01-20T04:12:45Z,2021-01-23T09:27:35Z,,,
9687,"b""Can't load previously built tokenizers""",2021-01-20T00:57:12Z,2021-04-23T15:03:31Z,,ValueError,"ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.`"
9686,b'BertGenerationDecoder .generate() issue during inference with PyTorch Lightning',2021-01-20T00:10:09Z,2021-04-23T15:03:32Z,,,
9685,"b'Fix Trainer and Args to mention AdamW, not Adam.'",2021-01-19T23:13:38Z,2021-01-20T16:59:31Z,,,
9684,b'Fix model templates and use less than 119 chars',2021-01-19T21:35:04Z,2021-01-19T22:11:23Z,,,
9683,b'Fix Funnel Transformer conversion script',2021-01-19T21:02:34Z,2021-01-20T14:50:21Z,,,
9682,b'Add a community page to the docs',2021-01-19T20:39:33Z,2021-01-20T09:54:37Z,,,
9681,b'Restrain tokenizer.model_max_length default',2021-01-19T20:20:35Z,2021-01-20T09:17:40Z,,,
9680,b'Generating sentence embeddings from pretrained transformers model',2021-01-19T20:12:33Z,2021-01-20T09:44:59Z,,,
9679,b'Visualize self-attention for GLUE task',2021-01-19T19:57:13Z,2021-01-20T09:56:22Z,,,
9678,b'bert-base-cased predicts tokens instead of whole words after fine-tuning on fill-mask task',2021-01-19T17:43:44Z,2021-01-21T11:05:22Z,,,
9677,b'Use datasets squad_v2 metric in run_qa',2021-01-19T17:21:37Z,2021-01-20T09:52:14Z,,,
9676,b'Fix GPT conversion script',2021-01-19T14:43:10Z,2021-01-19T14:55:38Z,,,
9675,b'Fix old Seq2SeqTrainer',2021-01-19T14:38:09Z,2021-01-19T14:56:26Z,,,
9674,b'Fix imports in conversion scripts',2021-01-19T14:30:35Z,2021-01-19T14:40:16Z,,,
9673,b'add mbart to automodel for masked lm',2021-01-19T14:01:46Z,2021-01-19T14:19:12Z,,,
9672,"b""AttributeError: 'Seq2SeqTrainer' object has no attribute '_actual_model'""",2021-01-19T10:46:03Z,2021-01-19T21:27:40Z,,,
9671,b'How to enable tokenizer padding option in feature extraction pipeline?',2021-01-19T10:22:24Z,2021-04-23T15:03:33Z,,,
9670,b'bert_tokenizer.decode(bert_tokenizer.encode(sentence))!=sentence',2021-01-19T09:11:37Z,2021-01-19T10:51:24Z,,,
9669,b'[Bart-like tests] Fix torch device for bart tests',2021-01-19T07:56:03Z,2021-01-19T08:06:25Z,,,
9668,b'Cannot compile tokenizers on PowerPC 9 while installing transformers',2021-01-19T03:28:43Z,2021-01-28T22:05:45Z,,Sidenote,"Sidenote: before getting this, I had an error complaining that I didn't have rust installed, but I did so using the command given on the official website."
9667,b'Add new model docs',2021-01-18T22:16:01Z,2021-02-01T14:55:11Z,,,
9666,b'Fine-tuning LM with NSP',2021-01-18T21:47:07Z,2021-04-23T15:03:33Z,,RuntimeError,"RuntimeError: CUDA error: device-side assert `triggered "
9665,b'IndexError: index out of bounds when running run_mlm.py',2021-01-18T21:35:24Z,2021-01-20T09:17:40Z,,IndexError,"IndexError: index out of bounds "
9664,b'Missing `return_dict` in Doc example',2021-01-18T20:54:19Z,2021-01-19T09:42:28Z,,,
9663,"b""Fix DPRReaderTokenizer's attention_mask""",2021-01-18T20:44:00Z,2021-01-19T10:43:12Z,,"""AttributeError","""AttributeError: Can't pickle local object 'measure_peak_memory_cpu.<locals>.MemoryMeasureProcess'"""
9662,b'Fix TFTrainer prediction output',2021-01-18T15:57:32Z,2021-01-25T09:27:13Z,,,
9661,b'Fix TF Flaubert and XLM',2021-01-18T14:48:41Z,2021-01-19T17:02:58Z,,,
9660,b'run_ner.py crashes when dev or test contain previously unseen labels',2021-01-18T14:35:43Z,2021-04-23T15:03:34Z,,KeyError,"KeyError: 'PC.PRF.UTR.SIN.IND.NOM'"
9659,b'Wav2Vec2',2021-01-18T13:28:42Z,2021-02-02T12:52:11Z,PR for Model Addition,,
9658,b'Tokenizstion',2021-01-18T12:49:55Z,2021-01-18T14:01:11Z,,,
9657,b'ModuleAttributeError occurs during Converting TensorFlow Checkpoints (BERT)',2021-01-18T12:37:11Z,2021-04-23T15:03:35Z,,torch.nn.modules.module.ModuleAttributeError,"torch.nn.modules.module.ModuleAttributeError: 'BertEmbeddings' object has no attribute 'shape'"
9656,"b'""Converting Tensorflow Checkpoints"" document has wrong link in v4.2.0+'",2021-01-18T12:21:00Z,2021-01-26T08:37:57Z,,,
9655,b'BertTokenizer and encode_plus()',2021-01-18T12:09:45Z,2021-01-19T01:36:12Z,,`AttributeError,"`AttributeError: 'BertTokenizer' object has no attribute 'encoder_plus'`"
9654,b'Add t5 convert to transformers-cli',2021-01-18T11:56:41Z,2021-01-20T14:34:28Z,,,
9653,b'AutoModelForMaskedLM not working when using MBartForConditionalGeneration architecture.',2021-01-18T10:52:40Z,2021-01-19T14:19:11Z,,ValueError,"ValueError: Unrecognized configuration class <class 'transformers.models.mbart.configuration_mbart.MBartConfig'> for this kind of AutoModel: AutoModelForMaskedLM."
9652,b'Update integrations.py',2021-01-18T10:24:20Z,2021-01-19T16:39:50Z,,UnboundLocalError,"UnboundLocalError: local variable 'SummaryWriter' referenced before assignment"
9651,b'RAG Fine Tuning',2021-01-18T10:09:16Z,2021-04-23T15:03:36Z,,,
9650,b'Error w/Transformers 4.2.0 and TF Nightly',2021-01-18T04:42:56Z,2021-01-19T05:23:45Z,,ImportError,"ImportError: "
9649,b'Does the latest huggingface-transformers version work with tokenizers==0.10.0?',2021-01-18T01:58:57Z,2021-02-04T01:42:55Z,,,
9648,b'Easier perplexity computation',2021-01-17T23:37:44Z,,Feature request,,
9647,b'Training Bert2Bert with EncoderDecoderModel and Seq2SeqTrainer results with Cuda OOM',2021-01-17T22:31:07Z,2021-01-24T14:34:33Z,,,
9646,b'RAG : Adding end to end training for the retriever (both question encoder and doc encoder) ',2021-01-17T21:39:04Z,2021-05-15T17:36:15Z,Feature request,,
9645,b'Odd predictions of T5 models in recent versions ',2021-01-17T20:19:52Z,2021-01-18T17:53:47Z,,,
9644,b'Fail to convert the Funnel Transformer tensorflow version to transformer one when use the official script',2021-01-17T15:07:42Z,2021-01-20T14:50:21Z,,torch.nn.modules.module.ModuleAttributeError,"torch.nn.modules.module.ModuleAttributeError: 'FunnelForPreTraining' object has no attribute 'embeddings'"
9643,b'[Feature Request] Add 3D attention mask for T5Model',2021-01-17T14:35:04Z,2021-05-13T11:02:27Z,"Good Second Issue, Feature request",,
9642,b'Multi-GPU inference with Tensorflow backend',2021-01-17T14:25:43Z,2021-05-03T15:02:19Z,,,
9641,b'Conditional branching logic in modeling_tf_flaubert.py causing errors with TF Graph',2021-01-17T02:09:37Z,2021-01-18T15:38:27Z,,,
9640,b'Renamed `nlp` variables #9455',2021-01-17T01:03:02Z,2021-04-23T15:03:37Z,,,
9639,b'Add head_mask/decoder_head_mask for TF BART models',2021-01-16T22:55:31Z,2021-01-26T08:50:01Z,,,
9638,"b""ValueError: Expected floating point type, got <dtype: 'int32'> for TFGPT2LMHeadModel""",2021-01-16T20:58:03Z,2021-05-15T15:02:28Z,,,
9637,b'XLMRobertaTokenizerFast producing wrong tokenized output',2021-01-16T20:40:59Z,2021-06-21T15:06:49Z,,,
9636,b'key error when use trainer to fine_tuning a dataset ',2021-01-16T14:14:39Z,2021-01-16T19:02:29Z,,KeyError,"KeyError: 2`"
9635,b'Weights used for Masked LM predictions',2021-01-16T12:32:46Z,2021-04-14T15:14:46Z,,,
9634,b'Add separated decoder_head_mask for T5 Models',2021-01-16T11:29:07Z,2021-01-19T21:50:26Z,,,
9633,b'Wrong offsets_mapping in T5TokenizerFast ',2021-01-16T11:03:55Z,2021-05-19T15:09:24Z,,AssertionError,"AssertionError: Wrong offset mapping for 'This is a test sentence'! "
9632,b'Missing argument: decoder_head_mask for T5',2021-01-16T09:37:03Z,2021-01-19T22:22:29Z,,,
9631,"b""ImportError: cannot import name 'Dataset'""",2021-01-16T04:00:47Z,2021-01-20T11:00:07Z,,ImportError,"ImportError: cannot import name 'Dataset'"
9630,b'key error when use trainer to fine_tuning a dataset',2021-01-16T02:49:40Z,2021-01-16T19:00:21Z,,KeyError,"KeyError: 2`"
9629,b'[Question] How to use threads for huggingface transformers',2021-01-16T00:26:18Z,2021-04-23T15:03:39Z,,,
9628,b'Issue with TrainingArguments docs.',2021-01-16T00:21:03Z,2021-01-20T16:59:31Z,,,
9627,b'Passing in custom BartForConditionalGeneration model as generator to RagSequenceForGeneration',2021-01-15T22:41:47Z,2021-01-16T03:33:08Z,,`ModuleAttributeError,"`ModuleAttributeError: 'BartForConditionalGeneration' object has no attribute 'to_dict'`"
9626,b'Fix: torch.utils.checkpoint.checkpoint attribute error.',2021-01-15T21:12:37Z,2021-01-18T09:33:39Z,,,
9625,b'Weighted Loss in BertForTokenClassification',2021-01-15T19:57:39Z,2021-01-18T20:10:05Z,Feature request,,
9624,b'[wip] [deepspeed] AdamW is now supported by default',2021-01-15T18:52:31Z,2021-03-12T21:40:08Z,DeepSpeed,,
9623,b'wandb breaks tests - importlib.util.find_spec-related under forked process',2021-01-15T17:50:40Z,2021-03-06T00:12:53Z,wontfix,,
9622,b'[deepspeed] --gradient_accumulation_steps fix',2021-01-15T17:45:03Z,2021-01-15T18:12:27Z,DeepSpeed,,
9621,"b'Remove duplicated extras[""retrieval""]'",2021-01-15T16:51:48Z,2021-01-18T09:24:22Z,,,
9620,b'SQuAD 2.0 metric not supported',2021-01-15T16:28:59Z,2021-01-20T09:52:14Z,,"FileNotFoundError, TypeError","FileNotFoundError: Couldn't find file locally at .../squad_v2_local/squad_v2_local.py,TypeError: string indices must be integers"
9619,b'Train robertatokenizer failed due to pad token not found',2021-01-15T12:38:32Z,2021-03-06T00:12:54Z,wontfix,,
9618,b'Text generation pipeline - output_scores parameter',2021-01-15T12:22:18Z,2021-03-06T00:12:56Z,wontfix,,
9617,b'Error in GPT2 while using gradient checkpointing.',2021-01-15T10:49:32Z,2021-01-18T09:33:39Z,,,
9616,b'Fix label datatype in TF Trainer',2021-01-15T10:02:58Z,2021-01-20T11:08:00Z,,,
9615,b'Ignore lm_head decoder bias warning',2021-01-15T09:33:43Z,2021-01-15T14:40:23Z,,,
9614,b'Conditional branching logic in modeling_tf_xlnet.py causing error with TF Graph',2021-01-15T07:23:18Z,2021-03-06T00:12:57Z,wontfix,,
9613,b'training_loss in TFTrainer',2021-01-15T07:03:29Z,2021-01-27T05:34:13Z,,,
9612,"b""Why do not use 'torch.nn.MultiheadAttention' to substitude 'Class BertSelfAttention+BertSelfOutput' for pytorch""",2021-01-15T06:53:37Z,2021-03-06T00:12:58Z,"wontfix, Migration",,
9611,"b'[bugs]: class DataCollatorForWholeWordMask e[""input_ids""] not have the size,change to len()'",2021-01-15T05:46:42Z,2021-03-06T00:12:59Z,wontfix,,
9610,b'[DeepSpeed docs] new information',2021-01-15T03:48:39Z,2021-02-10T06:16:20Z,DeepSpeed,,
9609,b'change masked_bias to -inf',2021-01-15T03:31:40Z,2021-01-15T09:42:29Z,,,
9608,b'Convert ckpt from TFTrainer to huggingface format.',2021-01-15T03:14:01Z,2021-01-15T11:11:28Z,,,
9607,b'[run_ner.py]You need to instantiate RobertaTokenizerFast with add_prefix_space=True to use it with pretokenized inputs',2021-01-15T01:55:08Z,2021-06-15T13:33:21Z,,AssertionError,"AssertionError: You need to instantiate RobertaTokenizerFast with add_prefix_space=True to use it with pretokenized inputs."
9606,b'[DeepSpeed] Features to integrate / Optimizations to add / Experiments to do',2021-01-14T22:35:35Z,,"Feature request, DeepSpeed",,
9605,b'New run_seq2seq script',2021-01-14T22:29:15Z,2021-01-19T20:22:19Z,,,
9604,"b'Mistake in the ""Summary of the tasks"" article'",2021-01-14T19:27:01Z,2021-03-06T00:13:01Z,wontfix,,
9603,"b""TypeError: on_init_end() got an unexpected keyword argument 'model'""",2021-01-14T18:55:51Z,2021-03-06T00:13:02Z,wontfix,TypeError,"TypeError: on_init_end() got an unexpected keyword argument 'model'"
9602,"b""TypeError: on_init_end() got an unexpected keyword argument 'model'""",2021-01-14T18:54:52Z,2021-01-14T18:59:41Z,,TypeError,"TypeError: on_init_end() got an unexpected keyword argument 'model'"
9601,b'[TF Led] Fix wrong decoder attention mask behavior',2021-01-14T17:32:25Z,2021-01-15T11:40:28Z,,,
9600,b'Speed up RepetitionPenaltyLogitsProcessor (pytorch)',2021-01-14T17:15:33Z,2021-01-20T09:23:02Z,,,
9599,b'saving the model during run_mlm',2021-01-14T15:42:41Z,2021-01-14T15:47:08Z,,,
9598,b'saving the model during run_mlm.py',2021-01-14T15:42:19Z,2021-01-14T15:47:12Z,,,
9597,b'[Model Exporting] How to export a fine tuned model to a single pytorch or tensorflow model file?',2021-01-14T15:38:46Z,2021-01-14T19:12:29Z,,,
9596,b'Update `past_key_values` in GPT-2',2021-01-14T14:54:53Z,2021-01-19T15:00:16Z,,,
9595,b'Order of inputs (difference between doc and output)',2021-01-14T14:29:15Z,2021-03-06T00:13:04Z,wontfix,,
9594,b'why set masked_bias as -10000 in GPT2',2021-01-14T14:20:19Z,2021-03-06T00:13:05Z,wontfix,,
9593,b'Difference in decoded strings between a tokenizer and the corresponding fast tokenizer',2021-01-14T12:53:08Z,2021-03-06T00:13:07Z,wontfix,,
9592,"b'disable message ""Some layers from the model checkpoint ...""'",2021-01-14T11:53:50Z,2021-01-14T12:35:04Z,,,
9591,"b'disable message ""Some layers from the model checkpoint at bert-base-cased were not used when initializing""'",2021-01-14T11:53:12Z,2021-01-14T12:34:44Z,,,
9590,b'WARNING:tensorflow:AutoGraph',2021-01-14T10:56:47Z,2021-04-14T15:05:16Z,,,
9589,b'Fix conda build',2021-01-14T10:51:32Z,2021-01-14T10:51:53Z,,,
9588,b'Longformer version of RoBERTa error',2021-01-14T10:08:40Z,2021-03-06T00:13:09Z,wontfix,ValueError,"ValueError: too many values to unpack (expected 4)"
9587,b'How to fine-tune T5/Bart for other languages on summarization?',2021-01-14T08:58:37Z,2021-02-16T08:13:19Z,,,
9586,b'[bugs] 1. fix chinese_ref column will ignore even we add it in to Datasets.',2021-01-14T08:56:17Z,2021-01-15T00:06:12Z,,,
9585,b'Gradient accumulation for TFTrainer',2021-01-14T08:47:50Z,2021-01-14T15:16:40Z,,,
9584,b'BatchEncoding.to with device with tests',2021-01-14T08:33:57Z,2021-01-14T12:57:58Z,,,
9583,b'Custom mask when performing forward pass',2021-01-14T07:09:11Z,2021-01-14T09:08:42Z,,,
9582,b'[deepspeed doc] install issues + 1-gpu deployment',2021-01-14T05:45:10Z,2021-01-14T19:05:05Z,DeepSpeed,,
9581,b'A question about the weight decay',2021-01-14T05:33:24Z,2021-01-14T05:33:49Z,,,
9580,b'BatchEncoding.to() throwing torch NameError in 4.2.0; identical code works in 4.1.1',2021-01-14T01:21:57Z,2021-01-14T12:57:58Z,,NameError,"NameError: name 'torch' is not defined"
9579,b'Some weights of XLMRobertaForMaskedLM were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized',2021-01-14T00:51:31Z,2021-01-15T14:40:23Z,,,
9578,b'Fix Trainer with a parallel model',2021-01-13T23:37:53Z,2021-01-14T08:23:42Z,,,
9577,b'Trainer is using DataParallel on parallelized models ',2021-01-13T22:10:49Z,2021-01-14T08:23:42Z,,`RuntimeError,"`RuntimeError: module must have its parameters and buffers on device cuda:0 (device_ids[0]) but found one of them on device: cuda:1`"
9576,b'Pipeline - Truncation Keyword not Recognized  ',2021-01-13T21:49:43Z,2021-01-13T22:23:01Z,,TypeError,"TypeError: __init__() got an unexpected keyword argument 'truncation'"
9575,b'Converting original BERT tf checkpoints to BertForMaskedLM',2021-01-13T21:33:41Z,2021-01-15T04:02:01Z,,,
9574,b'Upstream (and rename) sortish sampler',2021-01-13T20:51:28Z,2021-01-14T15:38:14Z,,,
9573,b'Multilingual MiniLM',2021-01-13T19:41:18Z,2021-01-13T20:05:16Z,,,
9572,b'How to train the models in smaller spochs',2021-01-13T18:31:51Z,2021-01-14T08:35:24Z,,,
9571,b'Tensorflow pretrained FlauBERT mixed precision error',2021-01-13T17:25:47Z,2021-03-06T00:13:11Z,wontfix,InvalidArgumentError,"InvalidArgumentError: cannot compute AddV2 as input #1(zero-based) was expected to be a half tensor but is a float tensor [Op:AddV2]"
9570,b'Compliancy with tf-nightly',2021-01-13T16:48:45Z,2021-01-14T09:35:36Z,,,
9569,b'Add head_mask/decoder_head_mask for BART',2021-01-13T16:22:37Z,2021-01-18T12:35:22Z,,,
9568,"b""pegasus fine-tune: TypeError: shift_tokens_right() missing 1 required positional argument: 'decoder_start_token_id'""",2021-01-13T16:19:50Z,2021-01-13T16:39:28Z,,TypeError,"TypeError: shift_tokens_right() missing 1 required positional argument: 'decoder_start_token_id'"
9567,b'Switch metrics in run_ner to datasets',2021-01-13T15:28:10Z,2021-01-14T08:37:08Z,,,
9566,b'Fix data parallelism in Trainer',2021-01-13T14:34:44Z,2021-01-13T14:54:42Z,,,
9565,b'Make logs TF compliant',2021-01-13T12:59:00Z,2021-01-14T09:56:54Z,,,
9564,b'Remove unused token_type_ids in MPNet',2021-01-13T11:39:56Z,2021-01-15T13:06:30Z,,,
9563,b'finetune_trainer.py script is not using given config file',2021-01-13T11:38:16Z,2021-03-06T00:13:12Z,wontfix,,
9562,b'Fix barthez tokenizer',2021-01-13T11:13:52Z,2021-01-13T11:24:11Z,,,
9561,b'Fix slow tests v4.2.0',2021-01-13T10:36:29Z,2021-01-13T14:55:49Z,,,
9560,b'Adding Megatron models.',2021-01-13T08:55:36Z,,New model,,
9559,b'tokenizer decode method',2021-01-13T08:49:57Z,2021-03-06T00:13:13Z,wontfix,,
9558,b'SMITH Google',2021-01-13T07:51:23Z,2021-01-13T12:14:30Z,New model,,
9557,b'Speed up TopKLogitsWarper and TopPLogitsWarper (pytorch)',2021-01-13T07:24:57Z,2021-01-13T12:47:48Z,,,
9556,b'Where is convert_bert_original_tf_checkpoint_to_pytorch.py?',2021-01-13T02:49:48Z,2021-03-06T00:13:15Z,"wontfix, Migration",,
9555,b'DPRReaderTokenizer does not generate the attention_mask properly',2021-01-13T02:47:38Z,2021-01-20T10:26:08Z,,,
9554,b'Fix classification script: enable dynamic padding with truncation',2021-01-13T01:19:33Z,2021-01-13T12:46:49Z,,,
9553,b'[setup.py] note on how to get to transformers exact dependencies from shell',2021-01-13T00:26:09Z,2021-01-14T10:04:09Z,,,
9552,b'[CI] use correct deps for torchhub',2021-01-13T00:13:46Z,2021-01-13T13:02:54Z,,,
9551,b'Dynamic padding + truncation in classification script',2021-01-12T23:56:53Z,2021-01-13T12:46:48Z,,,
9550,b'Use the right version of tokenizers',2021-01-12T23:14:57Z,2021-01-12T23:55:46Z,,,
9549,b'Use the right version of tokenizers for torchhub',2021-01-12T23:11:38Z,2021-01-12T23:13:07Z,,,
9548,b'Quick tour runs into OOM on Colab ',2021-01-12T21:14:25Z,2021-01-12T21:55:09Z,,ResourceExhaustedError,"ResourceExhaustedError: 2 root error(s) found."
9547,b'Fine-tuning LMwithNSP',2021-01-12T20:59:46Z,2021-03-06T00:13:16Z,wontfix,,
9546,b'Entity level F-1 scores in run_ner.py',2021-01-12T20:55:19Z,2021-01-14T08:37:08Z,,,
9545,b'Doc: Update pretrained_models wording',2021-01-12T20:51:45Z,2021-01-13T10:58:06Z,,,
9544,b'RFC: ternary assignment style in transformers code revisited ',2021-01-12T20:47:42Z,2021-01-13T17:33:36Z,,,
9543,b'Generating sequence from two input sequences',2021-01-12T17:13:25Z,2021-01-13T10:53:44Z,,,
9542,b'Is the GPT-2 forward too different from Bert or RoBerta?',2021-01-12T15:49:04Z,2021-03-06T00:13:17Z,wontfix,IndexError,"IndexError: index out of range in self"
9541,b'Fix fill mask pipeline slow test using deprecated argument',2021-01-12T15:47:32Z,2021-01-12T21:21:30Z,,,
9540,"b'bounding by compute, retraining from the time the model is killed '",2021-01-12T15:13:13Z,2021-03-06T00:13:19Z,wontfix,,
9539,b'LayoutLM Config',2021-01-12T14:43:58Z,2021-01-12T15:03:51Z,,,
9538,b'fix BlenderbotSmallTokenizer',2021-01-12T14:30:38Z,2021-01-13T05:23:44Z,,,
9537,b'BertForTokenClassificiation save',2021-01-12T13:41:42Z,2021-01-13T11:02:27Z,,,
9536,b'[WIP][EncoderDecoder] Fix label behavior',2021-01-12T13:39:50Z,2021-03-06T00:13:20Z,,,
9535,b'strange output of fast/slow tokenizers',2021-01-12T13:32:45Z,2021-01-14T14:18:09Z,,,
9534,b'Need clarification in /examples/research_projects/rag/use_own_knowledge_dataset.py',2021-01-12T11:12:03Z,2021-01-18T05:23:41Z,,,
9533,b'xla_spawn.py crashes when training on TPU V3-32',2021-01-12T10:48:38Z,2021-03-06T00:13:21Z,wontfix,Exception,"Exception: process 0 terminated with exit code 17"
9532,b'[Blenderbot] Fix Links',2021-01-12T10:33:12Z,2021-01-12T10:53:32Z,,,
9531,b'Seq2Seq include custom glossary/dictionary',2021-01-12T10:23:45Z,2021-03-06T00:13:22Z,wontfix,,
9530,b'Data format for TFTrainer for TFGpt2',2021-01-12T08:59:44Z,2021-01-13T05:10:02Z,,,
9528,b'Print All Tokens Over a Certain Probability Threshold: T5 ',2021-01-12T02:04:20Z,2021-03-06T00:13:24Z,wontfix,`ValueError,`ValueError: You have to specify either decoder_inputs or decoder_inputs_embeds`
9527,b'[BlenderbotSmallTokenizer] Cannot download tokenizer',2021-01-12T00:59:51Z,2021-01-12T10:53:32Z,,JSONDecodeError,"JSONDecodeError: Expecting value: line 1 column 1 (char 0)"
9526,b'Siamese Multi-depth Transformer-based Hierarchical Encoder',2021-01-11T21:19:30Z,,"New model, Feature request",,
9525,b'mBART is not saving (learned) position embeddings',2021-01-11T21:11:31Z,2021-01-18T19:56:33Z,,,
9524,b'Refactor `prepare_seq2seq_batch`',2021-01-11T20:42:35Z,2021-01-12T23:19:39Z,,,
9523,"b""Documentation's example script linked does no exist anymore""",2021-01-11T19:48:48Z,2021-03-06T00:13:25Z,wontfix,,
9522,b'[make docs] parallel build',2021-01-11T19:38:12Z,2021-01-11T21:00:09Z,,,
9521,b'Converting T5 (text to text transfer transformer model) checkpoints to pytorch ',2021-01-11T17:16:36Z,2021-01-11T19:51:36Z,,,
9520,"b""T2TDataCollator 'target_ids' key error""",2021-01-11T16:37:07Z,2021-01-12T18:30:11Z,,KeyError,"KeyError: 'target_ids'"
9519,"b""Update 'Develop on Windows' guidelines""",2021-01-11T15:58:55Z,2021-01-12T09:15:17Z,,,
9518,"b""Model Hub hanging in model's loading""",2021-01-11T15:35:19Z,2021-04-14T16:35:26Z,,,
9517,"b""UnicodeDecodeError: 'utf-8' codec can't decode byte 0x80 in position 64: invalid start byte """,2021-01-11T15:18:37Z,2021-01-15T16:16:39Z,,,
9516,b'Make doc styler behave properly on Windows',2021-01-11T14:53:40Z,2021-01-11T15:25:25Z,,,
9515,"b""Can't run T5 models because of missing protoc""",2021-01-11T14:30:42Z,2021-01-13T09:05:07Z,,,
9514,b'[ProphetNet] Fix naming and wrong config',2021-01-11T13:35:03Z,2021-01-12T09:10:06Z,,,
9513,b'[TF Led] Fix flaky TF Led test',2021-01-11T13:04:55Z,2021-01-11T13:14:49Z,,,
9512,b'Fix template',2021-01-11T12:43:31Z,2021-01-11T13:03:29Z,,,
9511,"b""Shouldn't stale issues/PRs with feature request label""",2021-01-11T10:35:11Z,2021-01-12T09:49:16Z,,,
9510,b'config.json not found when loading fasttext-language-id model',2021-01-11T10:21:25Z,2021-04-14T18:28:34Z,,,
9509,b'[Benchmark]onnx-export',2021-01-11T09:32:53Z,2021-04-24T15:02:39Z,,,
9508,b'bug in distributed codes AssertionError: Default process group is not initialized',2021-01-11T09:16:18Z,2021-04-24T15:02:40Z,,,
9507,b'Remove tolerance + drop_rows_to_fit by default',2021-01-11T09:12:07Z,2021-01-11T13:02:42Z,,,
9506,b'Model previews not working for models that require MecabTokenizer',2021-01-11T08:37:27Z,2021-01-11T18:56:43Z,dependencies,,
9505,b'Fix cardinality',2021-01-11T08:04:57Z,2021-01-11T14:42:19Z,,,
9504,b'Fix template',2021-01-11T07:54:44Z,2021-01-11T10:21:25Z,,,
9503,"b""torch.nn.modules.module.ModuleAttributeError: 'RecursiveScriptModule' object has no attribute 'resize_token_embeddings'""",2021-01-11T07:23:22Z,2021-04-24T15:02:40Z,,torch.nn.modules.module.ModuleAttributeError,"torch.nn.modules.module.ModuleAttributeError: 'RecursiveScriptModule' object has no attribute 'resize_token_embeddings'"
9502,b'RoBERTa tokenizer does not add start and end token at the beginning and end of the sentence',2021-01-11T06:59:53Z,2021-01-12T08:12:02Z,,,
9501,b'Question About Attention Score Computation & Intuition',2021-01-11T05:38:26Z,2021-04-24T15:02:41Z,,,
9500,b'Question on the example script run_glue.py for text classification',2021-01-11T01:21:51Z,2021-04-24T15:02:42Z,,,
9499,b'[ray] add maintainers for Ray / Tune',2021-01-11T00:39:21Z,2021-01-11T01:34:18Z,,,
9498,b'Can not load a saved tokenizer using AutoTokenizer',2021-01-10T22:31:45Z,2021-01-11T08:40:42Z,,OSError,"OSError: Can't load config for './t5-tokenizer-test/'. Make sure that:"
9497,b'[TFBart] Split TF-Bart',2021-01-10T14:37:27Z,2021-01-12T01:06:33Z,,,
9496,b'[make docs] please help make the validation process easier',2021-01-09T23:45:50Z,2021-01-18T17:52:28Z,Feature request,,
9495,b'tf trainer dataset cardinality issue - potentially a bug',2021-01-09T19:36:14Z,2021-01-11T14:53:09Z,,,
9494,b'New Updated DistilGPT-2 Finetuning and Generation',2021-01-09T11:54:48Z,2021-01-11T13:34:39Z,,,
9493,b'Added a new DistilGPT2 fine-tuning and generation Tutorial',2021-01-09T08:16:50Z,2021-01-09T11:36:32Z,,,
9492,b'Problems with using LongFormer',2021-01-09T07:24:17Z,2021-04-24T15:02:42Z,,,
9491,b'[trainer] round numbers in trainer state',2021-01-08T23:16:32Z,2021-01-11T18:17:49Z,,,
9490,b'Using Huggingface library with DeepSpeed',2021-01-08T21:28:09Z,2021-01-09T18:47:50Z,,,
9489,b'fix(wandb): fix config',2021-01-08T19:22:08Z,2021-01-08T19:32:03Z,,,
9488,b'Make doc styler detect lists on rst and better support for Windows',2021-01-08T19:19:28Z,2021-01-11T13:53:42Z,,,
9487,b'[T5] enable T5 fp16',2021-01-08T17:54:38Z,2021-01-12T11:42:33Z,,,
9486,b'Update run_glue for do_predict with local test data (#9442)',2021-01-08T17:44:16Z,2021-01-13T12:48:36Z,,,
9485,b'ProphetNetNgramAttention: Number of attention heads',2021-01-08T17:06:54Z,2021-01-12T09:10:06Z,,,
9484,b'[Flax] Adapt Flax models to new structure',2021-01-08T16:13:27Z,2021-03-18T06:44:18Z,Flax,,
9483,b'Fixing tests. It seems master changed something in the warnings.',2021-01-08T13:59:10Z,2021-01-10T14:08:21Z,,,
9482,b'Reformat the TF serving outputs',2021-01-08T13:20:29Z,2021-01-10T14:10:16Z,,,
9481,b'dataset not being sent to device when using Trainer (distributed)',2021-01-08T11:40:02Z,2021-04-14T16:45:13Z,,RuntimeError,"RuntimeError: Input, output and indices must be on the current device"
9480,b'request for run_text_classification.py',2021-01-08T11:02:03Z,2021-04-24T15:02:43Z,,,
9479,b'Makes HfArgumentParser compatible with Python 3.9',2021-01-08T10:53:22Z,2021-01-08T13:10:44Z,,,
9478,b'Fix TF s2s models',2021-01-08T10:09:12Z,2021-01-21T16:03:30Z,,,
9477,"b'rename ""gpu"" --> ""device""'",2021-01-08T09:06:57Z,2021-04-24T15:02:44Z,,,
9476,b'Improve LayoutLM',2021-01-08T08:52:02Z,2021-01-12T14:26:32Z,,,
9475,b'[trainer] fractional epoch ',2021-01-08T02:44:25Z,2021-01-11T18:17:49Z,,,
9474,b'Fast imports part 3',2021-01-07T21:39:53Z,2021-01-08T12:41:00Z,,,
9473,b'[Generation Tests] Small speed-up by just generating two tokens',2021-01-07T20:58:12Z,2021-01-07T21:33:56Z,,,
9472,b'[Generation] Fix bug for manual decoder_input_ids + warning message',2021-01-07T19:31:27Z,2021-01-08T10:50:40Z,,,
9471,b'model.generate() has the same speed on CPU and GPU',2021-01-07T18:58:54Z,2021-01-07T20:21:42Z,,,
9470,b'max_target length for question answering system',2021-01-07T18:44:00Z,2021-01-08T12:51:09Z,,,
9469,b'Cannot Evaluate While Training Using the Trainer',2021-01-07T18:17:35Z,2021-01-08T16:26:45Z,,,
9468,b'Have RAG return generator cross-attentions when output_attentions=True',2021-01-07T18:02:28Z,2021-01-26T17:32:47Z,,,
9467,b'Unable to train sequence classification task using TFTrainer',2021-01-07T17:05:44Z,2021-04-24T15:02:44Z,,ValueError,"ValueError: in user code:"
9466,b'RuntimeError when running Reformer model',2021-01-07T17:00:22Z,2021-01-07T17:13:13Z,,RuntimeError,"RuntimeError: Overflow when unpacking long (more details below)"
9465,b'[README] Add new models',2021-01-07T16:04:41Z,2021-01-08T10:49:44Z,,,
9464,b'UnboundLocalError when generating sequences',2021-01-07T15:18:00Z,2021-01-08T08:05:15Z,,UnboundLocalError,"UnboundLocalError: local variable 'next_tokens' referenced before assignment"
9463,b'FileNotFoundError when instantiating RagRetriever',2021-01-07T14:28:13Z,2021-01-07T21:00:53Z,,FileNotFoundError,"FileNotFoundError: Couldn't find file at https://storage.googleapis.com/huggingface-nlp/cache/datasets/wiki_dpr/psgs_w100.nq.compressed/0.0.0/psgs_w100.nq.IVF4096_HNSW128_PQ128-IP-train.faiss"
9462,b'Fix scatter import',2021-01-07T13:56:15Z,2021-01-07T13:56:41Z,,,
9461,b'Error while loading finetuned distilbert model: embedding dimension mismatch',2021-01-07T11:46:01Z,2021-01-08T06:15:20Z,,ValueError,"ValueError: Layer #0 (named ""distilbert""), weight <tf.Variable 'tf_distil_bert_for_sequence_classification_9/distilbert/embeddings/word_embeddings/weight:0' shape=(119547, 768) dtype=float32, numpy="
9460,b'[TFGPT2] - Fix flaky past_key_values test',2021-01-07T11:34:59Z,2021-01-07T15:12:09Z,,,
9459,b'[LED Test] fix common inputs pt for flaky pt-tf led test',2021-01-07T11:03:51Z,2021-01-07T11:29:04Z,,,
9458,b'Closed',2021-01-07T11:03:50Z,2021-01-07T11:26:08Z,,,
9457,b'[Blenderbot] Model yields weird results',2021-01-07T10:38:19Z,2021-05-10T15:02:51Z,,,
9456,"b'[EncoderDecoder] Make sure `use_cache` is set to `True` for all Bert2Bert, Roberta2Roberta by default'",2021-01-07T10:24:02Z,2021-01-07T12:05:17Z,,,
9455,b'Rename `nlp` variables into more appropriate names',2021-01-07T09:19:36Z,2021-05-18T13:47:29Z,Good First Issue,,
9454,b'[Docs] Improve model sharing doc',2021-01-07T09:17:01Z,2021-01-07T10:51:03Z,,,
9453,b'Prophetnet optimization',2021-01-07T09:12:33Z,2021-01-07T10:41:59Z,,,
9452,b'Error when running run_clm.py on Python3.9/MacOS',2021-01-07T07:49:42Z,2021-01-08T15:01:43Z,,TypeError,"TypeError: issubclass() arg 1 must be a class"
9451,b'[trainer] remove `--model_parallel`',2021-01-07T04:59:14Z,2021-01-11T14:39:29Z,,,
9450,"b""Some layers of pretrained Albert model albert-base-v2 didn't match the architecture of AlbertForMaskedLM in latest transfomers 4.1.1.""",2021-01-07T04:05:17Z,2021-01-17T02:35:24Z,,,
9449,b'[make fixup] a more reliable version of branching point discovery',2021-01-07T03:53:39Z,2021-01-07T09:47:51Z,,,
9448,b'Cannot use TransfoXLLMHeadModel with Trainer class because it returns a non scalar loss',2021-01-07T03:48:06Z,2021-04-25T15:03:52Z,,RuntimeError,"RuntimeError: grad can be implicitly created only for scalar outputs"
9447,b'urgent please help on memory issue during save ',2021-01-07T03:00:03Z,2021-04-25T15:03:53Z,,,
9446,b'Transformers fast import part 2',2021-01-06T22:28:47Z,2021-01-07T14:36:15Z,,,
9445,b'Loading fine-tuned models ',2021-01-06T19:09:35Z,2021-04-25T15:03:54Z,,OSError,"OSError: Unable to load weights from pytorch checkpoint file for './model_source_450_v2' at './model_source_450_v2/pytorch_model.bin'If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True. "
9444,b'Fix init',2021-01-06T18:52:54Z,2021-04-25T15:03:56Z,,,
9443,b'[GenerationOutputs] Fix GenerationOutputs Tests ',2021-01-06T17:30:07Z,2021-01-06T18:37:03Z,,,
9442,b'[examples/text-classification] `do_predict` for the test set of local datasets',2021-01-06T16:19:37Z,2021-01-13T12:48:36Z,,,
9441,b'Fast transformers import part 1',2021-01-06T16:00:06Z,2021-01-06T17:17:24Z,,,
9440,b'Remove nested lxmert',2021-01-06T15:57:57Z,2021-01-07T09:10:42Z,,,
9439,b'Adding Stochastic Weight Averaging to transformer optimizers',2021-01-06T15:10:25Z,2021-01-12T06:04:31Z,,,
9438,b'Doc styling utils adds parasites new lines',2021-01-06T13:23:54Z,2021-01-11T13:53:42Z,Documentation,ValueError,"ValueError: 345 files should be restyled!"
9437,"b""Can't find pretrained model for TFPegasusForConditionalGeneration """,2021-01-06T13:03:27Z,2021-01-06T14:00:50Z,,,
9436,b'RuntimeError: The size of tensor a (128) must match the size of tensor b (32) at non-singleton dimension 1',2021-01-06T11:51:58Z,2021-01-07T15:02:56Z,Migration,RuntimeError,"RuntimeError: The size of tensor a (128) must match the size of tensor b (32) at non-singleton dimension 1"
9435,b'Fix URLs to TAPAS notebooks',2021-01-06T10:00:56Z,2021-01-06T12:20:42Z,,,
9434,b'Making Conversation possible to create directly a full conversation',2021-01-06T09:24:32Z,2021-01-08T13:33:26Z,,,
9433,"b'Removing duplicated code for Translation,Summarization and Text2TextGeneration pipelines'",2021-01-06T09:09:09Z,2021-01-07T22:10:17Z,,,
9432,b'Enable TruncationStrategy override for pipelines',2021-01-06T08:52:05Z,2021-01-11T14:23:29Z,,,
9431,b'[Docs] Add useful links to model sharing ',2021-01-06T08:32:09Z,2021-01-06T13:36:56Z,,,
9430,b'T5 base use a lot of memory to train on ',2021-01-06T07:57:22Z,2021-04-25T15:03:57Z,,,
9429,b'Apache Hadoop (HDFS) File Loading from_pretrained',2021-01-06T06:49:06Z,2021-04-25T15:03:58Z,,,
9428,b'Improve documentation coverage for Herbert',2021-01-06T05:36:34Z,2021-01-06T14:13:44Z,,,
9427,b'Improve documentation coverage for Phobert ',2021-01-06T03:37:42Z,2021-01-06T15:04:33Z,,,
9426,b'Is it possible to export a pytorch .pt file after finetuning a model?',2021-01-06T00:47:40Z,2021-01-06T23:09:56Z,,,
9425,b'[utils/get_modified_files.py] fails with a few PR checkout tools',2021-01-05T20:56:04Z,2021-01-07T09:47:51Z,,subprocess.CalledProcessError,"subprocess.CalledProcessError: Command '['git', 'merge-base', '--fork-point', 'master']' returned non-zero exit status 1."
9424,b'improve readme text to private models/versioning/api',2021-01-05T19:55:18Z,2021-01-05T20:02:46Z,,,
9423,b'Upgrade styler to better handle lists',2021-01-05T19:25:34Z,2021-01-06T12:46:18Z,,,
9422,b'[Announcement] Changing model type of Barthez',2021-01-05T16:54:20Z,2021-04-15T18:46:57Z,,,
9421,b'Store transformers version info when saving the model',2021-01-05T16:39:37Z,2021-01-06T15:34:49Z,,,
9420,b'Transformer models for semantic parsing',2021-01-05T16:20:53Z,2021-01-06T08:48:46Z,,,
9419,b'New serving',2021-01-05T16:02:06Z,2021-01-07T10:48:50Z,,,
9418,b'New TF embeddings (cleaner and faster)',2021-01-05T11:25:33Z,2021-01-20T11:08:13Z,,,
9417,"b'shift_tokens_right in BART, FSMT incompatible with DataCollatorForLanguageModelling'",2021-01-05T09:38:44Z,2021-01-07T08:27:27Z,,RuntimeError,"RuntimeError: index -1 is out of bounds for dimension 1 with size 213"
9416,b'Why was DataCollatorForNextSentencePrediction removed ?',2021-01-05T09:04:30Z,2021-01-06T02:21:24Z,,,
9415,b'About Multi GPU',2021-01-05T05:31:12Z,2021-01-05T23:34:12Z,,,
9414,b'Fix link to Evaluate TAPAS Notebook',2021-01-05T05:26:54Z,2021-01-06T08:42:51Z,,,
9413,b'Fix link to Notebook to fine-tune TAPAS',2021-01-05T05:23:43Z,2021-01-06T08:44:53Z,,,
9412,b'[model parallel] add experimental warning',2021-01-05T04:39:46Z,2021-01-05T15:05:33Z,,,
9411,b'[examples/text-classification] Fix a bug for using own regression dataset',2021-01-05T03:17:09Z,2021-01-05T13:15:07Z,,,
9410,b'`pip install -e .[dev]` in Python 3.9.1+ fails because `jaxlib==0.1.55` cannot be found',2021-01-05T02:00:39Z,2021-01-18T01:59:33Z,,,
9409,b'[trainer] group fp16 args together',2021-01-05T01:14:38Z,2021-01-05T14:39:39Z,,,
9408,b'[autoformatters] wrapping destroying items/lists',2021-01-04T23:42:32Z,2021-01-06T12:46:18Z,,,
9407,b'Allow example to use a revision and work with private models',2021-01-04T22:33:28Z,2021-01-06T11:49:23Z,,,
9406,b'Unable to train xlnet with tensorflow',2021-01-04T21:26:47Z,2021-04-25T15:04:00Z,,"TypeError, ImportError","TypeError: in converted code:ImportError: cannot import name 'TFTrainer' from 'transformers' (/opt/conda/lib/python3.7/site-packages/transformers/__init__.py)"
9405,b'Retrieval Collapse when fine-tuning RAG',2021-01-04T21:14:38Z,2021-05-04T15:02:56Z,,,
9404,b'Add head_mask/decoder_head_mask for BART',2021-01-04T20:24:25Z,2021-01-13T16:24:07Z,,,
9403,b'added head_mask/decoder_head_mask for BART',2021-01-04T20:15:57Z,2021-01-04T20:17:47Z,,,
9402,b'Bump notebook from 6.1.4 to 6.1.5 in /examples/research_projects/lxmert',2021-01-04T14:59:44Z,2021-01-04T15:02:07Z,dependencies,,
9401,b'Put back LXMert example',2021-01-04T14:28:17Z,2021-01-04T14:59:08Z,,,
9400,"b'Generate Function - Manual decoder_input_ids Error (Bart, Pegasus)'",2021-01-04T13:34:05Z,2021-01-08T10:50:40Z,,TypeError,"TypeError: prepare_inputs_for_generation() got multiple values for argument 'decoder_input_ids'"
9399,b'How to use Longformer for summarization',2021-01-04T13:27:21Z,2021-04-25T15:04:03Z,,,
9398,b'trainer.predict() returns different values from model.logits',2021-01-04T13:13:36Z,2021-01-08T10:42:24Z,,,
9397,b'CUDA runtime error during benchmarking',2021-01-04T11:19:37Z,2021-03-06T00:13:27Z,wontfix,ValueError,"ValueError: too many values to unpack (expected 2)"
9396,b'run_glue.py with XLNet model on CoLA dataset reaches 0 accuracy',2021-01-04T10:05:38Z,2021-03-06T00:13:28Z,wontfix,,
9395,b'wrong output for Bert-larged-uncased',2021-01-04T06:59:01Z,2021-03-06T00:13:29Z,wontfix,,
9394,b'Simplify  marian distillation script',2021-01-03T18:17:55Z,2021-01-04T05:51:25Z,,,
9393,b'`run_glue.py` fails when using my own dataset of regression task',2021-01-03T15:28:19Z,2021-01-05T13:15:07Z,,UnboundLocalError,"UnboundLocalError: local variable 'label_list' referenced before assignment"
9392,b'Model inputs and outputs are ``None`` when converting fine-tuned gpt2 to Tensorflow?',2021-01-03T14:44:48Z,2021-03-06T00:13:31Z,wontfix,,
9391,b'Similar usage of `past_key_values` in CausalLM and Seq2SeqLM',2021-01-03T05:51:03Z,2021-01-19T15:00:15Z,Good First Issue,,
9390,b'[trainer] self.model_wrapped + _model_unwrap',2021-01-03T05:46:15Z,2021-01-06T11:50:12Z,,,
9389,b'[trainer] self.model_wrapped + _model_unwrap',2021-01-03T05:37:07Z,2021-01-03T05:46:21Z,,,
9388,b'Conditional Generation using input_embeds instead of input_ids',2021-01-03T03:13:24Z,2021-01-05T05:39:09Z,,AttributeError,"AttributeError: 'NoneType' object has no attribute 'clone'"
9387,b'Where is the impact when output_attentions=True?',2021-01-02T23:16:57Z,2021-03-06T00:13:32Z,wontfix,,
9386,b'replace apex.normalization.FusedLayerNorm with torch.nn.LayerNorm',2021-01-02T20:48:13Z,2021-01-04T18:00:09Z,,,
9385,b'[logging] autoflush',2021-01-02T20:19:23Z,2021-01-05T08:57:58Z,,,
9384,b'[model parallelism] Bart goes parallel',2021-01-02T20:09:18Z,2021-06-04T06:15:59Z,"Model Parallel, WIP",,
9383,"b'[Marian] Doc says `config.add_bias_logits=True`, but config is has `config.add_bias_logits=False`'",2021-01-02T16:17:51Z,2021-03-06T00:13:35Z,wontfix,,
9382,b'[docs] Fix TF base model examples: outputs.last_hidden_states -> state',2021-01-02T15:14:17Z,2021-01-02T16:58:17Z,,,
9381,b'[Docs] `past_key_values` return a tuple of tuple as a default',2021-01-02T14:27:30Z,2021-01-02T14:55:08Z,,,
9380,"b""BartModel's `past_key_values` seems to have different explanations in input_doc and output_doc""",2021-01-02T12:52:26Z,2021-01-02T14:55:07Z,,,
9379,b'Improve documentation coverage for Bertweet',2021-01-02T12:39:32Z,2021-01-04T18:12:59Z,,,
9378,b'[Docs] Tokenizer Squad 2.0 example',2021-01-02T12:37:37Z,2021-01-04T16:27:30Z,,,
9377,b'replacing apex.normalization.FusedLayerNorm with torch.nn.LayerNorm',2021-01-02T06:13:21Z,2021-01-04T18:00:09Z,,,
9376,b'[docs] TFRobertaModel example: last_hidden_states -> last_hidden_state',2021-01-01T17:48:47Z,2021-01-02T16:58:17Z,,,
9375,b'Fix Typo',2021-01-01T13:32:28Z,2021-03-06T00:13:36Z,wontfix,,
9374,b'How do I handle class imbalance for text data when using pretrained models like BERT?',2021-01-01T12:58:59Z,2021-03-07T05:36:57Z,,,
9373,b'how to evaluate models on SUPER_GLUE benchmark?',2021-01-01T12:14:32Z,2021-03-06T00:13:38Z,wontfix,,
9372,"b'Why does datasets get imported when running ""from transformers.models.roberta.tokenization_roberta_fast import RobertaTokenizerFast""'",2020-12-31T21:03:49Z,2021-03-06T00:13:39Z,wontfix,,
9371,b'Excessive GPU-GPU communication with GPT2 making multi-GPU training slow?',2020-12-31T17:47:12Z,2021-03-06T00:13:40Z,"wontfix, Benchmarks, Performance",,
9370,b'Custom train/validation file not supported in  run_qa.py',2020-12-31T12:17:47Z,2021-04-24T15:02:47Z,,AssertionError,"AssertionError: `train_file` should be a csv or a json file."
9369,b'TF >= 2.3 cleaning',2020-12-31T11:23:49Z,2021-01-05T08:58:26Z,,,
9368,b'Fix utils on Windows',2020-12-31T10:51:16Z,2021-01-04T15:22:15Z,,,
9367,b'Add-support-for-examples-scripts-to-run-on-sagemaker',2020-12-31T08:59:41Z,2021-03-06T00:13:42Z,wontfix,,
9366,b'How to implement seq2seq attention mask conviniently?',2020-12-31T06:14:52Z,2021-04-24T15:02:47Z,,,
9365,b'Multi turn conversation with Blender Bot ',2020-12-31T01:14:11Z,2021-05-05T15:02:36Z,,,
9364,b'Finetune mbart rouge score difference between training and evaluation part',2020-12-30T20:48:27Z,2021-03-06T00:13:43Z,wontfix,,
9363,b'Make sure to use return dict for the encoder call inside RagTokenForGeneration',2020-12-30T19:40:13Z,2021-01-02T11:39:15Z,,,
9362,b'Jupyter Notebook Kernel crashes when tokenizing large dataset',2020-12-30T16:01:16Z,2021-03-06T00:13:45Z,wontfix,,
9361,b'DeBERTa in TF (TFAutoModel): unrecognized configuration class',2020-12-30T15:25:01Z,2021-03-06T00:13:46Z,wontfix,ValueError,"ValueError: Unrecognized configuration class <class 'transformers.models.deberta.configuration_deberta.DebertaConfig'> for this kind of TFAutoModel: TFAutoModel."
9360,b'Loading a set of tokenized files for training',2020-12-30T14:54:23Z,2021-03-06T00:13:48Z,wontfix,,
9359,b'Training loss not getting logged',2020-12-30T12:32:15Z,2021-03-06T00:13:49Z,wontfix,,
9358,b'error while finetuning for Regression task.',2020-12-30T11:33:23Z,2021-03-06T00:13:50Z,wontfix,,
9357,b'Blenderbot-3B config seems to be a little wrong',2020-12-30T10:07:22Z,2021-03-08T13:14:19Z,,,
9356,b'[examples/language-modeling] Add dataset download instructions',2020-12-30T06:49:53Z,2021-01-05T03:21:26Z,,,
9355,b'Fix typos in README and bugs in RAG example code for end-to-end evaluation and finetuning',2020-12-30T05:25:13Z,2021-01-03T15:00:30Z,,TypeError,"TypeError: forward() got an unexpected keyword argument 'clean_up_tokenization'"
9354,b'[test_model_parallelization] multiple fixes',2020-12-30T01:23:22Z,2021-01-04T20:09:12Z,Model Parallel,,
9353,b'Fixes crash when `compute_metrics` is not passed to `Trainer` in run_mlm example',2020-12-30T00:17:32Z,2020-12-30T18:08:12Z,,,
9352,b'[trainer] parametrize default output_dir',2020-12-29T23:00:49Z,2021-01-04T15:14:33Z,,,
9351,b'XLNet evaluation on SQuAD',2020-12-29T22:25:01Z,2021-03-06T00:13:53Z,Good Second Issue,IndexError,"IndexError: list index out of range**"
9350,b'[apex.normalizations.FusedLayerNorm] torch.cuda.is_available() is redundant as apex handles that internally',2020-12-29T21:47:23Z,2020-12-30T09:09:52Z,,,
9349,b'[prophetnet] wrong import',2020-12-29T19:53:50Z,2020-12-29T21:32:07Z,,ImportError,"ImportError: cannot import name 'FusedProphetNetLayerNorm' from 'apex.normalization' (/home/stas/anaconda3/envs/main-38/lib/python3.8/site-packages/apex/normalization/__init__.py)"
9348,b'Fix TF Longformer',2020-12-29T19:44:08Z,2021-01-05T08:49:55Z,,,
9347,"b""[trainer] --model_parallel hasn't been implemented for most models""",2020-12-29T19:12:37Z,2021-01-05T09:01:30Z,Model Parallel,,
9346,b'[Seq2Seq Templates] Add forgotten imports to templates',2020-12-29T18:20:42Z,2020-12-29T18:35:07Z,,,
9345,b'Training of BART slow on TPU - aten ops investigation',2020-12-29T17:42:32Z,2021-03-06T00:13:55Z,wontfix,,
9344,b'MBart prepare_seq2seq_batch',2020-12-29T16:31:48Z,2021-03-06T00:13:56Z,wontfix,,
9343,b'[PyTorch Bart] Split Bart into different models',2020-12-29T15:55:30Z,2021-01-05T21:00:06Z,,,
9342,b'[Seq2Seq Templates] Add embedding scale to templates',2020-12-29T15:46:09Z,2020-12-29T15:48:44Z,,,
9341,b'[PyTorch Bart] Split Bart',2020-12-29T15:37:45Z,2020-12-29T15:49:06Z,,,
9340,b'Possible bug in `train_batch_size`',2020-12-29T14:12:11Z,2021-01-04T17:14:19Z,,,
9339,b'Arrow file is too large when saving vector data',2020-12-29T13:16:36Z,2021-01-18T03:12:01Z,,,
9338,b'Multiprocessing CUDA issues when importing transformers',2020-12-29T11:17:37Z,2021-01-08T12:26:19Z,,`RuntimeError,"`RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method`"
9337,b'[WIP][Research projects] Add folder for Music AI / Music Transformers',2020-12-29T10:18:11Z,2021-03-06T00:13:57Z,wontfix,,
9336,"b'""RuntimeError: Input, output and indices must be on the current device"" when trying to finetune MBart'",2020-12-29T09:44:42Z,2021-01-05T09:01:30Z,,`RuntimeError,"`RuntimeError: Input, output and indices must be on the current device`"
9335,b'Data Loading as a Service',2020-12-29T09:43:08Z,2021-03-06T00:13:59Z,wontfix,,
9334,b'[Seq2Seq Templates] Correct some TF-serving errors and add gradient checkpointing to PT by default.',2020-12-28T15:15:40Z,2020-12-28T16:51:04Z,,,
9333,b'TF Longformer has some graph compilation/execution issue',2020-12-28T14:09:10Z,2021-03-06T00:14:00Z,wontfix,,
9332,b'block sparse bert',2020-12-28T13:14:52Z,2021-03-06T00:14:02Z,wontfix,RuntimeError,"RuntimeError: Error(s) in loading state_dict for BertModel:"
9331,b'[WIP] Temp work on pipelines.',2020-12-28T13:04:27Z,2021-03-01T14:53:05Z,,,
9330,b'Fail to reload tokenizer from save_pretrained method',2020-12-28T12:24:23Z,2020-12-28T13:47:35Z,,OSError,"OSError: file ./config.json not found"
9329,b'how to checkpoint all the validation scores in huggingface trainer ',2020-12-28T09:42:26Z,2021-04-24T15:02:48Z,,,
9328,"b'expected str, bytes or os.PathLike object, not NoneType'",2020-12-28T08:57:17Z,2021-04-24T15:02:49Z,,,
9327,"b""No module named 'transformers.modeling_albert'""",2020-12-28T07:23:06Z,2020-12-28T12:26:38Z,,,
9326,"b""Issue with 'char_to_token()' function of DistilBertTokenizerFast """,2020-12-28T06:53:50Z,2021-01-04T16:27:30Z,,,
9325,b' Add FAVOR+ / Performer attention',2020-12-28T06:13:18Z,2021-03-30T12:14:01Z,,,
9324,b'Music Transformers',2020-12-28T04:40:28Z,2021-05-09T15:02:20Z,,,
9323,b'[T5 model parallel] implement input auto-relocation + lots of refactoring/cleanup',2020-12-28T03:02:15Z,2021-06-04T06:15:57Z,"Model Parallel, WIP",,
9322,b'Conda dependencies conflict with pip dependencies',2020-12-27T23:59:30Z,2021-04-24T15:02:50Z,,,
9321,b'Splitting texts longer that `tokenizer.max_length` into blocks of same size ',2020-12-27T23:35:48Z,2020-12-28T20:56:14Z,,,
9320,b'[Seq2SeqTrainer] Fix Typo',2020-12-27T20:48:59Z,2020-12-27T20:57:51Z,,,
9319,"b""Some weights of AlbertForPreTraining were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['sop_classifier.classifier.weight', 'sop_classifier.classifier.bias']""",2020-12-27T16:44:17Z,2021-01-10T16:38:04Z,,,
9318,b'Fail when running the multimodal example',2020-12-27T14:59:36Z,2021-01-05T14:56:08Z,,torch.nn.modules.module.ModuleAttributeError,"torch.nn.modules.module.ModuleAttributeError: 'MMBTForClassification' object has no attribute 'config'"
9317,b'Bug: metrics inside on_evalute callback is passed wrongly ',2020-12-27T14:12:06Z,2021-04-24T15:02:51Z,,,
9316,b'[t5 model parallel] misc fixes',2020-12-27T05:30:30Z,2021-06-04T06:15:53Z,"Model Parallel, WIP",,
9315,"b'[model site] search UI: language: tags, directionality and filtering'",2020-12-27T03:24:42Z,2021-03-18T01:51:08Z,,,
9314,b'[model site] missing language tags for t5 models',2020-12-27T02:38:53Z,2021-03-18T01:59:07Z,,,
9313,b'[TFBart-like models] Problem with tf saving',2020-12-26T23:35:59Z,2021-04-24T15:02:51Z,,,
9312,b'RAG model implementation seems different from the paper',2020-12-26T22:36:40Z,2020-12-28T19:55:09Z,,,
9311,b'T5-base goes out of memory on 4 GPUs with as small batch size as 4 ',2020-12-26T17:04:27Z,2021-04-24T15:02:52Z,,"RuntimeError, subprocess.CalledProcessError","RuntimeError: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 2; 15.78 GiB total capacity; 14.10 GiB already allocated; 20.25 MiB free; 14.42 GiB reserved in total by PyTorch)subprocess.CalledProcessError: Command '['/opt/conda/envs/t5/bin/python', '-u', 'finetune_trainer.py', '--local_rank=3', 'configs/glue.json']' returned non-zero exit status 1."
9310,"b""ModuleNotFoundError: No module named 'tokenizations.tokenizations'""",2020-12-26T09:13:44Z,2020-12-26T09:14:11Z,,ModuleNotFoundError,"ModuleNotFoundError: No module named 'tokenizations.tokenizations'"
9309,b'Entry-level demo of visual question answering',2020-12-26T04:49:40Z,2021-01-04T14:59:08Z,,AssertionError,"AssertionError: `visual_feats` cannot be `None` "
9308,b'[GPT2] Correct gradient checkpointing',2020-12-25T21:09:37Z,2020-12-25T22:28:13Z,,,
9307,b'from_pretrained does not load the modified part of model ',2020-12-25T16:05:47Z,2021-04-24T15:02:53Z,,,
9306,b'comment correction in test_retrieval_rag.py?',2020-12-25T11:05:46Z,2020-12-30T13:54:44Z,,,
9305,"b'[Don\'t merge] New design proposition for MAPPINGS in ""auto"" files'",2020-12-25T10:12:21Z,2021-04-20T22:26:05Z,,,
9304,"b'\xe3\x80\x90 run_mlm.py \xe3\x80\x91attention_mask will be set to [1,1,...1] with DataCollatorForLanguageModeling '",2020-12-25T06:45:57Z,2021-05-26T15:09:22Z,,,
9303,b'add translation example',2020-12-25T06:34:00Z,2020-12-25T09:17:50Z,,,
9302,b'Fix TF TransfoXL',2020-12-24T19:49:54Z,2020-12-28T19:52:19Z,,,
9301,b'Fix TF T5',2020-12-24T19:04:29Z,2020-12-28T19:51:41Z,,,
9300,b'Fix  TF Funnel',2020-12-24T15:53:53Z,2021-01-05T10:54:50Z,,,
9299,b'[Bart doc] Fix outdated statement',2020-12-24T13:28:42Z,2020-12-24T13:47:54Z,,,
9298,b'`transformers.models.bart.modeling_bart._prepare_bart_decoder_inputs` seems to be renamed but remains in the document',2020-12-24T12:48:00Z,2020-12-24T13:47:53Z,,,
9297,b'fix typo in modeling_encoder_decoder.py',2020-12-24T12:19:56Z,2020-12-24T13:38:09Z,,,
9296,b'[bert_generation] enable cache by default',2020-12-24T12:05:27Z,2020-12-24T12:17:37Z,,,
9295,b'Good Second Issue: T5 FP16 in Pytorch',2020-12-24T10:25:43Z,,Good Second Issue,,
9294,b'Fix TF input for np.ndarray',2020-12-24T10:17:43Z,2021-01-08T13:23:30Z,,,
9293,b'Update tokenization_utils_base.py',2020-12-24T09:32:16Z,2020-12-24T13:43:15Z,,,
9292,b'Fix TF Flaubert',2020-12-24T09:30:03Z,2021-01-04T15:06:29Z,,,
9291,b'Fix TF CTRL',2020-12-24T09:24:44Z,2021-01-04T14:56:51Z,,,
9290,b'Problem converting slow tokenizer to fast: token out of vocabulary',2020-12-24T09:00:53Z,2021-02-25T10:30:37Z,"Core: Tokenization, Fast Tokenizers",Exception,"Exception: Error while initializing BPE: Token `` out of vocabulary"
9289,b'Fix typo in file_utils.py',2020-12-24T06:38:53Z,2020-12-24T08:18:34Z,,,
9288,b'[doc] How To Request Support document stab',2020-12-24T04:16:06Z,2021-01-11T14:23:52Z,,,
9287,"b'SummarizationModule, Trainer and BertPreTrainedModel'",2020-12-24T03:37:59Z,2021-04-24T15:02:54Z,,,
9286,b'Why Bert-chinese use do_lower_case=False?',2020-12-24T01:37:10Z,2021-04-19T01:28:48Z,,,
9285,"b'TFRobertaModel warning  - `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated'",2020-12-24T00:39:08Z,2021-01-04T21:27:17Z,,,
9284,b'[Templates] Adapt Bert',2020-12-24T00:08:18Z,2020-12-24T00:44:34Z,,,
9283,b'Fix TF DPR',2020-12-23T16:53:59Z,2021-01-04T16:26:56Z,,,
9282,b'Adapt to new name of `label_smoothing_factor` training arg',2020-12-23T15:58:21Z,2020-12-23T16:05:22Z,,,
9281,b'tapas utils',2020-12-23T12:58:36Z,2020-12-23T12:58:49Z,,,
9280,b'issue with evaluation of seq2seq_trainer.py on multiple gpus',2020-12-23T11:16:13Z,2021-04-24T15:02:55Z,,OSError,"OSError: file outputs/mixture1/meta-adapters-task-projector-new_sampler-num-gpus-4/mp-lr-3e-02-r-8-l-true/config.json not found"
9279,b'[Refactor] Splitting pipelines.py into its own module.',2020-12-23T11:11:47Z,2021-01-06T08:33:51Z,,,
9278,b'LED',2020-12-23T10:50:03Z,2021-01-05T12:14:31Z,PR for Model Addition,,
9277,b'[Seq2Seq Templates] Fix check_repo.py templates file',2020-12-23T10:26:15Z,2020-12-23T10:40:21Z,,,
9276,b'Vision Transformer',2020-12-23T10:21:13Z,2021-09-17T12:52:35Z,New model,,
9275,b'Disable progress bar for Trainer',2020-12-23T10:19:38Z,2020-12-23T17:50:59Z,,,
9274,b'Loss printed by tensorflow fit() differs from loss using custom loop for RoBERTa',2020-12-23T10:17:20Z,2021-04-24T15:02:56Z,,,
9273,b'Fix param error',2020-12-23T09:56:57Z,2020-12-23T10:34:58Z,,TypeError,"TypeError: forward() got an unexpected keyword argument 'token_type_ids'"
9272,b'Fix gpt2 document',2020-12-23T07:58:19Z,2020-12-23T10:34:16Z,,AttributeError,"AttributeError: 'GPT2DoubleHeadsModelOutput' object has no attribute 'lm_logits'"
9271,b'allow integer device for BatchEncoding',2020-12-23T05:54:21Z,2020-12-24T08:01:57Z,,,
9270,"b""how can I change the AlbertModel's vocab""",2020-12-23T02:39:41Z,2020-12-23T12:00:13Z,,,
9269,b'Output probability from model.generate',2020-12-23T00:04:24Z,2021-05-26T15:09:24Z,,,
9268,b'Unable to load LayoutLM from pretrained',2020-12-22T22:12:22Z,2020-12-23T12:49:38Z,,OSError,"OSError: "
9267,"b""[hf args] shouldn't match partial arg names""",2020-12-22T21:58:23Z,2020-12-23T16:59:27Z,,,
9266,b'Minor documentation revisions from copyediting',2020-12-22T21:37:57Z,2020-12-23T15:15:50Z,,,
9265,b'[finetune_trainer]  max length cl args redesign',2020-12-22T20:41:15Z,2021-03-18T01:28:27Z,,,
9264,b' compute_metrics in the trainer does not seem to be extensible ',2020-12-22T19:35:52Z,2021-04-24T15:02:56Z,,,
9263,b'Adds MuRIL - BERT based model for 17 Indian Languages to the library',2020-12-22T19:27:19Z,2021-04-24T15:02:57Z,,,
9262,b'Revert renaming in finetune_trainer',2020-12-22T19:13:54Z,2020-12-22T20:42:35Z,,,
9261,b'[seq2seq] memory regression',2020-12-22T18:33:49Z,2021-01-21T17:05:47Z,,,
9260,b'Add speed metrics to all example scripts + template',2020-12-22T16:55:00Z,2020-12-22T19:02:26Z,,,
9259,b'Fix script that check objects are documented',2020-12-22T15:47:36Z,2020-12-22T16:12:59Z,,,
9258,"b""torch.hub colab doesn't work""",2020-12-22T15:27:15Z,2020-12-22T15:29:23Z,,,
9257,b'Pegasus Documentation May Conflict With Seq2Seq ReadMe',2020-12-22T15:17:44Z,2021-03-06T00:14:03Z,wontfix,,
9256,b'[EncoderDecoder] Make tests more aggressive',2020-12-22T14:48:39Z,2020-12-22T16:00:05Z,,,
9255,b'Fix link to bertabs/README.md',2020-12-22T14:31:47Z,2020-12-22T16:41:24Z,,,
9254,b'Fix link to old language modeling script',2020-12-22T14:25:52Z,2020-12-22T16:40:48Z,,,
9253,b'Prediction problem of glue task',2020-12-22T10:54:34Z,2021-03-06T00:14:05Z,wontfix,,
9252,b'Fix TF BART for saved model creation',2020-12-22T10:45:21Z,2020-12-22T17:07:05Z,,,
9251,b'Model Templates for Seq2Seq',2020-12-22T08:30:48Z,2020-12-22T22:41:21Z,,,
9250,b'ValueError: Tokenizer class T5Tokenizer does not exist or is not currently imported.',2020-12-22T07:14:51Z,2021-03-06T00:14:06Z,wontfix,ValueError,"ValueError: Tokenizer class T5Tokenizer does not exist or is not currently imported.`"
9249,b'GPT2 distributed TPU pre-training using run_clm.py ',2020-12-22T06:30:55Z,2020-12-22T14:51:00Z,,torch.multiprocessing.spawn.ProcessExitedException,"torch.multiprocessing.spawn.ProcessExitedException: process 0 terminated with exit code 17"
9248,b'numpy ndarray type is not allowed on process pytorch model',2020-12-22T06:16:19Z,2021-02-03T06:09:17Z,,ValueError,"ValueError: Data of type <class 'numpy.ndarray'> is not allowed only (<class 'tensorflow.python.framework.ops.Tensor'>, <class 'bool'>, <class 'int'>, <class 'transformers.file_utils.ModelOutput'>, <class 'tuple'>, <class 'list'>, <class 'dict'>) is accepted for attention_mask."
9247,b'T5 tokenizer.vocab_size and config.vocab_size mismatch?',2020-12-22T05:02:11Z,2020-12-22T11:48:14Z,,,
9246,"b""AssertionError: Non-consecutive added token '<pad>' found. Should have index 40002 but has index 40000 in saved vocabulary""",2020-12-22T04:35:38Z,2021-04-24T15:02:58Z,,AssertionError,"AssertionError: Non-consecutive added token '<pad>' found. Should have index 40002 but has index 40000 in saved vocabulary."
9245,b'[s2s] test_finetune_trainer_slow fails when run in group',2020-12-22T04:29:31Z,2021-03-06T00:14:07Z,wontfix,,
9244,b'BatchEncoding.to accepted types too restrictive',2020-12-22T03:47:37Z,2020-12-24T08:01:57Z,,,
9243,b'AssertionError with model_parallel in run_clm.py',2020-12-22T01:27:58Z,2021-03-06T00:14:09Z,wontfix,"AssertionError, subprocess.CalledProcessError","AssertionError: DistributedDataParallel device_ids and output_device arguments only work with single-device CUDA modules, but got device_ids [2], output_device 2, and module parameters {device(type='cpu')}.subprocess.CalledProcessError: Command '['/opt/conda/bin/python', '-u', 'run_clm.py', '--local_rank=3', '--do_train', '--do_eval', '--fp16', '--logging_first_step', '--model_parallel', '--evaluation_strategy', 'epoch', '--logging_steps', '50', '--model_name_or_path', 'gpt2', '--model_type', 'gpt2', '--num_train_epochs', '1', '--output_dir', '/opt/ml/model/', '--per_device_eval_batch_size', '2', '--per_device_train_batch_size', '2', '--save_steps', '50', '--save_total_limit', '1', '--train_file', '/opt/ml/input/data/data/train.txt', '--validation_file', '/opt/ml/input/data/data/val.txt']' returned non-zero exit status 1."
9242,b'Load from a TF 1.0 checkpoint in modeling_tf_utils.py',2020-12-22T00:43:48Z,2021-03-06T00:14:10Z,wontfix,,
9241,b'Seq2seq trainer',2020-12-21T22:36:01Z,2020-12-22T16:33:44Z,,,
9240,b'Help: How to deploy a fine tuned t5 model in production',2020-12-21T19:13:27Z,2020-12-21T20:54:42Z,,,
9239,b'Adding performer fine-tuning research exampke',2020-12-21T18:50:34Z,2020-12-21T20:19:41Z,,,
9238,b'Bug SqueezeBERT stops with no error ',2020-12-21T17:48:19Z,2021-03-06T00:14:12Z,wontfix,,
9237,b'Update the README of the text classification example',2020-12-21T16:34:50Z,2020-12-21T20:23:41Z,,,
9236,b'mBART finetuned on XSUM',2020-12-21T16:29:17Z,2021-03-06T00:14:13Z,wontfix,,
9235,b'run_mlm.py crashes when saving model checkpoint',2020-12-21T15:54:45Z,2021-03-06T00:14:15Z,wontfix,TypeError,"TypeError: expected str, bytes or os.PathLike object, not NoneType"
9234,b'Fix TF template',2020-12-21T12:41:15Z,2020-12-21T12:52:17Z,,,
9233,b'[MPNet] Add slow to fast tokenizer converter',2020-12-21T11:19:22Z,2020-12-21T14:41:35Z,,,
9232,b'command line_by_line missing in https://github.com/huggingface/transformers/tree/master/examples/language-modeling',2020-12-21T11:13:14Z,2021-03-06T00:14:17Z,wontfix,,
9231,b'[T5] Fix warning for changed EncDec Attention Bias weight',2020-12-21T09:33:23Z,2020-12-21T09:41:34Z,,,
9230,b'add base model classes to  bart subclassed models',2020-12-21T07:58:03Z,2020-12-21T14:26:47Z,,,
9229,b'Generate function does not work with GPU',2020-12-21T07:29:03Z,2021-03-06T00:14:18Z,wontfix,`RuntimeError,"`RuntimeError: Input, output and indices must be on the current device`"
9228,b'Differences between original implementation and HuggingFace implementation',2020-12-21T06:19:42Z,2021-03-06T00:14:19Z,wontfix,,
9227,"b""Can't lazy initialize BART model on GPU""",2020-12-21T06:17:12Z,2020-12-22T07:16:33Z,,RuntimeError,"RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method"
9226,"b'n_gpus is set to 1 in case of distributed training on multiple gpus, how to access to the correct n_gpus'",2020-12-21T00:34:17Z,2020-12-21T03:09:57Z,,,
9225,b'n_gpu=1 in case of using distributed pytorch launcher ',2020-12-21T00:33:34Z,2020-12-21T03:10:11Z,,,
9224,b'allowing the user to set booleans with HfArgumentParser',2020-12-20T20:43:36Z,2020-12-21T09:46:09Z,,"ValueErrorValueError, subprocess.CalledProcessError","ValueErrorValueError: : Some specified arguments are not used by the HfArgumentParser: ['True', 'true', 'False', 'False', 'True', 'True', 'True', 'True', 'True', 'True', 'True', '--add_adapters_in_decoder', 'False']Some specified arguments are not used by the HfArgumentParser: ['True', 'true', 'False', 'False', 'True', 'True', 'True', 'True', 'True', 'True', 'True', '--add_adapters_in_decoder', 'False']subprocess.CalledProcessError: Command '['/opt/conda/envs/internship/bin/python', '-u', 'finetune_t5_trainer.py', '--local_rank=1', '--model_name_or_path', 't5-small', '--tokenizer_name', 't5-small', '--learning_rate', '1e-2', '--output_dir', 'outputs/test', '--max_source_length', '128', '--max_target_length', '128', '--val_max_target_length', '128', '--test_max_target_length', '128', '--num_train_epochs', '10', '--warmup_steps', '500', '--eval_steps', '200', '--overwrite_output_dir', 'True', '--tasks', '[scitail,', 'boolq]', '--eval_tasks', '[rte,', 'boolq]', '--sampling', 'true', '--label_smoothing', '0.1', '--freeze_encoder', 'False', '--freeze_embeds', 'False', '--per_device_train_batch_size', '64', '--per_device_eval_batch_size', '64', '--save_steps', '20', '--logging_first_step', 'True', '--logging_steps', '200', '--save_total_limit', '1', '--train_adapters', 'True', '--adapter_config_name', 'parametric-meta-adapter', '--temperature', '10', '--do_eval', 'True', '--predict_with_generate', 'True', '--n_train', '10', '--task_embedding_dir', 'test_data/task_embeddings/n-train-all', '--task_embedding_dim', '512', '--n_val', '10', '--n_train', '10', '--do_finetune', 'True', '--do_train', 'True', '--n_finetune', '100', '--eval_output_dir', 'outputs/eval_test', '--reduction_factor', '16', '--non_linearity', 'relu', '--train_task_embeddings', 'True', '--projected_task_embedding_dim', '512', '--add_adapters_in_decoder', 'False', '--unfreeze_lm_head', '--unfreeze_layer_norms']' returned non-zero exit status 1."
9223,b'passing config file to train with on multiple gpus',2020-12-20T20:11:34Z,2020-12-20T21:29:42Z,,subprocess.CalledProcessError,"subprocess.CalledProcessError: Command '['/opt/conda/envs/internship/bin/python', '-u', 'finetune_t5_trainer.py', '--local_rank=1', 'configs/experiments/test.json']' returned non-zero exit status 2."
9222,b'Does provide any scripts about how to convert mpnet pretrain model to transformers pretrain one?',2020-12-20T04:52:18Z,2020-12-22T02:18:32Z,,,
9221,b'TAPAS: IndexError: index out of range in self',2020-12-20T04:35:34Z,2021-01-11T13:06:09Z,,IndexError,"IndexError: index out of range in self"
9220,"b'Proposed Fix : [RagSequenceForGeneration] generate ""without"" input_ids '",2020-12-20T04:22:34Z,2020-12-24T12:38:01Z,,,
9219,b'Fix beam search generation for GPT2 and T5 on model parallelism',2020-12-19T23:34:39Z,2020-12-21T13:05:24Z,,,
9218,"b""run_clm.py AttributeError: 'NoneType' object has no attribute 'keys'""",2020-12-19T21:56:21Z,2020-12-22T06:13:05Z,,AttributeError,"AttributeError: 'NoneType' object has no attribute 'keys'"
9217,b'Fix documentation links always pointing to master.',2020-12-19T18:49:49Z,2021-01-05T11:18:49Z,,,
9216,b'checkpoint callbacks ',2020-12-19T18:03:34Z,2020-12-19T19:48:26Z,,,
9215,"b'File ""/opt/conda/envs/updated/lib/python3.7/site-packages/fairscale/nn/data_parallel/sharded_ddp.py"", line 280, in _setup_backward_hooks     assert p_tmp.grad_fn is not None'",2020-12-19T16:57:52Z,2020-12-19T17:29:25Z,,"subprocess.CalledProcessError, ImportError","subprocess.CalledProcessError: Command '['/opt/conda/envs/updated/bin/python', '-u', 'finetune_trainer.py', '--local_rank=1', '--model_name_or_path', 't5-small', '--output_dir', 'output_dir', '--adam_eps', '1e-06', '--data_dir', 'wmt_en_ro', '--do_train', '--freeze_embeds', '--label_smoothing', '0.1', '--learning_rate', '3e-5', '--logging_first_step', '--logging_steps', '1000', '--max_source_length', '128', '--max_target_length', '128', '--num_train_epochs', '1', '--overwrite_output_dir', '--per_device_train_batch_size', '4', '--sortish_sampler', '--task', 'translation', '--val_max_target_length', '128', '--warmup_steps', '500', '--n_train', '500', '--sharded_ddp']' returned non-zero exit status 1.ImportError: Sharded DDP training requires fairscale: `pip install fairscale`."
9214,b'Save underlying BertModel only',2020-12-19T14:28:07Z,2021-03-06T00:14:21Z,wontfix,,
9213,b'bert-mlm-converting from tensorflow to pytorch',2020-12-19T14:09:51Z,2021-03-06T00:14:22Z,wontfix,,
9212,b'LXMERT cross_modality_matching logits order in code vs. documentation',2020-12-19T10:08:13Z,2021-03-06T00:14:23Z,wontfix,,
9211,b'[trainer] deepspeed integration',2020-12-19T07:13:32Z,2021-01-13T03:05:19Z,DeepSpeed,,
9210,b'MLFlow logger breaks training',2020-12-19T06:24:57Z,2021-03-06T00:14:24Z,wontfix,,
9209,b'Error while loading model file - .ckpt file :: Missing key(s) in state_dict',2020-12-19T05:56:30Z,2020-12-21T10:53:33Z,,,
9208,b'[docs] outline sharded ddp doc',2020-12-19T05:34:13Z,2021-01-06T01:34:16Z,,,
9207,b'Saving Pretrained Tokenizer',2020-12-19T04:39:18Z,2021-03-06T00:14:25Z,wontfix,,
9206,"b""DataTrainingArguments: __init__() got an unexpected keyword argument 'evaluate_during_training'""",2020-12-19T03:12:09Z,2020-12-21T15:32:49Z,,TypeError,"TypeError: __init__() got an unexpected keyword argument 'evaluate_during_training'"
9205,b'[model_utils] very slow model instantiation',2020-12-19T02:44:41Z,2021-05-05T15:22:21Z,"Performance, WIP",,
9204,b'Load saved Pytorch model into Tensorflow or convert from Pytorch model to TF',2020-12-19T01:00:51Z,2021-01-17T00:25:48Z,,,
9203,b'[finetune trainer] better logging and help',2020-12-19T00:42:43Z,2020-12-20T18:28:28Z,,,
9202,b'[wanted] explicit docs for inherited methods ',2020-12-18T23:40:29Z,2021-06-02T16:21:06Z,WIP,,
9201,b'when to use sortish sampler ',2020-12-18T23:20:37Z,2021-03-06T00:14:29Z,wontfix,,
9200,b'Beam search fails when using model parallelism',2020-12-18T23:08:13Z,2020-12-21T13:33:45Z,,RuntimeError,"RuntimeError: Input, output and indices must be on the current device"
9199,b'[t5 doc] typos',2020-12-18T22:46:08Z,2020-12-19T00:03:28Z,,,
9198,b'[run_glue] add speed metrics',2020-12-18T21:50:34Z,2020-12-19T01:09:30Z,,,
9197,b'[RAG] Add Ray implementation for distributed retrieval',2020-12-18T19:33:10Z,2020-12-21T09:39:31Z,,,
9196,b'Add timing inside Trainer',2020-12-18T18:50:56Z,2020-12-18T20:10:39Z,,,
9195,"b'Error ""if input.dim() == 2 and bias is not None""'",2020-12-18T17:53:38Z,2020-12-18T21:14:42Z,,AttributeError,"AttributeError: 'str' object has no attribute 'dim'"
9194,b'Loading MPNet from disc: ValueError: An instance of tokenizer class MPNetTokenizer cannot be converted in a Fast tokenizer instance.',2020-12-18T17:12:29Z,2020-12-21T14:41:35Z,,ValueError,"ValueError: An instance of tokenizer class MPNetTokenizer cannot be converted in a Fast tokenizer instance. No converter was found. Currently available slow->fast convertors: ['AlbertTokenizer', 'BartTokenizer', 'BarthezTokenizer', 'BertTokenizer', 'CamembertTokenizer', 'DistilBertTokenizer', 'DPRReaderTokenizer', 'DPRQuestionEncoderTokenizer', 'DPRContextEncoderTokenizer', 'ElectraTokenizer', 'FunnelTokenizer', 'GPT2Tokenizer', 'HerbertTokenizer', 'LayoutLMTokenizer', 'LongformerTokenizer', 'LxmertTokenizer', 'MBartTokenizer', 'MobileBertTokenizer', 'OpenAIGPTTokenizer', 'PegasusTokenizer', 'ReformerTokenizer', 'RetriBertTokenizer', 'RobertaTokenizer', 'SqueezeBertTokenizer', 'T5Tokenizer', 'XLMRobertaTokenizer', 'XLNetTokenizer']"
9193,b'Full rework of the TF input/output embeddings and bias resizing',2020-12-18T16:28:25Z,2021-01-11T11:27:29Z,,,
9192,b'example code for fine-tuning CLM does not work for GPT',2020-12-18T16:18:01Z,2020-12-18T21:41:51Z,,RuntimeError,"RuntimeError: The size of tensor a (1024) must match the size of tensor b (512) at non-singleton dimension 1"
9191,b'Segfault on python 3.9 exit',2020-12-18T15:18:11Z,2021-01-27T15:16:06Z,,,
9190,b'Addition of MuRIL - BERT based model for 17 Indian Languages to the library',2020-12-18T15:03:18Z,2020-12-22T19:32:23Z,,,
9189,b'GPT-model attention heads pruning example',2020-12-18T15:00:56Z,2020-12-18T21:32:10Z,,,
9188,b'MRPC Reproducibility with transformers-4.1.0',2020-12-18T12:45:20Z,2020-12-18T13:50:47Z,,,
9187,b'Problem with pretraining GPT-2 on TPU with Pytorch/XLA',2020-12-18T11:21:04Z,2020-12-26T06:32:52Z,,RuntimeError,"RuntimeError: tensorflow/compiler/xla/xla_client/mesh_service.cc:316 : Check failed: impl_->channel->WaitForConnected( std::chrono::system_clock::now() + std::chrono::seconds(connect_wait_seconds)) "
9186,b'fixed not JSON serializable error in run_qa.py with fp16',2020-12-18T09:40:50Z,2020-12-18T12:53:24Z,,TypeError,"TypeError: Object of type 'float16' is not JSON serializable"
9185,b'can we use ckpt model file generated after finetuning the pre-trained models on custom dataset',2020-12-18T09:06:55Z,2021-03-06T00:14:31Z,wontfix,,
9184,"b'[RagSequenceForGeneration] generate ""without"" input_ids'",2020-12-18T08:51:55Z,2020-12-24T12:58:15Z,,,
9183,"b'Add caching mechanism to BERT, RoBERTa'",2020-12-18T08:38:51Z,2020-12-23T17:31:33Z,,,
9182,b'Fix link to old NER fine-tuning script',2020-12-18T00:11:46Z,2020-12-18T00:50:02Z,,,
9181,b'Fix link to old SQUAD fine-tuning script',2020-12-17T23:58:17Z,2020-12-18T14:12:11Z,,,
9180,b'[trainer] apex fixes and tests',2020-12-17T23:55:41Z,2020-12-18T00:49:11Z,,,
9179,"b""[trainer]  speed issues: --fp16 doesn't improve speed, DP runs really slow """,2020-12-17T22:45:01Z,2021-03-06T00:14:32Z,wontfix,,
9178,b'[ci] install fairscale on self-runner CIs',2020-12-17T22:28:45Z,2021-04-24T15:02:59Z,,,
9177,b'add tests for the new sharded ddp fairscale integration',2020-12-17T22:11:04Z,2020-12-17T22:24:03Z,,,
9176,b'[setup] correct transformers version format',2020-12-17T22:06:28Z,2020-12-18T13:55:56Z,,,
9175,b'Add new run_swag example',2020-12-17T20:53:46Z,2020-12-18T19:19:25Z,,,
9174,b'[WIP] Adapt Cookie Cutter For EncoderDecoder',2020-12-17T19:51:19Z,2020-12-22T08:43:18Z,,,
9173,b'GPT2 eval with attention_mask not returning expected result',2020-12-17T18:18:05Z,2020-12-21T12:48:45Z,,,
9172,"b'[Flax] Implement FlaxElectraModel, FlaxElectraForMaskedLM, FlaxElectraForPreTraining'",2020-12-17T18:10:20Z,,WIP,,
9171,b'Trainer returns logits of only one sequence instead of entire evaluation dataset',2020-12-17T17:09:20Z,2020-12-19T13:13:05Z,,,
9170,b'Put all models in the constants',2020-12-17T16:04:45Z,2020-12-17T16:23:22Z,,,
9169,b'Added TF TransfoXL Sequence Classification',2020-12-17T15:16:21Z,2020-12-19T13:44:05Z,,,
9168,b'Fix gradient clipping for Sharded DDP',2020-12-17T14:25:41Z,2020-12-17T14:44:24Z,,,
9167,b'Add disclaimer to TAPAS rst file',2020-12-17T14:23:17Z,2020-12-17T14:34:07Z,,,
9166,b'Language modeling logging',2020-12-17T13:57:18Z,2020-12-18T18:10:19Z,,,
9165,b'Roberta python Tokenizer encodes differently across transformers==2.11 and transformers==4.0.1',2020-12-17T13:47:49Z,2020-12-22T19:27:08Z,,,
9164,b'Add clipping to relative positional embedding',2020-12-17T13:30:18Z,2021-06-03T15:12:14Z,,,
9163,b'Fix mixed precision in TF models',2020-12-17T12:53:05Z,2021-01-21T12:00:12Z,,,
9162,b'IndexError: index out of range in self while using longformers when i try to pass token_type_ids',2020-12-17T08:34:31Z,2021-03-06T00:14:35Z,wontfix,IndexError,"IndexError: index out of range in self"
9161,b'Metric calculation across batches in seq2seq examples',2020-12-17T05:59:23Z,2021-03-01T09:49:40Z,,,
9160,b'Trainer bug? Loss and logits are \xe2\x80\x9cnan\xe2\x80\x9d when fine-tuning NLI model (both RoBERTa/BART)',2020-12-16T23:54:37Z,2020-12-17T14:30:29Z,,,
9159,b'Unified transformer interface',2020-12-16T23:07:25Z,2021-03-06T00:14:37Z,wontfix,,
9158,"b""Getting a 404 error when loading 'model=facebook/bart-large-mnli' from pipeline('zero-shot-classification')""",2020-12-16T20:25:38Z,2020-12-16T20:44:36Z,,"HTTPError, OSError","HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/facebook/bart-large-mnli/resolve/main/tf_model.h5OSError: Can't load weights for 'facebook/bart-large-mnli'. Make sure that:"
9157,b'T5 checkpoint contains weights missing on current model.',2020-12-16T19:35:39Z,2020-12-17T11:20:17Z,,,
9156,b'Sharded DDP training fails with seq2seq models',2020-12-16T18:50:31Z,2021-01-06T01:33:52Z,,KeyError,"KeyError: Parameter containing:"
9155,b'evaluate_during_training is not acceptable in newer version of the Transformer',2020-12-16T16:45:22Z,2020-12-16T17:24:04Z,,,
9154,b'AutoModelForTableQuestionAnswering',2020-12-16T15:49:26Z,2020-12-16T17:14:33Z,,,
9153,b'BertForSequenceClassification and DistilBertForSequenceClassification use pooler output in different ways',2020-12-16T15:36:36Z,2020-12-16T22:04:55Z,,,
9152,"b""Add message to documentation that longformer doesn't support token_type_ids""",2020-12-16T15:16:11Z,2020-12-16T16:06:15Z,,,
9151,b'Added TF CTRL Sequence Classification',2020-12-16T13:15:04Z,2020-12-17T23:10:58Z,,,
9150,"b'Add flags to return scores, hidden states and / or attention weights in GenerationMixin'",2020-12-16T12:56:03Z,2021-01-06T16:11:43Z,"Core: Modeling, Ex: Generation",,
9149,b'Log metrics along with hparams in TensorBoardCallback',2020-12-16T12:15:28Z,2021-03-06T00:14:38Z,wontfix,,
9148,b'DistilBertForSequenceClassification',2020-12-16T10:26:53Z,2020-12-16T14:21:43Z,,,
9147,b'BertTokenizer.from_pretrained fails for local_files_only=True when added_tokens.json is missing',2020-12-16T08:28:14Z,2021-01-28T08:29:14Z,,ValueError,"ValueError: Cannot find the requested files in the cached path and outgoing traffic has been disabled. To enable model look-ups and downloads online, set 'local_files_only' to False."
9146,b'Ray tune hyperparameters search error',2020-12-16T05:08:34Z,2021-01-25T10:01:56Z,,redis.exceptions.ConnectionError,"redis.exceptions.ConnectionError: Error 104 while writing to socket. Connection reset by peer."
9145,b'TableQuestionAnsweringPipeline',2020-12-16T02:25:45Z,2020-12-16T17:31:50Z,,,
9144,b'Saving model errors',2020-12-16T00:48:01Z,2021-03-06T00:14:39Z,wontfix,TypeError,"TypeError: a bytes-like object is required, not 'str'"
9143,"b""Pass kwargs to Pipeline's tokenizer call""",2020-12-15T22:46:41Z,2021-01-16T09:18:26Z,,,
9142,b'RAGRetriever loads dataset in the default cache dir even if a different one is specified',2020-12-15T22:13:10Z,2021-04-24T15:03:00Z,,,
9141,b'Support for private models from huggingface.co',2020-12-15T22:02:22Z,2020-12-16T15:09:58Z,,,
9140,b'Fix T5 Encoder model parallel tests',2020-12-15T21:03:52Z,2020-12-15T21:04:00Z,,,
9139,b'Experimental support for fairscale ShardedDDP',2020-12-15T20:47:32Z,2020-12-16T18:47:48Z,,,
9138,b'adapting trainer.py for multiple optimizers ',2020-12-15T19:32:16Z,2020-12-16T14:14:09Z,,,
9137,b'Add possibility to switch between APEX and AMP in Trainer',2020-12-15T19:30:54Z,2020-12-15T21:38:11Z,,,
9136,b'Update notebook table and transformers intro notebook',2020-12-15T18:42:46Z,2020-12-16T15:24:31Z,,,
9135,b'Fix Bart Shift',2020-12-15T17:42:20Z,2020-12-15T18:04:32Z,,,
9134,b'[Bart] Correct wrong order in shift token to right in Bart',2020-12-15T17:27:03Z,2020-12-15T17:38:32Z,,,
9133,b'[Examples] Add automatic dataset splitting in language-modeling examples',2020-12-15T17:17:20Z,2020-12-15T21:02:44Z,,,
9132,b'Fix typo in trainer_tf.py',2020-12-15T17:04:05Z,2020-12-15T17:12:29Z,,,
9131,b'[Bart] fix bart loss masking',2020-12-15T16:52:07Z,2020-12-15T17:17:18Z,,,
9130,b'Trainer: support iterable datasets for evaluation ',2020-12-15T16:39:45Z,2020-12-15T17:33:58Z,,ValueError,"ValueError: DataLoader with IterableDataset: expected unspecified sampler option, but got sampler=<torch.utils.data.sampler.SequentialSampler object at 0x7f53900c50d0>"
9129,b'Fix TF Transfo XL',2020-12-15T16:37:24Z,2020-12-15T20:16:57Z,,,
9128,b'BartForCausalLM analogs to `ProphetNetForCausalLM`',2020-12-15T16:13:07Z,2021-02-04T08:56:13Z,,,
9127,b'[Flax] Bugfixes in `run_mlm_flax.py`',2020-12-15T16:05:53Z,2021-04-25T15:04:10Z,,,
9126,b'seq2seq finetuning scripts break before training (cannot import name ParallelMode)',2020-12-15T15:32:57Z,2020-12-15T15:37:04Z,,ImportError,"ImportError: cannot import name 'ParallelMode' from 'transformers.training_args'"
9125,b'Predict single sentence for Glue Tasks',2020-12-15T15:04:05Z,2020-12-16T12:07:24Z,,,
9124,b'Improve BERT-like models performance with better self attention',2020-12-15T13:07:04Z,2020-12-21T12:10:16Z,,,
9123,b'BART cannot accept -100 as ignored label',2020-12-15T12:56:05Z,2020-12-15T17:17:17Z,,,
9122,"b'RobertaTokenizer fails to do_lower_case, different behavior between version 2 and 3'",2020-12-15T12:20:22Z,2021-04-25T15:04:11Z,,,
9121,b'[Generation] Add generation outputs',2020-12-15T11:03:39Z,2021-01-06T16:11:43Z,,,
9120,b'Fix tf2.4',2020-12-15T10:20:50Z,2020-12-15T15:10:47Z,,,
9119,"b'Which dataset is used for training GPT, GPT2 from scratch?'",2020-12-15T09:00:01Z,2020-12-15T14:16:06Z,,,
9118,b'Different inference results of a keras including transformer model on TPU vs CPU?',2020-12-15T08:39:37Z,2020-12-15T14:15:40Z,,,
9117,b'Tapas v4 (tres)',2020-12-15T08:09:38Z,2020-12-15T22:08:49Z,"model card, PR for Model Addition",,
9116,b'Roberta training crashing due to position_id embedding',2020-12-15T06:47:16Z,2021-04-25T15:04:12Z,,,
9115,b'[doc] pytorch native amp leak fix landed in 1.7.1',2020-12-15T04:42:36Z,2020-12-15T14:10:42Z,,,
9114,b'Fix stack overflow',2020-12-15T04:14:56Z,2020-12-15T14:15:49Z,,,
9113,b'Some Models do not support gradient checkpointing',2020-12-15T03:28:41Z,2021-04-25T15:04:14Z,,,
9112,b'Add BORT',2020-12-15T00:24:40Z,2021-01-27T08:04:10Z,New model,,
9111,b'Longformer `token_type_ids` Vocabulary Size is 1 But Documentation States Otherwise',2020-12-14T22:39:35Z,2020-12-16T16:06:15Z,,IndexError,"IndexError: index out of range in self"
9110,b'Not able to train RoBERTa language model from scratch',2020-12-14T21:04:15Z,2020-12-15T19:34:46Z,,,
9109,b'Cannot disable logging from trainer module',2020-12-14T19:55:26Z,2020-12-15T14:49:33Z,,,
9108,b'Time for second encoding is much higher than first time',2020-12-14T18:36:55Z,2021-04-25T15:04:15Z,,,
9107,b'Fix T5 model parallel test',2020-12-14T18:32:01Z,2020-12-15T14:51:13Z,,,
9106,b'Cannot load community model on local machine',2020-12-14T15:37:08Z,2020-12-14T15:54:06Z,,OSError,"OSError: Can't load config for 'huggingtweets/xinqisu'. Make sure that:"
9105,b'Added TF OpenAi GPT1 Sequence Classification',2020-12-14T14:47:08Z,2020-12-15T16:27:09Z,,,
9104,b'Cannot load custom tokenizer for Trainer',2020-12-14T14:20:31Z,2020-12-14T14:52:22Z,,,
9103,b'Seq2Seq training calculate_rouge with precision and recall',2020-12-14T13:57:17Z,2020-12-14T14:30:05Z,,TypeError,"TypeError: unsupported operand type(s) for /: 'dict' and 'int'"
9102,b'Unexpected logits shape on prediction with TFRobertaForSequenceClassification',2020-12-14T13:39:42Z,2020-12-14T21:54:59Z,,,
9101,b'Fix a broken link in documentation',2020-12-14T13:00:25Z,2020-12-14T14:12:28Z,Documentation,,
9100,b'Link to BERTology example is broken',2020-12-14T12:57:36Z,2020-12-14T14:12:28Z,Documentation,,
9099,b'bug with _load_optimizer_and_scheduler in trainer.py',2020-12-14T11:00:20Z,2021-04-25T15:04:16Z,,,
9098,"b'[RAG, Bart] Align RAG, Bart cache with T5 and other models of transformers'",2020-12-14T10:36:53Z,2020-12-14T11:32:27Z,,,
9097,b'Is the LayoutLM working now?',2020-12-14T09:25:13Z,2021-04-25T15:04:17Z,,,
9096,b'Fix variable name in TrainingArguments docstring',2020-12-14T07:51:55Z,2020-12-14T14:02:55Z,,,
9095,b'[TorchScript] Received several warning during Summarization model conversion',2020-12-14T07:14:20Z,2020-12-15T04:49:51Z,,RuntimeError,"RuntimeError: Encountering a dict at the output of the tracer might cause the trace to be incorrect, this is only valid if the container structure does not change based on the module's inputs. Consider using a constant container instead (e.g. for `list`, use a `tuple` instead. for `dict`, use a `NamedTuple` instead). If you absolutely need this and know the side effects, pass strict=False to trace() to allow this behavior."
9094,b'head mask issue  transformers==3.5.1',2020-12-14T06:56:53Z,2021-04-26T15:02:37Z,,,
9093,b'Not able to load T5 tokenizer',2020-12-14T06:55:44Z,2020-12-14T09:53:50Z,,,
9092,b'Patch *ForCausalLM model with TF resize_token_embeddings',2020-12-14T05:30:42Z,2020-12-14T05:39:56Z,,,
9091,b'Chinese',2020-12-14T02:46:27Z,2020-12-14T03:34:14Z,,,
9090,b'run_clm example gives `CUDA out of memory. Tried to allocate` error',2020-12-14T02:17:31Z,2020-12-14T17:50:37Z,,RuntimeError,"RuntimeError: CUDA out of memory. Tried to allocate 384.00 MiB (GPU 0; 15.90 GiB total capacity; 14.75 GiB already allocated; 185.88 MiB free; 14.81 GiB reserved in total by PyTorch)"
9089,b'Fix a bug in eval_batch_retrieval of eval_rag.py',2020-12-14T01:49:45Z,2020-12-15T13:46:56Z,,AttributeError,"AttributeError: 'tuple' object has no attribute 'pooler_output'"
9088,b'run_clm.py Early stopping with ^C',2020-12-13T20:19:17Z,2021-04-16T11:22:20Z,,,
9087,b'BertForSequenceClassification finetune training loss and accuracy have some problem',2020-12-13T18:39:28Z,2020-12-14T03:34:03Z,,,
9086,"b""Getting a 404 error when loading TFXLMRobertaModel from 'xlm-roberta-large'""",2020-12-13T17:42:20Z,2020-12-14T17:38:06Z,,,
9085,b'Adding to docs how to train CTRL Model with control codes. ',2020-12-13T15:20:18Z,2021-04-26T15:02:38Z,,,
9084,b'Problem with Token Classification models ',2020-12-13T12:29:21Z,2020-12-14T03:18:30Z,,,
9083,b'Image rendering not working in example notebook',2020-12-12T21:36:15Z,2020-12-16T15:24:31Z,,,
9082,b'Add parallelization support for T5EncoderModel',2020-12-12T21:14:18Z,2020-12-14T17:00:46Z,,,
9081,b'Segmentation fault (core dumped) running run_qa.py',2020-12-12T20:30:08Z,2021-04-26T15:02:39Z,,,
9080,b'Fine tune GPT-2 pytorch',2020-12-12T15:21:19Z,2021-04-26T15:02:40Z,,,
9079,"b""T5 fails on many datasets with [libprotobuf FATAL /sentencepiece/src/../third_party/protobuf-lite/google/protobuf/repeated_field.h:1505] CHECK failed: (index) >= (0):  terminate called after throwing an instance of 'google::protobuf::FatalException'   what():  CHECK failed: (index) >= (0):  Aborted """,2020-12-12T14:41:00Z,2020-12-14T02:58:22Z,,,
9078,b'Add Definition of a transformer to the glossary',2020-12-12T14:23:02Z,2021-03-20T14:02:38Z,,,
9076,b'Clarify use of TrainingArguments.disable_tqdm in Jupyter Notebooks  ',2020-12-12T11:00:35Z,2020-12-15T14:00:20Z,,,
9075,b'Zero Shot Classification Pipeline gives poor results locally than online demo',2020-12-12T10:59:34Z,2020-12-15T06:38:45Z,,,
9072,b'get type error when I run the example code of token classification',2020-12-12T09:53:44Z,2021-04-26T15:02:40Z,,,
9071,b'attention_mask size',2020-12-12T03:32:21Z,2021-01-24T08:48:28Z,,,
9070,b'[CI doc] safely testing experimental CI features',2020-12-11T23:09:14Z,2020-12-14T15:35:00Z,,,
9069,b'Fix some typos',2020-12-11T21:20:22Z,2021-04-07T10:37:42Z,model card,,
9068,b'[wip] [ci] experiment for documentation',2020-12-11T20:49:44Z,2020-12-11T23:12:58Z,,,
9067,b'Fix min_null_pred in the run_qa script',2020-12-11T18:04:19Z,2020-12-11T21:26:05Z,,,
9066,b'Add BartForCausalLM analogs to `ProphetNetForCausalLM`',2020-12-11T16:26:12Z,2021-02-04T08:56:13Z,"Good First Issue, Good Second Issue",,
9065,b'Remove docs only check',2020-12-11T15:24:51Z,2020-12-11T15:27:32Z,,,
9064,b'Embedding documents on multi-GPU single-ode Docker using pretrained models of huggingface transformers and pytorch DistributedDataParallel',2020-12-11T15:11:53Z,2021-04-26T15:02:41Z,,,
9063,b'Fix T5 and BART for TF',2020-12-11T15:10:33Z,2020-12-14T17:47:01Z,,,
9062,b'Bump notebook from 6.1.4 to 6.1.5 in /examples/research_projects/movement-pruning/lxmert',2020-12-11T15:08:06Z,2020-12-11T15:32:44Z,dependencies,,
9061,b'CharacterBERT',2020-12-11T14:26:37Z,,New model,,
9060,"b""ImportError: cannot import name 'SAVE_STATE_WARNING' from 'torch.optim.lr_scheduler' - SAVE_STATE_WARNING has been removed from pytorch""",2020-12-11T13:30:30Z,2020-12-11T13:44:27Z,,ImportError,"ImportError: cannot import name 'SAVE_STATE_WARNING' from 'torch.optim.lr_scheduler' (......./my37/lib/python3.7/site-packages/torch/optim/lr_scheduler.py)"
9059,b'overflow_to_sample_mapping missing in in documentation',2020-12-11T12:37:46Z,,WIP,,
9058,"b'""resize_token_embeddings"" in BertForeMaskedLM won\'t change last linear layer ""output dimension""'",2020-12-11T12:28:25Z,2020-12-11T12:35:03Z,,,
9057,b'Having to specify too many `ignore_keys` in `Trainer.prediction_step`',2020-12-11T11:47:02Z,2020-12-12T16:25:39Z,,,
9056,b'Token classification example (run_ner.py) should work without fast tokenizers',2020-12-11T10:55:46Z,2021-04-26T15:02:42Z,,,
9055,"b""Can't load mt5 model after resizing token embedding""",2020-12-11T10:15:32Z,2021-04-26T15:02:43Z,,,
9054,"b'[Flax] Align FlaxBertForMaskedLM with BertForMaskedLM, implement from_pretrained, init'",2020-12-11T10:02:59Z,2020-12-16T12:03:33Z,,,
9053,b'TFTrainingArguments',2020-12-11T09:34:21Z,2020-12-15T17:12:29Z,,AttributeError,"AttributeError: 'TFTrainingArguments' object has no attribute 'evaluate_strategy'"
9052,b'Add caching mechanism to BERT/RoBERTa/GPT2 for Seq2Seq accelerated generation',2020-12-11T09:30:36Z,2020-12-23T17:31:33Z,Good Second Issue,,
9051,b'update tatoeba workflow',2020-12-11T09:09:15Z,2020-12-11T14:59:16Z,,,
9050,b'yuk',2020-12-11T08:26:54Z,2020-12-11T14:55:50Z,,,
9049,b'New version of flax requires frozen dicts',2020-12-11T04:47:13Z,2021-01-05T05:58:45Z,,,
9048,b'\xf0\x9f\x90\x9b [TFBART] LayerDrop not working on TPU',2020-12-11T00:53:09Z,2021-04-24T15:03:02Z,,ValueError,"ValueError: in user code:"
9047,b'Change nn.dropout to layer.Dropout in TFBart',2020-12-11T00:41:27Z,2020-12-11T09:40:26Z,,,
9046,b'BlenderBot RuntimeError: CUDA error: device-side assert triggered',2020-12-11T00:35:24Z,2020-12-15T17:17:18Z,,`RuntimeError,"`RuntimeError: CUDA error: device-side assert triggered`"
9045,"b'\xf0\x9f\x90\x9b [TF_BART] ""<internal expr>"" has dtype float32 in the TRUE branch, but dtype=int32 in the FALSE branch'",2020-12-11T00:10:24Z,2020-12-11T09:40:25Z,,TypeError,"TypeError: in user code:"
9044,"b'XLNet ONNX model giving error: ""Attempting to broadcast an axis by a dimension other than 1""'",2020-12-10T22:21:37Z,2021-04-24T15:03:02Z,,RuntimeException,"RuntimeException: [ONNXRuntimeError] : 6 : RUNTIME_EXCEPTION : Non-zero status code returned while running Add node. Name:'Add_26' Status Message: /onnxruntime_src/onnxruntime/core/providers/cpu/math/element_wise_ops.h:361 void onnxruntime::BroadcastIterator::Init(int64_t, int64_t) axis == 1 || axis == largest was false. Attempting to broadcast an axis by a dimension other than 1. 5 by 6"
9043,b'The example code does not work',2020-12-10T22:01:35Z,2020-12-10T22:18:29Z,,AttributeError,"AttributeError: 'tuple' object has no attribute 'start_logits"
9042,b'[finetune_trainer] enhancements and fixes',2020-12-10T21:56:50Z,2020-12-15T01:45:34Z,,,
9041,"b""google/bert2bert_L-24_wmt_de_en doesn't match official implementation""",2020-12-10T21:53:54Z,2021-06-15T15:07:09Z,,,
9040,b'Zero Shot Classification Pipeline fails when running in CPU-only Docker container',2020-12-10T21:35:05Z,2021-04-24T15:03:03Z,,,
9039,b'BERT outputs are different with the same input in training mode',2020-12-10T21:32:25Z,2021-04-24T15:03:04Z,,,
9038,b'Fix typo #9012 (#1)',2020-12-10T21:13:06Z,2020-12-10T21:41:00Z,,,
9037,b'fix the typo 9012',2020-12-10T20:30:28Z,2020-12-10T21:16:41Z,,,
9036,b'[docs] missing info on call back registry',2020-12-10T18:21:55Z,2021-04-05T16:27:23Z,Good First Issue,,
9035,b'Improve coverage of the documentation',2020-12-10T17:00:52Z,2021-04-12T15:59:47Z,"Documentation, Good First Issue",,
9034,b'Refactor FLAX tests',2020-12-10T16:55:24Z,2020-12-10T20:57:40Z,,,
9033,b'Make ProphetNetModel really compatible with EncoderDecoder',2020-12-10T16:08:19Z,2020-12-11T15:59:55Z,,,
9032,"b""ImportError: cannot import name 'DPRReader' from 'transformers'""",2020-12-10T14:15:28Z,2021-03-06T00:14:41Z,wontfix,ImportError,"ImportError: cannot import name 'DPRReader' from 'transformers' (C:\<some_path>\env\lib\site-packages\transformers\__init__.py)"
9031,b'GPT2 attention mask',2020-12-10T14:10:36Z,2020-12-10T22:15:57Z,,,
9030,b'Initial README for `t5-small-indonesian-summarization-cased` model',2020-12-10T11:08:31Z,2020-12-11T14:17:30Z,model card,,
9029,b'[TF Bart] Refactor TFBart',2020-12-10T11:08:12Z,2020-12-15T16:31:29Z,,,
9028,b'Initial README for `t5-base-indonesian-summarization-cased` model',2020-12-10T10:59:24Z,2020-12-11T14:18:17Z,model card,,
9027,b'Uber AI plug and play language model (PPLM)',2020-12-10T10:07:41Z,2020-12-12T12:37:30Z,,,
9026,b'Compatibility scripts',2020-12-10T09:10:58Z,2021-03-06T00:14:43Z,wontfix,,
9025,b'Untranslation of some words from an external dictionary',2020-12-10T08:03:20Z,2020-12-11T14:06:55Z,,,
9024,b'Use Softmax classifiering for run_glue.py example',2020-12-10T07:08:45Z,2021-03-06T00:14:44Z,wontfix,ValueError,"ValueError: Expected input batch_size (768) to match target batch_size (3)."
9023,b'run_clm.py Issue | MODEL_FOR_CAUSAL_LM_MAPPING is None',2020-12-10T07:06:35Z,2021-03-06T00:14:45Z,wontfix,AttributeError,"AttributeError: 'NoneType' object has no attribute 'keys'"
9022,b'About the input of BERT',2020-12-10T05:38:24Z,2020-12-10T14:14:14Z,,,
9021,"b' Error tokenizer = AutoTokenizer.from_pretrained(""vinai/phobert-base"")'",2020-12-10T03:43:59Z,2021-03-06T00:14:46Z,wontfix,ValueError,ValueError: **Tokenizer class PhobertTokenizerFast does not exist or is not currently imported.**
9020,b'Fix typo in modeling_tf_bart',2020-12-10T03:32:55Z,2020-12-10T07:22:53Z,,,
9019,b'getattr introduces bug when setting booleans with config file ',2020-12-09T23:14:16Z,2021-03-06T00:14:48Z,wontfix,,
9018,b'Fix PreTrainedTokenizer.pad when first inputs are empty',2020-12-09T22:20:49Z,2020-12-11T15:25:01Z,,,
9017,b'Fix documention of book in LayoutLM',2020-12-09T22:02:45Z,2020-12-10T14:28:50Z,,,
9016,b'LayoutLM wrong shape for bbox in docs',2020-12-09T21:46:55Z,2020-12-10T14:28:50Z,,,
9015,b'MPNet copyright files',2020-12-09T21:30:01Z,2020-12-10T14:29:39Z,,,
9014,b'Enforce all objects in the main init are documented',2020-12-09T20:36:16Z,2020-12-10T16:57:14Z,,,
9013,b'[model_cards] Migrate cards from this repo to model repos on huggingface.co',2020-12-09T20:17:11Z,2020-12-11T23:24:42Z,"High-Level feature, Documentation, model card, cleanup",,
9012,"b'""run_mlm_wwm.py"", line 284 AttributeError: \'DataTrainingArguments\' object has no attribute \'valid_ref_file\''",2020-12-09T19:06:58Z,2020-12-10T20:16:22Z,,,
9011,b'Create README.md',2020-12-09T17:43:33Z,2020-12-09T17:48:46Z,model card,,
9010,b'Reorganize examples',2020-12-09T16:09:27Z,2020-12-11T15:07:03Z,,,
9009,b'fixes #8968',2020-12-09T15:14:19Z,2020-12-09T15:21:42Z,,,
9008,b'[Docs] Fix some typos for group beam search',2020-12-09T14:11:25Z,2020-12-09T14:14:34Z,,,
9007,b'Fix link to stable version in the doc navbar',2020-12-09T14:10:00Z,2020-12-09T14:11:40Z,,,
9006,b'Diverse beam search 2',2020-12-09T13:47:19Z,2020-12-09T14:00:37Z,,,
9005,b'Add the code_search_net datasets tag to CodeBERTa model cards',2020-12-09T12:12:41Z,2020-12-09T14:43:19Z,model card,,
9004,b'Add MP Net 2',2020-12-09T10:27:35Z,2020-12-09T15:34:31Z,PR for Model Addition,,
9003,b'Turn attentions and hidden-states into a tensor',2020-12-09T09:46:12Z,2020-12-14T18:45:11Z,,ValueError,"ValueError: Got a dictionary containing non-Tensor value (<tf.Tensor 'StatefulPartitionedCall:0' shape=(None, 12, None, None) dtype=float32>, <tf.Tensor 'StatefulPartitionedCall:1' shape=(None, 12, None, None) dtype=float32>, <tf.Tensor 'StatefulPartitionedCall:2' shape=(None, 12, None, None) dtype=float32>, <tf.Tensor 'StatefulPartitionedCall:3' shape=(None, 12, None, None) dtype=float32>, <tf.Tensor 'StatefulPartitionedCall:4' shape=(None, 12, None, None) dtype=float32>, <tf.Tensor 'StatefulPartitionedCall:5' shape=(None, 12, None, None) dtype=float32>, <tf.Tensor 'StatefulPartitionedCall:6' shape=(None, 12, None, None) dtype=float32>, <tf.Tensor 'StatefulPartitionedCall:7' shape=(None, 12, None, None) dtype=float32>, <tf.Tensor 'StatefulPartitionedCall:8' shape=(None, 12, None, None) dtype=float32>, <tf.Tensor 'StatefulPartitionedCall:9' shape=(None, 12, None, None) dtype=float32>, <tf.Tensor 'StatefulPartitionedCall:10' shape=(None, 12, None, None) dtype=float32>, <tf.Tensor 'StatefulPartitionedCall:11' shape=(None, 12, None, None) dtype=float32>) for key attentions in the output of the function __inference_serving_15889 used to generate a SavedModel signature. Dictionaries outputs for functions used as signatures should have one Tensor output per string key."
9002,b'Add TFRag',2020-12-09T09:42:39Z,2021-03-08T21:49:51Z,,,
9001,b'\xf0\x9f\x8c\x9f CTRLsum',2020-12-09T08:12:29Z,,New model,,
9000,b'ValueError: You have to specify either decoder_inputs or decoder_inputs_embeds',2020-12-09T04:57:17Z,2021-02-01T01:29:30Z,,ValueError,"ValueError: in user code:"
8999,b'AlbertTokenizer handles special tokens incorrectly',2020-12-09T04:40:58Z,,Good Second Issue,,
8998,b'Marge - Pre-training via Paraphrasing',2020-12-09T03:40:31Z,,New model,,
8997,b'[wip] [ci] doc-job-skip take #4.5 dry-run via github direct edit',2020-12-09T00:47:46Z,2020-12-11T17:13:37Z,,,
8996,b'Remove use of deprecated method in Trainer HP search',2020-12-09T00:24:32Z,2020-12-09T14:13:42Z,,,
8995,"b""AttributeError: 'Trainer' object has no attribute 'is_world_master'""",2020-12-08T23:51:30Z,2020-12-09T14:13:42Z,,,
8994,b'DistilBert PyTorch to TensorFlow conversion - input sequence length is max 5 tokens for tensorflow',2020-12-08T23:38:09Z,2020-12-10T21:44:16Z,,,
8993,b'Templates overhaul 1',2020-12-08T22:49:45Z,2020-12-08T23:00:07Z,,,
8992,b'New squad example',2020-12-08T19:11:09Z,2020-12-08T19:39:30Z,,,
8991,b'fixes #8968',2020-12-08T17:22:24Z,2020-12-09T15:06:03Z,,,
8990,"b'[Flax] Serialization, Design changes'",2020-12-08T16:34:08Z,2020-12-10T09:03:15Z,,,
8989,b'Make `ModelOutput` pickle-able',2020-12-08T16:24:35Z,2020-12-08T16:59:40Z,,,
8988,b'[WIP] Add Tapas (bis)',2020-12-08T16:17:45Z,2020-12-14T19:25:59Z,model card,,
8987,b'Tensor arrays',2020-12-08T16:02:20Z,2020-12-09T09:48:23Z,,,
8986,b'Checking output format + check raises ValueError',2020-12-08T15:49:04Z,2020-12-08T17:25:58Z,,,
8985,b'Remove value error',2020-12-08T13:01:58Z,2020-12-10T22:17:19Z,,,
8984,b'[libprotobuf FATAL /sentencepiece/src/../third_party/protobuf-lite/google/protobuf/repeated_field.h:1505] CHECK failed: (index) >= (0): ',2020-12-08T10:41:07Z,2021-03-06T00:14:51Z,wontfix,,
8983,"b'BertConfig.id2label use list instead of ""int: string"" dict'",2020-12-08T09:24:06Z,2021-03-06T00:14:52Z,wontfix,,
8982,b'[Example] Fix the argument name mismatch in the distillation example',2020-12-08T07:49:35Z,2021-03-06T00:14:54Z,wontfix,,
8981,b'Model templates overhaul',2020-12-08T05:21:03Z,2020-12-08T22:45:30Z,,,
8980,b'[wip] [ci] doc-job-skip take #4 dry-run',2020-12-08T05:00:07Z,2020-12-09T20:36:36Z,,,
8979,b'[training] SAVE_STATE_WARNING was removed in pytorch',2020-12-08T02:42:29Z,2020-12-08T05:59:56Z,,,
8978,b'Deepcopy and pickling fails for modeling_outputs',2020-12-08T02:07:18Z,2020-12-08T16:59:40Z,,TypeError,"TypeError: __init__() missing 1 required positional argument: 'last_hidden_state'"
8977,b'BertForMaskedLM train',2020-12-08T01:33:59Z,2020-12-08T05:35:29Z,,,
8976,b'Check table as independent script',2020-12-07T23:03:34Z,2020-12-08T00:55:13Z,,,
8975,b'Update quicktour docs to showcase the use of truncation',2020-12-07T22:56:36Z,2020-12-07T23:35:40Z,,,
8974,b'Add option to only check copies',2020-12-07T22:48:36Z,2020-12-07T23:03:50Z,,,
8973,b'Small fix to the run clm script',2020-12-07T22:13:50Z,2020-12-07T22:32:10Z,,,
8972,b'Removed unused `encoder_hidden_states` and `encoder_attention_mask` ',2020-12-07T20:27:00Z,2020-12-08T17:04:35Z,,,
8971,b'MPNet: Masked and Permuted Pre-training for Language Understanding',2020-12-07T19:02:47Z,2020-12-09T10:36:07Z,,,
8970,b'Copyright',2020-12-07T18:26:04Z,2020-12-07T23:36:34Z,,,
8969,b'MobileBERT decoder capabilities',2020-12-07T17:33:04Z,2020-12-08T17:04:35Z,,,
8968,"b""02-transformery.ipynb - output from model only strings 'last_hidden_state', 'pooler_output'""",2020-12-07T17:20:01Z,2020-12-09T15:21:42Z,,AttributeError,"AttributeError: 'str' object has no attribute 'shape'"
8967,b'EncoderDecoderModel works poorly with Mlflow',2020-12-07T17:18:10Z,2021-03-06T00:14:55Z,wontfix,`MlflowException,"`MlflowException: Param value '{'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'use_bfloat16': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'is_encoder_decoder': False, 'is_decoder': False, 'add_cross_attention': Fa' had length 1669, which exceeded length limit of 250`"
8966,b'Make loss function an init parameter',2020-12-07T16:43:24Z,2020-12-08T13:14:47Z,,,
8965,b'Remove sourcerer',2020-12-07T16:12:59Z,2020-12-07T16:15:30Z,,,
8964,b'Create README.md',2020-12-07T15:48:27Z,2020-12-07T21:04:14Z,model card,,
8963,b'PegasusTokenizer requires the SentencePiece library but it was not found in your environment',2020-12-07T15:42:05Z,2020-12-22T10:04:16Z,,ImportError,"ImportError: "
8962,b'Use word_ids to get labels in run_ner',2020-12-07T15:11:10Z,2020-12-07T19:26:37Z,,,
8961,b'Optional layers',2020-12-07T15:11:10Z,2020-12-08T14:14:10Z,,,
8960,b'TFBertModel NOT learning at all! ',2020-12-07T14:30:25Z,2020-12-07T16:09:58Z,,,
8959,"b""FileNotFoundError: [Errno 2] No such file or directory: 'cached_train_BertTokenizer_180.lock'""",2020-12-07T14:16:54Z,2020-12-08T10:19:12Z,,,
8958,b'run_ner.py with xlm-roberta-base raises an IndexError in tokenize_and_align_labels',2020-12-07T14:10:17Z,2020-12-07T19:26:37Z,,IndexError,"IndexError: list index out of range"
8957,b'Update README.txt',2020-12-07T13:11:19Z,2020-12-07T21:01:50Z,model card,,
8956,b'Update README.txt',2020-12-07T04:15:13Z,2020-12-07T21:01:38Z,model card,,
8955,"b""shutil.Error: Destination path '/home/ubuntu/.cache/huggingface/transformers/transformers' already exists""",2020-12-07T04:12:13Z,2020-12-07T16:37:58Z,,shutil.Error,"shutil.Error: Destination path '/home/ubuntu/.cache/huggingface/transformers/transformers' already exists"
8954,b'Fine-tuning on Language Model using two tasks',2020-12-07T02:42:25Z,2020-12-07T16:19:23Z,,,
8953,b'Wrong shape output for loss of TFGPT2LMHeadModel',2020-12-06T21:48:00Z,2021-04-15T15:05:54Z,,,
8952,b'batch_sampler with trainer.py would not set the epoch ',2020-12-06T21:14:00Z,2021-03-06T00:14:57Z,wontfix,,
8951,b'vocab_file and merges_file still required params for loading serialized tokenizers',2020-12-06T19:46:46Z,2021-03-06T00:14:59Z,wontfix,,
8950,b'Fix Code quality issues',2020-12-06T19:19:14Z,2021-03-12T09:42:08Z,,,
8949,b'Adds flashcards to Glossary & makes small corrections',2020-12-06T16:13:34Z,2021-01-20T18:28:41Z,,,
8948,b'Add model card',2020-12-06T15:48:28Z,2020-12-06T16:16:33Z,model card,,
8947,b'Fix QA pipeline on Windows',2020-12-06T15:13:54Z,2020-12-07T14:50:33Z,,,
8946,b'Error during validation Trainer step',2020-12-06T13:49:05Z,2020-12-07T16:40:40Z,,IndexError,"IndexError: tuple index out of range"
8945,b'Sparse Transormer',2020-12-06T11:10:33Z,,New model,,
8944,b'how to use EncoderDecoderModel to do en-de translation?',2020-12-06T10:58:57Z,,Good First Issue,,
8943,"b'Why BertSelfAttention reshape Q,K,V from 3-D tensor to 4-D tensor'",2020-12-06T09:15:53Z,2020-12-07T14:43:26Z,New model,,
8942,b'NER Pipeline Issue',2020-12-06T08:50:54Z,2021-05-17T15:03:42Z,,```ValueError,"```ValueError: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length.```"
8941,b'Error running source code -- import',2020-12-06T08:42:30Z,2021-03-06T00:15:00Z,wontfix,,
8940,"b""failure to use conda-forge apex  with torch1.6 and --amp_backend='apex' + --fp16_opt_level O1 """,2020-12-05T20:25:39Z,2021-03-06T00:15:02Z,wontfix,AttributeError,"AttributeError: 'function' object has no attribute '__self__'"
8939,b'sorry I mistakenly submitted a issue twice. Plz ignore (help delete) this one. ',2020-12-05T20:19:06Z,2020-12-05T20:27:28Z,,AttributeError,"AttributeError: 'function' object has no attribute '__self__'"
8938,b'MobileBertForSequenceClassification outputs super-high logits',2020-12-05T19:50:03Z,2021-03-06T00:15:04Z,wontfix,,
8937,b'Gradients of BERT layer outputs to inputs',2020-12-05T17:42:09Z,2020-12-07T14:32:01Z,,RuntimeError,"RuntimeError: One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior."
8936,b'Unexpected behavior when using TFRoberta model inside tf.keras model',2020-12-05T16:33:18Z,2020-12-08T15:02:09Z,,,
8935,b'phase level tokenizer',2020-12-05T16:14:18Z,2021-03-06T00:15:05Z,wontfix,,
8934,b'Updating outdated fairseq checkpoint to HF script',2020-12-05T12:41:49Z,2021-03-06T00:15:07Z,wontfix,,
8933,b'Relative Attention Bias not initialized for T5ForConditionalGeneration in version 4.0.0',2020-12-05T12:17:10Z,2020-12-05T20:01:09Z,,,
8932,b'Documentation License Query',2020-12-05T10:01:54Z,2021-03-06T00:15:08Z,wontfix,,
8931,b'Fix typo for `modeling_bert` import resulting in ImportError',2020-12-05T06:58:57Z,2020-12-05T14:57:38Z,,,
8930,b'[seq2seq] document the caveat of leaky native amp',2020-12-04T22:04:09Z,2020-12-04T23:43:36Z,,,
8929,"b""Don't pass in token_type_ids to BART for GLUE""",2020-12-04T21:05:21Z,2020-12-05T14:52:17Z,,,
8927,b'run_glue.py fails with RoBERTa but succeeds with other models',2020-12-04T20:25:24Z,2021-04-15T15:05:57Z,,RuntimeError,"RuntimeError: CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`"
8926,b'[ci] skip doc jobs - circleCI is not reliable - disable skip for now',2020-12-04T16:58:41Z,2020-12-04T18:13:43Z,,,
8925,b'Fix TF T5 only encoder model with booleans',2020-12-04T16:44:04Z,2020-12-04T17:28:48Z,,,
8924,b'Add new SQUAD example',2020-12-04T15:12:18Z,2020-12-08T19:10:19Z,,,
8923,b' ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds',2020-12-04T11:32:20Z,2021-01-26T23:02:54Z,,ValueError,"ValueError: You have to specify either decoder_inputs or decoder_inputs_embeds"
8922,b'Add comet',2020-12-04T08:27:41Z,2021-03-06T00:15:10Z,wontfix,,
8921,b'TransfoXL Slow Test Fails',2020-12-04T07:59:56Z,2021-04-24T15:03:07Z,,,
8920,b'Patch model parallel test',2020-12-03T21:09:41Z,2020-12-03T22:15:47Z,,,
8919,b'BertModel outputs string instead of tensor',2020-12-03T20:23:18Z,2020-12-04T14:00:42Z,,,
8918,b'Put Transformers on Conda',2020-12-03T19:01:38Z,2020-12-03T19:28:50Z,,,
8917,b'Fix move when the two cache folders exist',2020-12-03T15:43:54Z,2020-12-03T15:50:14Z,,,
8916,b'Impossible to use  sentencepiece',2020-12-03T15:27:09Z,2020-12-03T18:49:12Z,,,
8915,b'Avoid erasing the attention mask when double padding',2020-12-03T15:26:42Z,2020-12-03T15:45:08Z,,,
8914,b'Tweak wording + Add badge w/ number of models on the hub',2020-12-03T13:40:07Z,2020-12-03T15:56:56Z,,,
8913,b'Fine-tune with custom data',2020-12-03T10:55:09Z,2020-12-03T15:48:23Z,,,
8912,b'Create README.md',2020-12-03T07:32:11Z,2020-12-07T21:00:59Z,model card,,
8911,"b""Help to run an Example Code (it's a bug maybe ?)""",2020-12-03T00:36:27Z,2020-12-14T12:21:05Z,,RuntimeError,"RuntimeError: Expected tensor for argument #1 'indices' to have scalar type Long; but got torch.IntTensor instead (while checking arguments for embedding)"
8910,"b'""No log"" when training RobertaForSequenceClassification using Trainer'",2020-12-02T22:02:35Z,2021-04-24T15:03:07Z,,,
8909,b'FlaxBertModel examples (and fast attention)',2020-12-02T21:26:54Z,2021-05-19T15:09:38Z,,,
8908,"b""Question: What's the difference between tokenizer_utils, tokenizer_utils_base & tokenizer_utils_fast""",2020-12-02T19:53:56Z,2021-04-24T15:03:08Z,,,
8907,b'Unexpected situation when freezing BertForMaskedLM',2020-12-02T17:12:22Z,2021-02-24T08:38:06Z,,,
8906,b'Corrected a typo in the ReadMe',2020-12-02T17:04:07Z,2020-12-02T17:28:44Z,,,
8905,b'Fix typo in docstring in src/transformers/models/bert_japanese/tokenization_bert_japanese.py',2020-12-02T17:02:03Z,2020-12-02T17:08:32Z,,,
8904,b'Using doc chunks without answer token during training ( BertForQuestionAnswering )',2020-12-02T16:55:58Z,2021-04-24T15:03:09Z,,,
8903,b'[trainer] improve code readability',2020-12-02T16:30:43Z,2020-12-02T17:07:43Z,,,
8902,b'fix(pipeline): error when model not in AutoModel',2020-12-02T14:42:01Z,2021-04-23T09:58:18Z,,,
8901,b'Removing Head Layer/Model Conversion',2020-12-02T13:59:50Z,2020-12-02T16:42:05Z,,,
8900,"b'[Bart] Refactor - fix issues, consistency with the library, naming'",2020-12-02T13:17:47Z,2020-12-09T19:55:25Z,,,
8899,b'Wrong Length of Dataset in examples/seq2seq/finetune_trainer.py',2020-12-02T12:34:14Z,2021-04-24T15:03:10Z,,,
8898,"b'Command ""python setup.py egg_info"" failed with error code 1 in /tmp/pip-install-Q_fyRn/sacrebleu/'",2020-12-02T12:22:32Z,2020-12-02T12:28:35Z,,,
8897,b'finetune_trainer with python -m torch.distributed.launch',2020-12-02T11:55:25Z,2021-04-24T15:03:11Z,,AssertionError,"AssertionError: Default process group is not initialized"
8896,b'Create README.md',2020-12-02T11:48:17Z,2020-12-11T14:23:12Z,model card,,
8895,b'Create README.md',2020-12-02T11:45:30Z,2020-12-11T14:23:07Z,model card,,
8894,b'custom prepare_inputs_for_generation for generation ',2020-12-02T09:53:21Z,2020-12-02T10:19:55Z,,,
8893,"b'[\xf0\x9f\x9a\x80 Feature request]  Performer support, tensorflow code, not jax.'",2020-12-02T08:14:28Z,2020-12-02T08:44:06Z,,,
8892,b'TFRag draft #1 (page BROKEN) - Should close and use #9002 instead',2020-12-02T07:54:12Z,2021-04-24T15:03:11Z,,,
8891,b'providing an example with a dummy iterative dataloaders',2020-12-02T07:34:44Z,2021-04-24T15:03:12Z,,,
8890,b'Update generation_beam_search.py',2020-12-02T04:16:56Z,2020-12-09T10:03:26Z,,,
8889,b'trainer.py does not handle distributed training for iterative datasets and is very slow',2020-12-02T00:56:36Z,2021-04-24T15:03:13Z,,,
8888,b'clip_grad_norm on Multiple GPUs: (CUDA error: device-side assert triggered)',2020-12-01T23:57:43Z,2021-04-24T15:03:14Z,,RuntimeError,"RuntimeError: CUDA error: device-side assert triggered"
8887,"b""'Some weights of BertModel were not initialized from the model checkpoint at ./model and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']'""",2020-12-01T22:26:50Z,2021-04-24T15:03:14Z,,,
8886,b'UnicodeEncodeError: surrogates not allowed with GPT2Tokenizer',2020-12-01T21:53:38Z,2021-04-24T15:03:15Z,,UnicodeEncodeError,"UnicodeEncodeError: 'utf-8' codec can't encode characters in position 0-1: surrogates not allowed"
8885,b'[ci] skip doc jobs take #3',2020-12-01T20:25:20Z,2020-12-02T15:06:46Z,,,
8884,b'[s2s finetune_trainer] add instructions for distributed training',2020-12-01T19:53:37Z,2020-12-04T00:05:56Z,,,
8883,b'Extracting important information',2020-12-01T19:35:21Z,2020-12-02T14:30:02Z,,,
8882,b'[trainer] start using training_args.parallel_mode',2020-12-01T19:05:43Z,2020-12-01T19:40:37Z,,,
8881,b'Better warning when loading a tokenizer with AutoTokenizer w/o Sneten\xe2\x80\xa6',2020-12-01T17:54:56Z,2020-12-01T18:13:12Z,,AttributeError,"AttributeError: 'NoneType' object has no attribute 'from_pretrained'"
8880,b'[PyTorch] Refactor Resize Token Embeddings',2020-12-01T16:59:23Z,2020-12-02T18:19:51Z,,,
8879,"b""dropout(): argument 'input' (position 1) must be Tensor, not str With Bert""",2020-12-01T16:48:17Z,2021-04-24T15:03:16Z,,TypeError,"TypeError: dropout(): argument 'input' (position 1) must be Tensor, not str`"
8878,b'Better support for resuming training',2020-12-01T16:44:26Z,2020-12-01T18:45:22Z,,,
8877,b'Add a `parallel_mode` property to TrainingArguments',2020-12-01T16:20:15Z,2020-12-01T18:46:10Z,,,
8876,b'Resume training from checkpoint: not progressing',2020-12-01T16:06:17Z,2020-12-01T21:33:12Z,,,
8875,b'Fix mlflow parameter overflow',2020-12-01T15:42:45Z,2021-02-08T13:30:57Z,,,
8874,b'Results are different when fine-tuning continues after loading model from checkpoint ',2020-12-01T15:22:40Z,2020-12-01T18:45:22Z,,,
8873,b'How to pass the attention mask as a param to model forward when using torchscript?',2020-12-01T14:28:25Z,2020-12-02T06:26:33Z,,,
8872,b'Deberta Tokenizatiion',2020-12-01T13:33:03Z,2020-12-23T15:42:05Z,,,
8871,b'Decrease Longformer window size / computational cost',2020-12-01T12:43:49Z,2021-04-24T15:03:16Z,,,
8870,b'Token classification example only returns labels as -100 for longformer',2020-12-01T12:40:15Z,2021-04-24T15:03:17Z,,,
8869,b'Exporting ALBERT model to onnx increases model size by 7x',2020-12-01T11:40:04Z,2021-04-24T15:03:18Z,,,
8868,b'Transfoxl seq classification',2020-12-01T10:09:34Z,2020-12-02T15:08:33Z,,,
8867,"b'length_penalty not influencing results (Bart, Pegasus)'",2020-12-01T10:00:02Z,2020-12-01T16:54:31Z,,,
8866,b'different embedding weights for base-uncased with different transformers versions',2020-12-01T09:40:28Z,2021-04-24T15:03:19Z,,,
8865,b'can the BertModel convert to onnx? whether any one had done sucessfully ?',2020-12-01T09:18:07Z,2020-12-01T11:27:07Z,,,
8864,"b""AttributeError: 'NoneType' object has no attribute 'from_pretrained'""",2020-12-01T01:59:44Z,2020-12-01T18:13:12Z,,,
8863,b'Unwanted left shift of target tokens in `get_nll`',2020-11-30T23:04:13Z,2021-03-06T00:15:12Z,wontfix,,
8862,"b""TypeError: forward() got an unexpected keyword argument 'past'""",2020-11-30T22:36:55Z,2020-11-30T22:58:59Z,,TypeError,"TypeError: forward() got an unexpected keyword argument 'past'"
8861,b'Add warnings for incompatible generation parameters',2020-11-30T22:33:15Z,2021-03-06T00:15:13Z,wontfix,,
8860,b'Prevent BatchEncoding from blindly passing casts down to the tensors it contains',2020-11-30T22:05:28Z,2020-12-01T18:01:53Z,,,
8859,b'transformers/trainer.py stops after some iterations for iterative dataloaders.',2020-11-30T21:34:06Z,2020-12-01T12:53:27Z,,,
8858,b'[trainer] add distributed_env to TrainingArguments',2020-11-30T21:14:19Z,2020-12-01T18:46:10Z,,,
8857,b'keys_to_ignore_at_inference -> output_keys_to_ignore_at_inference',2020-11-30T20:54:08Z,2020-12-01T22:56:14Z,,,
8856,b'Make the big table creation/check platform independent',2020-11-30T20:48:52Z,2020-12-01T16:45:57Z,,,
8855,"b""KeyError: 'labels' in   training_step in transformers/trainer.py""",2020-11-30T20:19:57Z,2020-12-01T12:54:12Z,,KeyError,"KeyError: 'labels'"
8854,b'Fix interaction of return_token_type_ids and add_special_tokens',2020-11-30T18:57:03Z,2020-12-08T17:04:02Z,,,
8853,b'[CI] skip docs-only jobs take #2',2020-11-30T17:26:40Z,2020-12-01T18:15:26Z,,,
8852,b'Remove deprecated `evalutate_during_training`',2020-11-30T15:44:11Z,2020-11-30T16:12:16Z,,,
8851,b'Transfoxl sequence classification',2020-11-30T15:40:35Z,2020-12-01T09:25:34Z,,,
8850,b'Add a direct link to the big table',2020-11-30T15:25:29Z,2020-11-30T15:29:24Z,,,
8849,b'Some unintended things happen in Seq2SeqTrainer example',2020-11-30T10:03:55Z,2021-02-08T16:58:03Z,,mlflow.exceptions.MlflowException,"mlflow.exceptions.MlflowException: Param value '{'summarization': {'length_penalty': 1.0, 'max_length': 128, 'min_length': 12, 'num_beams': 4}, 'summarization_cnn': {'length_penalty': 2.0, 'max_length': 142, 'min_length': 56, 'num_beams': 4}, 'summarization_xsum': {'length_penalty': 1.0, 'max_leng' had length 293, which exceeded length limit of 250"
8848,b'Fix docstring for language code in mBart',2020-11-30T08:55:02Z,2020-12-01T09:44:37Z,,,
8847,"b""KeyError: 'mt5'""",2020-11-30T08:34:31Z,2020-12-02T23:31:16Z,,KeyError,KeyError: 'mt5'
8846,b'How to globally change the PYTORCH_PRETRAINED_BERT_CACHE path',2020-11-30T08:30:51Z,2021-03-06T00:15:15Z,wontfix,,
8845,b'Correct docstring.',2020-11-30T07:27:13Z,2020-11-30T14:33:31Z,,,
8844,b'mT5 fine-tuned model generate wrong answer',2020-11-30T05:05:24Z,2020-12-04T00:15:16Z,,,
8843,"b'""BertForMaskedLM - pretrained model"" cannot resize vocab output size'",2020-11-30T02:51:27Z,2020-11-30T03:52:36Z,,,
8842,b'T5 generations for pretraining objective degenerate',2020-11-30T02:21:38Z,2021-03-06T00:15:16Z,wontfix,,
8841,"b""Don't warn that models aren't available if Flax is available.""",2020-11-30T01:07:13Z,2020-12-03T15:33:13Z,,,
8840,b'Diverse number of return sequences for greedy search and sampling generation',2020-11-29T20:52:12Z,2021-03-06T00:15:18Z,wontfix,,
8839,b'[Needs Discussion] [WIP] [Docs] Clean tokenizer doc api and add fast tokenizers',2020-11-29T15:58:30Z,2020-12-14T23:23:38Z,,,
8838,b'RuntimeError: found torch.cuda.HalfTensor expected torch.cuda.FloatTensor while fine-tuning RAGSequence-base with custom data',2020-11-29T15:17:06Z,2021-03-06T00:15:19Z,wontfix,RuntimeError,"RuntimeError: Found param model.rag.question_encoder.question_encoder.bert_model.embeddings.word_embeddings.weight with type torch.cuda.HalfTensor, expected torch.cuda.FloatTensor."
8837,b'Inconsistent PreTrainedTokenizerBase.pad  argument default value & docstring',2020-11-29T14:50:45Z,2020-11-30T14:33:31Z,,,
8836,b'Add utility function for retrieving locally cached models ',2020-11-29T13:32:41Z,2021-01-04T14:53:56Z,,,
8835,"b'cannot run ""examples/language-modeling/run_mlm.py""'",2020-11-29T11:48:57Z,2020-11-29T22:45:43Z,,ModuleNotFoundError,"ModuleNotFoundError: No module named 'datasets'"
8834,b'Allow none-tensor fields in BatchEncoding',2020-11-29T02:53:12Z,2021-03-06T00:15:21Z,wontfix,,
8833,"b""AutoTokenizer can't find model/tokenizer config.json """,2020-11-29T00:39:51Z,2020-12-22T00:58:49Z,,OSError,"OSError: file xlm-roberta-large/config.json not found"
8832,b'[MT5] Add use_cache to config',2020-11-28T18:33:10Z,2020-11-28T18:50:49Z,,,
8831,b'logging.set_verbosity_error() displays dict instead of NotebookTrainingTracker',2020-11-28T15:25:58Z,2020-12-15T14:00:20Z,,,
8830,b'Longform QA demo breaks after clearing cache',2020-11-28T13:38:36Z,2021-03-06T00:15:23Z,wontfix,RuntimeError,"RuntimeError: Error in void faiss::gpu::allocMemorySpaceV(faiss::gpu::MemorySpace, void**, size_t) at gpu/utils/MemorySpace.cpp:26: Error: 'err == cudaSuccess' failed: failed to cudaMalloc 8987501056 bytes (error 2 out of memory)"
8829,b'Attempt to fix Flax CI error(s)',2020-11-28T11:36:14Z,2020-11-30T18:43:18Z,,,
8828,b'token-classification: use is_world_process_zero instead of  is_world_master()',2020-11-28T01:14:40Z,2020-11-30T14:21:56Z,,AttributeError,"AttributeError: 'Trainer' object has no attribute 'is_world_master'"
8827,"b""error: sentencepiece 0.1.94 is installed but sentencepiece==0.1.91 is required by {'transformers'}""",2020-11-27T21:50:33Z,2021-03-06T00:15:24Z,wontfix,,
8826,b'[CI] implement job skipping for doc-only PRs',2020-11-27T21:43:37Z,2020-11-29T16:31:30Z,,,
8825,"b'Model parallel tests should return, not pass in non model parallel se\xe2\x80\xa6'",2020-11-27T21:39:11Z,2020-11-27T21:41:30Z,,,
8824,b'suggest a numerical limit of 50MB for determining @slow',2020-11-27T20:48:51Z,2020-11-27T21:04:55Z,,,
8823,b'[s2s trainer] fix DP mode',2020-11-27T20:03:24Z,2020-11-30T20:55:56Z,,,
8822,b'[s2s finetune_trainer] a mess around distributed ',2020-11-27T20:02:21Z,2020-11-30T21:01:20Z,,RuntimeError,"RuntimeError: Default process group is not initialized"
8821,b'Shared vocabulary with EncoderDecoderModel',2020-11-27T19:46:17Z,2020-11-29T04:50:38Z,,,
8820,b'Update README.md',2020-11-27T19:33:22Z,2020-12-11T14:24:21Z,model card,,
8819,b'[Examples] fix few typos in help messages and arguments',2020-11-27T14:33:17Z,2020-11-30T16:01:18Z,,,
8818,b'Slower training time per batch for increasing dataset size',2020-11-27T14:20:12Z,2021-03-06T00:15:25Z,wontfix,,
8817,b'cache reuse',2020-11-27T14:09:01Z,2020-11-27T14:27:32Z,,,
8816,b'[Flax test] Add require pytorch to flix flax test',2020-11-27T13:23:52Z,2020-11-27T13:40:42Z,,,
8815,b'Fixed typo in README.md of bert-base-greek-uncased-v1',2020-11-27T12:51:11Z,2020-11-27T13:34:58Z,model card,,
8814,"b'I can not find a Linear layer in the end of Multi-Head Attention layer like Figure 2 right, could someone help me solve it'",2020-11-27T12:32:46Z,2020-11-30T02:30:11Z,,,
8813,b'Fix check copies',2020-11-27T10:15:23Z,2020-11-27T16:00:15Z,,,
8812,b'Ctrl for sequence classification',2020-11-27T08:58:02Z,2020-12-01T08:49:27Z,,,
8811,b'HuggingFace pipeline sentiment analysis giving wrong results.',2020-11-27T06:17:15Z,2020-11-27T09:50:11Z,,,
8810,b'typo',2020-11-26T22:38:15Z,2020-11-26T22:47:36Z,,,
8809,b'[model loading] remove pointless log entries',2020-11-26T22:34:22Z,2020-11-27T20:35:35Z,,,
8808,b'Fix dpr<>bart config for RAG',2020-11-26T21:37:34Z,2020-11-27T15:26:46Z,,,
8807,b'[s2s finetune trainer] potpurri of small fixes',2020-11-26T20:26:37Z,2020-11-26T22:06:27Z,,,
8806,b'Create README.md',2020-11-26T19:17:24Z,2020-11-26T19:19:53Z,,,
8805,"b'Revert ""[s2s] finetune.py: specifying generation min_length""'",2020-11-26T19:05:51Z,2020-11-26T19:12:02Z,,,
8804,b'MPNet: Masked and Permuted Pre-training for Language Understanding',2020-11-26T17:17:32Z,2020-12-07T18:35:40Z,,,
8803,b'Get locally cached models programatically ',2020-11-26T16:30:27Z,2021-03-06T00:15:27Z,wontfix,,
8802,b'Use GPT to assign sentence probability/perplexity given previous sentence? ',2020-11-26T15:29:50Z,2020-11-27T17:28:23Z,,,
8801,b'Multiprocessing behavior change 3.1.0 -> 3.2.0',2020-11-26T15:20:28Z,2021-03-10T09:11:38Z,,RuntimeError,"RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method"
8800,b'Problem with using custom tokenizers with run_mlm.py',2020-11-26T13:56:00Z,2021-04-24T15:03:19Z,,IndexError,"IndexError: index out of bounds"
8799,b'Warning about too long input for fast tokenizers too',2020-11-26T11:35:12Z,2020-12-02T15:18:28Z,,,
8798,b'Fix setup.py on Windows',2020-11-26T10:55:49Z,2020-11-27T17:25:21Z,,,
8797,b'Minor docs typo fixes',2020-11-26T10:28:00Z,2020-11-29T16:27:01Z,,,
8796,b'QARiB Arabic and dialects models',2020-11-26T08:49:20Z,2020-12-11T14:38:38Z,model card,,
8795,b'Use model.from_pretrained for DataParallel also',2020-11-26T08:36:39Z,2020-11-30T16:11:10Z,,,
8794,b'Can I get logits for each sequence I acqired from model.generate()?',2020-11-26T07:33:02Z,2021-03-06T00:15:29Z,wontfix,,
8793,b'Loss pooling layer parameters after Fine-tune.',2020-11-26T06:40:20Z,2021-03-06T00:15:30Z,wontfix,,
8792,b'[finetune_trainer] --evaluate_during_training is no more',2020-11-26T06:37:54Z,2020-11-30T16:12:16Z,,ValueError,"ValueError: Some specified arguments are not used by the HfArgumentParser: ['--evaluate_during_training']"
8791,b'[FlaxBert] Fix non-broadcastable attention mask for batched forward-passes',2020-11-26T06:10:31Z,2020-11-27T12:21:20Z,,,
8790,b'[FlaxBert] Non-broadcastable attention mask in batched forward-pass',2020-11-26T06:09:09Z,2020-11-27T12:21:20Z,,,
8789,"b""KeyError: 'eval_loss' when fine-tuning gpt-2 with run_clm.py""",2020-11-26T04:15:56Z,2021-03-06T00:15:31Z,wontfix,KeyError,"KeyError: 'eval_loss'"
8788,b'Add QCRI Arabic and Dialectal BERT (QARiB) models',2020-11-25T20:51:34Z,2020-12-11T14:27:04Z,model card,,
8787,b'QA pipeline fails during convert_squad_examples_to_features',2020-11-25T17:28:10Z,2020-11-25T18:30:28Z,,TypeError,"TypeError: TextInputSequence must be str"
8786,b'What would be the license of the model files available in Hugging face repository?',2020-11-25T16:52:40Z,2020-12-01T15:07:55Z,,,
8785,b'Update README.md',2020-11-25T16:51:51Z,2020-11-27T19:27:20Z,model card,,
8784,b'Different ouputs from code and Hosted Inference API',2020-11-25T16:29:44Z,2020-11-26T16:56:50Z,,,
8783,b'MPNet: Masked and Permuted Pre-training for Natural Language Understanding',2020-11-25T16:28:09Z,2020-11-25T16:28:21Z,,,
8782,b'Unexpected output from bart-large',2020-11-25T16:24:47Z,2020-12-07T20:03:16Z,,,
8781,b'NerPipeline (TokenClassification) now outputs offsets of words',2020-11-25T15:18:54Z,2020-11-30T19:05:08Z,,,
8780,"b""Can't load tokenizer for 'facebook/rag-token-base/question_encoder_tokenizer'. """,2020-11-25T14:05:35Z,2020-11-27T15:26:46Z,,,
8779,b'Fix PPLM',2020-11-25T13:35:56Z,2020-11-26T21:23:37Z,,,
8778,b'Using the XLNet or Tranformer-XL as an EncoderDecoder ',2020-11-25T13:17:06Z,2021-03-06T00:15:33Z,wontfix,,
8777,b'Better booleans handling in the TF models',2020-11-25T12:01:38Z,2020-12-04T14:08:29Z,,,
8776,b'Documentation and source for `RobertaClassificationHead`',2020-11-25T06:47:03Z,2020-11-25T12:48:48Z,,,
8775,b'Converting all model Config classes to dataclasses',2020-11-25T04:34:33Z,,Feature request,,
8774,b'Big model table',2020-11-25T01:23:24Z,2020-11-25T17:02:16Z,,,
8773,b'saving checkpoints on gs bucket ',2020-11-24T21:49:52Z,2021-03-06T00:15:36Z,wontfix,,
8772,b'Possible to add additional features as input to TFBertForSequenceClassification?',2020-11-24T21:06:45Z,2020-11-25T20:30:50Z,,,
8771,b'Model Parallelism and Big Models',2020-11-24T20:29:42Z,,"Model Parallel, WIP",,
8770,b'Extend typing to path-like objects in `PretrainedConfig` and `PreTrainedModel`',2020-11-24T18:46:11Z,2020-11-27T15:52:59Z,,,
8769,"b""LXMERT - Visual features don't match original implementation""",2020-11-24T17:31:02Z,2020-12-11T19:39:27Z,,,
8768,b'Attempt to get a better fix for QA',2020-11-24T17:09:31Z,2020-11-25T18:45:38Z,,,
8767,b'Allow to set truncation strategy for pipeline',2020-11-24T16:50:37Z,2021-03-06T00:15:37Z,wontfix,,
8766,b'[Error: PyTorch to tf]convert_pytorch_checkpoint_to_tf2: AttributeError: bert.pooler.dense.weight not found in PyTorch model',2020-11-24T16:34:18Z,2021-03-06T00:15:38Z,wontfix,AttributeError,"AttributeError: bert.pooler.dense.weight not found in PyTorch model"
8765,b'Fix QA argument handler',2020-11-24T15:46:26Z,2020-11-25T19:02:16Z,,,
8764,b'Tokenizers - move from hardcoded configs and models to fully hosted',2020-11-24T15:12:23Z,2021-03-06T00:15:40Z,wontfix,,
8763,b'Migration guide from v3.x to v4.x',2020-11-24T15:04:32Z,2020-11-30T01:13:08Z,,,
8762,"b""AttributeError: type object 'T5ForConditionalGeneration' has no attribute 'from_config'""",2020-11-24T14:19:18Z,2020-11-24T15:40:53Z,,AttributeError,"AttributeError: type object 'T5ForConditionalGeneration' has no attribute 'from_config'"
8761,b'Create README.md',2020-11-24T13:24:52Z,2020-11-24T22:51:25Z,model card,,
8760,b'Create README.md',2020-11-24T13:20:35Z,2020-11-26T17:43:43Z,model card,,
8759,b'Version 3.5 broke the multi context/questions feature for the QuestionAnsweringPipeline',2020-11-24T12:59:28Z,2020-11-25T19:02:16Z,,AttributeError,"AttributeError: 'list' object has no attribute 'doc_tokens'"
8758,b'[Help] GPU with query answering',2020-11-24T12:16:30Z,2020-11-24T15:22:22Z,,,
8757,"b""UnicodeDecodeError: 'utf-8' codec can't decode byte 0x80 in position 49: invalid start byte""",2020-11-24T11:59:34Z,2021-03-06T00:15:42Z,wontfix,UnicodeDecodeError,"UnicodeDecodeError: 'utf-8' codec can't decode byte 0x80 in position 49: invalid start byte"
8756,b'Continued training of the original BERT models (not to PyTorch)',2020-11-24T11:00:23Z,2021-03-06T00:15:43Z,wontfix,,
8755,"b""Why there are no such 'cls/' layers in roberta pytorch checkpoints""",2020-11-24T10:11:40Z,2020-11-25T02:46:53Z,,tensorflow.python.framework.errors_impl.NotFoundError,"tensorflow.python.framework.errors_impl.NotFoundError: Key cls/predictions/transform/dense/kernel not found in checkpoint"
8754,b'Allow to provide specific params in WandbCallback',2020-11-24T10:08:12Z,2021-04-25T15:04:21Z,,,
8753,b'update README.txt',2020-11-24T10:06:37Z,2020-12-11T14:46:33Z,model card,,
8752,b'Create README.md',2020-11-24T05:46:47Z,2020-11-25T22:38:22Z,model card,,
8751,b'Create README.md',2020-11-24T05:31:48Z,2020-12-11T14:40:15Z,model card,,
8750,b'Fix minor bug to handle dynamic sequence length ',2020-11-24T03:10:14Z,,WIP,,
8749,b'[core] transformers version number normalization',2020-11-24T03:05:23Z,2020-12-18T13:55:56Z,,,
8748,"b'""AutoTokenizer.from_pretrained"" does not work when loading a pretrained Albert model'",2020-11-24T01:27:57Z,2021-03-06T00:15:46Z,wontfix,TypeError,"TypeError: not a string"
8747,b'Return correct Bart hidden state tensors',2020-11-24T00:01:55Z,2020-11-25T21:06:06Z,,,
8746,b'Fix slow tests v2',2020-11-23T23:32:18Z,2020-11-24T14:35:12Z,,,
8745,b'added instructions for syncing upstream master with forked master via PR',2020-11-23T23:25:45Z,2020-11-24T15:11:47Z,,,
8744,b'Added instructions for syncing forked masters to avoid references',2020-11-23T23:15:58Z,2020-11-23T23:20:50Z,,,
8743,b'MT5 should have an autotokenizer',2020-11-23T23:11:50Z,2020-11-24T14:50:26Z,,,
8742,b'Add instructions for syncing forked masters to avoid references',2020-11-23T22:59:16Z,2020-11-24T15:11:47Z,,,
8741,b'Model parallel documentation',2020-11-23T22:36:47Z,2020-11-24T01:14:49Z,,,
8740,b' Blank line indicates the end of a document for NER training ?',2020-11-23T20:42:27Z,2021-03-06T00:15:48Z,wontfix,,
8739,"b""AttributeError: 'BertTokenizerFast' object has no attribute 'max_len'""",2020-11-23T20:09:00Z,2020-11-24T23:47:20Z,,,
8738,b'Fix max length in run_plm script',2020-11-23T19:44:47Z,2020-11-23T21:02:32Z,,,
8737,b'consistent ignore keys + make private',2020-11-23T19:35:42Z,2020-11-23T20:33:14Z,,,
8736,b'[trainer] `model` argument is not the same depending on n_gpus',2020-11-23T19:09:13Z,2021-01-23T06:17:49Z,wontfix,,
8735,"b""Model can't be downloaded""",2020-11-23T18:46:26Z,2020-11-24T22:14:50Z,,OSError,"OSError: Can't load config for 'monilouise/ner_pt_br'. Make sure that:"
8734,b'Change default cache path',2020-11-23T18:36:57Z,2020-11-23T18:56:45Z,,,
8733,b'[proposal] do not load all 3rd party packaged unless needed',2020-11-23T18:18:32Z,2021-01-08T12:41:00Z,,,
8732,"b'[Benchmark] V100/A100 benchmarks, dashboard concept'",2020-11-23T17:41:59Z,2021-03-06T00:15:50Z,wontfix,,
8731,b'[Pegasus] Refactor Tokenizer',2020-11-23T16:58:16Z,2020-11-29T15:57:44Z,,,
8730,b'fix rag index names in eval_rag.py example',2020-11-23T16:32:47Z,2020-11-24T16:04:47Z,,,
8729,b'Create README.md',2020-11-23T16:05:54Z,2020-11-27T17:19:16Z,model card,,
8728,b'Flax Masked Language Modeling training example',2020-11-23T14:00:17Z,2020-12-09T16:13:57Z,,,
8727,b'[model_cards]: control input examples of Geotrend models',2020-11-23T11:29:48Z,2020-11-23T16:09:51Z,model card,,
8726,b'It seem do not support convert multilabel classification model  to onnx  ?',2020-11-23T10:37:34Z,2021-04-25T15:04:23Z,,,
8725,b'Longformer inference speed is slower than bert of the same length',2020-11-23T10:02:06Z,2021-04-25T15:04:24Z,Migration,,
8724,"b'@ehsan-soe I fixed the problem by truncating incomplete batches. So if there are 2001 examples and my batch size = 2, then I truncate the last example and train on the first 2000. This has fixed it for me both with and without distributed. My load_and_cache function now looks like this'",2020-11-23T09:18:04Z,2021-04-25T15:04:25Z,wontfix,,
8723,"b""Model conversion from PyTorch to TF2 doesn't work properly for XLM-Roberta""",2020-11-23T07:38:24Z,2020-11-25T01:41:20Z,,AssertionError,"AssertionError: Error, model absolute difference is >2e-2: 1.0000114440917969"
8722,b'a bug in generation_beam_search.py',2020-11-23T06:37:45Z,2021-04-25T15:04:26Z,,,
8721,"b'run_clm.py training script failing with CUDA out of memory error, using gpt2 and arguments from docs.'",2020-11-22T22:44:08Z,2020-11-23T23:03:14Z,,"RuntimeError, ```AttributeError","RuntimeError: CUDA out of memory.```AttributeError: 'GPT2TokenizerFast' object has no attribute 'max_len'```"
8720,b'Broken links in example for torch.load() after. converting tensorflow checkpoint to pytorch save model',2020-11-22T22:41:01Z,2021-01-26T08:37:58Z,"wontfix, Migration",,
8719,b'Unable to Tie Encoder Decoder Parameters When Using EncoderDecoderModel Constructor',2020-11-22T22:39:07Z,2020-11-22T23:59:16Z,,,
8718,b'Issues with finetune_trainer.py on multiple gpus',2020-11-22T21:53:16Z,2021-04-25T15:04:28Z,wontfix,,
8717,b'Add T5 Encoder for Feature Extraction',2020-11-22T21:52:53Z,2020-11-30T07:34:40Z,,,
8716,b'[trainer] make generate work with multigpu',2020-11-22T20:41:24Z,2020-11-23T18:57:28Z,,,
8715,b'placing the run dir only in the output_dir',2020-11-22T19:16:22Z,2021-04-25T15:04:29Z,wontfix,,
8714,b'Add TFGPT2ForSequenceClassification based on DialogRPT',2020-11-22T17:22:09Z,2020-12-07T15:58:38Z,,,
8713,b'eval of seq2seq/finetune_trainer does not work on multiple gpus ',2020-11-22T17:16:38Z,2020-11-23T18:57:28Z,,torch.nn.modules.module.ModuleAttributeError,"torch.nn.modules.module.ModuleAttributeError: 'DataParallel' object has no attribute 'generate'"
8712,b'distributed_eval does not run',2020-11-22T16:53:07Z,2021-04-25T15:04:30Z,wontfix,"requests.exceptions.HTTPError, OSError, subprocess.CalledProcessError","requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/sshleifer/distilbart-large-xsum-12-3/resolve/main/config.jsonOSError: Can't load config for 'sshleifer/distilbart-large-xsum-12-3'. Make sure that:subprocess.CalledProcessError: Command '['/opt/conda/envs/internship/bin/python', '-u', 'run_distributed_eval.py', '--local_rank=7', '--model_name', 'sshleifer/distilbart-large-xsum-12-3', '--save_dir', 'xsum_generations', '--data_dir', 'xsum', '--fp16']' returned non-zero exit status 1."
8711,b'Model predictions wrong',2020-11-22T15:41:18Z,2021-04-25T15:04:31Z,,,
8710,b'[BUG] Wrong Scores for many SQUAD models',2020-11-22T10:14:16Z,2021-04-25T15:04:32Z,wontfix,,
8709,"b""Can't load weights for""",2020-11-22T06:23:10Z,2020-11-22T08:36:02Z,New model,"RuntimeError, OSError, HTTPError","RuntimeError: [enforce fail at inline_container.cc:145] . PytorchStreamReader failed reading zip archive: failed finding central directoryOSError: Unable to load weights from pytorch checkpoint file for 'saburbutt/albert_xxlarge_tweetqa_v2' at '/root/.cache/torch/transformers/280e3f03092e3b52d227bc27519ff98aff017abcc160fc5138df7ce1bddcff1e.b5346cd8c01b1d2591b342ede0146ce26b68ad0a84ff87e5dc8f9d5a03a79910'If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True. HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/saburbutt/albert_xxlarge_tweetqa_v2/resolve/main/tf_model.h5"
8708,b'Fix many typos',2020-11-22T00:49:48Z,2020-11-22T03:58:11Z,model card,,
8707,b'Accuracy changes dramatically',2020-11-21T22:48:08Z,2020-11-23T17:23:29Z,,,
8706,b'T5v1.1 Addition of special tokens',2020-11-21T21:59:37Z,2020-12-01T21:35:18Z,,,
8705,"b'DPRReaderTokenizers returns, for multiple passages given, only the tokens & masks of one passage '",2020-11-21T20:52:11Z,2020-11-22T09:53:51Z,,,
8704,b'Generating from mT5',2020-11-21T17:43:51Z,2021-05-26T15:09:31Z,,,
8703,b'providing the user with possibility to set the cache path ',2020-11-21T14:06:07Z,2021-04-25T15:04:34Z,wontfix,,
8702,b'Question about beam_sample: using two softmax?',2020-11-21T12:36:00Z,2020-11-21T12:48:30Z,,,
8701,b'TypeError: an integer is required (got type NoneType)',2020-11-21T06:21:52Z,2020-12-03T03:31:30Z,,TypeError,"TypeError: an integer is required (got type NoneType)"
8700,b'training text_classification with tpu using xla_spawn gives wrong result',2020-11-21T04:47:25Z,2021-04-25T15:04:35Z,wontfix,,
8699,b'Cannot load tokenizer in community T5 pretrained model',2020-11-21T03:34:20Z,2021-04-25T15:04:36Z,wontfix,,
8698,b'CSV/JSON file format for examples/token-classification/run_ner.py',2020-11-21T01:56:50Z,2020-11-27T06:53:21Z,,,
8697,b'test',2020-11-20T22:28:10Z,2020-11-20T22:37:13Z,,,
8696,b'gpt2 and t5 parallel modeling',2020-11-20T21:41:51Z,2020-11-23T19:41:24Z,Model Parallel,,
8695,b'Update README.md to fix typo',2020-11-20T21:27:43Z,2021-03-31T10:04:57Z,,,
8694,b'[Generate Test] fix flaky ci',2020-11-20T20:40:03Z,2020-11-20T21:07:22Z,,,
8693,b'update tensorflow to functional version',2020-11-20T20:22:45Z,2020-11-23T17:14:23Z,,,
8692,b'issues with seq length with inference code for classification',2020-11-20T19:33:32Z,2020-11-23T16:56:46Z,,RuntimeError,"RuntimeError: The size of tensor a (1313) must match the size of tensor b (512) at non-singleton dimension 1```"
8691,b'Pegasus example not working',2020-11-20T18:45:32Z,2020-11-20T22:20:29Z,,AttributeError,"AttributeError: 'list' object has no attribute 'to' """
8690,b'connection issue',2020-11-20T17:40:29Z,2021-04-25T15:04:38Z,,ValueError,"ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on."
8689,b'[Question] Pegasus tokenizer',2020-11-20T15:03:31Z,2020-11-29T15:57:44Z,,,
8688,b'Document adam betas TrainingArguments',2020-11-20T14:17:08Z,2020-11-20T14:27:26Z,,,
8687,b'added bangla-bert-sentiment model card',2020-11-20T13:09:45Z,2020-11-23T10:51:16Z,model card,,
8686,b'moved temperature warper before topP/topK warpers',2020-11-20T12:53:34Z,2020-11-20T18:33:55Z,,,
8685,b'Pegasus Xsum Returning Tokens Not In Source Text',2020-11-20T12:39:01Z,2021-04-25T15:04:39Z,wontfix,,
8684,b'Bert variants pretrained on Wikipedia are easily downloaded. Are the optimizers from the pretraining also available?',2020-11-20T11:19:42Z,2021-04-25T15:04:40Z,wontfix,,
8683,b'use the torchscript in a gpt model is slower  than origin one.',2020-11-20T09:24:40Z,2021-04-25T15:04:42Z,wontfix,,
8682,b'create README.md',2020-11-20T08:44:38Z,2020-11-23T10:51:54Z,model card,,
8681,b'Create README.txt',2020-11-20T08:43:27Z,2020-12-11T14:46:38Z,model card,,
8680,"b""Result changes if we don't pass attension mask in TFDistilbert model on SQUADv1 dataset""",2020-11-20T07:53:11Z,2021-04-25T15:04:43Z,wontfix,,
8679,b'gpt2 and t5 model parallelism with tests',2020-11-20T06:09:01Z,2020-11-20T08:45:08Z,Model Parallel,,
8678,b'Update the bibtex with EMNLP demo',2020-11-20T05:22:08Z,2020-11-20T05:26:33Z,,,
8677,b'Model parallel v4',2020-11-20T05:19:26Z,2020-11-20T05:21:53Z,,,
8676,b'2 typos in modeling_rag.py',2020-11-20T03:04:30Z,2020-12-01T15:16:49Z,,,
8675,b'[WIP] Rewrite ProphetNet to adapt converting ONNX friendly',2020-11-20T02:26:51Z,2021-04-25T15:04:44Z,,,
8674,b'Issues Fine-tuning XLNET ',2020-11-19T23:39:20Z,2020-11-23T21:02:32Z,,IndexError,"IndexError: index out of bounds"
8673,b'[model_cards] Add card for gpt2-rnm',2020-11-19T22:21:18Z,2020-11-23T10:52:30Z,model card,,
8672,b'Add sentencepiece to the CI and fix tests',2020-11-19T21:07:32Z,2020-11-19T21:44:21Z,,,
8671,b'Running Roberta on Race Multi choice dataset giving error ',2020-11-19T20:26:44Z,2021-04-24T15:03:20Z,wontfix,FileNotFoundError,"FileNotFoundError: [Errno 2] No such file or directory: '/RACE/cached_train_RobertaTokenizer_80_race.lock'"
8670,b'Is Reformer supported under Encoder-Decoder framework?',2020-11-19T20:24:51Z,2021-04-24T15:03:21Z,wontfix,,
8669,b'Make signature of `compute_metrics` parameter in Trainer class more flexible',2020-11-19T19:56:16Z,2021-04-24T15:03:22Z,wontfix,,
8668,b'Update bert-base-multilingual-cased-README.md',2020-11-19T19:43:18Z,2020-11-19T20:45:07Z,model card,,
8667,b'Alternative to globals()',2020-11-19T19:21:10Z,2020-11-20T02:26:02Z,,,
8666,b'Fix a few last paths for the new repo org',2020-11-19T16:55:47Z,2020-11-19T16:56:43Z,,,
8665,b'Use return_dict in RagModel forward pass',2020-11-19T16:42:12Z,2020-11-20T18:05:30Z,,,
8664,b'Fix run_ner script',2020-11-19T16:29:13Z,2020-11-19T18:59:31Z,,,
8663,b'transformers-cli: LFS multipart uploads (> 5GB)',2020-11-19T16:14:30Z,2020-12-07T21:38:40Z,,,
8662,"b""Can't upload the larger model file(9GB)""",2020-11-19T15:42:01Z,2020-12-07T21:38:40Z,,,
8661,b'cannot load t5-base config ',2020-11-19T15:40:45Z,2020-11-20T14:29:10Z,,OSError,"OSError: file t5-base/config.json not found"
8660,b'Fix bug in x-attentions output for roberta and harden test to catch it',2020-11-19T14:26:32Z,2020-11-23T12:28:30Z,,,
8659,b'Improve bert-japanese tokenizer handling',2020-11-19T13:24:38Z,2020-11-23T16:15:02Z,,,
8658,b'ConvBERT',2020-11-19T13:10:01Z,2021-02-21T11:23:28Z,,,
8657,b'Fix embeddings resizing in TF models',2020-11-19T12:44:34Z,2020-12-14T04:05:25Z,,,
8656,b'Return output probabilities with Generate function ',2020-11-19T12:44:19Z,2021-05-16T15:02:36Z,,,
8655,b'[model card] : fix Geotrend/bert-base-15lang-cased',2020-11-19T10:06:17Z,2020-11-19T10:41:02Z,model card,,
8654,"b'Error in NER examples, run.sh'",2020-11-19T10:02:13Z,2020-11-19T12:00:42Z,,_pickle.PicklingError,"_pickle.PicklingError: Can't pickle typing.Union[str, NoneType]: it's not the same object as typing.Union"
8653,b'Fix missing return_dict in RAG example to use a custom knowledge source',2020-11-19T09:40:38Z,2020-11-19T14:17:19Z,,,
8652,b'WNLI benchmark results clarification',2020-11-19T09:24:59Z,2021-04-24T15:03:22Z,wontfix,,
8651,"b""RAG: OSError: Can't load tokenizer for 'facebook/rag-sequence-nq/question_encoder_tokenizer'""",2020-11-19T09:19:56Z,2020-11-19T09:38:10Z,,OSError,"OSError: Can't load tokenizer for 'facebook/rag-sequence-nq/question_encoder_tokenizer'. Make sure that:"
8650,"b""Why use 'BertLayerNorm'  instead of torch.nn.LayerNorm ?""",2020-11-19T07:50:31Z,2020-11-20T05:39:20Z,New model,,
8649,"b""from_pretrained()'s load() blocks forever in subprocess""",2020-11-19T06:50:39Z,2020-11-23T16:45:06Z,,,
8648,b'Create README.md',2020-11-19T06:41:55Z,2020-12-11T14:46:42Z,model card,,
8647,b'How can get the input embeddings_output for BERT?',2020-11-19T03:34:14Z,2020-11-19T16:21:23Z,,,
8646,b'CPM LM',2020-11-19T01:46:55Z,2021-02-25T00:44:54Z,New model,,
8645,b'[core] implement support for run-time dependency version checking',2020-11-19T01:32:23Z,2020-11-24T18:22:26Z,,,
8644,b'Fix small typo',2020-11-19T00:56:25Z,2020-11-19T16:24:12Z,,,
8643,b'Model embedding size and tokenizer size mismatch; resizing embedding will cause CUDA assert error',2020-11-19T00:32:18Z,2020-12-22T11:37:35Z,,RuntimeError,RuntimeError: CUDA error: device-side assert triggered
8642,b'Setting Evaluation Strategy in the TrainingArgs does not print validation metrics',2020-11-19T00:06:08Z,2020-11-19T01:07:17Z,,,
8641,b'Bi-Directional Reformer text multi class classification',2020-11-18T23:10:15Z,2021-04-24T15:03:23Z,wontfix,,
8640,b'Bump notebook from 6.1.4 to 6.1.5 in /examples/lxmert',2020-11-18T23:02:01Z,2021-03-06T00:15:52Z,wontfix,,
8639,b'grammar',2020-11-18T22:49:00Z,2020-11-18T23:04:25Z,,,
8638,"b""AttributeError: module 'typing' has no attribute '_ClassVar'""",2020-11-18T22:48:22Z,2020-12-05T19:43:24Z,,AttributeError,"AttributeError: module 'typing' has no attribute '_ClassVar'"
8637,b'Add FastFormers to the example directory',2020-11-18T22:46:19Z,2021-09-22T21:35:07Z,WIP,,
8636,b'Updated the Extractive Question Answering code snippets',2020-11-18T22:39:15Z,2020-11-18T23:56:48Z,,,
8635,b'Small formatting fix',2020-11-18T22:35:56Z,2020-11-18T23:35:23Z,,,
8634,b'Fix a bunch of slow tests',2020-11-18T22:09:54Z,2020-11-19T15:41:42Z,,,
8633,b'Better filtering of the model outputs in Trainer',2020-11-18T20:43:42Z,2020-11-19T15:43:16Z,,,
8632,b'[s2s] distillation.py fails with apex',2020-11-18T20:33:45Z,2021-03-06T00:15:57Z,wontfix,RuntimeError,"RuntimeError: expected scalar type Float but found Half"
8631,b'[s2s] distillation apex breaks return_dict obj',2020-11-18T20:31:14Z,2020-11-18T20:51:29Z,,,
8630,b'Create README.md',2020-11-18T20:29:14Z,2020-11-23T09:48:11Z,model card,,
8629,b'Fix mark-up (missing opening code-tag)',2020-11-18T19:21:26Z,2020-11-19T15:05:51Z,,,
8628,b'CUDA error when training roBERTa from scratch with data parallel.',2020-11-18T18:26:05Z,2020-11-25T10:23:41Z,,RuntimeError,"RuntimeError: Caught RuntimeError in replica 0 on device 0."
8627,b'Diverse beam search',2020-11-18T17:42:59Z,2020-12-09T13:50:00Z,,,
8626,b'run_pl_glue.py (almost equivalent performance with non-english bert models)',2020-11-18T17:27:48Z,2021-03-06T00:15:58Z,wontfix,,
8625,b'Model Card for abhilash1910/financial_roberta',2020-11-18T17:22:37Z,2020-11-18T18:22:28Z,model card,,
8624,b'Fixes the training resuming with gradient accumulation',2020-11-18T16:42:19Z,2020-11-18T17:00:12Z,,,
8623,b'Fix training from scratch in new scripts',2020-11-18T16:18:34Z,2020-11-18T17:15:26Z,,,
8622,b'[Tokenizer Doc] Improve tokenizer summary',2020-11-18T15:14:15Z,2020-11-18T16:14:15Z,,Terminology,"Terminology: Tried to make the difference between ""symbol"", ""character"", ""word"", and ""subword"""
8621,b'Fix DataCollatorForLanguageModeling',2020-11-18T14:48:32Z,2020-11-18T15:02:51Z,,,
8620,b'Create model_cards for Chinese Couplet and Poem GPT2 models  ',2020-11-18T14:44:55Z,2020-11-18T18:06:30Z,model card,,
8619,b'`DataCollatorForLanguageModeling` modifies `input_ids` via `labels` variable',2020-11-18T14:21:12Z,2020-11-18T15:02:51Z,,IndexError,"IndexError: index out of range in self "
8618,b'seq2seq_trainer optimization issue on TPU ',2020-11-18T14:08:54Z,2021-03-06T00:16:00Z,wontfix,RuntimeError,"RuntimeError: tensors must be 2-D"
8617,b'Add cards for all Geotrend models',2020-11-18T11:20:10Z,2020-11-19T09:47:25Z,model card,,
8616,b'Add pip install update to resolve import error in transformers notebook',2020-11-18T10:39:05Z,2020-11-23T14:58:52Z,,AttributeError,"AttributeError: module 'tensorflow_core.python.keras.api._v2.keras.activations' has no attribute 'swish'"
8615,b'Batch Size error',2020-11-18T09:15:44Z,2020-11-20T13:13:20Z,,,
8614,b'ValueError while running run_glue.py with xlnet model.',2020-11-18T07:35:39Z,2021-03-06T00:16:01Z,wontfix,ValueError,"ValueError: could not broadcast input array from shape (512,8,768) into shape (416,8,768)"
8613,b'[s2s] multigpu skip',2020-11-18T07:06:26Z,2020-11-18T15:15:54Z,,,
8612,b'[s2s] fix finetune.py to adjust for #8530 changes',2020-11-18T06:38:44Z,2020-11-18T15:25:01Z,,,
8611,b'tf_bart typo - self.self.activation_dropout',2020-11-18T04:01:47Z,2020-11-18T15:30:29Z,,,
8610,b'How to train EncoderDecoderModel using bert for seq-to-seq model',2020-11-18T03:39:40Z,2020-11-18T08:16:04Z,,,
8609,b'Missing `tokenizers` file?',2020-11-18T03:38:02Z,2020-11-18T08:59:08Z,,,
8608,"b'Extracting word representations from BPE-tokenization-based models (GPT-2, RoBERTa, etc.)'",2020-11-18T00:24:16Z,2020-11-26T23:43:57Z,,,
8607,b'Fixed link to the wrong paper.',2020-11-17T23:24:59Z,2020-11-18T00:00:44Z,,,
8606,b'converting REALM tensorflow checkpoints to pytorch',2020-11-17T23:11:41Z,2021-03-06T00:16:03Z,wontfix,"tensorflow.python.framework.errors_impl.NotFoundError, RuntimeError","tensorflow.python.framework.errors_impl.NotFoundError: /cns/li-d/home/lumiere/public/models/gatoatigrado/ner-with-dates/10923195/1-active_losses=mlm_loss/export/temp/1580364602/retriever/encoded; No such file or directoryRuntimeError: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for ./cc_news_pretrained/embedder/encoded/encode.ckpt.data-00000-of-00001"
8605,b'Add Harry Potter Model Card',2020-11-17T20:08:46Z,2020-11-17T21:50:59Z,model card,,
8604,b'Remove deprecated',2020-11-17T20:07:53Z,2020-11-17T20:11:30Z,,,
8603,b'TFTrainer & Eager mode',2020-11-17T19:50:41Z,2021-03-06T00:16:05Z,wontfix,,
8602,b'New TF model inputs',2020-11-17T19:33:38Z,2020-11-24T18:55:01Z,,,
8601,b'Accessing gradients of Bart hidden states',2020-11-17T17:32:11Z,2020-11-25T21:06:05Z,,,
8600,b'Fix check repo utils',2020-11-17T17:20:43Z,2020-11-17T19:01:47Z,,,
8599,b'Tokenizers should be framework agnostic',2020-11-17T16:54:45Z,2020-11-17T19:03:04Z,model card,,
8598,b'Vectorize RepetitionPenaltyLogitsProcessor to improve performance',2020-11-17T16:39:28Z,2020-11-20T18:59:07Z,,,
8597,b'BART & FSMT: fix decoder not returning hidden states from the last layer',2020-11-17T16:33:04Z,2020-11-27T17:35:35Z,,,
8596,b'Speed up repetition penalty logits processor',2020-11-17T15:58:53Z,2020-12-08T23:06:21Z,,,
8595,b'Fix model templates',2020-11-17T15:31:38Z,2020-11-17T15:35:38Z,,,
8594,b'PEGASUS do not have mask token',2020-11-17T14:28:24Z,2020-11-29T15:57:44Z,,,
8593,b'Fix missing space in unavailable PyTorch/TensorFlow warning',2020-11-17T14:27:48Z,2020-11-18T15:09:26Z,,,
8592,b'Improving performance results for BERT ',2020-11-17T13:35:43Z,2020-11-18T15:07:40Z,,,
8591,b'Fix init for MT5',2020-11-17T13:02:10Z,2020-11-17T13:52:14Z,,,
8590,b'Cannot train model from scratch using `run_mlm.py`.',2020-11-17T12:52:48Z,2020-11-18T17:15:26Z,,,
8589,b'[MT5] More docs',2020-11-17T11:39:40Z,2020-11-17T11:47:57Z,,,
8588,b'Hosting and online deployment of a transformer chatbot (built with huggingface library)',2020-11-17T11:32:14Z,2020-11-18T15:05:52Z,,,
8587,"b'The Albert tokenizer file cannot download automatically and the official Albert tokenizer file is wrong, I cannot use it.'",2020-11-17T11:24:47Z,2021-03-06T00:16:06Z,wontfix,,
8586,b'Tokenizers: ability to load from model subfolder',2020-11-17T10:59:36Z,2020-11-17T13:58:46Z,,,
8585,b'Fix rag finetuning + add finetuning test',2020-11-17T10:42:22Z,2020-11-20T18:05:04Z,,,
8584,b'Add output control for TFGPT2LMHeadModel',2020-11-17T10:35:05Z,2020-11-17T17:09:01Z,,,
8583,b'[RAG] Add Ray implementation for distributed retrieval',2020-11-17T08:01:51Z,2020-12-18T19:35:01Z,,,
8582,b'[examples tests] tests that are fine on multi-gpu',2020-11-17T04:47:09Z,2020-11-17T19:00:42Z,,,
8581,b'Add early stopping callback to pytorch trainer',2020-11-17T04:43:53Z,2020-11-23T22:25:36Z,,,
8580,b'Reorganize repo',2020-11-17T02:33:56Z,2020-11-17T02:43:43Z,,,
8579,b'Create README.md',2020-11-17T00:43:22Z,2020-11-17T08:36:50Z,model card,,
8578,b'Error: Asking to return token_type_ids while setting add_special_tokens to False',2020-11-17T00:29:14Z,2020-12-08T17:04:02Z,,ValueError,"ValueError: Asking to return token_type_ids while setting add_special_tokens to False results in an undefined behavior. Please set add_special_tokens to True or set return_token_type_ids to None."
8577,b'[examples/seq2seq] fix PL deprecation warning',2020-11-16T20:23:03Z,2020-11-19T20:46:05Z,,,
8576,b'run_pl_glue.py token_type_id error on fresh install',2020-11-16T19:07:59Z,2021-03-06T00:16:08Z,wontfix,TypeError,"TypeError: an integer is required (got type NoneType)"
8575,b'REALM checkpoints to pytorch checkpoints',2020-11-16T18:58:02Z,2020-11-20T00:46:27Z,New model,"tensorflow.python.framework.errors_impl.NotFoundError, RuntimeError","tensorflow.python.framework.errors_impl.NotFoundError: /cns/li-d/home/lumiere/public/models/gatoatigrado/ner-with-dates/10923195/1-active_losses=mlm_loss/export/temp/1580364602/retriever/encoded; No such file or directoryRuntimeError: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for ./cc_news_pretrained/embedder/encoded/encode.ckpt.data-00000-of-00001"
8574,b'[Improvements] Enable `git push` without requiring login when uploading model',2020-11-16T18:57:54Z,2020-11-19T21:48:39Z,,,
8573,b'Bert that receives text triplet as an input',2020-11-16T18:27:43Z,2021-03-06T00:16:10Z,wontfix,,
8572,b'Fix mixed precision issue for GPT2',2020-11-16T18:17:21Z,2020-11-16T19:44:20Z,,,
8571,b'[WIP] Move BERT and ALBERT',2020-11-16T16:58:57Z,2020-11-17T02:26:03Z,,,
8570,b'[T5] Add open / closed book answering models',2020-11-16T16:35:31Z,2020-12-07T19:52:57Z,New model,,
8569,b'After 2nd iteration: always same result when training in a loop',2020-11-16T15:51:11Z,2021-01-24T02:33:52Z,wontfix,,
8568,b'Update version to v4.0.0-dev',2020-11-16T15:15:16Z,2020-11-16T15:21:21Z,,,
8567,b'[XLNet] Fix mems behavior',2020-11-16T15:11:10Z,2020-11-25T21:55:00Z,,,
8566,"b'""setup.py"" does not seem to have been updated for v3.5.1'",2020-11-16T14:57:52Z,2020-11-16T15:21:20Z,,,
8565,b'replace performance table with markdown',2020-11-16T14:46:13Z,2020-11-18T18:17:47Z,model card,,
8564,b'Make BART more ONNX friendly',2020-11-16T14:46:11Z,2021-03-06T00:16:12Z,wontfix,,
8563,b'Wrong model_max_length for BERTOverflow tokenizer',2020-11-16T12:40:24Z,2021-04-15T15:06:59Z,,,
8562,b'Clearer Model Versioning Example in Model Card',2020-11-16T11:14:51Z,2020-11-16T11:59:10Z,model card,,
8561,b'Reset loss to zero on logging in Trainer to avoid bfloat16 issues',2020-11-16T11:14:16Z,2020-11-18T14:58:09Z,,,
8560,b'Prophetnet - predicted n-future tokens',2020-11-16T10:49:36Z,2020-11-25T17:48:00Z,,,
8559,b'TFGPT2LMHeadModel fp16 support',2020-11-16T10:39:08Z,2021-04-24T15:03:24Z,,TypeError,"TypeError: x and y must have the same dtype, got tf.float16 != tf.float32"
8558,b'Readme for Wiki Summary [Persian] bert2bert',2020-11-16T09:05:56Z,2020-11-16T10:04:47Z,model card,,
8557,b'Readme for News Headline Generation (bert2bert)',2020-11-16T07:01:09Z,2020-11-16T10:04:38Z,model card,,
8556,"b'tokenization_bart.py: return_tensors default should be ""pt""'",2020-11-15T23:31:53Z,2020-11-16T20:20:31Z,,,
8555,b'Allow the user to input positional embeddings',2020-11-15T22:54:02Z,2021-03-06T00:16:15Z,wontfix,,
8554,b'`disable_ngram_loss` fix for prophetnet',2020-11-15T21:55:06Z,2020-11-19T18:18:08Z,,,
8553,"b""`disable_ngram_loss` doesn't work correctly in ProphetNetForConditionalGeneration""",2020-11-15T21:21:07Z,2020-11-19T18:18:07Z,,,
8552,b'T5 & mT5',2020-11-15T21:03:41Z,2020-11-17T11:23:10Z,,,
8551,"b'""special token {} has to be either str or AddedToken but got:'",2020-11-15T20:55:26Z,2021-03-06T00:16:17Z,wontfix,"**""TypeError","**""TypeError: special token bos_token has to be either str or AddedToken but got: <class 'dict'>""**"
8550,b'Create README.md for Chinese RoBERTa Miniatures',2020-11-15T18:14:17Z,2020-11-16T10:01:29Z,model card,,
8545,b'Pretrain BERT with user defined vocabulary',2020-11-15T05:59:48Z,2021-01-17T00:27:08Z,wontfix,,
8544,b'Update README.md',2020-11-15T05:11:21Z,2020-11-18T18:19:33Z,model card,,
8543,b'Upload models using Git fails',2020-11-14T23:54:59Z,2021-01-19T08:00:10Z,wontfix,,
8542,b'Failed in predict function after converting xlnet model to onnx format',2020-11-14T20:49:35Z,2021-03-06T00:16:18Z,wontfix,onnxruntime.capi.onnxruntime_pybind11_state.RuntimeException,"onnxruntime.capi.onnxruntime_pybind11_state.RuntimeException: [ONNXRuntimeError] : 6 : RUNTIME_EXCEPTION : Non-zero status code returned while running Add node. Name:'Add_26' Status Message: /onnxruntime_src/onnxruntime/core/providers/cpu/math/element_wise_ops.h:475 void onnxruntime::BroadcastIterator::Init(int64_t, int64_t) axis == 1 || axis == largest was false. Attempting to broadcast an axis by a dimension other than 1. 6 by 48"
8541,b'Specify label name',2020-11-14T20:40:34Z,2020-11-15T19:43:28Z,,,
8540,b'Add model_max_length property on fast tokenizers',2020-11-14T19:40:20Z,2020-11-14T19:50:24Z,,AttributeError,"AttributeError: 'BertTokenizerFast' object has no attribute 'model_max_len'"
8539,"b""TFLongformer Error: TypeError: __init__() missing 1 required positional argument: 'last_hidden_state'""",2020-11-14T17:33:33Z,2021-01-24T02:33:51Z,wontfix,TypeError,"TypeError: in user code:"
8538,b'TFBertModel not working at all with any type of model',2020-11-14T15:22:13Z,2021-03-06T00:16:20Z,wontfix,tensorflow.python.framework.errors_impl.InvalidArgumentError,"tensorflow.python.framework.errors_impl.InvalidArgumentError:  indices[5,0] = 102 is not in [0, 2)"
8537,b'Add a new model ConvBert',2020-11-14T09:29:35Z,2021-02-03T09:38:40Z,New model,,
8536,b'Pretrain PEGASUS from scratch',2020-11-14T08:40:32Z,2021-06-04T15:19:21Z,wontfix,,
8535,b'[doc] typo fix',2020-11-14T06:10:50Z,2020-11-16T13:05:31Z,,,
8534,b'mBart prefix and suffix for language id',2020-11-14T00:13:40Z,2020-12-01T09:44:37Z,,,
8533,b'Bertabs example: index_select(): Expected dtype int64 for index',2020-11-13T23:42:47Z,2021-03-06T00:16:23Z,wontfix,RuntimeError,"RuntimeError: index_select(): Expected dtype int64 for index"
8532,b'converting tensorflow checkpoint to pytorch',2020-11-13T23:32:29Z,2020-11-16T02:51:19Z,,,
8531,b'[models website: files section] various issues/suggestions for a better UI',2020-11-13T20:34:59Z,2021-03-18T01:51:00Z,Feature request,,
8530,b'Switch `return_dict` to `True` by default.',2020-11-13T20:22:32Z,2020-11-16T16:43:00Z,model card,,
8529,b'Adding PrefixConstrainedLogitsProcessor',2020-11-13T19:48:10Z,2020-11-18T16:06:26Z,,,
8528,b'[T5] Fix load weights function',2020-11-13T19:14:11Z,2020-11-13T19:31:41Z,,,
8527,b'Add bart-large-mnli model card',2020-11-13T18:19:03Z,2020-11-13T19:07:26Z,model card,,
8526,b'Problem while pretraining MLM from scratch using Transformers',2020-11-13T17:51:07Z,2021-01-24T02:33:49Z,wontfix,FileNotFoundError,"FileNotFoundError: [Errno 2] No such file or directory: '**/checkpoint-128/optimizer.pt'"
8525,"b""`TypeError: unhashable type: 'list'` when using DataCollatorForWholeWordMask""",2020-11-13T16:37:42Z,2020-11-14T03:41:03Z,,TypeError,"TypeError: unhashable type: 'list'"
8524,b'LayoutLM Token Classification not learning',2020-11-13T16:04:14Z,2021-03-06T00:16:25Z,wontfix,,
8523,b'Reformer model crashes during casual LM evaluation',2020-11-13T14:50:44Z,2020-11-19T15:43:15Z,,AttributeError,"AttributeError: 'NoneType' object has no attribute 'detach'"
8522,b'Update deepset/roberta-base-squad2 model card',2020-11-13T14:40:27Z,2020-11-13T14:58:28Z,model card,,
8521,"b""Tagged versions of model in new model hub don't work""",2020-11-13T14:17:51Z,2020-11-13T14:54:02Z,,,
8520,b'Model sharing doc: more tweaks',2020-11-13T13:53:08Z,2020-11-13T17:10:27Z,,,
8519,b'MLflowCallback to log run_name argument',2020-11-13T12:07:03Z,2021-01-24T02:33:47Z,wontfix,,
8518,b'[T5] Bug correction & Refactor',2020-11-13T11:40:04Z,2020-11-13T15:57:32Z,,,
8517,b'XLM-RoBERTa tokenizer changes characters during tokenization',2020-11-13T09:53:07Z,2020-11-14T08:42:53Z,,,
8516,b'SWA',2020-11-13T09:44:08Z,2020-11-13T09:53:38Z,,,
8515,b'Adding the prepare_seq2seq_batch function to ProphetNet',2020-11-13T09:24:27Z,2020-11-16T13:18:26Z,,,
8514,b'How to pretrain the model (like Roberta) again?',2020-11-13T08:37:58Z,2021-01-24T02:33:50Z,wontfix,,
8513,b'Using Pretrained BERT model to add additional words that are not recognized by the model',2020-11-13T06:49:24Z,2020-11-13T17:46:05Z,,,
8512,b'Issue while model sharing and uploading on huggingface',2020-11-13T05:21:25Z,2020-11-13T13:55:37Z,,,
8511,b'Adding Confusion matrix support in Trainer',2020-11-13T02:26:34Z,2020-11-13T09:19:50Z,,,
8510,b'Finetune TFBertForMaskedLM model.fit() ValueError',2020-11-13T00:58:37Z,2021-03-06T00:16:27Z,wontfix,ValueError,"ValueError: in user code:"
8509,b'Model templates encoder only',2020-11-12T20:51:50Z,2020-11-13T16:59:31Z,,,
8508,b'TPU issue: possible memory leak in eval loop',2020-11-12T20:45:23Z,2020-11-30T18:52:56Z,,,
8507,b'Fill-mask pipeline removes space after token prediction when loading pre-training model based on roberta-base',2020-11-12T20:10:06Z,2021-01-24T02:33:43Z,wontfix,,
8506,"b""DPR model: FileNotFoundError: Couldn't find file""",2020-11-12T19:09:49Z,2020-11-12T19:50:36Z,,FileNotFoundError,"FileNotFoundError: Couldn't find file at https://storage.googleapis.com/huggingface-nlp/cache/datasets/wiki_dpr/psgs_w100.no_embeddings.compressed/0.0.0/psgs_w100.nq.IVFPQ4096_HNSW32_PQ64-IP-train.faiss"
8505,b'Unexpected behavior when using PubMedBERT with AutoModelForMaskedLM',2020-11-12T19:09:27Z,2021-01-24T02:33:42Z,wontfix,,
8504,b'Failed to push model repo',2020-11-12T18:31:57Z,2020-11-12T19:04:39Z,,,
8503,b'Training the TFGPT2LMHeadModel with model.fit produces error',2020-11-12T17:27:11Z,2021-04-15T15:07:01Z,,,
8502,b'TF T5-small with output hidden state and attention not owrking',2020-11-12T16:21:00Z,2021-01-18T10:30:11Z,wontfix,IndexError,"IndexError: list index out of range"
8501,b'Why is the XLM-RoBERTa sometimes producing a standalone start of the word character (the special underscore with ord = 9601)',2020-11-12T15:56:00Z,2020-12-08T14:17:48Z,,,
8500,b'Fix doc bug',2020-11-12T15:48:03Z,2020-11-12T16:47:23Z,,,
8499,b'Unable to install Transformers',2020-11-12T15:33:12Z,2020-11-13T15:50:09Z,,,
8498,b'Model sharing doc',2020-11-12T15:23:33Z,2020-11-12T16:53:23Z,,,
8497,b'Error when loading a model cloned without git-lfs is quite cryptic',2020-11-12T15:20:15Z,2021-09-08T12:28:22Z,Feature request,"UnpicklingError, OSError","UnpicklingError: invalid load key, 'v'.OSError: Unable to load weights from pytorch checkpoint file for './bart-large-cnn' at './bart-large-cnn/pytorch_model.bin'If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True. "
8496,b'Created ModelCard for Hel-ach-en MT model',2020-11-12T15:00:59Z,2020-11-18T19:42:13Z,model card,,
8495,b'Allow tensorflow tensors as input to Tokenizer',2020-11-12T14:29:01Z,2020-11-12T20:04:24Z,,AssertionError,"AssertionError: text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples)."
8494,b'error occurs when trainning transformer-xl by ddp',2020-11-12T12:16:58Z,2021-01-24T02:33:40Z,wontfix,RuntimeError,"RuntimeError: Expected object of scalar type Float but got scalar type Half for argument #4 'source' in call to th_index_copy"
8493,b'I meet the zero gradient descent',2020-11-12T11:29:45Z,2020-11-12T12:18:49Z,,,
8492,b'Rework some TF tests',2020-11-12T11:15:28Z,2020-11-13T22:07:18Z,,,
8491,b'Fix check scripts for Windows',2020-11-12T10:47:05Z,2020-11-12T18:52:41Z,,,
8490,b'New TF loading weights',2020-11-12T10:23:37Z,2020-11-18T15:48:32Z,,,
8489,b'Fix typo in roberta-base-squad2-v2 model card',2020-11-12T10:15:46Z,2020-11-12T10:29:38Z,model card,,
8488,b'[WIP] T5v1.1 & MT5',2020-11-12T09:46:23Z,2020-11-15T21:04:34Z,,,
8487,b'`log_history` does not contain metrics anymore',2020-11-12T09:28:09Z,2020-11-16T20:52:28Z,,,
8486,b'Gradient accumulation averages over gradients',2020-11-12T08:28:47Z,2021-01-19T00:56:38Z,wontfix,,
8485,b'Prediction loop: work with batches of variable length (fixed per batch)',2020-11-12T05:49:02Z,2020-11-12T21:51:42Z,,,
8484,b'automodel',2020-11-12T05:15:54Z,2020-11-12T05:16:29Z,,,
8483,b'transformers.TFTrainer: Does not support batches with sequences of variable lengths?',2020-11-12T05:14:14Z,2021-01-19T00:56:36Z,wontfix,,
8482,b'TAPAS tokenizer & tokenizer tests',2020-11-12T04:10:49Z,2020-11-16T16:30:40Z,,,
8481,b'TAPAS Tokenizer & tokenizer tests',2020-11-12T04:03:21Z,2020-11-12T04:08:55Z,,,
8480,"b'Error when upload models: ""LFS: Client error""'",2020-11-12T02:05:49Z,2021-01-24T02:33:48Z,wontfix,,
8479,b'Fix SqueezeBERT for masked language model',2020-11-12T01:12:38Z,2020-11-12T17:19:38Z,model card,,
8478,b'[s2s] finetune.py: specifying generation min_length',2020-11-12T00:53:22Z,2020-11-26T07:03:03Z,,,
8477,b'How to print out the probability for each bean search result in gpt2 text generator?',2020-11-12T00:21:14Z,2021-01-19T00:56:37Z,wontfix,,
8476,b'Trainer runs out of memory when computing eval score',2020-11-12T00:09:50Z,2020-11-12T21:32:37Z,,RuntimeError,"RuntimeError: [enforce fail at CPUAllocator.cpp:64] . DefaultCPUAllocator: can't allocate memory: you tried to allocate 57680691200 bytes. Error code 12 (Cannot allocate memory)"
8475,b'Update deploy-docs dependencies on CI to enable Flax',2020-11-11T23:13:10Z,2020-11-11T23:31:42Z,,,
8474,"b'Fix on ""examples/language-modeling"" to support more datasets'",2020-11-11T22:05:39Z,2020-11-12T14:47:09Z,,,
8473,b'Support fp16 for inference',2020-11-11T21:06:59Z,2021-03-06T00:16:30Z,wontfix,,
8472,b'GPT2 (pre-trained not fine-tuned) only generates additional special tokens ',2020-11-11T21:03:11Z,2020-11-14T02:17:04Z,,,
8471,b'TFBertForTokenClassification scoring only O labels on a NER task',2020-11-11T17:20:09Z,2020-11-11T19:53:18Z,,,
8470,b'Add pretraining loss computation for TF Bert pretraining',2020-11-11T16:32:09Z,2020-11-12T19:08:27Z,,,
8469,b'Pegasus models load very slowly or do not load at all on initial execution of from_pretrained() when Python is spawned from within a Node.js process',2020-11-11T16:24:27Z,2021-01-24T02:33:46Z,wontfix,,
8468,b'Example NER script predicts on tokenized dataset',2020-11-11T15:14:39Z,2020-11-11T15:28:24Z,,,
8467,b'Fine tuning a classification model with engineered features',2020-11-11T15:04:46Z,2020-11-11T19:51:34Z,,,
8466,b'Fix TF next sentence output',2020-11-11T14:28:44Z,2020-11-11T14:41:39Z,,,
8465,b'Pytorch Vs Onnx: Pytorch is faster and provides different output',2020-11-11T14:15:08Z,2021-03-06T00:16:32Z,wontfix,,
8464,b'Add model card for ai4bharat/indic-bert',2020-11-11T14:06:51Z,2020-11-18T18:28:50Z,model card,,
8463,b'Better regex expression for extracting language code in tokenization_marian.py',2020-11-11T13:34:25Z,2021-03-06T00:16:34Z,"wontfix, marian",,
8462,b'Add next sentence prediction loss computation',2020-11-11T13:00:10Z,2020-11-11T14:02:07Z,,,
8461,b'multiple hard-coded paths in transformers/file_utils.py',2020-11-11T11:55:40Z,2020-11-11T11:58:38Z,,,
8460,b'Fix TF Longformer',2020-11-11T11:48:08Z,2020-11-11T11:54:16Z,,,
8459,b'Question Answering Documentation Example Bug',2020-11-11T11:07:19Z,2021-03-06T00:16:36Z,wontfix,InvalidArgumentError,"InvalidArgumentError: Value for attr 'T' of string is not in the list of allowed values: float, double, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64, bool"
8458,b'Fix logging in the examples',2020-11-11T10:18:46Z,2020-11-12T18:43:00Z,,,
8457,b'Correct mark down grammar in readme file',2020-11-11T09:12:17Z,2021-03-06T00:16:37Z,wontfix,,
8456,"b""ValueError: No gradients provided for any variable: ['tf_bert_for_masked_lm_6/bert/embeddings/word_embeddings/weight:0', 'tf_bert_for_masked_lm_6/bert/embeddings/position_embeddings/embeddings:0'......""",2020-11-11T07:05:15Z,2020-11-13T00:46:13Z,,ValueError,"ValueError: in user code:"
8455,"b""Can't down models from huggingface.cn!""",2020-11-11T06:49:07Z,2020-11-11T07:45:47Z,,,
8454,b'Add POINTER model',2020-11-11T06:35:13Z,,New model,,
8453,"b""_pickle.PicklingError: Can't pickle typing.Union[str, NoneType]: it's not the same object as typing.Union""",2020-11-11T04:48:13Z,2021-01-24T02:33:41Z,wontfix,**_pickle.PicklingError,"**_pickle.PicklingError: Can't pickle typing.Union[str, NoneType]: it's not the same object as typing.Union**"
8452,b'Fine-tuning GPT: problems with padding',2020-11-11T04:17:35Z,2021-02-22T22:03:55Z,,,
8451,b'config.attention_head_size for structured pruning out-of-box',2020-11-11T04:10:38Z,2020-11-13T01:16:46Z,,,
8450,b'A layman wants to train DistilBERT',2020-11-11T03:12:00Z,2021-01-19T00:56:35Z,wontfix,,
8449,"b""Can't find model to down""",2020-11-11T02:52:56Z,2021-04-24T15:03:25Z,wontfix,,
8448,b'Make sure the slot variables are created under the same strategy scope.',2020-11-10T19:52:15Z,2020-11-10T20:01:32Z,,,
8447,"b""Model name 'facebook/rag-sequence-base/*' not found when running examples/rag/finetune.sh""",2020-11-10T19:06:07Z,2020-11-17T13:58:45Z,,**TypeError,"**TypeError: __init__() got an unexpected keyword argument 'index'**"
8446,b'using multi_gpu consistently',2020-11-10T18:03:14Z,2020-11-10T18:23:59Z,,,
8445,b'[marian.rst] remove combined lines',2020-11-10T18:03:09Z,2020-11-10T18:21:39Z,,,
8444,b'Add missing import',2020-11-10T16:44:36Z,2020-11-10T17:01:33Z,,,
8443,b'Dropout p is changing after loading',2020-11-10T16:15:40Z,2021-01-17T21:35:13Z,wontfix,,
8442,b'Models fine-tuned with gradient checkpointing (=True) fails to export to ONXX',2020-11-10T15:20:38Z,2021-01-18T07:02:17Z,wontfix,RuntimeError,"RuntimeError: ONNX export failed: Couldn't export Python operator CheckpointFunction"
8441,b'CUDA out of memory (ALBERT)!!',2020-11-10T14:25:00Z,2020-11-12T12:43:31Z,,,
8440,b'Question template',2020-11-10T14:17:50Z,2020-11-10T15:07:57Z,,,
8439,b'Model sharing rst',2020-11-10T13:25:45Z,2020-11-10T13:35:12Z,,,
8438,b'login to huggingface forum',2020-11-10T13:06:20Z,2020-11-11T08:43:20Z,,,
8437,b'[T5Tokenizer] fix t5 token type ids',2020-11-10T11:58:31Z,2020-11-10T19:21:54Z,,,
8436,b'Windows dev section in the contributing file',2020-11-10T11:16:03Z,2020-11-10T16:19:17Z,,,
8435,b'[T5 Tokenizer] Fix t5 special tokens',2020-11-10T11:10:09Z,2020-11-10T17:54:18Z,,,
8434,b'Support serialized tokenizer in AutoTokenizer',2020-11-10T10:54:03Z,2021-04-25T15:04:47Z,,,
8433,b'Replaced unnecessary iadd operations on lists in tokenization_utils.py with proper list methods',2020-11-10T10:20:07Z,2020-11-11T17:29:58Z,,,
8432,b'Add auto next sentence prediction',2020-11-10T09:39:30Z,2020-11-10T16:11:49Z,,,
8431,b'Get Scores for each NE Label',2020-11-10T08:55:21Z,2020-11-16T09:46:46Z,,,
8430,b'RAG: Explanation on Retriever Variables.',2020-11-10T03:10:56Z,2020-11-11T19:07:14Z,,,
8429,b'[examples] better PL version check',2020-11-10T03:04:33Z,2020-11-10T14:33:24Z,,,
8428,b'Add missing tasks to `pipeline` docstring',2020-11-10T02:08:40Z,2020-11-10T18:44:26Z,model card,,
8427,b'Set num_beams=4 for all Helsinki-NLP models',2020-11-09T21:09:56Z,2021-01-18T07:47:35Z,marian,,
8426,"b'Wrong files names in model list for ""xlm-roberta-large-finetuned-conll03-german""'",2020-11-09T19:38:25Z,2020-11-09T21:13:41Z,,,
8425,b'Check all models are in an auto class',2020-11-09T19:34:49Z,2020-11-09T20:44:55Z,,,
8424,b'Electra multi-gpu pretraining.',2020-11-09T19:21:07Z,2021-01-18T07:02:16Z,wontfix,,
8423,b'Fix bart shape comment',2020-11-09T18:04:26Z,2020-11-09T18:25:33Z,,,
8422,b'[docs] [testing] gpu decorators table',2020-11-09T17:59:20Z,2020-11-09T19:27:43Z,,,
8421,b'[docs] improve bart/marian/mBART/pegasus docs',2020-11-09T17:11:52Z,2020-11-10T15:18:35Z,,,
8420,b'Deprecate old data/metrics functions',2020-11-09T16:23:19Z,2020-11-09T17:10:10Z,,,
8419,b'Bump tokenizers',2020-11-09T15:56:26Z,2020-11-09T16:32:10Z,,,
8418,b'[docs] remove sshleifer from issue-template :(',2020-11-09T14:22:46Z,2020-11-09T17:51:39Z,,,
8417,b'Changing XLNet default from not using memories to 512 context size following paper',2020-11-09T13:11:57Z,2020-11-10T01:49:52Z,,,
8416,b'Does MBartTokenizer remove the parameter decoder_input_ids?',2020-11-09T12:51:58Z,2020-11-10T14:35:54Z,Documentation,KeyError,"KeyError: 'decoder_input_ids'"
8415,b'[Tests] Add Common Test for Training + Fix a couple of bugs',2020-11-09T09:26:38Z,2020-11-09T17:24:42Z,model card,,
8414,b'[seq2seq] translation tpu example doesnt work',2020-11-09T05:51:18Z,2021-04-26T15:02:47Z,,"RuntimeError, Exception","RuntimeError: Cannot access data pointer of Tensor that doesn't have storageException: process 0 terminated with exit code "
8413,b'continuing fine-tuning from the last checkpoint ',2020-11-08T22:09:33Z,2020-11-09T16:46:53Z,,OSError,"OSError: Can't load config for '/media/ai-students/Data/Nesara/Bert_MLM_fine_tune/new_results/result_dir/checkpoint_37000/'. Make sure that:"
8412,"b'[s2s/distill] remove run_distiller.sh, fix xsum script'",2020-11-08T21:56:53Z,2020-11-08T21:57:44Z,,,
8411,b'Tokenizer return nothing instead of unk for certain token? ',2020-11-08T20:16:59Z,2020-11-08T20:56:42Z,,,
8410,b'comet_ml init weirdness',2020-11-08T19:53:02Z,2020-11-09T08:36:07Z,,AttributeError,"AttributeError: module 'comet_ml' has no attribute 'config'"
8409,b'Bug fix for permutation language modelling',2020-11-08T19:23:25Z,2020-11-09T15:23:27Z,,,
8408,b'updating tag for exbert viz',2020-11-08T15:36:14Z,2020-11-09T08:43:55Z,model card,,
8407,b'All the weights of the model checkpoint at roberta-base were not used when initializing',2020-11-08T13:53:04Z,2020-11-11T06:23:09Z,,,
8406,b'Update README.md',2020-11-08T11:29:37Z,2020-11-09T08:44:44Z,model card,,
8405,b'Update README.md',2020-11-08T11:26:40Z,2020-11-18T18:23:09Z,model card,,
8404,"b""Tokenizer problem for model 'patrickvonplaten/longformer-random-tiny'""",2020-11-08T08:34:47Z,2020-11-09T08:18:30Z,,OSError,"OSError: Model name 'patrickvonplaten/longformer-random-tiny' was not found in tokenizers model name list (allenai/longformer-base-4096, allenai/longformer-large-4096, allenai/longformer-large-4096-finetuned-triviaqa, allenai/longformer-base-4096-extra.pos.embd.only, allenai/longformer-large-4096-extra.pos.embd.only). We assumed 'patrickvonplaten/longformer-random-tiny' was a path, a model identifier, or url to a directory containing vocabulary files named ['vocab.json', 'merges.txt'] but couldn't find such vocabulary files at this path or url."
8403,b'[s2s finetune] huge increase in memory demands with --fp16 native amp',2020-11-08T07:14:19Z,2020-12-04T19:35:53Z,,,
8402,b'Add gpt2-medium-chinese model card',2020-11-08T03:10:03Z,2020-11-08T10:00:20Z,model card,,
8401,b'[testing utils] get_auto_remove_tmp_dir more intuitive behavior',2020-11-08T03:00:33Z,2020-11-10T16:57:22Z,,,
8400,b'[s2s test_finetune_trainer] failing multigpu test',2020-11-08T02:33:50Z,2020-11-08T21:45:40Z,,,
8399,b'Fixed Trainer default labels for QA Model',2020-11-08T02:22:53Z,2020-11-08T14:08:15Z,,,
8398,b'[s2s examples test] fix data path',2020-11-08T02:15:21Z,2020-11-08T21:44:18Z,,,
8397,b'Fix DataCollatorForWholeWordMask again',2020-11-08T01:55:35Z,2020-11-08T14:53:02Z,,,
8396,b'Inreproducible loss when train same setting bert model >=2 times',2020-11-08T01:41:10Z,2021-01-18T07:02:10Z,wontfix,,
8395,b'[model card] fix md table',2020-11-08T01:18:01Z,2020-11-08T09:25:15Z,model card,,
8394,b'Fix run_mlm_wwm example',2020-11-08T00:58:41Z,2021-04-24T15:03:26Z,,,
8393,b'Add barthez model',2020-11-07T23:40:56Z,2020-11-27T17:31:43Z,,,
8392,"b'Training script ""run_mlm.py"" doesn\'t work for certain datasets'",2020-11-07T23:21:30Z,2020-11-11T22:11:09Z,,TypeError,"TypeError: can only concatenate list (not ""str"") to list"
8391,b'Bug fix for apply_chunking_to_forward chunking dimension check',2020-11-07T20:37:55Z,2020-11-10T20:33:12Z,,,
8390,b'Trainer QA Model Label Names',2020-11-07T19:13:17Z,2020-11-08T14:08:14Z,,,
8389,b'[fsmt tokenizer] support lowercase tokenizer',2020-11-07T18:36:02Z,2020-11-09T15:41:40Z,,,
8388,b'DataCollatorForWholeWordMask error persists after fix',2020-11-07T18:14:31Z,2020-11-08T14:53:02Z,,AttributeError,"AttributeError: 'list' object has no attribute 'tolist'"
8387,b'Add LMHeadModel similar to BertLMHeadModel to modeling_distilbert.py',2020-11-07T17:30:15Z,2021-02-28T09:57:02Z,,,
8386,"b""ImportError: cannot import name 'is_flax_available' from 'transformers.file_utils'""",2020-11-07T17:14:20Z,2020-11-09T19:25:28Z,,ImportError,"ImportError: cannot import name 'is_flax_available' from 'transformers.file_utils' (/opt/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/file_utils.py)`"
8385,b'TPU padding in seq2seq codes ',2020-11-07T15:45:26Z,2020-11-08T13:27:41Z,,,
8384,b'comment correction in modeling_bart.py',2020-11-07T15:06:55Z,2020-11-09T18:25:33Z,,,
8383,b'finetune_trainer crashes in the beginning ',2020-11-07T14:02:01Z,2020-12-18T23:46:48Z,,FileNotFoundError,"FileNotFoundError: [Errno 2] No such file or directory: '/idiap/home/rkarimi/.cache/huggingface'"
8382,b'[s2s/distill] hparams.tokenizer_name = hparams.teacher',2020-11-07T13:34:29Z,2020-11-10T14:32:02Z,,,
8381,"b""initialize student's tokenizer name Update distillation.py""",2020-11-07T12:45:54Z,2020-11-07T13:36:35Z,,,
8380,b'training T5 on  multiple datasets ',2020-11-07T12:22:55Z,2020-11-09T14:49:51Z,,,
8379,b'Fix DataCollatorForWholeWordMask',2020-11-07T11:53:23Z,2020-11-07T17:51:57Z,,,
8378,b'DataCollatorForWholeWordMask is missing _tensorize_batch method',2020-11-07T11:25:08Z,2020-11-07T17:53:16Z,,AttributeError,"AttributeError: 'DataCollatorForWholeWordMask' object has no attribute '_tensorize_batch'"
8377,b'[fsmt convert script] fairseq broke chkpt data - fixing that',2020-11-07T04:46:35Z,2020-11-09T16:57:43Z,,KeyError,"KeyError: 'source_lang'"
8376,b'[s2s] distill t5-large -> t5-small',2020-11-07T01:11:19Z,2020-11-11T22:58:46Z,,,
8375,b'RobertaTokenizerFast is around 10 times slower than BertTokenizerFast #510',2020-11-06T21:17:26Z,2021-01-18T07:02:15Z,wontfix,,
8374,b'[wip] [fsmt] possible support of iwslt14 in fsmt',2020-11-06T20:48:59Z,2020-11-07T04:58:14Z,,,
8373,b'removing runs folders ',2020-11-06T20:35:15Z,2021-01-18T07:02:04Z,wontfix,,
8372,b'Fine-tuning for QA: how to prepare custom dataset?',2020-11-06T20:26:39Z,2020-11-08T16:59:51Z,,,
8371,b'[make] rewrite modified_py_files in python to be cross-platform',2020-11-06T19:33:00Z,2020-11-07T17:45:17Z,,,
8370,"b""BertTokenizerFast object has no attribute 'ids_to_tokens'""",2020-11-06T19:16:54Z,2020-11-09T18:07:58Z,,,
8369,b'Model card: T5-base fine-tuned on QuaRTz',2020-11-06T18:50:21Z,2020-11-18T18:34:28Z,model card,,
8368,b'[TF generate] Cut encoder outptus to just last hidden states for now',2020-11-06T18:36:44Z,2020-11-06T20:03:26Z,,,
8367,b'Which model to choose for seq2seq(generating headers for articles)?',2020-11-06T18:00:14Z,2020-11-09T14:33:29Z,,,
8366,b'Some added tests for TokenClassificationArgumentHandler',2020-11-06T17:47:35Z,2020-11-06T18:16:57Z,,,
8365,b'TFTrainerArguments: ImportError: Method `device` requires PyTorch.',2020-11-06T16:23:46Z,2020-11-06T16:38:14Z,,ImportError,"ImportError: Method `device` requires PyTorch."
8364,b'Patch token classification pipeline',2020-11-06T15:54:03Z,2020-11-10T12:29:34Z,,,
8363,b'Create README.md',2020-11-06T15:26:03Z,2020-11-18T18:33:04Z,model card,,
8362,b'Create README.md',2020-11-06T15:25:08Z,2020-11-18T18:37:15Z,model card,,
8361,b'TF generate() function is incompatible with output_attention and output_hidden_states ',2020-11-06T14:30:52Z,2020-11-06T20:03:26Z,Should Fix,AssertionError,"AssertionError: There should be 4 past states. 2 (past / key) for self attention.2 (past / key) for cross attention Got 3 past key / value states"
8360,b'Update README.md',2020-11-06T13:10:02Z,2020-11-06T16:45:47Z,model card,,
8359,b'Fix some tooling for windows',2020-11-06T13:09:55Z,2020-11-09T12:50:38Z,,,
8358,b'[WIP] Add performer in flax',2020-11-06T11:56:32Z,2021-04-21T16:40:02Z,,,
8357,b'Cannot Load roberta tokenizer',2020-11-06T11:08:58Z,2020-11-07T11:29:19Z,,OSError,"OSError: Model name './models/RoBERTa_zh_Large_Pytorch/' was not found in tokenizers model name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). We assumed './models/RoBERTa_zh_Large_Pytorch/' was a path, a model identifier, or url to a directory containing vocabulary files named ['vocab.json', 'merges.txt'] but couldn't find such vocabulary files at this path or url."
8356,"b'  assert tgt_line, f""empty tgt line for index {index}"" with t5 '",2020-11-06T10:22:31Z,2021-04-24T15:03:27Z,,AssertionError,AssertionError: empty tgt line for index 4549523
8355,b'finetune_trainer segfault ',2020-11-06T09:10:42Z,2020-12-18T23:48:43Z,,"RuntimeError, Exception","RuntimeError: Resource exhausted: From /job:tpu_worker/replica:0/task:0:Exception: process 0 terminated with exit code 17"
8354,b'update from v3.0.0 to v3.4.0 got an error',2020-11-06T08:08:02Z,2021-01-18T07:02:09Z,wontfix,,
8353,b'[s2s trainer examples] a tight quality regression test',2020-11-06T06:20:10Z,2020-12-04T19:35:41Z,,,
8352,"b""I encountered an error when I was running code that the object could not be called.But the BertTokenizer doesn't exist in my code.""",2020-11-06T03:22:31Z,2021-01-18T07:02:03Z,wontfix,TypeError,"TypeError: 'BertTokenizer' object is not callable"
8351,b'Fix typo',2020-11-06T02:40:37Z,2020-11-06T16:19:42Z,,,
8350,b'torch 1.4.0 transformers segment fault',2020-11-06T02:08:19Z,2020-11-06T16:19:07Z,,,
8349,b'apply_chunking_to_forward should only be the same in the chunking dimension',2020-11-06T01:43:00Z,2020-11-10T20:33:11Z,,,
8348,b'PEGASUS generation/decoding VERY Slow',2020-11-06T00:37:22Z,2021-01-18T07:02:05Z,"wontfix, Summarization",,
8347,b'TFTrainer stuck in evaluation',2020-11-05T23:35:20Z,2021-01-18T07:02:01Z,wontfix,,
8346,b'Adding kNN language modeling and Machine Translation',2020-11-05T22:50:04Z,,"New model, Feature request",,
8345,b'Error in RAG finetuning script',2020-11-05T22:32:36Z,2020-11-20T18:05:04Z,,**torch.nn.modules.module.ModuleAttributeError,"**torch.nn.modules.module.ModuleAttributeError: 'GenerativeQAModule' object has no attribute 'opt'**"
8344,b'model parallelism for BART',2020-11-05T21:56:29Z,2021-04-24T15:03:28Z,Model Parallel,,
8343,b'[model_cards] Update Italian BERT models and introduce new Italian XX\xe2\x80\xa6',2020-11-05T21:47:03Z,2020-11-06T08:17:04Z,model card,,
8342,"b""got an unexpected keyword argument 'early_stop_callback'""",2020-11-05T21:15:28Z,2020-11-06T00:37:12Z,,,
8341,b'[github CI] add a multi-gpu job for all example tests',2020-11-05T21:11:24Z,2020-11-09T20:47:39Z,,,
8340,b'Add new token classification example',2020-11-05T21:10:05Z,2020-11-09T16:39:56Z,model card,,
8339,b'finetune_trainer being really slow on TPU',2020-11-05T20:27:46Z,2021-01-18T07:02:02Z,wontfix,,
8338,b'Update README.md',2020-11-05T20:03:59Z,2020-11-06T11:20:58Z,model card,,
8337,b'pip cannot install transformers with python version 3.X version on Ubuntu 18.04',2020-11-05T19:51:33Z,2021-01-18T07:01:58Z,wontfix,,
8336,b'Make Trainer evaluation handle dynamic seq_length',2020-11-05T19:31:08Z,2020-11-05T20:13:52Z,,,
8335,b'Language Model to get only the score of predefined tokens',2020-11-05T17:56:13Z,2021-01-17T21:35:16Z,wontfix,ResourceExhaustedError,"ResourceExhaustedError: OOM when allocating tensor"
8334,b'Model card: T5-base fine-tuned on QuaRel',2020-11-05T17:27:14Z,2020-11-06T08:09:55Z,model card,,
8333,b'FRCNN in the LXMERT demo outputs different features when using a local image vs. an image from a URL ',2020-11-05T17:24:18Z,2021-04-24T15:03:29Z,,,
8332,b'Measuring time when using xla_spawn on multiple cores ',2020-11-05T17:13:17Z,2021-01-17T21:35:17Z,wontfix,,
8331,b'Flax/Jax documentation',2020-11-05T16:43:14Z,2020-11-11T19:53:37Z,,,
8330,b'Docs bart training ref',2020-11-05T16:07:05Z,2020-11-05T22:20:58Z,,,
8329,b'[Seq2SeqDataCollator] dont pass add_prefix_space=False to all tokenizers',2020-11-05T15:15:21Z,2020-11-05T16:42:25Z,,,
8328,b'Return raw outputs in TextClassificationPipeline',2020-11-05T14:55:58Z,2021-08-04T12:42:47Z,,,
8327,b'Create README.md',2020-11-05T14:22:04Z,2020-11-06T08:22:53Z,model card,,
8326,b'Muti GPU training with torch==1.7.0 not working',2020-11-05T14:12:43Z,2020-11-06T09:20:54Z,,,
8325,b'Adding call back to measure time of each step ',2020-11-05T13:53:18Z,2021-01-17T21:35:10Z,wontfix,,
8324,b'Model versioning',2020-11-05T13:10:18Z,2020-11-10T12:11:03Z,"Core: CLI, Documentation, model card",,
8323,b'AlbertTransformer head_mask not subscriptable error when not passed',2020-11-05T12:58:53Z,2021-05-06T06:18:02Z,,TypeError,"TypeError: 'NoneType' object is not subscriptable"
8322,"b""Keyword arguments {'add_prefix_space': False} not recognized.""",2020-11-05T12:58:42Z,2020-11-05T16:42:25Z,,,
8321,b'tensorboard.compat.tensorflow_stub.errors.AlreadyExistsError: Directory already exists',2020-11-05T11:32:06Z,2021-01-17T21:35:14Z,wontfix,,
8320,b'Corrected tpu typo in examples readme',2020-11-05T09:06:44Z,2020-11-05T12:48:36Z,,,
8319,b' Does Tokenizer provide parameters to split the number\xef\xbc\x9f',2020-11-05T08:21:29Z,2021-01-17T21:35:07Z,wontfix,,
8318,b'[s2s] test_bash_script.py - actually learn something',2020-11-05T06:17:16Z,2020-11-06T04:15:14Z,,,
8317,"b""FutureWarning: This config doesn't use attention memories, a core feature of XLNet even though I'm using mem_len""",2020-11-05T04:23:33Z,2020-11-09T13:04:17Z,,,
8316,b'No loss in model output for TFElectraForPreTraining',2020-11-05T03:56:08Z,2021-01-17T21:35:11Z,wontfix,,
8315,b'[s2s] test_distributed_eval',2020-11-05T03:21:56Z,2020-11-05T21:01:16Z,,,
8314,b'[QA examples] fix inconsistent tokenization in _improve_answer_span ',2020-11-05T03:12:51Z,2021-04-24T15:03:31Z,,,
8313,b'OOMKilled with exit code 137 with finetune_trainer.py',2020-11-05T02:09:40Z,2021-01-17T21:35:06Z,wontfix,,
8312,b'Create README.md',2020-11-05T00:37:31Z,2020-11-06T11:19:59Z,model card,,
8311,"b'error  \'ascii\' codec can\'t decode byte 0xc3 in position 6550: ordinal not in range(128)\\n"", when running finetune_trainer.py on multiple tpus '",2020-11-04T23:57:10Z,2021-01-17T21:35:05Z,wontfix,,
8310,b'Running seq2seq_trainer with iterable datasets ',2020-11-04T23:00:37Z,2021-01-18T07:02:06Z,wontfix,,
8309,"b""examples/docs: caveat that PL examples don't work on TPU""",2020-11-04T20:44:58Z,2020-11-09T13:55:23Z,,,
8308,b'Clean up data collators and datasets',2020-11-04T20:15:25Z,2020-11-04T22:24:49Z,,,
8307,b'run_mlm.py: error: argument',2020-11-04T19:21:47Z,2021-04-24T15:03:32Z,,,
8306,b'Tokenizers save_pretrained broken when defining vocab and merges file arguments (v3.1) ',2020-11-04T19:12:40Z,2021-04-24T15:03:32Z,,"**""TypeError","**""TypeError: Object of type AddedToken is not JSON serializable""**"
8305,b'Resource exhausted when training in loop',2020-11-04T18:06:29Z,2021-01-17T21:35:02Z,wontfix,tensorflow.python.framework.errors_impl.ResourceExhaustedError,"tensorflow.python.framework.errors_impl.ResourceExhaustedError: 2 root error(s) found."
8304,b'Exception: process 0 terminated with exit code 17 when using xla_spawn ',2020-11-04T18:00:33Z,2020-12-18T23:48:09Z,,Exception,"Exception: process 0 terminated with exit code 17"
8303,b'How can we freeze the last few layers of a  BERT model using tf 2.0(or higher)',2020-11-04T17:57:39Z,2021-01-17T21:35:01Z,wontfix,,
8302,b'Fix path to old run_language_modeling.py script',2020-11-04T17:18:06Z,2020-11-04T18:17:58Z,,,
8301,b'Speedup doc build',2020-11-04T16:40:07Z,2020-11-04T16:51:21Z,,,
8300,b'adding model cards for distilled models',2020-11-04T16:23:09Z,2020-11-04T16:41:46Z,model card,,
8299,b'Model card: T5-base fine-tuned on QASC',2020-11-04T16:03:58Z,2020-11-04T16:20:16Z,model card,,
8298,b'Fix validation file loading in scripts',2020-11-04T15:34:39Z,2020-11-04T15:42:19Z,,,
8297,b'[s2s] 1 GPU test for run_distributed_eval',2020-11-04T15:17:15Z,2020-11-05T21:01:16Z,Help wanted,,
8296,b'Update README.md',2020-11-04T15:08:07Z,2020-11-06T02:47:08Z,model card,,
8295,b'Validation data in `run_mlm.py` is the same as train data',2020-11-04T14:56:27Z,2020-11-04T15:42:19Z,,,
8294,b'pipelines: Tentative fix for AutoModel for PegasusConfig.',2020-11-04T14:36:24Z,2021-03-01T16:36:56Z,,,
8293,b'[Generate Test] fix greedy generate test',2020-11-04T14:29:35Z,2020-11-04T14:44:36Z,,,
8292,b'Fine Tune Bert Ner using TFBertForTokenClassification.from_pretrained',2020-11-04T14:01:07Z,2021-01-18T07:02:00Z,wontfix,ValueError,"ValueError: No gradients provided for any variable: ['tf_bert_for_token_classification_8/classifier/kernel:0', 'tf_bert_for_token_classification_8/classifier/bias:0']."
8291,b'could you please give me a torch example of xlm-roberta-(base/large) for multilingual-text question?',2020-11-04T13:38:09Z,2020-11-13T06:04:13Z,,,
8290,"b'finetuning T5 on translation on TPU, questions about clarifying the setup'",2020-11-04T13:11:56Z,2020-11-09T13:55:23Z,,,
8289,b'Why do I use XLMRobertaTokenizer and return an error on token_type_ids?',2020-11-04T12:23:43Z,2020-11-04T12:29:13Z,,KeyError,"KeyError: 'token_type_ids'"
8288,b'Training T5-large model for Question Answering',2020-11-04T12:00:07Z,2021-01-17T21:35:08Z,wontfix,,
8287,b'Fix typo in language-modeling README.md',2020-11-04T11:14:27Z,2020-11-04T14:38:03Z,,,
8286,b'Improve QA pipeline error handling',2020-11-04T10:41:23Z,2020-11-04T16:30:43Z,,,
8285,b'RAG performance on Open-NQ dataset much lower than expected',2020-11-04T09:52:24Z,2021-04-24T15:03:33Z,,,
8284,b'[rag] missing a working End-to-end evaluation example',2020-11-04T06:15:03Z,2021-03-18T01:40:49Z,Feature request,pandas.errors.ParserError,"pandas.errors.ParserError: Error tokenizing data. C error: Expected 5 fields in line 2, saw 6"
8283,"b""[tokenizers] convert_to_tensors: don't reconvert when the type is already right""",2020-11-04T05:31:27Z,2020-11-19T20:06:02Z,,,
8282,b'[blenderbot] regex fix',2020-11-04T02:37:31Z,2020-11-04T14:02:29Z,,,
8281,b'Create README.md',2020-11-04T02:18:16Z,2020-12-11T14:41:30Z,model card,,
8280,"b""Translation finetuning error : TypeError: '>' not supported between instances of 'function' and 'int'""",2020-11-04T01:44:34Z,2020-11-04T10:31:09Z,,TypeError,"TypeError: '>' not supported between instances of 'function' and 'int'"
8279,b'Finetuning T5 on translation wmt19(de-en)',2020-11-04T01:28:05Z,2020-11-04T01:44:48Z,,TypeError,"TypeError: '>' not supported between instances of 'function' and 'int'"
8278,"b""[commit #29b536a]AttributeError: module 'numpy.random' has no attribute 'Generator'""",2020-11-04T01:04:24Z,2020-11-05T09:51:14Z,,AttributeError,"AttributeError: module 'numpy.random' has no attribute 'Generator'"
8277,b'SqueezeBert does not appear to properly generate text',2020-11-03T22:48:11Z,2020-11-12T17:19:38Z,,,
8276,b'Support various BERT relative position embeddings (2nd)',2020-11-03T22:28:26Z,2020-11-24T13:40:54Z,model card,,
8275,b'[CIs] Better reports everywhere',2020-11-03T21:17:03Z,2020-11-03T21:57:13Z,,,
8274,b'Data collator for token classification',2020-11-03T21:10:07Z,2020-11-03T21:33:27Z,,,
8273,"b""add evaluate doc - trainer.evaluate returns 'epoch' from training""",2020-11-03T20:01:04Z,2020-11-09T14:01:00Z,,,
8272,b'Saving and reloading DistilBertForTokenClassification fine-tuned model',2020-11-03T19:41:50Z,2021-04-24T15:03:34Z,,,
8271,b'Low accuracy after load custom pretrained model in a text binary classification problem',2020-11-03T19:34:32Z,2020-11-04T17:42:31Z,,,
8270,b'improve documentation of training_args.py',2020-11-03T18:56:16Z,2020-11-03T20:57:18Z,,,
8269,b'[wip/s2s/pl] attempt to sync metrics in DDP',2020-11-03T18:40:25Z,2021-04-24T15:03:35Z,,RuntimeError,"RuntimeError: Tensors must be CUDA and dense"
8268,b'Clean Trainer tests and datasets dep',2020-11-03T17:28:36Z,2020-11-03T20:50:56Z,,,
8267,b'[Seq2Seq] Make Seq2SeqArguments an independent file',2020-11-03T17:27:11Z,2020-11-03T20:13:34Z,,,
8266,b'german medbert model details',2020-11-03T16:56:58Z,2020-11-06T08:21:14Z,model card,,
8265,b'Is there a pre-trained BERT model with the sequence length of 2048?',2020-11-03T16:52:34Z,2021-01-10T13:54:11Z,wontfix,,
8264,b'New TensorFlow trainer version',2020-11-03T16:43:13Z,2021-04-24T15:03:36Z,,,
8263,b'GPT2 is not jit-traceable',2020-11-03T16:29:36Z,2020-11-04T14:13:21Z,,,
8262,b'[distributed testing] forward the worker stderr to the parent process',2020-11-03T16:24:18Z,2020-11-03T17:04:54Z,,,
8261,b'Encoder Decoder Model',2020-11-03T15:36:27Z,2020-11-04T07:54:14Z,,,
8260,b'[fix] Skip tatoeba tests if Tatoeba-Challenge not cloned',2020-11-03T14:48:26Z,2020-11-03T14:49:30Z,,,
8259,b'Disable default sigmoid function for single label classification Inference API',2020-11-03T13:31:55Z,2021-01-17T21:35:21Z,wontfix,,
8258,b'Create README.md',2020-11-03T13:16:49Z,2020-11-06T08:19:13Z,model card,,
8257,b'Create README.md',2020-11-03T13:13:23Z,2020-11-03T13:13:50Z,,,
8256,b'[FIX] TextGenerationPipeline is currently broken.',2020-11-03T12:17:42Z,2020-11-03T15:10:23Z,,,
8255,b'Create README.md',2020-11-03T12:11:01Z,2020-11-06T08:34:24Z,model card,,
8254,b'[Seq2Seq] Correct import in Seq2Seq Trainer',2020-11-03T10:59:23Z,2020-11-03T12:56:42Z,,,
8253,"b'when the txt file has 5GB, a Killed prompt appears.'",2020-11-03T09:53:03Z,2021-01-10T13:54:10Z,wontfix,,
8252,b'Updated Reformer to use caching during generation',2020-11-03T09:30:00Z,2020-11-03T11:40:09Z,,,
8251,b'Train BERT with CLI commands',2020-11-03T09:03:48Z,2021-01-10T13:54:09Z,wontfix,,
8250,b'tokenizer.vocab key and values is change begin line 261?',2020-11-03T07:32:06Z,2021-01-10T13:54:03Z,wontfix,,
8249,b'[ray] Support `n_jobs` for Ray hyperparameter search on CPUs',2020-11-02T21:07:00Z,2021-04-24T15:03:36Z,,,
8248,b'Model card: GPT-2 fine-tuned on CommonGen',2020-11-02T19:56:54Z,2020-11-06T08:15:12Z,model card,,
8247,b'Model card: CodeBERT fine-tuned for Insecure Code Detection',2020-11-02T19:31:52Z,2020-11-06T08:13:45Z,model card,,
8246,b'[Notebooks] Add new encoder-decoder notebooks',2020-11-02T19:20:33Z,2020-11-02T19:21:56Z,,,
8245,b'Add XLMProphetNetTokenizer to tokenization auto',2020-11-02T18:57:05Z,2020-11-02T19:10:10Z,,,
8244,b'_shift_right when to use',2020-11-02T18:53:22Z,2021-01-10T13:54:06Z,wontfix,,
8243,b'[EncoderDecoder] fix encoder decoder config model type bug',2020-11-02T18:36:26Z,2020-11-02T19:12:34Z,,,
8242,b'Error converting tensorflow checkpoints',2020-11-02T17:46:42Z,2021-04-24T15:03:37Z,wontfix,torch.nn.modules.module.ModuleAttributeError,torch.nn.modules.module.ModuleAttributeError: 'BertForPreTraining' object has no attribute 'bias'
8241,b'Update model cards of deepset/roberta-base-squad2 v1 and v2',2020-11-02T17:39:56Z,2020-11-04T16:21:26Z,model card,,
8240,b'Add line by line option to mlm/plm scripts',2020-11-02T16:53:43Z,2020-11-02T17:27:04Z,,,
8239,b'Fix TensorBoardCallback for older versions of PyTorch',2020-11-02T15:36:15Z,2020-11-02T15:43:29Z,,,
8238,b'Patch reports',2020-11-02T15:26:17Z,2020-11-02T15:26:26Z,,,
8237,b'Fix bad import with PyTorch <= 1.4.1',2020-11-02T15:13:21Z,2020-11-02T15:26:38Z,,,
8236,b'Weird Behavior in Finetuning Pegasus on a Custom Dataset/Longer Summaries Generated',2020-11-02T14:53:31Z,2020-11-09T18:48:38Z,,,
8235,b'doc: fix typo',2020-11-02T13:16:09Z,2020-11-02T13:53:18Z,,,
8234,"b'filelock hangs for example script ""run_language_modeling.py""'",2020-11-02T12:57:40Z,2020-11-02T19:36:58Z,,,
8233,b'Contributing trained Greek<->English NMT models implemented with fairseq',2020-11-02T11:41:55Z,2020-11-08T09:27:17Z,New model,,
8232,"b""ImportError: cannot import name 'SAVE_STATE_WARNING' from 'torch.optim.lr_scheduler'""",2020-11-02T11:20:56Z,2020-12-08T05:59:56Z,,ImportError,"ImportError: cannot import name 'SAVE_STATE_WARNING' from 'torch.optim.lr_scheduler' (/home/yuchenlin/anaconda3/envs/mcqa/lib/python3.7/site-packages/torch/optim/lr_scheduler.py)"
8231,b'Tf longformer for sequence classification',2020-11-02T10:10:51Z,2020-11-19T15:37:28Z,,,
8230,b'Fixed emmental example.',2020-11-02T10:00:14Z,2021-04-24T15:03:38Z,,,
8229,b'is it possible to extract the attention weights on test inputs when the pretrained model is fine-tuned on custom data?',2020-11-02T06:52:50Z,2021-01-10T13:54:19Z,wontfix,,
8228,b' segmentation fault (core dumped)  proxychains4 python xxx.py',2020-11-02T04:06:21Z,2021-01-10T13:54:18Z,wontfix,,
8227,b'convert_graph_to_onnx.py and associated example notebook are broken for TensorFlow',2020-11-02T02:05:24Z,2021-01-10T13:54:21Z,wontfix,,
8226,b'[bart] 2 SinusoidalPositionalEmbedding fixes',2020-11-02T01:50:59Z,2020-11-02T23:50:27Z,,,
8225,b'When would pegasus be able to be exported in ONNX format?',2020-11-01T22:37:34Z,2021-04-24T15:03:39Z,,,
8224,b'Add encoder-decoder word embeddings tying by default',2020-11-01T20:25:21Z,2021-04-26T15:02:48Z,,,
8223,b'Create README.md',2020-11-01T17:52:48Z,2020-11-05T08:03:20Z,model card,,
8222,b'Why is the accuracy rate of pre-trained GPT-2 model only ~26%?',2020-11-01T16:19:12Z,2020-11-02T15:56:46Z,,,
8221,b'[GPT2] Loss NaN after some time with FP16',2020-11-01T14:54:58Z,2021-01-10T13:54:24Z,wontfix,RuntimeError,"RuntimeError: Function 'SoftmaxBackward' returned nan values in its 0th output."
8220,b'Example for running T5 for translation ',2020-11-01T12:23:28Z,2021-01-11T11:58:38Z,wontfix,,
8219,b'Roberta weights are not initialized loading the bare Roberta',2020-11-01T11:16:37Z,2021-01-11T11:58:35Z,wontfix,,
8218,b'ValueError: decoder_start_token_id or bos_token_id has to be defined for encoder-decoder generation',2020-11-01T10:27:46Z,2020-11-01T10:51:42Z,,ValueError,"ValueError: decoder_start_token_id or bos_token_id has to be defined for encoder-decoder generation"
8216,"b""tokenizer's is_split_into_words seems not work""",2020-11-01T09:52:46Z,2020-11-01T09:17:09Z,New model,,
8215,"b""Setting os.environ['CUDA_VISIBLE_DEVICES'] = \xe2\x80\x981\xe2\x80\x99, but always training on GPU0, how to set it(GPT2)?""",2020-11-01T09:40:11Z,2021-01-11T11:58:35Z,wontfix,,
8217,"b'tokenizer ""is_split_into_words"" seems not work'",2020-11-01T09:19:09Z,2020-12-02T00:47:07Z,,,
8214,b'[Benchmark]',2020-11-01T05:06:56Z,2020-11-02T15:48:29Z,,,
8213,b'Fix ignore files behavior in doctests',2020-11-01T00:29:21Z,2020-11-02T13:47:37Z,,,
8212,b'Pickle error ',2020-10-31T20:46:51Z,2020-11-02T21:11:29Z,,_pickle.PicklingError,"_pickle.PicklingError: Can't pickle typing.Union[str, NoneType]: it's not the same object as typing.Union"
8211,b'Appropriate dataset format for language modeling example',2020-10-31T19:15:40Z,2020-11-04T19:37:07Z,,,
8210,b'Simple import issue for run_clm.py',2020-10-31T18:35:54Z,2020-10-31T18:46:58Z,,ImportError,"ImportError: cannot import name 'is_main_process'"
8209,b'XLMRobertaTokenizer potential bug ',2020-10-31T13:42:10Z,2021-04-26T15:02:49Z,,,
8208,b'Poor f1 score when validating existing models',2020-10-31T11:46:29Z,2021-01-11T11:58:30Z,wontfix,"NoAns_exact', NoAns_f1', NoAns_total'","NoAns_exact': 83.16232127838519NoAns_f1': 83.16232127838519NoAns_total': 5945"
8207,b'Updated ConversationalPipeline to work with encoder-decoder models',2020-10-31T10:22:04Z,2020-11-03T15:33:02Z,,,
8206,"b'Sentence transformer Segmentation Fault - Pytorch 1.4.0, 2.80'",2020-10-31T10:17:31Z,2021-01-11T11:58:39Z,wontfix,,
8205,b'[Bug fix] Fixed value for BlenderBot pad token',2020-10-31T09:27:43Z,2020-11-01T15:21:58Z,,,
8204,b'[Benchmark]',2020-10-31T04:04:40Z,2020-10-31T10:56:33Z,,,
8203,b'Add TFDPR',2020-10-31T03:09:32Z,2020-11-11T17:28:10Z,,,
8202,"b""'SummaryWriter' object has no attribute 'add_hparams'""",2020-10-31T01:04:52Z,2020-11-02T15:43:29Z,,AttributeError,"AttributeError: 'SummaryWriter' object has no attribute 'add_hparams'"
8201,b'New model addition',2020-10-31T00:41:40Z,2020-10-31T10:55:45Z,New model,,
8200,b'Mmmmianam',2020-10-31T00:12:38Z,2020-10-31T10:56:47Z,,,
8199,b'Sentencepiece dependency causing docker build to fail',2020-10-30T20:50:50Z,2021-01-17T21:35:03Z,wontfix,,
8198,b'Added 12 model cards for Indian Language Models',2020-10-30T20:12:11Z,2020-11-02T05:17:44Z,model card,,
8197,b'Remove deprecated arguments from new run_clm',2020-10-30T19:23:50Z,2020-10-30T19:27:20Z,,,
8196,b'pytest Errors',2020-10-30T18:22:25Z,2020-11-02T19:10:10Z,,NameError,"NameError: name 'XLMProphetNetTokenizer' is not defined"
8195,b'Attempt at a temporary fix on `model_max_length` for roberta and Camembert variants',2020-10-30T18:11:39Z,2021-09-15T08:56:27Z,WIP,,
8194,b'[Seq2SeqTrainer] Move import to init to make file self-contained',2020-10-30T18:06:54Z,2020-11-01T22:31:55Z,,,
8193,b'Fix two bugs with --logging_first_step',2020-10-30T17:35:31Z,2020-10-30T20:45:39Z,,,
8192,b'Add model cards.',2020-10-30T16:29:42Z,2020-11-02T05:19:39Z,model card,,
8191,b'Patch 3',2020-10-30T16:20:34Z,2020-10-30T16:21:34Z,,,
8190,b'TextDataset support for tensorflow?',2020-10-30T15:58:14Z,2021-01-01T01:29:47Z,wontfix,,
8189,b'Doc fixes and filter warning in wandb',2020-10-30T15:43:00Z,2020-10-30T16:37:34Z,,,
8188,b'Finalize lm examples',2020-10-30T15:25:46Z,2020-10-30T18:20:19Z,,,
8187,b'Configuration initialized from checkpoint does not keep the checkpoint identifier in its attributes',2020-10-30T15:16:55Z,2021-01-10T13:54:23Z,wontfix,,
8186,b'T5 (probably BART) issues with the `tf.saved_model.save` API and the `output_xxx` configuration attributes.',2020-10-30T15:12:28Z,2021-01-11T11:58:21Z,wontfix,,
8185,b'TensorFlow Longformer model as a saved model with attention outputs',2020-10-30T15:09:26Z,2021-01-18T07:01:59Z,wontfix,,
8184,"b""trainer.evaluate returns 'epoch' from training""",2020-10-30T14:01:39Z,2020-11-11T18:26:25Z,,,
8183,b'Summarization outputs on T5-small gets truncated ',2020-10-30T12:53:02Z,2021-01-18T07:02:20Z,wontfix,,
8182,b'cannot load pytorch_model.bin / pytorch version ?',2020-10-30T12:35:12Z,2021-01-10T13:54:22Z,wontfix,,
8181,b'Documentation on how to get results out of trainer is missing.',2020-10-30T12:06:02Z,2021-01-11T11:58:24Z,wontfix,,
8180,b'Fix the behaviour of DefaultArgumentHandler (removing it).',2020-10-30T11:23:50Z,2020-11-02T11:33:51Z,,,
8179,b'`do_predict` option of `TrainingArguments` - but no way to pass test set.',2020-10-30T11:18:28Z,2020-11-11T18:26:59Z,,,
8178,b'Minor style improvements for the Flax BERT and RoBERTa examples',2020-10-30T10:22:05Z,2020-10-30T20:25:40Z,,,
8177,b'AutoTokenizer.from_pretrained function cannot be customized ',2020-10-30T09:37:17Z,2021-01-10T13:54:04Z,wontfix,,
8176,b'Fixing some warnings in DeBerta',2020-10-30T09:20:56Z,2020-10-30T13:15:42Z,,,
8175,b'Onnx converted model output shape not matching with the finetuned model (BUG)',2020-10-30T09:02:07Z,2020-10-30T09:10:34Z,,,
8174,"b'Possible bug in ""trainer"" when training ""BertForPretraining.from_pretrained()""'",2020-10-30T08:15:39Z,2020-10-31T08:01:54Z,,RuntimeError,"RuntimeError: grad can be implicitly created only for scalar outputs"
8173,b'raining loss is not decreasing when using the Roberta pre-trained model from the transformers library',2020-10-30T07:27:05Z,2020-10-30T15:41:55Z,,,
8172,b'Create Speedtest.py',2020-10-30T04:21:05Z,2020-11-02T10:26:56Z,,,
8171,b'Need suggestion on contributing TFDPR',2020-10-30T03:48:35Z,2020-11-11T21:17:25Z,New model,,
8170,b'Create README.md',2020-10-30T03:18:36Z,2020-11-06T08:25:53Z,model card,,
8169,b'Create README.md',2020-10-30T02:54:30Z,2020-11-06T08:26:08Z,model card,,
8168,b'Create README.md',2020-10-30T02:50:24Z,2020-11-06T08:25:34Z,model card,,
8167,b'Create README.md',2020-10-30T02:19:19Z,2020-11-06T08:24:32Z,model card,,
8166,b'Replace swish with silu',2020-10-30T02:13:26Z,2020-10-30T19:09:11Z,,,
8165,b'Fix typo: s/languaged/language/',2020-10-30T02:11:32Z,2020-10-30T15:22:04Z,,,
8164,b'[s2s] Option to aggregate rouge deterministically',2020-10-29T23:05:21Z,2021-03-06T00:16:42Z,wontfix,,
8163,b'[CI] Better reports #2',2020-10-29T22:32:15Z,2020-10-29T23:30:05Z,,,
8162,b'Fix typo: s/Chinees/Chinese/',2020-10-29T21:37:07Z,2020-10-29T21:39:02Z,,,
8161,b'generate() always starts with bos_token_id',2020-10-29T21:35:47Z,2020-10-30T10:47:57Z,,,
8160,"b'ConnectionError: (\'Connection aborted.\', OSError(""(32, \'EPIPE\')""))'",2020-10-29T21:18:37Z,2020-11-10T12:11:03Z,,"OpenSSL.SSL.SysCallError, OSError, urllib3.exceptions.ProtocolError, requests.exceptions.ConnectionError","OpenSSL.SSL.SysCallError: (32, 'EPIPE')OSError: (32, 'EPIPE')urllib3.exceptions.ProtocolError: ('Connection aborted.', OSError(""(32, 'EPIPE')""))requests.exceptions.ConnectionError: ('Connection aborted.', OSError(""(32, 'EPIPE')""))"
8159,b'Fix typo: indinces -> indices',2020-10-29T20:21:03Z,2020-10-29T21:04:20Z,,,
8158,b'EncoderDecoderModel: tie weights between different classes of models',2020-10-29T18:41:46Z,2021-01-11T11:58:20Z,wontfix,,
8157,b'[testing] distributed: correct subprocess output checking',2020-10-29T18:00:23Z,2020-10-29T18:05:25Z,,,
8156,b'BertTokenizer loses unicode character',2020-10-29T17:47:17Z,2021-03-06T00:16:43Z,wontfix,,
8155,b'ONNX T5 with Beam Search',2020-10-29T16:59:11Z,2021-01-10T13:54:18Z,wontfix,,
8154,b'[s2s] Trainer vs PTL timings',2020-10-29T16:41:53Z,2021-01-18T07:01:57Z,wontfix,,
8153,b'Add a template for examples and apply it for mlm and plm examples',2020-10-29T15:18:44Z,2020-10-29T17:38:12Z,,,
8152,b'Document tokenizer_class in configurations',2020-10-29T14:13:57Z,2020-10-29T14:43:46Z,,,
8151,b'Smarter prediction loop and no- -> no_ in console args',2020-10-29T14:07:49Z,2020-10-29T14:56:25Z,,,
8150,b'[s2s] distillBART docs for paper replication',2020-10-29T13:37:50Z,2020-10-29T16:01:16Z,,,
8149,b'Model card: Update widget examples.',2020-10-29T12:47:06Z,2020-10-29T12:49:17Z,,,
8148,b'Masking in Pooling Layer from BERT Output?',2020-10-29T12:05:08Z,2021-01-11T11:58:17Z,wontfix,,
8147,b'[Model cards] Seq2Seq tags',2020-10-29T11:45:34Z,2020-10-29T11:45:56Z,,,
8146,b'Make tokenizer.pad() also pad `labels`',2020-10-29T10:47:41Z,2020-11-04T23:43:09Z,,,
8145,b'TransformerXL: StopIteration: Caught StopIteration in replica 0 on device 0',2020-10-29T09:32:06Z,2020-11-12T12:02:36Z,,,
8144,b'ETA on TFEncoderDecoderModel and is BERTShare from https://arxiv.org/pdf/1907.12461.pdf planned?',2020-10-29T08:27:51Z,2021-01-18T07:02:07Z,wontfix,,
8143,b'Trainer makes RAM go out of memory after a while',2020-10-29T07:44:21Z,2020-10-29T15:31:03Z,,,
8142,b'Is there any Jupyter notebook or detailed example using BertGeneration or EncoderDecoderModel classes?',2020-10-29T07:01:44Z,2020-11-10T04:35:59Z,,,
8141,b'Vocab files missing in community pre-trained t5 model',2020-10-29T06:16:28Z,2020-10-29T16:16:35Z,,>OSError,">OSError: Model name 'sshleifer/t5-base-cnn' was not found in tokenizers model name list (t5-small, t5-base, t5-large"
8140,"b""Customize tokenizer in model card's widget""",2020-10-29T03:01:07Z,2020-10-29T13:44:37Z,,,
8139,b'Fix doc errors and typos across the board',2020-10-29T02:09:56Z,2020-10-29T14:33:33Z,model card,,
8138,b'How to get translation of one batch of sentences after batch_encode_plus? ',2020-10-29T01:43:52Z,2020-11-01T15:35:36Z,,,
8137,"b'In built code not able to download for ""bert-base-uncased"" when running on cluster.  '",2020-10-29T01:06:01Z,2021-01-11T11:58:26Z,wontfix,OSError,"OSError: Can't load config for 'bert-base-uncased'. Make sure that:"
8136,b'How to perform model.predict loop with TFRobertaForSequenceClassification?',2020-10-28T23:54:04Z,2021-01-11T11:58:19Z,wontfix,,
8135,"b""Bort (Amazon's reduced BERT)""",2020-10-28T21:21:12Z,2021-09-17T12:45:14Z,New model,,
8134,b'Error with multi-gpu training ',2020-10-28T20:45:29Z,2021-01-10T13:54:29Z,wontfix,RuntimeError,"RuntimeError: grad can be implicitly created only for scalar outputs"
8133,b'[examples] minimal version requirement run-time check in PL',2020-10-28T20:15:26Z,2020-11-03T18:17:12Z,,,
8132,b'New template for example and MLM example.',2020-10-28T19:55:51Z,2020-10-29T15:17:04Z,model card,,
8131,b'[s2s test] cleanup',2020-10-28T19:33:13Z,2020-10-28T20:50:36Z,,,
8130,b'Name or path should be added on configuration as well',2020-10-28T19:22:08Z,2020-10-28T19:45:01Z,,,
8129,b'Fix typo in `AutoModelForMaskedLM` docs',2020-10-28T19:21:59Z,2020-10-28T19:52:29Z,,,
8128,b'test style',2020-10-28T18:47:10Z,2020-10-28T19:08:54Z,,,
8127,b'Use pipeline on fine tuned model',2020-10-28T18:35:40Z,2021-01-10T13:54:28Z,wontfix,,
8126,b'Update CI cache',2020-10-28T17:59:19Z,2020-10-28T17:59:44Z,,,
8125,b'Cannot load saved tokenizer using AutoTokenizer',2020-10-28T17:58:55Z,2021-01-11T11:58:25Z,wontfix,,
8124,b'[s2s] distributed eval gets stuck on error w/ multigpu',2020-10-28T16:37:57Z,2021-01-17T21:35:15Z,wontfix,ValueError,"ValueError: Output directory (/tmp/tmpqajqhzwo) already exists and has 7 items in it (expected 3 items). Use --overwrite_output_dir to overcome."
8123,b'[DOC] Improve pipeline() docstrings for config and tokenizer',2020-10-28T16:31:31Z,2020-10-28T17:26:13Z,,,
8122,b'behaviour of ZeroShotClassification using facebook/bart-large-mnli is different on online demo vs local machine',2020-10-28T16:25:39Z,2020-10-28T16:53:05Z,,,
8121,b'fix(trainer_callback]: typo',2020-10-28T16:08:48Z,2020-10-28T16:15:31Z,,,
8120,b'Rename add_start_docstrings_to_callable',2020-10-28T14:59:10Z,2020-10-28T17:42:32Z,,,
8119,b'feat(wandb): save model as artifact',2020-10-28T14:44:43Z,2021-01-05T08:30:47Z,,,
8118,b'Document the various LM Auto models',2020-10-28T14:41:36Z,2020-10-28T17:41:57Z,,,
8117,b'fast tokenizer issue on most user uploaded models',2020-10-28T11:53:23Z,2021-01-11T11:58:22Z,wontfix,,
8116,b'Add labels padding in tokenization_utils_base.py',2020-10-28T10:58:11Z,2020-11-04T14:19:18Z,,,
8115,b'Fix eval ref miss in Chinese WWM.',2020-10-28T10:13:48Z,2020-10-29T21:08:39Z,,,
8114,b'Pegasus: Error when training with increased input length',2020-10-28T09:22:19Z,2020-10-28T20:48:35Z,,TypeError,"TypeError: forward() missing 1 required positional argument: 'input_ids'"
8113,b'[WIP] Add Tapas model',2020-10-28T08:38:21Z,2020-12-15T08:11:22Z,,,
8112,b'Documentation code snippet has extra ) after model code ',2020-10-28T06:14:59Z,2020-10-28T13:38:15Z,,,
8111,b'[Model] mT5 Cross-Lingual Model',2020-10-28T05:50:38Z,2021-01-17T21:35:04Z,"wontfix, New model",,
8110,b'[gh actions] run artifacts job always',2020-10-28T05:39:14Z,2020-10-28T05:45:19Z,,,
8109,b'T5Tokenizer: decode does not show special tokens',2020-10-28T04:16:06Z,2020-11-10T21:24:18Z,,,
8108,b' Support various BERT relative position embeddings',2020-10-27T22:26:48Z,2020-11-03T22:41:20Z,,,
8107,b'[testing] port test_trainer_distributed to distributed pytest + TestCasePlus enhancements',2020-10-27T21:53:53Z,2020-10-28T15:51:33Z,,,
8106,b'Move installation instructions to the top',2020-10-27T21:31:36Z,2020-10-27T21:32:21Z,,,
8105,b'New run_clm script',2020-10-27T21:14:50Z,2020-10-28T14:38:59Z,,,
8104,b'RagSequenceForGeneration how to get document texts retrieved  in response to a query',2020-10-27T21:02:37Z,2020-10-27T23:23:48Z,,,
8103,"b""run_language_modeling crashes with import cannot import name 'DataCollatorForWholeWordMask' from 'transformers'""",2020-10-27T20:17:34Z,2020-10-28T01:24:29Z,,ImportError,"ImportError: cannot import name 'DataCollatorForWholeWordMask' from 'transformers' (/home/spacemanidol/miniconda3/envs/prunetransformer/lib/python3.8/site-packages/transformers/__init__.py)"
8102,b'Adjust setup so that all extras run on Windows',2020-10-27T18:37:29Z,2020-10-27T18:39:49Z,,,
8101,b'should be BartConfig.prefix  None?',2020-10-27T17:08:23Z,2021-01-10T13:54:14Z,wontfix,,
8100,b'Rename swish to silu to give appropriate credit',2020-10-27T16:46:34Z,2020-10-30T19:09:11Z,,,
8099,b'Reformer implementation in Tensorflow',2020-10-27T15:40:43Z,2021-01-10T13:54:25Z,wontfix,,
8098,b'RuntimeError: Trying to create tensor with negative dimension',2020-10-27T15:21:13Z,2020-10-28T13:50:41Z,,RuntimeError,"RuntimeError: Trying to create tensor with negative dimension -199744: [-199744, 16]"
8097,b'[wip/s2s] Aggregate Rouge Deterministically',2020-10-27T14:59:31Z,2020-10-29T23:05:45Z,,,
8096,b'Create README.md',2020-10-27T14:32:35Z,2020-12-11T14:45:13Z,model card,,
8095,b'Fix IterableDataset with __len__ in Trainer',2020-10-27T13:36:40Z,2020-10-27T13:52:36Z,,,
8094,b'Documentation error in question-answering pipeline',2020-10-27T13:09:13Z,2021-01-10T13:54:26Z,wontfix,,
8093,b'Fully remove codecov',2020-10-27T12:34:14Z,2020-10-27T18:14:13Z,,,
8092,b'Fix DeBERTa docs',2020-10-27T12:10:34Z,2020-10-27T13:07:42Z,,,
8091,b'Fix assertion error message for MLflowCallback',2020-10-27T12:03:26Z,2020-10-27T14:34:52Z,,,
8090,b'Update README.md',2020-10-27T11:13:02Z,2020-10-29T12:31:33Z,model card,,
8089,b'Create README.md',2020-10-27T11:11:35Z,2020-10-29T12:23:43Z,model card,,
8088,b'Create README.md',2020-10-27T11:10:07Z,2020-10-29T12:23:30Z,model card,,
8087,b'#7858 breaks IterableDataset with __len__ in Trainer',2020-10-27T10:43:20Z,2020-10-27T13:52:36Z,,,
8086,b'Hello world example fail with transformers-3.4',2020-10-27T10:15:13Z,2020-10-27T13:59:58Z,,UnicodeDecodeError,"UnicodeDecodeError: 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte"
8085,b'Merge pull request #1 from huggingface/master',2020-10-27T08:45:29Z,2020-10-27T13:12:01Z,,,
8084,b'Fix tf export path type in notebooks/04-onnx-export.ipynb',2020-10-27T06:36:14Z,2021-03-06T00:16:45Z,wontfix,,
8083,b'FastFormers into transformers',2020-10-27T04:51:44Z,2021-09-22T21:35:58Z,"wontfix, New model",,
8082,b'Fix doc examples',2020-10-27T03:20:10Z,2020-10-27T11:29:26Z,,,
8081,b'Move style_doc to extra_quality_checks',2020-10-27T03:06:33Z,2020-10-27T13:42:07Z,,,
8080,b'Pre style',2020-10-27T03:02:23Z,2020-10-27T03:06:36Z,,,
8079,b'Best practice to use this great repo for industry application.',2020-10-27T02:59:29Z,2020-10-27T09:44:50Z,,,
8078,b'Hope more GPT Chinese pretrained model.',2020-10-27T02:55:21Z,2020-10-27T08:14:06Z,,,
8077,b'Longformer crashes for position embeddings indexing?',2020-10-27T02:28:14Z,2021-01-11T11:58:23Z,wontfix,"RuntimeError, subprocess.CalledProcessError","RuntimeError: CUDA error: device-side assert triggeredsubprocess.CalledProcessError: Command '['/home/user/miniconda3/envs/marco/bin/python', '-u', 'finetune-marco.py', '--local_rank=0', '--model_type', 'longformer', '--model_path', 'allenai/longformer-base-4096', '--batch_size', '1', '--finetune_embedding', '0', '--cased', '1', '--num_neg', '1', '--eval_step', '1', '--num_epochs', '20', '--apex_level', 'O2', '--encoder_lr', '1e-5', '--projector_lr', '1e-5', '--num_ft_encoders', '2', '--seed', '611']' died with <Signals.SIGABRT: 6>."
8076,b'[setup] update/add setup targets',2020-10-27T02:08:20Z,2020-10-27T17:54:58Z,,,
8075,b'Create README.md',2020-10-27T01:56:04Z,2020-10-29T12:22:34Z,model card,,
8074,b'Doc styling fixes',2020-10-27T00:21:20Z,2020-10-27T11:54:50Z,,,
8073,b'[breaking|pipelines|tokenizers] Adding slow-fast tokenizers equivalence tests pipelines - Removing sentencepiece as a required dependency',2020-10-26T22:39:15Z,2020-11-15T21:50:59Z,,,
8072,b'`BartForConditionalGeneration.from_pretrained` suddenly fails',2020-10-26T22:35:23Z,2020-10-27T14:19:04Z,,TypeError,"TypeError: __init__() got an unexpected keyword argument 'output_past'"
8071,b'[All Seq2Seq model + CLM models that can be used with EncoderDecoder] Add cross-attention weights to outputs',2020-10-26T20:48:26Z,2020-11-06T18:34:49Z,,,
8070,b'Pretraining for encoder of TF T5 model',2020-10-26T20:36:51Z,2021-03-06T00:16:46Z,wontfix,,
8069,b'DEP: pinned sentencepiece to 0.1.91 in setup.py to fix build issues with newer versions',2020-10-26T20:28:20Z,2020-10-27T18:09:32Z,,,
8068,b'seq2seq/finetune.py: remove useless check',2020-10-26T20:13:04Z,2021-01-02T04:25:52Z,wontfix,,
8067,b'Doc styling',2020-10-26T19:31:58Z,2020-10-26T22:26:03Z,,,
8066,b'Missing Import',2020-10-26T18:53:56Z,2020-10-26T19:04:38Z,,ImportError,"ImportError: cannot import name 'DataCollatorForWholeWordMask'"
8065,"b""load 'microsoft/unilm-base-cased' failed """,2020-10-26T18:48:01Z,2021-01-17T21:35:13Z,wontfix,ValueError,"ValueError: Unrecognized model in microsoft/unilm-base-cased. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: retribert, t5, mobilebert, distilbert, albert, bert-generation, camembert, xlm-roberta, pegasus, marian, mbart, bart, blenderbot, reformer, longformer, roberta, deberta, flaubert, fsmt, squeezebert, bert, openai-gpt, gpt2, transfo-xl, xlnet, xlm-prophetnet, prophetnet, xlm, ctrl, electra, encoder-decoder, funnel, lxmert, dpr, layoutlm, rag"
8064,b'[QOL] PretrainedConfig.to_diff_dict(other_config)',2020-10-26T18:48:00Z,2020-10-26T19:15:30Z,,,
8063,b'Fix TF training arguments instantiation',2020-10-26T18:38:06Z,2020-10-26T18:39:26Z,,,
8062,b'Add AzureML in integrations via dedicated callback ',2020-10-26T18:34:20Z,2020-10-27T18:21:55Z,,,
8061,b'Doc fixes in preparation for the docstyle PR',2020-10-26T18:16:47Z,2020-10-26T19:01:09Z,,,
8060,b'a multitude of deprecations for pytorch-1.7+',2020-10-26T17:27:46Z,2021-03-06T00:16:48Z,wontfix,,
8059,b'infer entailment label id on zero shot pipeline',2020-10-26T17:24:00Z,2020-10-27T18:09:56Z,,,
8058,b'[testing] port test_trainer_distributed to run with pytest',2020-10-26T17:18:49Z,2020-10-28T01:41:50Z,,,
8057,b'[testing] fixing crash in deberta',2020-10-26T16:53:30Z,2020-10-26T17:19:11Z,,RuntimeError,"RuntimeError: diff_view_meta->output_nr_ == 0 INTERNAL ASSERT FAILED at ""/opt/conda/conda- \ "
8056,b'[TF] from_pt should respect authorized_unexpected_keys',2020-10-26T16:44:52Z,2020-10-26T17:53:28Z,,,
8055,"b""BertEncoder has no attribute 'bias' when convert tf checkpoint""",2020-10-26T16:37:17Z,2021-01-02T04:25:53Z,wontfix,ModuleAttributeError,"ModuleAttributeError: 'BertEncoder' object has no attribute 'bias'"
8054,b'Add m2m 100 multilingual translation model from FAIR',2020-10-26T16:17:38Z,2021-03-06T16:44:16Z,New model,,
8053,"b""Minor error fix of 'bart-large-cnn' details in the pretrained_models doc""",2020-10-26T14:54:37Z,2020-10-26T15:05:17Z,,,
8052,b'Fix a bug for `CallbackHandler.callback_list`',2020-10-26T14:47:10Z,2020-10-27T14:37:05Z,,TypeError,"TypeError: sequence item 0: expected str instance, DefaultFlowCallback found"
8051,b'minor model card description updates',2020-10-26T14:03:33Z,2020-10-26T14:04:21Z,model card,,
8050,b'invalid argument wwm passed to the run_language_modeling.py file',2020-10-26T13:40:40Z,2020-10-26T16:00:19Z,,,
8049,b'Fix + Test',2020-10-26T13:21:17Z,2020-10-26T16:32:28Z,,,
8048,b'Fix label name in DataCollatorForNextSentencePrediction test',2020-10-26T13:14:13Z,2020-10-26T13:23:13Z,,,
8047,b'[T5] Unused `n_positions` and `max_position_embeddings`.',2020-10-26T12:20:57Z,2020-11-13T15:57:32Z,,,
8046,b'Minor typo fixes to the preprocessing tutorial in the docs',2020-10-26T10:59:53Z,2020-10-26T14:22:30Z,,,
8045,b'Minor typo fixes to the tokenizer summary in the docs',2020-10-26T10:59:21Z,2020-10-26T12:08:34Z,,,
8044,"b""Automatically cuts input if the pipeline position_ids can't handle it.""",2020-10-26T10:54:40Z,2020-10-27T17:48:43Z,,,
8043,b'[Seq2Seq Trainer] Make sure padding is implemented for models without pad_token',2020-10-26T10:48:03Z,2020-10-26T16:28:16Z,,,
8042,b'Tentative improvement on sequence_length error for position_ids',2020-10-26T10:16:31Z,2020-10-26T10:57:12Z,,,
8041,b'Create model cards for guwenbert',2020-10-26T09:32:21Z,2020-10-29T12:21:54Z,model card,,
8040,b'Converting Transformers model to Tensorflow model',2020-10-26T09:00:23Z,2020-11-03T07:01:47Z,,,
8039,"b'Why the functions ""add_special_tokens()"" and ""resize_token_embeddings()"" hurt the performance of \'gpt2\' and \'gpt2-medium\' but not \'gpt2-large\' and \'gpt2-xl\' ?'",2020-10-26T06:36:28Z,2021-01-02T04:26:02Z,wontfix,,
8038,b'Model Card for Gujarati-XLM-R-Base',2020-10-26T00:42:39Z,2020-10-29T12:21:12Z,model card,,
8037,b'RAG: Do we need to pretrained the doc-encoder when using a custom dataset?',2020-10-25T23:09:07Z,2021-01-10T13:54:32Z,wontfix,,
8036,b'Add mixed precision evaluation',2020-10-25T18:55:54Z,2020-10-26T12:12:32Z,,,
8035,b'ModelUtilsTest.test_model_from_pretrained failiing on CUDA',2020-10-25T17:32:06Z,2021-01-02T04:25:57Z,wontfix,,
8034,b'DataCollatorIntegrationTest::test_nsp failing on GPU',2020-10-25T17:30:37Z,2020-10-26T13:23:13Z,,,
8033,"b'[cleanup] pegasus,marian,mbart pytorch tests'",2020-10-25T17:04:15Z,2020-10-26T12:59:06Z,"Tests, cleanup",,
8032,b'Commit 121dd43 changes DialoGPT generation behavior',2020-10-25T16:23:54Z,2021-01-10T13:54:16Z,wontfix,,
8031,b'[fix] FSMT slow test uses lists instead of torch tensors',2020-10-25T15:53:07Z,2020-10-26T12:32:37Z,,,
8030,b'Getting Hosted inference API working?',2020-10-25T13:03:40Z,2020-10-26T08:32:16Z,model card,,
8029,b'BlenderbotSmallTokenizer throws tuple index out of range error for stopword',2020-10-25T11:26:23Z,2020-10-26T16:32:28Z,,,
8028,b'[BUG] Unexpected overflowing_tokens in tokenizer.encode_plus',2020-10-25T11:13:30Z,2021-01-02T04:25:54Z,wontfix,,
8027,b'(Load dataset failure) ConnectionError: Couldn\xe2\x80\x99t reach https://raw.githubusercontent.com/huggingface/datasets/1.1.2/datasets/cnn_dailymail/cnn_dailymail.py',2020-10-25T10:59:33Z,2020-10-25T15:33:34Z,,ConnectionError,"ConnectionError: Couldnt reach https://raw.githubusercontent.com/huggingface/datasets/1.1.2/datasets/cnn_dailymail/cnn_dailymail.py"
8026,b'[Model Card] new cross lingual sentence model for German and English',2020-10-25T07:37:39Z,2020-10-26T18:48:26Z,model card,,
8025,b'Create README.md',2020-10-24T23:44:05Z,2020-10-25T07:20:47Z,model card,,
8024,b'T5 on multiple tasks',2020-10-24T22:22:31Z,2021-01-03T13:23:26Z,wontfix,,
8023,b'Tiny TF Bart fixes',2020-10-24T16:56:29Z,2020-10-26T13:29:56Z,,,
8022,b'[test] tests/test_modeling_deberta.py breaks on pytorch-nightly',2020-10-24T16:21:26Z,2020-10-26T17:19:11Z,,RuntimeError,"RuntimeError: diff_view_meta->output_nr_ == 0 INTERNAL ASSERT FAILED at ""/opt/conda/conda- \ "
8021,b'[bart] SinusoidalPositionalEmbedding breaks under pytorch-nightly',2020-10-24T16:17:33Z,2020-11-02T23:50:27Z,,,
8020,b'sentencepiece 0.1.94 causing segmentation fault',2020-10-24T15:37:29Z,2021-01-11T11:58:28Z,wontfix,,
8019,"b""Colab can't import trim_batch for T5, anything changed in transformers.tokenization_utils?""",2020-10-24T15:35:31Z,2021-01-03T13:23:25Z,wontfix,ImportError,"ImportError: cannot import name 'trim_batch'"
8018,b'tutorial document',2020-10-24T14:16:10Z,2021-01-02T04:25:57Z,wontfix,,
8017,b'Create README.md',2020-10-24T12:27:58Z,2020-10-29T12:19:48Z,model card,,
8016,b'Mlflow integration callback',2020-10-24T12:18:20Z,2020-10-26T13:41:59Z,,,
8015,b'Create README.md',2020-10-24T12:17:07Z,2020-10-29T12:19:35Z,model card,,
8014,b'weird output shape when fine-tuning TFDistilBertForSequenceClassification',2020-10-24T12:13:16Z,2020-10-24T12:41:55Z,,,
8013,b'[doc prepare_seq2seq_batch] fix docs',2020-10-24T07:39:20Z,2020-10-24T19:33:48Z,,,
8012,b'Add model_cards for DynaBERT',2020-10-24T07:24:07Z,2020-10-29T12:19:18Z,model card,,
8011,"b""AttributeError: module 'tensorflow.python.keras.utils.generic_utils' has no attribute 'populate_dict_with_module_objects'""",2020-10-24T06:53:12Z,2020-10-24T06:53:20Z,,,
8010,b'src->transformers->generation_tf_util.py ->_generate_beam_search->outputs = self(**model_inputs)    why self ?There is not a function?',2020-10-24T02:30:18Z,2021-01-02T04:25:51Z,wontfix,TypeError,TypeError: 'TFGenerationMixin' object is not callable
8009,b'Doc styling',2020-10-23T21:14:11Z,2020-10-29T15:12:36Z,,,
8008,b'TextDataset bug with big files',2020-10-23T20:40:11Z,2021-01-02T04:26:00Z,wontfix,,
8007,b'Ci test tf super slow',2020-10-23T20:12:28Z,2020-10-30T14:25:48Z,,,
8006,b'[tokenizers] Fixing #8001 - Adding tests on tokenizers serialization',2020-10-23T15:49:26Z,2020-10-26T09:27:48Z,,,
8005,b'Differences between facebook/bart-base and facebook/bart-large?',2020-10-23T15:26:39Z,2021-01-18T07:02:12Z,wontfix,,
8004,b'german medbert readme',2020-10-23T13:31:33Z,2020-10-23T15:01:08Z,model card,,
8003,b'Create model card for bert-italian-cased-finetuned-pos',2020-10-23T12:25:11Z,2020-10-23T14:58:06Z,model card,,
8002,b'Create README.md',2020-10-23T10:11:11Z,2020-12-11T14:25:49Z,model card,,
8001,b'do_lower_case not saved/loaded correctly for Tokenizers',2020-10-23T10:03:15Z,2020-10-26T09:27:48Z,,,
8000,b'How to load tokenizer for models without vocab.txt?',2020-10-23T08:44:59Z,2021-01-02T04:25:57Z,wontfix,,
7999,b'Add model cards for DynaBERT',2020-10-23T06:37:01Z,2020-10-23T14:53:54Z,model card,,
7998,b'update version for scipy',2020-10-23T05:06:40Z,2020-10-26T12:56:57Z,,,
7997,b'Create README.md',2020-10-23T03:23:46Z,2020-10-23T14:53:38Z,model card,,
7996,b'Added model cards for Tagalog ELECTRA models',2020-10-23T02:22:08Z,2020-10-23T14:52:22Z,model card,,
7995,b'[CI] generate separate report files as artifacts',2020-10-23T02:16:49Z,2020-10-27T13:25:08Z,,,
7994,"b""BertTokenizer's add_token won't add token""",2020-10-23T00:26:46Z,2020-10-23T00:49:29Z,,,
7993,b'[docs] [testing] distributed training',2020-10-22T21:25:36Z,2020-10-26T12:15:06Z,,,
7992,b'update zero shot default widget example',2020-10-22T21:19:15Z,2020-10-22T21:19:41Z,model card,,
7991,b'[Reformer] remove reformer pad_token_id',2020-10-22T20:59:50Z,2020-10-23T14:29:15Z,,,
7990,b'Handling longformer model_type',2020-10-22T20:18:15Z,2020-10-23T14:34:07Z,,,
7989,b'[gh ci] less output ( --durations=50)',2020-10-22T19:59:22Z,2020-10-22T20:10:16Z,,,
7988,b'[Good first issue] Documentation links in older docs versions',2020-10-22T19:36:34Z,2021-01-05T11:18:49Z,"Documentation, Good First Issue",,
7987,"b'TFMarian, TFMbart, TFPegasus, TFBlenderbot'",2020-10-22T19:26:42Z,2020-10-30T15:23:17Z,,,
7986,b'T5 Decoder Inputs',2020-10-22T19:17:40Z,2020-10-23T16:54:59Z,,,
7985,b'[setup] require torch>=1.4',2020-10-22T18:57:16Z,2021-06-04T06:14:00Z,WIP,,
7984,b'Reload checkpoint',2020-10-22T18:49:48Z,2020-10-22T19:48:53Z,,,
7983,b'add zero shot pipeline tags & examples',2020-10-22T17:59:28Z,2020-10-22T19:01:24Z,model card,,
7982,b'[s2s test] examples/seq2seq/test_finetune_trainer.py::TestFinetuneTrainer::test_finetune_trainer_slow fails on GPU',2020-10-22T17:15:52Z,2020-10-22T21:26:23Z,,,
7981,b'Only log total_flos at the end of training',2020-10-22T15:50:14Z,2020-10-22T18:26:56Z,,,
7980,"b""'DistributedDataParallel' object has no attribute 'save_pretrained'""",2020-10-22T15:21:41Z,2021-01-02T04:26:01Z,wontfix,,
7979,b'How to make some structural changes to the EncoderDecoderModel ?',2020-10-22T13:48:15Z,2020-10-30T07:30:13Z,,,
7978,b'Disable inference API for t5-11b',2020-10-22T12:39:29Z,2020-10-22T13:08:39Z,model card,,
7977,b'GPT2 - Remove else branch adding 0 to the hidden state if token_type_embeds is None.',2020-10-22T12:36:02Z,2020-10-22T14:41:42Z,,,
7976,b'[XLA] Cannot restore from checkpoint on TPU',2020-10-22T12:16:14Z,2020-10-22T19:48:53Z,,RuntimeError,"RuntimeError: don't know how to restore data location of torch.FloatStorage (tagged with xla:0)"
7975,"b'Fixing the ""translation"", ""translation_XX_to_YY"" pipelines.'",2020-10-22T11:38:48Z,2020-10-22T15:16:22Z,,,
7974,"b""TrainingArguments error : TypeError: __init__() got an unexpected keyword argument 'evaluation_strategy'""",2020-10-22T11:31:00Z,2021-01-02T04:26:09Z,wontfix,,
7973,b'support relative path for best_model_checkpoint',2020-10-22T11:11:02Z,2020-10-22T11:55:32Z,,ValueError,"ValueError: './results/use_pretrained_test/checkpoint-1162' is not in list"
7972,"b'Unable to load UnifiedQA models, tf throws DataLossError'",2020-10-22T10:29:59Z,2020-10-23T10:11:17Z,,"RuntimeError, DataLossError","RuntimeError: Unable to open table file /content/base: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator?DataLossError: Unable to open table file /path/to/models/unifiedqa-small: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator?"
7971,b'FillMaskPipeline: support passing top_k on __call__',2020-10-22T10:21:13Z,2020-10-22T16:54:26Z,,,
7970,b'[tests|tokenizers] Refactoring pipelines test backbone - Small tokenizers improvements - General tests speedups',2020-10-22T10:05:22Z,2020-10-23T13:58:20Z,,,
7969,b'Add model_cards',2020-10-22T09:59:58Z,2020-10-29T12:29:55Z,model card,,
7968,b'Herbert tokenizer auto load',2020-10-22T09:33:41Z,2020-10-22T09:48:29Z,,,
7967,"b""Should update version requirement for scipy in 'examples\\\\distillation\\\\requirements.txt'?""",2020-10-22T08:30:18Z,2020-10-27T02:03:03Z,,,
7966,b'T5 with allowing model changes ',2020-10-22T06:28:24Z,2020-12-18T23:50:10Z,,,
7965,b'[s2s trainer] tests to use distributed on multi-gpu machine',2020-10-22T05:47:49Z,2020-10-22T21:26:23Z,,,
7964,b'adding beginner-friendly notebook on text classification with DistilBERT/TF ',2020-10-22T05:26:56Z,2020-10-22T13:30:50Z,,,
7963,b'Load tuned model without downloading from huggingface',2020-10-22T02:56:27Z,2020-10-27T08:03:02Z,,,
7962,b'xla_spawn and run_language_modeling slow on TPUs',2020-10-22T02:19:39Z,2020-11-04T23:20:57Z,,,
7961,b'A question about shift_tokens_right in BART model',2020-10-22T02:13:12Z,2020-10-22T03:10:20Z,,,
7960,b'RoBERTa convert-script modified to support mapping of bpe tokens',2020-10-22T01:03:05Z,2021-04-26T15:02:51Z,,,
7959,b'T5-large on multiple gpus.',2020-10-22T00:50:10Z,2021-01-03T13:23:23Z,wontfix,,
7958,b'added qg evaluation notebook',2020-10-22T00:38:10Z,2020-10-22T09:02:12Z,,,
7957,"b'dropping "","" in date because of Tokenization'",2020-10-21T23:14:31Z,2021-01-02T04:26:07Z,wontfix,,
7956,b'T5 on multiple datasets',2020-10-21T22:22:53Z,2021-01-03T13:23:24Z,wontfix,,
7955,b'[pip/setup.py] target management and updates',2020-10-21T21:58:57Z,2020-10-26T17:35:08Z,,,
7954,b'TF: Faster to way to set one column/all but one column of a tensor to -inf',2020-10-21T20:16:33Z,2020-10-30T15:23:17Z,TensorFlow,,
7953,"b"" 'EncoderDecoderModel' object has no attribute '_init_weights' after `model.resize_token_embeddings(len(tokenizer))`""",2020-10-21T20:03:49Z,2020-10-22T17:07:31Z,,torch.nn.modules.module.ModuleAttributeError,"torch.nn.modules.module.ModuleAttributeError: 'EncoderDecoderModel' object has no attribute '_init_weights'"
7952,b'model card for German Sentence Embeddings V2',2020-10-21T19:59:35Z,2020-10-23T14:45:55Z,model card,,
7951,"b'Unexpected/wrong handling of added special tokens in special_tokens_mask (GPT1, BERT, possibly others)'",2020-10-21T19:40:41Z,,WIP,,
7950,b'Code bug in tokenization_utils.py?',2020-10-21T18:21:47Z,2021-01-02T04:26:08Z,wontfix,,
7949,"b""fix 'encode_plus' docstring for 'special_tokens_mask' (0s and 1s were reversed)""",2020-10-21T17:54:08Z,2020-10-21T17:57:45Z,,,
7948,"b""Error: should have a 'get_encoder' function defined when running model.generate()""",2020-10-21T16:53:24Z,2020-10-21T21:16:48Z,,AssertionError,"AssertionError: BartModel("
7947,b'[GPT2 batch generation] Make test clearer. `do_sample=True` is not deterministic.',2020-10-21T16:53:08Z,2020-10-21T17:06:24Z,,,
7946,b'EncoderDecoderModel loss function',2020-10-21T16:02:43Z,2020-10-26T08:45:46Z,,,
7945,b'Move NoLayerEmbedTokens',2020-10-21T15:08:26Z,2020-10-22T20:13:50Z,,,
7944,b'[ProphetNet] Correct Doc string example',2020-10-21T14:00:43Z,2020-10-21T15:27:21Z,,,
7943,b'[PretrainedConfig] Fix save pretrained config for edge case ',2020-10-21T11:17:30Z,2020-10-22T13:39:02Z,,,
7942,b'[ProphetNet] Add Question Generation Model + Test',2020-10-21T09:15:00Z,2020-10-21T09:49:59Z,,,
7941,b'[RAG] Handle the case when title is None while loading own datasets',2020-10-21T08:58:02Z,2020-10-23T13:54:46Z,,,
7940,b'Access bert output with output_hidden_states=True  of TFBertForSequenceClassification fails',2020-10-21T05:59:53Z,2020-10-26T17:20:31Z,,,
7939,b'Fix BatchEncoding.word_to_tokens for removed tokens',2020-10-20T22:11:54Z,2020-10-23T14:29:38Z,,,
7938,b'PPL guide code snippet minor fix',2020-10-20T22:07:34Z,2020-10-20T22:17:40Z,,,
7937,b'Your example code for WNUT NER produces array indexing ValueError',2020-10-20T20:17:05Z,2021-11-09T19:15:27Z,wontfix,ValueError,"ValueError: NumPy boolean array indexing assignment cannot assign 29 input values to the 24 output values where the mask is true"
7936,b'cannot load customized tokenizer with modified vocabulary',2020-10-20T18:11:49Z,2021-01-03T13:23:30Z,wontfix,,
7935,b'TensorBoard/Wandb/optuna/raytune integration improvements.',2020-10-20T17:30:22Z,2020-10-21T15:18:53Z,,,
7934,b'[s2s] create doc for pegasus/fsmt replication',2020-10-20T17:02:59Z,2020-10-20T19:07:52Z,,,
7933,b'Fix comet_ml import and add ensure availability',2020-10-20T17:00:01Z,2020-10-27T11:31:08Z,,,
7932,b'Addition of MMI-antiLM decoding',2020-10-20T16:53:21Z,2021-01-10T13:54:08Z,wontfix,,
7931,b'MMI-antiLM decoding',2020-10-20T16:51:47Z,2021-03-06T00:16:56Z,wontfix,,
7930,b'update model cards of Illuin models',2020-10-20T16:15:56Z,2020-10-21T12:05:54Z,model card,,
7929,b'Reformer model does not work with padded sequences',2020-10-20T15:10:36Z,2020-10-23T14:29:15Z,,IndexError,"IndexError: index out of range in self"
7928,b'Respect the 119 line chars',2020-10-20T14:56:08Z,2020-10-20T15:02:48Z,,,
7927,b'[ProphetNet] add model summary',2020-10-20T14:03:47Z,2020-10-20T14:11:03Z,,,
7926,b'Validation loop gives OOM when finetuning T5',2020-10-20T12:56:29Z,2021-03-06T00:16:57Z,wontfix,RuntimeError,"RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 14.73 GiB total capacity; 13.70 GiB already allocated; 13.88 MiB free; 13.72 GiB reserved in total by PyTorch)"
7925,b'# Add whole word mask support for lm fine-tune',2020-10-20T10:24:33Z,2020-10-22T13:19:00Z,,,
7924,b'EncoderDecoderModel not working with DDP',2020-10-20T06:57:08Z,2021-04-26T15:02:52Z,,RuntimeError,"RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by (1) passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`; (2) making sure all `forward` function outputs participate in calculating loss. If you already have done the above two steps, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable)."
7923,b'Loading a pytorch quantized model',2020-10-20T06:08:17Z,2021-01-02T04:26:04Z,wontfix,,
7922,b'Is it possible to recommend the deployment method for implementing trained mode',2020-10-20T05:57:09Z,2020-10-20T09:07:21Z,,,
7921,b'[testing] experiment with a different way of skipping torch-only test modules',2020-10-20T05:53:32Z,2020-10-20T20:25:32Z,,,
7920,"b""what's the values of start_positon and end_position while the answer is impossible in run_squad.py""",2020-10-20T02:07:32Z,2020-12-26T07:26:22Z,wontfix,,
7919,b'Expose the Flax code quality problems',2020-10-19T21:48:25Z,2020-10-20T13:06:51Z,,,
7918,b'Add Flax dummy objects',2020-10-19T21:44:00Z,2020-10-20T11:45:49Z,,,
7917,b'New run glue script',2020-10-19T21:19:12Z,2020-10-22T15:42:23Z,,,
7916,"b'TypeError: __init__() got an unexpected keyword argument \'vocab_file\' in transformers/tokenization_gpt2.py"", line 380'",2020-10-19T20:00:51Z,2020-11-02T07:08:39Z,,TypeError,"TypeError: __init__() got an unexpected keyword argument 'vocab_file'`"
7915,b'[EncoderDecoder] Fix Typo',2020-10-19T19:43:46Z,2020-10-19T20:02:42Z,,,
7914,b'[flax] fix repo_check',2020-10-19T19:25:55Z,2020-10-20T11:55:40Z,,,
7913,b' `add_prefix_space=True` option in the BPE tokenizer ',2020-10-19T19:23:25Z,2020-12-26T07:26:24Z,wontfix,,
7912,"b'run_tf_text_classification.py giving ""ValueError: too many values to unpack""'",2020-10-19T18:34:04Z,2021-03-06T00:16:58Z,wontfix,,
7911,b'[Docstring] fix t5 training docstring',2020-10-19T17:45:26Z,2020-10-19T19:49:48Z,,,
7910,b'[T5] Ignore sentinel indices for unsupervised denoising / masking objective?',2020-10-19T16:16:36Z,2020-10-26T20:13:46Z,,,
7909,b'pegasus/cnn_dm  12-2 distillation performing poorly',2020-10-19T15:48:42Z,2020-10-26T17:56:54Z,pegasus,,
7908,b'[Model] M2M-100 Multilingual machine translation',2020-10-19T15:19:16Z,2020-11-05T21:38:00Z,New model,,
7907,b'Reproducing Bart Xsum from Bart Large',2020-10-19T14:57:08Z,2020-11-24T19:36:33Z,,,
7906,b'labels and decoder_input_ids to Glossary',2020-10-19T14:39:50Z,2020-10-20T13:50:48Z,,,
7905,b'[RAG] How to extract generated strings from `RetrievAugLMMarginOutput`',2020-10-19T14:26:18Z,2020-10-21T10:40:32Z,,,
7904,b'T5 Docs training example has shifted labels',2020-10-19T14:19:19Z,2020-10-19T19:49:48Z,,,
7903,b'Modelling Encoder-Decoder | Error :- `decoder_config` used before intialisation',2020-10-19T13:47:02Z,2020-10-19T17:48:49Z,,,
7902,b'change TokenClassificationTask class methods to static methods',2020-10-19T13:11:03Z,2020-11-05T14:38:30Z,,,
7901,b'GPT2Tokenizer strips spaces surrounding special tokens',2020-10-19T11:51:44Z,2021-05-23T15:02:50Z,,,
7900,b'example for passage re-ranking using bert-multilingual-passage-reranking-msmarco',2020-10-19T10:13:01Z,2020-12-26T07:26:21Z,wontfix,,
7899,b'Create README.md',2020-10-19T09:53:15Z,2020-10-21T12:23:56Z,model card,,
7898,b'[EncoderDecoder] google/roberta2roberta_L-24_wikisplit',2020-10-19T09:12:58Z,2020-10-20T13:14:01Z,,,
7897,b'GPT2Tokenizer.add_tokens() didnt change tokenizer.vocab_size',2020-10-19T07:59:58Z,2020-12-26T07:26:20Z,wontfix,,
7896,b'Bert2bert EncoderDecoderModel from Huggingface is generating a zero tensor for any input',2020-10-19T04:15:08Z,2021-01-03T13:23:27Z,wontfix,,
7895,b'[testing] slow tests should be marked as slow',2020-10-19T00:56:28Z,2020-10-22T10:34:06Z,,,
7894,b'AdamW ',2020-10-19T00:55:39Z,2020-10-19T12:36:35Z,,,
7893,b'pip install transformers by default install 2.5.1',2020-10-19T00:45:05Z,2020-12-26T07:26:20Z,wontfix,,
7892,b'Issue with XLM-R for multiple-choice questions ',2020-10-19T00:01:37Z,2020-12-26T07:26:26Z,wontfix,,
7891,"b""[RAG] Propagating of n_docs as parameter to all RagModel's related functions""",2020-10-18T22:14:57Z,2020-10-19T13:15:53Z,,,
7890,b'[wip] improved github actions workflow',2020-10-18T20:17:17Z,2020-10-21T17:01:54Z,,,
7889,b'from_pretrained incompatible with the models being downloaded',2020-10-18T20:04:58Z,2020-10-21T07:24:45Z,,,
7888,"b'[tests] fix slow bart cnn test, faster marian tests'",2020-10-18T19:48:05Z,2020-10-19T00:18:09Z,,,
7887,b'Github actions: more readable/informative output',2020-10-18T19:37:51Z,2020-10-27T13:25:08Z,"Help wanted, Tests",,
7886,b'Question answering example errors with BrokenPipeError: [Errno 32] Broken pipe',2020-10-18T19:25:39Z,2021-01-28T14:40:14Z,wontfix,BrokenPipeError,"BrokenPipeError: [Errno 32] Broken pipe"
7885,b'[testing] the test suite is many times slower than 2 weeks ago',2020-10-18T19:02:54Z,2020-10-22T10:34:05Z,,,
7884,b'[CIs] report slow tests add --durations=0 to some pytest jobs',2020-10-18T18:42:59Z,2020-10-19T12:23:15Z,,,
7883,b'style: fix typo',2020-10-18T12:34:12Z,2020-10-19T10:14:53Z,,,
7882,b'style: fix typo in the README',2020-10-18T12:30:49Z,2020-10-19T12:43:26Z,,,
7881,b'Error(s) in loading state_dict for BertForTokenClassification',2020-10-18T11:30:35Z,2021-01-03T13:23:28Z,wontfix,RuntimeError,"RuntimeError: Error(s) in loading state_dict for BertForTokenClassification:"
7880,b'Fix bug in _sorted_checkpoints',2020-10-18T09:13:44Z,2020-10-20T11:50:48Z,,TypeError,"TypeError: 'str' object does not support item assignment"
7879,b'question about `add_special_tokens` and embedding layer',2020-10-18T08:33:35Z,2020-10-24T02:52:56Z,,,
7878,b'[multiple models] skip saving/loading deterministic state_dict keys',2020-10-18T04:29:14Z,2020-10-21T12:06:08Z,,,
7877,b'[wip] [pegasus] encode s/\\r?\\n/<n>/g + test',2020-10-18T03:57:10Z,2020-10-19T18:51:02Z,,,
7876,b'xlm-mlm-17-1280 & xlm-mlm-100-1280 include languages?',2020-10-18T02:59:59Z,2020-10-20T08:53:49Z,,,
7875,b'How to do categorical sequence classification?',2020-10-17T22:40:48Z,2020-10-18T10:19:27Z,,,
7874,b'[Rag] extend_enc_output fails when number of retrieved documents not equal to RagConfig.n_docs',2020-10-17T21:35:25Z,2020-10-19T13:15:53Z,,RuntimeError,"RuntimeError: shape '[0, 1, 5, 300]' is invalid for input of size 600"
7873,"b'can\'t set evaluation_strategy to ""epoch""'",2020-10-17T19:55:17Z,2020-12-25T09:25:02Z,wontfix,,
7872,b'Fix Rag example docstring',2020-10-17T19:45:14Z,2020-10-17T20:46:48Z,,,
7871,b'RAG generate function uses input_ids even when context_input_ids are given.',2020-10-17T18:56:53Z,2020-10-17T20:46:48Z,,,
7870,b'Add missing comma',2020-10-17T15:55:18Z,2020-10-21T12:24:13Z,model card,,
7869,b'Raise error when using AMP on non-CUDA device',2020-10-17T12:48:28Z,2020-10-19T19:59:31Z,,,
7868,b'Julibert model card',2020-10-17T12:06:44Z,2020-10-19T10:50:53Z,model card,,
7867,b'Add AI-SOCO models',2020-10-17T09:45:37Z,2020-10-21T13:24:44Z,model card,,
7866,"b""AttributeError: module 'tensorflow_core.keras.activations' has no attribute 'swish'""",2020-10-17T09:09:15Z,2020-10-20T08:49:53Z,,,
7865,b'labels and decoder_input_ids',2020-10-17T06:10:18Z,2020-10-20T13:50:48Z,,,
7864,b'Create model card for pre-trained NLI models.',2020-10-17T02:40:40Z,2020-10-24T07:16:07Z,model card,,
7863,b'[testing] rename skip targets + docs',2020-10-17T02:17:56Z,2020-10-20T08:39:14Z,,,
7862,b'unshared Albert',2020-10-17T02:05:30Z,2021-01-02T04:26:06Z,wontfix,,
7861,b'[testing] remove USE_CUDA',2020-10-17T01:53:55Z,2020-10-19T11:08:35Z,,,
7860,b'[fsmt test] basic config test with online model + super tiny model',2020-10-17T01:27:45Z,2020-10-22T13:14:55Z,,,
7859,"b'[s2s testing] turn all to unittests, use auto-delete temp dirs'",2020-10-16T20:37:34Z,2020-10-17T18:33:22Z,,,
7858,b'Trainer with Iterable Dataset',2020-10-16T20:25:19Z,2020-10-19T15:57:40Z,,,
7857,b'Create README.md',2020-10-16T18:16:04Z,2020-10-21T12:41:41Z,model card,,
7856,b'Remove duplicated mish activation function',2020-10-16T17:42:30Z,2020-10-17T21:31:53Z,,,
7855,b'Model card for German BERT fine-tuned for LER/NER',2020-10-16T17:18:26Z,2020-10-21T12:31:41Z,model card,,
7854,b'Fixing issue #7810',2020-10-16T17:14:47Z,2021-03-06T00:17:02Z,wontfix,,
7853,b'SequenceSummary class in modeling_utils.py',2020-10-16T14:51:59Z,2020-12-24T20:34:53Z,wontfix,,
7852,b'Upgrade PyTorch Lightning to 1.0.2',2020-10-16T13:09:30Z,2020-10-28T18:59:15Z,,,
7851,b'Trainer accepts iterable datasets',2020-10-16T13:03:56Z,2020-10-16T15:02:16Z,,,
7850,b'OperatorNotAllowedInGraphError in dbmdz/bert-base-italian-cased for Token Classification',2020-10-16T12:19:08Z,2020-12-26T07:26:18Z,wontfix,OperatorNotAllowedInGraphError,"OperatorNotAllowedInGraphError: in user code:"
7849,b'how to save and load fine-tuned model?',2020-10-16T10:34:16Z,2020-10-20T08:03:20Z,,,
7848,b'RuntimeError with DistributedDataParallel ',2020-10-16T08:43:48Z,2021-01-25T15:32:16Z,,RuntimeError,"RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.LongTensor [1, 200]] is at version 9; expected version 7 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True)."
7847,b'Where do the models go in colab?',2020-10-16T07:51:05Z,2020-12-24T20:34:50Z,wontfix,"FileNotFoundError, TypeError","FileNotFoundError: [Errno 2] No such file or directory: 'model/bert_config.json'TypeError: __init__() got an unexpected keyword argument 'num_labels'`"
7846,"b'BartTokenizer prepare_seq2seq_batch() does not return decoder_input_ids,  decoder_attention_mask as per document after passing tgt_texts'",2020-10-16T07:00:21Z,2020-11-22T14:05:09Z,,,
7845,b'[seq2seq testing] improve readability',2020-10-16T05:23:53Z,2020-10-16T13:05:29Z,,,
7844,b'[seq2seq distributed] child process stuck on error',2020-10-16T05:01:01Z,2020-10-28T15:56:19Z,,,
7843,b'[seq2seq] get_git_info fails gracefully',2020-10-16T04:13:52Z,2020-10-16T04:22:44Z,,,
7842,b'[testing] disable FutureWarning in examples tests',2020-10-16T03:24:46Z,2020-10-16T07:35:39Z,,,
7841,b'[DOC] Typo and fix the input of labels to `cross_entropy`',2020-10-15T23:09:02Z,2020-10-15T23:36:32Z,,,
7840,"b""Token Type IDs returned from the tokenizer for T5 don't work with special tokens""",2020-10-15T23:08:51Z,2020-11-10T19:21:54Z,,,
7839,b'Small fixes to HP search',2020-10-15T21:29:05Z,2020-10-16T07:23:45Z,,,
7838,b'State of ONNX',2020-10-15T20:37:44Z,2021-04-26T15:02:53Z,wontfix,,
7837,b'[testing] fix/hide warnings',2020-10-15T19:54:09Z,2020-10-16T07:19:52Z,,,
7836,b'model card for arabic-ner model',2020-10-15T19:34:50Z,2020-10-21T12:02:41Z,,,
7835,"b'[cleanup] assign todos, faster bart-cnn test'",2020-10-15T19:29:15Z,2020-10-16T07:11:19Z,,,
7834,b'[utils/check_copies.py] fix DeprecationWarning',2020-10-15T19:19:39Z,2020-10-15T20:21:10Z,,,
7833,b'[s2s trainer] tests fail on multi-gpu machine',2020-10-15T19:18:30Z,2020-10-22T21:26:22Z,,,
7832,"b""[testing] test_trainer_callback.py cannot collect test class 'TestTrainerCallback' """,2020-10-15T19:08:26Z,2020-10-16T07:19:51Z,,,
7831,"b'[RAG] RagSequenceForGeneration should not load ""facebook/rag-token-nq"" and RagTokenForGeneration also should not load ""facebook/rag-sequence-nq""'",2020-10-15T18:53:29Z,2020-10-16T14:10:40Z,,,
7830,b'fix wandb/comet problems',2020-10-15T18:52:46Z,2020-10-15T19:23:25Z,,,
7829,"b'[RAG] RagSequenceForGeneration: Running ""retriever separately example"" giving error'",2020-10-15T18:47:38Z,2020-10-17T20:46:48Z,,AssertionError,"AssertionError: Make sure that `context_input_ids` are passed, if no `retriever` is set. Alternatively, you can set a retriever using the `set_retriever(...)` function."
7828,b'fix: ignore padding tokens in Bart loss',2020-10-15T17:47:43Z,2020-11-29T12:36:26Z,,,
7827,b'Do I need to apply the softmax function to my logit before calculating the CrossEntropyLoss?',2020-10-15T17:23:08Z,2020-12-24T11:26:59Z,wontfix,,
7826,b'[Pipelines] Fix links to model lists',2020-10-15T17:07:16Z,2020-10-16T06:57:03Z,,,
7825,b'RFC: Move `_NoLayerEmbedTokens` to modeling_tf_utils.py',2020-10-15T17:00:09Z,2020-10-22T20:13:49Z,,,
7824,b'Bart Caching: do we need encoder outputs after step 1?',2020-10-15T16:41:07Z,2021-01-02T04:25:58Z,wontfix,,
7823,b'Support for custom data_collator in Trainer.train() with datasets.Dataset',2020-10-15T16:15:22Z,2020-10-16T09:14:47Z,,,
7822,b'[testing] test_modeling_deberta.py is failing',2020-10-15T16:13:12Z,2020-10-28T15:59:15Z,,,
7821,b'[testing] trainer tests fail - 2 issues',2020-10-15T16:09:00Z,2020-10-15T19:23:24Z,,OSError,"OSError: [Errno 29] Illegal seek"
7820,b'Fix small type hinting error',2020-10-15T15:44:47Z,2020-10-19T06:14:30Z,,,
7819,b'Create README.md',2020-10-15T15:05:41Z,2020-10-21T12:30:01Z,model card,,
7818,b'Remove masked_lm_labels from returned dictionary in DataCollatorForNextSentencePrediction',2020-10-15T14:59:41Z,2020-10-16T07:12:10Z,,,
7817,b'Fix missing reference titles in retrieval evaluation of RAG',2020-10-15T14:34:52Z,2020-10-16T08:15:49Z,,,
7816,"b""RAG - MissingIndex: Index with index_name 'embeddings' not initialized yet""",2020-10-15T14:34:12Z,2020-11-20T18:05:04Z,,,
7815,"b'""Cannot re-initialize CUDA in forked subprocess."" error when running ""transformers/notebooks/05-benchmark.ipynb"" notebook'",2020-10-15T14:22:24Z,2021-03-06T00:17:04Z,wontfix,ValueError,"ValueError: too many values to unpack (expected 2)"
7814,b'BART/TFBart: allow decoder_input_ids.shape[-1] > 1 + use_cache = True',2020-10-15T14:21:58Z,2020-12-09T19:55:25Z,,,
7813,b'Small fixes to NotebookProgressCallback',2020-10-15T14:13:20Z,2020-10-15T14:30:35Z,,,
7812,b'the results of run_squad.py is terrible',2020-10-15T13:31:29Z,2021-01-18T07:02:11Z,wontfix,,
7811,b'Fix issue #7781',2020-10-15T12:43:28Z,2020-10-20T07:04:27Z,,,
7810,b'Metrics calculate error: can only calculate the mean of floating types. Got Bool instead',2020-10-15T12:36:03Z,2020-12-24T20:34:48Z,wontfix,"RuntimeError, TypeError","RuntimeError: Can only calculate the mean of floating types. Got Bool instead.TypeError: cannot unpack non-iterable NoneType object"
7809,b'[Seq2Seq] Allow EncoderDecoderModels to be trained with Seq2Seq',2020-10-15T12:35:46Z,2020-10-23T21:05:52Z,,,
7808,b'Pipeline(summarization): CUDA error: an illegal memory access was encountered',2020-10-15T12:28:20Z,2020-10-16T18:14:56Z,Core: Pipeline,RuntimeError,"RuntimeError: CUDA error: an illegal memory access was encountered"
7807,b'ValueError: too many values to unpack (expected 4)',2020-10-15T12:14:26Z,2020-10-15T17:06:17Z,,ValueError,"ValueError: too many values to unpack (expected 4)"
7806,b'Import error when fine-tuning mbart from master branch',2020-10-15T11:48:21Z,2020-10-16T09:11:49Z,,ImportError,"ImportError: cannot import name 'Unigram'"
7805,b'pip3 install issue',2020-10-15T10:14:10Z,2020-10-15T10:30:34Z,,,
7804,b'a',2020-10-15T09:07:52Z,2020-10-20T18:52:21Z,,,
7803,b'TPU pod training with BERT ',2020-10-15T08:48:50Z,2020-12-24T20:34:47Z,wontfix,,
7802,b'simple fix for spurious PyTorch->TF BERT weight conversion warning',2020-10-15T05:24:51Z,2021-03-06T00:17:06Z,wontfix,,
7801,b'Can not convert the the custom trained BERT model to pytorch model for further use which should give me .bin file',2020-10-15T02:39:12Z,2020-12-24T11:26:57Z,"wontfix, Migration",_pickle.UnpicklingError,"_pickle.UnpicklingError: invalid load key, '\x00'."
7800,b'Empty Conversation Responses',2020-10-15T02:15:36Z,2021-03-06T00:17:08Z,wontfix,,
7799,b'model card for bert-base-NER',2020-10-15T01:38:53Z,2020-10-15T19:55:01Z,model card,,
7798,b'Herbert polish model',2020-10-14T22:09:37Z,2020-10-16T07:06:52Z,model card,,
7797,"b""BertForSequenceClassification  -> TFBertForSequenceClassification causes 'bert.embeddings.position_ids' not used error""",2020-10-14T21:09:35Z,2020-12-24T11:26:56Z,wontfix,,
7796,b'T5 finetune outputting gibberish',2020-10-14T21:09:04Z,2020-10-14T21:14:10Z,,,
7795,b'Fix TF savedmodel in Roberta',2020-10-14T20:02:17Z,2020-10-14T21:48:51Z,,,
7794,b'Updated Tokenizer to 0.9.1 from prerelease version',2020-10-14T19:14:14Z,2020-10-15T09:08:57Z,,,
7793,b'Add specific notebook ProgressCalback',2020-10-14T18:41:57Z,2020-10-15T09:05:09Z,,,
7792,b'[stas/sam] Newsroom dataset wierdness',2020-10-14T18:19:33Z,2020-10-14T18:44:25Z,,AssertionError,"AssertionError: empty tgt line for index 661"
7791,b'T5 Conversion from Original Tensorflow Produce rubbish Text',2020-10-14T18:15:27Z,2020-11-13T19:31:41Z,,,
7790,b'updated bangla-bert-base model card with evaluation results',2020-10-14T16:26:27Z,2020-10-14T16:50:43Z,model card,,
7789,b'Recommended Adafactor settings for T5 cause error',2020-10-14T16:10:34Z,2021-04-01T04:03:38Z,,,
7788,b'error when using the forward() function of the LongformerLayer class from the LongformerForMultipleChoice model',2020-10-14T15:53:09Z,2020-10-15T12:12:23Z,,ValueError,"ValueError: too many values to unpack (expected 4)"
7787,b'Fixing beam search output shapes',2020-10-14T15:51:45Z,2020-10-15T11:34:46Z,,,
7786,"b""Don't use `store_xxx` on optional bools""",2020-10-14T15:43:10Z,2020-10-14T16:05:03Z,,,
7785,b'[RAG] RagTokenizer failing in decoding RAG Generator output',2020-10-14T15:33:51Z,2020-10-14T16:23:14Z,,ValueError,"ValueError: invalid literal for int() with base 10: 'l'"
7784,b'Adding prefix constrained beam search',2020-10-14T15:29:33Z,2020-11-13T19:48:36Z,,,
7783,b'Unable to serialize/save TF2.3.1 RobertaSequenceClassification model to saved model format ',2020-10-14T15:22:50Z,2020-10-14T21:48:51Z,,TypeError,"TypeError: ('Not JSON Serializable:', RobertaConfig {"
7782,"b""RAG finetuning - unexpected keyword argument 'early_stop_callback'""",2020-10-14T15:22:04Z,2020-10-15T13:30:19Z,,TypeError,"TypeError: __init__() got an unexpected keyword argument 'early_stop_callback'"
7781,b'`decoder_config` variable not defined in EncoderDecoderModel.from_encoder_decoder_pretrained',2020-10-14T14:53:35Z,2020-10-20T07:04:06Z,,`UnboundLocalError,"`UnboundLocalError: local variable 'decoder_config' referenced before assignment`"
7780,"b""How can I tweak the `Longformer` code to control the input of a `Longformer`'s layer?""",2020-10-14T14:03:56Z,2020-10-14T15:48:14Z,,,
7779,"b'I\'m getting ""nan"" value for loss, while following a tutorial from the documentation'",2020-10-14T13:50:52Z,2020-12-24T11:26:52Z,wontfix,,
7778,b'multi task roberta',2020-10-14T11:59:43Z,2020-10-14T12:00:29Z,,,
7777,b'Adding RAG to text-generation pipeline',2020-10-14T09:32:29Z,2020-12-24T11:26:51Z,wontfix,,
7776,b'Fix bert position ids in DPR convert script',2020-10-14T09:23:33Z,2020-10-14T09:30:03Z,,,
7775,b'Create README.md',2020-10-14T08:12:29Z,2020-10-14T13:31:02Z,model card,,
7774,b'XLM-RoBERTa model for QA seems not properly work',2020-10-14T07:12:46Z,2020-12-26T07:26:25Z,wontfix,,
7773,"b""Error in run_ner.py - ModuleNotFoundError: No module named 'tasks'""",2020-10-14T07:02:23Z,2020-12-24T11:26:59Z,wontfix,ModuleNotFoundError,"ModuleNotFoundError: No module named 'tasks'"
7772,b'Added gpt2 model parallelism',2020-10-14T06:39:37Z,2020-11-24T18:25:25Z,Model Parallel,,
7771,"b""Cannot trace_module on models using model's generate function""",2020-10-14T04:35:09Z,2020-12-24T11:26:55Z,wontfix,AssertionError,"AssertionError: `max_length` should be a strictly positive integer."
7770,b'How to create a QA model where the answer can be from the question text as well?',2020-10-14T01:26:40Z,2020-10-24T14:46:24Z,,,
7769,b'from transformers import RagSequenceForGeneration gives ImportError',2020-10-13T21:51:05Z,2020-10-13T22:18:13Z,,ImportError,"ImportError: cannot import name 'RagSequenceForGeneration'"
7768,b'Is there any way to control the input of a layer of `Longformer`?',2020-10-13T21:15:35Z,2020-10-14T13:58:53Z,,AttributeError,"AttributeError: 'NoneType' object has no attribute 'squeeze'"
7767,b'Add predict step accumulation',2020-10-13T20:25:56Z,2020-10-14T15:41:47Z,,,
7766,b'Use Marian-MT to evaluate translated outputs by printing out per-word log-probility',2020-10-13T19:18:41Z,2021-01-10T13:54:07Z,wontfix,,
7765,"b'Seq2seq finetune example: ""Please save or load state of the optimizer""'",2020-10-13T18:20:47Z,2020-12-08T22:18:38Z,,,
7764,b'Update of DialoGPT `max_length`',2020-10-13T17:10:37Z,2020-10-14T12:01:24Z,,,
7763,b'Allow Custom Dataset in RAG Retriever',2020-10-13T16:33:14Z,2020-10-19T17:42:46Z,,,
7762,b'Faster pegasus tokenization test with reduced data size',2020-10-13T15:59:17Z,2020-10-13T20:22:30Z,,,
7761,"b""Deutsch to English Translation Model by Google doesn't work anymore...""",2020-10-13T15:13:24Z,2020-10-14T12:07:29Z,,,
7760,"b""AttributeError: 'tuple' object has no attribute 'detach'""",2020-10-13T15:03:30Z,2020-10-14T07:41:38Z,,AttributeError,"AttributeError: 'tuple' object has no attribute 'detach'`"
7759,b'Adding optional trial argument to model_init',2020-10-13T14:29:43Z,2020-10-13T15:07:03Z,,,
7758,b'fixed lots of typos.',2020-10-13T13:53:03Z,2020-10-13T14:00:20Z,,,
7757,b'Help with finetuning mBART on an unseen language',2020-10-13T12:54:54Z,2020-12-20T13:34:48Z,wontfix,,
7756,b'[Rag] Fix loading of pretrained Rag Tokenizer',2020-10-13T09:54:57Z,2020-10-13T12:34:23Z,,,
7755,b'HfArgumentParser not support optional bools',2020-10-13T09:43:46Z,2020-10-14T16:05:03Z,,,
7754,b'ElectraTokenizerFast',2020-10-13T08:48:27Z,2020-10-13T08:50:41Z,,KeyError,"KeyError: 'ElectraTokenizer'"
7753,b'New TF model design',2020-10-13T07:12:44Z,2021-02-08T13:31:52Z,,,
7752,b'Model Card',2020-10-13T06:48:56Z,2020-10-14T17:30:59Z,model card,,
7751,b'Unicode issue with tokenizer.decode()',2020-10-13T04:46:34Z,2020-10-13T12:20:01Z,,,
7750,b'Update pyarrow to meet datasets 1.1.2',2020-10-13T03:48:22Z,2020-10-17T01:55:57Z,,,
7749,b'Does bart need to cache prev_key_padding_mask?',2020-10-13T02:46:20Z,2020-10-21T11:10:17Z,,,
7748,b'Create README.md',2020-10-13T02:16:52Z,2020-10-14T17:10:31Z,model card,,
7747,"b'BertTokenizer meet multilingual corpus, it fails to work.@mfuntowicz'",2020-10-13T02:08:43Z,2020-10-13T02:16:09Z,,,
7746,b'Keep getting the same `Target 1 is out of bounds` error with `LongformerForMultipleChoice`',2020-10-13T00:29:09Z,2020-10-13T21:15:52Z,,IndexError,"IndexError: Target 1 is out of bounds."
7745,b'Attention masks are ignored when using model.generate() in batch setting for GPT-2',2020-10-12T21:50:29Z,2020-10-21T17:06:23Z,,,
7744,"b'cannot load ""layoutlm-base-uncased""'",2020-10-12T19:26:51Z,2020-10-12T20:13:32Z,,,
7743,b'should PegasusTokenizer replace `/n` with `<n>`?',2020-10-12T17:07:47Z,2020-10-20T15:52:12Z,pegasus,,
7742,b'Avoid unnecessary DDP synchronization when gradient_accumulation_steps > 1',2020-10-12T16:42:15Z,2020-10-13T13:46:45Z,,,
7741,b'Avoid unnecessary DDP synchronization when gradient_accumulation_steps > 1',2020-10-12T16:31:22Z,2020-10-12T16:42:25Z,,,
7740,b'examples/seq2seq/finetune_trainer.py: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`.',2020-10-12T16:14:53Z,2020-10-13T00:55:53Z,,,
7739,"b""Cannot load pretrained microsoft's layoutlm""",2020-10-12T16:09:28Z,2020-10-12T16:14:09Z,,"RuntimeError, OSError","RuntimeError: version_ <= kMaxSupportedFileFormatVersion INTERNAL ASSERT FAILED at /pytorch/caffe2/serialize/inline_container.cc:132, please report a bug to PyTorch. Attempted to read a PyTorch file with version 3, but the maximum supported version for reading is 2. Your PyTorch installation may be too old. (init at /pytorch/caffe2/serialize/inline_container.cc:132)OSError: Unable to load weights from pytorch checkpoint file. If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True. "
7738,b'Add license info to nlptown/bert-base-multilingual-uncased-sentiment',2020-10-12T15:16:57Z,2020-10-12T15:56:11Z,model card,,
7737,b'blenderbot-3B has wrong model card',2020-10-12T15:10:25Z,2020-10-29T21:56:38Z,model card,,
7736,b'Make T5 Supports Gradient Checkpointing',2020-10-12T14:26:10Z,2020-10-13T09:18:00Z,,,
7735,b'Tokenizer Fast bug: ValueError: TextInputSequence must be str',2020-10-12T14:25:46Z,2020-10-13T09:01:46Z,,ValueError,"ValueError: TextInputSequence must be str"
7734,"b""GLUE STS-B on longer sequence lengths doesn't work?""",2020-10-12T14:16:08Z,2021-03-06T00:17:09Z,wontfix,,
7733,b'[Prophetnet] Develop in parallel',2020-10-12T13:00:06Z,2020-11-13T13:10:22Z,model card,,
7732,b'Fix #7731',2020-10-12T12:47:52Z,2020-10-12T13:10:53Z,,,
7731,b'Pytorch 1.6 DataParallel',2020-10-12T11:58:33Z,2020-10-12T13:15:13Z,,,
7730,b'Upgrading in pipelines TFAutoModelWithLMHead to new Causal/Masked/Seq2Seq LM classes',2020-10-12T09:35:19Z,2020-10-15T09:26:08Z,,,
7729,b'Fix DeBERTa integration tests',2020-10-12T09:34:40Z,2020-10-16T06:49:14Z,,,
7728,"b""Improving Pipelines by defaulting to framework='tf' when pytorch seems unavailable.""",2020-10-12T09:13:37Z,2020-10-15T07:42:08Z,,OSError,OSError: Can't load weights for 'Narsil/small'. Make sure that:
7727,b'what is the perplexity of distilbert-base-uncased ? ',2020-10-12T09:11:49Z,2020-12-20T13:34:47Z,wontfix,,
7726,b'Do not softmax when num_labels==1',2020-10-12T09:09:10Z,2020-10-13T13:42:28Z,,,
7725,b'how we can replace/swap the Wikipedia data with our custom data for knowledge retrieval in the RAG  model and  the format of the retrieval data.  ',2020-10-12T09:02:56Z,2020-12-20T13:34:46Z,wontfix,,
7724,b'Fix tf text class',2020-10-12T08:47:03Z,2020-10-12T12:45:16Z,,,
7723,b'T5: Finetuning the language modelling objective on a  new dataset',2020-10-12T08:34:21Z,2020-12-20T13:34:45Z,wontfix,,
7722,b'Create Model-card for LIMIT-BERT',2020-10-12T07:46:38Z,2020-10-14T16:56:13Z,model card,,
7721,b'can u help me out with how to input custom data files in RAG retriever and the data format',2020-10-12T07:14:45Z,2020-12-19T08:30:33Z,wontfix,,
7720,b'Fix trainer callback',2020-10-12T03:22:30Z,2020-10-12T11:45:13Z,,,
7719,"b'wrong decoder_input_ids[:,0] for MarianMT models ?'",2020-10-12T00:49:02Z,2020-10-26T17:57:17Z,marian,,
7718,b'fixed typo in warning line 207.',2020-10-12T00:15:36Z,2020-10-12T07:58:59Z,,,
7717,b'The input training data files (multiple files in glob format).',2020-10-11T23:12:16Z,2020-10-12T11:44:02Z,,,
7716,"b""Hosted Inference API for Token Classification doesn't Highlight Tokens correctly""",2020-10-11T21:34:22Z,2021-01-10T13:54:12Z,wontfix,,
7715,"b'examples/rag: test coverage, tiny model'",2020-10-11T21:09:58Z,,"Help wanted, Tests, rag, Feature request",,
7714,b'Fix typo in all model docs',2020-10-11T20:51:31Z,2020-10-12T08:07:00Z,,,
7713,b'rag examples tests fail',2020-10-11T20:50:32Z,2020-10-14T15:35:00Z,,,
7712,"b'fix examples/rag imports, tests'",2020-10-11T20:49:41Z,2020-10-14T15:35:01Z,,,
7711,b'2 Deberta test failures',2020-10-11T20:47:05Z,2020-10-12T07:37:55Z,,,
7710,b'2 RAG test failures',2020-10-11T20:46:14Z,2020-10-13T12:34:23Z,,,
7709,b'[marian] Automate Tatoeba-Challenge conversion',2020-10-11T16:13:14Z,2020-10-12T16:24:25Z,marian,,
7708,b'Fine Tuning SciBERT NLI model',2020-10-11T13:55:33Z,2020-10-12T15:49:57Z,,,
7707,b'[NEW MODEL] Multilingual document embeddings: cT-LASER',2020-10-11T12:03:44Z,2020-12-19T08:30:32Z,"wontfix, New model",,
7706,b'run_tf_text_classification.py for custom dataset',2020-10-11T12:01:03Z,2020-10-12T12:45:16Z,,ValueError,"ValueError: too many values to unpack (expected 2)"
7705,b'Recording training loss and perplexity during training',2020-10-11T09:17:04Z,2020-12-19T08:30:31Z,wontfix,,
7704,b'SQuAD example docs inaccurately suggest settings for bert-large-uncased on a single V100',2020-10-11T07:51:27Z,2020-12-19T06:32:03Z,wontfix,,
7703,b'Corrected typo: maked \xe2\x86\x92 masked',2020-10-11T06:31:07Z,2020-10-11T20:45:01Z,,,
7702,b'Trainer callback breaks old code',2020-10-11T02:39:46Z,2020-10-12T11:45:13Z,,AttributeError,"AttributeError: 'NoneType' object has no attribute 'close'"
7701,b'Strange error while using the `LongformerForMultipleChoice`',2020-10-11T01:59:49Z,2020-12-20T13:34:44Z,wontfix,"IndexError, ValueError","IndexError: Target 1 is out of bounds.ValueError: 2 expected but found 1"
7700,b'GPT2DoubleHeadsModel documentation example question (error in documentation)?',2020-10-10T22:04:30Z,2020-12-19T06:32:01Z,wontfix,,
7699,b'Fix check for xla in PreTrainedModel.save_pretrained()',2020-10-10T19:42:56Z,2020-10-12T08:10:18Z,,,
7698,b'MLflow Trainer Callback',2020-10-10T19:22:36Z,2020-10-26T13:41:58Z,,,
7697,"b""tokenizers dependency warning: `transformers 3.3.1 has requirement tokenizers==0.8.1.rc2, but you'll have tokenizers 0.9.0`""",2020-10-10T16:26:13Z,2020-10-15T09:08:49Z,,`ERROR,"`ERROR: transformers 3.3.1 has requirement tokenizers==0.8.1.rc2, but you'll have tokenizers 0.9.0 which is incompatible.`"
7696,"b'Minor spelling corrections in docstrings. ""information"" is uncountable in English and has no plural.'",2020-10-10T12:31:31Z,2020-10-12T10:09:21Z,,,
7695,b'save_pretrained() does not check if xla is available',2020-10-10T12:29:20Z,2020-10-12T08:10:18Z,,ModuleNotFoundError,"ModuleNotFoundError: No module named 'torch_xla'"
7694,b'Fix docstring in AutoModel class',2020-10-10T12:26:47Z,2020-10-11T01:08:09Z,,,
7693,"b'How to get the word embedding  after pre-training ? for example, a embedding matrix'",2020-10-10T09:51:42Z,2020-12-20T13:34:37Z,wontfix,,
7692,b'Fail to run text classification example with run_tf_text_classification',2020-10-10T07:39:33Z,2021-03-06T00:17:11Z,wontfix,tensorflow.python.framework.errors_impl.NotFoundError,"tensorflow.python.framework.errors_impl.NotFoundError:  Resource localhost/_AnonymousVar110/N10tensorflow3VarE does not exist."
7691,b'Seq2Seq Example with Bart not Saving Best Model',2020-10-10T02:35:33Z,2020-10-17T01:56:38Z,,pytorch_lightning.utilities.exceptions.MisconfigurationException,"pytorch_lightning.utilities.exceptions.MisconfigurationException: ckpt_path is ""best"", but ModelCheckpoint is not configured to save the best model"
7690,b'RAG Tokenizer erroring out',2020-10-09T23:23:17Z,2020-10-13T12:34:23Z,,TypeError,"TypeError: unhashable type: 'dict'"
7689,b'Fix flaky test in test_trainer',2020-10-09T23:20:51Z,2020-10-10T00:01:16Z,,,
7688,b'Adds license information for default and distilbert models',2020-10-09T23:15:36Z,2020-10-10T07:55:12Z,model card,,
7687,b'Fix title level in Blenderbot doc',2020-10-09T23:09:56Z,2020-10-09T23:24:11Z,,,
7686,"b'When downloading RAG dpr indexes,  there is a pickle file loading error'",2020-10-09T23:02:28Z,2020-10-11T16:21:15Z,,"_pickle.UnpicklingError, OSError","_pickle.UnpicklingError: pickle data was truncatedOSError: Failed to interpret file <_io.BufferedReader name='.cache/huggingface/datasets/downloads/cd4183aaa482e0e3724cb8b2efafc6c762914aabed38c16a41f922ff7d5e90f9'> as a pickle"
7685,b'Using PaddingStrategy and TruncationStrategy throws an UnboundLocalError in tokenizers',2020-10-09T18:50:40Z,2020-12-20T13:34:38Z,wontfix,UnboundLocalError,"UnboundLocalError: local variable 'padding_strategy' referenced before assignment"
7684,b'Error with running run_language_modeling.py on GCP TPU',2020-10-09T16:37:06Z,2020-10-12T13:38:08Z,,,
7683,b'Gpt1 for sequence classification',2020-10-09T15:57:05Z,2020-10-13T09:06:16Z,,,
7682,b'Fine-tuning',2020-10-09T15:36:47Z,2020-12-24T11:26:54Z,wontfix,,
7681,b'Delete extra test file in repo root',2020-10-09T15:13:05Z,2020-10-09T15:16:36Z,,,
7680,b'Better links for models in README and doc index',2020-10-09T15:01:42Z,2020-10-09T15:17:17Z,,,
7679,b'TFEncoderDecoder',2020-10-09T14:53:19Z,2020-12-19T06:31:56Z,wontfix,,
7678,b'Fix dataset cardinality',2020-10-09T13:45:05Z,2020-10-09T14:38:26Z,,,
7677,b'Batch and smart batch support for pipelines.',2020-10-09T13:16:50Z,2020-12-19T08:30:34Z,wontfix,,
7676,"b""TFTrainer doesn't work""",2020-10-09T12:17:44Z,2020-10-12T08:17:11Z,,"tensorflow.python.framework.errors_impl.InvalidArgumentError, InternalError","tensorflow.python.framework.errors_impl.InvalidArgumentError: stream is uninitialized or in an error state [Op:DestroyResourceOp]InternalError: RET_CHECK failure (platforms/xla/service/jellyfish/bounds_check.cc:427) allocation_size_words <= std::numeric_limits<int32>::max() "
7675,b'Add FAVOR+ / Performer attention',2020-10-09T11:51:41Z,,New model,,
7674,b'Correctly tokenize sentence pairs',2020-10-09T09:31:14Z,2021-03-06T00:17:13Z,wontfix,,
7673,b'squad data preprocessor error (list index out of range) while finetuning bert on squad 1.1',2020-10-09T06:55:15Z,2020-12-19T06:31:49Z,wontfix,IndexError,"IndexError: list index out of range"
7672,b'[pegasus] Faster tokenizer tests',2020-10-09T05:05:04Z,2020-10-09T15:10:33Z,,,
7671,b'fix nn.DataParallel compatibility with PyTorch 1.5',2020-10-09T04:13:55Z,2020-10-09T09:15:09Z,,,
7670,b'[s2s] Switch README urls to cdn',2020-10-09T00:55:53Z,2020-10-09T01:22:22Z,,,
7669,b'Update XLM-RoBERTa pretrained model details',2020-10-08T20:01:12Z,2020-10-09T09:16:59Z,,,
7668,b'Default Model Licenses',2020-10-08T19:22:23Z,2020-12-25T09:25:03Z,wontfix,*Note,"*Note: table has been updated based on this discussion.*"
7667,b'Add multi-class processor to apply categorical classification',2020-10-08T18:36:44Z,2020-10-09T03:22:32Z,,,
7666,b'Clear up confusing translation pipeline task naming',2020-10-08T18:28:17Z,2020-12-19T06:31:48Z,wontfix,,
7665,b'tokenizer_bert.py not call _clean_text?',2020-10-08T18:24:20Z,2020-10-09T12:07:29Z,,,
7664,b'TF Slow test CI',2020-10-08T18:09:22Z,2020-12-19T06:31:58Z,"wontfix, Tests",,
7663,b'2 slow TF T5 common tests failing on master',2020-10-08T18:07:56Z,2020-12-19T06:31:48Z,"wontfix, TensorFlow",,
7662,b'loss.backward() being called twice in Trainer._training_step()',2020-10-08T17:31:07Z,2020-12-20T13:34:36Z,wontfix,RuntimeError,"RuntimeError: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time."
7661,b'[pseudo] Switch URLS to CDN',2020-10-08T17:09:03Z,2020-10-08T18:12:39Z,,,
7660,b'[broken] tf generate: use model_kwargs',2020-10-08T17:04:10Z,2021-03-06T00:17:14Z,wontfix,,
7659,b'[Dependencies|tokenizers] Make both SentencePiece and Tokenizers optional dependencies',2020-10-08T14:58:01Z,2020-10-18T18:51:25Z,,,
7658,b'Green tests: update torch-hub test dependencies (add protobuf and pin tokenizer 0.9.0-RC2)',2020-10-08T10:58:10Z,2020-10-08T11:21:15Z,,,
7657,b'SqueezBert link gives a 404 error',2020-10-08T10:26:33Z,2020-10-09T15:17:16Z,,,
7656,b'T5 Beam search num_beans always equals 1',2020-10-08T09:17:38Z,2020-10-08T12:33:03Z,,,
7655,b'Eval_loss in prediction is very high : transformers/examples/token-classification/run_ner.py ',2020-10-08T07:10:25Z,2021-03-06T00:17:18Z,wontfix,,
7654,b'output probabilities of generated sequences in generate function',2020-10-08T05:57:12Z,2021-04-26T15:02:55Z,wontfix,,
7653,b'[pseudolabels] cleanup markdown table',2020-10-08T02:22:47Z,2020-10-08T03:04:19Z,,,
7652,b'Fix 3 failing slow bart/blender tests',2020-10-08T01:38:15Z,2020-10-08T02:05:03Z,,,
7651,b'Fix Failing Slow tests',2020-10-08T01:21:14Z,2020-10-08T02:05:03Z,,,
7650,b'Import integration libraries first',2020-10-07T23:16:13Z,2020-10-09T16:13:23Z,,,
7649,b'setup of Trainer class for distributed trainning',2020-10-07T22:34:37Z,2021-01-02T04:26:05Z,wontfix,,
7648,b'does tokenizer support emoji?',2020-10-07T20:00:56Z,2020-10-09T11:12:53Z,,,
7647,b'Project: Gather summarization datasets and try to replicate pegasus results on them',2020-10-07T19:39:16Z,2020-10-26T17:54:33Z,"Help wanted, pegasus",,
7646,b'Openai gpt for classification',2020-10-07T18:50:31Z,2020-10-09T15:36:56Z,model card,,
7645,b'Fix integration tests of DeBERTa',2020-10-07T18:24:37Z,2020-10-16T06:49:14Z,,,
7644,b'NER pipeline documentation example failing',2020-10-07T18:17:16Z,2020-12-19T06:31:56Z,wontfix,,
7643,b'quick question about `BertForMaskedLM`',2020-10-07T17:26:34Z,2020-12-19T06:31:44Z,wontfix,,
7642,b'Fix RobertaForCausalLM docs',2020-10-07T16:06:09Z,2020-10-08T12:36:01Z,,,
7641,b'[s2s] configure lr_scheduler from command line',2020-10-07T15:54:01Z,2020-10-08T17:06:36Z,,,
7640,b'Create README.md for IsRoBERTa language model',2020-10-07T14:02:33Z,2020-10-07T20:46:04Z,model card,,
7639,b'[s2s] release pseudolabel links and instructions',2020-10-07T13:51:01Z,2020-10-07T15:20:44Z,,,
7638,"b""error AttributeError: 'tuple' object has no attribute 'logits'""",2020-10-07T13:34:40Z,2020-10-07T21:32:59Z,,AttributeError,"AttributeError: 'tuple' object has no attribute 'logits'     "
7637,"b'ValueError(""The training dataset must have an asserted cardinality"") when running run_tf_text_classification.py'",2020-10-07T13:10:50Z,2020-10-09T14:38:26Z,,ValueError,"ValueError: The training dataset must have an asserted cardinality"
7636,b'Model templates',2020-10-07T12:26:07Z,2020-11-12T21:47:12Z,,,
7635,"b""ImportError: cannot import name 'RobertaLMHeadModel'""",2020-10-07T11:28:01Z,2020-10-08T12:36:01Z,,,
7634,"b""ImportError: cannot import name 'RobertaLMHeadModel'""",2020-10-07T11:25:28Z,2020-10-07T11:26:19Z,,,
7633,b'How to get cross attention for bert when config.add_cross_attention is True',2020-10-07T09:50:24Z,2020-12-11T08:25:10Z,wontfix,,
7632,b'Unique names for dataset cache for each tokenizer ',2020-10-07T09:42:14Z,2020-12-19T06:31:43Z,wontfix,,
7631,b'Is there a fine-tuning script for DPR?',2020-10-07T09:29:07Z,2020-12-19T06:31:47Z,wontfix,,
7630,b'Add GPT2 to sequence classification auto model',2020-10-07T09:18:38Z,2020-10-07T09:20:06Z,,,
7629,b'Update model card - Fix arxiv link',2020-10-07T06:30:53Z,2020-10-07T20:36:09Z,model card,,
7628,b'The newly added config decoder_start_token_id for bart-base model is wrong?',2020-10-07T06:02:04Z,2020-10-07T13:27:42Z,,,
7627,"b""Added sampler'set_epoch when use distributed training""",2020-10-07T04:57:18Z,2020-12-15T22:50:34Z,wontfix,,
7626,b'Unable to pass encoder_outputs to generate calls',2020-10-07T04:32:13Z,2021-04-26T15:02:56Z,wontfix,,
7625,b'Create README.md',2020-10-07T00:48:04Z,2020-10-21T12:29:40Z,model card,,
7624,b'Free Inference API Not Accessible',2020-10-07T00:29:11Z,2020-10-07T17:38:21Z,,,
7623,b'Implement PyTorch and/or TensorFlow sequence classification architectures for causal language models',2020-10-06T21:42:07Z,2020-12-01T08:49:27Z,Good First Issue,,
7622,b'Implement a TF2 version of `GPT2ForSequenceClassification`',2020-10-06T21:36:44Z,2020-12-07T15:58:38Z,Good First Issue,,
7621,b'[No merge] TF integration testing',2020-10-06T21:26:14Z,2020-11-10T19:02:34Z,,,
7620,"b""Downloading DPR model ('facebook/dpr-ctx_encoder-single-nq-base')""",2020-10-06T21:02:23Z,2020-12-19T06:31:42Z,wontfix,,
7619,b'Enhance TFTrainer.save_model()',2020-10-06T19:33:26Z,2021-05-19T15:09:50Z,,,
7618,b'position_ids parameter cannot work with past parameter for GPT2Model during batch inference',2020-10-06T16:32:09Z,2020-10-07T01:56:45Z,,,
7617,"b""OSError: Can't load config for saved_model when deploying on EC2.""",2020-10-06T16:29:38Z,2021-05-10T15:03:04Z,,OSError,"OSError: Can't load config for 'model/final_model'. Make sure that:"
7616,b'Fix wrong reference name/filename in docstring of `SquadProcessor`',2020-10-06T16:02:11Z,2020-10-06T22:02:30Z,,,
7615,b'Feature Request: Support training/evaluation on Squad-format (json) files in SquadDataset for quick Squad fine-tuning',2020-10-06T14:59:17Z,2020-12-19T06:31:42Z,wontfix,,
7614,b'Feature Request: Support training and evaluating on Squad-format (json) files in SquadDataset for easy Squad fine-tuning',2020-10-06T14:57:36Z,2020-10-06T15:00:03Z,,,
7613,b'SquadProcessor: Wrong reference name/filename in docstring',2020-10-06T14:10:49Z,2020-10-06T22:02:30Z,,,
7612,b'updating modelcard with training dataset information.',2020-10-06T12:23:05Z,2020-10-06T12:46:56Z,model card,,
7611,b'typo fix',2020-10-06T11:26:47Z,2020-10-06T13:32:52Z,model card,,
7610,b'Fix tokenizer UnboundLocalError when padding is set to PaddingStrategy.MAX_LENGTH',2020-10-06T10:54:31Z,2020-10-06T22:16:01Z,,,
7609,b'Tokenizer: UnboundLocalError with PaddingStrategy MAX_LENGTH',2020-10-06T10:48:24Z,2020-10-06T22:16:01Z,,`UnboundLocalError,"`UnboundLocalError: local variable 'padding_strategy' referenced before assignment`"
7608,b'Ability to pre-train BART model',2020-10-06T10:29:08Z,2020-12-19T06:31:39Z,wontfix,,
7607,b'Create README.md (LEGAL-BERT Model card)',2020-10-06T09:34:38Z,2020-10-06T12:46:18Z,model card,,
7606,b'Add ProtT5-XL-BFD model card',2020-10-06T09:01:40Z,2020-10-06T10:19:22Z,model card,,
7605,b'TensorFlow training/inference optimization',2020-10-06T08:54:23Z,2020-11-09T13:02:03Z,,,
7604,b'way to make inference Zero Shot pipeline faster?',2020-10-06T07:37:22Z,2020-10-06T13:41:39Z,,,
7603,b'Added model cards for Tagalog BERT models',2020-10-06T06:35:41Z,2020-10-07T20:49:21Z,model card,,
7602,b'RAG : Can we fine-tune RAG with update frequency method similar to Fairseq framework?',2020-10-06T05:50:57Z,2020-10-06T21:45:31Z,,,
7601,b'Does tokenizer.from_pretrained tokenize text on CPU even a GPU is available?',2020-10-06T03:58:28Z,2020-10-06T12:44:25Z,,,
7600,"b""TFBertMode.pre_trained('bert-base-uncased') --> OSError  """,2020-10-06T02:02:28Z,2020-10-06T06:55:31Z,,OSError,"OSError: file bert-base-uncased/config.json not found"
7599,b'Support T5 Distillation w/hidden state supervision',2020-10-06T00:58:46Z,2020-10-06T01:31:49Z,,,
7598,b'Docker GPU Images: Add NVIDIA/apex to the cuda images with pytorch',2020-10-05T22:14:41Z,2020-10-06T13:23:32Z,,,
7597,b'Enhance TFTrainer.save_model()',2020-10-05T21:28:27Z,2020-10-06T19:36:17Z,,,
7596,b'Trainer callbacks',2020-10-05T20:47:48Z,2020-10-07T14:50:22Z,,,
7595,b'change return dicitonary for DataCollatorForNextSentencePrediction from masked_lm_labels to labels',2020-10-05T19:21:13Z,2020-10-06T13:12:04Z,,,
7594,b'RagTokenForGeneration.from_pretrained fails while running demo script',2020-10-05T19:03:02Z,2020-11-25T13:18:59Z,,"ValueError, tarfile.InvalidHeaderError, tarfile.ReadError, RuntimeError, OSError","ValueError: invalid literal for int() with base 8: 'del.embe'tarfile.InvalidHeaderError: invalid headertarfile.ReadError: invalid headerRuntimeError: /homes/thielk/.cache/torch/transformers/06fe449ffe41cbe16aeb1f5976989313464a3c44a605e9a8b91bf6440dfa6026.696574d8c17eafbac08f43f01e951252057f8feb133b64a33b76d4c47d65367a is a zip archive (did you mean to use torch.jit.load()?)OSError: Unable to load weights from pytorch checkpoint file. If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True."
7593,b'[bart] fix config.classif_dropout',2020-10-05T18:09:25Z,2020-10-06T15:33:52Z,,,
7592,b'Using `-1` to mask the loss for the token is deprecated. Please use `-100` instead. ',2020-10-05T17:12:27Z,2020-10-07T06:24:15Z,,,
7591,b'BartConfig saving and loading inconsistency',2020-10-05T17:09:47Z,2020-10-06T15:33:52Z,,,
7590,b'Update README.md',2020-10-05T16:02:34Z,2020-10-07T20:37:10Z,model card,,
7589,b'run_language_modeling.py TPU issue during evaluation',2020-10-05T15:51:40Z,2020-12-19T06:31:51Z,wontfix,,
7588,b'[makefile] check only .py files',2020-10-05T15:46:55Z,2020-10-06T09:25:21Z,,,
7587,b'Fix squeezebert docs',2020-10-05T15:26:08Z,2020-10-06T10:22:04Z,,,
7586,b'Documentation framework toggle should stick',2020-10-05T14:58:21Z,2020-10-05T15:23:57Z,,,
7585,b'Documentation fixes',2020-10-05T14:42:29Z,2020-10-05T15:01:04Z,,,
7584,"b""XLNet evaluation fails if the size of evaluation set can't be divided by a given evaluation batch size""",2020-10-05T14:37:15Z,2020-11-25T21:55:00Z,,RuntimeError,"RuntimeError: invalid argument 0: Sizes of tensors must match except in dimension 0. Got 40 and 64 in dimension 1 at /opt/conda/conda-bld/pytorch_1579061855666/work/aten/src/THC/generic/THCTensorMath.cu:71"
7583,"b""RagRetriever.from_pretrained doesn't get another cache_dir. """,2020-10-05T14:28:12Z,2020-12-19T06:31:40Z,wontfix,,
7582,b'[TF generation] Fix typo',2020-10-05T14:21:34Z,2020-10-06T10:47:17Z,,,
7581,b'Create README.md',2020-10-05T13:57:53Z,2020-10-07T20:37:40Z,model card,,
7580,b'Expand test to locate flakiness',2020-10-05T13:45:00Z,2020-10-05T13:45:48Z,,,
7579,b'make modified_only_fixup complains about non .py files',2020-10-05T13:41:00Z,2020-10-06T09:25:21Z,,,
7578,"b""RobertaTokenizer.get_special_tokens_mask doesn't check for all special tokens, only for the sep and cls tokens""",2020-10-05T12:33:46Z,2020-12-13T01:22:42Z,wontfix,,
7577,b'Add support to provide initial tokens to decoder of encoder-decoder type models',2020-10-05T12:02:00Z,2020-10-19T06:56:09Z,,,
7576,b'Trainer evaluate returns empty dictionary',2020-10-05T12:00:46Z,2020-11-23T13:18:11Z,,,
7575,b'docs(pretrained_models): fix num parameters',2020-10-05T11:18:58Z,2020-10-05T11:50:57Z,,,
7574,b'Some weights of GPT2DoubleHeadsModel were not initialized from the model checkpoint at gpt2 and are newly initialized',2020-10-05T11:13:10Z,2020-10-05T11:47:32Z,,,
7573,b'[model_card] bert-base-5lang-cased',2020-10-05T10:09:05Z,2020-10-07T20:38:14Z,model card,,
7572,b'Finetuning T5: Keyword arguments not recognized.',2020-10-05T10:01:08Z,2020-12-13T01:22:40Z,wontfix,,
7571,b'Sequence Classification One-Hot Encoded Data',2020-10-05T09:49:48Z,2020-10-05T10:19:00Z,,,
7570,b'Import error for MarianMTModel',2020-10-05T09:45:11Z,2020-12-13T01:22:41Z,wontfix,ImportError,"ImportError: cannot import name 'MarianMTModel' from 'transformers' (C:\Users\PRINCE\Anaconda3\envs\Brian's Enviroment\lib\site-packages\transformers\__init__.py)"
7569,b'Add Electra unexpected keys',2020-10-05T08:42:18Z,2020-10-05T08:49:40Z,,,
7568,b'[Model card] Java Code Summarizer model',2020-10-05T03:53:02Z,2020-10-05T08:49:17Z,model card,,
7567,b'Is training distilbert with TPU supported yet?',2020-10-05T03:11:21Z,2020-10-05T10:22:55Z,,,
7566,b'Trainer incorrectly checks pytorch version',2020-10-04T18:22:04Z,2020-12-10T19:09:07Z,wontfix,,
7565,b'Two slow deberta test failures',2020-10-04T16:43:57Z,2020-10-16T06:49:14Z,Tests,,
7564,b'Update normalising method in oneshot classifier',2020-10-04T11:21:19Z,2020-10-10T11:22:42Z,,,
7563,b'Error Loading Gpt-2 model after training from scratch.',2020-10-04T04:04:54Z,2020-10-15T09:02:44Z,,,
7562,b'Output global_attentions in Longformer models',2020-10-04T01:44:37Z,2020-11-05T20:10:45Z,,,
7561,b'Moved feature generation into getitem to save ram',2020-10-03T21:46:56Z,2020-12-15T22:50:36Z,wontfix,,
7560,b'Remove labels from the RagModel example',2020-10-03T21:00:36Z,2020-10-04T21:39:24Z,,,
7559,"b'Is this realy a list or a Dict[str, int]? I think the docstring might be wrong because in the model json file it is stored as a dict.'",2020-10-03T19:03:04Z,2020-10-05T15:01:04Z,,,
7558,b'[Model card] SinhalaBERTo model.',2020-10-03T18:12:09Z,2020-10-07T20:40:53Z,model card,,
7557,b'Enable debug with TF2 and eager execution',2020-10-03T17:38:48Z,2020-10-06T06:50:47Z,,,
7556,b'Problem with automatic best model loading.',2020-10-03T15:24:35Z,2020-10-05T12:19:25Z,,ZeroDivisionError,"ZeroDivisionError: integer division or modulo by zero"
7555,b'Update Code example according to deprecation of AutoModeWithLMHead',2020-10-03T12:27:00Z,2020-10-05T12:21:22Z,model card,,
7554,"b'RAG: error in outputs = model(input_ids=input_ids, labels=input_dict[""labels""])'",2020-10-03T12:08:12Z,2020-10-04T21:39:24Z,,TypeError,"TypeError: forward() got an unexpected keyword argument 'labels'**"
7553,b'[model_card] bert-base-5lang-cased',2020-10-03T11:35:48Z,2020-10-05T10:24:30Z,model card,,
7552,b'Add batch inferencing support for GPT2LMHeadModel',2020-10-03T10:48:36Z,2020-10-14T11:40:25Z,,,
7551,"b""RAG: NameError: name 'load_dataset' is not defined""",2020-10-03T09:12:00Z,2020-10-07T08:53:53Z,,NameError,NameError: name 'load_dataset' is not defined**
7550,b'Problem with Finetuned GPT-2',2020-10-03T06:49:23Z,2020-10-05T09:41:00Z,,,
7549,b'Incorrect tokenization with tokens added using tokenizer.add_tokens()',2020-10-02T23:32:53Z,2020-12-13T01:22:40Z,wontfix,,
7548,b'Longformer2Roberta: global_attention_mask is never used',2020-10-02T23:24:45Z,2021-01-03T13:23:18Z,wontfix,,
7547,b'Converting Tensorflow checkpoint to Pytorch not work for TF models downloaded using TFAutoModel.from_pretrained()',2020-10-02T22:14:59Z,2020-10-06T22:07:46Z,,,
7546,b'[s2s] label smoothing loss should be normalized',2020-10-02T19:59:54Z,2021-01-18T07:02:19Z,"Help wanted, wontfix",,
7545,b'[s2s] fix lockfile and peg distillation constants',2020-10-02T19:50:50Z,2020-10-02T19:58:15Z,,,
7544,"b'Create Model Card For ""abhilash1910/french-roberta"" Model'",2020-10-02T17:07:59Z,2020-10-07T20:35:28Z,model card,,
7543,b'Seq2SeqTrainer: missing features',2020-10-02T16:25:05Z,2020-10-08T17:06:35Z,,,
7542,b'Allow nested tensors in predicted logits',2020-10-02T16:09:06Z,2020-10-05T10:33:15Z,,,
7541,b'T5: forward and generate produce different results even for greedy decoding of a single token',2020-10-02T16:04:13Z,2020-12-07T00:11:44Z,"wontfix, t5",,
7540,b'Difference between CLS hidden state and pooled_output',2020-10-02T15:03:43Z,2021-01-10T13:54:20Z,wontfix,,
7539,b'Trainer fails to correctly tackle XLNetForSequenceClassification outputs',2020-10-02T14:44:16Z,2020-10-05T10:33:15Z,,AttributeError,"AttributeError: 'tuple' object has no attribute 'detach'"
7538,b'T5 supervised denoising task',2020-10-02T14:42:10Z,2020-12-25T09:25:01Z,wontfix,,
7537,b'Allow soft dependencies in the namespace with ImportErrors at use',2020-10-02T13:27:47Z,2020-10-05T13:12:05Z,,,
7536,b'RAG model card code not working in Colab',2020-10-02T11:53:05Z,2020-10-02T13:48:41Z,,NameError,"NameError: name 'load_dataset' is not defined"
7535,"b""TypeError: '<' not supported between instances of 'NamedSplit' and 'NamedSplit' when running run_tf_text_classification.py""",2020-10-02T10:37:13Z,2020-10-02T15:28:31Z,,TypeError,"TypeError: '<' not supported between instances of 'NamedSplit' and 'NamedSplit'"
7534,"b""The links to examples on the website don't work""",2020-10-02T10:13:52Z,2020-12-13T01:22:41Z,wontfix,,
7533,b'Add early stopping to trainer_tf.py',2020-10-02T10:08:07Z,2021-04-19T16:18:52Z,,,
7532,b'[s2s] add config params like Dropout in Seq2SeqTrainingArguments',2020-10-02T08:30:44Z,2020-10-04T16:42:31Z,,,
7531,b'Cammembert fine tuning from checkpoint',2020-10-02T07:56:38Z,2021-03-23T11:01:09Z,,UnicodeDecodeError,"UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfe in position 51: invalid start byte"
7530,b'ELECTRA - some weights are not loaded',2020-10-02T06:58:41Z,2020-10-05T08:49:39Z,,,
7529,b'[GPT-2] How many columns in LM model wte layer are positional embeddings?',2020-10-01T23:04:47Z,2020-10-05T11:53:17Z,,,
7528,b'QA pipeline fails with long context. ',2020-10-01T22:07:51Z,2020-10-01T23:35:10Z,,ValueError,"ValueError: expected sequence of length 384 at dim 1 (got 379)"
7527,b'Check and update model list in index.rst automatically',2020-10-01T21:45:55Z,2020-10-05T13:40:45Z,,,
7526,b'Almost Have Model Parallelism Working on GPT2 Fine-Tuning',2020-10-01T21:29:17Z,2020-11-24T18:26:39Z,Model Parallel,RuntimeError,"RuntimeError: expected device cuda:3 but got device cuda:0 (compute_types at ..\aten\src\ATen\native\TensorIterator.cpp:246)"
7525,b'Fix post_init of some TrainingArguments',2020-10-01T20:59:44Z,2020-10-05T13:19:17Z,,,
7524,b'Training loss suddenly increases and stays the same',2020-10-01T19:53:06Z,2020-12-12T20:44:49Z,wontfix,,
7523,"b'Cleanup documentation for BART, Marian, MBART and Pegasus'",2020-10-01T19:19:59Z,2020-10-05T08:22:13Z,,,
7522,b'[s2s] Adafactor support for builtin trainer',2020-10-01T19:09:31Z,2020-10-01T21:27:46Z,,,
7521,"b'[s2s] trainer scripts: Remove --run_name, thanks sylvain!'",2020-10-01T18:49:14Z,2020-10-01T21:18:48Z,,,
7520,b'MultiGPU Trainer: each processes uses more memory than 1 GPU job',2020-10-01T18:43:22Z,2020-10-02T00:39:22Z,Distributed Training / Models,,
7519,b'XLNet finetuning',2020-10-01T17:41:38Z,2020-12-12T20:44:48Z,wontfix,RuntimeError,"RuntimeError: Trying to create tensor with negative dimension -1: [-1, 768]"
7518,b'Fix seq2seq example test',2020-10-01T17:23:29Z,2020-10-01T18:13:29Z,,,
7517,"b""Overflow error: Can't convert negative value to unsigned it [RAG Model]""",2020-10-01T16:57:56Z,2020-11-10T03:29:07Z,,OverflowError,"OverflowError: can't convert negative value to unsigned int"
7516,"b""huggingface transformer running on CPU behind celery/redis doens't work (but works by itself)""",2020-10-01T16:57:50Z,2020-10-01T16:58:14Z,,,
7515,b'[s2s] fix nltk pytest race condition with FileLock',2020-10-01T16:34:59Z,2020-10-01T16:51:10Z,,,
7514,b'[Longformer] Output both local attentions and global attentions when `output_attentions=True` -> Good Second Issue',2020-10-01T16:33:52Z,2020-11-05T20:10:44Z,Good Second Issue,,
7513,b'[Attention Mask] Fix data type',2020-10-01T16:13:40Z,2020-10-01T16:15:42Z,,,
7512,b'[XLNet] attention_mask / input_mask - Why two `attention_mask` inputs?',2020-10-01T15:34:33Z,2020-12-12T20:44:42Z,wontfix,,
7511,b'[Transfo-XL] Impossible to pass `attention_mask` to model',2020-10-01T15:31:21Z,2020-12-12T20:44:41Z,wontfix,,
7510,"b'[Reformer, Longformer, Roberta, GPT2, CTRL] attention_mask should be at second argument'",2020-10-01T15:29:34Z,2020-12-12T20:44:41Z,wontfix,,
7509,b'[examples/s2s] clean up finetune_trainer',2020-10-01T15:21:35Z,2020-10-01T16:19:29Z,,,
7508,b'Fix Ray Tune progress_reporter kwarg',2020-10-01T14:04:27Z,2020-10-01T14:34:32Z,,,
7507,b'Report Tune metrics in final evaluation',2020-10-01T13:26:26Z,2020-10-01T13:52:37Z,,,
7506,b'configuration_utils: fix handling of `id2labels`',2020-10-01T13:04:56Z,2020-10-16T08:50:40Z,,,
7505,b'added script for fine-tuning roberta for sentiment analysis task',2020-10-01T13:04:26Z,2020-10-05T07:57:16Z,,,
7504,b'added script for fine-tuning roberta for sentiment analysis task',2020-10-01T13:01:42Z,2020-10-01T13:04:11Z,,,
7503,b'Turning the SQuAD dataset class into an iterator to save ram and redistribute time',2020-10-01T12:26:14Z,2020-12-12T20:44:37Z,wontfix,,
7502,b'Functionality to pass first few tokens as input to the decoder in T5 model',2020-10-01T12:20:47Z,2020-10-19T06:56:09Z,,,
7501,b'Add GPT2ForSequenceClassification based on DialogRPT',2020-10-01T11:57:48Z,2020-10-06T21:31:22Z,,,
7500,"b""Trucated Outputs while finetuning 'bart-base' on XSUM [Summarization Task]""",2020-10-01T11:22:28Z,2020-10-16T13:42:28Z,,,
7499,b'german distilbert not available?',2020-10-01T11:03:38Z,2020-10-01T13:09:53Z,,,
7498,b'Update README.md',2020-10-01T10:23:52Z,2020-10-01T11:42:07Z,,,
7497,b'How to generate data using beam search from a custom gpt2 model?',2020-10-01T06:45:55Z,2020-12-13T01:22:38Z,wontfix,,
7496,b'BertforSequenceClassification MSELoss() without normalizing using sigmoid/softmax',2020-10-01T06:43:29Z,2021-04-26T15:02:57Z,wontfix,,
7495,b'quick questions about the `BertModelLMHeadModel`.',2020-10-01T02:17:15Z,2020-12-12T20:44:46Z,wontfix,,
7494,b'Is the multiple-choice head for the pre-trained `LongformerForMultipleChoice` model pre-trained?',2020-10-01T02:15:06Z,2020-10-05T07:51:27Z,,,
7493,"b""Sharing Microsoft's DialogRPT (new dialog ranking model)""",2020-10-01T00:16:55Z,2020-10-07T09:20:06Z,New model,,
7492,"b""`run_squad_trainer` doesn't actually use a Rust tokenizer + errors in `squad_convert_example_to_features` when using a Rust tokenizer""",2020-09-30T23:47:44Z,2020-12-24T20:34:53Z,wontfix,,
7491,b'Update README.md',2020-09-30T23:06:15Z,2020-10-01T12:50:08Z,model card,,
7490,b'Clean the Trainer state',2020-09-30T21:19:45Z,2020-10-01T17:07:05Z,,,
7489,b'Use of global attention of Longformer when generating',2020-09-30T21:06:19Z,2021-01-03T13:23:17Z,wontfix,,
7488,b'[s2s] fix kwargs style',2020-09-30T20:59:42Z,2020-09-30T21:00:07Z,,,
7487,b'[s2s] Fix t5 warning for distributed eval',2020-09-30T19:18:21Z,2020-09-30T20:58:03Z,,,
7486,b'Using BERT for spelling correction',2020-09-30T19:08:27Z,2020-12-12T20:44:33Z,wontfix,,
7485,b'Tenosrflow Loading the saved Model For GPT2',2020-09-30T17:35:15Z,2020-12-12T20:44:35Z,wontfix,,
7484,b'Bump isort version.',2020-09-30T17:32:42Z,2020-09-30T17:44:59Z,,,
7483,b'Add forgotten return_dict argument in the docs',2020-09-30T17:27:45Z,2020-10-01T08:41:30Z,,,
7482,b'Issue with Summary of the tasks - Named Entity Recognition in Docs',2020-09-30T16:30:44Z,2020-10-01T08:41:30Z,,AttributeError,"AttributeError: 'tuple' object has no attribute 'logits'"
7481,b'Minor dead code clean-up',2020-09-30T16:07:24Z,2020-11-29T09:00:45Z,,,
7480,b'Upload models using transformers-cli fails ',2020-09-30T15:26:14Z,2021-01-17T11:01:20Z,wontfix,"BrokenPipeError, urllib3.exceptions.ProtocolError, requests.exceptions.ConnectionError","BrokenPipeError: [Errno 32] Broken pipe        urllib3.exceptions.ProtocolError: ('Connection aborted.', BrokenPipeError(32, 'Broken pipe'))requests.exceptions.ConnectionError: ('Connection aborted.', BrokenPipeError(32, 'Broken pipe'))                                                                                                                                                          "
7479,b'Loading saved model not working',2020-09-30T15:15:07Z,2020-12-12T20:44:36Z,wontfix,,
7478,b'Alphabetize model lists',2020-09-30T14:35:02Z,2020-09-30T14:43:59Z,,,
7477,b'[s2strainer] fix eval dataset loading',2020-09-30T14:31:58Z,2020-09-30T16:39:13Z,,,
7476,b'RAG: Can we have a document that explains the fine-tuning mechanism?',2020-09-30T14:17:25Z,2020-10-03T12:54:23Z,,,
7475,b'Small QOL improvements to TrainingArguments',2020-09-30T13:53:30Z,2020-09-30T16:12:04Z,,,
7474,b'[Seq2Seq] Fix a couple of bugs and clean examples',2020-09-30T13:34:42Z,2020-10-01T15:38:51Z,,,
7473,b'Make transformers install check positive',2020-09-30T11:31:31Z,2020-09-30T11:44:41Z,,,
7472,b'Number of GPUs for multi-gpu',2020-09-30T10:52:49Z,2020-09-30T10:53:20Z,,,
7471,b'Fix LXMERT with DataParallel',2020-09-30T10:41:14Z,2020-09-30T10:41:25Z,,,
7470,b'Seq2SeqDataset: avoid passing src_lang everywhere',2020-09-30T07:06:09Z,2020-09-30T17:27:49Z,,,
7469,"b""fix the first chunk's lower triangle""",2020-09-30T06:59:52Z,2020-12-15T22:50:36Z,wontfix,,
7468,b'Create README.md',2020-09-30T06:26:58Z,2020-10-01T12:50:26Z,model card,,
7467,b'[s2sTrainer] test + code cleanup',2020-09-30T05:32:39Z,2020-10-01T04:33:02Z,,,
7466,"b""Seq2SeqTrainer: add a fast test that doesn't learn anything but can run on CPU""",2020-09-30T03:39:29Z,2020-12-11T08:20:34Z,wontfix,,
7465,b'RAG - reproducing RAG-Sequence QA score ',2020-09-30T02:35:39Z,2020-12-12T20:44:34Z,wontfix,,
7464,b'Remove config assumption in Trainer',2020-09-29T23:25:18Z,2020-09-30T13:03:26Z,,,
7463,b'Trainer should not modify its TrainingArguments',2020-09-29T21:37:04Z,2020-09-30T14:41:51Z,,,
7462,b'RAG - how to precompute custom document index?',2020-09-29T21:28:00Z,2020-12-24T11:26:50Z,wontfix,,
7461,b'Distributed Trainer: 2 little fixes',2020-09-29T21:17:22Z,2020-10-01T02:14:15Z,,,
7460,b'Seq2SeqTrainer Distributed: AttributeError and the RuntimeError',2020-09-29T21:16:49Z,2020-10-01T02:14:15Z,,AttributeError,"AttributeError: DistributedDataParallel has no attribute ""config"""
7459,b'Update README.md',2020-09-29T20:04:09Z,2020-10-01T12:51:26Z,model card,,
7458,b'Fix Trainer tests in a multiGPU env',2020-09-29T17:56:11Z,2020-09-29T18:06:42Z,,,
7457,b'Get a better error when check_copies fails',2020-09-29T17:33:30Z,2020-09-30T08:05:14Z,,,
7456,b'Catch import datasets common errors',2020-09-29T17:31:49Z,2020-09-29T17:42:10Z,,,
7455,b'Adding the Streamlit demo app code for the RAG model',2020-09-29T17:10:58Z,2021-04-25T15:04:49Z,,,
7454,b'Seq2seq example for T5 keeps on generating warning',2020-09-29T16:41:54Z,2020-09-30T17:27:49Z,,,
7453,b'Multi-GPU Testing setup',2020-09-29T14:29:19Z,2020-09-30T09:53:35Z,,,
7452,b'LayoutLM: add exception handling for bbox values',2020-09-29T12:45:12Z,2020-10-05T08:17:15Z,,,
7451,b'T5 unsupervised training',2020-09-29T12:42:13Z,2020-10-28T13:02:13Z,,,
7450,b'deleted',2020-09-29T10:28:30Z,2020-09-29T10:41:25Z,,,
7449,"b""What's the most straightforward way to initialise BertForSequenceClassification for different token rather than [CLS]?""",2020-09-29T09:42:47Z,2020-12-13T01:23:03Z,wontfix,,
7448,"b""v3.3.0 - Issue with name conflict in transformers & datasets - AttributeError: module 'datasets' has no attribute '__version__'""",2020-09-29T09:40:04Z,2020-09-29T18:32:32Z,,AttributeError,"AttributeError: module 'datasets' has no attribute '__version__'"
7447,b'Getting Bert Embeddings in Batch',2020-09-29T09:09:48Z,2020-12-13T01:22:58Z,wontfix,,
7446,b'Adding gradient checkpointing to GPT2',2020-09-29T08:17:29Z,2020-09-29T16:26:27Z,,,
7445,b'Add is_split_into_words as an argument to tokenize',2020-09-29T07:10:27Z,2020-09-29T15:21:04Z,,,
7444,b'Update README.md',2020-09-29T06:54:46Z,2020-09-29T07:18:02Z,model card,,
7443,b'Error training GPT-2 from scratch on Hindi',2020-09-29T06:15:41Z,2020-09-29T07:19:27Z,,TypeError,"TypeError: not a string"
7442,b'Setting up transformers/examples/seq2seq',2020-09-29T05:38:53Z,2020-12-12T20:44:43Z,wontfix,,
7441,"b""Faced the TypeError:forward() got an unexpected keyword argument 'output_all_encoded_layers'""",2020-09-29T03:53:44Z,2020-12-13T01:22:43Z,wontfix,TypeError,"TypeError: forward() got an unexpected keyword argument 'output_all_encoded_layers'"
7440,b'creating readme for bert-base-mongolian-uncased',2020-09-29T03:31:25Z,2020-10-01T12:45:23Z,model card,,
7439,b'Creating readme for bert-base-mongolian-cased',2020-09-29T03:28:15Z,2020-10-01T12:46:28Z,model card,,
7438,b'CUDA out of memory (ALBERT) - run_squad.py ignores --per_gpu_train_batch_size',2020-09-29T00:25:30Z,2020-09-29T08:58:39Z,,RuntimeError,"RuntimeError: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 0; 15.90 GiB total capacity; 15.01 GiB already allocated; 7.88 MiB free; 15.03 GiB reserved in total by PyTorch)"
7437,"b""RAG Retriever  (NameError: name 'load_dataset' is not defined   in retrieval_rag.py) """,2020-09-28T21:52:19Z,2020-12-13T01:22:54Z,wontfix,NameError,"NameError: name 'load_dataset' is not defined"
7436,b'Create README.md',2020-09-28T21:06:11Z,2020-09-28T22:25:25Z,model card,,
7435,b'[s2s] consistent output format across eval scripts ',2020-09-28T20:22:50Z,2020-09-29T03:20:04Z,,,
7434,b'Document new features of make fixup',2020-09-28T20:14:16Z,2020-09-29T07:56:58Z,,,
7433,b'Add a code of conduct',2020-09-28T20:11:28Z,2020-09-29T17:38:47Z,,,
7432,b'Fine-tune BERTForMaskedLM',2020-09-28T19:43:50Z,2020-12-13T01:23:04Z,wontfix,ValueError,"ValueError: Some specified arguments are not used by the HfArgumentParser: ['bert', 'bert-base-cased']"
7431,b'Add automatic best model loading to Trainer',2020-09-28T19:41:37Z,2020-09-29T14:41:19Z,,,
7430,"b'import error in version 3.3.0, conflict with local directory ""datasets""'",2020-09-28T17:59:00Z,2020-09-29T03:00:49Z,,AttributeError,"AttributeError: module 'datasets' has no attribute '__version__'"
7429,b'Update README.md',2020-09-28T17:14:19Z,2020-09-28T17:40:10Z,model card,,
7428,b'Train T5 in Tensoflow 2 Community Notebook',2020-09-28T16:53:05Z,2020-10-01T14:54:29Z,,,
7427,b'Problem while using tokenizer.encode_plus for sentence pairs',2020-09-28T16:28:51Z,2020-12-13T01:22:55Z,wontfix,AssertionError,"AssertionError: "
7426,b'[T5] Automatic setting of decoder_input_ids is misleading and does not correspond to the expected behavior of T5',2020-09-28T16:03:14Z,2020-10-01T15:38:51Z,,,
7425,"b""Getting Import error ImportError: cannot import name 'quantize' from 'transformers.convert_graph_to_onnx' (/opt/conda/lib/python3.7/site-packages/transformers/convert_graph_to_onnx.py)""",2020-09-28T15:27:35Z,2020-12-19T06:31:37Z,wontfix,ImportError,ImportError: cannot import name 'quantize' from 'transformers.convert_graph_to_onnx' (/opt/conda/lib/python3.7/site-packages/transformers/convert_graph_to_onnx.py)
7424,b'[draft] codecov no comment',2020-09-28T14:03:33Z,2020-09-29T19:04:59Z,,,
7423,b'Reorganize documentation navbar',2020-09-28T13:53:35Z,2020-09-28T14:22:59Z,,,
7422,b'Custom TF weights loading',2020-09-28T09:44:42Z,2020-10-05T13:58:45Z,,,
7421,"b'""Sequence Classification with IMDb Reviews "" error, when using ""bert-base-multilingual-cased"" model.'",2020-09-28T09:29:48Z,2020-09-29T00:54:14Z,,ValueError,"ValueError: Expected input batch_size (1600) to match target batch_size (16)."
7420,b'[RAG] Model cards - clean cards',2020-09-28T09:08:17Z,2020-09-28T09:08:39Z,,,
7419,b'Cannot reproduce example token classification GermEval 2014 (German NER) dataset',2020-09-28T06:43:00Z,2020-09-28T13:15:43Z,,,
7418,b'Blenderbot',2020-09-28T05:31:29Z,2020-10-07T23:09:24Z,,,
7417,b'Add adapter support',2020-09-28T05:21:13Z,2020-12-12T20:44:38Z,wontfix,,
7416,b'Possible error in MBart Tokenization script -- target lang code is only present in seq once',2020-09-28T00:57:41Z,2020-10-16T18:15:22Z,Documentation,,
7415,b'Colab pro -fine RoBERTa error tcmalloc: large alloc 6325288960',2020-09-27T19:06:26Z,2021-04-25T15:04:50Z,wontfix,,
7414,b'GPT2LMHeadModel forward input',2020-09-27T16:23:59Z,2020-12-13T01:22:47Z,wontfix,,
7413,b'[RAG] Clean Rag readme in examples',2020-09-27T15:57:03Z,2020-09-28T08:06:39Z,,,
7412,b'Unable to load pipeline for question answering',2020-09-27T14:23:49Z,2020-09-28T12:57:43Z,,OSError,"OSError: Can't load config for 'distilbert-base-cased'. Make sure that:"
7411,b'Error: isTensor() INTERNAL ASSERT FAILED from traced RoBERTa model on iOS using LibTorch',2020-09-27T00:09:37Z,2020-12-04T22:08:36Z,wontfix,,
7410,b'[s2s] rougeLSum expects \\n between sentences',2020-09-26T22:12:55Z,2020-09-27T20:27:20Z,,,
7409,b'[T5] allow config.decoder_layers to control decoder size',2020-09-26T18:43:53Z,2020-09-28T07:08:05Z,,,
7408,b'Allow creation of asymmetrical T5',2020-09-26T17:58:21Z,2020-09-28T07:08:04Z,,,
7407,b'How to train a model based on CTRL',2020-09-26T14:16:13Z,2020-12-04T22:08:35Z,wontfix,,
7406,"b""Bert base chinese model gives error :- EagerTensor object has no attribute 'size'""",2020-09-26T11:46:31Z,2020-10-13T09:03:08Z,,AttributeError,"AttributeError: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'size'"
7405,b'Add summarization support to ONNX conversion',2020-09-26T03:58:14Z,2020-12-15T22:50:38Z,wontfix,,
7404,b'Add support for exporting summarization models to ONNX',2020-09-25T21:42:18Z,2021-07-11T15:02:49Z,,,
7403,b'[makefile] 10x speed up checking/fixing ',2020-09-25T19:29:20Z,2020-09-28T14:45:43Z,,,
7402,b'Tokenizers as an optional dependency',2020-09-25T18:52:13Z,2020-10-18T18:51:25Z,,,
7401,b'Catch PyTorch warning when saving/loading scheduler',2020-09-25T18:51:04Z,2020-09-28T12:20:11Z,,,
7400,b'remove codecov PR comments',2020-09-25T18:46:00Z,2020-09-29T19:16:44Z,,,
7399,b'[Rag] fix rag retriever save_pretrained method',2020-09-25T17:37:28Z,2020-09-25T17:47:13Z,,,
7398,b'Uploading/Sharing large models to HuggingFace',2020-09-25T17:20:08Z,2020-09-29T08:39:49Z,,,
7397,b'Add DistilBERTGeneration comparable to BertGeneration',2020-09-25T16:53:42Z,,Good Second Issue,,
7396,b'(GPT2) Running out of GPU memory(24G) on WSL2 but not on native linux.',2020-09-25T16:51:01Z,2020-12-05T05:00:47Z,wontfix,```RuntimeError,"```RuntimeError: CUDA out of memory. Tried to allocate 384.00 MiB (GPU 0; 24.00 GiB total capacity; 22.01 GiB already allocated; 342.71 MiB free; 65.09 MiB cached)```"
7395,b'[RAG] Remove dependency on `examples/seq2seq` from rag',2020-09-25T15:48:52Z,2020-09-25T16:20:49Z,,,
7394,b'Speedup check_copies script',2020-09-25T15:46:24Z,2020-09-25T15:47:22Z,,,
7393,b'[trainer] Training from scratch',2020-09-25T15:31:50Z,2020-09-25T19:51:13Z,,,
7392,b'Pull request template',2020-09-25T15:27:57Z,2020-09-28T13:51:49Z,,,
7391,b'Remove unhelpful bart warning',2020-09-25T14:22:02Z,2020-09-25T15:01:08Z,,,
7390,b'Fix BartModel output documentation',2020-09-25T13:50:12Z,2020-09-25T15:48:13Z,,,
7389,b'Custom preprocessing of text',2020-09-25T13:14:41Z,2020-12-05T05:00:45Z,wontfix,,
7388,b'Update LayoutLM doc',2020-09-25T13:14:21Z,2020-10-01T13:11:43Z,,,
7387,"b'Fix tokenization in SQuAD for RoBERTa, Longformer, BART'",2020-09-25T10:44:11Z,2020-10-05T10:34:14Z,,,
7386,b'[Rag] Fix wrong usage of `num_beams` and `bos_token_id` in Rag Sequence generation',2020-09-25T09:25:35Z,2020-09-25T12:35:50Z,,,
7385,"b'[s2s, examples] minor doc changes'",2020-09-25T09:20:51Z,2020-09-25T12:00:37Z,,,
7384,b'Flos fix',2020-09-25T08:34:19Z,2020-09-28T08:09:27Z,,,
7383,b'Missing keys when loading weights in TF are not useful',2020-09-25T08:29:34Z,2020-12-13T01:22:52Z,wontfix,,
7382,b'[RAG] Add missing doc and attention_mask to rag',2020-09-25T07:30:13Z,2020-09-25T09:23:56Z,,,
7381,b'modeling_bart: 3 small cleanups that dont change outputs',2020-09-25T01:24:25Z,2020-09-25T08:24:15Z,cleanup,,
7380,b'Incorrect output fields names in docs',2020-09-24T22:46:51Z,2020-09-25T15:50:08Z,,,
7379,b'Movement Pruning for GPT2',2020-09-24T22:16:38Z,2020-12-05T05:00:57Z,wontfix,,
7378,b'how to customize the position encoding',2020-09-24T21:42:16Z,2020-12-05T05:01:00Z,wontfix,,
7377,b'Document RAG again',2020-09-24T21:11:31Z,2020-09-28T12:31:47Z,,,
7376,b'Remove mentions of  RAG from the docs',2020-09-24T20:51:24Z,2020-09-24T21:07:16Z,,,
7375,b'CUDA out of memory error for Bert Model ',2020-09-24T20:43:21Z,2021-01-18T07:02:13Z,wontfix,,
7374,b'Fix FP16 and attention masks in FunnelTransformer',2020-09-24T19:37:35Z,2020-09-25T16:20:40Z,,,
7373,b'[RAG] Add `attention_mask` to RAG generate',2020-09-24T17:59:19Z,2020-09-24T21:22:05Z,,,
7372,"b""[RAG] Fix retrieval offset in RAG's HfIndex and better integration tests""",2020-09-24T16:19:27Z,2020-09-25T14:12:47Z,,,
7371,b'FunnelTransformerForSequenceClassification crashes when fine tuning with mixed precision flag',2020-09-24T16:19:01Z,2020-09-25T16:20:39Z,,RuntimeError,"RuntimeError: Expected object of scalar type Float but got scalar type Half for argument #2 'mat2' in call to _th_bmm"
7370,b'Add new PET Model',2020-09-24T15:42:28Z,,"New model, Feature request",,
7369,b'The absence of source/target language parameters when using MBart in Summarization example',2020-09-24T15:26:52Z,2020-09-24T21:18:57Z,,KeyError,"KeyError: Caught KeyError in DataLoader worker process 0."
7368,b'Formatter',2020-09-24T14:30:05Z,2020-09-24T14:59:22Z,,,
7367,b'Finetuning Pegasus for summarization task',2020-09-24T14:21:02Z,2021-04-25T15:04:53Z,,,
7366,b'test_rag_sequence_generate_batch failing on CUDA',2020-09-24T13:50:35Z,2020-09-25T14:41:51Z,rag,,
7365,b'Fixing case in which `Trainer` hung while saving model in distributed training',2020-09-24T13:45:28Z,2020-09-24T13:56:41Z,,,
7364,"b'Getting ""TypeError: forward() got multiple values for argument \'attention_mask\'"" when replacing pytorch_transformers with transformers'",2020-09-24T12:09:02Z,2020-12-26T07:26:23Z,"wontfix, Migration",TypeError,"TypeError: forward() got multiple values for argument 'attention_mask'"
7363,b'Check config type using `type` instead of `isinstance`',2020-09-24T10:52:48Z,2020-09-25T09:09:10Z,,,
7362,b'Difference between tokenize chinese char',2020-09-24T10:38:41Z,2020-12-05T05:00:55Z,wontfix,,
7361,"b""ImportError: cannot import name 'AutoModelForTokenClassification'""",2020-09-24T10:27:11Z,2020-09-24T15:00:31Z,,,
7360,b'How to add some parameters in T5 (in T5Block layer) and initialize the original T5 parameters with pre-trained model and the new introduced parameters randomly? ',2020-09-24T08:05:53Z,2020-09-24T08:27:40Z,,,
7359,b'Update modeling_tf_longformer.py',2020-09-24T06:57:52Z,2020-09-24T09:37:30Z,,,
7358,b'Example for T5 model from doc is not working.',2020-09-24T04:47:34Z,2020-10-01T15:38:51Z,,ValueError,"ValueError: You have to specify either inputs or inputs_embeds"
7357,b'how can i convert bert pytorch to tf2 ?',2020-09-24T03:41:07Z,2020-09-24T08:39:06Z,,,
7356,b'Fix eval to compute rouge correctly for rouge_score',2020-09-24T02:33:30Z,2020-09-26T22:13:37Z,,,
7355,b'Add token_type_ids to prepare_inputs_for_generation for gpt/gpt2',2020-09-24T00:06:25Z,2021-04-25T15:04:55Z,,,
7354,b'Faster Pegasus tokenizer tests',2020-09-23T22:46:18Z,2020-10-09T15:10:33Z,"Help wanted, Tests",,
7353,b'enable add_tokens for mbart tokenizer',2020-09-23T21:32:47Z,2020-12-04T22:07:21Z,wontfix,,
7352,b'Make PyTorch model files independent from each other',2020-09-23T21:07:32Z,2020-09-24T12:53:54Z,,,
7351,"b""generic text classification with TensorFlow error (AttributeError: 'TFTrainingArguments' object has no attribute 'args')""",2020-09-23T20:04:40Z,2020-09-24T16:07:23Z,,"AttributeError, TypeError","AttributeError: 'TFTrainingArguments' object has no attribute 'args'TypeError: '<' not supported between instances of 'NamedSplit' and 'NamedSplit'"
7350,b'Expand a bit the documentation doc',2020-09-23T20:03:43Z,2020-09-24T08:34:19Z,,,
7349,b'Create README.md',2020-09-23T19:35:41Z,2020-10-01T12:48:52Z,model card,,
7348,b'Clean RAG docs and template docs',2020-09-23T19:10:02Z,2020-09-24T13:24:42Z,,,
7347,b'[s2s] can distributed eval  intiate model download on each rank',2020-09-23T17:44:53Z,2020-10-16T15:40:11Z,,,
7346,b'Difference between bart-large and bart-large-cnn vocabulary',2020-09-23T17:35:15Z,2020-10-19T14:57:30Z,,,
7345,b'Models doc',2020-09-23T16:31:17Z,2020-09-23T17:20:46Z,,,
7344,b'Remove reference to args in XLA check',2020-09-23T16:28:00Z,2020-09-23T17:56:22Z,,,
7343,"b""AttributeError: 'TFTrainingArguments' object has no attribute 'args'""",2020-09-23T16:27:16Z,2020-09-23T17:56:21Z,,AttributeError,"AttributeError: 'TFTrainingArguments' object has no attribute 'args'"
7342,b'CentOS Error installing Transformers',2020-09-23T15:48:08Z,2020-12-13T01:22:47Z,wontfix,,
7341,"b'data_collator.py - line 326, in mask tokens - xlnet finetuning error'",2020-09-23T14:32:35Z,2020-12-25T09:25:04Z,wontfix,ValueError,"ValueError: This collator requires that sequence lengths be even to create a leakage-free perm_mask. Please see relevant comments in source code for details."
7340,b'Fixed evaluation_strategy on epoch end bug',2020-09-23T14:13:07Z,2020-09-23T17:17:01Z,,,
7339,"b'Trainer Evaluates at each step (Not of epoch end) , indentation bug'",2020-09-23T13:30:24Z,2020-09-23T17:17:01Z,,,
7338,b'BufferedWriter takes most of the time',2020-09-23T12:41:09Z,2020-12-05T05:00:49Z,wontfix,,
7337,"b""Trainer.py module 'datasets' has no attribute 'Dataset'""",2020-09-23T09:47:15Z,2020-09-23T14:53:43Z,,AttributeError,"AttributeError: module 'datasets' has no attribute 'Dataset'"
7336,b'Error when fine-tune RoBERTa on NSP using Trainer',2020-09-23T08:16:30Z,2020-12-13T01:22:49Z,wontfix,ValueError,"ValueError: empty range for randrange() (0, 0, 0)"
7335,b'is there a tokenizer only used whitespace for spliting chinese sentence?',2020-09-23T03:14:56Z,2020-11-29T11:50:04Z,wontfix,,
7334,"b'[testing] skip decorators: docs, tests, bugs'",2020-09-23T01:04:57Z,2020-09-23T09:16:20Z,,,
7333,b'Cannot import transformers with TF version 2.1.0',2020-09-23T00:01:24Z,2021-04-25T15:04:56Z,"wontfix, TensorFlow, Should Fix",AttributeError,"AttributeError: module 'tensorflow_core.python.keras.api._v2.keras.activations' has no attribute 'swish'"
7332,"b""data_collator error: AttributeError: 'dict' object has no attribute 'size'""",2020-09-22T22:55:36Z,2021-01-10T13:54:31Z,wontfix,AttributeError,"AttributeError: 'dict' object has no attribute 'size'"
7331,b'[s2s] only save metrics.json from rank zero',2020-09-22T22:17:00Z,2020-09-22T22:27:29Z,,,
7330,b'Ensure that integrations are imported before transformers or ml libs',2020-09-22T22:03:01Z,2020-09-23T17:23:46Z,,,
7329,b'Problem loading a dynamic quantized distilbert model.',2020-09-22T21:19:29Z,2021-01-19T00:56:34Z,wontfix,RuntimeError,"RuntimeError: Error(s) in loading state_dict for DistilBertForTokenClassification:"
7328,b'Add PRADO model',2020-09-22T20:33:36Z,2021-01-03T13:23:31Z,"wontfix, New model",,
7327,b'PegasusTokenizer: Newline symbol',2020-09-22T18:03:06Z,2020-11-29T11:50:03Z,"wontfix, Core: Tokenization, pegasus",,
7326,b'Check decorator order',2020-09-22T16:59:29Z,2020-09-24T08:54:37Z,,,
7325,b'Mark big downloads slow',2020-09-22T16:17:22Z,2020-09-22T16:21:53Z,,,
7324,b'[s2s] Marian beam search slow for en-de ',2020-09-22T16:09:18Z,2020-09-23T06:55:48Z,"translation, marian",,
7323,b'T5 Cross-attention Decoder - Possible bug with relative_bias',2020-09-22T15:48:11Z,2020-10-01T15:38:51Z,,,
7322,b'Add num workers cli arg',2020-09-22T15:31:29Z,2020-09-22T18:44:43Z,,,
7321,b'Example Format of Data for token classification',2020-09-22T15:19:10Z,2020-09-27T16:25:18Z,,,
7320,b'Test CI with higher timeout',2020-09-22T15:03:40Z,2020-09-22T16:27:54Z,,,
7319,"b""[Bug Fix] Fix run_squad.py evaluation code doesn't use probabilities""",2020-09-22T14:34:33Z,2021-04-25T15:04:57Z,,**Notice,"**Notice: many squad models were evaluated without the probabilities, therefore, the results published in their model cards are possibly wrong.**"
7318,b'Fixes for LayoutLM',2020-09-22T14:31:36Z,2020-09-22T14:37:11Z,,,
7317,b'Create README.md',2020-09-22T13:34:27Z,2020-09-22T22:26:13Z,model card,,
7316,b'Support for Windows in check_copies',2020-09-22T13:11:37Z,2020-09-22T14:17:49Z,,,
7315,b'Memory leak',2020-09-22T11:00:37Z,2020-09-27T15:01:30Z,,,
7314,b'Text generation with xlnet',2020-09-22T10:45:29Z,2020-09-22T15:13:41Z,,,
7313,b'Fixed results of SQuAD-FR evaluation',2020-09-22T10:35:45Z,2020-09-22T16:39:08Z,model card,,
7312,b'Adds FSMT to LM head AutoModel',2020-09-22T10:31:55Z,2020-09-22T10:35:52Z,,,
7311,b'Create an XLA parameter and fix the mixed precision',2020-09-22T09:24:04Z,2020-09-22T14:19:34Z,,,
7310,b'[code quality] new make target that combines style and quality targets',2020-09-22T05:11:35Z,2020-09-25T15:37:40Z,,,
7309,b'[code quality] fix confused flake8',2020-09-22T05:03:03Z,2020-09-23T02:12:37Z,,,
7308,b'[s2s] metrics.json is wrong on multigpu',2020-09-22T04:29:30Z,2020-11-29T11:50:07Z,wontfix,,
7307,b'Cuda OOM training gpt2-xl with Trainer in multi-GPUs',2020-09-22T02:17:48Z,2021-04-25T15:04:59Z,"wontfix, Distributed Training / Models, gpt2",,
7306,b'BertModel for 2 category classification - How to evaluate the performance',2020-09-22T01:44:58Z,2020-11-29T11:50:23Z,wontfix,,
7305,b'Fix #7304',2020-09-22T00:31:44Z,2020-09-22T13:20:04Z,,,
7304,b'Wrong arg order for `nested_xla_mesh_reduce` in trainer.py',2020-09-21T22:37:01Z,2020-09-22T13:20:04Z,,TypeError,"TypeError: _xla_rendezvous(): incompatible function arguments. The following argument types are supported:"
7303,b'BART metrics.json and validation checkpoint metrics seem to disagree',2020-09-21T22:31:11Z,2020-11-29T11:50:22Z,wontfix,,
7302,b'Add possibility to evaluate every epoch',2020-09-21T21:17:52Z,2020-09-22T13:52:30Z,,,
7301,b'[s2s] save hostname with repo info',2020-09-21T20:58:25Z,2020-09-21T21:26:24Z,,,
7300,b'[s2s] add src_lang kwarg for distributed eval',2020-09-21T20:25:49Z,2020-09-22T22:26:37Z,,,
7299,b'Create README.md',2020-09-21T18:38:23Z,2020-10-01T12:52:29Z,model card,,
7298,b'[s2s] s/alpha_loss_encoder/alpha_encoder_loss/',2020-09-21T17:59:48Z,2020-09-21T18:14:26Z,,,
7297,b'[s2s tests] fix test_run_eval_search',2020-09-21T17:46:21Z,2020-09-21T18:00:41Z,,,
7296,b'Marian/MBart should not save static position embeddings',2020-09-21T17:39:12Z,2020-10-21T12:06:08Z,,,
7295,b'test_run_eval_search SLOW failure',2020-09-21T17:32:06Z,2020-09-21T18:00:41Z,,,
7294,b'Bert Fine-Tuning on SQuAD with native TF2',2020-09-21T17:06:52Z,2020-09-22T20:18:52Z,,ValueError,"ValueError: Found unexpected keys that do not correspond to any Model output: dict_keys(['start_positions', 'end_positions', 'cls_index', 'p_mask', 'is_impossible']). Expected: ['output_1', 'output_2'] "
7293,b'Support serialized tokenizer in AutoTokenizer',2020-09-21T16:47:40Z,2020-11-29T11:50:20Z,wontfix,,
7292,b'[fsmt] SinusoidalPositionalEmbedding no need to pass device',2020-09-21T15:53:05Z,2020-09-22T09:39:07Z,,,
7291,b'Fix saving TF custom models',2020-09-21T15:23:56Z,2020-09-22T13:31:14Z,,,
7290,b'[s2s] add create student script',2020-09-21T15:04:09Z,2020-09-27T19:10:46Z,,,
7289,b'Fix #7284',2020-09-21T14:22:05Z,2020-09-21T14:31:27Z,,,
7288,b'Error importing MBart from transformers',2020-09-21T14:11:16Z,2020-09-21T19:16:01Z,,ImportError,"ImportError: cannot import name 'MBartForConditionalGeneration' from 'transformers' (/home/memduh/hf/venv/lib/python3.8/site-packages/transformers/__init__.py)"
7287,"b'""index out of range in self"" when calling BertForTokenClassification'",2020-09-21T13:17:11Z,2020-09-21T13:49:34Z,,IndexError,"IndexError: index out of range in self"
7286,b'Added RobBERT-v2 model card',2020-09-21T12:22:36Z,2020-09-21T20:17:28Z,model card,,
7285,b'scibert-nli out of dace',2020-09-21T11:52:40Z,2020-09-28T10:24:01Z,,,
7284,b'Fine tune BERT based models using Trainer fails',2020-09-21T11:00:20Z,2020-09-21T14:31:26Z,,TypeError,"TypeError: Caught TypeError in replica 0 on device 0."
7283,b'IXAmBERT model card',2020-09-21T10:46:26Z,2020-09-21T20:15:32Z,model card,,
7282,b'Disable missing weight warning for RobertaForMaskedLM/CamembertForMaskedLM',2020-09-21T09:36:27Z,2020-09-21T13:14:49Z,,,
7281,b'[seq2seq testing] multigpu test run via subprocess',2020-09-21T07:55:40Z,2020-10-21T21:20:54Z,"Tests, Distributed Training / Models",,
7280,"b'I want to use the Bert2GPT2 architecture,but my pretrained Bert and GPT2 have different vocabs,so what should I do for the vocabs? '",2020-09-21T07:34:43Z,2020-11-29T11:50:01Z,wontfix,,
7279,b'[wip/dont-merge] pegasus beam search implementation',2020-09-21T07:17:01Z,2020-09-25T15:32:21Z,,,
7278,b'[model card] distlbart-mnli model cards',2020-09-21T06:02:20Z,2020-09-21T16:26:19Z,model card,,
7277,b'Unable to serialize/save TF2.0 Bert model',2020-09-21T03:53:25Z,2020-09-22T13:31:14Z,"TensorFlow, Should Fix",`TypeError,"`TypeError: ('Not JSON Serializable:', BertConfig {"
7276,b'fix unnessasry cpu memory usage when training',2020-09-21T03:35:38Z,2020-09-21T12:55:44Z,,,
7275,b'Question about model configuration',2020-09-21T00:25:49Z,2020-09-22T15:19:03Z,,,
7274,b'[seq2seq] make it easier to run the scripts',2020-09-20T22:32:41Z,2020-09-24T19:23:48Z,,,
7273,b'[Bug/Question] Write With Transformers Implementation vs. Custom Implementation',2020-09-20T20:47:00Z,2020-11-29T11:50:15Z,wontfix,,
7272,"b'[Longformer, Bert, Roberta, ...] Fix multi gpu training'",2020-09-20T18:33:14Z,2020-09-25T18:33:21Z,,ValueError,"ValueError: Layer #0 (named ""bert"") expects 197 weight(s), but the saved weights have 199 element(s)."
7271,b'Distilbert classification ',2020-09-20T17:04:39Z,2020-11-29T11:50:13Z,wontfix,,
7270,b'Multitask pre-training setting',2020-09-20T15:54:30Z,2020-11-29T11:50:12Z,wontfix,,
7269,b'Add model cards for new pre-trained BERTweet-COVID19 models',2020-09-20T13:28:35Z,2020-09-21T10:12:52Z,model card,,
7268,b'Fix typo in model name',2020-09-20T11:00:10Z,2020-09-20T17:12:30Z,,,
7267,b'[Bug fix] Fixed target_mapping preparation for XLNet (Pytorch)',2020-09-20T09:57:19Z,2020-09-21T08:53:53Z,,,
7266,b'LXMERT pre-training tasks',2020-09-20T09:15:06Z,2021-05-29T15:09:21Z,,,
7265,b'Feature Request: Support Longformer 3D attention mask ?',2020-09-20T08:06:28Z,2020-12-05T05:00:46Z,wontfix,,
7264,b'Changing learning rate for BertModelforTokenClassification',2020-09-20T07:21:37Z,2020-12-13T01:22:47Z,wontfix,,
7263,b'[s2s] adjust finetune + test to work with fsmt',2020-09-20T05:41:42Z,2020-09-21T19:13:20Z,fsmt,,
7262,"b'When I updated my transformers to the latest, the previously trained model loaded with an error'",2020-09-20T05:19:58Z,2020-11-29T11:50:16Z,wontfix,,
7261,b'LXMERT visual feature extraction during training/fine-tuning phase',2020-09-20T03:50:11Z,2021-04-25T15:05:00Z,,,
7260,b'A confusion about mrc model',2020-09-20T03:37:07Z,2020-11-29T11:50:11Z,wontfix,,
7259,b' [broken] quantization util for CPU',2020-09-19T23:13:39Z,2020-09-25T14:15:50Z,,,
7258,"b'[save/load model] authorized keys, no save keys, etc.'",2020-09-19T22:57:05Z,2020-11-23T20:33:13Z,,,
7257,b'[fsmt] build/test scripts',2020-09-19T21:57:55Z,2020-09-24T21:10:27Z,fsmt,,
7256,b'[fsmt] Expanding Positional Embeddings',2020-09-19T21:16:41Z,2021-03-18T01:40:15Z,"wontfix, fsmt",,
7255,"b'Add ""Fine-tune ALBERT for sentence-pair classification"" notebook to the community notebooks'",2020-09-19T20:34:29Z,2020-09-21T08:25:22Z,,,
7254,b'[s2s] distributed eval allows num_return_sequences > 1',2020-09-19T19:24:12Z,2020-09-24T21:30:10Z,,,
7253,b'[wip] layernorm eps config',2020-09-19T19:06:45Z,2020-10-06T18:41:06Z,,,
7252,b'[s2s] add supported architecures to MD',2020-09-19T19:05:38Z,2020-09-22T17:09:36Z,,,
7251,b'[testing doc] @slow has to be last',2020-09-19T18:17:07Z,2020-09-20T13:17:30Z,,,
7250,b'[testing] when to @slow and when not to? (huge models download)',2020-09-19T17:45:15Z,2020-11-27T21:04:55Z,,,
7249,b'very poor performance of Longformer on SQuAD-like question-answering tasks ',2020-09-19T16:16:42Z,2020-11-29T11:49:59Z,wontfix,,
7248,b'[example/glue] fix compute_metrics_fn for bart like models',2020-09-19T13:24:28Z,2020-09-21T09:34:21Z,,,
7247,b'[example/glue] run_glue compute metrics fail for bart like models',2020-09-19T13:24:02Z,2020-09-21T09:34:21Z,,,
7246,"b""How to get cross attention weights of decoder when using 'encoderdecodermodel'""",2020-09-19T10:52:57Z,2021-01-10T13:54:15Z,wontfix,,
7245,b'[s2s] distributed_eval edge case',2020-09-19T06:00:26Z,2020-10-16T18:15:57Z,,,
7244,b'fsmt tiny model card + script',2020-09-19T03:24:43Z,2020-09-19T18:37:12Z,model card,,
7243,b'Enable pegasus fp16 by clamping large activations',2020-09-18T21:17:45Z,2020-10-01T08:48:37Z,,,
7242,b'[s2s] distributed_eval.py saves better speed info',2020-09-18T19:44:57Z,2020-09-18T19:46:01Z,,,
7241,"b""KeyError: 'squeezebert' which from model zoo""",2020-09-18T18:20:34Z,2020-09-18T18:26:19Z,,,
7240,b'Add title to model card',2020-09-18T16:42:20Z,2020-09-19T06:10:45Z,model card,,
7239,b'Create README.md',2020-09-18T16:40:01Z,2020-09-19T06:09:29Z,model card,,
7238,b'Update the TF models to remove their interdependencies',2020-09-18T15:45:36Z,2020-09-24T12:31:00Z,,,
7237,b'[wip] summarization dataset downloader',2020-09-18T15:12:43Z,2020-10-06T18:47:45Z,,,
7236,b'is_pretokenized -> is_split_into_words',2020-09-18T14:22:37Z,2020-09-22T13:34:36Z,,,
7235,b'[Bug Fix] The actual batch_size is inconsistent with the settings.',2020-09-18T12:45:50Z,2020-09-22T16:31:22Z,,,
7234,"b""TypeError: 'ByteLevelBPETokenizer' object is not callable""",2020-09-18T08:51:49Z,2021-01-17T21:35:09Z,wontfix,,
7233,b'Fixed typo in README',2020-09-18T08:48:35Z,2020-09-18T08:52:44Z,,,
7232,b'trainer.evaluate() aggregates predictions on GPU and causes CUDA out of memory issues for large datasets',2020-09-18T07:15:40Z,2020-10-14T15:41:46Z,,,
7231,b'[draft][s2s]reload-dl',2020-09-18T05:50:55Z,2020-09-25T14:17:25Z,,,
7230,b'test fsmt finetuning',2020-09-18T05:47:04Z,2020-09-21T19:13:20Z,fsmt,,
7229,"b""a possible hack for FSMT's SinusoidalPositionalEmbedding peculiarity""",2020-09-18T05:26:15Z,2020-09-22T03:08:33Z,,,
7228,b'FSMT Training scripts',2020-09-18T04:08:17Z,2020-11-29T11:50:09Z,wontfix,,
7227,"b'Suggestion: Better to change the task name of  ""sentiment-analysis"" to ""text-classification""'",2020-09-18T02:02:42Z,2020-09-18T08:34:05Z,,,
7226,b'The task name sentiment-analysis',2020-09-18T01:48:43Z,2020-09-18T02:03:26Z,,,
7225,b'Fix a typo',2020-09-18T01:21:19Z,2020-09-18T08:23:33Z,,,
7224,b'[fsmt] rewrite SinusoidalPositionalEmbedding + USE_CUDA test fixes + new TranslationPipeline test',2020-09-17T22:29:36Z,2020-09-21T13:13:36Z,,,
7223,b'[s2s] remove double assert',2020-09-17T22:24:44Z,2020-09-17T22:32:32Z,,,
7222,"b""tokenizer.add_tokens conflict with MBart's tokenizer""",2020-09-17T22:18:29Z,2020-11-24T02:59:34Z,"wontfix, Core: Tokenization",,
7221,b'model card improvements',2020-09-17T21:06:51Z,2020-09-19T21:02:05Z,model card,,
7220,b'skip failing FSMT CUDA tests until investigated',2020-09-17T20:17:13Z,2020-09-17T20:53:14Z,,,
7219,b'Copy code from Bert to Roberta and add safeguard script',2020-09-17T20:13:51Z,2020-09-22T09:02:28Z,,,
7218,b'[model cards] fix metadata - 3rd attempt',2020-09-17T19:58:04Z,2020-09-17T20:57:07Z,model card,,
7217,b'FSMT Cuda CI Failures',2020-09-17T19:49:34Z,2020-09-17T20:53:14Z,,,
7216,b'[model cards] fix dataset yaml',2020-09-17T19:25:24Z,2020-09-17T19:29:40Z,,,
7215,b'Links seem to be expired for German NER example ',2020-09-17T18:40:15Z,2020-09-18T10:19:37Z,Ex: Named Entity Recognition,,
7214,b'Does the default weight_decay of 0.0 in transformers.AdamW make sense?',2020-09-17T18:13:18Z,2020-09-18T10:40:10Z,,,
7213,b'Return cross-attention from T5 models',2020-09-17T18:04:48Z,2021-04-25T15:05:01Z,,,
7212,b'Create README.md',2020-09-17T17:40:52Z,2020-09-18T07:23:50Z,model card,,
7211,b'Rewrites BERT in Flax to the new Linen API',2020-09-17T17:39:58Z,2020-09-18T09:15:00Z,,,
7210,b'Create README.md',2020-09-17T17:34:43Z,2020-09-18T07:23:58Z,model card,,
7209,b'Create README.md',2020-09-17T17:29:59Z,2020-09-18T07:24:11Z,model card,,
7208,b'[model cards] yaml specs and related questions',2020-09-17T16:56:21Z,2020-10-15T21:08:45Z,,,
7207,b'[model cards] fix yaml in cards',2020-09-17T16:53:31Z,2020-09-17T18:11:17Z,model card,,
7206,b'[models website] search UI issues',2020-09-17T16:38:31Z,2021-03-18T01:49:14Z,Feature request,,
7205,b'Create README.md',2020-09-17T16:10:39Z,2020-09-18T07:24:31Z,model card,,
7204,b'Add customized text to widget',2020-09-17T16:02:34Z,2020-09-18T07:24:23Z,model card,,
7203,b'[s2s] reload dataloaders every epoch?',2020-09-17T14:54:16Z,2020-11-29T11:50:08Z,wontfix,,
7202,b'Change to use relative imports in some files & Add python prompt symbols to example codes',2020-09-17T11:51:51Z,2020-09-17T16:30:46Z,,,
7201,b'added multilabel text classification notebook using distilbert to community notebooks',2020-09-17T09:36:57Z,2020-09-17T09:58:58Z,,,
7200,b'[RAG] PR to save status of previous RAG code',2020-09-17T08:56:58Z,2020-09-25T17:57:31Z,,,
7199,b'Why does RoBERTa not label custom tokens as special tokens?',2020-09-17T08:27:52Z,2020-09-17T09:20:54Z,,,
7198,b'how to continue training from a checkpoint with Trainer?',2020-09-17T03:24:40Z,2020-09-20T20:19:34Z,,,
7197,b'[s2s]add wandb summary metric that tracks best val bleu/rouge',2020-09-17T02:46:15Z,2020-11-24T02:58:49Z,wontfix,,
7196,b'[s2s] fix kwarg typo',2020-09-17T01:58:23Z,2020-09-17T01:58:58Z,,,
7195,"b""GPT2 Heatmap Error: 'Parameter' object has no attribute 'get_shape'""",2020-09-17T01:33:15Z,2020-09-17T10:02:22Z,,`AttributeError,`AttributeError: 'Parameter' object has no attribute 'get_shape'`
7194,b'examples/seq2seq/__init__.py mutates sys.path',2020-09-17T00:58:35Z,2020-09-20T20:54:43Z,,,
7193,b'Add bos and eos to gpt2 tokenizer',2020-09-17T00:26:22Z,2020-11-24T02:58:09Z,wontfix,,
7192,b'[s2s] run_eval/run_eval_search tweaks',2020-09-17T00:20:55Z,2020-09-17T18:26:39Z,,,
7191,b'Trainer multi label',2020-09-16T21:48:46Z,2020-09-17T12:15:37Z,,,
7190,b'[build scripts] update',2020-09-16T21:39:30Z,2020-09-16T21:58:57Z,,,
7189,"b"" When trying to train LongformerModel got **forward() got an unexpected keyword argument 'labels'**""",2020-09-16T21:15:50Z,2020-11-24T02:58:47Z,wontfix,**TypeError,"**TypeError: forward() got an unexpected keyword argument 'labels'**"
7188,b'Change to use relative imports in some files & Add python prompt symbols to example codes',2020-09-16T20:42:50Z,2020-09-17T11:48:47Z,,,
7187,b'[s2s-wip] ray instructions',2020-09-16T20:19:54Z,2020-11-29T00:01:39Z,wontfix,,
7186,b'[s2s] distributed eval cleanup',2020-09-16T19:17:49Z,2020-09-16T19:38:38Z,,,
7185,b'Create README.md for indobert-lite-large-p2',2020-09-16T18:50:37Z,2020-09-18T07:21:40Z,model card,,
7184,b'Create README.md for indobert-lite-large-p1',2020-09-16T18:49:46Z,2020-09-18T07:22:12Z,model card,,
7183,b'Create README.md for indobert-lite-base phase 2 model card',2020-09-16T18:48:39Z,2020-09-18T07:21:54Z,model card,,
7182,b'Create README.md for indobert-lite-base-p1',2020-09-16T18:47:24Z,2020-09-18T07:22:32Z,model card,,
7181,b'Create README.md for indobert-large-p2 model card',2020-09-16T18:44:58Z,2020-09-18T07:21:29Z,model card,,
7180,b'Create README.md for indobert-large-p1 model card',2020-09-16T18:42:36Z,2020-09-18T07:21:17Z,model card,,
7179,b'Create README.md for indobert-base-p1 model card',2020-09-16T18:40:32Z,2020-09-18T07:21:00Z,model card,,
7178,b'Create README.md for indobert-base-p2',2020-09-16T18:40:05Z,2020-09-18T07:20:29Z,model card,,
7177,b'RuntimeError: CUDA out of memory. ',2020-09-16T18:33:26Z,2020-09-16T18:36:56Z,,RuntimeError,"RuntimeError: CUDA out of memory. Tried to allocate 384.00 MiB (GPU 0; 15.90 GiB total capacity; 14.75 GiB already allocated; 225.81 MiB free; 14.81 GiB reserved in total by PyTorch)"
7176,b'distributed eval cleanup',2020-09-16T17:49:42Z,2020-09-16T19:38:38Z,,,
7175,b'Fix a few countings (steps / epochs) in trainer_tf.py',2020-09-16T17:43:35Z,2020-09-18T13:28:57Z,,,
7174,b'use the correct add_start_docstrings',2020-09-16T17:38:11Z,2020-09-16T18:40:36Z,,,
7173,b'remove duplicated code',2020-09-16T17:34:19Z,2020-09-17T09:51:41Z,,,
7172,b'Bug in finetuning ALBERT on text-classification in GLUE',2020-09-16T17:30:07Z,2020-09-22T08:06:13Z,,OSError,"OSError: Model name 'albert-base-v2' was not found in tokenizers model name list (albert-base-v1, albert-large-v1, albert-xlarge-v1, albert-xxlarge-v1, albert-base-v2, albert-large-v2, albert-xlarge-v2, albert-xxlarge-v2). We assumed 'albert-base-v2' was a path, a model identifier, or url to a directory containing vocabulary files named ['spiece.model'] but couldn't find such vocabulary files at this path or url."
7171,b'remove deprecated flag',2020-09-16T17:10:57Z,2020-09-17T09:52:13Z,,,
7170,b'[s2s] Try to get ray/optuna + examples/seq2seq working',2020-09-16T16:27:49Z,2020-11-24T02:58:46Z,wontfix,,
7169,b'BERT Trainer.train() CUDA out of memory error',2020-09-16T16:15:52Z,2020-11-29T11:50:10Z,wontfix,,
7168,b'DistilBERT for token classification (pytorch) predicts wrong classes for <PAD> tokens',2020-09-16T15:07:56Z,2020-11-06T11:15:21Z,,,
7167,b'weights partially missing for CamembertForMaskedLM ',2020-09-16T12:15:01Z,2020-09-21T08:58:34Z,,,
7166,b'Create README.md',2020-09-16T10:09:27Z,2020-09-16T16:16:01Z,model card,,
7165,"b""__init__() got an unexpected keyword argument 'cache_dir' """,2020-09-16T09:52:36Z,2020-09-16T11:39:12Z,,TypeError,"TypeError: init() got an unexpected keyword argument 'cache_dir'"
7164,b'tf.keras.models.load_model() does not load saved model that includes TFOpenAIGPTLMHeadModel layer',2020-09-16T08:54:18Z,2021-03-06T00:17:20Z,wontfix,ValueError,"ValueError: The two structures don't have the same nested structure."
7163,b'Pegasus- Arxiv predicts random text',2020-09-16T08:11:07Z,2020-10-16T15:39:33Z,,,
7162,b'Create larger summaries by using Summarization models like T5 or Pegasus',2020-09-16T07:12:46Z,2020-09-16T16:44:58Z,,,
7161,b'Add empty random document case to DataCollatorForNextSentencePrediction',2020-09-16T07:00:08Z,2020-09-16T13:15:11Z,,,
7160,b'distributed launch raise Error',2020-09-16T06:18:04Z,2021-03-06T00:17:22Z,wontfix,"RuntimeError, subprocess.CalledProcessError","RuntimeError: Expected to mark a variable ready only once. This error is caused by one of the following reasons: 1) Use of a module parameter outside the `forward` function. Please make sure model parameters are not shared across multiple concurrent forward-backward passes2) Reused parameters in multiple reentrant backward passes. For example, if you use multiple `checkpoint` functions to wrap the same part of your model, it would result in the same set of parameters been used by different reentrant backward passes multiple times, and hence marking a variable ready multiple times. DDP does not support such use cases yet.subprocess.CalledProcessError: Command '['/opt/conda/bin/python', '-u', 'run_squad.py', '--local_rank=3', '--model_type', 'longformer', '--do_train', '--model_name_or_path', 'longformer-base-len4K', '--do_eval', '--do_lower_case', '--threads', '30', '--fp16', '--eval_all_checkpoints', '--save_steps', '2500', '--train_file', './data/marco_v1.0/train.json.demo', '--predict_file', './data/marco_v1.0/dev.json', '--per_gpu_train_batch_size', '6', '--learning_rate', '3e-5', '--num_train_epochs', '2', '--max_seq_length', '2048', '--doc_stride', '1024', '--output_dir', 'output/marco_pyramidlocalatt']' returned non-zero exit status 1."
7159,"b""I reduce the longformer's attention window from 512 to 256, but train speed not changed """,2020-09-16T06:07:31Z,2020-11-24T02:58:48Z,wontfix,,
7158,b'OOM Issue when evaluating with Trainer',2020-09-16T05:43:43Z,2020-11-24T02:58:57Z,wontfix,RuntimeError,"RuntimeError: CUDA out of memory. Tried to allocate 2.58 GiB (GPU 0; 7.79 GiB total capacity; 2.72 GiB already allocated; 1.94 GiB free; 4.88 GiB reserved in total by PyTorch)"
7157,b'ProphetNet',2020-09-16T05:36:08Z,2020-10-19T15:36:09Z,model card,,
7156,b'[doc] [testing] improve/expand the Parametrization section',2020-09-16T02:53:27Z,2020-09-16T12:45:50Z,,,
7155,b'build/eval/gen-card scripts for fsmt',2020-09-15T22:49:35Z,2020-09-16T12:41:27Z,,,
7154,b'Inconsistent parameter naming conventions in ModelConfigs',2020-09-15T22:38:29Z,2020-09-16T10:18:23Z,,,
7153,"b'[model cards] ported allenai Deep Encoder, Shallow Decoder models'",2020-09-15T21:21:49Z,2020-09-17T15:58:50Z,model card,,
7152,b'Gradient checkpointing for GPT-2',2020-09-15T18:03:17Z,2020-12-16T08:52:57Z,,,
7151,b'fix the warning message of overflowed sequence',2020-09-15T17:43:03Z,2020-09-16T11:40:57Z,,,
7150,b'Refactoring the TF activations functions',2020-09-15T15:50:01Z,2020-09-16T11:03:47Z,,,
7149,b'Ignore me!',2020-09-15T14:56:55Z,2020-09-15T14:57:21Z,,,
7148,b'add new model prophetnet',2020-09-15T14:49:23Z,2020-09-15T15:00:57Z,model card,,
7147,b'Funnel model cards',2020-09-15T14:26:38Z,2020-09-15T14:40:57Z,model card,,
7146,"b""Fine tune with local model raised `torch.nn.modules.module.ModuleAttributeError: 'DataParallel' object has no attribute 'config'`""",2020-09-15T13:39:36Z,2020-09-28T08:09:27Z,,torch.nn.modules.module.ModuleAttributeError,"torch.nn.modules.module.ModuleAttributeError: 'DataParallel' object has no attribute 'config'"
7145,b'Load Pre-Trained Model Using Docker',2020-09-15T13:15:39Z,2020-12-05T05:00:54Z,wontfix,OSError,"OSError: Model name 'gpt2-medium' was not found in tokenizers model "
7144,"b""unexpected keyword argument 'force_fusions' when running the onnx notebook""",2020-09-15T12:41:49Z,2020-09-16T14:09:33Z,,`Error,"`Error: quantize() got an unexpected keyword argument 'force_fusions'`"
7143,b'Tiny typo fix',2020-09-15T11:54:31Z,2020-09-15T12:18:43Z,,,
7142,b'Add quotes to paths in MeCab arguments',2020-09-15T10:50:11Z,2020-09-15T11:04:51Z,,,
7141,b'Adding Fast tokenizers for SentencePiece based tokenizers - Breaking: remove Transfo-XL fast tokenizer',2020-09-15T10:29:36Z,2020-10-08T09:32:17Z,,,
7140,b'Onnx + TensorRT uses CPU not GPU',2020-09-15T09:47:33Z,2020-10-17T19:59:22Z,,,
7139,b'generate text function support part of the  target inputs',2020-09-15T09:34:48Z,2020-11-24T02:58:50Z,wontfix,,
7138,"b""OSError: Can't load weights for 'nlptown/bert-base-multilingual-uncased-sentiment'. """,2020-09-15T07:47:15Z,2020-09-15T23:38:47Z,,OSError,"OSError: Can't load weights for 'nlptown/bert-base-multilingual-uncased-sentiment'. Make sure that:"
7137,"b'Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!'",2020-09-15T07:45:08Z,2020-11-22T08:04:41Z,wontfix,RuntimeError,"RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
7136,b' BertForMaskedLM Loss function ',2020-09-15T07:22:56Z,2020-09-15T07:29:32Z,,,
7135,b'Loss mask for fine-tuning GPT2LMHeadModel model',2020-09-15T03:13:52Z,2020-09-16T17:38:53Z,,,
7134,b'evaluate_during_training after each epoch',2020-09-15T00:34:45Z,2020-11-24T02:58:52Z,wontfix,,
7133,b'Update README',2020-09-14T20:32:47Z,2020-09-16T16:12:13Z,,,
7132,b'Add tokenizer file save in convert_graph_to_onnx.py ',2020-09-14T20:11:59Z,2020-11-24T02:58:59Z,wontfix,,
7131,b'[EncoderDecoderModel] fix indentation error ',2020-09-14T19:25:58Z,2020-09-15T19:10:08Z,,,
7130,b'AssertionError with multiple GPU',2020-09-14T19:01:45Z,2020-12-05T05:00:50Z,wontfix,,
7129,b'[WIP RAG] Finalize RAG parallel',2020-09-14T18:59:40Z,2020-09-17T09:00:32Z,,,
7128,b'Fix the HF logger',2020-09-14T18:37:31Z,2020-09-28T08:13:51Z,,,
7127,b'only fine tune the encoder part of BART',2020-09-14T18:34:34Z,2020-11-04T14:36:53Z,,,
7126,b'Multi predictions trainer',2020-09-14T17:36:54Z,2020-09-15T14:27:25Z,,,
7125,b'fix ZeroDivisionError and epoch counting',2020-09-14T17:33:59Z,2020-09-15T15:51:51Z,,,
7124,b'[s2s] distributed eval in one command',2020-09-14T16:46:38Z,2020-09-14T19:57:57Z,,,
7123,b'Seq2SeqDataset experiment: try to use arrow datasets',2020-09-14T15:41:50Z,2020-11-24T02:58:58Z,"Help wanted, wontfix",,
7122,b'backtranslation script',2020-09-14T15:34:39Z,2020-10-16T18:11:13Z,,,
7121,b'[blk] backtranslation script',2020-09-14T15:17:58Z,2020-09-25T14:18:14Z,,,
7120,b'modeling_xlnet.py:283: UserWarning: Mixed memory format inputs detected while calling the operator.',2020-09-14T14:47:34Z,2020-12-20T13:34:41Z,wontfix,,
7119,b'Fix reproducible tests in Trainer',2020-09-14T14:12:25Z,2020-09-15T07:32:44Z,,,
7118,b'Temporarily skip failing tests due to dependency change',2020-09-14T11:37:00Z,2020-09-14T11:42:13Z,,,
7117,b'Feature request: State the goals and none goals of the library in the README',2020-09-14T10:42:39Z,2020-11-22T08:04:36Z,wontfix,,
7116,b'fix link to paper',2020-09-14T10:05:21Z,2020-09-14T11:43:40Z,,,
7115,b'can i self-define the decoder when i user EncoderDecoderModel?',2020-09-14T09:45:35Z,2020-12-05T05:00:53Z,wontfix,,
7114,b'How to return the word embeddings and how to understand the hidden_states in return?',2020-09-14T07:56:43Z,2020-11-24T02:58:53Z,wontfix,,
7113,b'Generate coherent text with T5',2020-09-14T06:32:39Z,2020-09-16T05:47:57Z,,,
7112,b'prepare for the label for EncoderDecoderModel',2020-09-14T03:30:46Z,2020-09-15T06:22:08Z,,,
7111,b'MBART/Marian for low resource/backtranslation',2020-09-14T03:22:12Z,2020-09-14T15:01:59Z,,,
7110,b'[s2s] distributed eval cleanup',2020-09-14T03:08:13Z,2020-09-14T03:40:39Z,,,
7109,b'[s2s run_eval] new features',2020-09-14T02:14:36Z,2020-09-16T17:59:58Z,,,
7108,b'[QOL] add signature for prepare_seq2seq_batch',2020-09-14T00:54:22Z,2020-09-15T00:33:08Z,,,
7107,b'Update xsum length penalty to better values',2020-09-14T00:46:50Z,2020-09-14T00:48:48Z,,,
7106,b'One command to run+aggregate distributed evaluation results',2020-09-13T21:24:18Z,2020-09-21T18:01:29Z,,,
7105,b'[s2s] two stage run_distributed_eval.py',2020-09-13T19:30:43Z,2020-09-13T21:28:19Z,,,
7104,b'[s2s distill] allow pegasus-12-12',2020-09-13T19:27:08Z,2020-09-14T04:04:00Z,,,
7103,b'ValueError: Wrong shape for input_ids (shape torch.Size([18])) or attention_mask (shape torch.Size([18]))',2020-09-13T17:04:26Z,2020-09-19T23:02:15Z,,ValueError,"ValueError: Wrong shape for input_ids (shape torch.Size([18])) or attention_mask (shape torch.Size([18]))"
7102,b'Longformer inference time',2020-09-13T13:28:50Z,2020-11-22T08:04:33Z,wontfix,,
7101,b'[docs] add testing documentation',2020-09-13T05:41:02Z,2020-09-15T23:25:27Z,,,
7100,b'[logging] remove no longer needed verbosity override',2020-09-13T05:00:56Z,2020-09-15T08:01:14Z,,,
7099,b'[examples testing] restore code',2020-09-13T00:38:55Z,2020-09-14T12:54:23Z,,,
7098,b'broken pypi scipy package that affects tests under `examples`',2020-09-13T00:29:47Z,2020-09-13T00:30:16Z,,ModuleNotFoundError,"ModuleNotFoundError: No module named 'scipy.sparse'"
7097,b'Create README.md',2020-09-12T23:49:21Z,2020-09-15T12:48:26Z,model card,,
7096,b'Trying to speed up lost speed of tokenizer.encode',2020-09-12T23:45:39Z,2021-03-06T00:17:24Z,wontfix,,
7095,b'Create README.md',2020-09-12T21:10:18Z,2020-09-18T07:19:46Z,model card,,
7094,b'fix bug in pegasus converter',2020-09-12T20:41:35Z,2020-09-13T19:11:47Z,,,
7093,b'Update convert_pegasus_tf_to_pytorch.py',2020-09-12T18:45:17Z,2020-09-12T20:58:34Z,,,
7092,b'needing area to put download/convert/eval scripts',2020-09-12T18:43:36Z,2020-09-16T12:41:27Z,,,
7091,b'is config argument necessary for XXModel.from_pretrained method? And when is needed?',2020-09-12T16:56:54Z,2020-11-22T08:04:34Z,wontfix,,
7090,"b""TypeError: __init__() got an unexpected keyword argument 'gradient_checkpointing'""",2020-09-12T16:15:55Z,2020-11-29T11:50:14Z,wontfix,TypeError,"TypeError: __init__() got an unexpected keyword argument 'gradient_checkpointing'"
7089,b'German electra model card v3 update',2020-09-12T15:49:58Z,2020-09-15T12:48:14Z,model card,,
7088,b'train/eval step results log not shown in terminal for tf_trainer.py',2020-09-12T12:53:59Z,2020-12-13T01:23:01Z,wontfix,,
7087,b'Transformer-XL: Remove unused parameters',2020-09-12T09:55:10Z,2020-09-17T10:10:35Z,,,
7086,b'Longformer run error',2020-09-12T08:22:44Z,2020-09-13T13:06:07Z,,RuntimeError,"RuntimeError: cuda runtime error (710) : device-side assert triggered at /pytorch/aten/src/THC/THCReduceAll.cuh:327"
7085,"b""Distilbart's summaries start with an empty space?""",2020-09-12T06:07:42Z,2020-12-13T01:22:55Z,wontfix,,
7084,b'How to implement LayoutLM for information extraction',2020-09-12T06:00:07Z,2021-01-10T13:54:13Z,"wontfix, New model",,
7083,b'SqueezeBERT architecture',2020-09-12T02:10:28Z,2020-10-05T08:25:44Z,model card,,
7082,b'Longformer output_hidden_states=True outputs sequence length=512 for all inputs of different lengths',2020-09-11T23:29:24Z,2020-09-20T17:51:28Z,,,
7081,b'Clean up autoclass doc',2020-09-11T21:36:13Z,2020-09-14T13:26:41Z,,,
7080,b'Importing unittests using python unittest framework',2020-09-11T18:40:02Z,2020-11-22T08:04:32Z,wontfix,,
7079,b'ignore FutureWarning in tests',2020-09-11T18:33:25Z,2020-09-14T11:50:52Z,,,
7078,b'[T5Tokenizer] remove prefix_tokens',2020-09-11T18:00:15Z,2020-09-11T18:18:46Z,,,
7077,"b""T5Tokenizer shouldn't add pad token as prefix to labels""",2020-09-11T17:57:51Z,2020-09-11T18:18:46Z,,,
7076,"b""some sshleifer/xsum hub models  have bart-large-cnn task_specific_params['summarization']""",2020-09-11T16:22:35Z,2020-09-13T19:05:44Z,,,
7075,b'[Benchmarks] Change all args to from `no_...` to their positive form',2020-09-11T16:15:48Z,2020-09-23T17:25:25Z,,,
7074,b'Compute loss method',2020-09-11T15:59:11Z,2020-09-11T16:06:32Z,,,
7073,b'Add tests and fix various bugs in ModelOutput',2020-09-11T15:37:55Z,2020-09-11T16:01:34Z,,,
7072,"b'Clean up `benchmark_args_utils.py` ""no_..."" arguments'",2020-09-11T14:35:23Z,2020-09-23T17:25:25Z,Good First Issue,,
7071,b'added bangla-bert-base model card and also modified other model cards',2020-09-11T13:27:12Z,2020-09-11T19:17:26Z,model card,,
7070,b'MobileBERT inconsistent output (padded / not padded text)',2020-09-11T10:19:57Z,2020-11-22T08:04:39Z,wontfix,,
7069,b'How to use hugging on onw embedding',2020-09-11T09:41:32Z,2020-09-13T13:07:38Z,,,
7068,b'[BertGeneration] Clean naming',2020-09-11T06:52:15Z,2020-09-11T07:57:54Z,,,
7067,b'Create model card',2020-09-11T06:45:24Z,2020-09-11T19:20:55Z,model card,,
7066,b'Create model card',2020-09-11T06:43:05Z,2020-09-11T19:21:06Z,model card,,
7065,b'Further Pretraining of Longformer RAM Consumption',2020-09-11T06:02:20Z,2020-09-30T13:48:20Z,,,
7064,b'Add LayoutLM Model',2020-09-11T03:34:14Z,2020-09-22T13:28:02Z,model card,,
7063,b'EncoderDecoderModel  generate function',2020-09-11T02:20:43Z,2020-09-14T02:56:33Z,,TypeError,"TypeError: full() received an invalid combination of arguments - got (tuple, NoneType, device=torch.device, dtype=torch.dtype), but expected one of:"
7062,b'circleci testing issue',2020-09-10T21:59:11Z,2020-09-11T07:18:15Z,,,
7061,b'Automate the lists in auto-xxx docs',2020-09-10T21:34:06Z,2020-09-11T14:42:09Z,,,
7060,b'mBART 50',2020-09-10T19:58:45Z,2021-02-17T07:04:47Z,New model,,
7059,b'these tests require non-multigpu env',2020-09-10T19:52:53Z,2020-09-10T22:52:56Z,,,
7058,b'Document the dependcy on datasets',2020-09-10T19:22:56Z,2020-09-11T08:43:20Z,,,
7057,b'Add option to pass parameters to the loss function.',2020-09-10T18:36:34Z,2020-09-12T13:57:53Z,,,
7056,b'[wip/s2s] DistributedSortishSampler',2020-09-10T18:34:40Z,2020-09-10T19:23:45Z,,,
7055,b'[testing] test_trainer.py is failing',2020-09-10T18:30:19Z,2020-09-10T18:51:09Z,,,
7054,b'Fix CI with change of name of nlp',2020-09-10T18:15:32Z,2020-09-10T18:51:09Z,,,
7053,b'[examples] bump pl=0.9.0',2020-09-10T17:58:41Z,2020-10-11T20:39:39Z,,,
7052,b'TFBert activation layer will be casted into float32 under mixed precision policy',2020-09-10T17:56:05Z,2021-01-22T21:03:30Z,,,
7051,b'How to pass tokenized hypotheses to TFRobertaForSequenceClassification model directly for faster inference?',2020-09-10T17:47:30Z,2020-10-28T16:56:07Z,,,
7050,"b'[BertGeneration, Docs] Fix another old name in docs'",2020-09-10T15:11:41Z,2020-09-10T15:12:34Z,,,
7049,b'Convert 12-1 and 6-1 en-de models from AllenNLP',2020-09-10T15:09:02Z,2020-09-17T15:58:49Z,New model,,
7048,b'[BertGeneration] Correct Doc Title',2020-09-10T15:08:20Z,2020-09-10T15:08:41Z,,,
7047,b'T5-11b model parallelism',2020-09-10T15:00:39Z,2020-12-05T05:00:48Z,wontfix,,
7046,b'Usage of targets argument in fill-mask pipeline (Pipeline cannot handle mixed args and kwargs)',2020-09-10T14:56:49Z,2020-09-10T15:15:59Z,,,
7045,b'max_length does not seem to work',2020-09-10T14:50:14Z,2020-09-10T15:22:54Z,,,
7044,b'Small fixes in tf template',2020-09-10T14:35:02Z,2020-09-10T14:36:03Z,,,
7043,b'Batch_encode_plus with is_pretokenized=True outputs incomplete input_ids',2020-09-10T13:36:10Z,2020-10-02T12:19:39Z,,,
7042,b'the time of loading different models to GPU is nearly the same?',2020-09-10T13:26:27Z,2020-11-21T03:00:14Z,wontfix,,
7041,b'[wip/token clasification] Introduce datasets and metrics in token classification examples',2020-09-10T13:15:54Z,2020-11-24T02:58:10Z,wontfix,,
7040,b'Fix template',2020-09-10T12:39:53Z,2020-09-10T12:45:53Z,,,
7039,b'fix to ensure that returned tensors after the tokenization is Long',2020-09-10T12:16:18Z,2020-09-10T15:04:04Z,,,
7038,b'Question about the test results of my own testsets with a fine-tuned BERT',2020-09-10T11:46:20Z,2020-09-11T06:18:49Z,,,
7037,b'Update eval dataset to pick start_position at first index',2020-09-10T09:14:58Z,2020-09-10T12:29:17Z,,,
7036,"b'""You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference"" when I am finetuning on distilert pretrained model, After printing this it is taking a lot of time and using only one CPU, how can we parallelized to all the cores in the system ( even I hve 8 GPU\'s but it is not using tht)'",2020-09-10T05:32:28Z,2020-11-21T03:00:14Z,wontfix,,
7035,b'add -y to bypass prompt for transformers-cli upload',2020-09-10T05:22:57Z,2020-09-10T08:58:30Z,,,
7034,b'[xlm tok] config dict: fix str into int to match definition',2020-09-10T03:34:03Z,2020-09-10T17:31:02Z,,,
7033,b'fix deprecation warnings',2020-09-10T03:28:15Z,2020-09-14T11:51:19Z,,,
7032,b'SQuAD: Implement eval in Trainer-backed run_squad_trainer',2020-09-09T22:08:25Z,2021-01-04T14:52:41Z,Good First Issue,,
7031,b'Unable to recreate onnx speedups demonstrated in 04-onnx-export.ipynb on mac or linux',2020-09-09T21:33:28Z,2020-09-14T11:28:26Z,,machdep.cpu.tsc_ccc.denominator,"machdep.cpu.tsc_ccc.denominator: 2"
7030,b'[s2s] dynamic batch size with --max_tokens_per_batch',2020-09-09T20:48:44Z,2020-09-17T19:19:35Z,,,
7029,b'Add TF Funnel Transformer',2020-09-09T20:17:55Z,2020-09-10T14:41:57Z,,,
7028,"b'No way around ""Truncation was not explicitely activated..."" error when using SingleSentenceClassificationProcessor.'",2020-09-09T17:02:06Z,2020-11-16T03:04:53Z,wontfix,,
7027,b'Getting underling S3 URL',2020-09-09T16:13:47Z,2020-12-04T22:08:34Z,wontfix,,
7026,"b""RuntimeError: Expected tensor for argument #1 'indices' to have scalar type Long; but got torch.FloatTensor instead (while checking arguments for embedding)""",2020-09-09T16:03:10Z,2020-09-10T15:04:04Z,,,
7025,b'Python',2020-09-09T15:15:58Z,2020-09-09T18:49:53Z,,,
7024,b'Adding `class_weights` argument for the loss function of transformers model',2020-09-09T10:13:44Z,2020-09-13T02:46:06Z,,,
7023,b'PreTrained (custom) model not correctly initializing when using AutoModel methods',2020-09-09T08:30:20Z,2020-09-11T16:01:51Z,,,
7022,b'New TF output proposal',2020-09-09T08:26:44Z,2020-11-24T02:58:11Z,wontfix,,
7021,b'where can I download the pre-trained pytorch_model.bin files ? ',2020-09-09T05:35:55Z,2020-09-09T07:26:16Z,,,
7020,"b'Use  `run_language_modeling.py` to finetune gpt2, but it core unexpectedly.'",2020-09-09T02:50:13Z,2020-09-09T12:23:28Z,,,
7019,b'Proposal: Offset based Token Classification utilities ',2020-09-08T19:43:43Z,2020-11-21T21:53:07Z,wontfix,,
7018,b'[s2s] --eval_max_generate_length',2020-09-08T19:25:32Z,2020-09-10T18:11:34Z,,,
7017,b'pegasus.rst: fix expected output',2020-09-08T17:09:50Z,2020-09-08T17:29:17Z,,,
7016,b'[Longformer] Fix longformer documentation',2020-09-08T16:19:10Z,2020-09-08T16:51:29Z,,,
7015,"b'Longformer global attention mask, 2 or 1? '",2020-09-08T15:55:28Z,2020-09-08T16:51:29Z,,,
7014,b'[wip] Pegasus: Hack to never generate unk',2020-09-08T15:06:01Z,2020-10-06T18:48:07Z,,,
7013,b'Fixing FLOPS merge by checking if torch is available',2020-09-08T14:22:53Z,2020-09-08T14:51:59Z,,,
7012,b'Error in run_language_modeling on TPU: Transferring data with element type U8 has not been implemented on TPUs',2020-09-08T11:35:14Z,2020-11-15T17:49:58Z,wontfix,RuntimeError,"RuntimeError: tensorflow/compiler/xla/xla_client/xrt_computation_client.cc:383 : Check failed: session->session()->Run( session_work->feed_inputs, session_work->outputs_handles, &outputs) == ::tensorflow::Status::OK() (Unimplemented: From /job:tpu_worker/replica:0/task:0:"
7011,"b""getting 'ValueError-TextInputSequence must be str' in 'train_dataset = train_dataset.map(convert_to_features)'""",2020-09-08T10:30:34Z,2020-09-09T09:27:56Z,,,
7010,"b'Huggingface ""sentiment-analysis"" pipeline always output ""POSITIVE"" label even for negative sentences'",2020-09-08T10:11:27Z,2020-09-08T11:25:30Z,,,
7009,b'Index out of range in Bart-large-xsum',2020-09-08T09:35:31Z,2020-09-09T14:19:51Z,,,
7008,b'Diverse Beam Search decoding',2020-09-08T08:47:47Z,2021-02-25T14:24:49Z,,,
7007,b'MLM performance difference between bert-base-cased and Conversational BERT',2020-09-08T08:12:42Z,2020-11-15T17:49:57Z,wontfix,,
7006,"b""__init__() got an unexpected keyword argument 'cache_dir'""",2020-09-08T06:55:52Z,2020-09-09T02:07:52Z,,TypeError,"TypeError: __init__() got an unexpected keyword argument 'cache_dir'"
7005,b'[Community notebooks] Add notebook on fine-tuning GPT-2 Model with Trainer Class',2020-09-08T06:44:52Z,2020-09-08T07:42:20Z,,,
7004,"b'[seq2seq Examples] Use _step instead of generate for val, test'",2020-09-08T05:53:54Z,2020-09-17T00:13:05Z,,,
7003,"b""On Gpu sharing mechanism, specify model.to(' CPU '), but still use the Gpu""",2020-09-08T02:59:33Z,2020-09-08T04:12:59Z,,,
7002,"b'How to bypass ""Special tokens have been added in the vocabulary..."" warning?'",2020-09-08T02:19:40Z,2020-09-11T18:14:18Z,,,
7001,b'typo',2020-09-08T02:01:34Z,2020-09-08T05:22:21Z,,,
7000,b'access to the embeddings for query and text used in a downstream NLP task',2020-09-07T21:28:49Z,2020-09-07T22:12:14Z,,,
6999,b'fixed trainer tr_loss memory leak',2020-09-07T21:00:26Z,2020-09-08T12:07:34Z,,,
6998,b'Fix TF Trainer loss calculation',2020-09-07T19:49:53Z,2020-09-15T09:41:01Z,,,
6997,b'run_squad.py not working on 3.1.0 version',2020-09-07T19:36:36Z,2020-11-16T03:04:54Z,wontfix,AttributeError,"AttributeError: type object 'BertConfig' has no attribute 'pretrained_config_archive_map'"
6996,b'[generation] decoder priority for choosing decoder_start_token_id value',2020-09-07T18:27:44Z,2020-09-08T18:06:22Z,,,
6995,b'[from_pretrained] Allow tokenizer_type \xe2\x89\xa0 model_type',2020-09-07T17:15:30Z,2020-09-09T08:23:00Z,,,
6994,b'Fix typo',2020-09-07T16:38:05Z,2020-09-08T08:22:58Z,,,
6993,b'PegasusForConditionalGeneration stops at unknown token',2020-09-07T13:43:28Z,2020-10-16T18:11:37Z,seq2seq,,
6992,b'Mobile Bert Tiny model',2020-09-07T12:30:56Z,2020-09-07T12:34:00Z,,,
6991,"b""Conversion scripts shouldn't have relative imports""",2020-09-07T12:30:53Z,2020-09-07T12:31:07Z,,,
6990,b'README for HooshvareLab/bert-fa-base-uncased',2020-09-07T12:04:59Z,2020-09-07T20:43:51Z,model card,,
6989,"b""TypeError: __init__() got an unexpected keyword argument 'cache_dir'""",2020-09-07T11:53:05Z,2020-09-07T11:59:34Z,,TypeError,"TypeError: __init__() got an unexpected keyword argument 'cache_dir'"
6988,b't5 embed_tokens',2020-09-07T10:41:11Z,2020-09-07T10:41:28Z,,,
6987,"b"" DefaultCPUAllocator: can't allocate memory: you tried to allocate 100663296 bytes""",2020-09-07T10:00:19Z,2020-09-09T11:50:52Z,,RuntimeError,"RuntimeError: [enforce fail at CPUAllocator.cpp:64] . DefaultCPUAllocator: can't allocate memory: you tried to allocate 100663296 bytes. Error code 12 (Cannot allocate memory)"
6986,b'Demoing LXMERT with raw images by incorporating the FRCNN model for roi-pooled extraction and bounding-box predction on the GQA answer set.',2020-09-07T09:27:54Z,2020-09-14T14:07:05Z,,,
6985,b'Enhance a MarianMT pretrained model from HuggingFace with more training data',2020-09-07T09:13:00Z,2020-11-14T09:26:53Z,wontfix,,
6984,b'Cannot index `None`',2020-09-07T08:39:25Z,2020-09-07T08:56:08Z,,,
6983,b'[generation] multiple eos/pad asserts/ifs in generate search functions',2020-09-07T07:07:55Z,2020-10-16T18:12:01Z,,,
6982,b'[generation] consistently add eos tokens',2020-09-07T06:19:50Z,2020-09-09T08:08:37Z,,,
6981,b'LongformerForQuestionAnswering sample code error',2020-09-07T04:51:49Z,2020-09-07T07:21:55Z,,TypeError,"TypeError: __init__() got an unexpected keyword argument 'return_dict'"
6980,b'[gen utils] missing else case',2020-09-07T02:50:42Z,2020-09-07T11:28:07Z,,,
6979,b'RunTime Error: CUDA out of memory when running trainer.train()',2020-09-07T02:48:13Z,2020-09-07T03:10:18Z,,,
6978,b'[gen utils] missing else case',2020-09-07T02:46:53Z,2020-09-07T02:50:50Z,,,
6977,b'[s2s] warn if --fp16 for torch 1.6',2020-09-06T23:28:32Z,2020-09-07T00:41:30Z,,,
6976,b'LXMERT imports',2020-09-06T20:49:08Z,2020-10-15T18:18:41Z,,ValueError,"ValueError: Unrecognized configuration class <class 'transformers.configuration_lxmert.LxmertConfig'> for this kind of AutoModel: AutoModelWithLMHead."
6975,b'Created README for labse_bert model card',2020-09-06T19:09:44Z,2020-09-15T12:54:13Z,model card,,
6974,b'Create README.md',2020-09-06T15:29:55Z,2020-09-07T11:31:23Z,model card,,
6973,b'Fixed the default number of attention heads in Reformer Configuration',2020-09-06T09:45:18Z,2020-09-07T10:12:23Z,,,
6972,b'The configuration of 3.0.2 and 3.1.0 is not compatible',2020-09-06T08:25:34Z,2020-09-08T02:41:54Z,,,
6971,b'distilled bart-large/bart-base',2020-09-06T02:32:04Z,2020-12-24T11:26:49Z,"wontfix, New model",,
6970,b'Error installing transformers 3.1.0',2020-09-05T22:29:00Z,2020-11-29T11:50:21Z,wontfix,,
6969,b'Incorrect loss calculation for the last batch in TFTrainer if dataloader_drop_last is False',2020-09-05T21:53:51Z,2020-11-14T09:26:52Z,wontfix,,
6968,b'Potential incorrect loss calculation for TFTokenClassification in TFTrainer',2020-09-05T20:59:01Z,2020-09-15T09:41:01Z,,,
6967,b'hack to extract cross attention for bart decoder',2020-09-05T20:23:49Z,2020-09-25T15:28:24Z,,,
6966,b'SPM Tokenizer confusion with fairseq Roberta',2020-09-05T20:23:09Z,2020-09-09T20:27:42Z,,,
6965,b'transformers-cli upload individual files simplification',2020-09-05T20:10:03Z,2020-10-28T20:27:03Z,,,
6964,b'Create README.md model card',2020-09-05T18:44:50Z,2020-09-07T10:01:41Z,model card,,
6963,b'Longformer config - vocabulary size',2020-09-05T17:28:59Z,2020-09-07T07:36:11Z,,,
6962,b'Tokenizers became slow compared to 2.8.0',2020-09-05T15:43:18Z,2020-11-22T08:04:31Z,wontfix,,
6961,b'adding TRANSFORMERS_VERBOSITY env var',2020-09-05T06:38:01Z,2020-09-09T08:08:01Z,,,
6960,b'create model card for astroGPT',2020-09-05T01:25:43Z,2020-09-05T16:50:20Z,model card,,
6959,b'typo',2020-09-05T00:25:07Z,2020-09-07T09:16:25Z,,,
6958,b'[testing] add dependency: parametrize',2020-09-05T00:02:48Z,2020-09-07T09:50:19Z,,,
6957,b'PRETRAINED_INIT_CONFIGURATION for local model path',2020-09-04T21:26:28Z,2020-09-05T20:54:19Z,,,
6956,"b'[doc] remove the implied defaults to :obj:`None`, s/True/ :obj:`True/, etc.'",2020-09-04T20:32:25Z,2020-09-04T22:22:25Z,,,
6955,b'[WIP] Language modeling example for TF Trainer',2020-09-04T19:26:40Z,2020-11-14T05:02:32Z,wontfix,,
6954,b'How to insert a hidden output from GPT2 model directly into a BERT layer?',2020-09-04T19:22:39Z,2020-09-04T19:42:23Z,,RuntimeError,"RuntimeError: number of dims don't match in permute"
6953,b'[s2s] run_eval supports --prefix clarg.',2020-09-04T18:50:44Z,2020-09-12T05:08:22Z,,,
6952,b'typo',2020-09-04T18:47:34Z,2020-09-04T20:14:38Z,,,
6951,b'How to enable grad_fn when calling the generate() method of a T5 model',2020-09-04T18:17:44Z,2020-09-04T19:30:02Z,,,
6950,b'head_mask in modeling_bert.py',2020-09-04T18:14:25Z,2020-09-07T08:56:08Z,,,
6949,b'Refactoring the generate() function',2020-09-04T17:22:00Z,2020-11-03T15:04:23Z,,,
6948,b'[s2s] run_eval.py parses generate_kwargs',2020-09-04T17:12:49Z,2020-09-04T18:19:32Z,,,
6947,b'Training script for other language (except English)',2020-09-04T14:36:42Z,2020-09-04T20:09:16Z,,,
6946,b'[LXMERT] Fix tests on gpu',2020-09-04T14:00:18Z,2020-09-04T14:08:55Z,,,
6945,"b""Restoring ELECTRA-Small checkpoint doesn't work properly""",2020-09-04T13:58:28Z,2020-09-10T08:53:08Z,,,
6944,b'Finetuning XLM-Roberta-2XLM-Roberta on custom dataset gives the following error:',2020-09-04T09:14:14Z,2020-11-29T11:50:00Z,wontfix,,
6943,b'Transformer-XL: Remove unused/unnecessary Parameters',2020-09-04T09:08:12Z,2020-09-17T10:10:35Z,,,
6942,b'Create Readme.MD for KanBERTo',2020-09-04T08:08:23Z,2020-09-04T22:24:33Z,model card,,
6941,"b""match CI's version of flake8""",2020-09-04T06:49:49Z,2020-09-07T12:12:26Z,,,
6940,b'[ported model] FSMT (FairSeq MachineTranslation)',2020-09-04T06:34:31Z,2020-09-17T15:31:30Z,model card,,
6939,b'PyTorch (with GPU) Trainer leaks CPU memory on Google Colab',2020-09-04T05:02:44Z,2020-09-08T12:07:34Z,,,
6938,b'The downloading url of GermEval 2014 dataset is out dated.',2020-09-04T03:25:25Z,2020-11-14T09:27:08Z,wontfix,,
6937,b'Finetune other models for sentence-classification',2020-09-04T03:16:27Z,2020-09-07T12:17:25Z,,,
6936,b'Load BERT+GPT2 in EncoderDecoder',2020-09-04T03:10:18Z,2020-11-14T09:26:54Z,wontfix,,
6935,b'Replaced torch.load for loading the pretrained vocab of TransformerXL tokenizer to pickle.load',2020-09-04T01:53:56Z,2020-10-08T08:16:11Z,,,
6934,b'non-interactive transformers-cli upload?',2020-09-04T00:46:46Z,2020-09-10T08:58:30Z,,,
6933,b'[docstring] missing arg',2020-09-03T22:14:07Z,2020-09-07T09:36:17Z,,,
6932,b'[docstring] misc arg doc corrections',2020-09-03T22:07:34Z,2020-09-04T14:09:43Z,,,
6931,b'remove arg that is not being used',2020-09-03T21:56:37Z,2020-09-24T21:24:40Z,,,
6930,b'Trainer with grad accum',2020-09-03T20:54:23Z,2020-09-07T08:54:00Z,,,
6929,b'replace torch.triu with onnx compatible code',2020-09-03T20:49:15Z,2020-09-09T08:56:41Z,,,
6928,b'onnx-export example notebook is failing for TF',2020-09-03T18:50:26Z,2020-09-07T10:51:11Z,,AttributeError,"AttributeError: 'str' object has no attribute 'parent'"
6927,"b'[s2s] support early stopping based on loss, rather than rouge'",2020-09-03T17:05:53Z,2020-09-03T21:31:36Z,,,
6926,b'[s2s] use --eval_beams command line arg',2020-09-03T16:19:35Z,2020-09-03T16:42:10Z,,,
6925,b'Reopen: Unable to use run_squad with xla_spawn.py on TPU',2020-09-03T14:55:36Z,2020-11-14T09:26:55Z,wontfix,AttributeError,"AttributeError: module 'run_squad' has no attribute '_mp_fn'"
6924,"b""AttributeError: 'list' object has no attribute 'clone' with BartTokenizer""",2020-09-03T14:52:29Z,2020-09-03T15:02:33Z,,AttributeError,"AttributeError: 'list' object has no attribute 'clone'"
6923,b'[s2s] allow task_specific_params=summarization_xsum',2020-09-03T14:10:38Z,2020-09-03T15:11:41Z,,,
6922,b'inference over onnx output',2020-09-03T13:58:31Z,2020-11-14T09:27:06Z,wontfix,,
6921,b'[model_cards] Fixed some typing mistakes in usage sections in model cards.',2020-09-03T13:00:30Z,2020-09-03T13:13:44Z,model card,,
6920,b'(ONNX) Error while converting the model: bad allocation',2020-09-03T10:07:20Z,2020-09-04T04:03:09Z,,,
6919,b'tweak tar command in readme',2020-09-03T03:39:18Z,2020-09-03T13:29:02Z,,,
6918,"b'RuntimeError: Internal: /sentencepiece/src/sentencepiece_processor.cc(818) [model_proto->ParseFromArray(serialized.data(), serialized.size())] '",2020-09-03T02:41:03Z,2020-11-14T09:27:00Z,wontfix,RuntimeError,"RuntimeError: Internal: /sentencepiece/src/sentencepiece_processor.cc(818) [model_proto->ParseFromArray(serialized.data(), serialized.size())] "
6917,b'T5 Tokenizer fails to decode correctly and prints \xe2\x81\x87',2020-09-03T01:56:47Z,2020-09-07T20:49:35Z,,,
6916,"b""[model weights caching] model upload doesn't check model weights hash""",2020-09-02T21:47:07Z,2020-11-08T00:59:03Z,wontfix,,
6915,b'Fix mixed precision issue in TF DistilBert',2020-09-02T20:54:10Z,2020-09-04T12:29:58Z,,,
6914,b'Template updates',2020-09-02T19:57:03Z,2020-09-03T08:14:59Z,,,
6913,b'Small bug on website',2020-09-02T19:49:36Z,2020-09-03T08:29:28Z,,,
6912,b'batch_encode_plus does not lead to the same predictions as encode_plus',2020-09-02T19:34:48Z,2020-11-14T09:27:04Z,wontfix,,
6911,b'[s2s]: script to convert pl checkpoints to hf checkpoints',2020-09-02T19:15:41Z,2020-09-03T13:47:00Z,,,
6910,b'adding additional additional_special_tokens to tokenizer has inconsistent behavior',2020-09-02T18:40:55Z,2020-11-09T01:56:13Z,wontfix,,
6909,b'[style] automate reformatting with pre-commit hooks',2020-09-02T17:38:07Z,2020-11-09T01:56:11Z,wontfix,,
6908,b'Funnel transformer',2020-09-02T16:21:06Z,2020-09-08T12:08:09Z,,,
6907,b'Torchscript benchmark measure',2020-09-02T15:31:10Z,2020-11-13T13:10:40Z,wontfix,,
6906,b'Update to the huBERT model card.',2020-09-02T14:27:22Z,2020-09-03T13:20:04Z,model card,,
6905,b'Changed link to the correct paper in the second paragraph',2020-09-02T14:02:51Z,2020-09-03T13:23:42Z,model card,,
6904,b'Greedy decoding for non-beam-search appears to ignore postprocessing',2020-09-02T12:41:06Z,2020-09-02T12:57:46Z,,,
6903,b'Output attention takes an s',2020-09-02T11:58:40Z,2020-09-02T12:11:46Z,,,
6902,"b""Example config code uses invalid 'output_attention' rather than 'output_attentions'""",2020-09-02T11:25:31Z,2020-09-02T12:11:46Z,,AttributeError,"AttributeError: 'BertConfig' object has no attribute 'output_attention'"
6901,b'Relaxing `PreTrainedModel` requirement in _save',2020-09-02T06:59:02Z,2020-09-29T14:41:19Z,,,
6900,b'Can DistilBert.forward() support token_type_ids ?',2020-09-02T06:50:04Z,2020-11-29T11:50:08Z,wontfix,,
6899,b'Can the GPT2 of Transformers receive output hidden_states from external Encoder?',2020-09-02T04:41:52Z,2020-11-15T17:50:00Z,wontfix,,
6898,b'[testing] fix ambiguous test',2020-09-02T04:31:31Z,2020-09-02T14:18:17Z,,,
6897,b'Update modeling_bert.py',2020-09-02T03:40:44Z,2020-09-02T10:39:02Z,,,
6896,"b'the result of translation task on en-zh is not good,especially in short text'",2020-09-02T03:26:33Z,2020-11-09T01:56:16Z,wontfix,,
6895,b'Create README.md',2020-09-02T03:04:27Z,2020-09-02T03:05:00Z,,,
6894,b'Getting import error',2020-09-01T23:47:16Z,2020-11-14T09:26:58Z,wontfix,OSError,"OSError: Can't load 'roberta-large'. Make sure that:"
6893,b'Model card for huBERT',2020-09-01T23:14:07Z,2020-09-02T08:50:10Z,model card,,
6892,b'[t5] Missing requirements in examples/seq2seq',2020-09-01T22:49:09Z,2020-09-01T23:40:10Z,,,
6891,"b""AttributeError: 'DistilBertConfig' object has no attribute 'return_dict'""",2020-09-01T21:40:28Z,2020-09-02T00:36:57Z,,AttributeError,"AttributeError: 'DistilBertConfig' object has no attribute 'return_dict'"
6890,"b'[Docs, Examples] Fix QA example for PT'",2020-09-01T20:24:27Z,2020-09-02T07:53:10Z,,,
6889,b'minor docs grammar fixes',2020-09-01T19:50:08Z,2020-09-02T10:45:20Z,,,
6888,b'Create README.md',2020-09-01T18:27:28Z,2020-09-01T21:09:02Z,model card,,
6887,b'Create README.md',2020-09-01T18:26:16Z,2020-09-01T21:09:10Z,model card,,
6886,b'Create README.md',2020-09-01T18:16:35Z,2020-09-01T21:06:15Z,model card,,
6885,b'Create README.md',2020-09-01T17:44:33Z,2020-09-01T20:57:49Z,model card,,
6884,b'[Electra] fix warning for position ids',2020-09-01T17:34:02Z,2020-09-02T10:44:52Z,,,
6883,b'Create README.md',2020-09-01T17:23:57Z,2020-09-01T17:24:46Z,,,
6882,b'Bert Checkpoint Breaks 3.02 -> 3.1.0 due to new buffer in BertEmbeddings',2020-09-01T17:22:02Z,2020-12-19T06:31:51Z,wontfix,,
6881,"b""'BertEmbeddings' object has no attribute 'bias' while converting tf checkpoint""",2020-09-01T16:14:50Z,2020-09-02T08:24:18Z,,"**AttributeError, AttributeError","**AttributeError: 'BertEmbeddings' object has no attribute 'bias'**AttributeError: 'BertEmbeddings' object has no attribute 'bias'"
6880,b'Fix TF Trainer for TPU',2020-09-01T15:40:14Z,2020-11-04T21:18:37Z,,,
6879,b'Add cache_dir to save features TextDataset',2020-09-01T15:15:29Z,2020-09-01T15:42:18Z,,,
6878,b'[EncoderDecoder] Add xlm-roberta to encoder decoder',2020-09-01T14:17:00Z,2020-09-01T19:56:40Z,,,
6877,"b'[WIP, TF] replace keras dense by keras.layers.DenseEinsum'",2020-09-01T11:25:35Z,2020-10-19T09:22:50Z,,,
6876,b'[TF T5] Possible Error using TF T5 with Keras',2020-09-01T10:28:04Z,2021-04-16T15:04:01Z,,,
6875,b'Restore PaddingStrategy.MAX_LENGTH on QAPipeline while no v2.',2020-09-01T09:33:26Z,2020-09-01T09:35:36Z,,,
6874,b'gradient_accumulation_steps in trainer_tf',2020-09-01T09:15:50Z,2020-11-21T03:00:18Z,wontfix,,
6873,b'Memory blowup with TPU Trainer in master',2020-09-01T07:57:24Z,2020-12-24T11:26:53Z,wontfix,,
6872,b'transformer multitasking',2020-09-01T07:45:56Z,2020-11-09T01:56:21Z,wontfix,,
6871,b'Albert loads model on both CPU and GPU at the same time',2020-09-01T07:06:50Z,2020-11-09T01:56:20Z,wontfix,,
6870,b'Updata tokenization_auto.py',2020-09-01T06:45:46Z,2020-09-24T10:52:11Z,,,
6869,b'How does relative distance is computed for cross-attention in T5 model?',2020-09-01T06:18:40Z,2020-11-09T01:56:19Z,wontfix,,
6868,b'MarianMTModel.generate error: Segmentation fault (core dumped)',2020-09-01T06:13:06Z,2020-09-01T08:52:08Z,,,
6867,b'[doc] typos',2020-09-01T06:09:49Z,2020-09-02T10:51:52Z,,,
6866,b'test_tf_common: remove un_used mixin class parameters',2020-09-01T05:36:05Z,2020-09-02T14:54:41Z,,,
6865,b'Is it possible to finetune reformer model for summarization task? ',2020-09-01T04:58:27Z,2020-12-13T01:22:44Z,wontfix,,
6864,b'How to save the whole model as SavedModel format for inference?',2020-09-01T04:32:44Z,2020-11-21T03:00:16Z,wontfix,,
6863,b'special token inconsistency for [UNK] token',2020-09-01T03:43:31Z,2020-11-09T01:56:14Z,wontfix,,
6862,b'cleanup: fix typo chunk_size_feed_forward in configuration_utils.py',2020-09-01T03:23:53Z,2020-09-01T07:43:28Z,,,
6861,b'add a final report to all pytest jobs',2020-09-01T02:10:50Z,2020-09-01T02:47:24Z,,,
6860,b'Support nested data structures for input data',2020-08-31T23:51:59Z,2020-11-09T01:56:18Z,wontfix,,
6859,b'[fix] typo in available in helper function',2020-08-31T20:13:53Z,2020-08-31T21:59:34Z,,,
6858,b'Remove hard-coded uses of float32 to fix mixed precision use in Distilbert',2020-08-31T20:10:27Z,2020-09-04T12:29:58Z,,,
6857,b'Split hp search methods',2020-08-31T18:56:09Z,2020-08-31T19:16:40Z,,,
6856,b'Changes in Pytorch 1.6 multinomial could break backward compatibility',2020-08-31T18:40:29Z,2020-11-14T09:27:05Z,wontfix,,
6855,b'Hugging face - RuntimeError: Caught RuntimeError in replica 0 on device 0 on Azure Databricks',2020-08-31T18:38:05Z,2020-11-07T05:43:03Z,wontfix,RuntimeError,"RuntimeError: Caught RuntimeError in replica 0 on device 0."
6854,b'Fix marian slow test',2020-08-31T18:34:02Z,2020-08-31T20:10:44Z,,,
6853,b'FAILED tests/test_modeling_marian.py::TestMarian_EN_DE_More::test_forward ',2020-08-31T18:24:17Z,2020-08-31T20:10:44Z,,,
6852,b'Logging doc',2020-08-31T17:47:24Z,2020-09-01T07:16:35Z,,,
6851,b'Distill marian',2020-08-31T16:48:40Z,2020-08-31T16:48:51Z,,,
6850,b'move wandb/comet logger init to train() to allow parallel logging',2020-08-31T15:49:26Z,2020-09-03T15:49:15Z,,,
6849,b'Printing probabilities',2020-08-31T15:47:01Z,2020-09-02T10:30:54Z,,,
6848,b'unexpected behavior on RoBERTa tokenizer when using additional special tokens ',2020-08-31T15:21:19Z,2020-11-07T05:43:02Z,wontfix,,
6847,b'Fix resuming training for Windows',2020-08-31T14:59:45Z,2020-08-31T15:02:31Z,,,
6846,b'Separate implementation for Torch-Scriptable BERT model',2020-08-31T14:55:51Z,2021-03-06T00:17:26Z,wontfix,,
6845,b'Fix in Adafactor docstrings',2020-08-31T14:45:46Z,2020-08-31T14:52:48Z,,,
6844,b'Pegasus: replication and distillation results',2020-08-31T14:39:14Z,2020-10-16T16:04:45Z,"Help wanted, Replication",,
6843,b'Adding another translation example',2020-08-31T12:38:08Z,2020-11-02T08:20:57Z,wontfix,,
6842,b'Update `convert_bart` script to allow loading nonstandard model architectures and custom pretrained Fairseq models.',2020-08-31T10:58:25Z,2020-10-30T13:22:33Z,,,
6841,b'TF Flaubert w/ pre-norm',2020-08-31T07:52:24Z,2020-08-31T08:53:21Z,,,
6840,b'Model cards for loodos',2020-08-31T07:26:45Z,2020-09-01T21:35:24Z,model card,,
6839,b'pegasus-large: Can we have input text descriptions more than the maximum input length of 512?',2020-08-31T06:43:19Z,2020-08-31T12:20:50Z,,,
6838,b'fix typo in comments (modeling_bert)',2020-08-31T03:23:26Z,2020-09-02T10:55:37Z,,,
6837,b'tokenization_gpt2 save vocabulary is not saving special tokens',2020-08-31T01:41:13Z,2020-08-31T07:59:23Z,,,
6836,b'RAM MemoryError',2020-08-31T01:11:42Z,2020-12-24T20:34:49Z,wontfix,,
6835,b'DistributedSortishSampler',2020-08-30T23:09:30Z,2020-09-10T19:23:45Z,"Help wanted, Examples",,
6834,b'[s2s] distill: --normalize_hidden --supervise_forward',2020-08-30T21:16:20Z,2020-09-04T18:05:57Z,,,
6833,b'[s2s] command line args for faster val steps',2020-08-30T21:09:26Z,2020-08-31T20:16:11Z,,,
6832,b'Model.fit on GPT2 and TPUs',2020-08-30T20:23:07Z,2020-11-07T05:43:01Z,wontfix,InvalidArgumentError,"InvalidArgumentError: 9 root error(s) found."
6831,b'Update ONNX notebook to include section on quantization.',2020-08-30T20:16:33Z,2020-08-31T19:28:00Z,,,
6830,b'Related to abstractive text summarization',2020-08-30T20:05:44Z,2020-11-07T05:43:03Z,wontfix,,
6829,"b""No attribute '_mp_fn' when fine-tuning mbart for en-ro translation task using TPU""",2020-08-30T18:04:17Z,2020-10-16T18:13:26Z,,AttributeError,"AttributeError: module 'finetune' has no attribute '_mp_fn'"
6828,b'regarding the max token length of longformer',2020-08-30T13:44:28Z,2020-11-16T03:04:52Z,wontfix,,
6827,b'Add model card for singbert lite.',2020-08-30T08:43:48Z,2020-08-30T10:21:49Z,model card,,
6826,b'Loading a converted pytorch model in huggingface transformers properly',2020-08-30T06:55:07Z,2020-11-07T05:42:55Z,wontfix,,
6825,b'Fixed open in colab link',2020-08-30T05:56:51Z,2020-08-30T10:21:01Z,,,
6824,"b""How to convert '.bin' model to '.onnx'""",2020-08-30T04:57:45Z,2020-11-07T05:42:54Z,wontfix,,
6823,b'How to use encode_plus to force padding to specific length',2020-08-30T01:47:07Z,2020-08-30T07:05:40Z,,,
6822,b'[s2s README] link to cnn dataset with empty lines removed',2020-08-29T22:02:00Z,2020-08-29T22:05:56Z,,,
6821,b'How to generate on multiple GPUs?',2020-08-29T21:12:11Z,2021-05-27T15:10:55Z,,,
6820,b'Bert transformer issue',2020-08-29T18:46:16Z,2020-11-07T05:42:53Z,wontfix,,
6819,b'Unable to establish Lock on cached tokenizer output from RobertaTokenizer',2020-08-29T18:44:26Z,2020-08-29T18:54:35Z,,FileNotFoundError,"FileNotFoundError: [Errno 2] No such file or directory: '../glue_data/cached_dev_RobertaTokenizer_128_mnli.lock'"
6818,b'[tests] fix typos in inputs',2020-08-29T18:10:46Z,2020-08-30T10:19:58Z,,,
6817,b'Tensorflow 2 Finetuning TF T5 using keras fit ',2020-08-29T10:34:12Z,2020-09-23T07:05:55Z,,ValueError,"ValueError: in user code:"
6816,b'control framework loglevel in scripts and tests',2020-08-29T04:51:38Z,2020-09-09T19:21:18Z,,,
6815,b'make the tmp dir configurable/persistent in tokenizer tests',2020-08-29T01:33:58Z,2020-10-07T03:17:26Z,,,
6814,b'Create smaller number of heads in attn without pruning using shared parameters',2020-08-29T01:31:30Z,2020-11-07T05:42:52Z,wontfix,,
6813,b'RAG',2020-08-28T22:34:26Z,2020-09-22T16:29:59Z,,,
6812,b'Potential bug in PLM training',2020-08-28T22:29:43Z,2021-01-18T07:02:08Z,wontfix,,
6811,b'Pegasus finetune script: add --adafactor',2020-08-28T21:21:00Z,2020-08-29T21:43:33Z,,,
6810,b'[s2s] save first batch to json for debugging purposes',2020-08-28T21:08:39Z,2020-10-06T20:11:57Z,,,
6809,b'[s2s] Test hub configs in self-scheduled CI',2020-08-28T21:04:41Z,2020-08-28T21:05:53Z,Tests,,
6808,b'bart-large-cnn ROUGE-L scores',2020-08-28T20:56:32Z,2020-09-27T20:27:19Z,,,
6807,b'[WIP] Added token_type_id support to GPT2Model',2020-08-28T20:32:07Z,2020-11-14T05:02:31Z,wontfix,,
6806,b'distilbart-cnn reproduction',2020-08-28T20:29:08Z,2020-11-14T09:27:08Z,wontfix,,
6805,b'[WIP] Added token_type_id support to GPT2Model',2020-08-28T20:04:31Z,2020-08-28T20:07:34Z,,,
6804,b'BART ce loss ignores pad_token_id instead of -100',2020-08-28T18:57:23Z,2020-09-01T13:26:54Z,,,
6803,b'Fix style',2020-08-28T18:52:52Z,2020-08-28T19:02:25Z,,,
6802,b'Only access loss tensor every logging_steps',2020-08-28T18:47:10Z,2020-08-31T15:35:52Z,model card,,
6801,b'Model card for primer/BART-Squad2',2020-08-28T16:52:12Z,2020-09-01T21:52:32Z,model card,,
6800,b't5 model should make decoder_attention_mask',2020-08-28T16:44:45Z,2020-08-28T19:22:34Z,,,
6799,b'Marian distill scripts + integration test',2020-08-28T16:42:07Z,2020-08-31T17:48:27Z,,,
6798,b'[s2s] round runtime in run_eval',2020-08-28T15:41:42Z,2020-08-29T21:36:32Z,,,
6797,b'2 Slow Test Failures That Sam Can Fix',2020-08-28T15:34:43Z,2020-08-31T17:48:27Z,Tests,,
6796,"b""AttributeError: 'MarianTokenizer' object has no attribute 'prepare_translation_batch'""",2020-08-28T12:43:29Z,2020-08-31T16:40:06Z,marian,"AttributeError, RuntimeError","AttributeError: 'MarianTokenizer' object has no attribute 'prepare_translation_batch'RuntimeError: Internal: /sentencepiece/src/sentencepiece_processor.cc(818) [model_proto->ParseFromArray(serialized.data(), serialized.size())] "
6795,b'Maybe global step should be initialized to 0',2020-08-28T12:33:06Z,2020-11-07T05:42:48Z,wontfix,,
6794,b'Unexpected behavior encoding token_type_ids in GPT models',2020-08-28T12:13:42Z,2020-11-07T05:42:50Z,wontfix,,
6793,b'Update README of my model',2020-08-28T11:48:44Z,2020-08-30T11:02:47Z,model card,,
6792,b'F16 support for DistilBert',2020-08-28T09:39:10Z,2020-09-04T06:01:31Z,,,
6791,b'Enable wandb logging for Ray Tune HPO runs',2020-08-28T09:30:00Z,2020-09-03T18:44:37Z,,,
6790,"b""What is the size of the context window in the 'openai-gpt' pre-trained model?""",2020-08-28T09:17:02Z,2020-11-07T05:42:47Z,wontfix,,
6789,b'How to fine-tune T5 with some additional special tokens ?',2020-08-28T08:44:33Z,2020-08-31T16:56:19Z,,,
6788,b'Update multilingual passage rereanking model card',2020-08-28T06:56:22Z,2020-09-01T21:56:19Z,model card,,
6787,"b'Dear transformers team, how could I use bert to NER task?'",2020-08-28T04:28:59Z,2020-11-22T08:04:39Z,wontfix,,
6786,b'T5 batch inference same input data gives different outputs?',2020-08-28T03:51:47Z,2021-01-11T11:58:18Z,wontfix,,
6785,b'[help] Add multigpu evalulation script for seq2seq',2020-08-28T03:43:48Z,2020-09-13T21:28:18Z,"Help wanted, seq2seq, Examples, translation",,
6784,b'[style] set the minimal required version for `black`',2020-08-28T03:29:41Z,2020-08-28T03:38:10Z,,,
6783,b'Why does examples/seq2seq/finetune.py only use sortish sampler for train? ',2020-08-28T02:39:32Z,2020-09-21T18:03:19Z,"seq2seq, Examples",,
6782,b'[style] a new `black` version is out there (20.8)',2020-08-28T02:05:26Z,2020-08-28T02:09:42Z,,,
6781,b'Typo in modling_tf_bert',2020-08-28T01:15:26Z,2020-11-21T03:00:17Z,wontfix,,
6780,b'How can I get rid of this message every time I run GPT2?',2020-08-28T00:55:27Z,2020-11-07T05:42:44Z,wontfix,,
6779,b'Character-level tokenization?',2020-08-27T23:24:49Z,2020-11-07T05:42:45Z,wontfix,,
6778,"b'Bertology-like Analysis for BART, T5?'",2020-08-27T23:19:39Z,2020-11-07T05:42:42Z,wontfix,,
6777,b'[transformers-cli] fix logger getter',2020-08-27T22:57:20Z,2020-08-28T00:01:17Z,,AttributeError,"AttributeError: module 'transformers.utils.logging' has no attribute 'getLogger'"
6776,b'PL: --adafactor option',2020-08-27T22:37:23Z,2020-08-28T02:19:47Z,,,
6775,b'Support fast tokenizer in Question Answering Pipeline',2020-08-27T22:00:42Z,2020-11-16T03:04:39Z,wontfix,,
6774,b'Task specific params for pegasus-large to allow finetuning with correct generation_parameters',2020-08-27T20:12:58Z,2020-08-29T21:37:25Z,,,
6773,b'Error in run_ner.py',2020-08-27T19:24:58Z,2020-11-07T05:42:43Z,wontfix,UnicodeDecodeError,"UnicodeDecodeError: 'utf-8' codec can't decode byte 0xb4 in position 46: invalid start byte"
6772,b'Unable to install Transformers Master version',2020-08-27T18:57:29Z,2020-08-27T20:47:42Z,,,
6771,"b""Exported TF Bert model is much slower than that exported from Google's Bert""",2020-08-27T18:07:07Z,2021-01-20T22:36:30Z,,,
6770,b'Roberta Large not working with SNLI dataset always gives random baseline that is 33 percent accuracy.',2020-08-27T17:56:56Z,2020-11-07T05:42:41Z,wontfix,,
6769,b'Seq2SeqTrainer',2020-08-27T17:32:09Z,2020-09-24T22:46:59Z,,,
6768,b'Floating-point operations logging in trainer',2020-08-27T17:06:00Z,2020-09-08T14:00:57Z,,,
6767,b'Add NLP install to self-scheduled CI',2020-08-27T15:07:26Z,2020-08-27T15:08:15Z,,,
6766,b'Distributed training doesn\xe2\x80\x99t work',2020-08-27T11:57:30Z,2020-11-07T05:42:40Z,wontfix,`RuntimeError,"`RuntimeError: CUDA out of memory. Tried to allocate 3.82 GiB (GPU 0; 11.17 GiB total capacity; 7.59 GiB already allocated; 594.31 MiB free; 10.28 GiB reserved in total by PyTorch) (malloc at /pytorch/c10/cuda/CUDACachingAllocator.cpp:289)`"
6765,b'Adds Adafactor to the docs and slightly fixes the formatting',2020-08-27T09:16:34Z,2020-08-27T09:16:51Z,,,
6764,b'Add ProtBert model card',2020-08-27T08:05:00Z,2020-08-28T04:12:29Z,model card,,
6763,b'Cuda out of memory when fp16 training',2020-08-27T07:04:33Z,2020-12-05T05:00:44Z,wontfix,,
6762,b'Checklist for Model Hub',2020-08-27T03:12:21Z,2021-03-06T00:17:28Z,wontfix,,
6761,b's2s distillation uses AutoModelForSeqToSeqLM',2020-08-27T03:11:53Z,2020-08-27T03:25:12Z,,,
6760,b'Question: Differentiable Search in generate',2020-08-27T01:54:33Z,2020-11-07T05:42:38Z,wontfix,,
6759,b'added model card for flexudys t5 model',2020-08-26T22:43:59Z,2020-09-01T21:38:56Z,model card,,
6758,b'Bart can make decoder_input_ids from labels',2020-08-26T22:14:00Z,2020-08-31T20:16:47Z,,,
6757,b'Seq2Seq tokenization and training fixes',2020-08-26T22:06:48Z,2020-08-27T02:31:02Z,,,
6756,b'Fix run_squad.py to work with BART',2020-08-26T22:03:32Z,2020-08-27T13:04:51Z,,,
6755,b'Model Card for Multilingual Passage Reranking BERT',2020-08-26T21:32:49Z,2020-08-26T22:00:27Z,model card,,
6754,b'add __init__.py to utils',2020-08-26T21:08:10Z,2020-08-26T21:51:11Z,,ModuleNotFoundError,"ModuleNotFoundError: No module named 'transformers.utils'"
6753,b'Removing memory/deleting a model: how to properly do this',2020-08-26T19:07:25Z,2020-09-01T20:41:46Z,,,
6752,b'Unable to install Transformers Master version',2020-08-26T19:06:46Z,2020-09-01T12:30:14Z,,,
6751,b'Add Language Agnostic Bert Sentence Embedding',2020-08-26T18:52:51Z,2021-01-10T13:54:27Z,"wontfix, New model",,
6750,b'RuntimeError: zero-dimensional tensor (at position 0) cannot be concatenated',2020-08-26T18:44:42Z,2021-03-06T00:17:30Z,wontfix,RuntimeError,"RuntimeError: zero-dimensional tensor (at position 0) cannot be concatenated"
6749,b'RuntimeError: grad can be implicitly created only for scalar outputs',2020-08-26T18:24:37Z,2020-10-21T17:06:40Z,,RuntimeError,"RuntimeError: grad can be implicitly created only for scalar outputs"
6748,"b'Pin code quality dependencies: black,isort,flake8'",2020-08-26T18:22:10Z,2020-08-29T22:06:47Z,,,
6747,b'Add checkpointing to Ray Tune HPO',2020-08-26T16:11:37Z,2020-08-31T18:38:46Z,,,
6746,b'[s2s] run_eval.py QOL improvements and cleanup',2020-08-26T16:07:19Z,2020-08-26T22:59:21Z,,,
6745,b'[s2s] run_eval saves samples/second',2020-08-26T15:01:41Z,2020-08-27T22:27:33Z,,,
6744,b'[pipelines] Text2TextGenerationPipeline',2020-08-26T12:14:44Z,2020-09-02T11:34:36Z,,,
6743,b'BART for Pre-Training',2020-08-26T12:08:25Z,,"Ex: LM (Pretraining), Good Second Issue",,
6742,"b'How to generate sentences in batches, instead of generating sentences one by one'",2020-08-26T11:27:46Z,2020-12-19T06:31:47Z,wontfix,,
6741,b'Fix tf boolean mask in graph mode',2020-08-26T08:49:07Z,2020-08-26T09:15:35Z,,,
6740,b'[Torchscript] Fix docs',2020-08-26T07:43:57Z,2020-08-26T08:51:57Z,,,
6739,b'convert_BertForQuestionAnswering_pytorch_checkpoint_to_tf',2020-08-26T07:30:10Z,2021-03-06T00:17:31Z,wontfix,,
6738,b'How to use the reformer for question answering?',2020-08-26T07:00:19Z,2020-09-02T07:53:10Z,,,
6737,b'[s2s README] Add more dataset download instructions',2020-08-26T01:32:17Z,2020-08-30T20:29:24Z,,,
6736,b'Fix run_squad.py to work with BART & add skip_decoder step for BART',2020-08-25T23:48:07Z,2020-08-26T21:48:16Z,,,
6735,b'[Generate] Facilitate PyTorch generate using `ModelOutputs`',2020-08-25T22:34:28Z,2020-09-01T10:38:26Z,,,
6734,"b'id2lang in tokenization_xlm.py should be int, and removing hardcoding'",2020-08-25T21:38:05Z,2020-09-10T17:31:02Z,,,
6733,b'grad checkpoint for T5 model -- and lots of debug not yet removed',2020-08-25T21:04:24Z,2021-03-06T00:17:33Z,wontfix,,
6732,b'Documentation detail in (TF)RobertaForSequenceClassification ',2020-08-25T20:20:04Z,2020-11-01T03:10:13Z,wontfix,,
6731,b'Model card for kuisailab/albert-xlarge-arabic',2020-08-25T18:48:30Z,2020-08-26T21:27:43Z,model card,,
6730,b'Model card for kuisailab/albert-large-arabic',2020-08-25T18:47:06Z,2020-08-26T21:27:57Z,model card,,
6729,b'Model card for kuisailab/albert-base-arabic',2020-08-25T18:45:10Z,2020-08-26T21:27:35Z,model card,,
6728,b'Install nlp for github actions test',2020-08-25T18:44:23Z,2020-08-25T18:58:38Z,,,
6727,b'added model card for codeswitch-spaeng-sentiment-analysis-lince',2020-08-25T18:11:37Z,2020-08-26T21:26:33Z,model card,,
6726,b'Fix pegasus-xsum integration test',2020-08-25T17:45:59Z,2020-08-25T18:06:29Z,,,
6725,b'Dataset Lazyloader for transformers trainer',2020-08-25T17:18:40Z,2020-09-07T07:57:01Z,,,
6724,b'create ProtBert-BFD model card.',2020-08-25T16:52:52Z,2020-08-27T00:19:20Z,model card,,
6723,b'add xlm-roberta-large-xnli model card',2020-08-25T16:13:28Z,2020-08-26T20:06:00Z,model card,,
6722,b'Add AdaFactor optimizer from fairseq',2020-08-25T15:55:27Z,2020-08-27T08:58:14Z,,,
6721,b'Add model card for singbert large',2020-08-25T15:51:11Z,2020-08-25T16:11:24Z,,,
6720,b'Resuming training from a checkpoint on Windows does not resume at the correct global_step',2020-08-25T15:46:38Z,2020-08-31T15:02:31Z,,,
6719,b'[Albert] Add position ids to allowed uninitialized weights',2020-08-25T15:16:09Z,2020-08-25T15:38:52Z,,,
6718,b'Create model card for lordtt13/COVID-SciBERT',2020-08-25T12:43:22Z,2020-08-26T21:22:26Z,model card,,
6717,b'Fix TF optimizer',2020-08-25T12:29:27Z,2020-08-26T15:12:45Z,,,
6716,b'Fix ONNX test_quantize unittest',2020-08-25T12:06:44Z,2020-08-25T17:24:41Z,Tests,,
6715,b'tensor.nonzero() is deprecated in PyTorch 1.6',2020-08-25T11:34:48Z,2020-08-25T12:12:55Z,,,
6714,b'RuntimeError: forward() Expected a value of type \xe2\x80\x98Tensor\xe2\x80\x99 for argument \xe2\x80\x98input_ids\xe2\x80\x99 but instead found type \xe2\x80\x98list\xe2\x80\x99 while loading a torchscript model following the documentation ',2020-08-25T11:10:35Z,2020-08-26T08:51:57Z,,RuntimeError,"RuntimeError: forward() Expected a value of type 'Tensor' for argument 'input_ids' but instead found type 'list'."
6713,b'Fix the TF Trainer gradient accumulation and the TF NER example',2020-08-25T11:02:17Z,2020-08-27T12:45:35Z,,,
6712,b'longformer padding logging',2020-08-25T09:23:12Z,2020-11-01T03:10:12Z,wontfix,,
6711,b'Pegasus finetuning: OOM',2020-08-25T08:19:07Z,2020-08-28T15:15:18Z,,,
6710,b'[squad] make examples and dataset accessible from SquadDataset object',2020-08-25T06:38:27Z,2020-08-25T17:32:57Z,,,
6709,b'consolidate tf activation functions',2020-08-25T04:42:41Z,2020-10-16T15:41:23Z,"Help wanted, cleanup",,
6708,b'[bart] rename self-attention -> attention',2020-08-25T04:29:49Z,2020-08-29T22:03:09Z,,,
6707,b'copy of #6654 for easier pulling',2020-08-25T03:44:39Z,2020-08-26T23:43:57Z,,,
6706,b'ci/gh/self-scheduled: add newline to make examples tests run even if src/ tests fail',2020-08-25T03:12:04Z,2020-08-25T10:26:30Z,Tests,,
6705,b' PegasusXSUMIntegrationTest.test_pegasus_xsum_summary',2020-08-25T03:08:30Z,2020-08-25T18:06:29Z,,,
6704,"b'[s2s] round bleu, rouge to 4 digits'",2020-08-25T03:04:57Z,2020-08-25T04:33:12Z,,,
6703,b'Adding model cards for 5 models',2020-08-25T01:33:33Z,2020-08-26T21:20:56Z,model card,,
6702,b'Questions on the date of Wikipedia dumps for pretrained checkpoints (BERT and RoBERTa).',2020-08-25T01:08:49Z,2020-11-01T03:10:20Z,wontfix,,
6701,b'NER GermEval preprocessor not working as documented',2020-08-24T22:53:31Z,2020-08-25T18:19:48Z,,AttributeError,"AttributeError: 'BertTokenizer' object has no attribute 'num_special_tokens_to_add'"
6700,"b""Some weights of AlbertModel were not initialized ['albert.embeddings.position_ids'] """,2020-08-24T22:32:15Z,2020-08-25T15:38:52Z,,,
6699,b'More tests to Trainer',2020-08-24T20:17:41Z,2020-08-25T11:07:37Z,,,
6698,b'[fixdoc] Add import to pegasus usage doc',2020-08-24T19:46:30Z,2020-08-24T19:54:57Z,,,
6697,b'words of overflowing_tokens in function truncate_sequences is not in right order',2020-08-24T19:31:12Z,2020-11-01T03:10:18Z,wontfix,,
6696,b'Use separate tqdm progressbars',2020-08-24T18:35:47Z,2020-08-25T11:06:58Z,,,
6695,b'Fix hyperparameter_search doc',2020-08-24T17:35:17Z,2020-08-25T01:04:08Z,,,
6694,b'Move unused args to kwargs',2020-08-24T17:13:47Z,2020-08-24T17:20:04Z,,,
6693,b'Longformer finetuning on TPUs IndexError: tuple index out of range',2020-08-24T17:12:15Z,2020-11-09T01:56:25Z,wontfix,IndexError,"IndexError: tuple index out of range"
6692,"b'Add ""tie_word_embeddings"" config param'",2020-08-24T16:28:56Z,2020-08-26T08:58:22Z,,,
6691,b'Lat fix for Ray HP search',2020-08-24T16:07:51Z,2020-08-24T16:15:01Z,,,
6690,b'Add DPR to models summary',2020-08-24T15:05:05Z,2020-08-25T07:57:29Z,,,
6689,b'Add tokenizer to Trainer',2020-08-24T14:48:40Z,2020-08-25T11:47:09Z,,,
6688,b'Question Answering demonstrator for contribute model stopped working',2020-08-24T14:43:09Z,2020-08-25T11:01:21Z,,,
6687,b'Typo fix in longformer documentation',2020-08-24T14:18:34Z,2020-08-25T13:39:03Z,,,
6686,b'Update repo to isort v5',2020-08-24T13:07:08Z,2020-08-24T15:03:02Z,,,
6685,b'Fixed DataCollatorForLanguageModeling not accepting lists of lists',2020-08-24T12:49:20Z,2020-08-24T13:31:45Z,,,
6684,b'missing reference `from model_bertabs import BertAbsSummarizer`',2020-08-24T12:28:45Z,2020-11-01T03:10:17Z,wontfix,,
6683,"b""Don't reset the dataset type + plug for rm unused columns""",2020-08-24T11:54:16Z,2020-08-24T13:22:03Z,,,
6682,b'Fix PL token classification examples',2020-08-24T11:47:43Z,2020-08-24T15:30:06Z,,,
6681,b'BUILD upgrade to isort v5',2020-08-24T10:55:48Z,2020-08-24T15:03:02Z,,,
6680,b'Tokenizers works different between NFD/NFKD and NFC/NFKC normalize functions in lowercase Turkish(and probably some other languages)',2020-08-24T10:45:45Z,2021-03-06T00:17:35Z,wontfix,,
6679,b'Add Mirror Option for Downloads',2020-08-24T10:21:58Z,2020-09-14T15:50:23Z,,,
6678,"b""Can't load config for New Community Model""",2020-08-24T09:42:29Z,2020-08-25T10:21:31Z,,,
6677,b'Batch encore plus and overflowing tokens fails when non existing overflowing tokens for a sequence',2020-08-24T08:56:08Z,2020-09-09T10:55:18Z,,,
6676,b'Allow numpy array as tokenizer input',2020-08-24T08:37:17Z,2021-03-06T00:17:36Z,wontfix,,
6675,b'ner example failed on examples/token-classification % bash run.sh',2020-08-24T08:00:35Z,2020-11-16T03:04:55Z,"wontfix, Ex: Named Entity Recognition",KeyError,"KeyError: '[null,""AIzaSyCF97XfLoejM9NhWDAZeOcjC6kOEsEmv6A"",""897606708560-a63d8ia0t9dhtpdt4i3djab2m42see7o.apps.googleusercontent.com"",null,null,""v2"",null,null,null,null,null,null,null,""https://content.googleapis.com"",""SITES_%s"",null,null,null,null,null,0,null,null,null,[""AHKXmL0ZzONWw2TXF2GVALSixIY_wY8DFDhrOeiPL5czjvgRVJRjibFVAqFSDdzkAGNCFzy2FNRZ"",1,""CJDXlfOos-sCFZWnIwAdnZoJHA"",1598254209133000,[5703022,5703839,5704621,5705837,5705841,5706601,5706832,5706836,5707711,5709888,5710567,5710768,5710806,5711078,5711206,5711530,5711563,5711808,5711866,5711929,5712328,5713049,5714628,14100031,14100834,14100854,14101054,14101218,14101254,14101334,14101346,14101350,14101354,14101358,14101374,14101378,14101386,14101410,14101418,14101430,14101442,14101446,14101458,14101462,14101474,14101492]'"
6674,b'Add model card for singbert.',2020-08-24T06:17:45Z,2020-08-25T02:09:14Z,,,
6673,b'New training arg: warmup_ratio',2020-08-24T01:21:23Z,2021-02-18T17:23:34Z,Feature request,,
6672,"b""TFTrainer with TPUs: Here's a suggestion on getting it to work""",2020-08-23T18:01:31Z,2020-11-15T17:49:59Z,wontfix,,
6671,b'Value Error & dev file parameter: run_squad.py BERT QA finetuning',2020-08-23T11:17:22Z,2020-11-01T03:10:24Z,wontfix,,
6670,b'Pretrained GPT2DoubleHeadsModel',2020-08-22T22:19:31Z,2020-11-01T03:10:47Z,wontfix,,
6669,b'Inconsistent handling of empty string in tokenizers',2020-08-22T21:57:41Z,2020-09-18T20:06:19Z,,Exception,"Exception: Truncation error: Specified max length is too low to respect the various constraints"
6668,b'Zero-Shot-Classification: multi_class or multi_label?',2020-08-22T21:12:16Z,2021-03-15T22:02:47Z,,,
6667,b'Why does the median cross entropy loss change when I change the random seed?',2020-08-22T19:14:27Z,2020-08-22T20:06:10Z,,,
6666,b'added multiple model_cards for below models',2020-08-22T17:05:10Z,2020-08-24T09:08:33Z,model card,,
6665,b'Finetune.sh showing killed',2020-08-22T15:05:24Z,2020-08-25T17:25:14Z,,,
6664,b'convert_BertForQuestionAnswering_pytorch_checkpoint_to_tf',2020-08-22T14:35:40Z,2020-08-25T14:20:26Z,,,
6663,b'added model_card for model codeswitch-hineng-lid-lince and codeswitch-spaeng-lid-lince',2020-08-22T12:58:59Z,2020-08-22T16:13:22Z,model card,,
6662,b'Integer division of tensors using div or / is no longer supported torch',2020-08-22T12:48:51Z,2020-11-01T03:11:00Z,wontfix,,
6661,b'Sequence packing',2020-08-22T07:12:59Z,2020-11-01T03:11:00Z,wontfix,,
6660,b'Create PULL_REQUEST_TEMPLATE.md',2020-08-22T05:08:55Z,2020-08-24T16:30:38Z,,,
6659,b'[doc] remove BartForConditionalGeneration.generate',2020-08-22T05:01:18Z,2020-08-24T16:42:35Z,,,
6658,b'wip: add from scratch arg to lightning_base',2020-08-22T04:55:22Z,2020-09-25T15:32:46Z,,,
6657,"b'Error while loading pretrained model with ""return_dict=True""'",2020-08-22T00:29:08Z,2020-08-24T10:19:28Z,,TypeError,"TypeError: __init__() got an unexpected keyword argument 'return_dict'"
6656,b'Add bibtex for new paper',2020-08-21T22:27:14Z,2020-08-23T14:03:41Z,,,
6655,b'Error when wandb is installed',2020-08-21T21:20:53Z,2020-08-26T15:31:29Z,,SyntaxError,"SyntaxError: Missing parentheses in call to 'print'. Did you mean print(c_count.value)?"
6654,b'prepare_seq2seq_batch makes labels/ decoder_input_ids made later.',2020-08-21T21:16:23Z,2020-08-28T15:15:18Z,,now,"now: `""Fostul ef al personalului prezidenial al Braziliei va fi judecat"".`"
6653,"b""old nlp causes error that pip install -e. can't fix.""",2020-08-21T20:52:14Z,2020-11-01T16:29:48Z,wontfix,,
6652,"b""['encoder.version', 'decoder.version'] are unexpected when loading a pretrained BART model""",2020-08-21T19:41:07Z,2020-09-10T15:35:01Z,,(note,"(note: I re-edited this issue once I understood it better to save reader's time, the history is there if someone needs it)"
6651,"b""[doc] bart doc examples aren't for bart""",2020-08-21T19:36:31Z,2020-08-24T16:42:35Z,,,
6650,b'Add model card for electricidad-base-generator',2020-08-21T17:05:23Z,2020-08-21T18:18:16Z,,,
6649,b'[Doc model summary] add MBart model summary',2020-08-21T16:43:52Z,2020-08-21T17:43:00Z,,,
6648,b'Remove hard-coded uses of float32 to fix mixed precision use',2020-08-21T16:37:22Z,2020-08-25T07:42:33Z,,,
6647,b'mbart broken in summarization pipeline',2020-08-21T16:18:50Z,2020-08-21T16:58:33Z,,,
6646,b'Error when loading my trained model',2020-08-21T16:03:02Z,2020-08-21T17:45:10Z,,OSError,"OSError: Unable to open file (file signature not found)`"
6645,b'New config param for cross-attention dimensionality',2020-08-21T15:30:37Z,2021-01-10T13:54:05Z,wontfix,,
6644,b'Dataset and DataCollator for BERT Next Sentence Prediction (NSP) task',2020-08-21T14:46:34Z,2020-08-31T12:25:00Z,,,
6643,b'bert finetuning for multilingual question answering',2020-08-21T14:19:09Z,2020-11-01T16:29:47Z,wontfix,,
6642,b'fix order of input/target of cross_entropy',2020-08-21T14:13:32Z,2020-10-28T22:38:01Z,wontfix,,
6641,b'Dataset and DataCollator for BERT Next Sentence Prediction (NSP) task',2020-08-21T13:57:38Z,2020-08-21T14:00:21Z,,,
6640,b'[Docs model summaries] Add pegasus to docs',2020-08-21T13:31:20Z,2020-08-21T14:22:11Z,,,
6639,"b'Run_glue.py, how can I continue previous fine-tuning training?'",2020-08-21T12:49:14Z,2020-11-01T16:29:49Z,wontfix,,
6638,b'Running squad_convert_examples_to_features causes warnings.',2020-08-21T11:09:19Z,2020-11-01T16:29:42Z,wontfix,,
6637,b'Add typing.overload for convert_ids_tokens',2020-08-21T09:43:37Z,2020-08-25T08:57:08Z,,,
6636,b'Pre-training a language model on a large dataset',2020-08-21T08:56:56Z,2020-12-19T06:32:01Z,wontfix,Exception,"Exception: process 0 terminated with signal SIGKILL"
6635,b'How to convert tokenizer output to train_dataset which is required by Trainer API ',2020-08-21T08:43:30Z,2020-08-24T11:37:30Z,,,
6634,b'Fix error class instantiation',2020-08-21T08:41:05Z,2020-09-02T11:36:33Z,,,
6633,b'model card for Spanish electra base',2020-08-21T07:19:46Z,2020-08-21T09:04:30Z,,,
6632,"b'Error on `PreTrainedTokenizerBase.batch_encode_plus` with `return_overflowing_tokens=True, truncation=True`'",2020-08-21T06:01:20Z,2020-09-09T10:55:18Z,,AssertionError,"AssertionError: Some items in the output dictionnary have a different batch size than others."
6631,b'fine tuning with Chinese data LCQMC val_acc not increase',2020-08-21T02:07:12Z,2020-11-01T16:29:42Z,wontfix,,
6630,"b""Tokenize got an unexpected keyword argument 'pad_to_max_length', 'return_attention_mask'""",2020-08-21T01:32:32Z,2020-11-01T16:29:45Z,"wontfix, Core: Tokenization",TypeError,"TypeError: _tokenize() got an unexpected keyword argument 'pad_to_max_length'"
6629,b'Remove accidental comment',2020-08-21T01:19:46Z,2020-08-21T09:07:33Z,,,
6628,"b""PreTrainedModel's tie_weights invocation needs to be configurable""",2020-08-21T00:59:57Z,2020-08-27T16:37:09Z,,,
6627,b'BartTokenizerFast cannot decode PyTorch tensors',2020-08-21T00:37:04Z,2020-09-24T08:20:12Z,Fast Tokenizers,,
6626,b'Specify config filename in HfArgumentParser',2020-08-20T23:47:41Z,2020-08-24T11:27:58Z,,,
6625,b'**Specifically for Pegasus-arxiv** -  PegasusForConditionalGeneration - Error in loading state dictionary',2020-08-20T22:11:05Z,2020-08-23T18:18:58Z,,RuntimeError,"RuntimeError: Error(s) in loading state_dict for PegasusForConditionalGeneration:"
6624,b'Bart: make decoder_input_ids correctly if labels specified.',2020-08-20T18:52:22Z,2020-08-28T15:15:18Z,seq2seq,,
6623,b'Fix confusing warnings during TF2 import from PyTorch',2020-08-20T18:43:51Z,2020-09-10T09:32:00Z,,,
6622,b'Move threshold up for flaky test with Electra',2020-08-20T17:37:58Z,2020-08-20T17:59:41Z,,,
6621,b'[Tests] fix attention masks in Tests',2020-08-20T16:39:54Z,2020-08-20T17:23:48Z,,,
6620,b'Pegasus: OSError: Unable to load weights from pytorch checkpoint file.',2020-08-20T16:23:36Z,2020-08-20T19:29:40Z,,"RuntimeError, OSError","RuntimeError: unexpected EOF, expected 10498989 more bytes. The file might be corrupted.OSError: Unable to load weights from pytorch checkpoint file. If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True. "
6619,b'[DistilBert] Flaky tests',2020-08-20T16:07:40Z,2020-08-20T17:23:48Z,,,
6618,b'TFTrainer dataset doc & fix evaluation bug',2020-08-20T15:15:36Z,2020-08-20T16:11:37Z,,,
6617,b'unk handling in v3.0 different than v2.0?',2020-08-20T14:27:33Z,2020-11-01T16:29:46Z,"wontfix, Core: Tokenization",TypeError,"TypeError: can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, int64, int32, int16, int8, uint8, and bool."
6616,"b""Fine tune masked language model on custom dataset 'index out of range in self'""",2020-08-20T13:57:37Z,2021-03-06T00:17:38Z,wontfix,,
6615,"b""I can't reproduce the results of tf-xlm-r-ner-40-lang model """,2020-08-20T12:37:37Z,2020-11-07T05:43:01Z,wontfix,,
6614,b'removed redundant arg in prepare_inputs',2020-08-20T09:08:51Z,2020-08-20T12:23:36Z,,,
6613,b'FillMaskPipeline return special tokens i.e. <mask> as prediction',2020-08-20T07:24:12Z,2020-11-01T16:29:35Z,wontfix,,
6612,b'wip/mbart: make batches that are identical to fairseq',2020-08-20T02:41:47Z,2020-08-22T04:55:53Z,,,
6611,b'TextGenerationPipeline giving FutureWarning about AutoModelWithLMHead',2020-08-20T00:37:02Z,2020-10-26T01:55:04Z,wontfix,,
6610,b'[seq2seq Example] Convert tensor to List[int] for decoding',2020-08-19T23:58:39Z,2020-08-20T18:40:30Z,,,
6609,b'PegasusForConditionalGeneration - Error in loading state dictionary',2020-08-19T23:31:38Z,2020-08-20T19:34:43Z,,RuntimeError,"RuntimeError: Error(s) in loading state_dict for PegasusForConditionalGeneration:"
6608,b'How to use Huggingface model for continuous values directly?',2020-08-19T20:58:12Z,2020-11-09T01:56:17Z,wontfix,,
6607,b'[Longformer] try if multi gpu works',2020-08-19T20:24:59Z,2020-10-01T16:16:43Z,,,
6606,b'Regression test for pegasus bugfix',2020-08-19T20:10:33Z,2020-08-20T19:34:43Z,,,
6605,b'Add tests to Trainer',2020-08-19T20:00:38Z,2020-08-20T15:13:50Z,,,
6604,b'Fix confusing warnings during TF2 import from PyTorch',2020-08-19T19:52:46Z,2020-08-20T18:32:55Z,,,
6603,b'[cleanup] remove confusing newline',2020-08-19T19:08:53Z,2020-08-20T04:33:36Z,,,
6602,b'Create README.md',2020-08-19T17:56:24Z,2020-09-01T21:58:44Z,model card,,
6601,b'Fix GPT2DoubleHeadsModel to work with model.generate()',2020-08-19T17:27:25Z,2020-11-16T13:35:45Z,model card,,
6600,b'[wip] Seq2SeqTrainer',2020-08-19T16:25:36Z,2020-08-27T17:32:57Z,model card,,
6599,b'Pegasus: IndexError: index out of range in self',2020-08-19T15:40:55Z,2020-08-20T19:33:55Z,,IndexError,"IndexError: index out of range in self"
6598,b'Create README.md',2020-08-19T15:32:26Z,2020-09-01T21:59:16Z,model card,,
6597,b' tf2 transformers cache dir',2020-08-19T14:32:12Z,2020-08-21T14:36:38Z,,,
6596,b'Fix #6575',2020-08-19T13:26:15Z,2020-08-19T17:04:33Z,,,
6595,b'Typo fix in 04-onnx-export',2020-08-19T12:42:24Z,2020-08-20T08:17:17Z,,,
6594,"b'Add ""Leveraging Pretrained Checkpoints for Generation"" Seq2Seq models.'",2020-08-19T12:36:56Z,2020-09-10T14:40:51Z,,,
6593,b'[Tests common] Fix flaky test ',2020-08-19T12:25:13Z,2020-08-19T14:18:52Z,,,
6592,b'Unable to save and load RoBERTa model using tensorflow',2020-08-19T10:59:10Z,2020-12-11T08:20:37Z,wontfix,ValueError,"ValueError: Error when exporting object <tensorflow.python.keras.layers.core.Activation object at 0x7fef7d287128> of with identifier=_tf_keras_layer. The object has an attribute named regularization_losses, which is reserved. List of all reserved attributes: dict_keys(['regularization_losses', 'variables', 'trainable_variables', 'keras_api'])"
6591,b'tf generation utils: remove unused kwargs',2020-08-19T05:12:41Z,2020-08-19T13:37:46Z,cleanup,,
6590,b'Delete Unused TFModelTesterMixin attributes',2020-08-19T05:06:58Z,2020-09-02T14:54:40Z,"Help wanted, cleanup",,
6589,b'[seq2seq] finetune.sh  OOMs in fp16 w torch 1.6 on colab',2020-08-19T04:07:45Z,2020-09-10T18:11:34Z,Help wanted,,
6588,b'all_hidden_states indentation bug in modeling_bert.py',2020-08-19T02:59:54Z,2020-08-19T03:07:35Z,,,
6587,b'Fix bart base test',2020-08-19T00:38:21Z,2020-08-19T01:28:11Z,,,
6586,b'wip: Code to add lang tags to marian model cards',2020-08-19T00:34:07Z,2020-09-23T22:11:06Z,,,
6585,b'Failing bart-base slow test',2020-08-18T23:22:42Z,2020-08-19T01:28:10Z,,,
6584,b'Failing ONNX Slow Test',2020-08-18T23:21:59Z,2020-08-27T21:14:04Z,,,
6583,b'add intro to nlp lib & dataset links to custom datasets tutorial',2020-08-18T21:03:19Z,2020-08-20T14:32:52Z,,,
6582,b'BatchEncoding interacts poorly with apex.amp',2020-08-18T20:08:21Z,2020-12-01T18:01:53Z,,,
6581,b'Error using pipeline with officiel docker image (transformers 3.0.2)',2020-08-18T19:17:51Z,2020-10-25T00:40:43Z,wontfix,KeyError,"KeyError: 0"
6580,b'[examples/text-classification] update xnli-mt url',2020-08-18T16:41:14Z,2020-08-18T17:10:48Z,,,
6579,b'[Pegasus Doc] minor typo',2020-08-18T16:18:35Z,2020-08-18T16:47:48Z,,,
6578,b'Latest source has bug in pipelines for short inputs (regardless of padding)',2020-08-18T16:09:55Z,2021-01-10T13:54:30Z,wontfix,ValueError,"ValueError: expected sequence of length 512 at dim 1 (got 481)"
6577,b'CamembertForCausalLM',2020-08-18T15:46:48Z,2020-08-21T11:52:54Z,,,
6576,b'Add hyperparameter search to Trainer',2020-08-18T15:30:32Z,2020-08-24T15:48:46Z,,,
6575,b'Tokenizer further tokenizes pretokenized input',2020-08-18T14:23:54Z,2020-08-19T17:04:37Z,,,
6574,"b""[docs] Fix number of 'ug' occurrences in tokenizer_summary""",2020-08-18T13:57:36Z,2020-08-18T14:23:26Z,,,
6573,b'[docs] Fix wrong newline in the middle of a paragraph',2020-08-18T13:55:09Z,2020-08-18T14:22:44Z,,,
6572,b'Dataset and DataCollator for BERT Next Sentence Prediction (NSP) task.',2020-08-18T13:34:03Z,2020-08-21T13:53:00Z,,,
6571,b'token-classification: update url of GermEval 2014 dataset',2020-08-18T13:09:58Z,2020-09-18T10:18:06Z,,,
6570,b'Can not use the convert_graph_to_onnx.py to convert the pytorch model to onnx model',2020-08-18T12:47:18Z,2020-11-24T14:07:11Z,,,
6569,b'[Model card] Bert2GPT2 EncoderDecoder model',2020-08-18T12:03:26Z,2020-08-18T17:28:18Z,model card,,
6568,b'KeyError with DPR reader in Question Answering pipeline',2020-08-18T11:28:43Z,2020-11-29T11:50:17Z,wontfix,KeyError,"KeyError: 'dpr'"
6567,b'XLNet Bug when training with apex 16-bit precision',2020-08-18T11:13:02Z,2020-08-20T17:34:24Z,,,
6566,b'Can not convert the pytorch pretrained bert model to onnx model ',2020-08-18T10:16:11Z,2020-10-25T11:29:46Z,wontfix,`IndexError,"`IndexError: index out of range in self`"
6565,b'New Feature: Best-First Beam Search',2020-08-18T09:29:30Z,2020-10-25T00:40:41Z,wontfix,,
6564,b'T5 Gradient Checkpointing',2020-08-18T09:10:48Z,2021-04-30T08:43:55Z,Good Second Issue,,
6563,b'Pretokenized text handling mistake in  tokenization_utils.py file',2020-08-18T07:55:36Z,2020-10-25T00:40:39Z,wontfix,,
6562,b'[Urgent] Word embedding initialization documentation and code might mismatch',2020-08-18T07:23:48Z,2020-12-20T13:34:42Z,wontfix,,
6561,b'Create my own language model',2020-08-18T07:23:08Z,2020-10-25T00:40:40Z,wontfix,,
6560,b'Huggingface create_optimizer method not working',2020-08-18T07:14:52Z,2020-08-26T15:12:45Z,,,
6559,b'Finetuning GPT2 produces IndexError: index out of range in self error',2020-08-18T06:38:47Z,2020-11-01T16:29:52Z,wontfix,"IndexError, max_grad_norm, no_cuda, layer_norm_epsilon","IndexError: index out of range in selfmax_grad_norm: 1.0no_cuda: Falselayer_norm_epsilon: 1e-05"
6558,b'Can we resize embedding with embedding weighted initialized differently??',2020-08-18T06:33:08Z,2020-12-20T13:34:43Z,wontfix,,
6557,b'Create README.md',2020-08-18T06:18:53Z,2020-08-18T16:42:14Z,model card,,
6556,b'Create README.md',2020-08-18T06:10:18Z,2020-08-18T16:43:21Z,model card,,
6555,b'Small typo fixes for model card: electra-base-german-uncased',2020-08-18T05:45:49Z,2020-08-18T12:21:53Z,model card,,
6554,"b""The squad processor's multi-threading crashes the script / causes large models to reload every call""",2020-08-18T04:17:03Z,2020-10-25T00:40:37Z,wontfix,BrokenPipeError,"BrokenPipeError: [Errno 32] Broken pipe"
6553,b'fix incorrect codecov reports',2020-08-18T03:23:34Z,2020-08-18T14:21:14Z,,,
6552,b'Add model card for facebook/mbart-large-en-ro',2020-08-18T01:33:34Z,2020-08-20T16:46:46Z,,,
6551,b'TFTrainer Example',2020-08-18T00:53:39Z,2020-11-01T16:29:34Z,wontfix,,
6550,b'504 Gateway Time-out when trying to access Uploaded Model page ',2020-08-17T23:27:22Z,2020-08-21T11:36:50Z,,,
6549,b'fixed fast tokenizer use in QA pipeline and added corresponding test',2020-08-17T23:06:29Z,2020-08-18T21:04:20Z,,,
6548,b'Model Clipping reduce the size of the model.',2020-08-17T21:38:28Z,2020-10-25T00:40:36Z,wontfix,,
6547,b'There is an error in the run_tf_ner.py script with the get_labels fun\xe2\x80\xa6',2020-08-17T21:27:03Z,2020-09-10T13:14:51Z,,,
6546,b'[model_cards] update jimregan/BERTreach card with #s of sentences/tokens',2020-08-17T20:41:39Z,2020-08-17T20:48:05Z,model card,,
6545,b'QA pipeline fails when using fast tokenizer',2020-08-17T20:14:57Z,2020-11-07T05:42:36Z,wontfix,,
6544,b'[model_cards] Add a new model for Irish',2020-08-17T19:02:20Z,2020-08-17T19:56:57Z,model card,,
6543,b'[BartTokenizerFast] add prepare_seq2seq_batch',2020-08-17T16:46:33Z,2020-08-19T14:37:49Z,,,
6542,b'WNUT17 TF example stuck at first epoch ',2020-08-17T16:44:00Z,2020-11-01T03:10:11Z,"wontfix, TensorFlow, Examples",,
6541,b'replace _ with __ rst links',2020-08-17T16:09:29Z,2020-08-17T16:27:03Z,,,
6540,b'bart-base config.*attention_heads (should be 12 was 16)',2020-08-17T16:05:40Z,2020-08-17T16:23:14Z,,,
6539,"b""Widget can't load model""",2020-08-17T15:49:17Z,2020-10-21T06:14:43Z,wontfix,,
6538,b'[EncoderDecoder] Add functionality to tie encoder decoder weights',2020-08-17T13:45:09Z,2020-08-19T12:23:46Z,,,
6537,"b'tokenizers/tokenizers.cpython-36m-darwin.so, 2): Symbol not found: ____chkstk_darwin '",2020-08-17T12:25:31Z,2020-10-25T00:40:32Z,wontfix,ImportError,"ImportError: dlopen(/Users/user1/anaconda3/lib/python3.6/site-packages/tokenizers/tokenizers.cpython-36m-darwin.so, 2): Symbol not found: ____chkstk_darwin"
6536,b'[model_cards] Add model cards for Urduhack model (roberta-urdu-small)',2020-08-17T11:39:14Z,2020-08-17T20:04:30Z,model card,,
6535,b'Passing inputs_embeds into GenerationMixin.generate()',2020-08-17T10:57:45Z,2020-10-25T11:29:44Z,wontfix,,
6534,b'How to fine-tune GPT2 on Arithmetic Problem',2020-08-17T10:30:27Z,2020-08-18T21:12:45Z,,,
6533,b'[Docs Pegasus] Correct Pegasus Link in doc',2020-08-17T10:24:13Z,2020-08-17T10:24:43Z,,,
6532,b'Remove deprecated assertEquals',2020-08-17T08:41:46Z,2020-08-17T09:13:59Z,,,
6531,b'Fix flaky ONNX tests',2020-08-17T07:31:54Z,2020-08-17T13:04:36Z,Tests,,
6530,b'Added first model card',2020-08-17T07:19:04Z,2020-08-17T20:24:11Z,model card,,
6529,b'skip onnx test until morgan comes back',2020-08-17T00:49:17Z,2020-08-17T13:03:33Z,,,
6528,b'attempt to fix onnx test',2020-08-17T00:41:14Z,2020-08-17T01:41:33Z,,,
6527,b'Update bert-base-portuguese-cased and bert-large-portuguese-cased cards',2020-08-16T22:32:00Z,2020-08-17T02:49:50Z,model card,,
6526,b'add BartConfig.force_bos_token_to_be_generated',2020-08-16T20:24:49Z,2020-08-18T23:15:50Z,,"noforce, **noforce","noforce: 87:40 {'rouge1': 44.26, 'rouge2': 21.22, 'rougeL': 30.72}**noforce: 34:12, {'rouge1': 45.45, 'rouge2': 22.38, 'rougeL': 37.25}**"
6525,"b'[s2s] docs, document desired filenames nicely'",2020-08-16T20:24:36Z,2020-08-17T00:31:23Z,,,
6524,b'[wip] experiment with mbart special tokens',2020-08-16T20:08:48Z,2020-08-25T18:56:59Z,,,
6523,b'[testing] replace hardcoded paths to allow running tests from anywhere',2020-08-16T18:18:22Z,2020-08-27T16:22:18Z,,,
6522,b'Create model cards for indonesian models',2020-08-16T17:54:26Z,2020-08-17T07:42:26Z,model card,,
6521,"b'allow spaces in bash args with ""$@""'",2020-08-16T17:35:45Z,2020-08-17T13:06:36Z,,,
6520,"b""Can't load pegasus models.""",2020-08-16T15:27:50Z,2020-10-22T13:33:35Z,,,
6519,b'[WIP]  Create SequenceClassification MultipleChoice and TokenClassification for tf_longformer',2020-08-16T14:28:13Z,2020-10-27T12:00:13Z,,,
6518,"b""[docs] Copy code button misses '...' prefixed code""",2020-08-16T13:41:33Z,2020-08-20T09:35:06Z,,,
6517,"b""Can't load t5-11b from pre-trained""",2020-08-16T12:14:23Z,2020-08-17T18:45:53Z,,OSError,"OSError: Can't load weights for 't5-11b'. Make sure that:"
6516,b'How to enable sampling when using unigram tokenizers?',2020-08-16T11:58:53Z,2020-10-25T00:40:31Z,wontfix,,
6515,b'Support additional dictionaries for BERT Japanese tokenizers',2020-08-16T08:18:58Z,2020-08-17T04:00:24Z,,,
6514,"b'Unexpected output(prediction) for TokenClassification, using pipeline'",2020-08-16T07:23:14Z,2020-10-25T00:40:30Z,wontfix,,
6513,b'Longformer pretrained weights are not really pretrained?',2020-08-16T07:04:17Z,2020-08-19T04:08:26Z,,,
6512,"b""[doc] lighter 'make test'""",2020-08-16T05:14:56Z,2020-08-20T09:24:26Z,,,
6511,b'[doc] Summary of the models fixes',2020-08-16T04:51:30Z,2020-08-17T08:04:54Z,,,
6510,b'new Makefile target: docs',2020-08-16T02:51:35Z,2020-08-27T16:25:17Z,,,
6509,"b'[doc] multiple corrections to ""Summary of the tasks""'",2020-08-16T02:33:36Z,2020-08-17T15:49:16Z,,,
6508,"b'[doc] make the text more readable, fix some typos, add some disambiguation'",2020-08-15T22:26:58Z,2020-08-17T15:07:58Z,,,
6507,"b""trainer.train() fails on 'fmikaelian/flaubert-base-uncased-squad' fine-tuning SQuAD""",2020-08-15T20:38:46Z,2020-10-25T00:40:33Z,wontfix,,
6506,"b""Loading 'fmikaelian/flaubert-base-uncased-squad' throws unexpected, difficult to comprehend warning""",2020-08-15T20:30:56Z,2020-10-23T08:08:51Z,wontfix,,
6505,b'[doc] typos',2020-08-15T19:42:29Z,2020-08-17T02:57:36Z,,,
6504,b'[doc] fix invalid env vars',2020-08-15T19:35:04Z,2020-08-17T03:11:40Z,,,
6503,b'tf BERT model produced by convert_graph_to_onnx has unclear or wrong input shapes',2020-08-15T17:29:34Z,2020-11-01T16:29:51Z,wontfix,,
6502,b'Truncated last sentence after bart finetuning on custom dataset.',2020-08-15T16:18:02Z,2020-12-13T01:23:03Z,wontfix,,
6501,b'Longformer slow than Bert',2020-08-15T14:59:35Z,2020-08-18T21:11:57Z,,,
6500,b'Always got RuntimeError while converting ALBERT model to TorchScript (.pt file)',2020-08-15T12:17:53Z,2020-08-17T07:55:38Z,,`RuntimeError,"`RuntimeError: The size of tensor a (15) must match the size of tensor b (14) at non-singleton dimension 3`"
6499,b'Add examples/bert-loses-patience who can help',2020-08-15T11:54:38Z,2020-08-16T08:30:17Z,,,
6498,b'Could not output hidden states using TFBertModel',2020-08-15T09:19:01Z,2020-11-01T03:10:10Z,wontfix,,
6497,b'BERT and SpanBERT for Coreference Resolution',2020-08-15T06:33:23Z,,New model,,
6496,b'Add Model Card for electra-base-german-uncased',2020-08-15T05:26:16Z,2020-08-17T03:02:33Z,model card,,
6495,b'Model Upload does not show up `german-nlp-group/electra-base-german-uncased`',2020-08-15T05:11:55Z,2020-08-17T19:49:11Z,,,
6494,b'[testing] a new TestCasePlus subclass + get_auto_remove_tmp_dir() ',2020-08-15T04:47:51Z,2020-08-17T12:12:19Z,,,
6493,b'Fixes paths with spaces in seq2seq example',2020-08-14T19:14:13Z,2020-08-16T17:36:39Z,,,
6492,b'Fixed label datatype for STS-B',2020-08-14T18:15:16Z,2020-08-18T12:09:40Z,,,
6491,b'Whole Word Masking Implementation',2020-08-14T17:54:35Z,2020-10-23T08:08:46Z,wontfix,,
6490,b'[Doc] add more MBart and other doc',2020-08-14T16:51:15Z,2020-08-17T16:30:27Z,,,
6489,b'GitHub Template: Tag @stefan-it for token classification related bug reports',2020-08-14T16:48:31Z,2020-08-18T12:38:54Z,,,
6488,b'Fix TPU Convergence bug introduced by PR#6151',2020-08-14T16:27:04Z,2020-08-14T16:47:38Z,,,
6487,b'about encoder and decoder input when using seq2seq model',2020-08-14T16:01:47Z,2020-08-18T13:17:29Z,,,
6486,b'from_pretrained() never works',2020-08-14T13:23:55Z,2020-08-14T18:15:24Z,,OSError,"OSError: Can't load weights for 'xlnet-base-cased'. Make sure that:"
6485,b'Add tests/test_tokenization_reformer.py',2020-08-14T12:35:50Z,2020-08-20T16:58:44Z,,,
6484,b'Assertion error when training a new RoBERTa from scratch',2020-08-14T12:35:10Z,2020-10-23T08:08:45Z,wontfix,RuntimeError,"RuntimeError: CUDA error: device-side assert triggered"
6483,b'Regarding GPU use for LM',2020-08-14T11:26:31Z,2020-08-14T14:01:18Z,,,
6482,b'Longformer Memory Consumption query',2020-08-14T10:46:24Z,2020-10-25T00:40:38Z,wontfix,,
6481,"b""what's the difference between TFBertOutput and TFBertSelfOutput?""",2020-08-14T10:43:23Z,2020-08-18T21:10:16Z,,,
6480,b'Import accuracy_score',2020-08-14T10:42:08Z,2020-08-14T12:16:16Z,,,
6479,b'[TFTrainer] gradient accumulation error ',2020-08-14T10:22:49Z,2020-08-27T13:27:49Z,,,
6478,b'Upladed model is not indexed',2020-08-14T08:52:26Z,2020-08-17T19:51:45Z,,,
6477,b'finetune.py: error: unrecognized arguments',2020-08-14T05:30:39Z,2020-08-16T17:36:39Z,,,
6476,b'Question about loss computing in BartForConditionalGeneration',2020-08-14T03:34:48Z,2020-10-23T08:08:44Z,wontfix,,
6475,b'Use hash to clean the test dirs',2020-08-14T02:49:29Z,2020-08-14T07:34:40Z,,,
6474,b'Training Data of  xlm-roberta-large-finetuned-conll03-* models',2020-08-14T02:36:51Z,2020-08-16T02:53:45Z,,,
6473,b'[sched] polynomial_decay_schedule use default power=1.0',2020-08-14T02:13:31Z,2020-08-17T12:33:13Z,,,
6472,"b'""BertEncoder\' object has no attribute \'output_hidden_states""'",2020-08-14T01:50:38Z,2020-08-20T17:43:41Z,,,
6471,b'[testing] automatically clean up temp dirs during teardown',2020-08-14T00:13:28Z,2020-08-27T10:30:52Z,,,
6470,b'Generation doc',2020-08-13T20:23:50Z,2020-08-14T13:46:40Z,,,
6469,b'Fix typo',2020-08-13T18:53:50Z,2020-08-13T19:01:09Z,model card,,
6468,b'convert_graph_to_onnx not working as expected.',2020-08-13T18:25:00Z,2020-08-15T17:52:52Z,,,
6467,"b""Error: 'GPT2Model' object has no attribute '_step' when converting tf-based checkpoint into pytorch""",2020-08-13T17:29:48Z,2020-11-07T05:42:46Z,wontfix,ModuleAttributeError,"ModuleAttributeError: 'GPT2Model' object has no attribute '_step'"
6466,b'add custom datasets tutorial',2020-08-13T17:21:58Z,2020-08-17T13:15:35Z,,,
6465,b'Longformer convert error',2020-08-13T15:53:23Z,2021-03-06T00:17:40Z,wontfix,,
6464,b'[BartTokenizerFast] add BartTokenizerFast in AutoTokenizer',2020-08-13T15:14:16Z,2020-08-13T16:08:11Z,,,
6463,b'[LongformerTokenizerFast] add LongformerTokenizerFast in AutoTokenizer',2020-08-13T15:08:43Z,2020-08-13T16:06:44Z,,,
6462,b'minor typo fix in modeling_utils',2020-08-13T12:44:43Z,2020-08-13T13:36:50Z,,,
6461,b'Sort unique_no_split_tokens to make it deterministic',2020-08-13T12:40:57Z,2020-08-14T08:36:59Z,,,
6460,b'Hashing a tokenizer using the \xf0\x9f\xa4\x97 nlp lib is not deterministic',2020-08-13T12:39:35Z,2020-08-14T08:36:58Z,,,
6459,b'Autotokenizer not returning instance of LongformerTokenizerFast',2020-08-13T10:46:03Z,2020-08-13T16:06:44Z,,NotImplementedError,"NotImplementedError: return_offsets_mapping is not available when using Python tokenizers.To use this feature, change your tokenizer to one deriving from transformers.PreTrainedTokenizerFast.`"
6458,b'Unknown task zero-shot-classification',2020-08-13T10:08:55Z,2020-08-13T10:39:40Z,,KeyError,"KeyError: ""Unknown task zero-shot-classification, available tasks are ['feature-extraction', 'sentiment-analysis', 'ner', 'question-answering', 'fill-mask', 'summarization', 'translation_en_to_fr', 'translation_en_to_de', 'translation_en_to_ro', 'text-generation']"""
6457,b'Add POS tagging and Phrase chunking token classification examples',2020-08-13T09:17:44Z,2020-08-13T16:09:51Z,,,
6456,b'Open-Retrieval Question Answering (ORQA)',2020-08-13T09:03:28Z,2020-10-21T03:28:20Z,"wontfix, New model",,
6455,b'MASS : A generalization of BERT and GPT',2020-08-13T07:51:18Z,2020-11-24T02:58:55Z,"wontfix, New model",,
6454,b'Memory Issue while following LM tutorial',2020-08-13T07:37:21Z,2020-08-13T21:23:39Z,,,
6453,b'Clean directory after script testing',2020-08-13T07:05:24Z,2020-08-13T16:34:04Z,,,
6452,"b'getting error while training bert language model. ""ValueError: Expected input batch_size (8) to match target batch_size (1024).""'",2020-08-13T05:09:29Z,2020-12-05T05:00:51Z,wontfix,**ValueError,"**ValueError: Expected input batch_size (8) to match target batch_size (1024).**"
6451,b'ERROR: No matching distribution found for tokenizers==0.8.1.rc1 (from transformers)',2020-08-13T02:59:03Z,2020-10-21T03:28:22Z,wontfix,,
6450,b'Error in PyTorch Trainer when used with TPU',2020-08-13T00:46:35Z,2020-08-21T12:35:05Z,,TypeError,"TypeError: 'NoneType' object cannot be interpreted as an integer"
6449,b'Trainer automatically drops unused columns in nlp datasets',2020-08-12T19:42:58Z,2020-08-20T20:29:15Z,,,
6448,b'[DO NOT SUBMIT] Run TPU examples for PR commits.',2020-08-12T17:47:29Z,2020-08-12T18:06:34Z,,,
6447,b'[TF Longformer] Improve Speed for TF Longformer',2020-08-12T16:41:48Z,2020-08-26T18:55:42Z,,,
6446,b'Get GKE logs via kubectl logs instead of gcloud logging read.',2020-08-12T15:44:37Z,2020-08-12T15:46:25Z,,,
6445,b'Test model outputs equivalence',2020-08-12T15:43:46Z,2020-08-13T15:59:36Z,,,
6444,"b""Can't download  'Helsinki-NLP/opus-mt-hye-eng' model""",2020-08-12T15:40:22Z,2020-08-13T09:44:08Z,marian,OSError,"OSError: "
6443,b'Simple train from the start for translation transformer',2020-08-12T15:28:31Z,2020-08-13T14:13:25Z,,,
6442,b'Adding PaddingDataCollator',2020-08-12T15:22:31Z,2020-08-12T15:32:28Z,,,
6441,b'MBartForConditionalGeneration',2020-08-12T14:49:14Z,2020-08-14T07:21:17Z,,,
6440,b'Getting Error from Default Data Collator while training Bert on SQUAD 2.0',2020-08-12T14:31:00Z,2020-11-29T11:50:05Z,wontfix,TypeError,"TypeError: an integer is required (got type dict)"
6439,b'TrainingArguments are ignored?!',2020-08-12T13:34:48Z,2020-08-12T18:28:12Z,,,
6438,b'Training GPT2 and Reformer from scratch.  ',2020-08-12T12:59:15Z,2020-08-13T14:50:08Z,,,
6437,b'Fix #6428',2020-08-12T12:46:11Z,2020-08-12T12:47:31Z,,,
6436,b'Epoch iterator for run_pl_ner.py',2020-08-12T12:18:26Z,2020-10-21T03:28:23Z,wontfix,,
6435,b'Update README.md',2020-08-12T10:23:29Z,2020-08-13T09:01:17Z,model card,,
6434,b'Centralize logging',2020-08-12T09:31:53Z,2020-08-26T15:10:36Z,,,
6433,b'Fix PABEE & PL CI failure',2020-08-12T09:10:53Z,2020-08-13T16:35:42Z,,,
6432,b'TF2 implementation of LineByLineTextDataset?',2020-08-12T08:18:09Z,2020-10-18T21:11:04Z,wontfix,,
6431,b'Disabled pabee test',2020-08-12T06:48:54Z,2020-08-12T06:52:51Z,,,
6430,b'[WIP] QA Loss refactoring',2020-08-12T06:41:13Z,2020-09-09T19:23:53Z,,,
6429,"b""[test schedulers] adjust to test the first step's reading""",2020-08-12T03:38:31Z,2020-08-27T16:23:29Z,,,
6428,b'Error in run_tf_squad.py script',2020-08-11T23:57:54Z,2020-08-12T12:47:32Z,,TypeError,"TypeError: issubclass() arg 1 must be a class"
6427,b'Activate check on the CI',2020-08-11T21:09:19Z,2020-08-12T12:42:15Z,,,
6426,b'Move prediction_loss_only to TrainingArguments',2020-08-11T20:15:16Z,2020-08-12T12:03:46Z,,,
6425,b'[examples] add pytest dependency',2020-08-11T20:14:15Z,2020-08-11T21:58:10Z,,,
6424,b'actions CI self-scheduled: run_examples torch even if run_torch_tests fails',2020-08-11T19:14:55Z,2020-10-11T15:32:42Z,wontfix,,
6423,b'Fixes to make life easier with the nlp library',2020-08-11T18:46:47Z,2020-08-12T12:00:57Z,,,
6422,b'[test] replace capsys with the more refined CaptureStderr/CaptureStdout',2020-08-11T18:37:22Z,2020-08-12T11:54:29Z,,,
6421,b'test_run_glue_with_pabee failing',2020-08-11T18:20:08Z,2020-08-14T17:30:42Z,,,
6420,b'Experiment: ROUGE impact of using pegasus length-penalty implementation',2020-08-11T17:57:08Z,2020-10-11T15:32:25Z,"Help wanted, wontfix",,
6419,b'Add pegasus model cards',2020-08-11T17:56:42Z,2020-08-20T14:39:06Z,,,
6418,b'All learning rates are 0 warning',2020-08-11T16:55:36Z,2020-08-15T16:44:17Z,,,
6417,b'how to fine tune t5 model for summarization task using tensorflow2?',2020-08-11T16:49:25Z,2020-08-11T16:53:01Z,,,
6416,b'Docs: Separate documentation for mbart',2020-08-11T16:21:01Z,2020-08-21T15:57:35Z,"Help wanted, Documentation",,
6415,b'[EncoderDecoder] Add Cross Attention for GPT2',2020-08-11T16:03:26Z,2020-08-14T07:43:30Z,,,
6414,"b""TypeError: forward() got an unexpected keyword argument 'labels'""",2020-08-11T14:30:00Z,2020-08-11T16:15:32Z,,,
6413,b'Create README.md',2020-08-11T13:55:19Z,2020-08-11T14:35:32Z,model card,,
6412,b'Create model card T5-base fine-tuned on event2Mind for Intent Prediction',2020-08-11T13:11:25Z,2020-08-11T22:35:28Z,model card,,
6411,b'[EncoderDecoder] Add encoder-decoder for roberta/ vanilla longformer',2020-08-11T12:40:55Z,2020-08-12T16:23:30Z,,,
6410,b'Cannot unzip the XNLI-MT 1.0 zip file.',2020-08-11T09:11:11Z,2020-10-18T05:14:57Z,wontfix,,
6409,b'TF2 TPU slow?',2020-08-11T07:24:04Z,2020-08-26T00:38:12Z,,,
6408,"b'i have used t5_base for abstractive summarization but it is not giving good results,Could you please give me solution for this'",2020-08-11T07:17:15Z,2020-10-18T05:14:58Z,wontfix,,
6407,b'Slow Decoding Speed when using BertForLMModel',2020-08-11T06:16:53Z,2020-08-13T09:23:10Z,,,
6406,b'RuntimeError: Error while creating shape using tf-xlm-roberta-large',2020-08-11T05:05:27Z,2020-12-13T01:22:48Z,wontfix,RuntimeError,RuntimeError: Error while creating shape
6405,b'[s2s] wmt download script use less ram',2020-08-11T03:44:07Z,2020-08-11T16:04:17Z,,,
6404,"b'[lightning_base] fix s2s logging, only make train_loader once'",2020-08-11T02:27:10Z,2020-08-17T02:49:42Z,,,
6403,b'[s2s] Script to save wmt data to disk',2020-08-11T01:55:57Z,2020-08-11T02:49:40Z,,,
6402,b'remove lr_scheduler redundancy',2020-08-10T21:59:38Z,2020-08-11T02:31:55Z,,,
6401,"b'[TF Longformer] Add Multiple Choice, Seq Classification Model'",2020-08-10T21:24:24Z,2020-11-19T15:37:28Z,Good First Issue,,
6400,b'ZeroDivisionError with Reformer',2020-08-10T20:47:48Z,2020-08-11T10:57:34Z,,ZeroDivisionError,"ZeroDivisionError: integer division or modulo by zero"
6399,b'DPR retriever module',2020-08-10T20:12:24Z,2020-12-24T20:34:51Z,wontfix,,
6398,b'Data collator with padding',2020-08-10T19:47:54Z,2020-08-12T15:20:39Z,,,
6397,b'Create README.md',2020-08-10T19:20:50Z,2020-08-11T13:03:23Z,model card,,
6396,b'switch Hindi-BERT to S3 README',2020-08-10T17:51:39Z,2020-08-11T14:34:23Z,model card,,
6395,b'Bug in the question answering pipeline',2020-08-10T16:45:57Z,2020-08-11T09:14:59Z,,"Notice, KeyError","Notice: little changes in the context text make the bug to not show upKeyError: 0"
6394,b'Error while loading albert for token classification',2020-08-10T16:17:12Z,2020-12-13T01:22:55Z,wontfix,OSError,"OSError: Can't load weights for 'albert-base-v1'. Make sure that:"
6393,b'Add missing docker arg for TPU CI.',2020-08-10T16:12:52Z,2020-08-11T06:48:50Z,,,
6392,b'seq2seq examples require pytest',2020-08-10T15:32:16Z,2020-08-11T21:58:10Z,,"ImportError, ModuleNotFoundError","ImportError: Failed to import test module: seq2seq.test_bash_scriptModuleNotFoundError: No module named 'pytest'"
6391,b'Fix links for open in colab',2020-08-10T15:15:18Z,2020-08-10T15:16:18Z,,,
6390,b'Warn if debug requested without TPU fixes (#6308)',2020-08-10T15:07:42Z,2020-08-11T09:31:27Z,,,
6389,b'Colab button',2020-08-10T15:00:11Z,2020-08-10T15:12:30Z,,,
6388,b'[T5 3B Covid 19] Adapt T5 TF conversion script to handle covid-19 3b t5',2020-08-10T14:22:38Z,2020-10-17T19:59:27Z,wontfix,,
6387,b'Fix docs and bad word tokens generation_utils.py',2020-08-10T14:04:04Z,2020-08-13T11:12:17Z,,`AssertionError,"`AssertionError: Greedy decoding will always produce the same output for num_beams == 1 and num_return_sequences > 1. Please set num_return_sequences = 1`"
6386,b'Create README.md',2020-08-10T13:53:19Z,2020-08-11T20:56:52Z,model card,,
6385,b'[POC] Notebooks cron',2020-08-10T13:35:28Z,2020-08-10T13:46:53Z,,,
6384,"b'AttributeError: type object ""BartTokenizer"" has no attribute \'name\''",2020-08-10T12:36:51Z,2020-10-18T21:11:14Z,wontfix,,
6383,b'hi',2020-08-10T12:23:10Z,2020-08-10T12:28:30Z,,,
6382,b'Ci GitHub caching',2020-08-10T11:56:31Z,2020-08-10T14:39:32Z,,,
6381,b'Create README.md',2020-08-10T11:51:19Z,2020-08-11T22:34:11Z,model card,,
6380,b'Add metadata to be indexed properly',2020-08-10T11:11:38Z,2020-08-11T22:32:29Z,model card,,
6379,b'Change metadata to be indexed correctly',2020-08-10T11:10:08Z,2020-08-11T22:32:19Z,model card,,
6378,b'Create README.md',2020-08-10T11:07:19Z,2020-08-11T22:32:38Z,model card,,
6377,b'[EncoderDecoderModel] add a `add_cross_attention` boolean to config',2020-08-10T10:27:44Z,2020-08-10T17:46:48Z,,,
6376,b'Introduce dataset and data collator for Bert pretrain NSP',2020-08-10T07:26:24Z,2020-08-31T12:26:56Z,,,
6375,b'CUDA Out of Memory',2020-08-10T06:28:09Z,2020-08-10T07:49:10Z,,RuntimeError,"RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 14.73 GiB total capacity; 13.67 GiB already allocated; 15.88 MiB free; 13.72 GiB reserved in total by PyTorch)"
6374,b'[s2s] remove lr_scheduler redundancy',2020-08-10T03:02:03Z,2020-08-23T20:30:36Z,,,
6373,b'Pegasus finetuning diary',2020-08-10T02:59:52Z,2020-08-28T07:45:57Z,,,
6372,b'Update modeling_tf_utils.py',2020-08-09T19:38:21Z,2020-08-10T06:55:12Z,,,
6371,b'the test now works again',2020-08-09T18:28:11Z,2020-08-10T06:55:52Z,,,
6370,b'FastTokenizer not returning batch_size for offset_mapping for short texts',2020-08-09T17:47:39Z,2020-10-18T21:11:07Z,wontfix,,
6369,b'trainer/lightning_base: Arbitrary config updates through command line',2020-08-09T17:05:24Z,2020-10-16T18:17:00Z,wontfix,,
6368,"b""Can't load a saved tokenizer with AutoTokenizer.from_pretrained without saving Config as well""",2020-08-09T17:05:20Z,2020-11-01T16:29:43Z,wontfix,OSError,"OSError: file ./config.json not found"
6367,b'[s2s] pass max_length to config through command line',2020-08-09T16:35:17Z,2020-09-24T21:19:46Z,,,
6366,b'[WIP] Lm loss feed forward chunking',2020-08-09T15:16:38Z,2020-10-18T21:10:52Z,wontfix,,
6365,b'Feed forward chunking others',2020-08-09T15:13:46Z,2020-08-19T12:31:11Z,,,
6364,b'correct pl link in readme',2020-08-09T13:11:28Z,2020-08-10T07:08:47Z,,,
6363,b'[s2s] add BartTranslationDistiller for distilling mBART',2020-08-09T06:29:57Z,2020-08-12T15:41:05Z,,,
6362,"b'[TFTrainer] Error ""iterating over `tf.Tensor` is not allowed""'",2020-08-09T04:43:46Z,2020-08-21T13:21:54Z,,tensorflow.python.framework.errors_impl.OperatorNotAllowedInGraphError,"tensorflow.python.framework.errors_impl.OperatorNotAllowedInGraphError: in user code:"
6361,b'lr_schedulers: add get_polynomial_decay_schedule_with_warmup',2020-08-09T04:07:09Z,2020-08-11T21:56:42Z,model card,,
6360,b'Bug in squad example with XLNet',2020-08-09T02:51:35Z,2020-11-01T03:10:25Z,wontfix,,
6359,b'Mult rouge by 100: standard units',2020-08-09T02:17:10Z,2020-08-13T16:15:55Z,,,
6358,b'[s2s] fix --gpus clarg collision',2020-08-08T23:54:40Z,2020-08-09T01:51:38Z,,,
6357,b'Create Model Card File',2020-08-08T23:08:35Z,2020-08-11T14:36:15Z,model card,,
6356,b'Create Model Card',2020-08-08T22:33:55Z,2020-08-08T22:49:07Z,model card,,
6355,b'Create Model Card File',2020-08-08T22:26:13Z,2020-08-08T22:31:32Z,model card,,
6354,b'GPU memory consumption increases while training',2020-08-08T19:37:19Z,2020-11-01T03:10:19Z,wontfix,RuntimeError,"RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 11.17 GiB total capacity; 10.10 GiB already allocated; 13.81 MiB free; 10.74 GiB reserved in total by PyTorch)"
6353,b'BartModel decodes sequence of incorrect length when decoder_input_ids is specified / Output shape mismatch due to when `use_cache` True/False',2020-08-08T18:36:02Z,2020-12-09T19:55:25Z,,,
6352,b'[GPT2] Correct typo in docs',2020-08-08T18:30:28Z,2020-08-08T18:37:30Z,,,
6351,b'Why is distillbart-cnn done with no teacher and distilbart-xsum has a teacher?',2020-08-08T18:06:53Z,2020-10-18T05:15:19Z,"Discussion, wontfix",,
6350,b'Add model card for electra-base-turkish-cased-ner',2020-08-08T16:54:33Z,2020-08-09T07:39:52Z,model card,,
6349,b'[testing] USE_CUDA default and intuitive skip decorators',2020-08-08T16:29:59Z,2020-10-19T11:08:35Z,,,
6348,b'[Bart] Cannot use Bart decoder cache with torchscript',2020-08-08T15:24:15Z,2020-12-09T19:55:25Z,,RuntimeError,"RuntimeError: Only tensors, lists, tuples of tensors, or dictionary of tensors can be output from traced functions"
6347,"b""ModuleNotFoundError: No module named 'transformers' on Google Colab""",2020-08-08T14:45:03Z,2020-08-11T22:18:51Z,,,
6346,b'Create README.md',2020-08-08T14:43:55Z,2020-08-11T22:31:35Z,model card,,
6345,"b'Is it necessary to provide attention_mask, or model will calculate itself?'",2020-08-08T11:15:25Z,2020-08-08T18:26:25Z,,,
6344,b'[s2s] fix label_smoothed_nll_loss',2020-08-08T08:02:27Z,2020-08-08T08:21:13Z,,,
6343,"b'The default cache directory is lack of disk capacity, I need change the configure of the default cache directory.'",2020-08-08T08:00:49Z,2020-10-18T05:15:20Z,wontfix,,
6342,b'[marian] converter supports models from new Tatoeba project',2020-08-08T07:24:20Z,2020-08-18T03:55:43Z,"New model, marian",,
6341,b'[s2s] tiny QOL improvement: run_eval prints scores',2020-08-08T04:59:44Z,2020-08-08T06:45:56Z,,,
6340,b'PegasusForConditionalGeneration (torch version)',2020-08-08T04:54:59Z,2020-08-11T18:31:24Z,,,
6339,b'refactor almost identical tests',2020-08-08T03:36:10Z,2020-08-10T09:31:20Z,,,
6338,b'remove a TODO item to use a tiny model',2020-08-08T00:57:38Z,2020-08-08T01:30:40Z,,,
6337,b'[CI] add manual workflow dispatch option to github actions runners',2020-08-08T00:43:53Z,2020-10-18T05:15:15Z,wontfix,,
6336,b'broken ONNX slow test',2020-08-08T00:42:31Z,2020-08-17T13:04:36Z,Tests,,
6335,b'delete unused tiny models',2020-08-08T00:32:51Z,2020-10-18T05:15:14Z,wontfix,,
6334,b'[WIP] Avoid call to torch.triu',2020-08-07T23:52:37Z,2020-08-10T23:43:54Z,,,
6333,b'add tests/test_tokenization_reformer.py',2020-08-07T22:51:14Z,2020-08-21T15:54:33Z,"Help wanted, Tests",,
6332,b'[CI] Self-scheduled runner also pins torch',2020-08-07T22:31:01Z,2020-08-07T22:40:22Z,,,
6331,b'Delete this line in label_smoothed_nll_loss',2020-08-07T20:41:14Z,2020-08-08T08:21:13Z,,,
6330,b'BertForPreTraining with NSP',2020-08-07T18:00:19Z,2020-10-18T21:11:10Z,wontfix,,
6329,"b""OSError: Model name 'lonePatient/albert_chinese_small' was not found in tokenizers model""",2020-08-07T16:17:32Z,2020-10-18T05:15:12Z,wontfix,OSError,"OSError: Model name 'lonePatient/albert_chinese_small' was not found in tokenizers model name list (albert-base-v1, albert-large-v1, albert-xlarge-v1, albert-xxlarge-v1, albert-base-v2, albert-large-v2, albert-xlarge-v2, albert-xxlarge-v2). We assumed 'lonePatient/albert_chinese_small' was a path, a model identifier, or url to a directory containing vocabulary files named ['spiece.model'] but couldn't find such vocabulary files at this path or url."
6328,b'Small docfile fixes',2020-08-07T15:46:31Z,2020-08-10T09:37:13Z,,,
6327,b'Batched pipeline',2020-08-07T15:10:56Z,2021-03-06T00:17:42Z,wontfix,,
6326,b'Patch models',2020-08-07T15:04:53Z,2020-08-10T14:39:18Z,,,
6325,b'Text-to-SQL Query',2020-08-07T13:18:05Z,2020-11-07T05:42:49Z,wontfix,,
6324,b'Create README.md',2020-08-07T11:32:26Z,2020-08-11T22:38:19Z,model card,,
6323,"b'Hi , I am having trouble locating the transformers/examples/summarization/bart/ file. I was wondering if it has been renamed or changed?'",2020-08-07T10:40:53Z,2020-08-07T14:23:35Z,,,
6322,b'Transformer-XL: Improved tokenization with sacremoses',2020-08-07T09:26:47Z,2020-08-28T13:56:18Z,,,
6321,b'[Community notebooks] Add notebook on fine-tuning Electra and interpreting with IG',2020-08-07T09:24:07Z,2020-08-08T09:47:34Z,,,
6320,b'Multi-gpu LM finetuning',2020-08-07T08:10:46Z,2020-10-18T05:15:11Z,wontfix,,
6319,b'num_beams error in GPT2DoubleHead model',2020-08-07T07:27:08Z,2020-09-01T10:38:26Z,,IndexError,"IndexError: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
6318,b'TFBert runs slower than keras-bert\xef\xbc\x8c any plan to speed up\xef\xbc\x9f',2020-08-07T02:40:21Z,2020-11-09T01:56:13Z,wontfix,,
6317,b'codecov invalid reports due to inconsistent code coverage outputs (non-idempotent test-suite)',2020-08-07T02:38:13Z,2020-09-29T19:16:44Z,,,
6316,b'Dataloader number of workers in Trainer',2020-08-07T02:29:13Z,2020-09-22T18:44:43Z,,,
6315,"b'[examples] consistently use --gpus, instead of --n_gpu'",2020-08-07T02:02:07Z,2020-08-07T14:36:33Z,,,
6314,"b'[pl] restore lr logging behavior for glue, ner examples'",2020-08-07T01:54:39Z,2020-08-11T20:27:12Z,,,
6313,b'Error trying to import SquadDataset',2020-08-07T00:20:07Z,2020-08-07T20:16:09Z,,ImportError,"ImportError: cannot import name 'SquadDataset' from 'transformers' (/home/brian/miniconda3/envs/ML38/lib/python3.8/site-packages/transformers/__init__.py)"
6312,b'clarify shuffle',2020-08-07T00:18:28Z,2020-08-30T11:26:10Z,,,
6311,b'modify ``val_loss_mean``',2020-08-07T00:11:50Z,2020-10-17T19:59:25Z,wontfix,,
6310,b'collision between different cl arg definitions in examples',2020-08-06T23:01:42Z,2020-10-18T21:11:08Z,wontfix,fail.argparse.ArgumentError,"fail.argparse.ArgumentError: argument --gpus: conflicting option string: --gpus"
6309,b'pl version: examples/requirements.txt is single source of truth',2020-08-06T22:18:13Z,2020-08-11T14:58:55Z,,ImportError,"ImportError: cannot import name 'LightningDataModule' from 'pytorch_lightning.core' (/home/stas/anaconda3/envs/main/lib/python3.7/site-packages/pytorch_lightning/core/__init__.py)"
6308,b'Debug flag to `run_language_modeling` triggers error',2020-08-06T22:15:45Z,2020-08-11T09:47:01Z,,NameError,"NameError: name 'xm' is not defined"
6307,b'fix the shuffle agrument usage and the default',2020-08-06T22:01:08Z,2020-08-07T00:32:28Z,,TypeError,"TypeError: get_dataloader() missing 1 required positional argument: 'shuffle'"
6306,b'solving `make quality` failures',2020-08-06T20:02:39Z,2020-10-10T03:59:04Z,wontfix,,
6305,b'Remove redundant line in run_pl_glue.py',2020-08-06T19:36:17Z,2020-08-06T19:43:46Z,,,
6304,b'Add_argument ``gpus``',2020-08-06T19:29:42Z,2020-08-06T19:54:24Z,,,
6303,b'default `n_tpu_cores` in lightning_base.py',2020-08-06T19:25:17Z,2020-08-06T19:54:50Z,,``pytorch_lightning.utilities.exceptions.MisconfigurationException,"``pytorch_lightning.utilities.exceptions.MisconfigurationException: `tpu_cores` can only be 1, 8 or [<1-8>]``"
6302,b'Default value of `n_tpu_cores` in lightning_base.py',2020-08-06T19:15:36Z,2020-08-06T19:26:00Z,,``pytorch_lightning.utilities.exceptions.MisconfigurationException,"``pytorch_lightning.utilities.exceptions.MisconfigurationException: `tpu_cores` can only be 1, 8 or [<1-8>]``"
6301,b'Redundant code',2020-08-06T17:37:00Z,2020-08-07T01:18:57Z,Help wanted,,
6300,b'[Reformer] fix default generators for pytorch < 1.6',2020-08-06T16:49:17Z,2020-08-06T19:14:47Z,,,
6299,b'Added an Adapter training example',2020-08-06T16:03:04Z,2020-10-28T22:38:04Z,wontfix,,
6298,b'Add a script to check all models are tested and documented',2020-08-06T15:55:09Z,2020-08-07T13:18:38Z,,,
6297,b'Question about BERT model size (transformer block number) ',2020-08-06T15:47:34Z,2020-08-06T16:32:27Z,,,
6296,b'Argument to set GPT2 inner dimension',2020-08-06T15:31:25Z,2020-08-06T15:47:33Z,,,
6295,b'Fix/test convert_mbart.py',2020-08-06T15:27:21Z,2020-10-18T05:15:05Z,wontfix,,
6294,b'How to get word and sentence level embeddings from T5-11b',2020-08-06T14:52:13Z,2020-10-18T05:15:05Z,wontfix,,
6293,b'[s2s]Use prepare_translation_batch for Marian finetuning',2020-08-06T14:47:21Z,2020-08-06T18:58:39Z,,,
6292,b'inconclusive truncation strategies in encode_plus?',2020-08-06T14:45:02Z,2020-10-18T05:15:03Z,wontfix,,
6291,b'Why is the lm_head layer in GPT2LMHeadModel not a parameter?',2020-08-06T14:14:12Z,2020-08-06T16:40:03Z,,,
6290,b'Update model card',2020-08-06T14:03:33Z,2020-08-06T15:42:43Z,model card,,
6289,b'added functionality to output the probabilities of the generated tokens #5164',2020-08-06T13:43:33Z,2020-08-06T16:44:02Z,,,
6288,b'Adding a translation end-to-end example.',2020-08-06T11:22:33Z,2020-11-03T11:45:39Z,,,
6287,b'CI dependency wheel caching',2020-08-06T11:20:50Z,2020-08-07T06:49:00Z,,,
6286,b'F1 decreases resuming from last saved checkpoint of fine-tuning',2020-08-06T10:41:17Z,2020-10-18T05:15:02Z,wontfix,,
6285,b'\xf0\x9f\x8c\x9f T5 V1.1',2020-08-06T09:41:23Z,2020-11-17T11:23:09Z,New model,,
6284,b'Fix the tests for Electra',2020-08-06T08:48:32Z,2020-08-07T13:30:58Z,,,
6283,b'Problem with converting XLM checkpoint to pytorch (missing merges.txt)',2020-08-06T08:26:08Z,2020-12-12T20:44:47Z,wontfix,,
6282,b'Using tensorrt model.engine Inference speed is relatively fast. Why is onnxruntime based on tensorrt as slow as CPU inference',2020-08-06T08:02:57Z,2020-10-18T05:15:01Z,wontfix,,
6281,b'Patch GPU failures',2020-08-06T07:28:57Z,2020-08-07T06:58:16Z,,,
6280,b'Add strip_accents to basic BertTokenizer.',2020-08-06T04:47:30Z,2020-08-06T10:52:29Z,,,
6279,b'TFRobertaMarkedLM model output issue',2020-08-06T03:17:56Z,2020-08-06T16:56:05Z,,,
6278,b'Returning the attention heads using Longformer',2020-08-06T01:55:30Z,2020-08-06T01:55:54Z,,,
6277,b'Reformer now requires PyTorch 1.6.0',2020-08-05T21:56:09Z,2020-08-06T19:14:46Z,,,
6276,b'Add tensorflow version of ElectraForSequenceClassification',2020-08-05T20:25:00Z,2020-08-05T21:23:08Z,,,
6275,"b'How to access the parameters of the uppermost layer of the HuggingFace Transformers via "".modules()""?'",2020-08-05T17:19:02Z,2020-08-07T18:35:47Z,,,
6274,b'Jme p development',2020-08-05T16:55:50Z,2020-10-05T09:33:14Z,wontfix,,
6273,b'Create README.md',2020-08-05T16:31:52Z,2020-08-05T16:36:25Z,model card,,
6272,b'Create README.md for uploaded classifier',2020-08-05T16:26:30Z,2020-08-05T16:27:46Z,model card,,
6271,b'Deleting position IDS when fine-tuning BERT',2020-08-05T16:25:31Z,2020-10-11T23:06:29Z,wontfix,,
6270,b'Adding example with Finnish BERT fine-tuning for NER task',2020-08-05T15:37:43Z,2020-10-18T05:15:00Z,wontfix,,
6269,b'added t5 base and small bahasa summarization readme',2020-08-05T15:36:58Z,2020-08-05T16:27:28Z,model card,,
6268,"b""[Don't merge yet][T5, Bart] Allow t5 torch trace""",2020-08-05T15:36:30Z,2020-10-01T16:16:39Z,,,
6267,b'Unable to make inference from hosted api for a pretrained model that I uploaded. ',2020-08-05T13:07:47Z,2020-08-05T15:10:52Z,,,
6266,b'Bert Mesh Tensorflow',2020-08-05T10:35:40Z,2021-01-24T02:33:45Z,"wontfix, New model",,
6265,b'fix consistency CrossEntropyLoss in modeling_bart',2020-08-05T09:25:46Z,2020-08-07T09:44:28Z,,,
6264,b'TF LMHead very slow: TFGPT2LMHeadModel is 7 times slower than Torch GPT2LMHeadModel ',2020-08-05T09:19:18Z,2021-01-03T13:23:23Z,wontfix,,
6263,b'RuntimeError: CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`',2020-08-05T07:24:25Z,2020-08-05T07:51:40Z,,'RuntimeError,"'RuntimeError: CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`'"
6262,b'Incorrect tokenization for MarianNMT models in example script.',2020-08-05T06:44:11Z,2020-08-06T18:58:39Z,,,
6261,b'Fix typo at get_linear_schedule_with_warmup.',2020-08-05T05:58:40Z,2020-08-05T11:34:58Z,,,
6260,b'cleanup tf unittests: part 2',2020-08-05T04:22:33Z,2020-08-13T08:29:06Z,,,
6259,b'Bart encoder with add_final_layer_norm',2020-08-05T03:44:18Z,2020-09-25T08:24:15Z,,,
6258,b'Gradient Checkpointing with Transformers BERT model',2020-08-05T02:46:20Z,2020-10-11T23:06:30Z,wontfix,,
6257,b'Fix dostring of  class XLNetConfig',2020-08-05T01:17:35Z,2020-08-05T11:37:58Z,,,
6256,"b'LongformerForSequenceClassification has unused layers, making it unable to fine-tune with Data Distributed Parallel (required for gradient checkpointing)'",2020-08-04T22:55:21Z,2020-09-25T18:33:21Z,,RuntimeError,"RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by "
6255,b'Update Model Card',2020-08-04T22:45:15Z,2020-08-07T09:45:53Z,model card,,
6254,b'Update Model Card',2020-08-04T22:45:12Z,2020-08-07T09:46:28Z,model card,,
6253,b'Update Model Card',2020-08-04T22:45:08Z,2020-08-07T09:47:05Z,model card,,
6252,b'Update Model Card',2020-08-04T22:45:05Z,2020-08-07T09:47:27Z,model card,,
6251,b'Update Model Card',2020-08-04T22:45:01Z,2020-08-07T09:47:43Z,model card,,
6250,b'Update Model Card',2020-08-04T22:44:33Z,2020-08-07T09:48:10Z,model card,,
6249,b'Update Model Card',2020-08-04T22:44:14Z,2020-08-07T09:48:39Z,model card,,
6248,b'Update Model Card',2020-08-04T22:43:53Z,2020-08-07T09:51:03Z,model card,,
6247,b'Tf model outputs',2020-08-04T21:49:31Z,2020-08-05T15:34:40Z,,,
6246,b'Update Model Card',2020-08-04T21:21:35Z,2020-08-04T21:40:48Z,model card,,
6245,b'fix zero shot pipeline docs',2020-08-04T20:19:41Z,2020-08-04T20:37:50Z,,,
6244,b'[Reformer] Make random seed generator available on random seed and not on model device',2020-08-04T17:11:16Z,2020-08-04T17:22:44Z,,,
6243,b'Implement DeLighT: Very Deep and Light-weight Transformers',2020-08-04T16:27:59Z,2020-10-11T11:26:05Z,"wontfix, New model",,
6242,b'Add license info to German Bert models',2020-08-04T16:16:23Z,2020-08-04T17:40:49Z,model card,,
6241,b'Trainer + wandb quality of life logging tweaks',2020-08-04T14:55:16Z,2020-08-05T13:05:53Z,,,
6240,b'Documentation bug in GPT2Config',2020-08-04T14:47:17Z,2020-08-08T18:37:29Z,,,
6239,b'add targets arg to fill-mask pipeline',2020-08-04T14:19:38Z,2020-08-12T16:48:30Z,,,
6238,b'Discrepancy in the pad_token_id between the tokenizer and the model code of the T5',2020-08-04T12:56:46Z,2020-08-04T18:43:19Z,,,
6237,b'[Reformer] fix reformer fp16 test',2020-08-04T10:55:33Z,2020-08-04T11:02:26Z,,,
6236,b'losses does not decrease when trainning  with TFTransfoXLLMHeadModel  . ',2020-08-04T10:20:18Z,2020-10-18T21:11:06Z,wontfix,,
6235,b'Upgrade pip ci',2020-08-04T06:52:38Z,2020-08-04T07:20:19Z,,,
6234,b'Upgrade pip when doing CI',2020-08-04T06:26:55Z,2020-08-04T06:37:12Z,,,
6233,b'mismatch keys of glue tasks',2020-08-04T05:37:38Z,2020-10-11T11:26:03Z,wontfix,,
6232,b'[WIP] lightning_base: support --lr_scheduler with multiple possibilities',2020-08-04T04:28:49Z,2020-08-05T13:01:18Z,,,
6231,b'testing utils: capturing std streams context manager',2020-08-04T03:50:55Z,2020-08-11T07:56:48Z,,,
6230,b'mBART Conversion script',2020-08-03T22:39:43Z,2020-08-04T13:53:52Z,,,
6229,b'[s2s] Document better mbart finetuning command',2020-08-03T22:20:59Z,2020-08-03T22:22:31Z,,,
6228,b'ValueError: Unrecognized model identifier in facebook/bart-large-cnn.',2020-08-03T21:44:21Z,2020-08-27T22:32:11Z,,ValueError,"ValueError: Unrecognized model identifier in facebook/bart-large-cnn. Should contains one of 'bert', 'openai-gpt', 'gpt2', 'transfo-xl', 'xlnet', 'xlm', 'roberta', 'distilbert,' 'camembert', 'ctrl', 'albert'"
6227,b'Add SequenceClassification and MultipleChoice TF models to Electra',2020-08-03T21:08:41Z,2020-08-05T13:04:28Z,,,
6226,"b""Can't load config for [community model]""",2020-08-03T20:40:43Z,2020-08-05T15:27:17Z,,,
6225,b'typo',2020-08-03T20:00:25Z,2020-08-04T13:31:50Z,,,
6224,b'test_tokenization_common.py: Remove redundant coverage',2020-08-03T19:54:51Z,2020-08-04T06:59:21Z,,,
6223,"b'benchmarking API: `no_` arguments, double negation, defaults'",2020-08-03T19:42:10Z,2020-10-10T03:57:00Z,wontfix,,
6222,b'memory benchmarking: should the cudnn kernels loading be included',2020-08-03T19:22:49Z,2020-12-12T20:44:45Z,wontfix,,
6221,b'run_hans label fix',2020-08-03T18:42:30Z,2020-08-03T19:02:52Z,,,
6220,b'Improve type annotations in many places',2020-08-03T16:58:22Z,2020-10-11T07:22:30Z,wontfix,,
6219,b'Add setup for TPU CI to run every hour.',2020-08-03T15:58:21Z,2020-08-07T15:17:08Z,,,
6218,b'Comparison different methods for benchmarking',2020-08-03T15:50:38Z,2020-10-18T05:15:07Z,wontfix,,
6217,b'Remove outdated BERT tips',2020-08-03T15:31:24Z,2020-08-03T17:17:56Z,,,
6216,b'Add do_lower_case parameter to GPT2TokenizerFast and RobertaTokenizerFast tokenizers',2020-08-03T14:48:53Z,2020-10-10T08:13:37Z,wontfix,,
6215,b'Add `do_lower_case` handling to GPT2TokenizerFast and descendant tokenizers',2020-08-03T14:47:34Z,2020-10-10T08:13:55Z,wontfix,,
6214,b'Fix _shift_right function in TFT5PreTrainedModel',2020-08-03T13:53:23Z,2020-08-03T14:21:24Z,,,
6213,b'[DataCollatorForLanguageModeling] fix labels ',2020-08-03T12:55:37Z,2020-08-03T14:19:36Z,,,
6212,b'[BartTokenizer] add prepare s2s batch',2020-08-03T12:39:33Z,2020-08-17T15:44:47Z,,,
6211,b'Error when fine tuning GPT2 on GPU ',2020-08-03T12:19:09Z,2020-08-03T14:19:36Z,,TypeError,"TypeError: eq() received an invalid combination of arguments - got (NoneType), but expected one of:"
6210,b'XLM-R has extremely low accuracy after fine-tuning on MNLI',2020-08-03T12:15:42Z,2020-10-23T08:08:43Z,wontfix,,
6209,b'Tips for Tensorflow 2.0 NER task by using fit method.',2020-08-03T12:13:56Z,2020-10-11T11:26:06Z,wontfix,,
6208,b'torch\xe6\xa8\xa1\xe5\x9e\x8b\xe7\x9a\x84forward\xe6\x96\xb9\xe6\xb3\x95\xe9\x87\x8c\xe9\x9d\xa2\xe5\x86\x85\xe5\xae\xb9\xe4\xb8\xba\xe7\xa9\xba\xef\xbc\x8c\xe7\x94\xa8Electra\xe6\xa8\xa1\xe5\x9e\x8b\xe6\x97\xa0\xe6\xb3\x95\xe8\xae\xad\xe7\xbb\x83\xe3\x80\x82',2020-08-03T10:24:58Z,2020-10-10T08:13:51Z,wontfix,,
6207,b'Problems with TFT5',2020-08-03T10:06:41Z,2020-08-03T14:22:30Z,,,
6206,"b'Error while saving electra model in tensorflow ""savedModel"" format'",2020-08-03T09:45:10Z,2020-08-03T19:02:23Z,,,
6205,"b's2s: fix LR logging, remove some dead code.'",2020-08-02T22:25:34Z,2020-08-03T14:36:26Z,,,
6204,b'QA Loss Cleanup',2020-08-02T18:38:15Z,2020-11-16T07:33:57Z,"wontfix, cleanup",,
6203,b'Issue with fp16_opt_level default',2020-08-02T17:14:48Z,2020-10-10T08:13:52Z,wontfix,RuntimeError,"RuntimeError: Found param model.model.shared.weight with type torch.cuda.HalfTensor, expected torch.cuda.FloatTensor."
6202,b'Cannot fine tune my distilbart-cnn-12-6 model because of cuda memory',2020-08-02T15:12:47Z,2020-10-11T11:26:09Z,wontfix,RuntimeError,"RuntimeError: CUDA out of memory. Tried to allocate 100.00 MiB (GPU 0; 11.17 GiB total capacity; 10.59 GiB already allocated; 91.81 MiB free; 10.66 GiB reserved in total by PyTorch)"
6201,b'Update model card',2020-08-02T11:44:33Z,2020-08-04T21:44:15Z,model card,,
6200,b'Update model card',2020-08-02T11:44:13Z,2020-08-04T21:44:06Z,model card,,
6199,b'Update model card',2020-08-02T11:43:46Z,2020-08-04T21:43:49Z,model card,,
6198,b'Update model card',2020-08-02T11:43:34Z,2020-08-04T21:43:39Z,model card,,
6196,b'cleanup torch unittests',2020-08-02T06:15:28Z,2020-08-04T06:42:57Z,,,
6195,b'Encoder decoder config docs',2020-08-02T06:03:43Z,2020-08-04T07:23:29Z,,,
6194,b'longformertokenizerFast gives error',2020-08-02T04:10:00Z,2020-10-18T05:15:16Z,wontfix,Exception,"Exception: Truncation error: Specified max length is too low to respect the various constraints"
6193,b'Some weights not initialized in pre-trained RobertaForMaskedLM',2020-08-01T23:07:00Z,2020-08-03T14:11:08Z,,,
6192,b'GPT2 crashing at loss.backward()',2020-08-01T19:01:41Z,2020-08-03T08:56:58Z,,,
6191,b'How to integrate the Pyro module with HuggingFace Transformers?',2020-08-01T18:24:51Z,2020-08-07T20:30:13Z,,,
6190,b'Add support for truncation argument when calling a Pipeline',2020-08-01T16:04:58Z,2020-11-24T02:58:51Z,wontfix,,
6189,b'Support new tokenizers in distillation example',2020-08-01T15:56:47Z,2020-10-10T03:29:31Z,wontfix,,
6188,b'taeminlee/kogpt2 not working',2020-08-01T15:34:30Z,2020-10-10T08:13:58Z,wontfix,,
6187,b'add new model prophetnet',2020-08-01T12:43:52Z,2020-09-16T07:45:09Z,model card,,
6186,b'Remove inconsistency between BertTokenizer and BertTokenizerFast ',2020-08-01T10:24:56Z,2020-08-06T12:31:23Z,,,
6185,b'Fix docstring for `BertTokenizerFast`.',2020-08-01T10:06:21Z,2020-08-02T07:58:26Z,,,
6184,b'[s2s] clean up + doc',2020-08-01T05:45:39Z,2020-08-01T18:51:08Z,,,
6183,b'Fix tokenizer saving/loading with custom token objects',2020-07-31T23:28:33Z,2020-11-05T01:47:46Z,wontfix,,
6182,b'Failing XLMModelTest',2020-07-31T21:19:34Z,2020-08-07T06:58:16Z,,,
6181,b'Failing ONNX Export test',2020-07-31T21:13:37Z,2020-10-10T08:14:06Z,wontfix,,
6180,b'Fixed typo in Longformer',2020-07-31T21:11:03Z,2020-08-01T10:20:49Z,,,
6179,b'HANS Dataset: Incorrect `label_list` and `label`.',2020-07-31T21:05:02Z,2020-08-03T16:36:31Z,,,
6178,b'Why are the `device()` and `dtype()` functions in `modelling_utils.py` needed?',2020-07-31T20:52:09Z,2020-10-10T08:14:07Z,wontfix,,
6177,b'RoBERTa for QuestionAnswering ',2020-07-31T20:49:29Z,2020-08-19T20:05:14Z,,ValueError,"ValueError: 102 is not in list"
6176,b'Adds comet_ml to the list of auto-experiment loggers',2020-07-31T20:18:26Z,2020-08-06T15:31:31Z,,,
6175,b'Doc pipelines',2020-07-31T20:15:33Z,2020-08-03T15:44:47Z,,,
6174,b't',2020-07-31T16:14:28Z,2020-07-31T16:16:44Z,,,
6173,"b'My finetuned gpt2 model is taking wayy too long to generate samples, like 5-8 minutes'",2020-07-31T11:59:52Z,2020-08-11T16:55:05Z,,,
6172,b'\xf0\x9f\x90\x9b Not adding `token_type_ids` when the model is `electra` (pytorch_lightning example)',2020-07-31T10:22:25Z,2020-10-10T08:14:05Z,wontfix,,
6171,b'Update convert_pytorch_checkpoint_to_tf2.py',2020-07-31T08:37:45Z,2020-10-10T03:29:32Z,wontfix,,
6170,b'[Benchmark]',2020-07-31T07:38:28Z,2020-07-31T09:12:04Z,,,
6169,b'Create README.md',2020-07-31T06:54:19Z,2020-07-31T08:28:36Z,model card,,
6168,b'Albert pretrain datasets/ datacollator',2020-07-31T06:29:52Z,2020-09-10T11:56:30Z,,,
6167,b'fix the slow tests doc',2020-07-31T00:36:16Z,2020-08-07T13:17:33Z,,,
6166,b'[wip] diagnose MT metrics regression from pl 0.8.5 upgrade',2020-07-30T23:46:57Z,2020-08-03T22:24:05Z,"Help wanted, lightning",,
6165,b'update min tf requirements',2020-07-30T23:13:28Z,2020-08-03T22:26:02Z,,,
6164,b'RoBERTa ``tokenizer.decode`` does not produce the same sentence.',2020-07-30T22:32:05Z,2020-10-10T12:13:29Z,wontfix,,
6163,b'correct the correction',2020-07-30T21:43:34Z,2020-07-30T22:00:03Z,,,
6162,b'typos',2020-07-30T20:20:06Z,2020-07-30T21:18:27Z,,,
6161,b'Padding Strategy Code missing an else case (maybe?)',2020-07-30T20:11:14Z,2020-11-05T17:07:56Z,,,
6160,b'run_squad.py eval metrics meaning',2020-07-30T20:10:19Z,2021-03-06T00:17:43Z,wontfix,,
6159,b'OSError: Unable to load weights from pytorch checkpoint file. ',2020-07-30T18:35:08Z,2020-10-18T05:15:21Z,wontfix,OSError,"OSError: Unable to load weights from pytorch checkpoint file. If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True. "
6158,b'Add CircleCI config to run TPU tests.',2020-07-30T18:18:07Z,2020-08-03T16:00:05Z,model card,,
6157,b'Harmonize both Trainers API',2020-07-30T18:07:04Z,2020-07-31T13:43:24Z,,,
6156,b'should mBART-large-en-ro have decoder_start_token_id by default?',2020-07-30T16:35:52Z,2020-08-22T02:03:48Z,"Help wanted, translation",,
6155,b'Model output test',2020-07-30T15:59:51Z,2020-07-31T13:44:37Z,,,
6154,b'Hidden State Embedding-Transformers',2020-07-30T14:52:16Z,2020-07-31T15:41:33Z,,,
6153,b'readme m3hrdadfi/albert-fa-base-v2',2020-07-30T11:04:31Z,2020-07-31T10:19:07Z,model card,,
6152,b'Using BertWordPiece Tokenizer',2020-07-30T09:36:22Z,2020-10-10T08:14:03Z,wontfix,`OSError,"`OSError: Model name './EsperBERTo_italian' was not found in tokenizers model name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). We assumed './EsperBERTo_italian' was a path, a model identifier, or url to a directory containing vocabulary files named ['vocab.json', 'merges.txt'] but couldn't find such vocabulary files at this path or url."
6151,b'Add Pytorch Native AMP support in Trainer',2020-07-30T07:45:36Z,2020-07-31T08:23:30Z,,,
6150,b'\xf0\x9f\x90\x9b T5 Tokenizer ignores \\n \\t characters and more than one whitespace together',2020-07-30T04:53:04Z,2020-09-07T20:51:36Z,,,
6149,b'[s2s] add support for overriding config params',2020-07-30T03:01:12Z,2020-07-30T05:09:47Z,,,
6148,b'tokenize cache for examples/language-modeling',2020-07-30T02:56:11Z,2020-10-10T08:14:11Z,wontfix,,
6147,"b""the documents for transformer don't work""",2020-07-30T02:44:55Z,2020-10-10T12:13:28Z,wontfix,,
6146,b'\xf0\x9f\x8c\x9f Mirostat decoding algorithm',2020-07-30T01:45:13Z,2020-10-10T08:14:13Z,"wontfix, New model",,
6145,b'TOKENIZER: truncation not working for batch',2020-07-30T00:59:28Z,2020-08-04T19:02:52Z,,,
6144,"b""Question-Answering pipeline doesn't work anymore with long text""",2020-07-29T21:48:05Z,2020-09-01T09:35:52Z,,KeyError,"KeyError: 0"
6143,b'Tf trainer cleanup',2020-07-29T20:37:01Z,2020-07-30T13:13:17Z,,,
6142,b'Fix FlauBERT GPU test',2020-07-29T18:59:21Z,2020-07-30T15:11:49Z,,,
6141,b'Bug in language_modeling.py calling tokenizer.num_special_tokens_to_add',2020-07-29T18:41:26Z,2020-07-29T20:29:41Z,,,
6140,b'Copyright date and owner not filled out in LICENSE file',2020-07-29T17:58:01Z,2020-10-04T20:13:43Z,wontfix,,
6139,b'Applying hugging face transformer in sequence labeling problem',2020-07-29T17:07:25Z,2020-07-31T00:42:14Z,,,
6138,b'Switch from return_tuple to return_dict',2020-07-29T17:03:44Z,2020-07-30T13:17:00Z,,,
6137,b'StopIteration error when using HuggingFace Transformer models',2020-07-29T16:42:42Z,2020-10-04T20:13:44Z,wontfix,,
6136,b'frequent checkpoints have worse performance',2020-07-29T16:25:57Z,2020-10-11T11:26:01Z,wontfix,,
6135,b'How to combine the encoded representations of two transformers',2020-07-29T16:20:07Z,2020-10-04T20:13:42Z,wontfix,,
6134,b'Fix TF CTRL model naming',2020-07-29T16:11:33Z,2020-07-29T16:20:01Z,,,
6133,b'bart-large-mnli-yahoo-answers model card',2020-07-29T15:24:57Z,2020-07-31T14:56:33Z,model card,,
6132,b'MBartTokenizerTrimmed to support truncated embeddings',2020-07-29T14:39:44Z,2021-03-06T00:17:45Z,"wontfix, Core: Tokenization, translation",,
6131,b'Enable ONNX/ONNXRuntime optimizations through converter script',2020-07-29T14:07:02Z,2020-07-31T07:45:14Z,,,
6130,b'Use google style to document properties',2020-07-29T14:01:22Z,2020-07-29T16:28:13Z,,,
6129,b'Add new pre-trained models BERTweet and PhoBERT',2020-07-29T12:59:40Z,2020-09-18T17:16:44Z,model card,,
6128,b'add deepset/xlm-roberta-large-squad2 model card',2020-07-29T12:18:53Z,2020-07-29T15:34:17Z,model card,,
6127,b'Initializing XLMRobertaTokenizer using pretrained tokenizer expects serialized vocab',2020-07-29T11:45:28Z,2020-07-31T06:36:27Z,,,
6126,b'Add decoding inputs to generate',2020-07-29T10:50:18Z,2020-07-30T20:23:31Z,,,
6125,b'problem about geting hidden_states using TFBertModel',2020-07-29T10:48:15Z,2020-08-05T11:25:31Z,,,
6124,b'convert_pytorch_checkpoint_to_tf2.py AttributeError: cls.seq_relationship.weight not found in PyTorch model',2020-07-29T09:45:23Z,2020-10-18T21:11:05Z,wontfix,AttributeError,"AttributeError: cls.seq_relationship.weight not found in PyTorch model"
6123,b'Create README.md',2020-07-29T09:13:25Z,2020-08-04T21:42:54Z,model card,,
6122,b'[T5Tokenizer] add prepare_seq2seq_batch method',2020-07-29T09:10:12Z,2020-08-17T17:57:20Z,,,
6121,b'XLNet PLM Readme',2020-07-29T09:00:49Z,2020-07-29T15:38:16Z,,,
6120,"b""Don't see how to use correct padding with QA pipeline""",2020-07-29T08:49:52Z,2020-10-04T20:13:39Z,wontfix,,
6119,b'\xf0\x9f\x90\x9b Empty TypeError on BartTokenizerFast.decode(tensor)',2020-07-29T08:18:43Z,2020-10-04T20:13:41Z,wontfix,,
6118,b'Is `guid` allowed to be None in `InputExample`?',2020-07-29T06:19:40Z,2020-10-10T08:14:09Z,wontfix,,
6117,b'Using control codes for finetuning',2020-07-29T03:44:35Z,2020-10-04T20:13:38Z,wontfix,,
6116,b'No button for creating new post at the forum.',2020-07-29T03:35:07Z,2020-07-29T08:11:21Z,,,
6115,b'Usage of Pytorch Native AMP in place of apex (Pytorch 1.6) in Trainer',2020-07-29T03:08:06Z,2020-07-31T08:23:30Z,,,
6114,"b'namespace object has no attribute to ""enc_only""'",2020-07-29T02:55:00Z,2020-10-12T15:46:06Z,wontfix,AttributeError,"AttributeError: 'Namespace' object has no attribute 'enc_only'"
6113,b'\xf0\x9f\x8c\x9f BigBird',2020-07-29T01:45:45Z,2021-03-30T05:51:35Z,New model,,
6112,b'Is there any way that I can use the HuggingFace Transformers as Pyro models?',2020-07-29T00:48:12Z,2020-07-29T16:12:00Z,,,
6111,b'Use FutureWarning to deprecate',2020-07-28T21:49:32Z,2020-07-29T09:20:54Z,,,
6110,b'Doc tokenizer',2020-07-28T21:24:30Z,2020-07-30T18:51:20Z,,,
6109,b'StopIteration error in RobertaForMultipleChoice',2020-07-28T20:29:57Z,2020-07-29T00:48:28Z,,,
6108,b'allenai/longformer-large-4096 unavailable',2020-07-28T20:05:14Z,2020-07-28T22:14:14Z,,OSError,"OSError: Can't load config for 'allenai/longformer-large-4096'. Make sure that:"
6107,b'Where do the Masked Language Model perform mask on the input data',2020-07-28T19:46:47Z,2020-08-11T16:58:08Z,,,
6106,b'Weird Behavior on XLNetTokenizer after new tokens added',2020-07-28T19:34:10Z,2020-10-04T01:14:06Z,wontfix,,
6105,b'Recursive error calling generate in forward',2020-07-28T19:28:52Z,2020-11-01T16:29:51Z,wontfix,RecursionError,"RecursionError: maximum recursion depth exceeded while calling a Python object"
6104,b'Fix zero-shot pipeline single seq output shape',2020-07-28T18:39:03Z,2020-07-28T18:46:03Z,,,
6103,b'rename prepare_translation_batch -> prepare_seq2seq_batch',2020-07-28T18:24:59Z,2020-08-11T19:57:08Z,,,
6102,b'Fix deebert tests',2020-07-28T17:59:03Z,2020-07-28T22:30:17Z,,,
6101,b'Use HFArgParser instead of Fire',2020-07-28T17:43:18Z,2020-10-06T18:46:50Z,cleanup,,
6100,b'[Fix] position_ids tests again',2020-07-28T17:42:17Z,2020-07-28T22:29:36Z,,,
6099,b'[fix] add bart to LM_MAPPING',2020-07-28T17:38:03Z,2020-07-28T22:29:19Z,,,
6098,"b""Fix #6096: MBartTokenizer's mask token""",2020-07-28T16:35:53Z,2020-07-28T22:27:59Z,,,
6097,b'Logs should not be hidden behind a logger.info',2020-07-28T16:26:01Z,2020-07-28T16:44:26Z,,,
6096,b'mBART: incorrect <mask> token id',2020-07-28T14:25:21Z,2020-07-28T22:27:59Z,,,
6095,b'Add BERTweet and PhoBERT',2020-07-28T14:08:16Z,2020-07-29T12:13:16Z,model card,,
6094,b'5 Slow test failures',2020-07-28T14:07:02Z,2020-07-29T01:29:13Z,,,
6093,b'Fix #6092',2020-07-28T13:32:53Z,2020-07-28T13:48:39Z,,,
6092,b'i dont know what Tranier`s Dataset is.',2020-07-28T13:11:48Z,2020-07-28T13:48:43Z,,,
6091,b'Fix local_files_only for TF',2020-07-28T10:47:35Z,2020-10-01T09:06:03Z,,ValueError,"ValueError: Cannot find the requested files in the cached path and outgoing traffic has been disabled. To enable model look-ups and downloads online, set 'local_files_only' to False."
6090,b'customize special tokens',2020-07-28T10:22:25Z,2020-08-01T16:23:12Z,,,
6089,b'Added capability to quantize a model while exporting through ONNX.',2020-07-28T09:17:46Z,2020-07-29T11:21:30Z,,,
6088,b'Finetuning German BERT for QA on biomedical domain ',2020-07-28T09:09:32Z,2021-03-06T00:17:47Z,wontfix,,
6087,"b'fixed typos, added example question.'",2020-07-28T08:27:04Z,2020-07-28T12:33:53Z,model card,,
6086,b'Replace mecab-python3 with fugashi for Japanese tokenization',2020-07-28T07:21:25Z,2020-07-31T08:41:15Z,,,
6085,b'tf.saved_model.save is not worked on TFElectra* series.',2020-07-28T06:27:43Z,2020-10-04T01:14:14Z,wontfix,TypeError,"TypeError: Expected Operation, Variable, or Tensor, got None"
6084,b'ValueError raises when load Flaubert from pre-train with Transformers >=3.0.0',2020-07-28T06:17:54Z,2020-08-31T08:53:21Z,,ValueError,"ValueError: not enough values to unpack (expected 6, got 5)"
6083,b'Error when using np.where() during squad tokenization ',2020-07-28T03:11:43Z,2020-10-04T01:14:13Z,wontfix,,
6082,b'\xf0\x9f\x90\x9b Inconsistencies between BartTokenizer and BartTokenizerFast',2020-07-28T02:21:15Z,2020-07-29T14:16:07Z,,ValueError,"ValueError: Keyword arguments {'add_prefix_space': True} not recognized."
6081,"b'[s2s] Delete useless method, log tokens_per_batch'",2020-07-28T00:46:46Z,2020-07-28T15:24:24Z,,,
6080,b'Proposal: seq2seq tokenizers expose a prepare_seq2seq_batch method',2020-07-28T00:09:49Z,2020-08-21T15:56:33Z,Core: Tokenization,,
6079,"b""[s2s] Don't mention packed data in README""",2020-07-28T00:07:08Z,2020-07-28T00:07:22Z,,,
6078,b'model.roberta.from_pretrained() fails to change the parameters',2020-07-27T23:27:04Z,2020-10-04T01:14:12Z,wontfix,,
6077,b'[s2s] dont document packing because it hurts performance',2020-07-27T22:25:37Z,2020-07-27T22:26:01Z,,,
6076,b'Create README.md',2020-07-27T22:06:32Z,2020-07-28T13:36:01Z,model card,,
6075,b'Create README.md',2020-07-27T22:03:43Z,2020-08-04T21:41:23Z,model card,,
6074,b'Cannot use the RobertaForMultipleChoice model for processing multiple choice questions with 4 options',2020-07-27T22:02:32Z,2020-10-04T01:14:02Z,wontfix,ValueError,"ValueError: too many values to unpack (expected 2)"
6073,b'Create README.md',2020-07-27T22:01:00Z,2020-07-27T22:04:18Z,model card,,
6072,b'Error in the RobertaTokenizer?',2020-07-27T21:31:29Z,2020-10-04T01:14:10Z,wontfix,,
6071,"b'Loading and running on CPU, the RoBERTa model traced/saved on GPU.'",2020-07-27T21:17:16Z,2020-10-04T01:14:11Z,wontfix,RuntimeError,"RuntimeError: The following operation failed in the TorchScript interpreter."
6070,b'lightning_base: new clarg: lr_scheduler=polynomial_decay',2020-07-27T20:49:36Z,2020-08-11T21:56:42Z,,,
6069,b'lightning_base: new clarg: adam_betas',2020-07-27T20:47:41Z,2020-10-04T01:14:10Z,wontfix,,
6068,b'link to README.md',2020-07-27T20:39:24Z,2020-07-28T12:34:59Z,,,
6067,b'fix typo',2020-07-27T20:34:35Z,2020-07-28T12:38:06Z,,,
6066,b'Add fire to setup.cfg to make isort happy',2020-07-27T18:14:12Z,2020-07-27T19:17:34Z,,,
6065,b'Make all data collators accept dict',2020-07-27T17:13:03Z,2020-07-28T13:08:20Z,,,
6064,"b'[Performance improvement] ""Bad tokens ids"" optimization'",2020-07-27T16:17:39Z,2020-08-11T09:56:40Z,,,
6063,b'[fix] no warning for position_ids buffer',2020-07-27T16:12:32Z,2020-07-28T00:00:45Z,,,
6062,b'Add new AutoModel classes in pipeline',2020-07-27T15:43:08Z,2020-07-27T15:50:09Z,,,
6061,b'Pipelines should use tuples instead of namedtuples',2020-07-27T15:39:34Z,2020-07-28T07:14:31Z,,,
6060,b'new AutoModel classes in pipeline',2020-07-27T14:42:22Z,2020-07-27T15:52:05Z,,,
6059,b'Errors while using TFAutoModelForMultipleChoice and TFTrainer on winogrande dataset ',2020-07-27T14:32:24Z,2020-10-04T01:14:07Z,wontfix,TypeError,"TypeError: in converted code:"
6058,b'Create README.md',2020-07-27T13:42:15Z,2020-07-27T13:42:52Z,,,
6057,b' Transformer layers + Functional API is failed but subclass is successful',2020-07-27T10:46:01Z,2020-09-18T06:09:53Z,,`ValueError,"`ValueError: It appears you are trying to construct a functional model, but not all of the inputs in the first positional argument of your layer call are symbolic tensors. (Input objects, or the output of another layer) Functional models cannot correctly track custom layers unless all values in the first call argument are symbolic.`"
6056,b'Empty assert hunt',2020-07-27T09:53:40Z,2020-08-03T08:19:04Z,,,
6055,b'add another e.g. to avoid confusion',2020-07-27T09:36:27Z,2020-07-30T12:53:35Z,,,
6054,b'Errors while creating a subclass of BertForTokenClassification in run_ner.py file',2020-07-27T08:57:51Z,2020-10-04T01:14:06Z,wontfix,,
6053,"b'Errors when creating a subclass of ""BertForTokenClassification"" '",2020-07-27T08:54:50Z,2020-07-27T09:49:12Z,,,
6052,b'add gpt2 padding for tflite',2020-07-27T05:14:11Z,2020-08-05T08:55:02Z,,,
6051,b'examples/seq2seq:  add a dataloader that supports dynamic batch size',2020-07-27T03:29:00Z,2020-09-17T19:19:35Z,,,
6050,b'CI: run tests against torch=1.6',2020-07-27T03:25:40Z,2020-07-29T01:30:13Z,"Help wanted, Tests",,
6049,b'examples/seq2seq/test_bash_script.py :: actually learn something',2020-07-27T03:24:13Z,2020-11-06T04:15:14Z,"Help wanted, Tests",,
6048,b'examples/seq2seq/test_bash_script.py covers summarization',2020-07-27T03:22:19Z,2020-10-11T11:26:02Z,"Help wanted, wontfix, Tests",,
6047,b'Feed to forward new parameters as computed manually by update rule',2020-07-27T03:09:23Z,2020-10-10T08:14:11Z,wontfix,,
6046,b'is_pretokenized seems to work incorrectly ',2020-07-26T23:23:46Z,2020-08-25T12:09:59Z,,,
6045,"b""Test BART's memory  consumption""",2020-07-26T21:24:39Z,,"Help wanted, Tests, Benchmarks, WIP",,
6044,b'slow from_pretrained failures',2020-07-26T20:57:18Z,2020-07-28T00:00:44Z,,,
6043,b'docs(pretrained_models): fix num parameters',2020-07-26T17:20:12Z,2020-10-05T11:33:01Z,,,
6042,b'Update README.md of my model',2020-07-26T16:44:52Z,2020-07-26T21:31:49Z,model card,,
6041,b'docs(pretrained_models): fix num parameters',2020-07-26T16:19:04Z,2020-07-26T17:33:30Z,,,
6040,b'Draft Etalab QA model',2020-07-26T14:21:08Z,2020-07-27T09:15:09Z,model card,,
6039,b'MarianMT - How to find out the actual names of the languages? - Only language-codes are available',2020-07-26T10:42:57Z,2020-08-06T19:08:22Z,,,
6038,b'Rework TF trainer',2020-07-26T09:16:19Z,2020-07-29T18:32:02Z,,,
6037,b'Model card for Vamsi/T5_Paraphrase_Paws',2020-07-26T08:45:40Z,2020-07-27T09:12:46Z,model card,,
6036,"b""don't complain about missing W&B when WANDB_DISABLED=true""",2020-07-26T00:45:04Z,2020-07-26T18:29:54Z,,,
6035,b'add a summary report flag for run_examples on CI',2020-07-25T22:23:05Z,2020-07-26T18:09:15Z,,,
6034,b'add pl_glue example test',2020-07-25T20:38:50Z,2020-08-11T07:16:53Z,,,
6033,b'Is there an easy way to access the multiple choice head of the RobertaForMultipleChoice?',2020-07-25T18:24:16Z,2020-07-29T16:10:10Z,,,
6032,b'Create README.md',2020-07-25T17:55:38Z,2020-07-27T20:25:38Z,model card,,
6031,b'Error with batch_encode_plus',2020-07-25T10:02:16Z,2020-07-25T11:30:06Z,,ValueError,"ValueError: not enough values to unpack (expected 2, got 1)"
6030,b'create model-card for lordtt13/emo-mobilebert',2020-07-25T09:39:39Z,2020-07-28T14:00:24Z,model card,,
6029,b'tensorflow bert model can\xe2\x80\x98t  return all hidden_states ',2020-07-25T09:38:28Z,2020-10-25T00:40:42Z,wontfix,IndexError,"IndexError: tuple index out of range"
6028,b'examples/text-classification/run_pl.sh multiple problems',2020-07-25T03:21:16Z,2020-08-07T01:55:55Z,,"AttributeError, pytorch_lightning.utilities.exceptions.MisconfigurationException","AttributeError: 'Namespace' object has no attribute 'gpus'pytorch_lightning.utilities.exceptions.MisconfigurationException: `tpu_cores` can only be 1, 8 or [<1-8>]"
6027,b'[Fix] text-classification PL example',2020-07-25T02:02:17Z,2020-08-06T19:46:43Z,Examples,,
6026,b'Fix tokenizer saving and loading error',2020-07-25T00:15:45Z,2020-08-11T08:49:17Z,,,
6025,b'Failed to save tokenizer with AddedToken in additional_special_tokens',2020-07-25T00:14:05Z,2020-08-11T08:49:39Z,,TypeError,"TypeError: Object of type AddedToken is not JSON serializable"
6024,b'Feed forward chunking',2020-07-24T20:00:09Z,2020-08-11T07:12:45Z,,,
6023,b'Remove unused file',2020-07-24T19:20:32Z,2020-07-27T12:31:24Z,,,
6022,b'Fix the return documentation rendering for all model outputs',2020-07-24T18:05:20Z,2020-07-27T13:19:00Z,,,
6021,"b""[CI] Don't test apex""",2020-07-24T17:04:46Z,2020-07-24T19:34:17Z,,,
6020,b'Create README.md',2020-07-24T15:29:48Z,2020-07-24T18:11:15Z,model card,,
6019,b'Update the new model template',2020-07-24T15:14:35Z,2020-07-24T18:15:38Z,,,
6018,b'seq2seq/finetune.py can take config train_params through command line',2020-07-24T14:38:23Z,2020-07-30T05:10:12Z,,,
6017,b'convert_bart script should support mbart through command line.',2020-07-24T14:35:40Z,2020-10-01T07:08:11Z,wontfix,,
6016,b'Create model card for RuPERTa-base',2020-07-24T14:31:54Z,2020-07-24T18:12:30Z,model card,,
6015,b'Tf trainer cleanup',2020-07-24T14:00:22Z,2020-07-29T20:29:45Z,,,
6014,b'Fix question template',2020-07-24T13:12:35Z,2020-07-24T14:04:26Z,,,
6013,b'Cannot import DPRReader',2020-07-24T09:36:50Z,2020-07-24T13:14:47Z,,,
6012,"b'When i train a GPT-2 which uses BPE, the computed perplexity is per sub-word right?'",2020-07-24T08:38:56Z,2021-03-06T00:17:50Z,wontfix,,
6011,b'[Benchmark]',2020-07-24T03:57:29Z,2020-07-30T10:25:17Z,,,
6010,b'install sentence-transformers on Linux by python error',2020-07-24T03:50:44Z,2020-10-21T03:28:21Z,wontfix,,
6009,b'[model_cards] roberta-base-finetuned-yelp-polarity',2020-07-24T02:26:50Z,2020-07-24T13:45:22Z,model card,,
6008,b'fix: model card readme clutter',2020-07-23T22:23:59Z,2020-07-24T08:17:53Z,model card,,
6007,b'Fine tune T5 for paraphrase generation',2020-07-23T22:08:47Z,2020-10-04T01:14:03Z,wontfix,,
6006,b'Model cards: add roberta-base-squad-v1 and bert-base-uncased-squad-v1',2020-07-23T20:46:29Z,2020-07-23T21:53:48Z,model card,,
6005,b'Model utils doc',2020-07-23T20:33:07Z,2020-07-24T13:16:29Z,,,
6004,"b""CI/Examples: ModuleNotFoundError: No module named '_sqlite3'""",2020-07-23T20:25:00Z,2020-07-24T14:28:37Z,,,
6003,b'MBART: support summarization tasks where max_src_len > max_tgt_len',2020-07-23T20:13:44Z,2020-07-28T12:18:12Z,,,
6002,b'Deebert Examples test failure',2020-07-23T20:05:18Z,2020-07-28T22:30:17Z,,,
6001,b'MbartDataset can support Summarization',2020-07-23T19:56:14Z,2020-07-28T12:18:12Z,,,
6000,b'Bert german dbmdz uncased sentence stsb',2020-07-23T19:40:20Z,2020-07-23T21:56:46Z,model card,,
5999,b'Fix #5974',2020-07-23T16:04:55Z,2020-07-23T17:51:29Z,,,
5998,b'MbartTokenizer: do not hardcode vocab size',2020-07-23T15:51:08Z,2020-07-23T19:41:14Z,,,
5997,b'bug in trainer.py line297',2020-07-23T15:42:34Z,2020-07-23T16:07:12Z,,,
5996,b'T5 pre training on different languages from scratch',2020-07-23T13:00:19Z,2020-11-24T02:58:56Z,wontfix,,
5995,"b'[WIP] Trainer supports all Datasets as train_dataset, with/without __len__ #5990'",2020-07-23T12:34:15Z,2020-10-11T07:22:29Z,wontfix,,
5994,b'[examples (seq2seq)] fix preparing decoder_input_ids for T5 ',2020-07-23T10:34:54Z,2020-07-27T14:10:44Z,,,
5993,b'Store Predictions on CPU in Every Prediction Iteration (Trainer)',2020-07-23T09:35:56Z,2020-10-01T07:08:06Z,wontfix,,
5992,b'ONNX documentation',2020-07-23T09:22:55Z,2020-07-29T09:02:35Z,,,
5991,b'T5 Tensorflow: _shift_right returns wrong result',2020-07-23T08:45:53Z,2020-08-03T18:22:44Z,,,
5990,b'Trainer: exception raised when calling len() on IterableDataset',2020-07-23T08:07:07Z,2020-10-01T07:08:07Z,wontfix,TypeError,"TypeError: object of type 'MultiTextDataset' has no len()"
5989,b'Create README.md',2020-07-23T08:07:04Z,2020-07-23T15:41:34Z,model card,,
5988,b'EncoderDecoderModel:  weight can not be init from the checkpoint',2020-07-23T07:40:13Z,2020-11-09T01:56:22Z,wontfix,,
5987,b'Possible bug in preparing deocder_input_ids for T5 in seq2seq finetune.y',2020-07-23T05:42:13Z,2020-07-27T14:10:44Z,,,
5986,b'how can I download T5-11B pretrained model?',2020-07-23T01:36:51Z,2020-10-01T07:08:09Z,wontfix,**OSError,"**OSError: Can't load weights for 't5-11b'. Make sure that:"
5985,b'Update doc of the model page',2020-07-22T21:37:19Z,2020-07-22T22:14:58Z,,,
5984,b'Albert pre-train from scratch convergence problem',2020-07-22T21:34:05Z,2020-09-10T11:56:29Z,,notice,"notice: I was deliberately set the eval dataset the same as training set for checking training loss at last run."
5983,"b'pipeline does not do truncation on long texts input, error message found'",2020-07-22T20:40:48Z,2020-07-22T21:35:33Z,,IndexError,"IndexError: index out of range in self"
5982,b'Cleanup Trainer and expose customization points',2020-07-22T20:03:58Z,2020-07-23T16:05:42Z,,,
5981,b'[WIP] Proposal for TF model outputs',2020-07-22T18:19:50Z,2020-07-29T20:39:26Z,,,
5980,b'add fine-tuned mobilebert squad v1 and squad v2 model cards',2020-07-22T17:55:20Z,2020-07-23T15:57:30Z,model card,,
5979,b'dynamic masking for RoBERTa model',2020-07-22T17:20:45Z,2020-10-04T20:24:30Z,,,
5978,b'Training data format',2020-07-22T16:54:37Z,2020-11-09T01:56:15Z,wontfix,,
5977,b'[demo] Broken fp16 test',2020-07-22T16:27:16Z,2020-07-22T18:16:16Z,,,
5976,b'[test] partial coverage for train_mbart_enro_cc25.sh',2020-07-22T16:00:53Z,2020-07-22T18:34:50Z,,,
5975,b'Transformer-XL: Fixed returned outputs when using `return_tuple=True`',2020-07-22T14:43:26Z,2020-07-24T07:56:30Z,,,
5974,"b""Transformer-XL: no mems are return when using 'return_tuple'""",2020-07-22T14:27:26Z,2020-07-23T17:51:34Z,,ValueError,"ValueError: not enough values to unpack (expected 3, got 2)"
5973,b'[cleanup] much cruft in unittests',2020-07-22T14:17:30Z,2020-08-04T06:42:57Z,"Help wanted, cleanup",,
5972,b'Update to match renamed attributes in fairseq master',2020-07-22T13:52:56Z,2020-08-05T11:23:56Z,,,
5971,"b""ImportError: cannot import name 'MODEL_WITH_LM_HEAD_MAPPING'""",2020-07-22T13:14:03Z,2020-07-22T13:21:51Z,,,
5970,b'[WIP] Ner pipeline grouped_entities fixes',2020-07-22T13:02:59Z,2020-11-03T22:21:05Z,,,
5969,"b""run_squad example doesn't work with XLM model""",2020-07-22T13:01:26Z,2020-10-10T08:14:02Z,wontfix,AttributeError,"AttributeError: 'SquadResult' object has no attribute 'cls_logits'"
5968,b'Loss becoming nearly zero in first 5K steps when training LM from scratch',2020-07-22T12:02:59Z,2020-10-10T08:14:16Z,wontfix,,
5967,b'Actually the extra_id are from 0-99 and not from 1-100',2020-07-22T11:39:24Z,2020-07-30T10:13:30Z,,,
5966,"b""Bug fix: NER pipeline shouldn't group separate entities of same type""",2020-07-22T11:16:19Z,2020-09-27T19:14:58Z,wontfix,,
5965,b'BerTweet tokenizer issue',2020-07-22T08:53:22Z,2020-10-31T08:13:49Z,"wontfix, Core: Modeling",OSError,"OSError: Model name 'vinai/bertweet-base' was not found in tokenizers model name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). We assumed 'vinai/bertweet-base' was a path, a model identifier, or url to a directory containing vocabulary files named ['vocab.json', 'merges.txt'] but couldn't find such vocabulary files at this path or url."
5964,b'text classification reuse without classifier',2020-07-22T08:39:41Z,2020-07-30T09:05:28Z,,,
5963,b'Bert forward reports error on GPU; but runs fine on CPU',2020-07-22T05:51:56Z,2020-07-31T13:55:33Z,,RuntimeError,"RuntimeError: CUDA error: device-side assert triggered"
5962,b'tensorflow\xe8\xbd\xac\xe4\xb8\xbapytorch\xe7\x9a\x84\xe4\xb8\xa4\xe4\xb8\xaa\xe6\x96\x87\xe4\xbb\xb6\xe5\x9c\xa8\xe5\x93\xaa\xe9\x87\x8c\xef\xbc\x9f',2020-07-22T03:47:23Z,2020-07-30T08:50:49Z,,,
5961,b'[docs] Add integration test example to copy pasta template',2020-07-22T03:23:05Z,2020-07-22T16:48:39Z,,,
5960,b'Adding Minimal Reproducible Usage Example For TPU support on examples/seq2seq',2020-07-22T02:26:14Z,2020-09-26T17:54:11Z,wontfix,,
5959,"b""Can't load weights of models""",2020-07-22T02:21:25Z,2020-07-23T14:38:37Z,,OSError,"OSError: Can't load weights for 'bert-base-uncased'. Make sure that:"
5958,b'Add functioning early stopping (patience) and weighted random sampling',2020-07-22T01:59:26Z,2020-12-04T22:07:21Z,wontfix,,
5957,b'NoneType error when using Trainer',2020-07-22T01:51:38Z,2020-07-23T21:00:12Z,,TypeError,"TypeError: 'NoneType' object is not subscriptable"
5956,b'[CI] Install examples/requirements.txt',2020-07-22T00:58:52Z,2020-07-22T01:07:48Z,,,
5955,"b""module 'tensorflow_core._api.v2.config' has no attribute 'list_physical_devices'""",2020-07-21T23:48:55Z,2020-09-27T07:40:45Z,wontfix,,
5954,"b""[pack_dataset] don't sort before packing, only pack train""",2020-07-21T21:03:39Z,2020-07-27T16:14:23Z,,,
5953,b'CL util to convert models to fp16 before upload',2020-07-21T20:16:37Z,2020-07-27T16:21:25Z,,,
5952,b'Create README.md',2020-07-21T19:43:20Z,2020-07-24T18:12:15Z,model card,,
5951,b'Create README.md',2020-07-21T19:34:39Z,2020-07-24T18:12:10Z,model card,,
5950,b'seq2seq: checkpoint callback seems messed up',2020-07-21T19:24:02Z,2020-07-21T20:28:16Z,Help wanted,,
5949,b'seq2seq/run_eval.py can take decoder_start_token_id',2020-07-21T19:11:20Z,2020-07-21T20:58:45Z,,,
5948,b'Exporting T5 to ONNX',2020-07-21T19:07:18Z,2021-06-04T15:19:26Z,,,
5947,b'Test on a new string of gpt2 fine tuned',2020-07-21T18:59:16Z,2020-07-30T08:55:56Z,,,
5946,b'Update doc to new model outputs',2020-07-21T18:38:32Z,2020-07-21T22:13:56Z,,,
5945,b'consistently use True/False for `return_tuple`',2020-07-21T18:22:00Z,2020-07-23T00:34:08Z,,,
5944,b'process stuck at LineByLineTextDataset. training not starting',2020-07-21T17:06:31Z,2020-07-30T08:19:57Z,,,
5943,b'[Doc] explaining romanian postprocessing for MBART BLEU hacking',2020-07-21T16:22:32Z,2020-07-21T18:12:49Z,,,
5942,b'Converting GPT2 logits to token ids directly',2020-07-21T15:58:13Z,2020-08-18T06:40:11Z,,,
5941,b'a transparent solution for  DataParallel.gather not supporting ModelOutput (dataclass)',2020-07-21T15:54:25Z,2020-07-29T20:38:32Z,,,
5940,b'What is the difference between the function of add_tokens() and add_special_tokens() in tokenizer',2020-07-21T15:29:14Z,2020-07-31T00:45:43Z,,,
5939,"b""Can't use BatchEncoding in the fit function""",2020-07-21T15:20:49Z,2020-09-26T23:57:09Z,wontfix,ValueError,"ValueError: too many values to unpack (expected 2)"
5938,b'How does AdamW weight_decay works for L2 regularization?',2020-07-21T15:09:38Z,2020-07-21T16:50:45Z,,,
5937,b'typos in seq2seq/readme',2020-07-21T13:01:33Z,2020-07-21T13:45:00Z,,,
5936,b'Easy selection of a learning rate scheduler when initializing a Trainer',2020-07-21T12:59:11Z,2020-09-26T23:57:13Z,wontfix,,
5935,"b'Getting ""AttributeError: \'Tensor\' object has no attribute \'numpy\'"" while fine-tuning BERT for NER'",2020-07-21T12:55:19Z,2020-07-22T11:58:54Z,,,
5934,b'InvalidArgumentError: Cannot convert a Tensor of dtype resource to a NumPy array.',2020-07-21T10:32:13Z,2020-09-21T06:08:17Z,wontfix,`InvalidArgumentError,"`InvalidArgumentError: Cannot convert a Tensor of dtype resource to a NumPy array.`"
5933,b'How to get a language model score in BertModel?',2020-07-21T09:22:46Z,2020-09-26T23:57:12Z,wontfix,,
5932,b'Expose padding_strategy on squad processor to fix QA pipeline performance regression',2020-07-21T09:17:12Z,2020-07-22T14:11:58Z,,,
5931,b'ALBERT tokenizer is not callable',2020-07-21T06:54:37Z,2020-07-23T13:39:20Z,,,
5930,"b""code copy button on the website doesn't copy `...` lines""",2020-07-21T04:56:02Z,2020-10-01T07:08:10Z,wontfix,,
5929,b'Add DeBERTa model',2020-07-21T04:49:55Z,2020-09-30T11:07:30Z,"model card, New model",,
5928,b'Feed forward chunking for all pretrained models',2020-07-21T01:27:10Z,2020-10-01T07:08:12Z,wontfix,,
5927,b'[CI] self-scheduled runner tests examples/',2020-07-21T00:51:45Z,2020-07-21T21:01:08Z,,,
5926,b'DataParallel fix: multi gpu evaluation',2020-07-20T20:43:10Z,2020-07-20T21:54:09Z,,,
5925,b'Allow user to see actual error if a download has failed ',2020-07-20T19:11:49Z,2020-10-04T01:13:40Z,wontfix,,
5924,b'Create README.md',2020-07-20T18:36:47Z,2020-07-21T07:41:38Z,model card,,
5923,b'how (if at all) are those models related...',2020-07-20T18:27:14Z,2020-07-20T23:13:37Z,,,
5922,b'Avoid unnecessary warnings when loading pretrained model',2020-07-20T18:08:47Z,2020-07-23T22:13:36Z,,,
5921,b'Create README.md',2020-07-20T18:08:07Z,2020-07-21T07:52:53Z,model card,,
5920,b'Create README.md',2020-07-20T18:00:30Z,2020-07-21T07:31:43Z,model card,,
5919,b'[examples/seq2seq]: add --label_smoothing option',2020-07-20T17:55:22Z,2020-07-21T20:51:40Z,,,
5918,b'Add Fast Transformers - Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention',2020-07-20T16:44:56Z,2021-01-03T13:23:29Z,"wontfix, New model",,
5917,b'convert_roberta: AttributeError when converting CamemBERT model.pt to pytorch_model.bin',2020-07-20T16:43:35Z,2020-08-05T11:23:56Z,,AttributeError,"AttributeError: 'RobertaModel' object has no attribute 'decoder'"
5916,b'Clarify arg class',2020-07-20T16:35:26Z,2020-07-20T23:47:07Z,,,
5915,b'Incompatible tensor type when running BART on TPU',2020-07-20T16:07:32Z,2020-07-21T14:49:12Z,,RuntimeError,"RuntimeError: Attempted to call `variable.set_data(tensor)`, but `variable` and `tensor` have incompatible tensor type."
5914,b'Add AlbertForPretraining to doc',2020-07-20T16:06:18Z,2020-07-20T21:53:22Z,,,
5913,b'[Fix] seq2seq pack_dataset.py actually packs',2020-07-20T15:52:37Z,2020-07-20T19:18:26Z,,,
5912,b'Improve doc of use_cache',2020-07-20T15:17:32Z,2020-07-20T15:50:42Z,,,
5911,b'[WIP] Add Pegasus',2020-07-20T14:18:39Z,2020-10-17T19:59:26Z,"wontfix, Summarization",,
5910,b'QA Pipeline: Key Error due to predicting a token in question',2020-07-20T12:34:04Z,2020-11-01T03:10:14Z,"wontfix, Core: Pipeline",KeyError,"KeyError: 3"
5909,b'Make Tokenizers Faster When There Are Many Additional Special Tokens',2020-07-20T11:42:01Z,2020-09-26T10:43:48Z,wontfix,,
5908,"b""ImportError: cannot import name 'DataCollatorForPermutationLanguageModeling'""",2020-07-20T11:22:20Z,2020-07-20T12:12:35Z,,ImportError,"ImportError: cannot import name 'DataCollatorForPermutationLanguageModeling'"
5907,"b""ModuleNotFoundError: No module named 'torch_xla'""",2020-07-20T10:32:26Z,2020-07-29T19:27:26Z,,,
5906,b'Word frequencies in TransfoXLTokenizer',2020-07-20T10:28:33Z,2020-09-26T10:44:25Z,wontfix,,
5905,b'Retrain/reuse fine-tuned models on a different set of labels',2020-07-20T09:44:24Z,2020-11-22T08:04:30Z,wontfix,RuntimeError,"RuntimeError: Error(s) in loading state_dict for BertForTokenClassification:"
5904,b'RobertaTokenizerFast unexpectedly quits when creating a TextDataset',2020-07-20T07:36:22Z,2020-10-04T08:13:39Z,wontfix,,
5903,b'[WIP] Add Theseus Compression',2020-07-20T07:22:25Z,2021-06-08T17:26:58Z,WIP,,
5902,b'\xf0\x9f\x90\x9b BART : Same representations for different `<s>` tokens ',2020-07-20T05:30:12Z,2020-09-26T10:44:25Z,"Help wanted, wontfix",,
5901,b'How can I check the loss during pretraing huggingface/transformers',2020-07-20T03:09:13Z,2020-07-28T16:44:26Z,trainer,,
5900,b'Is there any api for intermediate layer outputs?',2020-07-20T02:42:37Z,2020-07-21T07:16:32Z,,,
5899,b'Smaller output vocabulary for GPT-2',2020-07-20T02:03:02Z,2020-09-26T10:44:23Z,wontfix,,
5898,b'Fix pack_dataset.py',2020-07-20T00:29:06Z,2020-07-20T19:18:26Z,,,
5897,b'examples/seq2seq: add label_smoothing cross entropy option',2020-07-20T00:28:20Z,2020-07-21T20:51:40Z,,,
5896,b'MT: automate/experiment with pruning embeddings',2020-07-20T00:27:42Z,2020-09-26T10:44:22Z,wontfix,,
5895,b'examples/seq2seq/finetune.py and BART supports TPU',2020-07-20T00:26:12Z,2020-09-27T07:40:45Z,"Help wanted, wontfix",,
5894,b'fix typo in training_args_tf.py',2020-07-19T21:08:11Z,2020-07-20T07:48:22Z,,,
5893,b'fix typo in training_args.py',2020-07-19T21:08:05Z,2020-07-20T07:53:03Z,,,
5892,b'Benchmark: traceback does not describe real problem',2020-07-19T19:07:04Z,2020-09-26T10:44:20Z,wontfix,ValueError,"ValueError: too many values to unpack (expected 2)"
5891,b'[WIP] Fix mbart benchmark',2020-07-19T18:50:38Z,2020-08-17T13:11:20Z,,,
5890,b'How to finetune distillbart from distilbart-cnn-12-6 checkpoint using cnn_daily mail or gigawords dataset?',2020-07-19T14:53:43Z,2020-09-26T10:44:26Z,wontfix,,
5889,"b""AttributeError: 'GPT2LMHeadModel' object has no attribute 'h'""",2020-07-19T07:17:44Z,2020-09-19T03:02:04Z,wontfix,,
5888,b'Reading transformer package from local codes and NOT the pip installed version',2020-07-19T03:28:24Z,2020-07-20T23:55:01Z,,ImportError,"ImportError: attempted relative import with no known parent package"
5887,b'Seq2Seq: same MultiGPU test failing twice!',2020-07-19T01:01:06Z,2020-10-21T21:20:54Z,,,
5886,b'DataCollatorForLanguageModeling - Shift labels for left-to-right LM?',2020-07-19T00:30:53Z,2020-07-19T13:30:42Z,,,
5885,b'feat: allow prefix for any generative model',2020-07-18T19:46:28Z,2020-09-07T07:03:46Z,,,
5884,b'Add ComVE model cards',2020-07-18T17:53:14Z,2020-07-21T16:54:30Z,model card,,
5883,b'Xlnet outputs',2020-07-18T15:16:09Z,2020-07-18T15:33:14Z,,,
5882,"b'Revert ""Xlnet outputs""'",2020-07-18T15:11:08Z,2020-07-18T15:15:41Z,,,
5881,b'Xlnet outputs',2020-07-18T14:41:12Z,2020-07-18T14:53:29Z,,,
5880,b'How to get probabilities from MarianMT models?',2020-07-18T13:23:30Z,2020-09-26T10:44:37Z,wontfix,,
5879,b'Create README.md',2020-07-18T13:03:38Z,2020-07-21T17:27:13Z,model card,,
5878,b'Create README.md',2020-07-18T13:02:29Z,2020-07-21T17:20:48Z,model card,,
5877,b'Create README.md',2020-07-18T13:01:07Z,2020-07-21T17:20:44Z,model card,,
5876,b'Create README.md',2020-07-18T12:59:19Z,2020-07-21T17:28:23Z,model card,,
5875,b'Create README.md',2020-07-18T12:57:57Z,2020-07-21T17:20:33Z,model card,,
5874,b'Create README.md',2020-07-18T12:55:40Z,2020-07-21T17:27:31Z,model card,,
5873,b'Create README.md',2020-07-18T12:53:49Z,2020-07-21T17:28:07Z,model card,,
5872,b'Create README.md',2020-07-18T12:51:52Z,2020-07-21T17:20:22Z,model card,,
5871,b'Create README.md',2020-07-18T12:49:47Z,2020-07-21T17:19:49Z,model card,,
5870,b'[cleanup] Less aggressive warnings about checkpoint mismatches',2020-07-18T12:16:08Z,2020-07-21T00:53:33Z,,,
5869,b'Silenced error while downloading pretrained model',2020-07-18T11:57:30Z,2020-09-26T10:44:24Z,wontfix,,
5868,b'[cleanup] squad processor',2020-07-18T11:48:00Z,2020-07-20T14:44:11Z,,,
5867,b'Update tokenizers to 0.8.1.rc to fix Mac OS X issues',2020-07-18T11:43:24Z,2020-07-18T12:20:12Z,,,
5866,b'T5Tokenizer adds EOS token if not already added',2020-07-18T11:29:53Z,2020-08-25T18:56:09Z,,,
5865,b'[seq2seq] distillation.py accepts trainer arguments',2020-07-18T11:21:35Z,2020-07-18T11:43:58Z,,,
5864,b'Create README.md',2020-07-18T11:02:11Z,2020-07-21T17:15:07Z,model card,,
5863,b'not able to reproduce accuracy at the end of same epoch',2020-07-18T08:33:02Z,2020-10-18T05:15:08Z,wontfix,,
5862,b'Potential security vulnerability regarding Hosted Interface API?',2020-07-18T05:51:36Z,2020-07-18T07:43:50Z,,,
5861,b'[seq2seq] add back clargs',2020-07-18T04:05:36Z,2020-07-18T11:44:30Z,,,
5860,b'issue with loading pretrained model - xlnet',2020-07-18T03:59:12Z,2020-07-18T20:07:31Z,,,
5859,b'[seq2seq] organize commands into scripts/ subdir',2020-07-18T02:48:39Z,2020-09-26T10:44:35Z,wontfix,,
5858,b'wrong args name: n_gpu -> gpus',2020-07-18T02:32:38Z,2020-07-18T02:44:28Z,,KeyError,"KeyError: 'n_gpu'"
5857,b'Update README.md',2020-07-17T22:15:12Z,2020-07-21T17:14:28Z,model card,,
5856,b'Update README.md',2020-07-17T22:13:46Z,2020-07-21T17:11:08Z,model card,,
5855,b'docs: fix model sharing file names',2020-07-17T21:34:18Z,2020-09-28T12:17:31Z,,,
5854,"b'Revert ""XLNet `use_cache` refactor""'",2020-07-17T18:33:37Z,2020-07-17T18:33:45Z,,,
5853,"b""[BartModel] Question for BartModel Output Shape when I pass the 'decoder_input_ids'""",2020-07-17T16:58:09Z,2020-09-26T10:44:34Z,wontfix,,
5852,"b""Exception in device=TPU:1: 'ascii' codec can't decode byte 0xc2 in position 37: ordinal not in range(128)""",2020-07-17T16:51:49Z,2020-07-19T10:20:33Z,,,
5851,b'Covid-19 - TPU V3-1024 - T5 11B:  Tensorflow to Pytorch conversion failed',2020-07-17T16:37:47Z,2020-08-19T12:23:46Z,,AttributeError,"AttributeError: 'T5LayerSelfAttention' object has no attribute 'weight'"
5850,b'pip install error',2020-07-17T16:22:49Z,2020-09-07T05:14:17Z,,,
5849,b'Update shortcut name for reformer in pretrained_models.srt',2020-07-17T16:15:37Z,2020-07-26T06:52:57Z,,,
5848,"b""AttributeError: type object 'BertConfig' has no attribute 'pretrained_config_archive_map'""",2020-07-17T16:05:04Z,2020-07-17T20:02:02Z,,,
5847,b'Create README.md',2020-07-17T15:26:58Z,2020-07-17T18:03:54Z,model card,,
5846,b'Decode [UNK] from tokenizer',2020-07-17T15:06:41Z,2020-10-11T11:26:07Z,wontfix,,
5845,b'Added model card for neuraly/bert-base-italian-cased-sentiment',2020-07-17T12:57:32Z,2020-07-17T17:50:50Z,model card,,
5844,b'problem about Custom class inheriting from <TFBertPreTrainedModel>',2020-07-17T12:16:39Z,2020-09-26T10:44:32Z,wontfix,,
5843,b'Added model card for neuraly/bert-base-italian-cased-sentiment',2020-07-17T10:23:07Z,2020-07-17T11:51:06Z,model card,,
5842,"b""ImportError: cannot import name 'BERT_PRETRAINED_MODEL_ARCHIVE_MAP' from 'transformers'""",2020-07-17T10:01:54Z,2020-07-17T14:55:16Z,,,
5841,b'[Model card] Bert2Bert',2020-07-17T09:24:38Z,2020-07-17T09:41:57Z,model card,,
5840,"b""[WIP - Don't merge][EncoderDecoder] Extend Trainer for Bert2Bert""",2020-07-17T09:20:09Z,2020-09-26T10:43:50Z,wontfix,,
5839,b'Created model card for my extreme summarization model',2020-07-17T08:53:22Z,2020-07-21T07:54:58Z,model card,,
5838,b'Created model card for my summarization model',2020-07-17T08:25:39Z,2020-07-21T07:54:15Z,model card,,
5837,b'[seq2seq] MAX_LEN env var for MT commands',2020-07-17T07:06:39Z,2020-07-18T02:51:32Z,,,
5836,b'[not sure whether to] pin torch<=1.5.1',2020-07-17T05:39:02Z,2020-07-17T19:01:57Z,,,
5835,b'solve Illegal seek in wandb teardown (test suite)',2020-07-17T05:25:30Z,2020-09-09T19:52:07Z,,,
5834,b'Trainer support for iterabledataset',2020-07-17T05:21:33Z,2020-07-20T13:07:38Z,,,
5833,b'OpenAI GPT NoPaddingTokenFastTokenizerMatchingTest test fails',2020-07-17T04:41:47Z,2020-09-17T04:47:39Z,wontfix,,
5832,b'[wip] T5 tokenizer should add special tokens',2020-07-17T04:00:01Z,2020-07-18T11:22:09Z,,,
5831,b'minor doc fixes',2020-07-17T02:44:05Z,2020-07-22T17:22:34Z,,,
5830,b'[cleanups] make Marian save as Marian',2020-07-17T01:28:30Z,2020-07-17T06:54:26Z,,,
5829,"b'ValueError: DataLoader with IterableDataset: expected unspecified sampler option,'",2020-07-16T23:03:08Z,2020-07-30T09:46:44Z,,ValueError,"ValueError: DataLoader with IterableDataset: expected unspecified sampler option, but got sampler=<torch.utils.data.sampler.RandomSampler object at 0x7fff5803b2b0>"
5828,b'language tag addition on albert-mongolian',2020-07-16T22:10:19Z,2020-07-17T05:40:38Z,model card,,
5827,b'Reproducibility when using pretrained GPT2',2020-07-16T20:25:57Z,2020-09-26T10:44:36Z,"wontfix, gpt2",,
5826,b'Vocab size mismatch on EncoderDecoder model from_pretrained',2020-07-16T20:22:06Z,2020-07-24T08:50:08Z,,RuntimeError,"RuntimeError: Error(s) in loading state_dict for EncoderDecoderModel:"
5825,b'Add inference widget examples',2020-07-16T20:00:19Z,2020-07-28T13:14:01Z,,,
5824,b'New Community NB Add',2020-07-16T19:03:05Z,2020-07-28T08:25:13Z,,,
5823,b'Add model card for dv-wave',2020-07-16T19:02:23Z,2020-07-16T19:13:51Z,model card,,
5822,b'[seq2seq] test that finetune takes < 7 seconds',2020-07-16T18:19:26Z,2020-07-27T15:46:46Z,,,
5821,b'Create README.md',2020-07-16T18:03:42Z,2020-07-16T19:18:32Z,model card,,
5820,b'Migrate Marian models names to ISO-639-3 where possible',2020-07-16T17:58:35Z,2020-09-24T04:15:50Z,"wontfix, marian",,
5819,b'[seq2seq] pack_dataset.py rewrites dataset in max_tokens format',2020-07-16T17:18:16Z,2020-07-16T18:06:50Z,,,
5818,"b""[seq2seq] Don't copy self.source in sortishsampler""",2020-07-16T16:39:09Z,2020-07-17T05:53:25Z,,,
5817,"b'error with transformers 2.9.1 but not with 2.3.0, same code, why?'",2020-07-16T16:24:46Z,2020-07-28T08:03:43Z,,,
5816,b'Additional layers to BERT ',2020-07-16T16:05:30Z,2020-09-26T10:44:31Z,wontfix,,
5815,b'cast_bool_to_primitive breaks TensorFlow graph support.',2020-07-16T16:03:17Z,2020-10-10T08:14:14Z,wontfix,ValueError,"ValueError: not enough values to unpack (expected 2, got 1)"
5814,b'How to download original weights of gpt2',2020-07-16T14:50:30Z,2020-09-17T07:09:23Z,wontfix,,
5813,b'Create README.md',2020-07-16T14:06:11Z,2020-07-20T08:06:43Z,model card,,
5812,b'Update README.md',2020-07-16T13:59:04Z,2020-07-16T14:25:51Z,model card,,
5811,b'[Longformer] fix longformer slow-down',2020-07-16T13:49:56Z,2020-07-16T14:19:38Z,,,
5810,b'Add missing arguments for BertWordPieceTokenizer',2020-07-16T13:46:30Z,2020-09-07T12:35:41Z,,,
5809,"b""TypeError: forward() got an unexpected keyword argument 'head_mask'""",2020-07-16T13:09:06Z,2020-07-16T14:45:11Z,,TypeError,TypeError: forward() got an unexpected keyword argument 'head_mask'`
5808,b'[Benchmark] Fix models without `architectures` param in config',2020-07-16T13:03:39Z,2020-07-16T13:15:11Z,,,
5807,"b'While running finetune.py in seq2seq examples on a custom dataset, I am getting the following error. '",2020-07-16T12:58:56Z,2020-10-04T01:14:08Z,wontfix,RuntimeError,"RuntimeError: cannot reshape tensor of 0 elements into shape [-1, 0] because the unspecified dimension size -1 can be any value and is ambiguous"
5806,b'BERT-viewer is broken for russian?',2020-07-16T12:11:11Z,2020-09-24T04:15:51Z,wontfix,,
5805,b'[Fix] Checkpoint saving and I/O for XLNetQASimple ',2020-07-16T11:51:16Z,2020-09-26T10:43:51Z,wontfix,,
5804,b'Add MPNet',2020-07-16T11:41:39Z,2020-11-25T16:17:29Z,,,
5803,"b""Hosted Inference API: Error loading tokenizer Can't load config""",2020-07-16T11:13:52Z,2020-07-26T16:42:37Z,,,
5802,b'[WIP - Benchmark] Add generate function',2020-07-16T10:31:01Z,2020-09-24T04:15:18Z,wontfix,,
5801,b'[Benchmark] fix benchmark non standard model',2020-07-16T10:02:42Z,2020-07-16T10:13:11Z,,,
5800,"b""GPT2 weights don't initialize from checkpoint""",2020-07-16T09:29:03Z,2020-07-16T15:08:42Z,,,
5799,b'Issue when load pretrained weights',2020-07-16T07:43:38Z,2020-10-18T21:11:11Z,wontfix,OSError,"OSError: Can't load weights for 'bert-base-chinese'. Make sure that:"
5798,b'Lightning Updates for v0.8.5',2020-07-16T06:04:51Z,2020-07-18T02:43:07Z,,,
5797,b'Can I use the pretrained BERT-Base model directly for predict isNextSentence task?',2020-07-16T06:02:14Z,2020-07-17T04:23:21Z,,,
5796,b'Moving \x0ctransformers package import statements to relative imports in some files',2020-07-16T05:25:19Z,2020-07-28T08:19:17Z,,,
5795,b'LongFormerAttention For AutoRegressive Models',2020-07-16T04:05:34Z,2020-07-16T15:13:40Z,,,
5794,b'Print all next tokens of a sentence over a certain probability threshold.',2020-07-16T02:38:30Z,2020-07-16T15:15:22Z,,,
5793,"b""Adding the LXMERT pretraining model (MultiModal  languageXvision)  to HuggingFace's suite of models""",2020-07-16T02:01:34Z,2020-09-03T08:02:26Z,,,
5792,b'Seq2SeqDataset uses linecache to save memory by @Pradhy729 (#5792) ',2020-07-16T01:23:45Z,2020-07-18T17:57:33Z,,,
5791,b'Add script to convert tf2.x checkpoint to PyTorch',2020-07-15T23:13:14Z,2020-08-03T07:53:39Z,,,
5790,b'github issue template suggests who to tag',2020-07-15T22:44:41Z,2020-07-28T12:41:28Z,,,
5789,b'Update README.md',2020-07-15T22:28:06Z,2020-07-16T09:26:17Z,model card,,
5788,"b'add attention_dropout, relu_dropout command line args to lightning_base.py'",2020-07-15T21:35:58Z,2020-07-27T19:19:07Z,lightning,,
5787,"b""Can't load weights for GPT2 error""",2020-07-15T20:20:44Z,2021-03-06T00:17:54Z,wontfix,OSError,"OSError: Can't load weights for 'gpt2'. Make sure that:"
5786,b'Faster mBART finetuning',2020-07-15T19:05:18Z,2020-09-23T22:50:03Z,"Help wanted, seq2seq, Examples",,
5785,b'Sentence-transformers model outputs different than when loaded in HuggingFace',2020-07-15T18:35:52Z,2020-07-15T18:39:29Z,,,
5784,b'[fix] Style. Trying again',2020-07-15T18:08:15Z,2020-07-15T18:10:09Z,,,
5783,b'Marian Conversion Script',2020-07-15T18:07:22Z,2020-07-29T01:36:20Z,marian,,
5782,b'Create README.md',2020-07-15T17:52:31Z,2020-07-15T20:17:20Z,model card,,
5781,b'Create README.md',2020-07-15T17:44:30Z,2020-07-15T20:17:26Z,model card,,
5780,b'Error in conversion to tensorflow',2020-07-15T17:20:28Z,2020-07-16T12:09:21Z,,ValueError,"ValueError: Expected floating point type, got <dtype: 'int32'>."
5779,b'[bart] decoder.last_hidden_state shape changes when passing labels',2020-07-15T16:25:25Z,2020-07-15T16:47:23Z,,,
5778,b'Error using DataParallel with reformer model: There were no tensor arguments to this function',2020-07-15T15:44:16Z,2020-10-18T05:15:13Z,wontfix,RuntimeError,"RuntimeError: Caught RuntimeError in replica 0 on device 0."
5777,b'Bug in MiniLM-L12-H384-uncased modelhub model files',2020-07-15T15:41:55Z,2020-07-20T18:28:18Z,,AssertionError,"AssertionError: tokenizer vocab_size 30521 doesn't match embedding vocab size 30522"
5776,b'Update README.md',2020-07-15T15:40:57Z,2020-07-15T20:19:22Z,model card,,
5775,b'[squad] make examples and dataset accessible from SquadDataset object',2020-07-15T15:13:59Z,2020-08-25T06:31:18Z,,,
5774,b'[fix] check_code_quality',2020-07-15T15:08:09Z,2020-07-15T18:09:57Z,,,
5773,b'Ensure OpenAI GPT position_ids is correctly initialized and registered at init.',2020-07-15T14:45:53Z,2020-07-24T13:37:52Z,,,
5772,b'[fix] check code quality',2020-07-15T14:37:15Z,2020-07-15T18:59:38Z,,,
5771,b'Fine-tune BERT for regression problem',2020-07-15T14:36:51Z,2020-09-20T19:00:02Z,wontfix,,
5770,b'XLNet `use_cache` refactor',2020-07-15T13:59:06Z,2020-07-17T18:24:17Z,,,
5769,b'[fix] T5 ONNX test: model.to(torch_device)',2020-07-15T12:37:44Z,2020-07-15T14:11:23Z,,,
5768,b'Confidence score prediction of pretrained models in extractive QA - similar to pipeline',2020-07-15T12:35:45Z,2020-09-20T18:00:03Z,wontfix,,
5767,b'How to get parameters from a Query.',2020-07-15T11:41:14Z,2020-09-28T10:40:17Z,wontfix,,
5766,"b""Hello,I have this problem in running 'run_glue.py'!""",2020-07-15T09:30:57Z,2020-07-28T08:26:33Z,,ImportError,"ImportError: cannot import name 'glue_compute_metrics' from 'transformers' (/home/wangbingchen/wangbingchen/anaconda3/lib/python3.7/site-packages/transformers/__init__.py)"
5765,"b'When using ""transformers.WarmUp"" with tensorflow 2.0.0, warming up restart in each epoch!'",2020-07-15T08:40:37Z,2020-07-31T03:58:23Z,,,
5764,b'TF Longformer',2020-07-15T08:11:21Z,2020-08-10T21:25:07Z,,,
5763,b'ADD ERNIE model',2020-07-15T07:17:58Z,2020-07-16T03:03:05Z,model card,,
5762,b'Feature request of Sparselty Gated Mixture-of-Experts and PowerNorm',2020-07-15T05:02:29Z,2020-11-14T09:27:03Z,wontfix,,
5761,"b'[cleanup] T5 test, warnings'",2020-07-15T02:37:22Z,2020-07-15T12:23:23Z,cleanup,,
5760,b'Zero shot classification pipeline',2020-07-14T23:46:14Z,2020-07-27T13:42:59Z,,,
5759,b'T5 Model Cards',2020-07-14T22:53:22Z,2020-07-22T15:38:37Z,model card,,
5758,b'metadata',2020-07-14T22:11:42Z,2020-07-15T20:13:29Z,model card,,
5757,b'BART/T5 eli5 in model hub',2020-07-14T21:58:45Z,2020-09-20T03:00:09Z,wontfix,,
5756,b'BART MNLI + yahoo answer in the model hub for inference API',2020-07-14T21:13:02Z,2020-09-20T03:00:10Z,wontfix,,
5755,b'Problems with generating text using mbart-large-cc25',2020-07-14T20:52:28Z,2021-04-26T23:35:21Z,wontfix,,
5754,"b""T5 fine-tuned model doesn't appear in the model hub""",2020-07-14T19:54:13Z,2020-07-15T20:51:07Z,,,
5753,"b""Can't load `facebook/mbart-large-cc25` tokenizer""",2020-07-14T19:15:11Z,2020-07-14T23:32:41Z,,OSError,"OSError: Model name 'facebook/mbart-large-cc25' was not found in tokenizers model name list (facebook/mbart-large-en-ro, sshleifer/mbart-large-cc25). We assumed 'facebook/mbart-large-cc25' was a path, a model identifier, or url to a directory containing vocabulary files named ['sentencepiece.bpe.model'] but couldn't find such vocabulary files at this path or url."
5752,b'Update README.md',2020-07-14T19:02:12Z,2020-07-14T19:49:00Z,model card,,
5751,b'tiny ppl typo fix',2020-07-14T16:34:43Z,2020-07-14T16:39:45Z,,,
5750,b'fail to run trainer.train() with huggingface transformer',2020-07-14T15:56:47Z,2020-09-19T18:56:46Z,wontfix,,
5749,b'Reintroduce clean_text on BertTokenizer call which was removed by mistake in #4723',2020-07-14T14:22:48Z,2020-10-09T12:07:29Z,,,
5748,b'Long BERT TypeError: forward() takes from 2 to 4 positional arguments but 7 were given',2020-07-14T14:15:00Z,2020-10-18T05:15:18Z,wontfix,TypeError,"TypeError: forward() takes from 2 to 4 positional arguments but 7 were given"
5747,"b"" Unrecognized configuration class <class 'transformers.configuration_electra.ElectraConfig'>""",2020-07-14T14:11:40Z,2020-07-27T10:04:01Z,,,
5746,b'Where can I find raw code for char_to_token function.',2020-07-14T13:56:52Z,2020-07-14T14:00:28Z,,,
5745,b'google/reformer-enwik8 tokenizer was not found in tokenizers model name list',2020-07-14T13:52:38Z,2020-09-20T21:00:03Z,wontfix,,
5744,b'Create README.md for the model card of GPorTuguese-2 model (Portuguese GPT-2 small)',2020-07-14T13:18:27Z,2020-07-14T14:48:53Z,model card,,
5743,b'Customize inference widget input',2020-07-14T11:07:11Z,2020-07-14T14:58:46Z,model card,,
5742,b'How to use pytorch_model.bin to classify a single sentence?',2020-07-14T10:17:17Z,2020-07-16T15:29:39Z,,,
5741,b'FileNotFoundError: File not found when running run_squad.py to fine-tune the BERT on  SQuAD v1.1.',2020-07-14T10:08:44Z,2020-09-30T19:42:54Z,wontfix,,
5740,b'[ModelOutput] Proposal to fix compatibility issue with torch.DataParallel',2020-07-14T10:04:59Z,2020-07-29T20:39:51Z,,,
5739,"b""TypeError: join() argument must be str or bytes, not 'NoneType'""",2020-07-14T09:25:32Z,2020-09-19T14:29:13Z,wontfix,TypeError,"TypeError: join() argument must be str or bytes, not 'NoneType'"
5738,b'Unicode normalization for bert-cased models',2020-07-14T09:16:26Z,2020-09-19T14:29:12Z,wontfix,,
5737,b'Update model_summary.rst',2020-07-14T09:00:02Z,2020-07-27T09:34:03Z,,,
5736,b'TypeError: an integer is required (got type NoneType) while using  run_language_modeling.py',2020-07-14T08:41:43Z,2020-07-15T07:06:20Z,,TypeError,"TypeError: an integer is required (got type NoneType)"
5735,b'Create README.md (Model card for Norod78/hewiki-articles-distilGPT2py-il)',2020-07-14T08:36:10Z,2020-07-14T14:50:45Z,model card,,
5734,b'Fix typo (model saving TF)',2020-07-14T05:44:23Z,2020-07-27T14:55:16Z,,,
5733,b'DataParallel fixes',2020-07-14T04:51:21Z,2020-07-20T13:29:12Z,,,
5732,b'Add `power` argument for TF PolynomialDecay',2020-07-14T04:24:15Z,2020-10-05T09:16:30Z,,,
5731,b'[fix] mbart_en_ro_generate test now identical to fairseq',2020-07-14T03:57:35Z,2020-07-14T10:12:24Z,,,
5730,"b""Using pipeline('ner'), partial tokens returned when grouped_entities=True""",2020-07-14T02:34:55Z,2020-09-19T14:29:17Z,wontfix,,
5729,b'[Feature request] Pass any Iterable to tokenizer.__call__()',2020-07-14T02:33:17Z,2020-09-26T10:44:33Z,wontfix,,
5728,b'Return tokens from tokenizer.__call__()',2020-07-14T01:33:32Z,2020-07-14T20:32:47Z,,,
5727,b't5 model card',2020-07-14T01:30:48Z,2020-07-22T15:38:37Z,Documentation,,
5726,b'Finetuning GPT2 with Custom Loss',2020-07-14T00:08:17Z,2020-07-14T07:44:18Z,,,
5725,b'TPU CI testing',2020-07-13T22:02:45Z,2020-07-30T20:10:20Z,,,
5724,b'T5 ONNX Export Test Failing on GPU',2020-07-13T21:15:56Z,2020-07-15T14:11:22Z,,,
5723,b'Fix slow test_enro_generate',2020-07-13T21:13:38Z,2020-07-14T10:47:27Z,,,
5722,"b""Cannot preprocess WNUT'17 dataset for token-classification""",2020-07-13T20:31:45Z,2020-10-11T11:26:08Z,wontfix,,
5721,b'Unable to finetune BERT on own dataset',2020-07-13T19:06:49Z,2020-09-19T14:29:17Z,wontfix,,
5720,"b""TypeError: To be compatible with tf.contrib.eager.defun, Python functions must return zero or more Tensors; in compilation of <function tf_if_stmt.<locals>.error_checking_body at 0x7f55400e3c80>, found return value of type <class 'tensorflow.python.keras.losses.MeanSquaredError'>, which is not a Tensor.""",2020-07-13T19:01:11Z,2020-09-19T03:01:27Z,wontfix,,
5719,"b'generator` yielded an element that could not be converted to the expected type. The expected type was int32, but the yielded element was None.'",2020-07-13T18:38:01Z,2020-09-19T03:01:25Z,wontfix,,
5718,"b""[Don't merge - Bert2Bert] Add training scripts and slight changes to Trainer""",2020-07-13T18:32:59Z,2020-07-17T14:22:01Z,,,
5717,b'Update tokenization_t5.py',2020-07-13T18:21:18Z,2020-07-14T04:02:04Z,,,
5716,b'Add generic text classification example in TF',2020-07-13T15:56:55Z,2020-09-22T16:05:06Z,,,
5715,b'Extending vocabulary by a large size crashes RobertaTokenizerFast',2020-07-13T15:34:15Z,2020-09-19T18:56:47Z,wontfix,,
5714,b'facebook/bart-large-mnli input format',2020-07-13T15:20:17Z,2020-09-24T21:17:19Z,wontfix,,
5713,b'ONNX export broken for QA models',2020-07-13T15:16:57Z,2020-07-28T07:14:31Z,,,
5712,b'How to download Pre-trained T5 model?',2020-07-13T14:02:27Z,2020-08-21T11:54:33Z,,,
5711,b'QA Pipeline: Key Error due to predicting a token outside of allowed context',2020-07-13T13:38:04Z,2020-07-18T10:36:30Z,,KeyError,"KeyError: 13"
5710,b'Attention heads attend equally after conversion from tensorflow checkpoint',2020-07-13T13:04:43Z,2020-09-19T03:01:29Z,wontfix,,
5709,b'Run Language Modeling on Colab TPU cores terminates',2020-07-13T09:31:01Z,2020-07-25T22:29:35Z,,"TypeError, Exception","TypeError: 'NoneType' object cannot be interpreted as an integerException: process 0 terminated with signal SIGKILL"
5708,"b'For Roberta pretraining, how to enable large batch training using gradient accumulation?'",2020-07-13T08:50:27Z,2020-07-27T08:46:56Z,,,
5707,b'Span Mask Fill',2020-07-13T06:33:20Z,2020-09-19T03:01:26Z,wontfix,,
5706,"b""can't resume training from a saved checkpoint in run_glue""",2020-07-13T06:17:35Z,2020-10-25T00:40:35Z,wontfix,"""OSError","""OSError: Model name 'models/tmp/roberta512/checkpoint-27000' was not found in tokenizers model name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). We assumed 'models/tmp/roberta512/checkpoint-27000' was a path, a model identifier, or url to a directory containing vocabulary files named ['vocab.json', 'merges.txt'] but couldn't find such vocabulary files at this path or url."""
5705,b'Any insight to this mystery issue? Using the Keras functional API results in whole deleted weights/layers for transformer layers. ',2020-07-13T05:49:53Z,2020-09-19T03:01:32Z,wontfix,,
5704,b'Make the order of additional special tokens deterministic',2020-07-13T05:40:38Z,2020-08-04T06:38:31Z,,,
5703,b'Make the order of additional special tokens deterministic',2020-07-13T05:23:20Z,2020-07-13T05:36:56Z,,,
5702,"b""help\xef\xbc\x9aOSError: Model name 'ctrl' was not found in tokenizers model name list (ctrl). We assumed 'ctrl' was a path, a model identifier, or url to a directory containing vocabulary files named ['vocab.json', 'merges.txt'] but couldn't find such vocabulary files at this path or url.""",2020-07-13T03:34:57Z,2020-07-28T07:20:50Z,,,
5701,"b""How to generate sentences from Transformer's sentence embeddings?""",2020-07-13T00:30:06Z,2020-09-19T03:01:28Z,wontfix,,
5700,b'How to visualize the output of the encoder using T-sne plots? ',2020-07-12T22:57:23Z,2020-09-19T03:01:45Z,wontfix,,
5699,b'Add beta 1 and beta 2 option in `TrainingArguments` for `AdamW` optimizer.',2020-07-12T19:04:31Z,2020-07-14T08:06:56Z,,,
5698,b'Create README.md',2020-07-12T13:40:08Z,2020-07-14T14:51:54Z,model card,,
5697,b'How can I evaluate on GLUE but without fine-tune BERT. Just train the rest layers?',2020-07-12T09:56:25Z,2020-09-19T03:01:44Z,wontfix,,
5696,b'Update README.md',2020-07-12T09:36:15Z,2020-07-14T16:51:58Z,model card,,
5695,b'Update README.md',2020-07-12T09:25:38Z,2020-07-14T17:02:05Z,model card,,
5694,"b""[Don't merge] Run make style on templates""",2020-07-12T08:14:02Z,2020-07-12T08:33:54Z,,,
5693,"b""__init__() missing 1 required positional argument: 'logits'""",2020-07-12T07:12:05Z,2020-09-26T23:57:11Z,wontfix,TypeError,"TypeError: __init__() missing 1 required positional argument: 'logits'"
5692,b'rename the functions to match the rest of the test convention',2020-07-12T07:05:37Z,2020-07-13T10:09:50Z,,,
5691,b'Cannot import EvalPrediction from transformers',2020-07-12T06:14:06Z,2020-07-12T06:36:20Z,,,
5690,"b'How I can  predict missing letters in a sentence, like "" I want to b _ _  the car because it is cheap.""'",2020-07-12T01:22:02Z,2020-07-27T09:43:08Z,,,
5689,b'Is Writing With Transform open source?',2020-07-11T22:36:58Z,2020-09-15T09:47:41Z,wontfix,,
5688,b'doc improvements',2020-07-11T19:21:08Z,2020-07-13T10:10:17Z,,,
5687,b'Making ONNX conversion directly load the model and tokenizer + adding tests',2020-07-11T17:16:42Z,2020-10-25T00:40:18Z,wontfix,,
5686,b'[Fix] github actions CI by reverting #5138',2020-07-11T14:53:03Z,2020-07-13T21:12:19Z,,,
5685,b'Fix Trainer in DataParallel setting',2020-07-11T12:27:33Z,2020-07-13T12:37:39Z,,,
5684,b'fix incorrect docstring on bart summarization example',2020-07-11T12:07:13Z,2020-07-13T16:35:10Z,,,
5683,"b""Add Microsoft's CodeBERT""",2020-07-11T12:04:18Z,2020-07-11T13:37:31Z,model card,,
5682,b'What is the decoder_input for encoder-decoder transformer in training time?',2020-07-11T10:48:07Z,2020-07-12T02:18:53Z,,,
5681,b'[pipelines] Update fill mask pipeline to remove special tokens in the output',2020-07-11T10:08:26Z,2020-10-04T01:13:42Z,wontfix,,
5680,b'How to produce customized attention mask for BertModel?',2020-07-11T09:05:48Z,2020-09-19T03:01:42Z,wontfix,,
5679,b'Pipeline model type check',2020-07-11T08:27:33Z,2020-07-12T04:34:22Z,,,
5678,b'Weird output when using unexpected model type for pipelines',2020-07-11T06:52:41Z,2020-07-12T04:34:35Z,,,
5677,b'[WIP] Added indexes in grouped entity NER',2020-07-11T04:35:20Z,2020-10-04T08:13:35Z,wontfix,,
5676,b'Add indexes to grouped entity NER pipeline',2020-07-11T03:51:52Z,2020-11-22T08:04:40Z,wontfix,,
5675,b'Deepset model not loading using default code',2020-07-10T23:57:23Z,2020-07-14T01:26:35Z,,OSError,"OSError: Model name 'deepset/bert-large-uncased-whole-word-masking-squad2' was not found in tokenizers model name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). We assumed 'deepset/bert-large-uncased-whole-word-masking-squad2' was a path or url to a directory containing vocabulary files named ['vocab.txt'] but couldn't find such vocabulary files at this path or url."
5674,"b""Can't get BART to generate EOS token.""",2020-07-10T21:40:42Z,2020-07-29T01:33:06Z,,,
5673,b'Document model outputs',2020-07-10T21:10:30Z,2020-07-10T21:31:03Z,,,
5672,b'Added first description of the model',2020-07-10T18:41:09Z,2020-07-13T06:53:49Z,model card,,
5671,b'Deprecate old past arguments',2020-07-10T18:07:28Z,2020-07-10T21:25:53Z,,,
5670,b'[WIP] add DeFormer (ACL 2020) example',2020-07-10T17:56:35Z,2020-09-18T23:52:58Z,wontfix,,
5669,b'[squad] add version tag to squad cache',2020-07-10T17:39:01Z,2020-07-10T20:34:22Z,,,
5668,b'SquadDataset should use version number in cache file name',2020-07-10T17:31:41Z,2020-07-10T20:34:43Z,,,
5667,b'pytorch_model.bin file is different after uploading to HuggingFace Models',2020-07-10T17:30:13Z,2020-08-06T15:19:04Z,,,
5666,b'How do you connect Convolutional layers to Transformers?',2020-07-10T17:13:21Z,2020-09-19T03:01:40Z,wontfix,,
5665,b'[AutoModels] Fix config params handling of all PT and TF AutoModels',2020-07-10T15:45:53Z,2020-07-15T07:51:14Z,,,
5664,b'[PyTorch] Load and run a model CPU which was traced and saved on GPU',2020-07-10T15:35:56Z,2021-04-16T15:04:06Z,,RuntimeError,"RuntimeError: Expected object of device type cuda but got device type cpu for argument #1 'self' in call to _th_index_select"
5663,b'Request for Support to Adapt a Model (Human Dignity Observatory: Non-Profit Project) ',2020-07-10T15:34:59Z,2020-09-19T03:01:30Z,wontfix,,
5662,"b""[WIP - don't merge][TF generate] Make tf generate compatible with tf.function""",2020-07-10T15:19:04Z,2020-11-15T17:48:42Z,wontfix,,
5661,b'Create Model card for RoBERTa-hindi-guj-san',2020-07-10T14:18:23Z,2020-07-10T15:34:24Z,model card,,
5660,"b'""How to train a new language model from scratch"" colab stuck at training'",2020-07-10T13:49:22Z,2020-09-19T03:01:34Z,wontfix,,
5659,b'[Longformer] fix longformer global attention output',2020-07-10T13:31:21Z,2020-07-13T15:23:23Z,,,
5658,b'Create README.md - Model card',2020-07-10T11:50:49Z,2020-07-10T15:37:56Z,model card,,
5657,b'Create README.md - Model card',2020-07-10T11:37:46Z,2020-07-10T15:38:04Z,model card,,
5656,b'Truncated Outputs by t5 fine-tuned models',2020-07-10T10:50:31Z,2020-08-25T18:56:09Z,t5,,
5655,b'Create model card',2020-07-10T10:35:52Z,2020-07-10T15:38:12Z,model card,,
5654,b'\xe2\x9d\x93 Difficulties to reproduce BART results on CNN/DM by fine-tuning bart-large',2020-07-10T10:33:19Z,2020-12-25T09:25:05Z,wontfix,,
5653,"b'AutoTokenizer.from_pretrained(""hfl/chinese-roberta-wwm-ext"")  '",2020-07-10T10:11:08Z,2020-07-11T16:52:07Z,,TypeError,"TypeError: expected str, bytes or os.PathLike object, not NoneType"
5652,b'Create README.md - Model card for sentence-transformers/bert-base-nli-mean-tokens',2020-07-10T09:21:41Z,2020-07-10T09:41:11Z,model card,,
5651,b'T5 fp16 overflow in forward (T5DenseReluDense)',2020-07-10T07:14:12Z,2020-07-10T12:53:37Z,,,
5650,b'Wrong answers from Longformer model even on simple questions',2020-07-10T04:57:46Z,2020-10-18T05:15:17Z,wontfix,,
5649,b'Bugs due to design choices in LongformerTokenizer',2020-07-10T03:48:56Z,2020-09-19T03:01:39Z,wontfix,,
5648,"b""Classification accuracy on validation set didn't improve while fine-tuning BERT""",2020-07-10T03:12:27Z,2020-09-19T03:01:33Z,wontfix,,
5647,b'T5 TorchScript (Trace) Conversion ',2020-07-10T03:09:26Z,2020-10-01T15:38:51Z,,ValueError,"ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds"
5646,"b""Can't get (global) attention probs using Longformer""",2020-07-10T02:12:45Z,2020-07-13T15:23:23Z,,RuntimeError,"RuntimeError: shape '[1, 12, 5, 512]' is invalid for input of size 3182592"
5645,b'enable easy checkout switch',2020-07-10T01:02:51Z,2020-07-31T08:34:47Z,,,
5644,b'FlaubertForTokenClassification ',2020-07-10T00:16:20Z,2020-07-13T18:59:53Z,,,
5643,b'Help with Using TFXLNet on custom embeddings',2020-07-09T23:24:15Z,2020-09-17T04:45:37Z,wontfix,,
5642,b'Improvements to PretrainedConfig documentation',2020-07-09T22:08:17Z,2020-07-10T14:31:48Z,,,
5641,b'Multiple Mask Tokens',2020-07-09T21:42:18Z,2020-09-17T04:45:38Z,wontfix,,
5640,b'Cleanup bart caching logic',2020-07-09T21:17:10Z,2020-07-14T10:13:06Z,,,
5639,b'test suite fails due to pytorch bug in torch.seed',2020-07-09T19:16:36Z,2020-08-14T02:03:13Z,,RuntimeError,"RuntimeError: Overflow when unpacking long"
5638,b'Create README.md',2020-07-09T19:04:33Z,2020-07-10T15:38:23Z,model card,,
5637,b'Add forum link in the docs',2020-07-09T19:02:21Z,2020-07-09T19:13:22Z,,,
5636,b'Should check that torch TPU is available',2020-07-09T17:43:14Z,2020-07-09T17:54:33Z,,,
5635,b'[WIP][Examples] Adding more examples and more introductory tutorials',2020-07-09T16:46:46Z,2020-08-11T18:44:31Z,,,
5634,b'T5 has no module  ```torch_xla``` when using T5 fine-tuned on SQUADv2',2020-07-09T16:32:26Z,2020-07-09T17:54:33Z,,ModuleNotFoundError,"ModuleNotFoundError: No module named 'torch_xla'"
5633,b'More explicit error when failing to tensorize overflowing tokens',2020-07-09T16:27:54Z,2020-07-09T17:35:21Z,,,
5632,b'Fixed use of memories in XLNet (caching for language generation + warning when loading improper memoryless model)',2020-07-09T15:59:50Z,2020-07-10T15:38:37Z,,,
5631,b'Correct extension for model summary links',2020-07-09T14:50:01Z,2020-07-09T15:03:08Z,,,
5630,b'How can I fine-tune on custom model',2020-07-09T14:26:39Z,2020-11-07T05:42:39Z,wontfix,,
5629,b'Fixed TextGenerationPipeline on torch + GPU',2020-07-09T13:48:38Z,2020-07-09T20:29:33Z,,,
5628,b'Support for Polyencoder and other retriever based models',2020-07-09T13:26:14Z,2020-09-15T01:05:29Z,wontfix,,
5627,b'Model doc failed',2020-07-09T13:17:42Z,2020-07-09T15:27:16Z,,,
5626,b'doc: fix apparent copy-paste error in docstring',2020-07-09T13:09:28Z,2020-07-14T07:47:41Z,,,
5625,b'Cannot reproduce roberta-large on SQuAD',2020-07-09T13:08:35Z,2020-07-13T03:46:37Z,,,
5624,b'Inference widgets for self-hosted models?',2020-07-09T12:09:48Z,2020-10-09T07:48:20Z,,,
5623,b'Predictor in Streamlit Docker eating all memory and OOM',2020-07-09T11:57:58Z,2020-09-14T13:12:32Z,wontfix,,
5622,b'TextGenerationPipeline breaks when used with device=0',2020-07-09T09:46:42Z,2020-07-09T20:29:33Z,,TypeError,"TypeError: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
5621,b'Add freshly trained `codegram/calbert-base-uncased`',2020-07-09T07:44:44Z,2020-07-10T15:39:04Z,model card,,
5620,b'Fix re-tokenization (ignoring is_pretokenized=True) when passing a pretokenized batch to both batch_encode_plus and tokenizer.__call__ methods',2020-07-09T07:32:29Z,2020-08-29T05:14:11Z,,,
5619,b'Should t5-small generate coherent text as summaries without finetuning?',2020-07-09T07:27:28Z,2020-09-19T03:01:41Z,wontfix,,
5618,b'Generate up to max_target_length sequences',2020-07-09T07:16:33Z,2020-07-12T19:49:55Z,,,
5617,b'Update README.md',2020-07-09T06:20:10Z,2020-07-10T15:43:28Z,model card,,
5616,b'fix 404',2020-07-09T03:56:32Z,2020-07-09T19:12:29Z,,,
5615,b'\xf0\x9f\x90\x9b Bart Tokenization difference between 2.11.0 and 3.0.2',2020-07-09T01:48:34Z,2020-09-14T03:12:30Z,wontfix,,
5614,"b'[WIP] Test TF Flaubert + Add {XLM, Flaubert}{TokenClassification, MultipleC\xe2\x80\xa6'",2020-07-09T00:30:02Z,2020-07-29T18:26:27Z,,,
5613,b'doc fixes',2020-07-08T23:39:54Z,2020-07-08T23:52:44Z,,,
5612,b'Did the run_language_model support TPU? ',2020-07-08T21:32:15Z,2020-11-09T01:56:26Z,wontfix,,
5611,b'IndexError: index out of range in self',2020-07-08T19:19:57Z,2020-07-30T08:41:59Z,,"`IndexError, IndexError","`IndexError: index out of range in self`IndexError: index out of range in self"
5610,b'create model cards for qg models',2020-07-08T17:45:57Z,2020-07-08T20:08:57Z,model card,,
5609,"b""Duplicate grouped entities when using 'ner' pipeline""",2020-07-08T16:21:09Z,2020-09-29T05:28:17Z,wontfix,,
5608,b'Is there an implementation of BERT architecture in PyTorch that I can modify here?',2020-07-08T16:20:11Z,2020-07-09T15:38:23Z,,,
5607,b'docs(wandb): explain how to use W&B integration',2020-07-08T16:06:59Z,2020-07-14T09:12:34Z,,,
5606,b'OSError using FlauBERT',2020-07-08T14:55:44Z,2020-09-14T01:12:31Z,wontfix,"OSError, ValueError","OSError: ValueError: not enough values to unpack (expected 6, got 5)"
5605,"b'Here maybe a bug, when we load staged checkpoint'",2020-07-08T14:51:28Z,2020-09-13T17:12:34Z,wontfix,,
5604,b'[Benchmark] TFGPT2LMHeadModel is five times slower than GPT2LMHeadModel',2020-07-08T14:12:18Z,2020-10-11T11:26:00Z,wontfix,,
5603,b'Update benchmark notebook',2020-07-08T13:58:38Z,2020-07-08T14:04:00Z,,,
5602,"b'MarianMT: ""CUDA out of memory"" when translating many times with the MarianMT Model'",2020-07-08T13:26:13Z,2020-08-07T07:32:51Z,marian,,
5601,b'Create README.md',2020-07-08T13:05:12Z,2020-07-08T20:07:49Z,model card,,
5600,b'[MarianMT{',2020-07-08T12:41:25Z,2020-07-08T12:42:35Z,,,
5599,b'Add newly trained `calbert-tiny-uncased`',2020-07-08T12:29:33Z,2020-07-08T21:54:52Z,model card,,
5598,b'returned value from parse method of MeCab >= 1.0.0 was changed',2020-07-08T11:50:25Z,2020-07-11T14:10:40Z,,,
5597,b'DPR model examples / notebook / pipeline',2020-07-08T11:47:50Z,2020-09-13T17:12:36Z,wontfix,,
5596,b'Add data_collator with attention_mask feature',2020-07-08T10:09:37Z,2020-10-10T03:29:33Z,wontfix,,
5595,b'transformer dataset and masked LM',2020-07-08T09:42:20Z,2020-07-17T21:58:45Z,,,
5594,b'[Benchmark] Add benchmarks for TF Training',2020-07-08T08:50:33Z,2020-07-08T10:11:10Z,,,
5593,"b""AttributeError: 'Tensor' object has no attribute 'ndim'""",2020-07-08T08:28:19Z,2020-07-20T07:10:19Z,,"AttributeError, ValueError","AttributeError: 'Tensor' object has no attribute 'ndim'ValueError: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length."
5592,"b'Allow to set Adam beta1, beta2 in TrainingArgs'",2020-07-08T06:30:04Z,2020-07-27T09:31:38Z,,,
5591,b'KeyError Issue in Question answering',2020-07-08T05:12:23Z,2020-07-11T08:41:53Z,,,
5590,b'HF Trainer Segmentation Fault',2020-07-08T03:58:31Z,2020-10-10T08:14:00Z,wontfix,,
5589,b'Datasets & collators for NER',2020-07-08T01:49:06Z,2020-09-13T01:50:09Z,wontfix,,
5588,b'[Some weights or buffers of the PyTorch model TFGPT2LMHeadModel were not initialized] convert GPT2 pytorch to tensorflow model ',2020-07-08T01:27:49Z,2020-07-09T13:33:51Z,,,
5587,b'Difference between AutoTokenizer.from_pretrained and BertTokenizer.from_pretrained',2020-07-08T01:12:09Z,2020-07-09T13:32:08Z,,,
5586,b'GPT2 past usage',2020-07-07T21:51:27Z,2020-07-08T09:51:42Z,,,
5585,b'Update question template',2020-07-07T21:28:22Z,2020-07-08T12:46:36Z,,,
5584,"b""On running finetune.py for seq2seq, the following error comes up: optimizer_step() got an unexpected keyword argument 'using_native_amp'""",2020-07-07T21:04:42Z,2020-11-14T09:27:09Z,wontfix,TypeError,"TypeError: optimizer_step() got an unexpected keyword argument 'using_native_amp'"
5583,b'Test XLA examples',2020-07-07T18:55:28Z,2020-07-09T13:19:20Z,,,
5582,b'Rename files',2020-07-07T18:25:11Z,2020-07-07T18:40:43Z,,,
5581,b'[mbart] prepare_translation_batch passes **kwargs to allow DeprecationWarning',2020-07-07T17:35:44Z,2020-07-07T17:46:06Z,,,
5580,"b""TypeError: 'BertTokenizer' object is not callable""",2020-07-07T16:45:06Z,2020-07-07T18:26:23Z,,,
5579,"b""OSError: Model name 'facebook/bart-large-cnn' was not found in tokenizers model name list""",2020-07-07T16:13:41Z,2020-07-08T02:44:02Z,,OSError,"OSError: Model name 'facebook/bart-large-cnn' was not found in tokenizers model name list (facebook/bart-base, facebook/bart-large, facebook/bart-large-mnli, facebook/bart-large-cnn, facebook/bart-large-xsum, yjernite/bart_eli5). We assumed 'facebook/bart-large-cnn' was a path, a model identifier, or url to a directory containing vocabulary files named ['vocab.json', 'merges.txt'] but couldn't find such vocabulary files at this path or url."
5578,b'[Reformer] - Cache hidden states and buckets to speed up inference',2020-07-07T15:38:20Z,2020-07-17T14:17:43Z,,,
5577,b'Fix tokenizers pretrained saving/loading',2020-07-07T14:18:44Z,2020-08-11T08:51:18Z,,,
5576,b'Fix tests imports dpr',2020-07-07T14:08:54Z,2020-07-07T14:35:13Z,,,
5575,b'Seperating premise and hypothesis in MNLI',2020-07-07T14:05:29Z,2020-07-09T03:16:25Z,,,
5574,b'Create README.md for electra-base-squad2',2020-07-07T14:00:53Z,2020-07-10T15:39:45Z,model card,,
5573,b'MBARTTokenizer set_lang logic will only work for src_lang=en_XX',2020-07-07T13:46:56Z,2020-07-20T00:31:12Z,"seq2seq, translation",,
5572,b'Create README.md',2020-07-07T12:26:43Z,2020-07-10T15:42:49Z,model card,,
5571,"b""Tokenizers save_pretrained doesn't work with custom vocabs (v3.0.2)""",2020-07-07T09:40:21Z,2020-08-11T08:51:04Z,Core: Tokenization,TypeError,"TypeError: Object of type AddedToken is not JSON serializable"
5570,b'Freeze the token embeddings for finetuning',2020-07-07T09:06:06Z,2020-10-25T11:29:45Z,wontfix,,
5569,b'BertEmbeddings code for position_embeddings and word_embeddings',2020-07-07T08:21:30Z,2020-07-07T13:49:38Z,,,
5568,b'Use pretrained bert withou embedding layers.',2020-07-07T08:20:22Z,2021-03-06T00:17:56Z,wontfix,,
5567,b'How to get gradient wrt to a word embedding layer pytorch?',2020-07-07T05:40:17Z,2020-07-07T19:35:15Z,,,
5566,b'[docs] fix model_doc links in model summary',2020-07-07T05:09:16Z,2020-07-07T15:06:13Z,,,
5565,b'\xe2\x9d\x93 Why multiplying the output of T5 by some scalar before LM head ?',2020-07-07T04:53:44Z,2020-09-19T03:01:38Z,wontfix,,
5564,b'Where is the documentation on migrating to the 3.0 tokenizer API?',2020-07-07T03:17:26Z,2020-07-07T06:51:00Z,,,
5563,b'Bug in Question Answering pipeline when question is weird (unanswerable)',2020-07-07T01:17:06Z,2020-07-13T00:34:34Z,,KeyError,"KeyError: 0"
5562,b'Fix fast tokenizers too',2020-07-06T22:44:23Z,2020-07-06T22:45:01Z,,,
5561,b'[Docs] Incorrect links to models in the Summary of the Models page',2020-07-06T21:32:57Z,2020-07-07T18:28:54Z,,,
5560,b'[Reformer] Adapt Reformer MaskedLM Attn mask',2020-07-06T21:23:50Z,2020-07-07T08:48:07Z,,,
5559,b'Fix #5507',2020-07-06T21:13:11Z,2020-07-06T21:26:49Z,,,
5558,b'Various tokenizers fixes',2020-07-06T20:54:28Z,2020-07-06T22:27:55Z,,,
5557,"b""Roberta Large doesn't train for sentiment classification""",2020-07-06T19:00:21Z,2020-09-12T22:50:10Z,wontfix,,
5556,b'[pl examples] add using_native_amp flag to support pl 0.8.4',2020-07-06T18:47:47Z,2020-07-26T20:59:41Z,,TypeError,"TypeError: optimizer_step() got an unexpected keyword argument 'using_native_amp'"
5555,b'Training TFBertForSequenceClassification with DataFrame instead of tensorflow_datasets',2020-07-06T18:18:43Z,2020-09-19T03:01:46Z,wontfix,ValueError,"ValueError: Failed to find data adapter that can handle input: (<class 'list'> containing values of types {'(<class \'list\'> containing values of types {""<class \'int\'>""})'}), <class 'numpy.ndarray'>"
5554,b'huggingface optimizer cannot de-serialize',2020-07-06T18:01:10Z,2020-09-11T22:02:22Z,wontfix,,
5553,b'Customize widget text-generation inference with prepended input',2020-07-06T17:55:11Z,2020-09-17T14:19:43Z,wontfix,,
5552,"b""[Don't merge] Reformer Trax Integration Tests""",2020-07-06T17:19:04Z,2020-09-11T22:02:10Z,wontfix,,
5551,b'Fix #5544',2020-07-06T15:14:34Z,2020-07-06T15:22:25Z,,,
5550,b'Fix the tokenization warning noted in #5505',2020-07-06T14:58:21Z,2020-07-06T15:15:26Z,,,
5549,b'The `add_space_before_punct_symbol` is only for TransfoXL',2020-07-06T14:47:16Z,2020-07-06T16:17:06Z,,,
5548,b'Possibility to use WhitespaceSplit as pre_tokenizer instead of BPE/Sentencepiece?',2020-07-06T14:41:19Z,2020-09-05T08:04:39Z,wontfix,,
5547,b'[Feature Request] Extract Predictions from Trainer',2020-07-06T14:36:21Z,2020-12-13T01:22:51Z,wontfix,,
5546,b'GPT2 tokenizer should not output token type IDs',2020-07-06T14:23:40Z,2020-07-06T15:33:58Z,,,
5545,b'Batching (TF)BertForQuestionAnswering deployment',2020-07-06T14:16:24Z,2020-09-11T22:02:20Z,wontfix,,
5544,b'incorrect typehint for PreTrainedTokenizer.convert_ids_to_tokens() return value',2020-07-06T13:57:52Z,2020-07-06T15:22:27Z,,,
5543,b't5-base translation_en_to_de BLEU lower than the paper',2020-07-06T13:35:18Z,2020-09-04T18:03:44Z,,,
5542,b'QA pipeline should mask CLS tokens after handling impossible answer',2020-07-06T11:48:16Z,2020-07-09T13:50:14Z,,,
5541,b'High F1 score. But poor accuracy during Inference due to tokenisation',2020-07-06T11:32:34Z,2020-09-11T22:02:21Z,wontfix,,
5540,b'MobileBert embedding vectors values',2020-07-06T10:53:28Z,2020-09-13T17:12:38Z,wontfix,,
5539,b'Fine Tuning Using /question-answering/run_squad.py',2020-07-06T10:42:59Z,2020-07-13T11:40:11Z,,,
5538,b'Easier way to download pretrained model files to local',2020-07-06T09:35:31Z,2020-09-11T22:02:18Z,wontfix,,
5537,b'Longformer - Compression',2020-07-06T09:32:39Z,2020-09-11T22:02:19Z,wontfix,,
5536,b'Create README',2020-07-06T08:23:19Z,2020-07-07T10:38:16Z,model card,,
5535,b'How i can set the special token <|endoftext|> to an other id ?',2020-07-06T07:43:29Z,2020-09-11T22:02:16Z,wontfix,,
5534,b'How-to-fine-tune-bert-for-question-answering?',2020-07-06T07:02:33Z,2020-07-13T11:39:52Z,Help wanted,,
5533,b'[wip] Label smooth',2020-07-06T03:06:20Z,2020-07-21T00:53:47Z,,,
5532,b'Error in Loading bert-large-uncased-whole-word-masking-finetuned-squad',2020-07-06T00:37:56Z,2020-09-11T22:02:15Z,wontfix,OSError,"OSError: Unable to load weights from pytorch checkpoint file. If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True. "
5531,"b""fixed ImportError: cannot import name 'hf_bucket_url' on convert_pytorch_checkpoint_to_tf2.py""",2020-07-05T22:18:20Z,2020-07-06T13:55:11Z,,,
5530,b'Tabert',2020-07-05T20:29:42Z,,"New model, Feature request",,
5529,b'[fix] pin sacrebleu to fix CI ImportError',2020-07-05T19:11:44Z,2020-07-06T15:03:07Z,,,
5528,"b'3.0.1: ""unexpected keyword argument \'is_pretokenized\'"" when using batch_encode_plus() w/ Fast Tokenizers'",2020-07-05T18:40:52Z,2020-07-07T02:45:46Z,Core: Tokenization,TypeError,"TypeError: encode_batch() got an unexpected keyword argument 'is_pretokenized'"
5527,b'Tf to pytorch',2020-07-05T16:55:13Z,2020-07-06T02:36:29Z,,,
5526,b'Fix `RobertaClassificationHead` style consistency.',2020-07-05T16:01:04Z,2020-10-04T01:13:41Z,wontfix,,
5525,"b""WARNING:transformers.tokenization_utils:Keyword arguments {'add_space_before_punct_symbol': True} not recognized.""",2020-07-05T11:44:22Z,2020-07-06T16:17:05Z,Core: Tokenization,,
5524,b'How to fine-tune tinyBERT for question-asnwering',2020-07-05T10:18:44Z,2020-07-13T11:39:40Z,,,
5523,b'Create model card',2020-07-05T10:17:40Z,2020-07-07T10:33:43Z,model card,,
5522,b'Added data collator for permutation (XLNet) language modeling and related calls',2020-07-05T10:01:14Z,2020-07-07T08:17:38Z,,,
5521,b'What should i do if I want a model class similar to BertForSequenceClassification?',2020-07-05T02:41:39Z,2020-09-11T22:02:14Z,wontfix,,
5520,b'AdamW step device error',2020-07-04T20:24:08Z,2020-07-06T18:13:05Z,,RuntimeError,"RuntimeError: expected device cuda:0 but got device cpu"
5519,b'Get prediction_scores from BART forward method',2020-07-04T20:22:47Z,2020-07-06T13:58:50Z,,,
5518,b'Make T5 compatible with ONNX',2020-07-04T15:41:38Z,2020-07-07T09:32:29Z,,,
5517,b'getting different model result from tokenizer vs tokenizer.encode function',2020-07-04T12:09:45Z,2020-07-06T15:33:58Z,Core: Tokenization,,
5516,b'Addition of a DialoguePipeline',2020-07-04T10:45:00Z,2020-07-30T18:11:40Z,,,
5515,b'Create model card',2020-07-04T09:37:25Z,2020-07-07T10:39:10Z,model card,,
5514,b'added model card for ukr-roberta-base',2020-07-04T09:28:07Z,2020-07-07T10:40:24Z,model card,,
5513,b'Fail in some tests (with detailed description)',2020-07-04T08:35:14Z,2020-09-12T18:50:09Z,wontfix,,
5512,"b'Allow tests in examples to use cuda or fp16,if they are available'",2020-07-04T06:08:33Z,2020-08-25T10:02:08Z,,,
5511,b'example code missing `encode`',2020-07-04T04:55:22Z,2020-07-04T05:01:52Z,,TypeError,"TypeError: 'BertTokenizer' object is not callable"
5510,b'Fix typo in training',2020-07-04T03:17:13Z,2020-07-06T13:14:58Z,,,
5509,b'TPU Trainer memory leak and memory requirements',2020-07-03T23:32:29Z,2020-10-18T05:14:59Z,wontfix,,
5508,b'T5 Masking:',2020-07-03T22:20:00Z,2020-07-06T06:19:29Z,,,
5507,"b""What's the correct way to use add_prefix_space for the fast RoBERTa tokenizer in 3.0.0/3.0.1?""",2020-07-03T21:54:12Z,2020-07-06T21:26:52Z,Core: Tokenization,ValueError,"ValueError: Keyword arguments {'add_prefix_space': True} not recognized."
5506,b'Why is `encoder_extended_attention_mask = None` when `config.is_decoder == False`',2020-07-03T20:29:16Z,2020-07-06T06:28:53Z,,,
5505,"b'3.0.1 BertTokenizer batch_encode_plus() shows warnings ""Truncation was not explicitely activated but `max_length` is provided a specific value""'",2020-07-03T19:54:07Z,2020-07-06T15:15:26Z,,,
5504,b'Write With Transformers',2020-07-03T19:44:07Z,2020-09-11T02:02:48Z,wontfix,,
5503,b'T5 Training on TPU doesnt use TPU',2020-07-03T15:36:39Z,2020-09-12T18:50:08Z,wontfix,,
5502,b'licens',2020-07-03T14:18:28Z,2020-07-03T14:18:50Z,,,
5501,b'Merge pull request #1 from huggingface/master',2020-07-03T13:35:26Z,2020-07-03T13:35:34Z,,,
5500,"b""batch_encode_plus model output is different from tokenizer.encode model's output""",2020-07-03T13:03:42Z,2020-07-06T20:07:09Z,Core: Tokenization,,
5499,b'[ERROR] add_special_tokens = True not working in version 3.0.0',2020-07-03T13:02:55Z,2020-09-11T22:02:18Z,wontfix,,
5498,b'What happened to https://huggingface.co/zero-shot/ ?',2020-07-03T12:42:16Z,2020-07-03T16:09:56Z,,,
5497,b'[Generation] better error message',2020-07-03T12:40:22Z,2020-07-03T17:25:26Z,,,
5496,b'QA pipeline BART compatible',2020-07-03T11:46:03Z,2020-07-09T13:11:40Z,,,
5495,b'Typo fix in `training` doc',2020-07-03T11:26:05Z,2020-07-06T13:15:22Z,,,
5494,b'The inference speed of gpt2-xl has a gap between pytorch and tensorflow.',2020-07-03T09:24:01Z,2020-07-03T10:00:24Z,,,
5493,b'Create README.md',2020-07-03T08:57:02Z,2020-07-07T10:43:10Z,model card,,
5492,b'Update README.md',2020-07-03T08:55:27Z,2020-07-07T10:43:35Z,model card,,
5491,b'Update README.md',2020-07-03T08:54:24Z,2020-07-07T10:43:49Z,model card,,
5490,b'[ERROR] Tokenizer and TokenizerFast ??? ',2020-07-03T07:35:59Z,2020-07-06T22:27:55Z,Core: Tokenization,,
5489,b'encoder_outputs are always the same when generating with different inputs',2020-07-03T06:44:13Z,2020-07-24T00:39:57Z,,,
5488,b'Cannot train RoBERTa from scratch with multiple nodes and multiple GPUs',2020-07-03T06:13:40Z,2020-07-14T22:51:59Z,,,
5487,b'Better TPU Support in examples',2020-07-03T05:53:19Z,2020-09-11T02:02:49Z,wontfix,,
5486,"b'Tokenizers throwing warning ""The current process just got forked, Disabling parallelism to avoid deadlocks.. To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)""'",2020-07-03T05:13:36Z,2020-07-06T22:27:54Z,Core: Tokenization,,
5485,b'Bert-extractive-summaizer importing issue',2020-07-03T04:25:34Z,2020-09-08T07:19:15Z,wontfix,**NameError,"**NameError: name 'BertModel' is not defined**"
5484,b'Error using t5-base-cnn',2020-07-03T04:19:11Z,2020-07-29T01:28:38Z,,OSError,"OSError: Model name 'sshleifer/t5-base-cnn' was not found in tokenizers model name list (t5-small, t5-base, t5-large, t5-3b, t5-11b). We assumed 'sshleifer/t5-base-cnn' was a path, a model identifier, or url to a directory containing vocabulary files named ['spiece.model'] but couldn't find such vocabulary files at this path or url."
5483,"b"" can't get models directory after running python run_squad.py """,2020-07-03T01:25:18Z,2020-09-08T07:19:14Z,wontfix,,
5482,"b""Can't pickle tokenizers ...""",2020-07-03T01:03:17Z,2020-07-06T22:27:54Z,,,
5481,b'Merge pull request #1 from huggingface/master',2020-07-03T00:20:18Z,2020-07-03T13:31:10Z,,,
5480,"b""'Size' Error while loading t5-large model""",2020-07-02T23:23:33Z,2020-07-02T23:28:14Z,,"KeyError, AttributeError","KeyError: 'size'AttributeError: "
5479,b'Exposing prepare_for_model for both slow & fast tokenizers',2020-07-02T21:57:45Z,2020-07-03T14:51:22Z,Core: Tokenization,,
5478,"b'Possible breaking undetected change to ""data/processors/squad.py""'",2020-07-02T21:55:25Z,2020-07-03T17:53:47Z,,,
5477,b'Add DeeBERT (entropy-based early exiting for *BERT)',2020-07-02T21:38:19Z,2020-07-08T00:18:00Z,,,
5476,b'Seq2Seq: Option to not store whole dataset in memory',2020-07-02T20:17:07Z,2020-07-18T17:57:33Z,Help wanted,,
5475,b'35 Model Hub entries fail AutoConfig',2020-07-02T20:04:09Z,2020-07-03T14:26:26Z,,,
5474,"b""Can't use AutoModelForCausalLM with bert""",2020-07-02T19:59:14Z,2020-07-10T17:48:31Z,,"AssertionError, TypeError","AssertionError: If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True`.TypeError: __init__() got an unexpected keyword argument 'is_decoder'"
5473,"b'TFAutoModelForSequenceClassification: ValueError: Layer #1 (named ""classifier"") expects 2 weight(s), but the saved weights have 4 element(s).'",2020-07-02T19:11:28Z,2020-07-02T23:24:41Z,,ValueError,"ValueError: Layer #1 (named ""classifier"") expects 2 weight(s), but the saved weights have 4 element(s)."
5472,b'Truncation in GLUE should be longest first',2020-07-02T17:52:28Z,2020-07-06T14:16:31Z,,ValueError,"ValueError: expected sequence of length 128 at dim 1 (got 202)"
5471,b'Update: ElectraDiscriminatorPredictions forward.',2020-07-02T17:47:15Z,2020-07-02T17:57:34Z,,,
5470,b'Unable to use run_squad with xla_spawn.py on TPU',2020-07-02T16:45:52Z,2020-07-02T17:56:21Z,,AttributeError,"AttributeError: module 'run_squad' has no attribute '_mp_fn'"
5469,b'[Discussion] fix zero divison error (Reformer batch size bug)',2020-07-02T16:23:45Z,2020-07-02T17:04:53Z,,,
5468,b'Fix saved model creation',2020-07-02T15:06:40Z,2020-08-03T12:10:40Z,,,
5467,b'Tokenizer summary',2020-07-02T14:22:01Z,2020-07-02T21:07:43Z,,,
5466,b'Fix typo in glossary',2020-07-02T13:18:15Z,2020-07-02T13:19:34Z,,,
5465,b'Fixing missing arguments for TransfoXL tokenizer when using TextGenerationPipeline',2020-07-02T10:18:37Z,2020-07-02T11:53:33Z,,,
5464,b'Create model card',2020-07-02T10:13:48Z,2020-07-03T10:19:50Z,model card,,
5463,b'Pre-Trained Model (ipuneetrathore/bert-base-cased-finetuned-finBERT) loads in PyTorch but not Tensorflow',2020-07-02T09:37:45Z,2020-07-02T13:27:04Z,,OSError,"OSError: Can't load weights for 'ipuneetrathore/bert-base-cased-finetuned-finBERT'. Make sure that:"
5462,b'Changed expected_output_ids in TransfoXL generation test',2020-07-02T09:24:42Z,2020-07-02T09:56:45Z,,,
5461,b'[Reformer] combine reformer model with other tokenizers',2020-07-02T09:05:33Z,2020-07-02T14:23:18Z,,RuntimeError,"RuntimeError: CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`"
5460,b'BERT Huggingface trainer api: ValueError: expected sequence of length 128 at dim 1 (got 314)',2020-07-02T05:54:29Z,2020-07-03T14:51:22Z,,ValueError,"ValueError: expected sequence of length 128 at dim 1 (got 314)"
5459,"b""Error while saving model: TypeError: ('Not JSON Serializable:', DistilBertConfig""",2020-07-02T05:17:44Z,2020-07-14T15:25:50Z,,TypeError,"TypeError: ('Not JSON Serializable:', DistilBertConfig {"
5458,"b""\xf0\x9f\x90\x9b Can't use `AutoTokenizer` with `sshleifer/mbart-large-cc25`""",2020-07-02T05:05:33Z,2020-07-07T17:23:01Z,,>OSError,">OSError: Model name 'sshleifer/mbart-large-cc25' was not found in tokenizers model name list (facebook/bart-large, facebook/bart-large-mnli, facebook/bart-large-cnn, facebook/bart-large-xsum). We assumed 'sshleifer/mbart-large-cc25' was a path, a model identifier, or url to a directory containing vocabulary files named ['vocab.json', 'merges.txt'] but couldn't find such vocabulary files at this path or url."
5457,"b'[Bart] enable test_torchscript, update test_tie_weights'",2020-07-02T04:06:51Z,2020-07-07T14:06:49Z,,,
5456,b'Add description of required special symbols',2020-07-02T03:13:27Z,2020-07-03T12:16:28Z,model card,,
5455,b'How to batch encode sentences using BertTokenizer?',2020-07-02T02:36:01Z,2020-07-22T06:58:12Z,Core: Tokenization,,
5454,b'Error while saving Longformer pre-trained model',2020-07-02T00:30:45Z,2020-09-19T14:29:15Z,wontfix,AttributeError,"AttributeError: 'AddedToken' object has no attribute '__getstate__'"
5453,b'The output to be used for getting sentence embeddings from BERT',2020-07-02T00:04:40Z,2020-09-07T16:57:48Z,wontfix,,
5452,"b""Text Classification with PyTorch Lightning: 'dict' object has no attribute 'task'""",2020-07-02T00:04:01Z,2020-10-01T07:08:13Z,"wontfix, Examples",AttributeError,"AttributeError: 'dict' object has no attribute 'task'"
5451,b'TF: inputs vs input_ids',2020-07-01T23:17:26Z,2020-09-01T08:42:09Z,wontfix,,
5450,b'Add Reformer MLM notebook',2020-07-01T22:19:22Z,2020-07-01T22:20:50Z,,,
5449,b'Guide to fixed-length model perplexity evaluation',2020-07-01T22:05:44Z,2020-07-07T22:04:15Z,,,
5448,b'grammar corrections and train data update',2020-07-01T21:07:45Z,2020-07-03T12:25:57Z,model card,,
5447,"b'Where did ""prepare_for_model"" go?  What is the replacement?'",2020-07-01T19:20:34Z,2020-07-03T14:51:22Z,,,
5446,"b""Reformer language modeling using run_language_modeling.py: sentences didn't pad to max_length""",2020-07-01T18:49:32Z,2020-07-03T12:58:35Z,,ValueError,"ValueError: If training, sequence Length 444 has to be a multiple of least common multiple chunk_length 64. Please consider padding the input to a length of 448."
5445,"b'""Write With Transformer"" inserts a space whenever accepting a suggestion, even if a space doesn\'t belong there'",2020-07-01T18:19:54Z,2020-09-06T23:57:53Z,wontfix,,
5444,b'Inconsistent tokenizer handling of max_len',2020-07-01T17:52:01Z,2020-07-01T18:29:20Z,,,
5443,b'(TF) model.generate to tf.function for tf serving',2020-07-01T15:59:18Z,2020-09-24T04:15:48Z,wontfix,,
5442,b'[fix] Marian tests import',2020-07-01T15:29:00Z,2020-07-01T15:42:23Z,,,
5441,b'Benchmarking on TPU shows clearly wrong results',2020-07-01T15:16:17Z,2020-09-19T03:01:37Z,wontfix,,
5440,b'Fix dropdown bug in searches',2020-07-01T15:01:43Z,2020-07-01T15:03:00Z,,,
5439,"b""Don't discard entity_group when token is the last in the sequence.""",2020-07-01T14:59:38Z,2020-07-01T18:30:43Z,,,
5438,b'Change model outputs types to self-document outputs',2020-07-01T14:04:09Z,2020-07-10T15:36:54Z,,,
5437,"b'""Write With Transformer"" not generating text (502 Bad Gateway)'",2020-07-01T12:18:08Z,2020-07-01T13:59:40Z,,,
5436,b'Squad2 processor error',2020-07-01T12:12:29Z,2020-07-01T16:43:52Z,,ValueError,"ValueError: None is not in list"
5435,b'I want to load pre-trained model from file instead of file name',2020-07-01T11:52:34Z,2020-09-06T23:57:49Z,wontfix,,
5434,b'MiniLM transformers inconsistent log posteriors in multiple runs',2020-07-01T10:39:32Z,2020-07-01T14:52:00Z,,,
5433,b'[Reformer] Add QA head to reformer model',2020-07-01T10:36:29Z,2020-07-01T16:27:15Z,,,
5432,b'Create model card',2020-07-01T10:30:58Z,2020-07-02T14:16:31Z,model card,,
5431,"b""Can't load to predict a reproduced DistilBERT """,2020-07-01T10:20:57Z,2020-07-10T13:53:11Z,,,
5430,b'Create model card',2020-07-01T09:53:30Z,2020-07-07T10:41:42Z,model card,,
5429,b'QA Pipelines fixes',2020-07-01T09:40:53Z,2020-07-03T08:29:21Z,,,
5428,b'How to use (and preferably finetune) BART for text infilling? ',2020-07-01T09:27:05Z,2020-09-19T14:29:12Z,"Help wanted, wontfix, Examples",,
5427,"b""WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.""",2020-07-01T09:07:02Z,2020-07-09T13:26:14Z,,,
5426,b'[Reformer] Add Masked LM Reformer',2020-07-01T08:30:53Z,2020-07-01T20:43:18Z,,,
5425,b'[Quick poll] Give your opinion on the future of \xf0\x9f\xa4\x97 transformers',2020-07-01T08:07:59Z,2020-09-06T23:57:50Z,"Discussion, wontfix",,
5424,b'Bart EncoderLayer masked_fill not working properly with pytorch 1.4',2020-07-01T07:54:12Z,2020-09-06T23:57:48Z,wontfix,RuntimeError,"RuntimeError: Expected object of scalar type Bool but got scalar type Float for argument #2 'mask' in call to _th_masked_fill_bool_"
5423,b'Error Instantiating T5-11B from conributed models',2020-07-01T03:18:48Z,2020-07-23T16:16:23Z,,OSError,"OSError: Can't load weights for 't5-11b'. Make sure that:"
5422,b'Create README.md',2020-07-01T01:41:21Z,2020-07-01T09:01:51Z,model card,,
5421,"b'What to do about this warning message: ""Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification""'",2020-07-01T01:31:55Z,2020-07-01T18:37:50Z,,,
5420,"b'Refactor generation sampling parameters (e.g. top k, temperature) into ""Sampling"" classes '",2020-06-30T23:45:56Z,2020-09-13T17:12:29Z,wontfix,,
5419,b'High Quality EN-DE/EN-FR Translators',2020-06-30T23:41:39Z,2020-09-17T15:31:30Z,"Help wanted, New model, translation",,
5418,b'Bans SentencePiece 0.1.92',2020-06-30T22:52:37Z,2020-07-02T13:23:01Z,,,
5417,b'Clean up diffs in Trainer/TFTrainer',2020-06-30T22:50:29Z,2020-07-01T15:00:21Z,,,
5416,"b'Refactor generation sampling parameters (e.g. top k, temperature) into ""Sampling"" classes'",2020-06-30T22:30:10Z,2020-06-30T23:41:35Z,,,
5415,b'Gradient checkpointing BERT & ALBERT poc',2020-06-30T21:43:06Z,2020-11-14T05:02:32Z,wontfix,,
5414,b'Fix roberta model ordering for TFAutoModel',2020-06-30T21:36:21Z,2020-07-02T23:23:56Z,,,
5413,b'[mobilebert] Avoid F.tanh deprecation warning',2020-06-30T20:35:33Z,2020-06-30T20:41:44Z,cleanup,,
5412,b'[GH Runner] fix yaml indent',2020-06-30T20:16:47Z,2020-06-30T20:17:13Z,,,
5411,b'Add TFBartForConditionalGeneration',2020-06-30T19:52:10Z,2020-10-21T11:10:17Z,TensorFlow,,
5410,b'[cleanup] TF T5 tests only init t5-base once.',2020-06-30T19:50:41Z,2020-07-03T18:27:49Z,,,
5409,"b""[CI] gh runner doesn't use -v, cats new result""",2020-06-30T19:41:46Z,2020-06-30T20:12:15Z,,,
5408,b'Fix examples titles and optimization doc page',2020-06-30T19:37:37Z,2020-07-01T12:11:25Z,,,
5407,b'examples/seq2seq: never override $WANDB_PROJECT ',2020-06-30T19:11:30Z,2020-06-30T19:29:13Z,,,
5406,b'[fix] slow fill_mask test failure',2020-06-30T18:57:53Z,2020-06-30T19:28:16Z,,,
5405,b'Colab session crash with XLA & Tranformers',2020-06-30T18:09:11Z,2020-07-01T03:40:42Z,,,
5404,"b'How to interpret/act on this warning:  ""Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM""?'",2020-06-30T17:30:17Z,2020-06-30T17:34:00Z,,,
5403,"b'How to interpret/act on this warning:  ""Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM'",2020-06-30T17:26:27Z,2020-06-30T18:45:04Z,,,
5402,b'Help with Debugging TF Common tests',2020-06-30T17:19:30Z,2020-08-29T21:19:04Z,wontfix,"RuntimeError, AttributeError","RuntimeError: Unable to create link (name already exists)AttributeError: tf_bart_model_9.tf_bart_encoder_9.tf_shared_embeddings_9.weight not found in PyTorch model"
5401,b'Runtime for BERT and Roberta',2020-06-30T16:43:43Z,2020-09-07T09:57:48Z,wontfix,,
5400,b'Create model card for schmidek/electra-small-cased',2020-06-30T16:32:16Z,2020-07-01T08:01:57Z,model card,,
5399,b'Add support for past states',2020-06-30T16:31:33Z,2020-07-01T12:11:56Z,,,
5398,b'Inference time difference between pipeline and with standalone model and tokenizer',2020-06-30T13:18:18Z,2020-09-19T14:29:18Z,"wontfix, Core: Pipeline",,
5397,"b'tokenizer started throwing this warning, """"Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to \'only_first\' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.""""'",2020-06-30T12:07:33Z,2020-06-30T23:25:01Z,Core: Tokenization,,
5396,b'Create model card',2020-06-30T12:05:42Z,2020-07-03T12:29:10Z,model card,,
5395,b'[Almost all TF models] TF clean up: add missing CLM / MLM loss; fix T5 naming and keras compile',2020-06-30T10:44:39Z,2020-07-07T16:15:54Z,,,
5394,b'Upload DistilBART artwork',2020-06-30T10:10:39Z,2020-06-30T10:11:11Z,,,
5393,b'GPT2Tokenizer.save_pretrained does not work in v3.0.0',2020-06-30T09:43:06Z,2020-07-06T22:27:54Z,,TypeError,"TypeError: Object of type AddedToken is not JSON serializable"
5392,b'Windows: No matching distribution found for lightning_base',2020-06-30T09:09:36Z,2020-07-27T19:07:32Z,"Help wanted, Examples",,
5391,"b'Training a GPT-2 from scratch in Greek-text, results in a low perplexity score of 7 after 15 epochs. Is it normal that score?'",2020-06-30T08:37:47Z,2020-09-13T17:12:37Z,wontfix,,
5390,b'model.generate source code',2020-06-30T08:20:33Z,2020-08-29T21:34:27Z,wontfix,,
5389,b'Raises PipelineException on FillMaskPipeline when there are != 1 mask_token in the input',2020-06-30T08:16:16Z,2020-07-01T15:27:47Z,,,
5388,b'Why T5 do not generate the whole next sentence as one of the pretrain loss?',2020-06-30T07:34:59Z,2020-07-01T02:01:17Z,,,
5387,b'BART fine tuning on gpu issue',2020-06-30T06:56:36Z,2020-09-06T23:57:52Z,wontfix,RuntimeError,"RuntimeError: Expected object of device type cuda but got device type cpu for argument #3 'index' in call to _th_index_select"
5386,b'bart-large-cnn training related information',2020-06-30T06:46:30Z,2020-08-29T21:34:09Z,wontfix,,
5385,b'Example: PyTorch Lightning returns missing attribute error (Token Classification)',2020-06-30T00:24:10Z,2020-09-05T18:49:59Z,"wontfix, Ex: Named Entity Recognition",AttributeError,"AttributeError: Missing attribute ""n_gpu"""
5384,b'Positional and Segment Embeddings in BERT',2020-06-29T23:55:09Z,2020-10-23T08:08:47Z,wontfix,,
5383,b'Documentation for the Trainer API',2020-06-29T21:54:02Z,2020-06-30T15:43:44Z,,,
5382,"b"" cannot import name 'AutoModelForSeq2SeqLM' from transformers""",2020-06-29T21:15:30Z,2020-07-27T19:13:40Z,,,
5381,b'AssertionError: Padding_idx must be within num_embeddings',2020-06-29T19:29:03Z,2020-06-30T23:46:38Z,,AssertionError,"AssertionError: Padding_idx must be within num_embeddings"
5380,b'Unable to load pre-trained model/tokenizer when using Kubernetes',2020-06-29T19:10:16Z,2020-07-09T01:13:24Z,,,
5379,b'Cannot reduce n_ctx for distil gpt2 from 1024 to 256',2020-06-29T19:10:06Z,2020-09-05T18:49:56Z,wontfix,,
5378,b'Mention openAI model card and merge content',2020-06-29T18:23:09Z,2020-06-29T22:27:37Z,model card,,
5377,b'New tokenizer code in transformer 3.0.0 is creating error with old code',2020-06-29T18:20:28Z,2020-07-03T14:51:21Z,Core: Tokenization,,
5376,b'Unable to load Longformer pretrained weights',2020-06-29T18:19:54Z,2020-07-23T13:31:44Z,,,
5375,b'Update dependency mecab to at least version 1.0',2020-06-29T17:52:21Z,2020-07-31T12:55:34Z,,,
5374,b'How to share model cards with the CLI',2020-06-29T17:41:58Z,2020-06-30T12:59:33Z,,,
5373,b'Update README.md',2020-06-29T17:37:02Z,2020-06-30T10:01:45Z,model card,,
5372,b'Albert pooling dimension mismatches',2020-06-29T17:36:32Z,2020-09-05T18:49:55Z,wontfix,ValueError,"ValueError: Expected input batch_size (1600) to match target batch_size (16)."
5371,b'Update README.md',2020-06-29T17:33:18Z,2020-06-30T10:01:12Z,model card,,
5370,b'Update README.md',2020-06-29T17:29:25Z,2020-06-30T10:00:30Z,model card,,
5369,b'Update README.md',2020-06-29T17:26:26Z,2020-06-30T10:02:24Z,model card,,
5368,b'Fix model card folder name so that it is consistent with model hub',2020-06-29T15:44:12Z,2020-06-29T16:54:30Z,model card,,
5367,b'Add link to file and fix typos in model card',2020-06-29T15:30:52Z,2020-06-29T15:34:53Z,model card,,
5366,b'Doc for v3.0.0',2020-06-29T15:05:57Z,2020-06-29T15:08:55Z,,,
5365,"b'[seq2seq docs] Move evaluation down, fix typo'",2020-06-29T14:34:33Z,2020-06-29T14:36:04Z,,,
5364,"b'Layer #0 (named ""roberta"") expects 0 weight(s), but the saved weights have 199 element(s)'",2020-06-29T14:29:13Z,2020-06-29T14:42:09Z,,ValueError,"ValueError: Layer #0 (named ""roberta"") expects 0 weight(s), but the saved weights have 199 element(s)."
5363,b'[Benchmark] Readme for benchmark',2020-06-29T14:11:19Z,2020-07-07T21:21:24Z,,,
5362,b'Pin mecab for now',2020-06-29T13:46:53Z,2020-06-29T13:51:14Z,,,
5361,b'[WIP] update pl=0.8.5',2020-06-29T12:42:23Z,2020-07-21T12:45:26Z,,,
5360,b'[Docs] Benchmark docs',2020-06-29T12:12:43Z,2020-06-29T14:08:58Z,,,
5359,b'Segmentation fault (core dumped) after importing transformers',2020-06-29T11:46:22Z,2020-06-30T04:37:52Z,,,
5358,b'Do T5 have the next-sentence-predict loss?',2020-06-29T11:26:59Z,2020-06-30T01:21:27Z,,,
5357,b'Fix table format fot test tesults',2020-06-29T11:10:17Z,2020-06-29T13:02:34Z,model card,,
5356,b'Create model card',2020-06-29T11:08:56Z,2020-06-29T13:01:56Z,model card,,
5355,b'Update Bertabs example to work again',2020-06-29T08:34:33Z,2020-06-30T06:05:02Z,,,
5354,b'Added data collator for XLNet language modeling and related calls',2020-06-29T05:31:01Z,2020-07-05T09:46:06Z,model card,,
5353,b'Create model card for asafaya/bert-large-arabic',2020-06-29T00:57:48Z,2020-06-29T12:58:31Z,model card,,
5352,b'Create model card for asafaya/bert-mini-arabic',2020-06-29T00:55:17Z,2020-06-29T12:41:42Z,model card,,
5351,b'Create model card for asafaya/bert-medium-arabic',2020-06-29T00:54:53Z,2020-06-29T12:36:01Z,model card,,
5350,b'Move tests/utils.py -> transformers/testing_utils.py',2020-06-29T00:21:14Z,2020-07-01T14:31:18Z,cleanup,,
5349,b'T5 FP16: bad generations from my converted checkpoint',2020-06-29T00:06:51Z,2020-09-05T18:50:00Z,wontfix,,
5348,b'T5 Warning: embeddings are not initialized',2020-06-28T23:34:11Z,2020-06-29T15:55:18Z,,,
5347,b'Training with a large dataset',2020-06-28T23:12:13Z,2020-06-29T20:29:29Z,,"""ValueError","""ValueError: DataLoader with IterableDataset: expected unspecified sampler option, but got sampler=<torch.utils.data.sampler.RandomSampler object at 0x7f777bdbe710>"" "
5346,b'Upload model card with the CLI',2020-06-28T23:03:30Z,2020-06-29T13:36:52Z,,,
5345,b'Massive text generation slowdown when using repetition_penalty param on GPU',2020-06-28T19:05:50Z,2020-09-19T03:01:36Z,wontfix,,
5344,b'[examples] fix example links',2020-06-28T13:57:00Z,2020-06-28T16:54:55Z,,,
5343,b'[Reformer] Simpler reverse sort backward implementation',2020-06-28T12:11:15Z,2020-06-28T12:32:26Z,,**Note**,"**Note**: There was no bug in the code before / the logic of the code has not changed. This PR just makes the backward function of ReverseSort simpler."
5342,b'Added support for XLNet language modelling training in examples',2020-06-28T09:19:27Z,2020-06-29T05:27:30Z,,,
5341,b'In Tensorflow the serving is very slow',2020-06-28T07:43:47Z,2020-07-08T02:04:59Z,,,
5340,"b""GPT2Tokenizer remove the ' ' (space) if it is at the end of text?""",2020-06-28T07:14:56Z,2020-07-08T06:33:33Z,Core: Tokenization,,
5339,b'Predefined tasks in T5',2020-06-28T06:54:46Z,2020-06-29T02:36:21Z,,,
5338,"b'Confuse by  ""All learning rates are 0""'",2020-06-28T03:57:23Z,2020-06-29T06:57:23Z,,,
5337,b'arxiv-ai-gpt2 model card',2020-06-28T00:11:05Z,2020-06-29T12:53:21Z,model card,,
5336,b'BillSum dataset finetuning',2020-06-27T21:08:46Z,2020-10-23T08:08:42Z,wontfix,,
5335,b'BertForPreTraining and BertModel when loading TF checkpoints',2020-06-27T14:21:00Z,2020-06-29T19:56:43Z,,"TypeError, AttributeError","TypeError: forward() got an unexpected keyword argument 'output_hidden_states'AttributeError: 'BertModel' object has no attribute 'bias'"
5334,"b'Add ""labels"" functionality for all TF Causal LM and Masked LM models'",2020-06-27T14:06:03Z,2020-07-07T16:15:54Z,,,
5333,b'XLNet with high CPU usage',2020-06-27T13:15:58Z,2020-09-05T07:35:51Z,wontfix,,
5332,b'Link to the example/summarization in doc is broken',2020-06-27T10:34:49Z,2020-06-27T11:51:14Z,,,
5331,"b'Adds train_batch_size, eval_batch_size, and n_gpu to to_sanitized_dict output for logging.'",2020-06-27T04:13:54Z,2020-08-03T13:00:40Z,,,
5330,b'Better hyperparameter tensorboard logging in Trainer.',2020-06-27T04:11:53Z,2020-08-03T13:00:40Z,,,
5329,b'Add option to keep tb_writer open after training is done.',2020-06-27T04:06:45Z,2020-09-05T07:35:36Z,wontfix,,
5328,b'Adds option to keep tb_writer open after training finishes',2020-06-27T04:05:17Z,2020-11-07T05:42:26Z,wontfix,,
5327,"b'[mBART] skip broken forward pass test, stronger integration test'",2020-06-26T23:18:11Z,2020-06-28T19:08:29Z,translation,,
5326,"b'In the run_ner.py example, give the optional label arg a default value'",2020-06-26T23:07:14Z,2020-06-30T23:45:36Z,,	TypeError,	TypeError: __init__() missing 1 required positional argument: 'labels'
5325,b'Added a model card README.md for my pretrained model.',2020-06-26T22:14:20Z,2020-06-29T08:29:15Z,model card,,
5324,b'More model cards',2020-06-26T21:11:11Z,2020-06-29T09:06:06Z,model card,,
5323,b'New model sharing tutorial',2020-06-26T19:30:10Z,2020-06-27T15:10:03Z,,,
5322,b'examples/seq2seq/run_eval.py fixes and docs',2020-06-26T19:25:23Z,2020-06-26T23:20:43Z,,,
5321,b'[{m}bart] Fix final_logits bias warning',2020-06-26T19:24:12Z,2020-07-27T19:13:15Z,,,
5320,b'Reformer model axial.position.shape config not working',2020-06-26T18:34:19Z,2020-06-27T13:55:14Z,,**RuntimeError,"**RuntimeError: Error(s) in loading state_dict for ReformerForSequenceClassification:"
5319,b'Fix `xxx_length` behavior when using XLNet in pipeline',2020-06-26T18:17:50Z,2020-06-27T15:09:52Z,,,
5318,b'[CI] GH-runner stores artifacts like CircleCI',2020-06-26T18:00:17Z,2020-06-30T19:01:54Z,,,
5317,b'Clearer lr schedule math',2020-06-26T16:58:55Z,2020-09-05T18:49:41Z,wontfix,,
5316,b'[pl_examples] default warmup steps=0',2020-06-26T16:55:27Z,2020-06-26T19:03:42Z,,,
5315,b'Add BART-base modeling and configuration',2020-06-26T16:46:23Z,2020-06-26T16:53:10Z,,,
5314,b'Transformer-XL not working with DistributedDataParallel',2020-06-26T16:34:46Z,2020-09-05T07:35:35Z,wontfix,RuntimeError,"RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by (1) passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`; (2) making sure all `forward` function outputs participate in calculating loss. If you already have done the above two steps, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable). (prepare_for_backward at /opt/conda/conda-bld/pytorch_1579022034529/work/torch/csrc/distributed/c10d/reducer.cpp:514)"
5313,b'Model cards for finance-koelectra models',2020-06-26T15:33:34Z,2020-06-29T05:47:45Z,model card,,
5312,b'Add benchmark notebook',2020-06-26T15:32:27Z,2020-06-26T15:38:14Z,,,
5311,b'AllenNLP SPECTER model',2020-06-26T15:05:32Z,2020-09-05T07:35:50Z,"wontfix, New model",,
5310,b'overflowing tokens are now always returned',2020-06-26T14:47:10Z,2020-06-26T17:48:48Z,,,
5309,b'Links out of date under transformers/examples/README.md',2020-06-26T14:36:23Z,2020-06-28T16:55:02Z,,,
5308,"b'[tokenizers] Updates data processors, docstring, examples and model cards to the new API'",2020-06-26T14:27:14Z,2020-06-26T17:48:14Z,model card,,
5307,b'More model cards',2020-06-26T14:09:04Z,2020-06-29T08:58:56Z,model card,,
5306,b'[Generation] fix docs for decoder_input_ids',2020-06-26T12:14:43Z,2020-06-26T14:58:12Z,,,
5305,b'Does BERT public an embedding file like glove.840B.300d.txt? ',2020-06-26T09:49:08Z,2020-08-27T12:51:21Z,wontfix,,
5304,b'Bert Abs not using GPU',2020-06-26T09:00:35Z,2020-06-26T09:18:43Z,,,
5303,b'Attempted relative import with no known parent package',2020-06-26T08:02:26Z,2020-07-08T14:35:17Z,bertabs,ImportError,"ImportError: attempted relative import with no known parent package`"
5302,"b'Transformer-XL: Fixed tokenization of brackets, numbers etc.'",2020-06-26T08:01:25Z,2020-08-06T16:58:51Z,,,
5301,b'Request for inclusion of PEGASUS for text summarization by Google.',2020-06-26T07:53:14Z,2020-06-26T13:45:33Z,,,
5300,b'T5ForConditionalGeneration fp16 nan loss',2020-06-26T05:55:59Z,2020-06-28T23:16:04Z,,,
5299,b'No documentation for MMBT on official docs',2020-06-26T05:10:50Z,2020-09-11T02:02:47Z,wontfix,,
5298,b'The start and end position of BertForQuestionAnswering',2020-06-26T04:20:59Z,2020-06-26T11:47:05Z,,,
5297,b'Can we have a way for a tokenizer to transform word level or character level annotations?',2020-06-26T01:09:53Z,2020-06-26T19:23:43Z,Core: Tokenization,,
5296,b'Update outdated TensorFlow -> PyTorch model transfer CLI example',2020-06-26T00:02:05Z,2020-11-09T01:55:48Z,wontfix,,
5295,b'Is summing of attention_mask intended?',2020-06-25T22:01:33Z,2020-06-25T22:13:59Z,,,
5294,b'Slow Integration Test for examples/seq2seq/finetune.py',2020-06-25T20:39:29Z,2020-07-27T03:20:44Z,Help wanted,,
5293,"b'run_squad.py :: ValueError: Input [] is not valid. Should be a string, a list/tuple of strings or a list/tuple of integers.'",2020-06-25T20:19:38Z,2020-06-29T18:19:55Z,Core: Tokenization,ValueError,"ValueError: Input [] is not valid. Should be a string, a list/tuple of strings or a list/tuple of integers."
5292,b'Saving and loading tokenizers with torch.save fails',2020-06-25T20:08:39Z,2020-09-05T18:49:53Z,wontfix,,
5291,b'CircleCI stores cleaner output at test_outputs.txt',2020-06-25T19:47:40Z,2020-06-26T17:59:32Z,,,
5290,b'Model with fastest inference?',2020-06-25T19:45:12Z,2020-06-25T20:00:04Z,,,
5289,b'[pipelines] Change summarization default to distilbart-cnn-12-6',2020-06-25T19:31:12Z,2020-06-26T15:43:24Z,Core: Pipeline,,
5288,b'Is there a Longformer For Sequence Classification?',2020-06-25T19:29:17Z,2020-07-24T16:14:15Z,New model,,
5287,b'[tokenizers] Several small improvements and bug fixes',2020-06-25T19:14:49Z,2020-06-25T20:17:15Z,,,
5286,b'save_pretrained on master results in tokenizers that cannot be loaded in v2.11',2020-06-25T18:45:02Z,2020-07-03T11:14:09Z,Core: Tokenization,,
5285,"b""Roberta's Positional Embedding Offset""",2020-06-25T18:27:36Z,2020-11-01T16:29:33Z,wontfix,,
5284,b'Tokenizer batch_encode_plus unexpected behavior ',2020-06-25T18:04:32Z,2020-06-25T19:27:10Z,Core: Tokenization,,
5283,b'Gpt2 model card',2020-06-25T18:03:33Z,2020-06-26T12:08:32Z,model card,,
5282,b'Bart: Instatiate lm_head once without wasting memory',2020-06-25T15:47:33Z,2020-12-09T19:55:25Z,"wontfix, seq2seq",,
5281,b'Segmentation fault when trying to load models',2020-06-25T15:47:18Z,2020-07-30T09:39:06Z,,,
5280,b'Remove links for all docs',2020-06-25T15:05:44Z,2020-06-25T15:45:06Z,,,
5279,b'Add DPR model',2020-06-25T15:05:17Z,2020-07-07T12:56:13Z,,,
5278,b'[dbart] push picture',2020-06-25T14:51:41Z,2020-06-30T10:11:23Z,,,
5277,"b""can't open file 'transformers-cli'""",2020-06-25T14:51:12Z,2020-06-25T15:01:27Z,,,
5276,b'Bert base model card',2020-06-25T14:46:42Z,2020-06-26T12:01:19Z,model card,,
5275,b'Description of how to preprocess text corpus for roBERTa LM training',2020-06-25T14:34:16Z,2021-01-11T11:58:27Z,wontfix,,
5274,b'[examples/seq2seq] more README improvements',2020-06-25T14:11:50Z,2020-06-25T14:13:02Z,,,
5273,"b'I have some problems with the ""bert-large-uncased"" model'",2020-06-25T14:10:34Z,2020-06-26T02:37:10Z,,TypeError,"TypeError: new() received an invalid combination of arguments - got (str, int), but expected one of:"
5272,b'A question about the test accuracy of BERT-based-uncased model on the MNLI dataset',2020-06-25T12:49:35Z,2020-09-12T18:50:10Z,wontfix,,
5271,b'BART finetune.py: model not learning anything',2020-06-25T11:59:11Z,2020-06-26T19:03:41Z,Examples,,
5270,b'Create README.md',2020-06-25T09:27:47Z,2020-06-26T15:14:14Z,model card,,
5269,b'Fix LR decay in TF Trainer',2020-06-25T09:26:53Z,2020-06-29T06:38:33Z,,,
5268,b'Fix LR decay in TF Trainer',2020-06-25T09:11:13Z,2020-06-25T09:12:10Z,,,
5267,b'ValueError in T5 community colab notebook.',2020-06-25T04:54:45Z,2020-06-25T06:06:07Z,,ValueError,"ValueError: Cannot add elem. Use .add() instead."
5266,"b'Finetune T5 on other Dataset, AssertionError: assert tokenized.input_ids.shape[1] == max_length'",2020-06-25T04:42:18Z,2020-08-31T13:11:00Z,"wontfix, Core: Tokenization",,
5265,b'test_torch_fillmask failing on GPU',2020-06-25T03:35:54Z,2020-06-30T19:28:15Z,Help wanted,,
5264,b'Set the number of times to evaluate per epoch when using Trainer',2020-06-25T02:27:12Z,2020-08-31T04:11:04Z,wontfix,,
5263,b'[examples] Verify marian and mbart BLEU scores with examples/seq2seq/run_eval.py',2020-06-25T01:33:46Z,2020-10-16T15:42:21Z,"Examples, translation, marian",,
5262,b'Cannot control wandb metadata when running examples/seq2seq/finetune.py',2020-06-25T01:31:31Z,2020-07-14T09:12:34Z,"seq2seq, wandb",,
5261,b'[proposal] Move tests/utils.py to src/transformers/testing_utils.py so that examples can import',2020-06-25T01:13:04Z,2020-07-01T14:31:18Z,cleanup,,
5260,b'BertTokenizerFast does not support `pad_to_max_length` argument',2020-06-25T00:25:15Z,2020-06-25T16:51:49Z,Core: Tokenization,,
5259,b'Create README.md',2020-06-24T23:39:16Z,2020-06-25T05:56:08Z,model card,,
5258,b'save_pretrained: mkdir(exist_ok=True)',2020-06-24T22:22:13Z,2020-06-28T18:53:48Z,,,
5257,b'Tokenization tutorial',2020-06-24T21:16:54Z,2020-06-24T22:43:20Z,,,
5256,b'RobertaTokenizerFast produces a different output than RobertaTokenizer',2020-06-24T20:52:59Z,2020-06-25T20:17:15Z,Core: Tokenization,,
5255,b'Fix first test',2020-06-24T19:15:59Z,2020-06-24T19:16:05Z,,,
5254,b'Move GenerationMixin to separate file',2020-06-24T19:11:48Z,2020-06-30T14:42:08Z,,,
5253,b'Use master _static',2020-06-24T19:00:41Z,2020-06-24T19:06:15Z,,,
5252,b'[Tokenization] Fix #5181 - make #5155 more explicit - move back the default logging level in tests to WARNING',2020-06-24T16:48:42Z,2020-06-25T15:24:29Z,,,
5251,b'Fix version controller links (for realsies)',2020-06-24T16:12:46Z,2020-06-24T16:13:44Z,,,
5250,b'Fix tensor label type inference in default collator',2020-06-24T15:55:48Z,2020-07-01T16:40:14Z,,,
5249,b'Distilroberta Tokenizer and Encoder not aligning',2020-06-24T15:55:30Z,2020-06-25T21:05:34Z,Core: Tokenization,,
5248,b'Fix links in version selector',2020-06-24T15:30:32Z,2020-06-24T15:35:56Z,,,
5247,b'[WIP] Support label_smoothed_cross_entropy',2020-06-24T15:11:07Z,2020-10-28T22:38:02Z,wontfix,,
5246,b'Fix deploy doc',2020-06-24T14:56:48Z,2020-06-24T14:59:06Z,,,
5245,b'[Benchmarks] improve Example Plotter',2020-06-24T14:42:25Z,2020-06-26T13:00:15Z,,,
5244,b'Add some prints to debug deploy script',2020-06-24T14:33:01Z,2020-06-24T14:37:02Z,,,
5243,"b""Don't recreate old docs""",2020-06-24T13:37:40Z,2020-06-24T13:59:07Z,,,
5242,b'[Benchmark] fix print in benchmark',2020-06-24T13:33:29Z,2020-06-24T13:58:50Z,,,
5241,b'[Benchmark] Extend Benchmark to all model type extensions',2020-06-24T12:49:13Z,2020-06-24T13:11:43Z,,,
5240,b'[WIP] Add \xf0\x9f\xa4\x97nlp in examples using the updated tokenizer API',2020-06-24T12:48:16Z,2020-09-05T18:49:39Z,wontfix,,
5239,b'Multilingual MNLI model',2020-06-24T12:13:35Z,2020-08-30T23:11:33Z,wontfix,,
5238,b'Not Implemented Error ',2020-06-24T12:11:15Z,2020-09-19T14:29:14Z,"wontfix, TensorFlow",,
5237,b'BART(base) - Finetune Is this a bug ? Or I am doing something wrong?',2020-06-24T09:47:05Z,2020-06-24T11:42:13Z,,`TypeError,"`TypeError: special token mask_token has to be either str or AddedTokenFast but got: <class 'dict'>`"
5236,b'Model cards for Hate-speech-CNERG models',2020-06-24T09:33:39Z,2020-06-24T15:41:09Z,model card,,
5235,b'Does to T5 Transformer training scale to multiple GPUs?',2020-06-24T08:54:25Z,2020-08-31T07:11:01Z,wontfix,,
5234,b'Fix model path',2020-06-24T07:17:39Z,2020-08-30T07:47:42Z,wontfix,,
5233,b'Fix PABEE division by zero error',2020-06-24T06:34:18Z,2020-06-24T08:10:37Z,,,
5232,"b'BertTokenizerFast.convert_tokens_to_string converts ids to string, not tokens to string'",2020-06-24T04:53:02Z,2020-06-25T20:17:15Z,Core: Tokenization,TypeError,"TypeError: 'str' object cannot be interpreted as an integer"
5231,b'BertAbs run_summarization.py example fails with errors',2020-06-24T04:20:21Z,2020-08-30T07:47:49Z,wontfix,"ModuleNotFoundError, OSError","ModuleNotFoundError: No module named '__main__.utils_summarization'; '__main__' is not a packageOSError: Can't load config for 'bertabs-finetuned-cnndm'. Make sure that:"
5230,b'Fix convert_graph_to_onnx script',2020-06-24T00:28:26Z,2020-06-25T06:17:03Z,,,
5229,b'Cleaning TensorFlow models',2020-06-24T00:22:24Z,2020-06-24T15:37:20Z,,,
5228,b'Embedding index out of range in self',2020-06-23T23:20:58Z,2020-06-24T09:32:31Z,,,
5227,b'[pl_examples] revert deletion of optimizer_step',2020-06-23T20:20:43Z,2020-06-23T20:40:46Z,,,
5226,b'Self documenting Payload instead of Tuples as output of Transformer',2020-06-23T20:14:26Z,2020-07-10T17:14:31Z,Core: Modeling,,
5225,b'Add hugs',2020-06-23T19:21:37Z,2020-06-24T11:56:15Z,,,
5224,b'Use the script in utils',2020-06-23T18:40:25Z,2020-06-24T11:55:59Z,,,
5223,b'Only put tensors on a device',2020-06-23T18:28:37Z,2020-06-23T21:30:18Z,,,
5222,b'Add version control menu',2020-06-23T18:12:16Z,2020-06-23T21:05:13Z,,,
5221,b'gpt2.generate breaks on FP16 Apex training.',2020-06-23T17:37:18Z,2020-08-31T04:11:04Z,wontfix,,
5220,b'run_language_modeling.py does not output vocab/config/etc files until training completes',2020-06-23T17:05:52Z,2020-12-20T13:34:49Z,wontfix,`OSError,"`OSError: Model name './output_100k/checkpoint-12500' was not found in tokenizers model name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). We assumed './output_100k/checkpoint-12500' was a path, a model identifier, or url to a directory containing vocabulary files named ['vocab.json', 'merges.txt'] but couldn't find such vocabulary files at this path or url.`"
5219,b'[Longformer] Major Refactor',2020-06-23T16:50:28Z,2020-07-01T15:43:33Z,,,
5218,"b""AttributeError: module 'tensorflow' has no attribute 'repeat'""",2020-06-23T16:20:52Z,2020-06-25T22:19:25Z,TensorFlow,AttributeError,"AttributeError: module 'tensorflow' has no attribute 'repeat'"
5217,b'Create README.md',2020-06-23T14:58:27Z,2020-06-24T08:40:51Z,model card,,
5216,"b'[WIP - Don\'t merge yet][Pipeline] Make ""task"" a static class variable'",2020-06-23T13:46:59Z,2020-08-08T15:00:45Z,,,
5215,b'TF2 support for Longformer',2020-06-23T11:47:16Z,2020-06-26T14:29:39Z,,,
5214,b'How to predict on a batch?',2020-06-23T11:46:17Z,2020-06-23T13:31:39Z,,,
5213,b'Train EncoderDecoder Models for question generation',2020-06-23T09:54:21Z,2020-09-13T01:50:08Z,wontfix,,
5212,b'BartConfig wrong decoder_start_token_id?',2020-06-23T09:49:50Z,2021-04-16T15:04:07Z,,,
5211,b'Remove wandb warning as it is unnecessary',2020-06-23T09:11:50Z,2020-10-22T10:39:23Z,wontfix,,
5210,b'Increase the default max_length parameter when using TransfoXL & XLnet.',2020-06-23T09:11:42Z,2020-06-23T23:26:51Z,,,
5209,b'[Reformer] Axial Pos Emb Improve mem usage reformer',2020-06-23T08:42:41Z,2020-06-23T08:49:19Z,,,
5208,b'Train RobertaModel from scratch for my dataset',2020-06-23T07:34:08Z,2020-08-29T19:17:47Z,wontfix,,
5207,b'How to build Bimodel to search code snippets? [CodeBERTa]',2020-06-23T07:04:28Z,2020-09-07T15:57:48Z,"Discussion, wontfix",,
5206,b'[fix] remove unused import',2020-06-23T03:38:33Z,2020-06-23T03:39:05Z,,,
5205,"b'[fix] mobilebert had wrong path, causing slow test failure'",2020-06-23T03:30:32Z,2020-06-23T03:31:37Z,,,
5204,b'T5 Model : What is maximum sequence length that can be used with   pretrained T5 (3b model) checkpoint?',2020-06-23T02:36:22Z,2020-06-23T13:07:56Z,,,
5203,b'Can you release the code for Write For Transformer? ',2020-06-23T02:03:59Z,2020-08-29T09:18:19Z,wontfix,,
5202,b'examples/seq2seq supports translation',2020-06-23T00:59:58Z,2020-06-25T03:58:12Z,"Summarization, seq2seq, Examples, translation",,
5201,b'Linformer',2020-06-22T23:37:30Z,2020-06-23T07:44:34Z,New model,,
5200,b'tokenizer.convert_ids_to_tokens(tokenizer.convert_tokens_to_ids(x)) returning a different result',2020-06-22T23:30:39Z,2020-06-23T12:21:19Z,,,
5199,b'Update README.md',2020-06-22T22:24:51Z,2020-06-24T08:42:47Z,model card,,
5198,b'[Reformer classification head] Implement the reformer model classification head for text classification',2020-06-22T21:54:40Z,2020-07-14T07:16:23Z,,,
5197,"b'batch_encode_plus() causes OOM, while encode_plus does not '",2020-06-22T21:51:48Z,2020-11-07T05:42:51Z,"wontfix, Core: Tokenization",,
5196,b'[HANS] Fix label_list for RoBERTa/BART (class flipping)',2020-06-22T21:32:07Z,2020-06-24T18:38:15Z,,,
5195,b'Add link to new comunity notebook (optimization)',2020-06-22T21:29:02Z,2020-06-22T21:47:34Z,,,
5194,b'[Use cache] Align logic of `use_cache` with output_attentions and output_hidden_states',2020-06-22T20:42:00Z,2020-06-24T14:09:18Z,,,
5193,b'Switch master/stable doc and add older releases',2020-06-22T20:31:10Z,2020-06-22T20:38:54Z,,,
5192,b'Using segments ids in encoder-decoder model in generate function',2020-06-22T20:07:12Z,2020-07-16T16:43:15Z,,,
5191,b'[Benchmark] Jetson Nano DistillBERT SQuAD benchmark ',2020-06-22T19:43:52Z,2020-08-29T19:17:48Z,wontfix,,
5190,b'[bart] add config.extra_pos_embeddings to facilitate reuse',2020-06-22T18:50:26Z,2020-06-23T15:35:43Z,,,
5189,b'Have documentation fail on warning',2020-06-22T18:43:37Z,2020-06-22T19:49:51Z,,,
5188,b'Simplify LearnedPositionalEmbedding',2020-06-22T18:34:15Z,2020-06-26T20:55:05Z,"cleanup, work in progress",,
5187,b'Add TF auto model to the docs + fix sphinx warnings (again)',2020-06-22T18:03:05Z,2020-06-22T18:43:53Z,,,
5186,b'Trouble with PL Checkpoint loading after finetuning bart-large ',2020-06-22T17:28:19Z,2020-09-05T18:49:57Z,"wontfix, Summarization, Examples",,
5185,b'[T5] add missing docstring for some configurations',2020-06-22T16:49:11Z,2020-06-22T17:00:12Z,,,
5184,b'More clear error message in the use-case of #5169',2020-06-22T15:22:28Z,2020-06-23T11:37:30Z,,,
5183,"b'Unable to load the reformer pre-trained model, connection broken after X%'",2020-06-22T15:14:42Z,2020-09-19T03:01:35Z,wontfix,ChunkedEncodingError,"ChunkedEncodingError: ('Connection broken: OSError(""(10054, \'WSAECONNRESET\')"",)', OSError(""(10054, 'WSAECONNRESET')"",))"
5182,b'MarianTokenizer.prepare_translation_batch uses new tokenizer API',2020-06-22T14:50:06Z,2020-07-01T14:32:50Z,,,
5181,b'Is it possible to mimic trim_batch using new tokenizer strategies?',2020-06-22T14:46:02Z,2020-06-25T15:24:29Z,Core: Tokenization,AssertionError,"AssertionError: 3002"
5180,b'Which Marian version was used to train the Helsinki-NLP/* checkpoints?',2020-06-22T14:04:10Z,2020-09-05T07:35:50Z,"wontfix, marian",,
5179,b'Model card for t5-base-finetuned-emotion (recognition)',2020-06-22T12:48:27Z,2020-06-22T17:45:46Z,model card,,
5178,"b""Add model cards for Microsoft's MiniLM""",2020-06-22T12:42:22Z,2020-06-22T13:48:15Z,model card,,
5177,b'When is 2.12 coming out?',2020-06-22T10:52:14Z,2020-06-29T18:48:09Z,,,
5176,"b'object returned by  RobertaTokenizerFast() class are not serializable,'",2020-06-22T10:31:31Z,2020-06-24T13:01:58Z,"Core: Tokenization, Fast Tokenizers",,
5175,b'Update model card for COVID-QA model',2020-06-22T08:57:20Z,2020-06-22T22:26:13Z,model card,,
5174,b'Add README.md (nyu-mll)',2020-06-22T06:33:31Z,2020-06-22T21:24:28Z,model card,,
5173,"b'Trying to made a keras model with transformer layers defined in hf-transformers, keep running into `AttributeError: Tensor.op is meaningless when eager execution is enabled, when trying to make a keras model`'",2020-06-21T22:32:02Z,2020-08-29T09:18:13Z,wontfix,AttributeError,"AttributeError: Tensor.op is meaningless when eager execution is enabled."
5172,"b""Load a T5ForConditionalGeneration's encoder into a T5Model""",2020-06-21T21:20:20Z,2020-06-22T15:51:18Z,,,
5171,b'Fixing docs for Encoder Decoder Config',2020-06-21T21:15:31Z,2020-06-22T08:51:17Z,,,
5170,b'Add support for `encoder_hidden_states` and `encoder_attention_mask` in modeling_longformer',2020-06-21T18:51:37Z,2020-09-15T02:32:47Z,wontfix,,
5169,b'[Tokenizer] batch_encode_plus method cannot encode List[Tuple[str]] with is_pretokenized=True',2020-06-21T18:13:17Z,2020-06-23T11:37:30Z,Core: Tokenization,ValueError,"ValueError: Input is not valid. Should be a string, a list/tuple of strings or a list/tuple of integers."
5168,b'Summarization Examples: Support label_smoothed_cross_entropy',2020-06-21T16:39:17Z,2020-08-21T15:57:17Z,"Help wanted, Summarization, Examples",,
5167,b'results for wikitext-2 clm using GPT-2 differ between paper and example code',2020-06-21T15:23:26Z,2020-06-24T13:02:51Z,,,
5166,b'Cannot import AutoModelForSeq2SeqLM ',2020-06-21T13:57:43Z,2020-06-26T11:42:13Z,,ImportError,"ImportError: cannot import name 'AutoModelForSeq2SeqLM' from 'transformers' (/anaconda3/envs/transformers/lib/python3.8/site-packages/transformers/__init__.py)"
5165,b'Create README.md',2020-06-21T12:34:34Z,2020-06-22T17:49:14Z,model card,,
5164,b'output generate scores per hypothesis/token',2020-06-21T12:34:29Z,2021-04-25T15:05:08Z,,,
5163,"b""Why doesn't stride in squad_convert_example_to_features\xe2\x80\x98s encode_plus set to doc_stride?""",2020-06-21T05:39:35Z,2020-08-27T12:50:33Z,"wontfix, Core: Tokenization",,
5162,b'How to  save the created embedding of the text corpus.',2020-06-21T03:14:06Z,2020-08-29T09:18:08Z,wontfix,,
5161,b'Keras model created from individual Bert Layers has weights not shown in trainable_weights nor non_trainable_weights. model.summary() / utils.plot_model shows those weights as part of graph though',2020-06-20T21:57:51Z,2020-08-29T09:18:10Z,wontfix,,
5160,b'Create README.md',2020-06-20T18:26:48Z,2020-06-22T21:59:55Z,model card,,
5159,b'Transformer pipeline loading model and tokenizer on every prediction request',2020-06-20T16:17:19Z,2020-11-14T09:27:01Z,wontfix,,
5158,"b""Fix PABEE's result table""",2020-06-20T12:16:04Z,2020-06-20T14:56:40Z,,,
5157,b'[examples] fixes arguments for summarization finetune scripts',2020-06-20T11:11:03Z,2020-06-21T15:51:22Z,,,
5156,"b'BertForMaskedLM ""labels"" is an unexpected keyword'",2020-06-20T08:47:11Z,2020-06-23T11:53:06Z,,,
5155,b'new tokenizer backend breaks old code',2020-06-20T08:46:26Z,2020-06-29T18:39:37Z,Core: Tokenization,,
5154,b'Lite transformer',2020-06-20T06:35:28Z,2020-08-29T09:18:14Z,"wontfix, New model",,
5153,b'Create README.md',2020-06-20T05:27:12Z,2020-06-23T14:56:47Z,model card,,
5152,b'Create README.md',2020-06-20T03:32:05Z,2020-06-22T21:59:34Z,model card,,
5151,"b""TFBertForSequenceClassification: TypeError: call() got an unexpected keyword argument 'labels'""",2020-06-20T03:28:30Z,2020-06-29T19:53:25Z,,,
5150,b'[MobileBert] fix dropout',2020-06-20T03:18:31Z,2020-06-20T05:21:20Z,,,
5149,b'Create README.md',2020-06-20T03:17:13Z,2020-06-22T22:00:54Z,model card,,
5148,b'Update glossary',2020-06-19T21:33:49Z,2020-06-22T12:30:50Z,,,
5147,b'Typo in Reformer model card',2020-06-19T20:25:07Z,2020-06-22T08:49:23Z,model card,,
5146,b'Upgrade examples to pl=0.8.1',2020-06-19T19:43:34Z,2020-06-23T00:40:11Z,,,
5145,b'Quick tour',2020-06-19T19:31:19Z,2020-06-22T20:08:09Z,,,
5144,b'Scoring each word from the sentence using Pretrained LM',2020-06-19T19:00:05Z,2020-06-25T22:00:50Z,,,
5143,b'Passing in own embeddings for image input',2020-06-19T18:40:14Z,2020-06-22T22:23:00Z,,,
5142,b'T5 special tokens not mapped to unique indices in vocabulary',2020-06-19T17:10:47Z,2020-11-10T17:54:18Z,Core: Tokenization,,
5141,b'[bart-mnli] Fix class flipping bug',2020-06-19T16:57:45Z,2020-06-19T17:33:25Z,,,
5140,b'[pl_examples] deprecate BaseTransformer.is_logger',2020-06-19T16:35:03Z,2020-06-19T19:42:53Z,,,
5139,b'`facebook/bart-large-mnli` - random accuracy on MNLI',2020-06-19T14:30:40Z,2020-06-19T17:33:25Z,,,
5138,b'Fix in Reformer Config documentation',2020-06-19T13:17:24Z,2020-06-19T13:41:32Z,,,
5137,b'xlm-mlm-17-1280: after run model to get embeddings shape 20000',2020-06-19T12:45:43Z,2020-06-24T16:30:12Z,,,
5136,b'Transformer-XL tokenizer cannot properly tokenize brackets',2020-06-19T10:20:08Z,2020-08-28T13:56:18Z,Core: Tokenization,,
5135,"b'src/transformers/trainer.py relies on path to infer global training steps, skips training for glue example'",2020-06-19T08:25:39Z,2020-11-01T16:29:36Z,wontfix,,
5134,b'What does adjust_logits_during_generation (formerly prepare_logits_for_generation) do?',2020-06-19T07:59:32Z,2020-07-29T01:34:53Z,Discussion,,
5133,b'How to find and use fine-tuned model for GLUE-CoLA? ',2020-06-19T05:23:55Z,2020-06-22T10:21:33Z,,,
5132,b'fix bart doc',2020-06-19T04:03:46Z,2020-06-22T08:58:29Z,,,
5131,b'Update BERT-of-Theseus model card to avoid confusion',2020-06-19T03:56:37Z,2020-06-20T02:13:34Z,model card,,
5130,b'added subtitle for recent contributors in readme',2020-06-19T02:52:49Z,2020-06-29T13:05:09Z,,,
5129,"b'Add mbart-large-cc25, support translation finetuning'",2020-06-19T00:35:45Z,2020-07-07T17:23:02Z,"seq2seq, translation",,
5128,b'Pin `sphinx-rtd-theme`',2020-06-18T22:03:41Z,2020-06-18T22:07:59Z,,,
5127,b'Cached feature files - Naming introduces confusion/model mixing',2020-06-18T21:04:42Z,2020-08-27T05:17:30Z,wontfix,,
5126,b'[fix] Move _adjust_logits above postprocess to fix Marian.generate',2020-06-18T20:56:04Z,2020-06-18T22:06:28Z,,,
5125,b'[tokenizers] Fix #5081 and improve backward compatibility',2020-06-18T20:54:02Z,2020-06-22T15:25:44Z,,,
5124,b'SLOW GPU test Failures',2020-06-18T20:33:26Z,2020-06-18T22:06:28Z,marian,,
5123,b'Fine-Tuning GPT2',2020-06-18T20:21:45Z,2020-06-18T22:55:30Z,,,
5122,b'Fix #5114',2020-06-18T20:18:36Z,2020-06-18T23:20:05Z,,,
5121,b'AutoTokenizer supports mbart-large-en-ro',2020-06-18T19:48:07Z,2020-06-19T00:47:38Z,,,
5120,"b""AutoTokenizer.from_pretrained('facebook/mbart-large-en-ro') fails""",2020-06-18T19:12:19Z,2020-06-19T00:47:38Z,,,
5119,b'[cleanup] remove redundant code in SummarizationDataset',2020-06-18T18:57:02Z,2020-06-19T00:34:49Z,,,
5118,"b""UnboundLocalError: local variable 'next_tokens' referenced before assignment when using Generate()""",2020-06-18T17:58:03Z,2020-07-03T17:25:26Z,,UnboundLocalError,"UnboundLocalError: local variable 'next_tokens' referenced before assignment"
5117,b'An Implementation of ERNIE For Language Understanding (including Pre-training models and Fine-tuning tools) ',2020-06-18T16:42:05Z,2020-08-20T08:50:42Z,"wontfix, New model",,
5116,b'support local_files_only option for tf models',2020-06-18T16:33:08Z,2020-06-18T17:47:06Z,,,
5115,b'[cleanup] generate_beam_search comments',2020-06-18T16:26:08Z,2020-06-18T20:30:24Z,cleanup,,
5114,b'data_collator.py does not allow NoneType labels for test set predictions on Glue',2020-06-18T15:28:43Z,2020-06-18T23:20:05Z,,TypeError,"TypeError: must be real number, not NoneType`"
5113,b'GPU out of memory with Reformer enwik8 model',2020-06-18T15:24:41Z,2020-08-30T23:11:33Z,wontfix,,
5112,b'Strange exception',2020-06-18T15:14:42Z,2020-08-29T09:18:05Z,wontfix,TypeError,"TypeError: string indices must be integers"
5111,"b""[Marian] Run predictions on GPU? RuntimeError: Expected object of device type cuda but got device type cpu for argument #3 'index' in call to _th_index_select`  """,2020-06-18T15:07:34Z,2020-06-18T15:42:21Z,,RuntimeError,"RuntimeError: Expected object of device type cuda but got device type cpu for argument #3 'index' in call to _th_index_select`"
5110,b'XLMForMultipleChoice',2020-06-18T14:57:42Z,2020-08-24T11:26:01Z,wontfix,,
5109,b'Flaky tests sometimes caused by S3 failures',2020-06-18T14:43:28Z,2020-09-07T04:57:47Z,wontfix,,
5108,b'Create README.md',2020-06-18T14:31:16Z,2020-06-24T08:45:52Z,model card,,
5107,b'Create README.md',2020-06-18T13:40:25Z,2020-06-22T17:47:30Z,model card,,
5106,b'How can I initialize RobertaForSequenceClassification empty?',2020-06-18T13:31:51Z,2020-06-19T03:21:56Z,,`IndexError,"`IndexError: index out of range in self` on the RAM while on GPU I am getting `RuntimeError: CUDA error: an illegal memory access was encountered` . I have followed other [issues ](https://github.com/pytorch/pytorch/issues/21819) but of no help. My understanding since I am creating my own vocabulary, some of the tokens are not in pre-trained model. So is there a way to initialize RobertaForSequenceClassification empty or to train this classification model for my dataset?"
5105,b'Is there a helper script to randomly mask spans of text for T5 pretraining?',2020-06-18T13:09:50Z,2020-06-18T14:43:58Z,,,
5104,"b""Trainer evaluation doesn't return eval loss for question-answering.""",2020-06-18T12:20:34Z,2020-08-27T05:17:29Z,wontfix,,
5103,b'Tokenizers API developments',2020-06-18T11:27:08Z,2020-06-23T11:36:58Z,,,
5102,b'Loading Fine Tuned BERT Sequence Model after Training ',2020-06-18T09:52:02Z,2020-08-24T12:11:10Z,wontfix,RuntimeError,"RuntimeError: Error(s) in loading state_dict for DataParallel:"
5101,"b'OpenAIGPTDoubleHeadsModel Don\'t have the ""labels"" attributes as it is described in the documentation'",2020-06-18T09:36:47Z,2020-06-18T09:55:25Z,,TypeError,"TypeError: forward() got an unexpected keyword argument 'labels' ``"
5100,b'Update Conda Release',2020-06-18T08:32:56Z,2021-01-11T11:58:29Z,wontfix,,
5099,b'Fix TF WarmUp class',2020-06-18T05:16:29Z,2020-06-18T06:37:48Z,,,
5098,b'\xf0\x9f\x90\x9b [TF] `create_optimizer` wrong superposition of learning rate schedules',2020-06-18T05:08:37Z,2020-07-02T07:18:41Z,,,
5097,b'Training the BERTSUM model',2020-06-18T05:02:07Z,2020-06-24T15:08:48Z,,,
5096,b'Can I training a bart model from scratch by transformers?',2020-06-18T04:46:37Z,2020-06-18T08:23:22Z,,,
5095,b'Addition of VisualBERT',2020-06-17T21:59:28Z,2020-09-05T07:35:33Z,"wontfix, New model",,
5094,b'Different output from model on CPU and GPU',2020-06-17T21:03:53Z,2020-06-24T14:58:08Z,,,
5093,b'[style] add pandas to setup.cfg',2020-06-17T20:33:02Z,2020-06-17T20:39:18Z,,,
5092,b'[MarianTokenizer] Switch to sacremoses for punc normalization',2020-06-17T20:22:50Z,2020-06-17T20:31:06Z,marian,,
5091,b'encode_plus wrongly tokenizing a symbol',2020-06-17T20:14:07Z,2020-09-24T04:15:49Z,wontfix,,
5090,b'minor spelling correction in script execution command - movement pruning',2020-06-17T20:06:43Z,2020-06-17T20:08:43Z,,,
5089,b'Is there a helper script to preprocess data for T5 for masked language modeling? ',2020-06-17T19:56:48Z,2020-08-29T09:18:16Z,wontfix,,
5088,b'Image GPT',2020-06-17T19:10:43Z,2020-11-01T03:10:16Z,"wontfix, New model",,
5087,"b""Why does the T5Tokenizer prepend and '_' to every token?""",2020-06-17T18:25:36Z,2020-06-17T19:44:46Z,,,
5086,b'SummarizationPipeline: init required task name',2020-06-17T18:01:45Z,2020-06-20T07:16:31Z,,,
5085,b'Add missing arg in 02-transformers notebook',2020-06-17T15:38:29Z,2020-06-18T23:04:05Z,,,
5084,b'Update installation page and add contributing to the doc',2020-06-17T14:55:47Z,2020-06-17T18:01:11Z,,,
5083,b'updated hans eval instructions',2020-06-17T14:08:39Z,2020-06-17T14:18:53Z,,,
5082,b'Add header and fix command',2020-06-17T14:05:37Z,2020-06-17T15:45:06Z,,,
5081,b'01_how-to-train.ipynb broken',2020-06-17T12:54:37Z,2020-06-22T15:25:46Z,,Exception,"Exception: Is a directory (os error 21)"
5080,b'[docs] fix T5 training doc',2020-06-17T11:28:41Z,2020-06-18T07:16:30Z,,,
5079,b'How do I pre-train the T5 model in HuggingFace library using my own text corpus?',2020-06-17T10:41:14Z,2020-09-06T07:49:29Z,wontfix,,
5078,b'Add BERT Loses Patience (Patience-based Early Exit)',2020-06-17T10:07:35Z,2020-06-20T05:41:47Z,Ex: Sequence Classification,,
5077,b'Several problems with named entites predicted with the ner pipeline',2020-06-17T09:39:01Z,2020-10-11T11:26:04Z,wontfix,,
5076,b'Colab session crashes on transformers',2020-06-17T08:16:31Z,2020-08-23T22:11:10Z,wontfix,,
5075,"b""Converting to ONNX doesn't apply to all models""",2020-06-17T08:01:56Z,2020-09-09T08:56:40Z,,,
5074,b'how to get complete URLs to weights in 2.11.0',2020-06-17T07:48:44Z,2020-06-18T05:51:26Z,,,
5073,b'fix typo',2020-06-17T06:37:55Z,2020-06-20T15:00:04Z,,,
5072,b'\xf0\x9f\x90\x9b [TFTrainer] Wrong number of optimization steps',2020-06-17T01:23:18Z,2020-08-23T11:29:40Z,wontfix,,
5071,b'glue.py Data Processor Index Error for Large Data',2020-06-17T00:36:57Z,2020-06-17T18:28:06Z,,IndexError,"IndexError: list index out of range"
5070,b'Errors while running pytest',2020-06-16T21:08:30Z,2020-09-11T02:02:50Z,wontfix,,
5069,b'Typo',2020-06-16T20:44:17Z,2020-06-16T20:46:21Z,,,
5068,b'Fix all sphynx warnings',2020-06-16T20:31:50Z,2020-06-16T20:50:03Z,,,
5067,b'Modify BERT/BERT-descendants to be TorchScript-able (not just traceable)',2020-06-16T19:32:10Z,2021-01-02T04:25:59Z,wontfix,,
5066,b'Tokenization+Transformers works with PyTorch but not TensorFlow on TPU',2020-06-16T19:22:13Z,2020-11-14T09:26:57Z,"wontfix, Core: Tokenization",,
5065,b'[WIP] TF Trainer with TPUs',2020-06-16T18:17:30Z,2020-09-05T18:49:40Z,wontfix,,
5064,b'Reorganize documentation',2020-06-16T17:11:05Z,2020-06-17T11:55:21Z,,,
5063,b'Non-deterministic training issue on GPU: TF-BERT',2020-06-16T16:31:06Z,2020-08-24T02:11:10Z,wontfix,,
5062,b'What do the following parameters mean during the initialization of T5 model?',2020-06-16T15:54:45Z,2020-06-22T16:53:09Z,,,
5061,b'More flexible wandb support for Trainer',2020-06-16T15:31:14Z,2020-07-02T22:28:07Z,trainer,,
5060,b'Make default_data_collator more flexible and deprecate old behavior',2020-06-16T13:28:18Z,2020-06-17T19:24:52Z,,,
5059,b'[cleanup] examples test_run_squad uses tiny model',2020-06-16T12:47:53Z,2020-06-16T18:06:47Z,,,
5058,b'Error when loading Flaubert model ',2020-06-16T12:47:27Z,2020-08-22T23:29:40Z,wontfix,"AttributeError, OSError","AttributeError: 'NoneType' object has no attribute 'seek'OSError: Unable to load weights from pytorch checkpoint file. If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True. "
5057,b'Examples tests improvements',2020-06-16T12:45:32Z,2020-10-04T01:14:08Z,"Help wanted, wontfix, Examples, Good First Issue, cleanup",,
5056,b'Add more tests on tokenizers serialization - fix bugs ',2020-06-16T11:55:41Z,2020-06-24T19:53:09Z,,,
5055,b'How can I load the finetuned BART model to memory?',2020-06-16T11:47:57Z,2020-06-16T11:59:04Z,,,
5054,b'Add pad_to_multiple_of on tokenizers (reimport)',2020-06-16T11:37:40Z,2020-06-26T09:55:58Z,,,
5053,"b""T5 model for classification doesn't work properly for large number of classes.""",2020-06-16T10:56:30Z,2020-08-23T13:29:42Z,wontfix,,
5052,b'How to cosume movement-pruning .h5 models in QnA pipeline',2020-06-16T08:58:26Z,2020-10-01T07:08:08Z,wontfix,,
5051,b'Fix LR decay in TF Trainer',2020-06-16T08:44:58Z,2020-06-24T17:24:24Z,,,
5050,"b'TypeError: function() argument 1 must be code, not str'",2020-06-16T08:23:51Z,2020-06-16T22:57:27Z,,TypeError,"TypeError: function() argument 1 must be code, not str"
5049,b'DataCollator problem',2020-06-16T08:09:59Z,2020-06-17T19:24:52Z,,,
5048,"b'After I resume learning, loss is greater than prev checkpoint'",2020-06-16T05:52:31Z,2020-08-29T09:18:15Z,wontfix,,
5047,b'How to use 16 token types in pretrained Albert/BERT?',2020-06-16T03:03:49Z,2020-11-01T03:11:00Z,wontfix,,
5046,b'ref #4733',2020-06-16T03:00:37Z,2020-06-22T10:02:02Z,,tensorflow.python.framework.errors_impl.FailedPreconditionError,"tensorflow.python.framework.errors_impl.FailedPreconditionError:  Error while reading resource variable _AnonymousVar189 from Container: localhost. This could mean that the variable was uninitialized. Not found: Resource localhost/_AnonymousVar189/class tensorflow::Var does not exist."
5045,b'TFTrainer does not consider number of epochs when calculating learning rate',2020-06-16T01:55:05Z,2020-07-08T19:21:27Z,,,
5044,b'refactor(wandb): consolidate import',2020-06-16T01:43:03Z,2020-06-16T07:40:44Z,,,
5043,b'Fix marian tokenizer save pretrained',2020-06-16T00:18:18Z,2020-06-16T13:48:20Z,marian,,
5042,b'\xe2\x9d\x93 [TFTrainer] How to run on 8 TPU cores ?',2020-06-16T00:08:22Z,2020-06-17T00:31:04Z,TensorFlow,,
5041,"b'How can I use tokenizer.encode_plus to input and encode 2 sentences - (query,answer) pair for training a BERT binary classifier?'",2020-06-15T23:37:12Z,2020-08-29T09:18:07Z,wontfix,,
5040,"b'""AutoTokenizer.from_pretrained"" does not work when loading a pretrained MarianTokenizer from a local directory'",2020-06-15T22:51:09Z,2020-06-16T13:48:20Z,marian,,
5039,b'Ability to pickle/unpickle BatchEncoding pickle (reimport)',2020-06-15T22:38:12Z,2020-06-16T07:25:26Z,,,
5038,b'Cannot save and load pretrained MarianTokenizer',2020-06-15T22:25:22Z,2020-06-15T22:51:20Z,,,
5037,b'The correct way to save and load pretrained MarianTokenizer?',2020-06-15T22:23:40Z,2020-06-15T22:51:34Z,,,
5036,b'Refactor Code samples; Test code samples',2020-06-15T22:17:06Z,2020-06-25T20:46:00Z,,,
5035,b'update for roberta and xlm',2020-06-15T22:04:31Z,2020-08-22T05:34:05Z,wontfix,,
5034,b'Training & fine-tuning quickstart',2020-06-15T21:56:07Z,2020-06-25T21:11:11Z,,,
5033,b'update for roberta and xlm',2020-06-15T21:52:41Z,2020-06-15T22:01:09Z,,,
5032,b'Add DistilBertForMultipleChoice',2020-06-15T21:40:51Z,2020-06-15T22:31:41Z,,,
5031,b'Some changes to simplify the generation function',2020-06-15T20:53:45Z,2020-06-17T18:48:06Z,,,
5030,b'Update pipeline examples to doctest syntax',2020-06-15T20:28:44Z,2020-06-16T22:14:58Z,,,
5029,b'Add reference to NLP (package) dataset',2020-06-15T20:23:30Z,2020-06-16T08:17:47Z,model card,,
5028,b'Add reference to NLP dataset',2020-06-15T20:19:35Z,2020-06-16T08:19:09Z,model card,,
5027,b'Remove old doc page and add note about cache in installation',2020-06-15T20:10:07Z,2020-06-16T17:03:42Z,,,
5026,b'Error while trying to retrieve BERT embeddings for a custom task',2020-06-15T19:28:43Z,2020-08-29T09:18:18Z,wontfix,RuntimeError,"RuntimeError: index out of range at /pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp:193"
5025,b'Convert hans to Trainer',2020-06-15T19:27:49Z,2020-06-16T12:06:32Z,,,
5024,b'[Bart] Question Answering Model is added to tests',2020-06-15T18:51:38Z,2020-06-15T20:50:09Z,,,
5023,b'Multi class classification using Reformer Model',2020-06-15T18:09:10Z,2020-08-05T20:32:12Z,,,
5022,b'Latest merge [Benchmark] Memory benchmark utils #4198 fails at Windows',2020-06-15T17:09:51Z,2020-06-15T17:36:58Z,,,
5021,b'Add position_ids in TFElectra models docstring',2020-06-15T16:54:38Z,2020-06-15T19:50:17Z,,,
5020,b'Run error when run PPLM in example!',2020-06-15T16:53:13Z,2020-06-25T08:31:48Z,,,
5019,"b""Not able to reproduce XNLI results from Google's mBERT weights.""",2020-06-15T16:52:08Z,2020-08-22T05:34:19Z,wontfix,,
5018,"b""bart-large-xsum config task_specific_params['summarization_params'] wrong""",2020-06-15T16:38:07Z,2020-06-15T16:41:03Z,seq2seq,,
5017,b'Add CRF layer after Transformer model',2020-06-15T15:25:37Z,2020-08-23T07:29:42Z,wontfix,,
5016,b'Cached TF files cannot be loaded with `from_pretrained` without internet connection',2020-06-15T15:21:15Z,2020-10-01T09:06:03Z,,OSError,"OSError: Can't load weights for 'bert-base-uncased'. Make sure that:"
5015,b'Make DataCollator a callable',2020-06-15T14:56:47Z,2020-06-15T15:58:34Z,,,
5014,b'Add bart-base',2020-06-15T14:42:57Z,2020-06-15T17:29:27Z,,,
5013,b'[Modelcard] xlm-roberta-squadv2',2020-06-15T14:38:31Z,2020-06-22T22:40:00Z,model card,,
5012,b'Raise errors that are not raised now',2020-06-15T14:15:45Z,2020-08-22T13:26:28Z,wontfix,,
5011,b'[Modelcard] bart-squadv2',2020-06-15T13:50:02Z,2020-06-22T22:40:20Z,model card,,
5010,b'How can I print output attention on test data?',2020-06-15T12:16:48Z,2020-06-15T20:02:34Z,,,
5009,b'Create README.md for finetuned BERT model',2020-06-15T12:05:12Z,2020-06-22T22:39:30Z,model card,,
5008,b'Add model card for StackOBERTflow-comments-small',2020-06-15T11:26:00Z,2020-06-22T22:39:23Z,model card,,
5007,b'Create README.md',2020-06-15T11:22:57Z,2020-06-15T11:49:28Z,model card,,
5006,b'[Ignore] Added data collator for XLNet and added related calls',2020-06-15T10:42:13Z,2020-06-15T10:42:48Z,,,
5005,b'Increase pipeline support for ONNX export.',2020-06-15T10:10:08Z,2020-06-15T17:13:59Z,,,
5004,b'[\xf0\x9f\x9a\x80 Feature request] upload bart-base checkpoint',2020-06-15T09:57:58Z,2020-06-15T17:29:27Z,,,
5003,b'Cannot import transformers due to issue with signal.py (SIGKILL)',2020-06-15T09:48:02Z,2020-06-15T09:49:34Z,,**ImportError,"**ImportError: cannot import name 'SIGKILL' from 'signal' (C:\Users\15194\anaconda3\envs\ai_env\lib\signal.py)**"
5002,b'Illegal memory access (cudaErrorIllegalAddress)',2020-06-15T08:57:41Z,2020-08-22T05:34:20Z,"wontfix, dependencies",,
5001,b'Segmentation fault when loading pretrained file',2020-06-15T08:52:56Z,2020-06-16T21:38:18Z,,,
5000,b'Accessing scores for the entire vocabulary in GPT2',2020-06-15T08:28:26Z,2020-06-22T14:39:23Z,,,
4999,b'Improve ONNX logging',2020-06-15T07:58:34Z,2020-06-15T09:04:52Z,,,
4998,b'Remove deprecation warning TF2.2',2020-06-15T07:45:24Z,2020-06-18T04:03:56Z,,,
4997,b'Fix importing transformers on Windows - SIGKILL not defined',2020-06-15T07:29:23Z,2020-06-15T17:36:58Z,,,
4996,b'\xe2\x9d\x93 How to use TFTrainer on TPU ? Unable to destroy remote tensor handles',2020-06-15T05:45:32Z,2020-08-22T05:34:19Z,wontfix,tensorflow.python.framework.errors_impl.OutOfRangeError,"tensorflow.python.framework.errors_impl.OutOfRangeError: 4 root error(s) found."
4995,b'append keyword arguments to the output',2020-06-15T05:34:00Z,2020-08-22T02:06:58Z,wontfix,,
4994,b'\xf0\x9f\x90\x9b TFTrainer not working on TPU (TF2.2)',2020-06-15T03:09:14Z,2020-08-22T19:29:39Z,"wontfix, TensorFlow",>TypeError,">TypeError: Failed to convert object of type <class 'transformers.optimization_tf.AdamWeightDecay'> to Tensor. Contents: <transformers.optimization_tf.AdamWeightDecay object at 0x7faddc7cfe80>. Consider casting elements to a supported type."
4993,b'Importing transformers causes segmentation fault when setting cuda device',2020-06-15T03:02:35Z,2020-09-05T18:49:54Z,wontfix,,
4992,b'[TFTrainer] Tensorflow Warning : experimental_run_v2 is deprecated',2020-06-15T00:43:46Z,2020-06-22T14:27:44Z,,,
4991,b'evaluating with trainer.py with TPU results in sudden RAM spike and crash',2020-06-14T21:54:31Z,2020-08-22T02:07:50Z,wontfix,RuntimeError,"RuntimeError: CUDA out of memory. Tried to allocate 6.75 GiB (GPU 0; 15.90 GiB total capacity; 7.99 GiB already allocated; 6.40 GiB free; 8.81 GiB reserved in total by PyTorch)"
4990,b'Save the Dataset for training GPT2',2020-06-14T19:43:54Z,2020-08-22T02:07:48Z,wontfix,,
4989,b'attention',2020-06-14T17:01:14Z,2020-06-22T14:25:55Z,,,
4988,b'error while instantiating model',2020-06-14T16:07:04Z,2020-08-22T02:07:46Z,wontfix,"AttributeError, OSError","AttributeError: 'NoneType' object has no attribute 'seek'OSError: Unable to load weights from pytorch checkpoint file. If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True."
4987,b'Fix Inconsistent NER Grouping (Pipeline)',2020-06-14T14:07:40Z,2020-07-08T20:18:18Z,,,
4986,"b'BertTokenizer: ValueError: Input is not valid. Should be a string, a list/tuple of strings or a list/tuple of integers.'",2020-06-14T12:33:57Z,2020-06-14T14:55:25Z,,ValueError,"ValueError: Input is not valid. Should be a string, a list/tuple of strings or a list/tuple of integers."
4985,b'Word Embedding input to GPT-2',2020-06-14T11:55:58Z,2020-06-22T02:21:53Z,,,
4984,b'FillMaskPipeline return word-piece ',2020-06-14T09:29:37Z,2020-08-29T09:18:11Z,wontfix,,
4983,b'Getting very bad F1 Scores when training SQUAD v2.0 with robertadistil-base',2020-06-14T08:57:15Z,2020-08-22T02:07:56Z,wontfix,,
4982,b'Why run_language_modelling.py does not use segment embeddings or language embeddings?',2020-06-14T08:07:07Z,2020-08-22T02:07:53Z,wontfix,,
4981,b'Create README.md',2020-06-14T03:35:35Z,2020-06-14T03:35:46Z,,,
4980,b'keras',2020-06-13T17:23:08Z,2020-06-13T17:23:16Z,,,
4979,b'Add Code Coverage and Black badges to README',2020-06-13T16:02:25Z,2020-08-22T02:06:59Z,wontfix,,
4978,b'Output hidden states',2020-06-13T12:37:57Z,2020-06-22T14:10:45Z,Core: Modeling,,
4977,b'[model card] model card for bart-large-finetuned-squadv1',2020-06-13T11:01:14Z,2020-06-15T09:39:42Z,model card,,
4976,"b""Fix parameter 'output_attentions' docstring""",2020-06-13T10:30:46Z,2020-06-14T19:36:14Z,,,
4975,b'Create README.md',2020-06-13T07:51:02Z,2020-06-15T12:43:51Z,model card,,
4974,b'Patch 4',2020-06-13T07:44:20Z,2020-06-13T07:48:04Z,model card,,
4973,b'How to make my own dataset to use BART summarization?',2020-06-13T06:49:05Z,2020-06-13T16:47:45Z,,,
4972,b'Run run_tf_glue.py has bugs',2020-06-13T06:44:51Z,2020-06-27T14:57:33Z,,tensorflow.python.framework.errors_impl.NotFoundError,"tensorflow.python.framework.errors_impl.NotFoundError: /home/admin/tensorflow_datasets/glue/cola/1.0.0/glue-train.tfrecord-00000-of-00001; No such file or directory [Op:IteratorGetNextSync]"
4971,b'huggingface distillbert classification using multiprocessing',2020-06-13T04:30:15Z,2020-06-13T06:16:21Z,,,
4970,b'Handle unexpected weights in checkpoint loading (tf to pytorch)',2020-06-13T04:20:02Z,2020-09-03T03:50:47Z,wontfix,,
4969,"b'Request: pretrained distilgpt2-medium, distilgpt2-large models'",2020-06-12T23:34:22Z,2020-08-22T05:34:22Z,wontfix,,
4968,b'Eli5 examples',2020-06-12T23:16:33Z,2020-06-16T20:36:59Z,,,
4967,b'Add Linformer model',2020-06-12T23:09:34Z,2020-10-01T07:08:14Z,"wontfix, New model",,
4966,"b'Spanbert TACRED model not found, despite model card'",2020-06-12T21:29:41Z,2020-08-22T02:07:52Z,wontfix,,
4965,b'How to Paraphrase with GPT2?',2020-06-12T21:24:10Z,2020-09-24T04:15:53Z,wontfix,,
4964,b'GPT: Weights not being initialized',2020-06-12T21:06:00Z,2020-06-12T21:19:43Z,New model,,
4963,b'Cannot load optimizer and lr_scheduler states with TPU training',2020-06-12T19:00:51Z,2020-10-22T19:48:53Z,,RuntimeError,"RuntimeError: don't know how to restore data location of torch.FloatStorage (tagged with xla:0)"
4962,"b'How to implement differential learning rates and still ensure ""weight_decay"" = 0 for the the parameters it should?'",2020-06-12T18:57:37Z,2020-08-22T05:34:23Z,wontfix,,
4961,b'Extending Encoder Decoder to GPT-2',2020-06-12T17:40:51Z,2020-11-01T16:29:44Z,wontfix,,
4960,"b""unexpected keyword argument 'lm_labels' when using BertModel as Decoder with EncoderDecoderModel""",2020-06-12T16:29:50Z,2020-08-22T02:07:47Z,wontfix,TypeError,"TypeError: forward() got an unexpected keyword argument 'lm_labels'"
4959,b'Add AlbertForMultipleChoice',2020-06-12T13:52:13Z,2020-06-12T18:20:20Z,,,
4958,b'Issue with an inline code comment',2020-06-12T11:25:12Z,2020-06-17T22:25:26Z,,,
4957,b'Memory leakage with bert-large-uncased-whole-word-masking-finetuned-squad',2020-06-12T09:31:15Z,2020-06-16T03:45:52Z,,,
4956,"b""RobertaForMaskedLM Failing for example code given on Hugging Face' documentation page""",2020-06-12T09:05:33Z,2020-06-24T13:28:12Z,,TypeError,"TypeError: forward() got an unexpected keyword argument 'labels'"
4955,b'How to use fine-tuned BERT to fill <mask>',2020-06-12T08:59:25Z,2020-08-22T02:07:54Z,wontfix,,
4954,b'ElectraForMultipleChoice',2020-06-12T07:48:21Z,2020-06-18T18:59:35Z,,,
4953,b'Added feature to move added tokens in vocabulary for Transformer-XL',2020-06-12T07:28:32Z,2020-06-22T13:40:53Z,,,
4952,b'Data used for training MarianMT models',2020-06-12T04:20:52Z,2020-06-12T17:17:30Z,,,
4951,b'[examples] SummarizationModule improvements',2020-06-12T03:53:04Z,2020-06-17T17:51:35Z,"seq2seq, Examples",,
4950,b'GPTDoubleHeadsModel Unexpected node type: onnx:: Sub ',2020-06-12T01:31:41Z,2020-06-16T02:42:43Z,,RuntimeError,"RuntimeError: Unexpected node type: onnx::Sub"
4949,b'[mbart] Fix fp16 testing logic',2020-06-12T00:51:39Z,2020-06-12T02:11:35Z,,,
4948,b'[wip] Send slack message if self-scheduled runner fails',2020-06-12T00:46:51Z,2020-09-25T14:16:57Z,,,
4947,b'Trainer.evaluate does not support seq2seq models',2020-06-12T00:26:18Z,2020-10-18T05:15:10Z,wontfix,RuntimeError,"RuntimeError: Sizes of tensors must match except in dimension 0. Got 29 and 22 in dimension 1"
4946,b'feat(TFTrainer): improve logging',2020-06-11T23:06:19Z,2020-06-15T18:06:18Z,,,
4945,b'Unable to evaluate on fine-tuned bart for summarization',2020-06-11T22:23:20Z,2020-06-19T01:09:03Z,,OSError,"OSError: Can't load config for '/..PATH_TO../wiki_bart'. Make sure that:"
4944,b'Refactor proposition for multiple choice models',2020-06-11T22:14:33Z,2020-08-22T02:07:00Z,wontfix,,
4943,b'NER: fix construction of input examples for RoBERTa',2020-06-11T20:36:34Z,2020-06-15T12:30:42Z,,,
4942,b'Dataloader in Trainer num_workers > 0',2020-06-11T20:14:18Z,2020-10-18T05:15:09Z,wontfix,,
4941,b'[Benchmark] fix indentation error',2020-06-11T19:18:57Z,2020-06-11T19:28:02Z,,,
4940,b'Delay decay schedule until the end of warmup',2020-06-11T18:47:04Z,2020-06-24T15:18:30Z,,,
4939,b'[cleanup] Hoist ModelTester objects to top level',2020-06-11T17:38:47Z,2020-06-16T12:03:44Z,cleanup,,
4938,b'T5ForConditionalGeneration fp16 issues',2020-06-11T15:31:06Z,2020-08-22T05:34:28Z,wontfix,,
4937,b'What is the different options for pooler_type in Bert config ?',2020-06-11T14:26:20Z,2020-06-17T22:03:11Z,,,
4936,b'[Model card] model card for electra-base QA model',2020-06-11T11:48:22Z,2020-06-11T17:16:35Z,model card,,
4935,"b""name 'ElectraForSequenceClassification' is not defined""",2020-06-11T10:53:12Z,2020-06-17T22:20:48Z,,,
4934,b'Using LongformerForQuestionAnswering on large documents (40K+ characters)',2020-06-11T10:51:28Z,2020-06-18T13:31:43Z,,,
4933,"b'[AutoModel] Split AutoModelWithLMHead into clm, mlm, encoder-decoder'",2020-06-11T09:34:45Z,2020-06-12T08:01:50Z,,,
4932,b'Training ELECTRA model on TPU with the help of Trainer or TFTrainer classes',2020-06-11T08:11:20Z,2020-08-22T05:34:26Z,wontfix,,
4931,b'BertTokenizer from own vocab meet problem',2020-06-11T07:09:50Z,2020-06-11T07:11:22Z,,,
4930,b'Update setup.py for sentencepiece.',2020-06-11T06:59:08Z,2020-08-22T02:07:01Z,wontfix,,
4929,b'[ElectraForQuestionAnswering] fix qa example in doc',2020-06-11T06:47:04Z,2020-06-17T21:54:17Z,,,
4928,b'sentencepiece dependency must be a specific version. ',2020-06-11T06:42:50Z,2020-08-22T05:34:25Z,wontfix,,
4927,b'Incorrect loss values calculated for TPU training.',2020-06-11T06:31:48Z,2020-08-22T05:34:21Z,wontfix,,
4926,b'Fixing TPU training by disabling wandb.watch gradients logging',2020-06-11T05:54:20Z,2020-06-17T22:04:12Z,,,
4925,b'Use dataloader_drop_last in TF dataset',2020-06-11T02:28:25Z,2020-06-11T06:11:22Z,,,
4924,b'Fix deprecation warnings due to invalid escape sequences.',2020-06-11T02:12:19Z,2020-06-17T21:46:59Z,,,
4923,"b""Documentation doesn't include instructions for applying BertModel to documents using GPU acceleration""",2020-06-11T00:44:50Z,2020-06-11T17:08:43Z,,,
4922,b'Unexpected behavior encoding token_type_ids in GPT models',2020-06-10T21:01:59Z,2020-07-06T15:33:57Z,,,
4921,b'Make multiple choice models work with input_embeds',2020-06-10T21:00:17Z,2020-06-10T22:38:34Z,,,
4920,b'Support multiple choice in tf common model tests',2020-06-10T20:20:03Z,2020-06-11T14:31:26Z,,,
4919,b'File is not found due to extension',2020-06-10T20:18:21Z,2020-08-16T21:33:47Z,wontfix,,
4918,b'Pegasus for summarization ! ',2020-06-10T20:12:36Z,2020-10-30T15:23:17Z,"Summarization, New model",,
4917,b'enable invocation of run_ner.py and utils_ner.py in cython',2020-06-10T19:53:39Z,2020-08-16T21:33:42Z,wontfix,,
4916,"b""Don't init TPU device twice""",2020-06-10T19:43:03Z,2020-06-10T19:53:16Z,,,
4915,b'[generate] Increasing length_penalty makes generations longer',2020-06-10T19:20:35Z,2020-08-29T09:18:04Z,"Discussion, wontfix, seq2seq",,
4914,b'Simple way to convert a Python tokenizer to a fast tokenizer',2020-06-10T18:43:49Z,2020-10-02T11:26:32Z,wontfix,,
4913,b'ElectraForQuestionAnswering',2020-06-10T17:57:52Z,2020-06-10T19:17:52Z,,,
4912,b'Benchmarks',2020-06-10T17:08:06Z,2020-06-22T10:06:57Z,,,
4911,b'enable pickling for TF Bert models',2020-06-10T16:39:41Z,2020-08-23T23:11:07Z,wontfix,,
4910,b'Add more models to common tests',2020-06-10T16:26:04Z,2020-06-10T17:19:54Z,,,
4909,b'[All models] fix docs after adding output attentions to all forward functions',2020-06-10T15:15:03Z,2020-06-10T16:11:00Z,,,
4908,b'BartForQuestionAnswering',2020-06-10T15:02:38Z,2020-06-12T19:47:58Z,,,
4907,"b""ModuleNotFoundError: No module named 'xml.sax'; 'xml' is not a package""",2020-06-10T14:56:07Z,2020-08-16T16:33:46Z,wontfix,ModuleNotFoundError,"ModuleNotFoundError: No module named 'xml.sax'; 'xml' is not a package"
4906,"b""TypeError: export() got an unexpected keyword argument 'use_external_data_format'""",2020-06-10T14:30:08Z,2020-08-05T18:25:23Z,,TypeError,"TypeError: export() got an unexpected keyword argument 'use_external_data_format'"
4905,b'[How to] Carefully designing the head of a Transformer model?',2020-06-10T13:42:20Z,2020-06-16T10:25:14Z,,,
4904,b'[ctrl] fix pruning of MultiHeadAttention',2020-06-10T12:53:56Z,2020-06-10T18:06:56Z,,,
4903,b'Fix the CI',2020-06-10T12:39:42Z,2020-06-10T13:26:07Z,,,
4902,b'[cleanup] Hoist ModelTester objects to toplevel',2020-06-10T11:50:10Z,2020-06-16T12:03:44Z,,,
4901,b'Add MobileBert',2020-06-10T10:32:19Z,2020-06-19T20:38:37Z,,,
4900,b'Latest version of transformers available via conda-forge?',2020-06-10T10:29:56Z,2020-08-16T10:33:47Z,wontfix,,
4899,b'Error using inputs_embeds argument in TFXLNetModel',2020-06-10T10:24:54Z,2020-11-01T03:11:00Z,wontfix,`RuntimeError,"`RuntimeError: Attempting to capture an EagerTensor without building a function.`"
4898,b'update via web',2020-06-10T10:24:51Z,2020-06-10T10:25:27Z,,,
4897,b'KeyError when using non-default models in Huggingface transformers pipeline',2020-06-10T10:10:43Z,2020-06-10T21:32:42Z,,KeyError,"KeyError: 58129"
4896,b'[WIP] Add early stopping to the trainer',2020-06-10T09:53:33Z,2020-06-15T07:24:47Z,,,
4895,b'How do I fine-tune hyperparameters for a model from Huggingface library',2020-06-10T09:39:22Z,2020-06-12T07:59:41Z,,,
4894,b'\xf0\x9f\x9a\x80 Add early stopping to the trainer',2020-06-10T08:24:24Z,2022-03-11T07:08:50Z,"High-Level feature, Good First Issue",,
4893,b'\xf0\x9f\x90\x9b TPU Training broken due to recent changes',2020-06-10T07:47:12Z,2020-06-10T19:53:16Z,,RuntimeError,"RuntimeError: tensorflow/compiler/xla/xla_client/xrt_computation_client.cc:1245 : Check failed: session.Run({tensorflow::Output"
4892,b'Training RoBerta using transformers on masked language task giving weird results',2020-06-10T07:11:02Z,2020-09-14T07:12:30Z,wontfix,,
4891,b'\xf0\x9f\x90\x9b [TFTrainer] `dataloader_drop_last` unused',2020-06-10T05:24:15Z,2020-06-11T06:11:22Z,,,
4890,b'encode_plus( ) function for the GPT-2 Tokenizer',2020-06-10T01:01:50Z,2020-06-10T22:51:48Z,,,
4889,b'[RFC] Tokenizer.prepare_seq2seq_batch',2020-06-10T00:58:14Z,2020-06-10T11:45:17Z,,,
4888,b'Previous commit introduces bug in `convert_pytorch_checkpoint_to_tf2.py`',2020-06-10T00:24:57Z,2020-09-13T09:46:35Z,wontfix,,
4887,b'warn with FutureWarning when using `output_attentions` in the configu\xe2\x80\xa6',2020-06-09T23:07:24Z,2020-06-10T13:40:32Z,,,
4886,b'Deal with multiple choice in common tests',2020-06-09T21:43:22Z,2020-06-10T12:10:21Z,,,
4885,b'Add AlbertForMultipleChoice',2020-06-09T20:23:02Z,2020-06-12T13:51:19Z,,,
4884,b'Fix a bug in the initialization and serialization of TFRobertaClassificationHead',2020-06-09T19:55:50Z,2020-06-09T20:14:03Z,,,
4883,b'check type before logging in trainer to ensure values are scalars',2020-06-09T18:56:48Z,2020-06-10T22:25:56Z,,,
4882,b'fix huggingface/tokenizers#297 in 0.8.0',2020-06-09T18:27:22Z,2020-08-15T19:27:58Z,wontfix,,
4881,b'Fix TensorFlow dataset generator',2020-06-09T18:17:21Z,2020-06-30T23:49:12Z,,,
4880,b'AutoModelForSequenceClassification not working with prunebert model',2020-06-09T18:11:52Z,2020-06-10T15:12:26Z,,,
4879,b'[Draft] Prevent KeyError in QA pipeline',2020-06-09T17:07:37Z,2020-06-09T18:47:40Z,,,
4878,b'BartTokenizerFast',2020-06-09T16:51:33Z,2020-06-14T17:04:50Z,,,
4877,b'ProphetNet',2020-06-09T16:18:55Z,2020-08-22T23:29:39Z,"wontfix, New model",,
4876,b'[examples] Cleanup summarization docs',2020-06-09T15:19:56Z,2020-06-09T21:38:28Z,,,
4875,b'Inconsistent number of vocab from pretrained T5Tokenizer and T5ForConditionalGeneration',2020-06-09T14:18:42Z,2020-06-22T16:35:34Z,,,
4874,b'Split LMBert model in two',2020-06-09T14:01:11Z,2020-06-10T22:26:43Z,,,
4873,b'KeyError in Camembert in QuestionAnsweringPipeline',2020-06-09T13:31:41Z,2020-08-29T09:18:13Z,wontfix,KeyError,"KeyError: 377"
4872,b'Create README.md',2020-06-09T13:30:25Z,2020-06-12T13:03:13Z,model card,,
4871,b'Create README.md for gpt-2-pubmed-medium',2020-06-09T13:26:13Z,2020-06-10T21:29:42Z,model card,,
4870,b'readme change',2020-06-09T12:35:15Z,2020-06-09T12:35:48Z,,,
4869,b'parse arguments from dict',2020-06-09T12:14:44Z,2020-07-31T08:44:24Z,,,
4868,b'tokenizer.encode_plus stopped returning `attention_mask` and pad_to_max_length',2020-06-09T12:02:10Z,2020-06-09T23:21:07Z,,TypeError,"TypeError: _tokenize() got an unexpected keyword argument 'pad_to_max_length'"
4867,b'run_pplm.py bug fix',2020-06-09T12:01:19Z,2020-06-09T23:14:28Z,,,
4866,b'Funnel Transformers',2020-06-09T12:01:03Z,2020-06-10T22:53:19Z,New model,,
4865,b'Create README.md',2020-06-09T11:34:05Z,2020-06-12T13:03:44Z,model card,,
4864,b'Adding \xf0\x9f\xa4\x97nlp in the examples',2020-06-09T10:40:09Z,2020-06-24T12:48:33Z,,,
4863,b'how to train mask model e.g Bert using WordPieceToken',2020-06-09T10:05:22Z,2020-06-10T22:54:54Z,,,
4862,b'how to extract several layers of BERT or GPT as a new model?',2020-06-09T09:21:07Z,2020-08-22T02:07:51Z,wontfix,,
4861,b'can anyone tell me how to do the pretraining of Reformer model on my text data?',2020-06-09T08:46:24Z,2020-06-09T10:18:14Z,,,
4860,b'ROUGE_L score of summarization/t5 is very lower than that of paper.',2020-06-09T07:21:06Z,2020-08-15T10:28:04Z,wontfix,,
4859,b'Memory issues in Transformers',2020-06-09T04:48:49Z,2020-06-09T04:49:03Z,,,
4858,b'Add support for DeBERTa',2020-06-09T01:23:46Z,2020-11-29T11:50:18Z,"wontfix, New model",,
4857,b'sentencepiece==0.1.92 causing segmentation fault',2020-06-09T00:19:26Z,2020-07-30T09:38:40Z,dependencies,,
4856,b'Tensorflow Glue example script for finetuning not usable with DistilBert',2020-06-08T22:37:01Z,2020-06-30T23:49:12Z,"TensorFlow, Should Fix","tensorflow.python.framework.errors_impl.InvalidArgumentError, TypeError","tensorflow.python.framework.errors_impl.InvalidArgumentError: TypeError: `generator` yielded an element that could not be converted to the expected type. The expected type was int32, but the yielded element was None.TypeError: int() argument must be a string, a bytes-like object or a number, not 'NoneType'"
4855,b'Add XLMRobertaForQuestionAnswering',2020-06-08T22:02:49Z,2020-06-09T01:22:38Z,,,
4854,b'Hans data',2020-06-08T21:24:49Z,2020-06-13T13:35:14Z,,,
4853,b'Remove unused arguments in Multiple Choice example',2020-06-08T18:56:42Z,2020-06-10T00:05:10Z,,,
4852,b'issue in pretraining language model with checkpoint ',2020-06-08T17:35:34Z,2020-08-15T03:03:40Z,wontfix,,
4851,b'Run a single wandb instance per TPU run',2020-06-08T16:22:04Z,2020-06-10T20:28:19Z,,,
4850,b'[Benchmark] add tpu and torchscipt for benchmark',2020-06-08T15:22:36Z,2020-06-09T21:12:43Z,,,
4849,b'Clean documentation',2020-06-08T14:55:44Z,2020-06-08T15:28:20Z,,,
4848,"b""TFXLMRobertaForSequenceClassification: call() got an unexpected keyword argument 'labels'""",2020-06-08T14:17:52Z,2020-06-08T14:35:22Z,,TypeError,"TypeError: call() got an unexpected keyword argument 'labels'"
4847,b'Add optimal model size and stopping time feature',2020-06-08T12:53:29Z,2020-11-01T03:10:15Z,"Discussion, wontfix, High-Level feature, Ex: LM (Pretraining)",,
4846,b'Memory issue in Transformers',2020-06-08T12:42:29Z,2020-06-10T07:32:33Z,,,
4845,b'[Generate] beam search should generate without replacement',2020-06-08T11:46:27Z,2020-06-08T13:31:33Z,,,
4844,b'Add support for Funnel-Transformer',2020-06-08T08:25:15Z,2020-09-08T12:08:08Z,New model,,
4843,b'remove words from vocabulary',2020-06-08T06:28:29Z,2020-08-15T03:03:39Z,wontfix,,
4842,b'[Benchmark] Add optimization notebook',2020-06-08T05:30:39Z,2020-06-22T16:18:24Z,,,
4841,b'Multi-output regression support for Transformer models',2020-06-08T05:30:02Z,2020-08-15T03:03:40Z,wontfix,,
4840,b'BUG while calculate LM loss in AlbertForMaskedLM',2020-06-08T04:00:59Z,2020-06-08T14:37:32Z,,,
4839,b'[Longformer] Remove redundant code',2020-06-08T02:22:22Z,2020-06-08T16:28:51Z,,,
4838,"b'[Bert Model] ValueError: not enough values to unpack (expected 3, got 2)'",2020-06-07T21:07:33Z,2020-06-08T19:42:37Z,,ValueError,"ValueError: not enough values to unpack (expected 3, got 2)"
4837,b'[examples] consolidate summarization examples',2020-06-07T18:54:34Z,2020-06-09T15:14:12Z,,,
4836,b'Remove unneeded call convert_ids_to_tokens.',2020-06-07T18:21:02Z,2020-07-19T14:51:58Z,,,
4835,b'Any reason why BART does not have a ForTokenClassification variant?',2020-06-07T17:10:36Z,2020-06-07T18:49:24Z,Core: Modeling,,
4834,b'Why init specific layers rather than whole model  in BART',2020-06-07T17:09:12Z,2020-06-07T18:51:41Z,Core: Modeling,,
4833,b'GlossBert adding',2020-06-07T13:11:02Z,2020-06-07T17:12:47Z,,,
4832,b'Why exclude LayerNorm.bias from weight decay when finetuning?',2020-06-07T11:24:50Z,2020-08-14T09:39:57Z,wontfix,,
4831,b'TF Checkpoints',2020-06-07T10:44:48Z,2020-06-08T13:45:24Z,,,
4830,b'Add diagnostic dataset of glue tasks for prediction',2020-06-07T07:23:07Z,2020-08-22T05:34:04Z,wontfix,,
4829,b'[examples] Add trainer support for question-answering',2020-06-07T04:57:50Z,2020-07-07T12:57:09Z,,,
4828,"b'`run_glue.py` fails with models `bert-base-cased`, `distil-bert-cased`, others'",2020-06-07T04:06:07Z,2020-08-29T09:18:03Z,wontfix,RuntimeError,"RuntimeError: index out of range: Tried to a"
4827,b'How to remove token ?',2020-06-07T04:00:29Z,2020-08-13T09:19:43Z,wontfix,,
4826,b'Fix use of mems in Transformer-XL text generation',2020-06-07T01:44:39Z,2020-07-02T09:19:07Z,,,
4825,b'Onnx converted model has its output shape modified when compared to original (finetuned) model',2020-06-06T23:31:49Z,2020-07-19T21:12:43Z,,,
4824,b'Top-k sampling and top-p sampling for generating phrases on batches with GPT-2?',2020-06-06T23:07:49Z,2020-06-07T21:03:13Z,,,
4823,b'Discriminative fine-tuning for new (added) words',2020-06-06T23:00:42Z,2020-08-15T19:28:06Z,wontfix,,
4822,b'EncoderDecoderModel forwards return different values every time.',2020-06-06T22:12:34Z,2020-06-07T21:02:45Z,,,
4821,b'Enable multiprocessing in glue datasets',2020-06-06T21:16:43Z,2020-06-17T06:47:58Z,,,
4820,b'Updates args in tf squad example.',2020-06-06T20:22:07Z,2020-06-08T09:36:10Z,,,
4819,b'Export PretrainedBartModel from __init__',2020-06-06T18:53:58Z,2020-06-07T15:55:10Z,,,
4818,b'Enable multiprocessing in glue datasets',2020-06-06T18:37:05Z,2020-06-06T19:57:13Z,,,
4817,"b'Question: Where do I find the Transformer model from the paper ""Attention is all you need"" ?'",2020-06-06T10:34:56Z,2020-06-08T22:37:26Z,,,
4816,b'NER pipeline: Inconsistent entity grouping',2020-06-06T08:57:29Z,2020-09-17T04:45:36Z,wontfix,,
4815,b'[marian tests ] pass device to pipeline',2020-06-06T03:18:34Z,2020-06-06T04:52:17Z,,,
4814,b'TPU Training fails with --evaluate_during_training',2020-06-06T02:06:40Z,2020-08-24T00:11:10Z,wontfix,,
4813,b'Is albert lm finetuning with SOP in Pytorch supported?',2020-06-05T23:25:13Z,2020-06-05T23:43:29Z,,,
4812,b'[cleanup/marian] pipelines test and new kwarg',2020-06-05T22:35:06Z,2020-06-05T22:45:20Z,,,
4811,b'Add model and doc badges',2020-06-05T21:52:34Z,2020-06-05T22:45:43Z,,,
4810,b'[Benchmark] Add encoder decoder to benchmark and clean labels',2020-06-05T21:47:58Z,2020-06-08T13:31:13Z,,,
4809,b'[EncoderDecoderConfig] automatically set decoder config to decoder',2020-06-05T20:48:50Z,2020-06-05T21:16:38Z,,,
4808,b'Expose classes used in documentation',2020-06-05T20:33:34Z,2020-06-08T12:14:33Z,,,
4807,b'Use labels to remove deprecation warnings',2020-06-05T20:25:52Z,2020-06-05T20:41:47Z,,,
4806,b'Albert pretrained weights change across runs. ',2020-06-05T19:33:01Z,2020-06-22T16:24:54Z,,,
4805,b'Invalid Argument for Onnxruntime Inference on GPT2',2020-06-05T19:28:57Z,2020-06-10T02:20:10Z,,,
4804,b'Add link to community models',2020-06-05T19:19:52Z,2020-06-05T19:29:21Z,,,
4803,b'[WIP] Blenderbot',2020-06-05T19:13:23Z,2020-09-28T05:32:03Z,,,
4802,b'[cleanup] MarianTokenizer: delete unused constants',2020-06-05T18:45:16Z,2020-06-05T18:57:25Z,,,
4801,b'pip install -e does not always install the correct isort version',2020-06-05T18:15:26Z,2020-06-05T22:28:28Z,,,
4800,b'[isort] add matplotlib to known 3rd party dependencies',2020-06-05T17:55:00Z,2020-06-05T21:27:32Z,,,
4799,b'[cleanup] consolidate some prune_heads logic',2020-06-05T17:52:28Z,2020-06-08T21:08:05Z,,,
4798,b'[ctrl] has broken code for pruning that is not tested',2020-06-05T17:35:45Z,2020-06-10T19:49:18Z,,,
4797,b'Write With Transformer Request:',2020-06-05T17:27:50Z,2020-08-12T03:41:11Z,"wontfix, Write With Transformer",,
4796,b'Ignore simlink',2020-06-05T17:20:49Z,2020-06-06T00:09:32Z,,,
4795,b'Explain how to preview the docs in a PR',2020-06-05T17:14:50Z,2020-06-06T00:47:03Z,,,
4794,b'enable multiprocessing in glue dataset',2020-06-05T17:11:49Z,2020-06-06T16:04:41Z,,,
4793,b'\xf0\x9f\x90\x9b run_ner.py runtime error linked to TPU training',2020-06-05T17:02:57Z,2020-06-05T23:14:48Z,,RuntimeError,"RuntimeError: Found param longformer.embeddings.word_embeddings.weight with type torch.FloatTensor, expected torch.cuda.FloatTensor."
4792,b'Fix argument label',2020-06-05T17:02:50Z,2020-06-05T19:20:30Z,,,
4791,b'parse arguments from dict',2020-06-05T16:35:08Z,2020-06-09T12:01:00Z,,,
4790,b'Clean-up code',2020-06-05T16:31:31Z,2020-06-05T16:36:23Z,,,
4789,b'Add model summary',2020-06-05T14:35:55Z,2020-06-05T16:22:50Z,,,
4788,b'Onnx conversion for bert models with classification layers',2020-06-05T14:24:16Z,2020-08-13T09:19:44Z,wontfix,,
4787,b'\xf0\x9f\x9a\x80 [Feature Request] Add self-contained browsable examples/notebooks in the docs ',2020-06-05T13:36:37Z,2020-08-15T03:03:37Z,wontfix,,
4786,b'Usage of \xc4\xa0 in BPE tokenizer',2020-06-05T13:34:00Z,2020-12-20T13:34:39Z,wontfix,,
4785,b'Question Answering Pipeline with big texts.',2020-06-05T13:06:25Z,2020-06-05T19:29:49Z,,,
4784,b'Can I train question-answering on TPU using Huggingface',2020-06-05T09:42:05Z,2020-06-05T13:49:16Z,,,
4783,b'question about tokenizer',2020-06-05T08:36:16Z,2020-08-11T20:50:02Z,wontfix,,
4782,b'\xe2\x9d\x93 How to use Gradient Accumulator in TF_Trainer ?',2020-06-05T08:29:05Z,2020-06-08T07:54:46Z,,,
4781,b'Regarding generate method used in BART',2020-06-05T07:29:21Z,2020-06-05T08:16:47Z,,,
4780,b'Reformer hidden_size of output is doubled.',2020-06-05T03:10:38Z,2020-06-05T08:32:16Z,,,
4779,b'\xf0\x9f\x90\x9b [BART] Pipeline OOM ',2020-06-05T01:17:51Z,2020-08-31T18:09:59Z,wontfix,,
4778,"b'Updated path ""cd examples/text-generation/pplm""'",2020-06-05T00:04:56Z,2020-06-06T01:16:49Z,,,
4777,"b'The purpose of files merges.txt, special_tokens_map.json, training_args.bin and add_tokens.json'",2020-06-04T23:03:15Z,2020-08-29T09:18:06Z,wontfix,,
4776,b'Correcting path to pplm examples',2020-06-04T22:52:16Z,2020-06-06T01:16:58Z,,,
4775,b'Create model card for tblard/allocine',2020-06-04T21:24:44Z,2020-06-04T23:15:08Z,model card,,
4774,b'Add .vs to gitignore',2020-06-04T20:19:54Z,2020-06-05T11:56:12Z,,,
4773,"b""Don't access pad_token_id if there is no pad_token""",2020-06-04T18:15:50Z,2020-06-04T21:57:05Z,,,
4772,b'Fix the __getattr__ method in BatchEncoding',2020-06-04T17:58:29Z,2020-06-09T07:44:00Z,,,
4771,b'Remove unnecessary model_type arg in example',2020-06-04T16:31:07Z,2020-06-04T17:41:25Z,,,
4770,b'Add note about doc generation',2020-06-04T16:19:42Z,2020-06-04T17:43:15Z,,,
4769,b'Bert (sentence classification) output is non-deterministic for PyTorch (not for TF)',2020-06-04T15:36:50Z,2020-06-04T19:14:12Z,,,
4768,b'Codecov setup',2020-06-04T15:17:00Z,2020-06-04T15:44:39Z,,,
4767,b'Model is running on special characters and word pieces for token classification',2020-06-04T15:16:53Z,2020-08-11T20:49:45Z,wontfix,,
4766,b'Issue with HANS evaluation',2020-06-04T13:58:59Z,2020-08-17T12:23:28Z,wontfix,,
4765,b' Cannot load pretrained model from repo.',2020-06-04T13:57:06Z,2020-06-04T17:42:35Z,,`OSError,"`OSError: Model name 'NeuML/bert-small-cord19-squad2' was not found in tokenizers model name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). We assumed 'NeuML/bert-small-cord19-squad2' was a path or url to a directory containing vocabulary files named ['vocab.txt'] but couldn't find such vocabulary files at this path or url."
4764,b'GPT2TokenizerFast raises pad_token error even if not used',2020-06-04T12:45:11Z,2020-06-04T21:57:05Z,,,
4763,b'Model Card for RoBERTa trained on Sanskrit',2020-06-04T11:42:56Z,2020-06-04T20:58:41Z,model card,,
4762,b'KeyError in Pipeline Question Answering with LongFormer',2020-06-04T11:24:48Z,2020-08-11T20:50:00Z,wontfix,KeyError,"KeyError: 382"
4761,b'[cleanup] PretrainedModel.generate: remove unused kwargs',2020-06-04T11:18:29Z,2020-06-04T12:13:53Z,,,
4760,b'Fine-tuning of RoBERTa',2020-06-04T11:04:25Z,2020-06-04T17:53:37Z,,,
4759,b'Fix resize_token_embeddings for Transformer-XL',2020-06-04T10:49:49Z,2020-06-10T23:03:07Z,,,
4758,b'run_tf_ner.py output_dir/saved_model empty',2020-06-04T10:20:05Z,2020-06-08T02:36:15Z,,,
4757,b'Add drop_last arg for data loader',2020-06-04T05:28:27Z,2020-06-04T22:30:32Z,,,
4756,b'[WIP] feat(wandb): add logging to TFTrainer',2020-06-04T01:35:33Z,2020-06-15T18:43:48Z,,,
4755,b'run_ner.py crashes with RoBERTa because of incorrect sequence length',2020-06-04T00:56:32Z,2020-06-15T12:30:41Z,,,
4754,b'bart-large-cnn model weights updated?',2020-06-04T00:22:22Z,2020-06-04T14:34:19Z,,,
4753,b'Can I use TorchText Iterator output as the input_ids for Hugging Face Transformer?',2020-06-03T23:38:47Z,2020-08-10T01:32:19Z,wontfix,,
4752,b'Batching not speeding up Transformer-XL',2020-06-03T21:38:41Z,2020-07-02T12:39:59Z,,,
4751,b'Update encode documentation',2020-06-03T20:30:44Z,2020-06-03T20:31:00Z,,,
4750,b'Tokenizer.encode documentation not correct',2020-06-03T20:08:29Z,2020-06-03T20:31:00Z,,,
4749,b'Hugging Face GPT-2 Tokenizer',2020-06-03T19:56:37Z,2020-06-04T03:52:26Z,,,
4748,b'QuestionAnsweringPipeline query performance',2020-06-03T19:27:29Z,2020-10-10T08:13:57Z,wontfix,,
4747,b'No silent error when d_head already in the configuration',2020-06-03T17:24:00Z,2020-06-05T16:01:44Z,,,
4746,"b""Why I can't generate phrases in batches if I include an attention mask? (GPT2)""",2020-06-03T16:36:06Z,2020-06-05T08:26:59Z,,,
4745,b'[Generation Beam Search] Fix bug when changing the <EOS> token for generate',2020-06-03T15:34:12Z,2020-06-03T16:53:24Z,,,
4744,b'How to use pretrained model for inference?',2020-06-03T15:26:36Z,2020-08-11T20:49:44Z,wontfix,,
4743,b'Create README.md',2020-06-03T15:18:47Z,2020-06-04T20:59:37Z,model card,,
4742,b'Perform evaluation on HANS with Trainer (like GLUE example)',2020-06-03T15:08:49Z,2020-06-16T12:06:32Z,Good First Issue,,
4741,b'Implemented resizing of token embeddings for TensorFlow models',2020-06-03T13:52:40Z,2020-06-22T07:36:10Z,,,
4740,"b""Can't find config.json""",2020-06-03T13:12:26Z,2020-06-03T14:56:00Z,,,
4739,b'Extending run_language_modeling.py for XLNet',2020-06-03T13:09:14Z,2020-07-07T08:17:38Z,,,
4738,b'Question Answering Modeling through Hugging Face Models',2020-06-03T12:41:15Z,2020-06-05T19:29:14Z,,,
4737,b'Create model card for T5-base fine-tuned for Sentiment Span Extraction',2020-06-03T11:02:25Z,2020-06-04T20:59:57Z,model card,,
4736,b'BertModel Inputs',2020-06-03T09:32:57Z,2020-06-05T09:18:01Z,,,
4735,b'Bart model for textinfilling',2020-06-03T07:57:49Z,2020-06-04T06:04:14Z,seq2seq,,
4734,b'TFTrainer: Checkpoints not getting saved in `output_dir` but in {cwd}/checkpoint',2020-06-03T05:34:44Z,2020-06-03T20:29:40Z,,,
4733,"b'When I use TFBertEncoder in my laptop, I get an error.I can not build a model. Here is a simple examples.'",2020-06-03T03:39:33Z,2020-06-22T10:02:12Z,,tensorflow.python.framework.errors_impl.FailedPreconditionError,"tensorflow.python.framework.errors_impl.FailedPreconditionError:  Error while reading resource variable _AnonymousVar189 from Container: localhost. This could mean that the variable was uninitialized. Not found: Resource localhost/_AnonymousVar189/class tensorflow::Var does not exist."
4732,b'Adding notebooks for Fine Tuning [Community Notebook]',2020-06-03T01:44:16Z,2020-06-03T09:07:27Z,,,
4731,b'[DOT NOT MERGE] Tokenizers Shape Polymorphism - Introduce pad_to_next_multiple_of parameters',2020-06-02T22:51:35Z,2020-06-16T11:37:53Z,,,
4730,b'bert-small-cord19 model cards',2020-06-02T21:24:14Z,2020-06-03T07:40:14Z,model card,,
4729,b'[Feature request] Support batched conditional generation from GPT-2 ',2020-06-02T21:21:23Z,2020-10-10T08:14:08Z,wontfix,,
4728,b'Possible fix to make AMP work with DDP in the trainer',2020-06-02T18:29:49Z,2020-06-15T14:10:27Z,PyTorch,,
4727,b'Albert pretraining loss not decreasing',2020-06-02T18:29:03Z,2020-08-11T20:49:46Z,wontfix,,
4726,b'TFRobertaModelIntegrationTest requires tf',2020-06-02T16:50:42Z,2020-06-02T16:59:01Z,,,
4725,b'Save & load sparse models from the models database',2020-06-02T16:27:37Z,2020-08-08T21:35:57Z,wontfix,,
4724,b'Fix CI after killing archive maps',2020-06-02T14:20:55Z,2020-06-02T14:21:10Z,,,
4723,b'never_split on slow tokenizers should not split',2020-06-02T12:39:52Z,2020-06-03T20:48:29Z,,,
4722,b'Unify label args',2020-06-02T11:30:52Z,2020-06-03T13:36:27Z,,,
4721,b'Faster bert basic tokenizer',2020-06-02T11:26:19Z,2020-08-11T20:49:36Z,wontfix,,
4720,b'[Reformer] Improved memory if input is shorter than chunk length',2020-06-02T11:04:51Z,2020-06-02T21:08:40Z,,,
4719,b'About `do_basic_tokenize` behavior in BertTokenizer',2020-06-02T10:52:15Z,2020-08-08T14:35:59Z,"wontfix, Documentation",,
4718,b'Replace pad_token with -100 for LM loss calculation',2020-06-02T08:49:01Z,2020-06-24T16:14:50Z,,,
4717,b'Override get_vocab for fast tokenizer.',2020-06-02T08:32:47Z,2020-06-02T09:02:28Z,,,
4716,b'Can I save a word embedding from BERT and used again later for computational purpose(as BERT takes much more time). Just like in Glove. If yes then How? and is this good idea to do so?',2020-06-02T08:09:23Z,2020-08-11T20:50:00Z,wontfix,,
4715,b'how to make a multi-task deep neural network baseline using huggingface transformers?',2020-06-02T06:27:17Z,2020-08-08T09:36:00Z,wontfix,,
4714,"b""ImportError: cannot import name 'MODEL_WITH_LM_HEAD_MAPPING'""",2020-06-02T03:58:21Z,2020-06-03T17:37:27Z,,ImportError,"ImportError: cannot import name 'MODEL_WITH_LM_HEAD_MAPPING' from 'transformers' (C:\Users\<username>\anaconda3\envs\tensorflow\lib\site-packages\transformers\__init__.py)"
4713,b'Is there any need to fine-tune the already pre-trained GPT-2 models?',2020-06-01T23:38:03Z,2020-08-09T16:32:24Z,wontfix,,
4712,b'Converting model to pytorch',2020-06-01T22:44:20Z,2020-08-08T06:35:59Z,wontfix,"`OSError, `UnicodeDecodeError","`OSError: Model name '/content/biobert_v1.1_pubmed' was not found in tokenizers model name list (bart-large, bart-large-mnli, bart-large-cnn, bart-large-xsum). We assumed '/content/biobert_v1.1_pubmed' was a path, a model identifier, or url to a directory containing vocabulary files named ['vocab.json', 'merges.txt'] but couldn't find such vocabulary files at this path or url.``UnicodeDecodeError: 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte"
4711,b'Make docstring match args',2020-06-01T19:18:02Z,2020-06-01T19:22:52Z,,,
4710,b'Specify PyTorch versions',2020-06-01T18:52:30Z,2020-06-02T08:29:29Z,,,
4709,b'Wrong argument passed during TFRobertaClassificationHead initialization',2020-06-01T18:43:56Z,2020-06-09T20:14:02Z,"TensorFlow, Core: Modeling, Should Fix",,
4708,b'Is the separation token absolutely necessary if I use GPT2DoubleHeadsModel with token_type_ids?',2020-06-01T18:28:01Z,2020-08-08T06:35:57Z,wontfix,,
4707,b'How tokenizers work',2020-06-01T18:27:04Z,2020-06-01T18:41:46Z,,,
4706,"b'When using the Hugging Face Transformer, if I set my pad_token to be a different token then the default, do I need to train my model on that new pad_token as well?'",2020-06-01T17:56:07Z,2020-06-03T19:52:16Z,,,
4705,b'Is transformers 2.11.0 compatible with tokenizers 0.8.0(-dev*)?',2020-06-01T17:22:30Z,2020-06-13T03:01:33Z,,,
4704,b'Tensorflow XLMRoberta Multi-Class Problem',2020-06-01T15:40:30Z,2020-06-01T17:04:46Z,,ValueError,"ValueError: Shapes (None, 1, 8) and (None, 2) are incompatible"
4703,b'Fused_norm_layer_cuda',2020-06-01T14:41:22Z,2020-06-01T15:55:46Z,,,
4702,b'Why DataCollatorForLanguageModeling do not make attention_mask feature?',2020-06-01T07:50:12Z,2020-08-08T02:48:00Z,wontfix,,
4701,b'Why we need the init_weight function in BERT pretrained model',2020-06-01T02:38:22Z,2020-06-01T11:06:17Z,Usage,,
4700,b'Add community notebook for T5 sentiment span extraction',2020-06-01T01:51:25Z,2020-06-02T07:59:53Z,,,
4699,b'NER example doesn\xe2\x80\x99t work with tensorflow',2020-05-31T22:41:04Z,2020-08-27T12:58:54Z,"wontfix, TensorFlow, Ex: Named Entity Recognition",tensorflow.python.framework.errors_impl.InvalidArgumentError,"tensorflow.python.framework.errors_impl.InvalidArgumentError:  Received a label value of -1 which is outside the valid range of [0, 25).  Label values: -1 24 -1 -1 -1 -1 24 24 -1 -1 -1 24 -1 -1 -1 -1 -1 -1 -1 24 24 2 -1 -1 -1 24 0 24 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 24 24 -1 -1 -1 -1 24 24 -1 -1 24 -1 -1 -1 24 -1 -1 24 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 24 24 -1 -1 -1 -1 -1 24 -1 24 -1 24 -1 24 -1 -1 24 -1 24 24 6 24 -1 24 -1 24 -1 24 -1 -1 -1 24 -1 -1 -1 24 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 0 24 3 -1 24 24 24 24 -1 -1 1 24 -1 -1 -1 -1 9 21 -1 -1 24 0 -1 -1 -1 24 24 24 24 -1 -1 24 24 -1 -1 24 24 -1 -1 -1 3 15 15 24 -1 -1 -1 24 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 24 24 24 24 24 -1 -1 24 -1 24 24 -1 -1 24 -1 24 -1 -1 -1 -1 24 24 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 24 6 24 24 24 6 18 18 24 24 6 24 -1 -1 24 24 24 -1 -1 -1 24 24 9 21 -1 24 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 24 -1 -1 -1 24 -1 24 -1 24 -1 24 -1 24 -1 24 24 24 -1 -1 24 24 24 -1 24 -1 24 -1 24 -1 -1 -1 -1 24 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 24 24 -1 24 -1 -1 -1 24 -1 -1 -1 24 -1 9 24 9 21 24 24 -1 24 24 0 -1 -1 24 -1 -1 24 24 -1 24 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 24 -1 24 -1 24 -1 24 24 -1 -1 -1 24 -1 24 24 -1 24 -1 24 -1 24 -1 24 -1 -1 -1 24 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 24 24 -1 24 -1 -1 24 -1 0 -1 24 2 -1 -1 -1 24 24 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 24 24 24 -1 -1 -1 -1 -1 24 24 3 -1 -1 -1 -1 -1 -1 24 -1 24 -1 -1 24 24 -1 -1 24 24 24 -1 -1 -1 24 24 -1 -1 -1 -1 -1 24 -1 -1 24 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 24 -1 -1 24 24 24 -1 24 -1 24 -1 24 -1 24 24 -1 24 -1 -1 24 -1 24 24 24 24 -1 -1 -1 -1 24 24 -1 -1 -1 -1 24 -1 -1 24 -1 24 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 24 24 24 -1 24 24 -1 24 24 24 -1 -1 24 -1 -1 -1 24 -1 24 -1 -1 24 24 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 24 -1 24 -1 -1 -1 24 -1 -1 24 -1 24 -1 -1 24 24 -1 -1 24 -1 -1 -1 -1 24 -1 -1 24 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 24 -1 24 24 -1 24 -1 24 24 24 24 -1 9 24 -1 -1 24 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 24 24 24 -1 24 -1 24 24 -1 24 -1 24 -1 24 -1 24 24 24 24 -1 -1 24 -1 24 -1 24 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 24 -1 24 -1 -1 24 -1 24 -1 24 -1 -1 24 -1 -1 -1 -1 -1 -1 24 -1 24 24 -1 24 24 -1 -1 24 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 24 6 18 24 -1 -1 24 -1 -1 24 -1 -1 24 24 -1 -1 -1 -1 -1 24 -1 24 -1 -1 24 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 24 8 -1 -1 -1 -1 -1 -1 -1 24 -1 -1 24 -1 -1 -1 24 24 -1 -1 -1 8 -1 -1 -1 -1 -1 -1 -1 24 24 -1 24 -1 24 -1 24 24 -1 -1 -1 24 -1 -1 24 24 24 24 8 -1 -1 -1 -1 -1 -1 -1 24 -1 -1 -1 -1 24 24 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 24 24 6 -1 -1 18 18 -1 -1 -1 18 18 18 -1 18 18 -1 -1 -1 24 24 -1 -1 -1 -1 24 24 0 -1 24 -1 -1 -1 24 -1 -1 24 -1 -1 -1 24 24 -1 -1 24 24 -1 24 24 0 -1 24 -1 -1 24 24 -1 24 5 -1 -1 -1 -1 -1 -1 24 0 12 12 24 -1 -1 -1 24 -1 24 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 24 -1 -1 24 24 24 24 24 -1 -1 -1 -1 -1 24 -1 -1 -1 24 -1 -1 24 24 -1 24 24 24 24 -1 -1 24 24 -1 -1 -1 -1 0 -1 -1 -1 24 24 -1 -1 24 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 24 -1 -1 24 24 24 -1 24 -1 24 -1 6 -1 -1 24 24 1 -1 24 -1 -1 -1 -1 -1 -1 24 24 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 24 24 24 -1 -1 24 3 -1 -1 24 24 24 24 24 24 -1 24 -1 24 24 24 -1 24 -1 -1 -1 -1 24 24 -1 -1 -1 -1 24 24 -1 -1 -1 -1 -1 24 -1 -1 24 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 24 24 24 -1 24 24 -1 24 24 24 -1 -1 -1 24 24 -1 24 -1 24 -1 -1 -1 24 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 24 24 -1 24 -1 -1 -1 -1 -1 24 24 24 -1 -1 -1 -1 24 -1 24 -1 -1 24 24 3 15 15 -1 -1 -1 -1 -1 24 0 12 12 24 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 24 -1 -1 0 -1 -1 24 -1 -1 -1 24 -1 -1 -1 -1 -1 -1 -1 -1 24 24 24 -1 0 -1 -1 24 -1 24 -1 24 -1 24 -1 -1 24 24 -1 -1 -1 24 24 -1 -1 24 -1 -1 -1 -1 24 -1 -1 -1 24 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 24 24 -1 -1 9 21 -1 24 9 -1 21 24 9 21 24 24 -1 -1 -1 -1 24 -1 -1 -1 -1 24 -1 -1 24 24 -1 24 -1 24 24 -1 24 24 -1 -1 24 24 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 24 24 -1 24 24 -1 24 -1 24 -1 24 24 1 -1 -1 24 24 24 -1 24 -1 -1 24 24 -1 -1 -1 -1 -1 24 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 9 -1 24 -1 -1 -1 -1 24 -1 24 24 -1 24 24 9 24 24 24 24 -1 -1 -1 -1 24 24 -1 -1 -1 24 0 24 24 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 24 9 -1 24 24 -1 24 24 -1 24 -1 -1 -1 -1 24 -1 -1 -1 24 -1 -1 -1 -1 -1 24 24 24 -1 24 -1 -1 24 24 -1 24 -1 -1 -1 24 24 6 -1 -1 24 24 -1 24 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 24 24 -1 -1 24 -1 -1 -1 -1 24 -1 24 24 -1 -1 -1 -1 -1 24 24 -1 -1 -1 -1 24 -1 -1 24 24 -1 24 24 -1 24 24 -1 -1 -1 24 -1 -1 -1 -1 24 -1 24 -1 24 24 -1 -1 24 24 -1 -1 24 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 24 -1 -1 24 24 -1 -1 -1 -1 24 -1 -1 24 -1 -1 -1 -1 -1 -1 24 -1 24 -1 -1 24 24 24 -1 -1 -1 -1 -1 24 24 24 -1 -1 -1 24 -1 -1 24 -1 -1 24 -1 24 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1"
4698,b'Transformer-XL: Input and labels for Language Modeling',2020-05-31T18:18:42Z,2020-06-03T05:32:51Z,,,
4697,b'SpanBert always predicts the same token',2020-05-31T18:13:49Z,2020-08-08T02:48:02Z,wontfix,,
4696,b'Loading config file bug',2020-05-31T16:30:32Z,2020-06-05T16:01:44Z,,,
4695,b'Please add the functionality to save tokenizer model for run_language_modeling.py',2020-05-31T16:28:54Z,2021-04-25T15:05:09Z,,,
4694,b'Adding Neutral Score ',2020-05-31T16:20:53Z,2020-08-08T02:48:01Z,wontfix,,
4693,"b""TypeError: cannot create 'BPE' instances""",2020-05-31T04:06:54Z,2020-08-06T07:59:00Z,wontfix,TypeError,"TypeError: cannot create 'BPE' instances"
4692,b'Gradient overflow issue when i try to train gpt2 with run_language_modeling in fp16 with 02. Any idea why that maybe happen?',2020-05-31T01:36:07Z,2020-08-08T02:47:58Z,wontfix,,
4691,b'[EncoderDecoder] Add RoBERTa as a decoder',2020-05-30T22:52:32Z,2020-08-06T07:58:56Z,wontfix,,
4690,b'Keyword errors on tokenizer.encode_plus',2020-05-30T21:36:24Z,2020-05-30T22:22:07Z,,TypeError,"TypeError: _tokenize() got an unexpected keyword argument 'pad_to_max_length'   "
4689,b'Same logits value for different input',2020-05-30T18:57:17Z,2020-08-08T21:35:58Z,"Need more information, wontfix, Ex: Sequence Classification",,
4688,b'Compressive Transformer',2020-05-30T13:12:24Z,2020-08-29T09:18:17Z,"wontfix, New model",,
4687,b'Update HooshvareLab/bert-base-parsbert-uncased',2020-05-30T12:16:21Z,2020-06-01T12:27:01Z,model card,,
4686,b'[pipeline] Tokenizer should not add special tokens for text generation',2020-05-30T10:49:04Z,2020-06-02T09:03:47Z,,,
4685,b'AutoModel.from_config loads random parameter values.',2020-05-30T08:54:58Z,2020-06-01T16:58:11Z,Documentation,,
4684,b'Create README.md',2020-05-30T08:39:24Z,2020-06-01T09:45:55Z,model card,,
4683,"b'when I encode [unused1], return not one token'",2020-05-30T07:44:30Z,2020-06-04T02:58:45Z,Core: Tokenization,,
4682,b'XLNet Generation appears to reference padding text in run_generation script',2020-05-30T07:29:10Z,2020-06-03T13:56:42Z,Ex: Generation,,
4681,b'NER: Add new WNUT\xe2\x80\x9917 example',2020-05-29T23:57:16Z,2020-06-04T23:13:18Z,"Ex: Named Entity Recognition, Documentation",,
4680,b'[EncoderDecoder] Fix initialization and save/load bug',2020-05-29T22:51:50Z,2020-05-29T23:25:20Z,,,
4679,b'GPT-3',2020-05-29T19:36:38Z,2020-05-29T19:38:41Z,New model,,
4677,b'Documentation for non-nlp experts',2020-05-29T17:59:24Z,2020-10-10T08:13:59Z,"wontfix, Documentation",,
4676,b'Include `nlp` notebook for model evaluation',2020-05-29T17:38:22Z,2020-05-29T17:38:57Z,,,
4675,b'Gpt2 generation of text larger than 1024',2020-05-29T17:22:23Z,2020-08-05T23:30:02Z,"wontfix, Ex: Generation",,
4674,b'KeyError in Camembert in QuestionAnsweringPipeline',2020-05-29T17:16:57Z,2020-06-03T16:55:21Z,Ex: Question Answering,KeyError,"KeyError: 339"
4673,b'QUESTION: How do I know what type of positional encoding to input during fine-tuning or pretrained BERT?',2020-05-29T13:55:23Z,2020-05-29T14:04:14Z,,,
4672,b'[Longformer] Better handling of global attention mask vs local attention mask',2020-05-29T13:27:51Z,2020-05-29T15:58:43Z,,,
4671,b'get_from_cache in file_utils.py gobbles up error in making url requests',2020-05-29T12:50:53Z,2020-11-10T12:11:03Z,,`TypeError,"`TypeError: stat: path should be string, bytes, os.PathLike or integer, not NoneType`"
4670,b'Coversion between tokenizers',2020-05-29T12:39:35Z,2020-05-29T17:51:31Z,Core: Tokenization,,
4669,b'Cannot load labels from old models',2020-05-29T12:14:27Z,2020-06-01T14:33:34Z,Core: Modeling,,
4668,b' Colab crashes due to tcmalloc large allocation',2020-05-29T11:29:16Z,2020-08-29T09:18:09Z,wontfix,,
4667,b'HooshvareLab readme parsbert-peymaner',2020-05-29T09:41:24Z,2020-06-01T12:28:26Z,model card,,
4666,b'HooshvareLab readme parsbert-armananer',2020-05-29T09:35:01Z,2020-06-01T12:28:44Z,model card,,
4665,b'HooshvareLab readme parsbert-ner',2020-05-29T09:25:58Z,2020-06-01T12:29:10Z,model card,,
4664,b'run_tf_ner.py TFTrainer logdir cannot be none',2020-05-29T08:22:39Z,2020-05-29T15:58:31Z,,ValueError,"ValueError: logdir cannot be None"
4663,b'End-to-end object detection with Transformers',2020-05-29T04:33:09Z,2020-05-31T17:25:08Z,New model,,
4662,b'run_tf_ner.py cannot run',2020-05-29T03:47:35Z,2020-05-29T07:54:15Z,,,
4661,b'Write With Transformer: PPLM page is broken',2020-05-29T02:44:11Z,2020-06-01T15:06:38Z,Need more information,,
4660,b'Assert message error in Reformer chunking',2020-05-29T02:42:48Z,2020-06-02T21:08:40Z,reformer,,
4659,b'Add support for gradient checkpointing in BERT',2020-05-29T01:59:21Z,2020-06-22T14:47:15Z,,,
4658,b'Add upcoming GPT-3 model',2020-05-29T01:00:46Z,,New model,,
4657,b'--fp causes an issue when running example scripts in distributed mode',2020-05-28T21:07:43Z,2020-06-15T14:10:27Z,PyTorch,,
4656,b'Electra training from scratch',2020-05-28T20:48:50Z,2021-04-25T15:05:12Z,,,
4655,"b""Tokenization_utils doesn't work with Pytorch-Lightning on 2.10.0 version""",2020-05-28T18:04:14Z,2020-05-29T15:42:12Z,,KeyError,"KeyError: 'cuda'"
4654,b'TfElectraForSequenceClassification',2020-05-28T17:12:45Z,2020-08-04T07:12:16Z,,,
4653,b'[Longformer] fix model name in examples',2020-05-28T16:11:50Z,2020-05-29T11:12:35Z,,,
4652,b'[Community notebooks] add longformer-for-qa notebook',2020-05-28T16:03:49Z,2020-05-28T20:27:23Z,,,
4651,b'Update modeling_electra for adding ability to using electra as decoder',2020-05-28T14:47:32Z,2020-08-04T17:40:38Z,wontfix,,
4650,b'Allow pathlib.Path to be used on save_pretrained and save_vocabulary',2020-05-28T14:38:17Z,2020-08-04T17:40:40Z,wontfix,,
4649,b'Update modeling_electra for adding ability to using electra as decoder',2020-05-28T14:13:18Z,2020-05-28T14:18:40Z,,,
4648,b'Update modeling_electra for using it as decoder',2020-05-28T13:59:14Z,2020-05-28T14:07:01Z,,,
4647,"b'Encode-Decode after training, generation gives the same results regardless of the input'",2020-05-28T13:49:32Z,2020-09-26T10:44:32Z,wontfix,,
4646,b'add longformer docs',2020-05-28T12:51:28Z,2020-05-29T23:39:55Z,,,
4645,b'[Longformer] Multiple choice for longformer',2020-05-28T12:11:14Z,2020-05-29T11:46:09Z,,,
4644,b'LongformerForMultipleChoice',2020-05-28T12:10:38Z,2020-05-28T12:39:05Z,,,
4643,b'question-answering examples bug in pipelines document',2020-05-28T08:56:11Z,2020-07-30T13:39:16Z,wontfix,TypeError,"TypeError: forward() got an unexpected keyword argument 'token_type_ids'"
4642,b'[Longformer] Notebook to train Longformer',2020-05-28T07:31:04Z,2020-05-28T20:35:16Z,,,
4641,b'Fix onnx export input names order',2020-05-28T07:16:12Z,2020-06-01T14:12:49Z,,,
4640,b'Error when loading a trained Encoder-Decoder model. ',2020-05-28T06:51:09Z,2020-05-30T01:20:50Z,,,
4639,b'How to generate prediction/answer from a custom model fined-tuned/trained for self-defined questions?',2020-05-28T06:33:25Z,2020-08-05T23:30:08Z,wontfix,,
4638,b'LongformerForTokenClassification',2020-05-28T05:30:06Z,2020-05-28T10:48:19Z,,,
4637,b'Movement pruning',2020-05-28T04:59:33Z,2020-06-01T13:23:33Z,,,
4636,b'Kill model archive maps',2020-05-28T03:51:37Z,2020-06-02T13:39:34Z,,,
4635,"b'03-pipelines.ipynb on Colab: error on ""Summarization""'",2020-05-28T03:49:49Z,2020-05-30T22:33:29Z,,,
4634,b'tensorflow2_gpt2 Slow speed',2020-05-28T03:11:50Z,2020-08-09T13:32:19Z,wontfix,,
4633,b'Merge pull request #1 from huggingface/master',2020-05-28T00:43:16Z,2020-05-28T00:43:36Z,,,
4632,b'Pipelines: miscellanea of QoL improvements and small features...',2020-05-27T23:08:13Z,2020-06-03T07:51:32Z,,,
4631,b'Numpy format string issue in TFTrainer',2020-05-27T22:50:48Z,2020-08-05T23:30:03Z,wontfix,TypeError,"TypeError: unsupported format string passed to numpy.ndarray.__format__`"
4630,"b'Model evaluated at each checkpoint, but results not in checkpoint file'",2020-05-27T22:33:20Z,2020-06-01T10:55:26Z,,,
4629,b'gpt2 typo',2020-05-27T20:41:09Z,2020-05-28T20:44:43Z,,,
4628,b'[Longformer] more models + model cards',2020-05-27T20:02:38Z,2020-05-28T09:11:06Z,model card,,
4627,b'[WIP] lightning glue example uses nlp package',2020-05-27T19:56:56Z,2020-06-16T17:18:15Z,,,
4626,b'How to use run_glue.py with tensorboard?',2020-05-27T19:01:53Z,2020-08-05T23:30:08Z,wontfix,,
4625,b'[Model Card] model card for longformer-base-4096-finetuned-squadv1',2020-05-27T16:43:08Z,2020-05-27T16:48:04Z,model card,,
4624,"b""Can't see logger output""",2020-05-27T16:37:44Z,2020-05-28T06:05:54Z,,,
4623,b'`train.jsonl` file missing in MM-IMDb task',2020-05-27T16:00:06Z,2020-08-02T17:12:04Z,wontfix,FileNotFoundError,"FileNotFoundError: [Errno 2] No such file or directory: '.../mmimdb/dataset/train.jsonl'"
4622,b'GPU memory usage',2020-05-27T15:41:35Z,2020-08-02T19:12:05Z,wontfix,,
4621,b'Cleanup glue',2020-05-27T15:40:22Z,2020-06-02T17:40:15Z,,,
4620,b'The new abstractions in /master are counterproductive',2020-05-27T15:28:41Z,2020-08-04T17:41:11Z,wontfix,,
4619,b'removed deprecated use of Variable API from pplm example',2020-05-27T15:11:08Z,2020-06-04T22:07:51Z,,,
4618,b'per_device instead of per_gpu/error thrown when argument unknown',2020-05-27T14:22:21Z,2020-05-27T15:36:56Z,,,
4617,b'run evalation after every epoch in Trainer',2020-05-27T14:09:56Z,2020-05-27T14:21:09Z,,,
4616,b'[testing] LanguageModelGenerationTests require_tf or require_torch',2020-05-27T13:03:05Z,2020-05-27T13:10:27Z,,,
4615,b'[Longformer] longformer in question-answering pipeline',2020-05-27T12:52:31Z,2020-09-11T02:02:11Z,wontfix,,
4614,b'[Contributing Doc] Update version command when contributing',2020-05-27T10:28:39Z,2020-05-27T15:19:11Z,,,
4613,b'What does the output of feature-extraction pipeline represent?',2020-05-27T09:31:46Z,2020-08-04T17:41:12Z,wontfix,,
4612,b'Use fill-mask pipeline to get probability of specific token',2020-05-27T09:27:34Z,2020-08-08T14:36:00Z,wontfix,,
4611,b'Key error while evaluating the Language Model finetuning',2020-05-27T09:12:01Z,2020-05-29T10:09:40Z,,KeyError,"KeyError: 'eval_loss'"
4610,b'README for HooshvareLab',2020-05-27T07:55:44Z,2020-05-27T15:25:36Z,model card,,
4609,b'How to deal with summarization task to long sequences input? ',2020-05-27T06:13:24Z,2020-08-02T19:12:04Z,wontfix,,
4608,b'uncased readme',2020-05-27T06:00:02Z,2020-05-27T13:50:05Z,model card,,
4607,b'Create README.md',2020-05-27T05:41:04Z,2020-05-27T13:36:21Z,model card,,
4606,b'Inconsistency in how Electra doing sentence level prediction',2020-05-27T02:58:23Z,2020-08-02T08:51:01Z,wontfix,,
4605,b'Glue task cleanup',2020-05-26T23:35:57Z,2020-05-26T23:46:25Z,,,
4604,b'updated model cards for both models at aubmindlab',2020-05-26T19:36:45Z,2020-05-26T20:52:43Z,model card,,
4603,b'Creating a readme for ALBERT in Mongolian',2020-05-26T15:25:53Z,2020-05-26T20:54:43Z,model card,,
4602,b'Remove MD emojis',2020-05-26T15:09:02Z,2020-05-26T20:38:39Z,model card,,
4601,b'Which models can be using for encoder-decoder?',2020-05-26T15:02:28Z,2020-05-27T12:02:37Z,,TypeError,"TypeError: forward() got an unexpected keyword argument 'encoder_hidden_states'"
4600,b'Functionality for addressing imbalance data points?',2020-05-26T13:03:47Z,2020-07-12T17:56:11Z,,,
4599,"b""ImportError: cannot import name 'AutoModelForQuestionAnswering' from 'transformers """,2020-05-26T12:20:32Z,2020-05-26T13:27:35Z,,**ImportError,**ImportError: cannot import name 'AutoModelForQuestionAnswering' from 'transformers' (C:\Users\oguzk\anaconda3\lib\site-packages\transformers\__init__.py)**
4598,b'[Reformer] automate axial_pos_shape',2020-05-26T12:11:19Z,2020-05-26T13:08:52Z,,,
4597,b'[Draft] Bharaax outputattentions',2020-05-26T12:02:16Z,2020-06-08T11:56:22Z,,,
4596,"b""AttributeError: 'Namespace' object has no attribute 'to_json_string'""",2020-05-26T11:18:23Z,2020-05-27T01:07:28Z,,,
4595,b'KeyError when loading a trained EncoderDecoder model',2020-05-26T11:10:18Z,2020-06-03T09:25:28Z,,,
4594,"b'KeyError: ""Unable to open object (object \'bias:0\' doesn\'t exist)""'",2020-05-26T10:25:18Z,2020-05-27T01:46:22Z,,KeyError,"KeyError: ""Unable to open object (object 'bias:0' doesn't exist)"""
4593,"b'[Longformer For Question Answering] Conversion script, doc, small fixes'",2020-05-26T09:29:21Z,2020-05-26T12:58:48Z,,,
4592,"b'IndexError: Dimension out of range (expected to be in range of [-1, 0], but got 1)'",2020-05-26T09:11:41Z,2020-05-27T03:04:03Z,,,
4591,b'Create README.md',2020-05-26T08:25:18Z,2020-05-26T20:50:09Z,model card,,
4590,b'[Model hub web parsing MD code error]',2020-05-26T07:45:44Z,2020-05-26T14:41:05Z,,,
4589,b'[LongformerForQuestionAnswering] fix qa example in docstring',2020-05-26T07:45:10Z,2020-05-26T10:06:48Z,,,
4588,b'Help Wanted: Predict Next Two Tokens',2020-05-26T03:05:28Z,2020-08-01T04:18:37Z,wontfix,,
4587,b'ensure_ascii=False',2020-05-26T02:17:28Z,2020-05-26T02:17:58Z,,,
4586,b'T5Model in fp16 still yield nan with more complex examples',2020-05-25T22:31:43Z,2021-04-25T15:05:13Z,,,
4585,b'Introduce a new tensor type for return_tensors on tokenizer for NumPy',2020-05-25T22:22:12Z,2020-06-04T04:57:01Z,,,
4584,b'[ci] fix 3 remaining slow GPU failures',2020-05-25T22:11:58Z,2020-05-25T23:20:51Z,,,
4583,b'Provide simple way to train a new translation model from scratch',2020-05-25T21:33:51Z,2021-05-15T15:02:45Z,,,
4582,b'Improve model card for Tereveni-AI/gpt2-124M-uk-fiction',2020-05-25T21:30:11Z,2020-05-26T20:51:41Z,model card,,
4581,"b'[GPT2, CTRL] Allow input of input_ids and past of variable length'",2020-05-25T19:44:24Z,2020-05-26T17:43:59Z,,,
4580,b'LongformerForSequenceClassification',2020-05-25T18:22:39Z,2020-05-27T20:30:01Z,,,
4579,b'How to save tokenize data when training from scratch',2020-05-25T17:16:45Z,2020-08-01T15:52:02Z,wontfix,,
4578,b'Create model card',2020-05-25T16:36:14Z,2020-05-25T19:28:31Z,model card,,
4577,b'Using whole word masking on training LM from scratch',2020-05-25T14:31:34Z,2020-11-07T11:26:08Z,,,
4576,"b""OSError: Model name 'transfo-xl-wt103' was not found in tokenizers model name list (transfo-xl-wt103). We assumed 'transfo-xl-wt103' was a path, a model identifier, or url to a directory containing vocabulary files named ['vocab.bin', 'vocab.txt'] but couldn't find such vocabulary files at this path or url.""",2020-05-25T14:22:09Z,2020-08-01T04:18:40Z,wontfix,,
4575,b'Onnx notebook problem',2020-05-25T14:19:37Z,2020-05-28T05:14:38Z,,,
4574,b'Fix longformer attention mask type casting when using apex',2020-05-25T12:40:22Z,2020-05-29T16:13:31Z,,,
4573,"b""Transformers' trainer sequence classification problem""",2020-05-25T12:28:05Z,2020-07-04T10:44:14Z,,RuntimeError,"RuntimeError: expected scalar type Long but found Float"
4572,b'Typo in GPT2 documentation ',2020-05-25T11:48:08Z,2020-08-01T04:18:39Z,wontfix,,
4571,"b""cannot import name 'TFElectraModel' from 'transformers'""",2020-05-25T09:59:12Z,2020-07-30T10:59:34Z,wontfix,,
4570,b'Model card: Updated the link to the paper',2020-05-25T08:26:03Z,2020-05-25T19:29:51Z,model card,,
4569,b'bert embedding make OOM in albert',2020-05-25T07:36:15Z,2020-08-01T04:18:38Z,wontfix,,
4568,b'\xe2\x9d\x93 [BART] Different embedding sizes between pre-trained / fine-tuned checkpoint',2020-05-25T07:24:40Z,2020-05-26T23:36:32Z,,,
4567,b'\xe2\x9d\x93 [BART] Why using bias for LM head if not trained ? ',2020-05-25T05:39:48Z,2020-05-25T23:59:07Z,,,
4566,b'variable name changes for Issue #4141',2020-05-25T05:25:51Z,2020-08-01T15:51:55Z,wontfix,,
4565,"b""changing config.axial_pos_shape for 'ReformerModelWithLMHead' when fine-tuning """,2020-05-25T02:34:08Z,2020-08-22T05:34:18Z,"wontfix, reformer",,
4564,b'[Reformer] fix reformer num buckets',2020-05-24T18:57:31Z,2020-05-25T20:04:45Z,,,
4563,b'Decoding with DistilmBERT to generate text in different languages',2020-05-24T18:35:39Z,2020-08-01T15:52:00Z,wontfix,,
4562,b'implementation of transformers for abstractive summarization task',2020-05-24T17:28:58Z,2020-05-25T12:24:40Z,,,
4561,b'Fix the example command for SQuAD',2020-05-24T15:01:17Z,2020-05-27T01:14:14Z,,,
4560,b'Albert Tokenizer hangs',2020-05-24T14:38:51Z,2020-08-27T12:49:11Z,"wontfix, Core: Tokenization",,
4559,b'XLnet loss and accuracy not decreasing',2020-05-24T14:31:15Z,2020-05-24T19:32:35Z,,,
4558,b'Add DistilBERT to supported run_language_modeling models',2020-05-24T14:25:25Z,2020-05-25T18:50:46Z,,,
4557,b'Cleaner warning when loading pretrained models',2020-05-24T11:56:59Z,2020-06-22T19:58:48Z,,,
4556,b'Added reference to use for citing this model',2020-05-24T10:02:00Z,2020-05-25T19:12:23Z,model card,,
4549,b'Example script for SQuAD question answering unable to reproduce the claimed performance',2020-05-24T08:13:05Z,2020-05-27T01:24:21Z,Ex: Question Answering,,
4548,b'[WIP] Replace instances of  `config.output_hidden_states` with function argument  `output_hidden_states` in all possible models.',2020-05-24T07:56:03Z,2020-06-14T19:38:33Z,,,
4547,b'LongformerTokenizerFast',2020-05-24T06:22:48Z,2020-05-25T20:03:56Z,,,
4546,b'Fix two bugs on MNLI dataset and SST-2 respectively.',2020-05-24T06:16:15Z,2020-05-29T15:12:25Z,,,
4545,b'pass lowercase to fast tokenizer',2020-05-24T05:42:15Z,2020-07-31T10:24:05Z,wontfix,,
4544,b'seems that run_ner.py cannot handle the situation when example length exceed max_length? ',2020-05-24T03:37:17Z,2020-06-22T16:27:12Z,,,
4543,b'Automatically setting number of LSH buckets in Reformer may give invalid value',2020-05-24T03:33:53Z,2020-05-25T20:04:45Z,reformer,,
4542,b'RuntimeError: The size of tensor a (1025) must match the size of tensor b (1024) at non-singleton dimension 3',2020-05-24T03:22:35Z,2020-05-24T22:43:30Z,,RuntimeError,"RuntimeError: The size of tensor a (1024) must match the size of tensor b (1025) at non-singleton dimension 3"
4541,"b""'use_fast=True' results in 'TypeError' when trying to save tokenizer via AutoTokenizer""",2020-05-24T03:21:06Z,2020-08-08T04:56:56Z,Fast Tokenizers,,
4540,b'InvalidArgumentError while using GRU layer in custom training loop',2020-05-23T23:18:44Z,2020-08-02T01:55:04Z,"wontfix, TensorFlow",InvalidArgumentError,"InvalidArgumentError: 2 root error(s) found."
4539,b'Add BART fine-tuning summarization community notebook',2020-05-23T21:15:38Z,2020-05-26T14:43:42Z,,,
4538,b'[All models] Extend config.output_attentions with output_attentions function arguments',2020-05-23T17:49:56Z,2020-06-09T21:39:07Z,,,
4537,"b'DOC: Make `import torch` explicit for ""Quick tour TF 2.0"" example'",2020-05-23T17:44:17Z,2020-07-30T10:53:26Z,wontfix,NameError,"NameError: name 'BertForSequenceClassification' is not defined"
4536,b'resize_token_embeddings not implemented for TFGPT2LMHeadModel',2020-05-23T10:00:21Z,2020-08-09T16:32:23Z,wontfix,,
4535,b'How to speed up inference step in BertQuestionAnswering?',2020-05-23T08:01:40Z,2020-05-28T07:22:41Z,"External, Distillation",,
4534,b'DOC: Fix typos in modeling_auto',2020-05-23T06:01:17Z,2020-05-23T13:41:00Z,,,
4533,b'Add nn.Module as superclass',2020-05-23T04:36:27Z,2020-05-25T19:29:34Z,,,
4532,"b""MMBT doesn't inherit from nn.Module""",2020-05-23T04:30:04Z,2020-05-25T19:29:33Z,"PyTorch, Core: Modeling",TypeError,"TypeError: 'MMBTModel' object is not callable"
4531,b'Fix add_special_tokens on fast tokenizers',2020-05-23T01:01:33Z,2020-05-28T14:54:46Z,,,
4530,b'Tensorflow improvements',2020-05-22T22:45:50Z,2020-06-04T23:45:54Z,,,
4529,"b""Minor correction in Roberta Model docs, Roberta doesn't use NSP""",2020-05-22T22:41:54Z,2020-12-05T05:00:56Z,"wontfix, Documentation",,
4528,b'Warn the user about max_len being on the path to be deprecated.',2020-05-22T21:13:30Z,2020-05-22T22:08:31Z,,,
4527,"b""Tokenizers bug: version 2.10 doesn't honor `max_len` when instantiating a pretrained model""",2020-05-22T20:27:39Z,2020-05-22T21:48:57Z,,,
4526,b'link to paper was broken',2020-05-22T19:09:10Z,2020-05-22T19:17:09Z,,,
4525,b'Error in Longformer attention mask using apex mixed precision',2020-05-22T18:59:25Z,2020-05-31T03:58:21Z,PyTorch,RuntimeError,"RuntimeError: expected dtype Half but got dtype Float"
4524,b'Codecov migration to marketplace app',2020-05-22T18:16:23Z,2020-08-01T23:51:58Z,wontfix,,
4523,"b""Can't reproduce export to onnx with custom bert model""",2020-05-22T15:17:40Z,2020-06-02T14:41:10Z,External,,
4522,b'Added huseinzol05/t5-small-bahasa-cased README.md',2020-05-22T13:53:34Z,2020-05-22T19:04:07Z,model card,,
4521,b'Using DistillBert to train Bert (run_languge_modeling.py) for some languge from scratch',2020-05-22T13:11:20Z,2020-07-29T10:56:05Z,"wontfix, Ex: LM (Pretraining), Distillation",,
4520,b'How to use its own custom Optimizer (GLUE Example)',2020-05-22T10:28:13Z,2020-05-22T19:35:42Z,,,
4519,b'Specify device in DataCollator ',2020-05-22T10:21:11Z,2021-06-14T15:07:49Z,,,
4518,b'[marian] possible memory leak problem while translating & extracting internal representations',2020-05-22T10:08:06Z,2020-05-22T11:16:55Z,,,
4517,b'How to train a custom seq2seq model with BertModel',2020-05-22T10:02:43Z,2020-06-03T16:02:45Z,seq2seq,,
4516,"b'sometimes loss starts with nan when running ""Quick tour TF 2.0 training and PyTorch interoperability"" script'",2020-05-22T09:33:35Z,2020-05-25T08:25:18Z,"Ex: Sequence Classification, TensorFlow",,
4515,b'Allow BatchEncoding to be pickled',2020-05-22T09:21:07Z,2020-06-15T22:38:57Z,,,
4514,b'\xe2\x9d\x93 How Linear layer difference between TF2 and PT are handled ?',2020-05-22T07:46:46Z,2020-05-23T06:36:29Z,"PyTorch, TensorFlow",,
4513,"b""Couldn't reach server GPT-2 """,2020-05-22T07:24:02Z,2020-05-29T12:58:13Z,External,,
4512,"b""ValueError: TracedModules don't support parameter sharing between modules""",2020-05-22T07:06:09Z,2020-07-29T10:56:01Z,"Need more information, wontfix, PyTorch",ValueError,"ValueError: TracedModules don't support parameter sharing between modules"
4511,"b""AttributeError: 'SummaryWriter' object has no attribute 'add_hparams'""",2020-05-22T02:25:26Z,2020-05-22T09:24:41Z,Need more information,AttributeError,"AttributeError: 'SummaryWriter' object has no attribute 'add_hparams'"
4510,b'[HUGE] Refactoring tokenizers backend - padding - truncation - pre-tokenized pipeline - fast tokenizers - tests',2020-05-22T00:18:23Z,2020-06-15T21:12:52Z,,,
4509,b'Add packaging to setup.py',2020-05-21T22:11:54Z,2020-05-22T16:31:47Z,,ModuleNotFoundError,"ModuleNotFoundError: No module named 'packaging'"
4508,b'FillMaskPipeline crashes when executed on TPU',2020-05-21T21:11:55Z,2020-05-22T16:36:48Z,PyTorch,,
4507,b'Hard-coded force_download in run_squad forces expensive community download ',2020-05-21T20:50:13Z,2020-05-27T17:00:14Z,"Ex: Question Answering, Usage",,
4506,b'[Summarization Pipeline]: Fix default tokenizer',2020-05-21T20:33:04Z,2020-05-22T21:49:46Z,,,
4505,b'add 2 colab notebooks',2020-05-21T18:31:05Z,2020-05-28T09:18:17Z,,,
4504,b'SummarizationPipeline crashes',2020-05-21T18:26:14Z,2020-05-22T21:49:46Z,"Core: Pipeline, Summarization",AttributeError,"AttributeError: 'dict' object has no attribute 'batch_encode_plus'"
4503,b'Fix convert_token_type_ids_from_sequences for fast tokenizers',2020-05-21T17:10:40Z,2020-05-22T16:45:10Z,,,
4502,b'How to finetune ELECTRA on glue?',2020-05-21T15:46:09Z,2020-05-22T13:51:18Z,"Ex: Sequence Classification, Core: Modeling",ValueError,"ValueError: Unrecognized configuration class <class 'transformers.configuration_electra.ElectraConfig'> for this kind of AutoModel: AutoModelForSequenceClassification."
4501,b'Pipelines do not control input sequences longer than those accepted by the model',2020-05-21T14:53:40Z,2020-08-22T19:29:41Z,"wontfix, Core: Pipeline",RuntimeError,"RuntimeError: index out of range: Tried to access index 512 out of table with 511 rows. at /tmp/pip-req-build-808afw3c/aten/src/TH/generic/THTensorEvenMoreMath.cpp:418"
4500,b'Longformer for question answering',2020-05-21T14:35:07Z,2020-05-25T16:43:37Z,,,
4499,b'[T5] Fix Cross Attention position bias',2020-05-21T14:28:30Z,2020-05-26T12:57:25Z,,,
4498,"b""Pre-trained electra-large model doesn't converge when fine-tuned on SST-2""",2020-05-21T14:20:32Z,2020-07-31T10:24:06Z,"Need more information, wontfix",,
4497,"b'Tokenize something with a ""."" in between Decode these ids, you will find it mismatch'",2020-05-21T09:06:53Z,2020-05-22T03:44:01Z,,,
4496,"b""python run_glue.py with the AttributeError: 'NoneType' object has no attribute 'seek'""",2020-05-21T08:56:30Z,2020-07-29T10:55:56Z,"Need more information, wontfix, Ex: Sequence Classification","AttributeError, OSError","AttributeError: 'NoneType' object has no attribute 'seek'OSError: Unable to load weights from pytorch checkpoint file. If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True. "
4495,b'\xe2\x9d\x93 [BART] Why Decoder Layer Normalization is applied only at the last layer ? ',2020-05-21T07:37:20Z,2020-05-21T23:35:32Z,,,
4494,"b""Incorporate HuggingFace 'nlp' library in examples""",2020-05-21T03:34:49Z,2020-07-29T10:55:57Z,"wontfix, External",,
4493,b'Use args.per_gpu_train_batch_size instead of args.train_batch_size in\xe2\x80\xa6',2020-05-21T03:15:34Z,2020-05-22T20:16:56Z,,,
4492,b'Cannot load reformer-enwik8 tokenizer',2020-05-20T23:16:46Z,2020-05-22T09:22:37Z,"Core: Tokenization, reformer",OSError,"OSError: Model name 'google/reformer-enwik8' was not found in tokenizers model name list (google/reformer-crime-and-punishment). We assumed 'google/reformer-enwik8' was a path, a model identifier, or url to a directory containing vocabulary files named ['spiece.model'] but couldn't find such vocabulary files at this path or url."
4491,"b""Windows: Can't find vocabulary file for MarianTokenizer""",2020-05-20T22:47:34Z,2020-06-17T20:31:06Z,"Core: Tokenization, marian","FileNotFoundError, OSError","FileNotFoundError: [WinError 2] The system cannot find the file specifiedOSError: Unable to load vocabulary from file. Please check that the provided vocabulary is accessible and not corrupted."
4490,b'How to load a pruned Albert model with from_pretrained()?',2020-05-20T22:46:33Z,2020-05-21T18:20:07Z,,RuntimeError,"RuntimeError: Error(s) in loading state_dict for AlbertForSequenceClassification:"
4489,b'bugfix: pass on tokenizer to pipeline in load_graph_from_args',2020-05-20T18:01:52Z,2020-05-20T20:23:22Z,,,
4488,b'Make changes to german-bert vocab file more prominent',2020-05-20T16:01:02Z,2020-05-20T20:08:29Z,model card,,
4487,b'Fix slow gpu tests lysandre',2020-05-20T15:28:02Z,2020-05-20T15:59:46Z,,,
4486,b'tokenizer.vocab has not changed after using add_tokens',2020-05-20T14:29:23Z,2020-07-29T10:56:03Z,"wontfix, Core: Tokenization",,
4485,"b""Can't find vocabulary file or is corrupted for MarianTokenizer""",2020-05-20T12:42:24Z,2020-05-22T09:39:37Z,,"FileNotFoundError, OSError","FileNotFoundError: [WinError 2] The system cannot find the file specifiedOSError: Unable to load vocabulary from file. Please check that the provided vocabulary is accessible and not corrupted."
4484,b'Bug using Roberta models in QA Transformers pipeline.',2020-05-20T11:53:48Z,2020-08-01T15:51:59Z,"wontfix, Core: Pipeline, Core: Modeling",,
4483,b'Trying to add support for GPT2 as decoder in EncoderDecoder model',2020-05-20T11:24:44Z,,"Core: Encoder-Decoder, Good First Issue",,
4482,b'Create model card for RuPERTA-base-finetuned-pos',2020-05-20T11:14:55Z,2020-05-20T13:45:50Z,model card,,
4481,b'Add mecab dependency on slow tests.',2020-05-20T08:28:59Z,2020-05-20T22:27:48Z,,,
4480,b'[Reformer] Include char lm to Trainer',2020-05-20T07:57:07Z,2020-06-30T12:56:03Z,,,
4479,b'[examples] fix no grad in second pruning in run_bertology',2020-05-20T07:54:24Z,2020-05-21T13:17:04Z,,,
4478,b'\xe2\x9d\x93 [TPU] [Trainer] Moving model to device before setting optimizer slow the training ',2020-05-20T06:45:13Z,2020-07-29T10:55:58Z,wontfix,,
4477,b'Remove warning of deprecation',2020-05-20T06:37:09Z,2020-05-20T20:48:30Z,,,
4476,b'Tokenizer encode to have an option to overflow from left',2020-05-20T04:44:50Z,2020-11-07T05:42:37Z,"wontfix, High-Level feature, Core: Tokenization",,
4475,b'Request for hosting model files in a Virtual Hosted-Style S3 buckets',2020-05-20T04:08:29Z,2020-08-01T15:52:01Z,wontfix,,
4474,b'Remove warning of deprecation',2020-05-20T00:00:41Z,2020-05-20T00:33:19Z,,,
4473,b'Add Fine-tune DialoGPT on new datasets notebook',2020-05-19T23:33:20Z,2020-05-20T20:17:53Z,,,
4472,b'[gpu slow tests] fix mbart-large-enro gpu tests',2020-05-19T23:18:39Z,2020-05-19T23:45:32Z,,,
4471,b'batch_encode_plus returns same lengths when enable pad_to_max_length',2020-05-19T21:38:17Z,2020-07-29T10:56:00Z,"wontfix, Core: Tokenization",,
4470,b'Model card for Tereveni-AI/gpt2-124M-uk-fiction',2020-05-19T19:46:42Z,2020-05-20T13:44:27Z,model card,,
4469,b'Better None gradients handling in TF Trainer',2020-05-19T19:38:16Z,2020-05-20T20:46:22Z,,,
4468,"b'[Tests, GPU, SLOW] fix a bunch of GPU hardcoded tests in Pytorch'",2020-05-19T19:06:14Z,2020-05-19T19:35:05Z,,,
4467,b'TPU hangs when saving optimizer/scheduler',2020-05-19T19:03:13Z,2020-05-21T13:18:28Z,,,
4466,b'Model card for RuPERTa-base fine-tuned for NER',2020-05-19T17:34:05Z,2020-05-20T13:45:25Z,model card,,
4465,b'[ci] Slow GPU tests run daily',2020-05-19T17:05:06Z,2020-05-25T21:28:03Z,,,
4464,b'[Longformer] Docs and clean API',2020-05-19T16:00:01Z,2020-05-19T19:52:37Z,,,
4463,"b'Adds predict stage for glue tasks, and generate result files which can be submitted to gluebenchmark.com'",2020-05-19T14:55:50Z,2020-05-21T13:17:46Z,,,
4462,b'add T5 fine-tuning notebook [Community notebooks]',2020-05-19T14:51:08Z,2020-05-19T16:26:29Z,,,
4461,b'ProjectedAdaptiveLogSoftmax.log_prob raises Exception',2020-05-19T14:41:12Z,2020-07-25T15:38:44Z,wontfix,,
4460,b'Attempt to do some optimizations for BERT models',2020-05-19T14:36:50Z,2020-07-25T15:38:41Z,wontfix,,
4459,b'Pretrained Transformer-XL gives unreasonable result on WikiText-103',2020-05-19T14:33:04Z,2020-07-25T15:38:45Z,wontfix,,
4458,b'layer name change to match compatibility with pytorch layer name in BertForQuestionAnswering',2020-05-19T13:30:46Z,2020-07-25T14:38:42Z,wontfix,`AttributeError,"`AttributeError: 'BertForQuestionAnswering' object has no attribute 'classifier'`"
4457,b'FastTokenizer add_special_tokens also adding individual characters for multi character tokens',2020-05-19T13:00:40Z,2020-05-28T14:54:46Z,Fast Tokenizers,,
4456,b'Problems About Using the Run_language_modeling with Tf2.',2020-05-19T12:57:43Z,2020-05-20T01:48:37Z,,`ValueError,"`ValueError: Trying to create optimizer slot variable under the scope for tf.distribute.Strategy (<tensorflow.python.distribute.one_device_strategy.OneDeviceStrategy object at 0x15b49bed0>), which is different from the scope used for the original variable (<tf.Variable 'tf_bert_for_masked_lm/bert/embeddings/word_embeddings/weight:0' shape=(21128, 768) dtype=float32, numpy=array(),dtype=float32)>). Make sure the slot variables are created under the same strategy scope. This may happen if you're restoring from a checkpoint outside the scope`"
4455,b'get output from a particular layer of pre-trained transformer (xlnet) ',2020-05-19T11:15:54Z,2020-05-22T19:27:08Z,,,
4454,b'DMOZ - web page classification / multi-language',2020-05-19T10:59:51Z,2020-07-27T17:34:04Z,wontfix,,
4453,b'Bug - TFBertForSequenceClassification on SQUaD data',2020-05-19T10:02:29Z,2020-05-25T07:11:27Z,,AttributeError,"AttributeError: 'NoneType' object has no attribute 'strip'"
4452,b'Value matrix of self-attention',2020-05-19T07:39:10Z,2020-05-19T07:40:58Z,,,
4451,b'\xe2\x9d\x93 Warning : This overload of addcdiv_ is deprecated',2020-05-19T05:36:26Z,2020-05-20T20:48:30Z,,,
4450,b'[Trainer] move model to device before setting optimizer',2020-05-19T03:09:00Z,2020-05-19T03:13:34Z,,,
4449,"b""[Questions & Help] The loss doesn't decrease correctly while training BERT from scratch""",2020-05-19T03:00:42Z,2020-07-25T05:38:43Z,wontfix,,
4448,b'Correct TF formatting to exclude LayerNorms from weight decay',2020-05-19T02:46:46Z,2020-05-20T20:46:00Z,,,
4447,b'TF Beam Search generation seems to be flaky sometimes',2020-05-18T23:12:20Z,2020-06-08T13:32:09Z,,,
4446,b'Make get_last_lr in trainer backward compatible',2020-05-18T23:08:18Z,2020-05-19T00:17:36Z,,,
4445,b'Generation with EncoderDecoder Model',2020-05-18T19:00:52Z,2020-08-05T23:30:08Z,wontfix,,
4444,b'model.save() does not save keras model that includes DIstillBert layer',2020-05-18T18:00:40Z,2021-11-11T14:21:50Z,,ValueError,"ValueError: The two structures don't have the same nested structure."
4443,b'Issues with the EncoderDecoderModel for sequence to sequence tasks',2020-05-18T17:36:26Z,2020-11-24T02:58:54Z,wontfix,ValueError,"ValueError: You have to specify either input_ids or inputs_embeds"
4442,b'[Communtiy notebooks] Fine-tuning / Training ',2020-05-18T15:48:58Z,2020-05-18T15:58:08Z,,,
4441,b'[Community notebooks] General notebooks',2020-05-18T15:38:31Z,2020-05-18T18:23:58Z,,,
4440,b'Reformer training error',2020-05-18T15:15:00Z,2020-05-18T23:05:21Z,,"**TypeError, TypeError","**TypeError: forward() got an unexpected keyword argument 'masked_lm_labels'**TypeError: forward() got an unexpected keyword argument 'masked_lm_labels'"
4439,"b""Avoid abort due to missing paths in case of '--save_total_limit' argument""",2020-05-18T15:07:00Z,2020-07-25T02:38:40Z,wontfix,,
4438,b'BERT Fine-tuning problems',2020-05-18T14:47:31Z,2020-07-25T02:38:58Z,wontfix,"TypeError, subprocess.CalledProcessError","TypeError: string indices must be integerssubprocess.CalledProcessError: Command '['/home/address/anaconda3/bin/python', '-u', './examples/question-answering/run_squad.py', '--local_rank=1', '--model_type', 'bert', '--model_name_or_path', 'bert-base-uncased', '--do_train', '--do_eval', '--train_file', '/home/address/Desktop/address/train_split.json', '--predict_file', '/home/address/Desktop/address/val_split.json', '--learning_rate', '3e-5', '--num_train_epochs', '2', '--max_seq_length', '384', '--doc_stride', '128', '--output_dir', '../models/wwm_uncased_finetuned_squad/', '--per_gpu_eval_batch_size=3', '--per_gpu_train_batch_size=3']' returned non-zero exit status 1."
4437,b'Added model cards for Romanian BERT models',2020-05-18T13:50:49Z,2020-05-18T22:48:57Z,model card,,
4436,b'[T5 fp16] Fix fp16 in T5',2020-05-18T13:49:42Z,2020-05-18T15:25:59Z,,,
4435,b'added model card for german-sentiment-bert',2020-05-18T13:18:08Z,2020-05-18T22:44:42Z,model card,,
4434,b'albertModel object has no attribute bias',2020-05-18T11:58:07Z,2020-06-30T23:17:52Z,,AttributeError,AttributeError: 'AlbertModel' object has no attribute 'bias'
4433,b'Create README.md',2020-05-18T09:26:50Z,2020-05-18T22:41:35Z,model card,,
4432,b'Tag onnx export tests as slow',2020-05-18T08:53:08Z,2020-05-18T13:24:42Z,,,
4431,b'Adding optimizations block from ONNXRuntime.',2020-05-18T08:45:12Z,2020-05-18T18:32:33Z,,,
4430,b'\xf0\x9f\x90\x9b Weird learning rate with TPU Trainer',2020-05-18T08:25:03Z,2020-07-25T02:39:08Z,wontfix,,
4429,b'mbart config.json missing',2020-05-18T07:28:46Z,2020-05-20T11:51:51Z,,,
4428,b'How to extract the best candidate after token classification?',2020-05-18T05:55:51Z,2020-07-25T02:39:07Z,wontfix,,
4427,b'Refactored the README.md file',2020-05-18T04:46:46Z,2020-05-19T13:56:25Z,model card,,
4426,b'Lack of funetune examples for T5 model',2020-05-18T03:22:57Z,2020-06-03T13:51:07Z,,,
4425,b'BERT and other models pretraining from scratch example',2020-05-18T03:12:46Z,2020-10-04T20:13:40Z,wontfix,,
4424,b'Update README.md (model_card)',2020-05-18T02:22:12Z,2020-05-18T22:18:18Z,model card,,
4423,b'How to change transformers model embedding layer weights',2020-05-17T23:32:45Z,2020-07-25T02:39:06Z,wontfix,,
4422,b'[T5 Conf] rename docstring to acuatly argument names',2020-05-17T23:05:12Z,2020-05-18T15:31:36Z,,,
4421,"b'[test_pipelines] Mark tests > 10s @slow, small speedups'",2020-05-17T22:54:10Z,2020-05-18T16:23:21Z,,,
4420,"b'BERT Tokenization problem when the input string has a ""."" in the string, like floating number'",2020-05-17T22:37:40Z,2020-07-25T02:39:05Z,wontfix,,
4419,b'[TF generate] Fix issue for batch output generation of different output length.',2020-05-17T21:45:52Z,2020-05-18T13:51:41Z,,,
4418,b'Scaling text classification / reusing models',2020-05-17T20:09:50Z,2020-07-25T02:38:59Z,wontfix,,
4417,b'TypeError: add_() takes 1 positional argument but 2 were given',2020-05-17T19:25:49Z,2020-05-18T15:16:23Z,,TypeError,"TypeError: add_() takes 1 positional argument but 2 were given"
4416,b'Fixed spelling of training',2020-05-17T18:15:40Z,2020-05-18T15:23:30Z,,,
4415,b'GPT2 perplexity rolling/striding way for evaluating a document.',2020-05-17T17:47:43Z,2020-07-25T02:39:02Z,wontfix,,
4414,b'Get BERT sentence encoding',2020-05-17T15:33:36Z,2020-07-25T02:39:03Z,wontfix,,
4413,b'Modify example of usage',2020-05-17T15:16:17Z,2020-05-18T22:17:34Z,model card,,
4412,b'Tensorflow NER Training script Not working',2020-05-17T14:28:21Z,2020-05-18T13:55:26Z,,,
4411,b'Pipeline for Conditional Generation (T5 type models)',2020-05-17T12:10:22Z,2020-09-02T11:34:36Z,,,
4410,b'Remove pytorch codes in Class TFXLNetMainLayer',2020-05-17T12:05:55Z,2020-05-26T12:51:28Z,,,
4409,b'add model card for t5-base-squad',2020-05-17T07:16:30Z,2020-05-18T22:17:14Z,model card,,
4408,b'Request to add MobileBert',2020-05-17T06:33:25Z,2020-05-18T13:49:00Z,New model,,
4407,b'fix(run_language_modeling): use arg overwrite_cache',2020-05-17T04:57:00Z,2020-05-18T15:37:36Z,,,
4406,b'Summarization Fine Tuning ',2020-05-17T01:50:39Z,2020-12-20T13:34:40Z,"Discussion, wontfix",,
4405,b'add BERT trained from review corpus.',2020-05-16T22:33:40Z,2020-05-20T13:42:36Z,model card,,
4404,b'feat(wandb): display logger',2020-05-16T21:16:01Z,2020-05-18T15:44:36Z,,,
4403,b'Map optimizer to correct device after loading from checkpoint.',2020-05-16T18:48:42Z,2020-05-19T03:16:06Z,,,
4402,"b""Run Language Modeling on 8 TPU cores doesn't seem to terminate""",2020-05-16T12:03:33Z,2020-05-19T02:28:27Z,,,
4401,b'[TF T5] More coherent naming for inputs',2020-05-16T10:51:57Z,2020-05-18T15:34:01Z,,,
4400,b'BertWordPieceTokenizer cannot be pickled',2020-05-16T05:32:49Z,2020-07-27T17:34:05Z,wontfix,,
4399,b'Pipeline for question generation',2020-05-16T05:01:39Z,2020-12-12T20:44:44Z,wontfix,,
4398,b'Trainer is missing sampler.set_epoch for distributed mode',2020-05-16T01:14:40Z,2020-05-19T02:02:40Z,,,
4397,b'Training TFBertForQuestionAnswering on custom SquadV1 data',2020-05-16T00:59:19Z,2020-05-19T09:35:39Z,,"AttributeError, ValueError, tensorflow.python.framework.errors_impl.InvalidArgumentError","AttributeError: 'str' object has no attribute 'dtype'ValueError: Unknown entries in loss dictionary: ['start_position', 'end_position']. Only expected following keys: ['output_1', 'output_2']tensorflow.python.framework.errors_impl.InvalidArgumentError: 2 root error(s) found."
4396,b'Wrong model or tokenizer for MarianMT',2020-05-16T00:55:21Z,2020-05-16T01:05:34Z,,,
4395,b'MarianMT = How to return 5 best candidates for a translation.  ',2020-05-15T23:19:27Z,2020-07-29T01:32:49Z,"High-Level feature, marian",,
4394,b'the special token of XLNet',2020-05-15T21:48:59Z,2020-07-22T10:52:44Z,wontfix,,
4393,b'BertTokenizerFast does not load custom vocab created from Tokenizer library',2020-05-15T21:16:57Z,2020-10-10T08:14:00Z,wontfix,,
4392,b'MarianMTModel translate {tgt}-{src}',2020-05-15T18:43:31Z,2020-05-15T19:29:48Z,marian,,
4391,b'[PretrainedTokenizer] is  <unk> a special token?',2020-05-15T18:26:51Z,2020-07-22T10:52:43Z,"wontfix, Core: Tokenization",,
4390,b'[cleanup] test_tokenization_common.py',2020-05-15T18:05:54Z,2020-05-19T14:46:56Z,,,
4389,b'[MarianTokenizer] implement save_vocabulary and other common methods',2020-05-15T17:53:33Z,2020-05-19T23:45:50Z,marian,,
4388,"b'[docs] AutoModelWithLMHead(model_name, **kwargs)'",2020-05-15T15:37:27Z,2020-07-27T12:38:32Z,Documentation,,
4387,b'[Bart/Marian] ignore output_attentions when invoked through AutoModel',2020-05-15T15:16:28Z,2020-05-15T15:40:27Z,marian,,
4386,b'Finetuning BERT classifier on a non-GLUE dataset in GLUE format',2020-05-15T14:08:39Z,2020-07-22T10:52:40Z,wontfix,,
4385,b'Should return overflowing information for the log',2020-05-15T13:46:31Z,2020-05-15T13:49:12Z,,,
4384,b'Attempt to unpin torch version for Github Action.',2020-05-15T13:39:21Z,2020-05-15T13:47:16Z,,,
4383,b'Issue with lr during training from scratch',2020-05-15T12:28:26Z,2020-05-15T13:23:04Z,,,
4382,b'Need clarity on training Albert from scratch',2020-05-15T12:11:23Z,2020-10-05T07:56:44Z,,TypeError,"TypeError: not a string"
4381,b'Unknown task fill-mask',2020-05-15T09:39:27Z,2020-05-16T06:33:57Z,,,
4380,b'max_qa_length is needed for funetune on multiple-choice problems',2020-05-15T09:20:08Z,2020-05-15T13:49:12Z,,,
4379,b'MNLI finetuning results affected by values of max_steps',2020-05-15T07:03:33Z,2020-07-22T10:52:42Z,wontfix,,
4378,b'Implement SuperGLUE tasks and baselines',2020-05-15T04:33:10Z,2021-04-25T15:05:15Z,,,
4377,b'Added README huseinzol05/t5-base-bahasa-cased',2020-05-15T04:21:12Z,2020-05-18T22:10:24Z,model card,,
4376,b'Error on instantiating pipeline.summarizer',2020-05-15T02:31:43Z,2020-07-22T10:52:39Z,wontfix,AttributeError,"AttributeError: 'NoneType' object has no attribute 'config'"
4375,b'[docs] Restore examples.md symlink',2020-05-15T00:43:22Z,2020-05-27T20:41:36Z,,,
4374,b'Speed on various cards',2020-05-14T22:09:48Z,2020-07-22T10:52:45Z,wontfix,,
4373,b'Specify tensorboard logging in BART finetuning',2020-05-14T21:58:31Z,2020-09-05T18:49:38Z,wontfix,,
4372,b'Allow for None gradients in GradientAccumulator.',2020-05-14T21:55:15Z,2020-05-15T13:52:01Z,,,
4371,b'Save pretrained MarianTokenizer',2020-05-14T19:26:43Z,2020-05-19T23:45:50Z,marian,,
4370,b'run_squad with early stopping on a validation set',2020-05-14T19:09:46Z,2020-05-15T09:53:13Z,,,
4369,b'demo website: i info icon should link to resource about parameters',2020-05-14T18:24:13Z,2020-11-22T08:04:35Z,"wontfix, Write With Transformer",,
4368,b'past functionality broken in release 2.9.0 and 2.9.1',2020-05-14T18:23:13Z,2020-05-26T17:43:59Z,,RuntimeError,"RuntimeError: The size of tensor a (4) must match the size of tensor b (5) at non-singleton dimension 3"
4367,b'Fix: unpin flake8 and fix cs errors',2020-05-14T16:34:14Z,2020-05-14T17:14:27Z,,,
4366,b'[pipelines] Failing @slow test for TF Summarization',2020-05-14T16:28:10Z,2020-05-14T17:36:02Z,,,
4365,b'Has anyone successfully used TF2.0 to load pre-trained transformer-XL (wt103) weights and reproduce their sota results',2020-05-14T16:08:34Z,2020-07-14T00:42:57Z,wontfix,,
4364,b'Can not reproduce article numbers',2020-05-14T15:32:44Z,2020-08-27T05:17:31Z,wontfix,,
4363,b'Fix trainer evaluation',2020-05-14T14:39:30Z,2020-05-14T18:39:45Z,,,
4362,b'Trainer crashes on TPU at eval time when prediction_loss_only is True ',2020-05-14T14:30:39Z,2020-05-14T18:39:45Z,,ValueError,"ValueError: zero-dimensional arrays cannot be concatenated"
4361,"b""Trainer doesn't calculate eval loss for models with lm_labels parameter""",2020-05-14T14:30:32Z,2020-05-14T18:39:45Z,,,
4360,b'LayerNorm not excluded from weight decay in TF',2020-05-14T13:25:36Z,2020-05-20T20:46:00Z,,,
4359,b'Create README.md',2020-05-14T12:45:48Z,2020-05-14T18:07:33Z,model card,,
4358,"b""Trainer and Colab TPU: Training loss isn't declining""",2020-05-14T12:32:04Z,2020-05-25T06:40:56Z,,,
4357,b'Create README.md',2020-05-14T10:47:46Z,2020-05-14T18:06:11Z,model card,,
4356,b'GPT2 checkpoint breaks on new transformers version (2.9.1)',2020-05-14T10:24:44Z,2020-06-09T23:25:38Z,,RuntimeError,"RuntimeError: Error(s) in loading state_dict for LMWrapper:"
4355,b'DistilGPT2 Finetuning with GPU',2020-05-14T09:22:21Z,2020-05-15T21:43:04Z,,,
4354,b'Not getting expected results for freshly-pretrained BERT ',2020-05-14T09:02:34Z,2020-07-20T11:07:06Z,wontfix,,
4353,b'Fixing tokenization of extra_id symbols in T5Tokenizer',2020-05-14T02:01:36Z,2020-05-25T20:04:30Z,,,
4352,b'Longformer',2020-05-14T00:46:56Z,2020-05-19T14:04:44Z,,,
4351,b'tf add resize_token_embeddings method',2020-05-14T00:30:32Z,2020-06-18T22:41:27Z,,,
4350,b'RobertaForSequenceClassification for BERT',2020-05-13T23:59:46Z,2020-07-20T01:07:08Z,wontfix,,
4349,b'Ability to specify directory of tensorboard logs in BART finetuning example',2020-05-13T23:58:26Z,2020-07-20T01:07:07Z,wontfix,,
4348,b'Add link to W&B to see whole training logs',2020-05-13T22:59:43Z,2020-05-14T00:04:58Z,model card,,
4347,b'TracerWarning on modeling_gpt2.py:147 when using Torchscript',2020-05-13T21:42:17Z,2020-07-20T01:07:05Z,wontfix,,
4346,b'Discrepancy in the generation of T5 here vs the original code ',2020-05-13T19:23:12Z,2020-06-03T13:50:02Z,,,
4345,b'Add image and metadata',2020-05-13T18:00:11Z,2020-05-14T00:05:16Z,model card,,
4344,b'Added the feature to provide a generation prompt for encoder-decoder models.',2020-05-13T16:45:08Z,2020-07-24T19:01:34Z,,,
4343,b'[docs] XLNetLMHeadModel example in documentation does not produce the right probabilities',2020-05-13T16:45:00Z,2020-05-20T17:42:23Z,,,
4342,b'Added the option to seed generation in encoder-decoder models',2020-05-13T16:16:53Z,2020-05-13T16:40:14Z,,,
4341,b'rerun notebook 02-transformers',2020-05-13T16:02:30Z,2020-05-15T14:33:09Z,,,
4340,b'Support multitask learning',2020-05-13T15:54:33Z,2020-06-03T13:48:09Z,,,
4339,b'TPU needs a rendezvous',2020-05-13T15:12:14Z,2020-05-14T12:59:53Z,,,
4338,"b""can't load checkpoint file from examples/run_language_modeling.py""",2020-05-13T12:34:28Z,2020-05-13T14:40:15Z,,,
4337,"b'Wrong dimensions of input to loss function, multilabel classification'",2020-05-13T11:30:52Z,2020-07-20T07:07:06Z,wontfix,ValueError,"ValueError: Expected input batch_size (1) to match target batch_size (3)."
4336,b'Unable to load weights from pytorch checkpoint file',2020-05-13T10:17:24Z,2020-05-14T15:39:38Z,,"AttributeError, OSError","AttributeError: 'NoneType' object has no attribute 'seek'OSError: Unable to load weights from pytorch checkpoint file. If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True. "
4335,b'[MbartTokenizer] save to sentencepiece.bpe.model',2020-05-13T08:50:20Z,2020-05-18T12:54:04Z,,,
4334,b'[bug in run_glue.py] GlueDataset with no local_rank when init',2020-05-13T08:21:39Z,2020-05-13T12:07:34Z,,,
4333,b'Clarification of model upload instructions',2020-05-13T07:37:20Z,2020-06-09T20:21:19Z,,,
4332,b'Extractive Text Summarization',2020-05-13T06:54:17Z,2020-07-22T10:52:38Z,wontfix,,
4331,b'Add image and link to model paper',2020-05-13T05:39:12Z,2020-05-13T18:00:26Z,model card,,
4330,"b""Can't set attribute 'device'""",2020-05-13T05:10:49Z,2020-07-19T05:54:37Z,wontfix,AttributeError,"AttributeError: can't set attribute"
4329,b'Cache directory',2020-05-13T04:38:27Z,2020-07-19T05:54:36Z,wontfix,OSError,OSError: Couldn't reach server at '{}' to download vocabulary files.```
4328,b'wrong variable name used',2020-05-13T03:37:38Z,2020-05-13T14:22:03Z,model card,,
4327,"b""\xf0\x9f\x90\x9b Trainer on TPU : KeyError '__getstate__'""",2020-05-13T02:54:57Z,2020-07-28T08:27:39Z,,KeyError,"KeyError: '__getstate__'"
4326,b'Train model from scratch with Tensorflow',2020-05-13T00:49:39Z,2020-07-19T10:54:35Z,wontfix,,
4324,b'(v2) Improvements to the wandb integration',2020-05-12T23:00:52Z,2020-05-13T01:52:02Z,,,
4323,"b'Fix FFN dropout in TFAlbertLayer, and split dropout in TFAlbertAttent\xe2\x80\xa6'",2020-05-12T20:48:41Z,2020-08-12T11:52:43Z,,,
4322,b'Uploaded models not appearing on website',2020-05-12T20:02:26Z,2020-05-13T07:38:38Z,,,
4321,b'Add modelcard with acknowledgements',2020-05-12T18:15:43Z,2020-05-12T19:00:57Z,model card,,
4320,b'Question Answering for TF trainer',2020-05-12T17:31:22Z,2020-05-13T13:22:32Z,,,
4319,"b""AutoModel.from_pretrained with torchscript flag raises a TypeError: __init__() got an unexpected keyword argument 'torchscript'""",2020-05-12T16:01:30Z,2020-07-28T07:30:08Z,,`TypeError,"`TypeError: __init__() got an unexpected keyword argument 'torchscript'`"
4318,b'[model_cards]: \xf0\x9f\x87\xb9\xf0\x9f\x87\xb7 Add new ELECTRA small and base models for Turkish',2020-05-12T15:28:14Z,2020-05-12T19:01:18Z,model card,,
4317,b'How to use DistilBertTokenizer in C++',2020-05-12T14:33:29Z,2020-07-18T16:22:27Z,wontfix,,
4316,b'Allow BatchEncoding to be initialized empty.',2020-05-12T14:22:24Z,2020-05-12T19:02:46Z,,,
4315,b'Update README.md',2020-05-12T13:25:03Z,2020-05-12T19:01:35Z,model card,,
4314,"b'run_generation.py GPT2 model only using 1st GPU, OOM error'",2020-05-12T12:50:21Z,2020-07-23T05:26:05Z,wontfix,RuntimeError,"RuntimeError: CUDA out of memory. Tried to allocate 84.00 MiB (GPU 0; 14.76 GiB total capacity; 13.69 GiB already allocated; 37.44 MiB free; 13.93 GiB reserved in total by PyTorch)"
4313,b'Update README.md',2020-05-12T12:49:55Z,2020-05-12T19:01:46Z,model card,,
4312,b'how can I make BERT word embedding faster?',2020-05-12T12:23:54Z,2020-07-20T19:07:05Z,wontfix,,
4311,b'RobertaForQuestionAnsweringModel ',2020-05-12T11:59:41Z,2020-05-12T13:14:40Z,,,
4310,b'MobileBert form google-research added',2020-05-12T10:47:30Z,2020-05-18T13:49:37Z,New model,tensorflow.python.framework.errors_impl.NotFoundError,"tensorflow.python.framework.errors_impl.NotFoundError: /cns/tp-d/home/tensorflow-tpus/hongkuny/bert/pretrained_models/uncased_L-24_H-128_B-512_A-4_F-4_OPT/oss; No such file or directory"
4309,"b'Finetuning error: RuntimeError: Error(s) in loading state_dict for GPT2LMHeadModel: Missing key(s) in state_dict: \xe2\x80\x9ctransformer.h.0.attn.masked_bias\xe2\x80\x9d, \xe2\x80\x9ctransformer.h.1.attn.masked_bias\xe2\x80\x9d'",2020-05-12T10:15:59Z,2020-05-12T14:32:37Z,,,
4308,b'MPNet: Masked and Permuted Pre-training for Language Understanding',2020-05-12T09:47:35Z,2021-01-24T02:33:44Z,"wontfix, New model",,
4307,b'why Building wheel for tokenizers (PEP 517) ... error?',2020-05-12T09:04:13Z,2020-07-18T10:22:26Z,wontfix,,
4306,b'Multiple token prediction with MLM',2020-05-12T07:28:01Z,2020-07-18T16:22:26Z,wontfix,ValueError,"ValueError: only one element tensors can be converted to Python scalars"
4305,b'fixed missing torch module import',2020-05-12T07:21:29Z,2020-05-12T12:34:18Z,model card,,
4304,b'Latest Version Of HuggingFace is Not Using GPUs',2020-05-12T06:42:36Z,2020-05-12T08:42:58Z,,,
4303,"b'How to perform Agglomerative clustering on Albert model last hidden state output, it gives tuple of tensors for each sequence in a sentence. I am processing multiple sentences from a document, How to achieve this ?'",2020-05-12T06:24:37Z,2020-07-18T08:22:27Z,wontfix,,
4302,b'Remove args from args.tf_dump_path',2020-05-12T05:57:49Z,2020-07-13T10:14:34Z,wontfix,,
4301,b'why squad.py did not reproduce squad1.1 report result?',2020-05-12T02:15:48Z,2020-05-27T01:23:04Z,Migration,,
4300,b'Fix nn.DataParallel compatibility in PyTorch 1.5',2020-05-12T01:25:02Z,2020-05-19T00:34:50Z,,,
4299,b'Issue with XLNet using xlnet-base-cased',2020-05-11T23:40:53Z,2020-10-10T08:14:06Z,wontfix,TypeError,"TypeError: forward() got an unexpected keyword argument 'cls_index'"
4298,b'Fix BART tests on GPU',2020-05-11T23:36:29Z,2020-05-12T13:11:51Z,,,
4297,b'pin TF to 2.1',2020-05-11T22:56:06Z,2020-05-12T01:03:31Z,,,
4296,b'Error Training T5 Transformer Translation Model',2020-05-11T22:50:07Z,2020-06-03T13:48:00Z,,,
4295,"b'[Docs, Notebook] Include generation pipeline'",2020-05-11T20:53:23Z,2020-05-13T18:24:08Z,,,
4294,b'Documentation specification',2020-05-11T20:34:59Z,2020-05-11T20:43:57Z,,,
4293,b'No crossattention layers in decoder from pretrained encoder-decoder model',2020-05-11T20:09:58Z,2020-05-29T23:25:20Z,,,
4292,b'Fix special token doc',2020-05-11T19:04:33Z,2020-05-11T19:05:37Z,,,
4291,b'Fix quickstart',2020-05-11T18:07:45Z,2020-06-08T16:06:21Z,,,
4290,"b'[Marian Fixes] prevent predicting pad_token_id before softmax, support language codes, name multilingual models'",2020-05-11T17:59:40Z,2020-05-13T21:29:42Z,,,
4289,b'CamemBERT does not make use of Token Type IDs',2020-05-11T17:07:12Z,2020-05-11T17:31:04Z,,,
4288,b'TAPAS: Weakly Supervised Table Parsing via Pre-training',2020-05-11T15:31:56Z,2020-05-12T18:00:49Z,New model,,
4287,b'T5 fp16 forward yields nan',2020-05-11T14:12:28Z,2020-05-18T15:25:58Z,,,
4286,b'Reformer enwik8 - Model card',2020-05-11T14:12:01Z,2020-05-11T14:22:09Z,model card,,
4285,"b""Default code snippet for shared model doesn't work because of different model and tokenizer types""",2020-05-11T13:53:41Z,2020-07-18T05:22:32Z,wontfix,TypeError,"TypeError: expected str, bytes or os.PathLike object, not NoneType"
4284,b'[Marian] Fix typo in docstring',2020-05-11T13:32:51Z,2020-05-11T15:47:51Z,,,
4283,b'[TF 2.2 compat]  use tf.VariableAggregation.ONLY_FIRST_REPLICA',2020-05-11T13:21:42Z,2020-05-11T15:28:38Z,,,
4282,b'[Reformer] Add Enwiki8 Reformer Model - Adapt convert script',2020-05-11T12:48:59Z,2020-05-11T14:38:07Z,,,
4281,b'Model card for bert-turkish-question-answering question-answering model',2020-05-11T12:35:38Z,2020-05-11T15:32:25Z,model card,,
4280,b'Update model-card',2020-05-11T12:20:50Z,2020-05-11T15:24:11Z,model card,,
4279,b'Documentation: fix links to NER examples',2020-05-11T09:53:05Z,2020-05-11T16:48:21Z,Documentation,,
4278,b'Could you help with a huggingface/transformers pretrained models download link?',2020-05-11T07:41:58Z,2020-07-18T05:22:38Z,wontfix,,
4277,b'Use BART for longer documents',2020-05-11T07:15:15Z,2020-05-15T03:16:37Z,,,
4276,b'Update README.md',2020-05-11T06:53:09Z,2020-05-11T15:24:42Z,model card,,
4275,b'training script for any tensorflow based pre-trained model for question-answer',2020-05-11T04:33:15Z,2020-07-17T14:42:57Z,wontfix,,
4274,b'Wrong returns of API:get_special_tokens_mask()',2020-05-11T03:49:36Z,2020-05-11T19:05:37Z,,,
4273,b'Add migrating from `pytorch-transformers`',2020-05-11T00:57:07Z,2020-05-11T17:35:14Z,"Documentation, Migration",,
4272,b'FileNotFoundError when running distributed Trainer',2020-05-11T00:30:20Z,2020-05-19T02:02:40Z,,FileNotFoundError,"FileNotFoundError: [Errno 2] No such file or directory: '/home/shaoyent/transformers/examples/language-modeling/output/checkpoint-500/optimizer.pt'"
4271,b'Evaluating a checkpoint',2020-05-10T21:11:16Z,2020-05-11T16:11:36Z,,,
4270,b'Add MultipleChoice to TFTrainer [WIP]',2020-05-10T20:56:57Z,2020-05-12T12:48:49Z,"TensorFlow, Ex: Multiple Choice",,
4269,b'[Jax] top_k_top_p',2020-05-10T20:34:23Z,2020-05-25T14:48:13Z,,,
4268,b'KlDivBackward',2020-05-10T19:34:14Z,2020-07-17T14:42:58Z,wontfix,,
4267,b'useless chg',2020-05-10T18:02:04Z,2020-05-10T18:02:16Z,,,
4266,b'[CI] remove run_tests_torch_and_tf',2020-05-10T17:59:01Z,2020-08-19T04:11:38Z,,,
4265,"b""[DOCS] BertModel documentation doesn't mention the usage of tokenizers for creating input_ids""",2020-05-10T17:53:39Z,2020-07-23T05:26:06Z,wontfix,,
4264,"b""Align sentiment-analysis' tokenizer (currently cased) to the model (uncased)""",2020-05-10T17:22:50Z,2020-05-11T16:45:53Z,,,
4263,b'Sentiment Analysis Pipeline is predicting incorrect sentiment in 2.9',2020-05-10T10:50:43Z,2020-07-25T13:38:43Z,wontfix,,
4262,b'NER Models not using BIO tagging scheme',2020-05-10T08:29:36Z,2020-05-14T15:49:18Z,Ex: Named Entity Recognition,,
4261,"b'I am training a bert model for question classification task, now it is a binary classifier. 0 for Descriptive question and 1 for not descriptive. While testing I am getting the following error :-'",2020-05-10T08:14:04Z,2020-09-19T03:01:43Z,wontfix,ValueError,"ValueError: Wrong shape for input_ids (shape torch.Size([1])) or attention_mask (shape torch.Size([1]))"
4260,b'Strange Behaviour using Transformer pipline with FLASK',2020-05-10T05:48:15Z,2020-07-17T14:42:58Z,wontfix,ModuleNotFoundError,"ModuleNotFoundError: No module named 'tensorflow_core._api' "
4259,b'Fix bug in sample code',2020-05-09T22:13:16Z,2020-07-16T17:40:23Z,wontfix,,
4258,b'Model call with `inputs_embeds` does not match that with `input_ids`',2020-05-09T21:11:42Z,2020-05-09T22:50:58Z,,,
4257,b'added functionality for electra classification head',2020-05-09T19:33:42Z,2020-05-22T13:48:22Z,,,
4256,"b""Vaswani's Transformer (Encoder-Decoder) decoding methods""",2020-05-09T18:50:15Z,2020-07-17T14:42:57Z,wontfix,,
4255,b'bert uncased models needs lower casing option turned on in the example code.',2020-05-09T15:53:27Z,2020-05-27T01:22:15Z,,,
4254,"b""'utf-8' codec can't decode byte 0x80 in position 229888: invalid start byte""",2020-05-09T14:53:03Z,2020-05-10T11:33:12Z,Usage,,
4253,b'Conversion script to export transformers models to ONNX IR.',2020-05-09T13:30:48Z,2020-05-14T20:35:53Z,,,
4252,b'How to specify a TPU IP for run_tf_glue.py?',2020-05-09T13:20:41Z,2020-07-18T05:22:36Z,wontfix,,
4251,b'Encoder - Decoder loading wrong weights and missing decoders',2020-05-09T12:53:43Z,2020-05-14T13:26:51Z,,`TypeError,"`TypeError: forward() got an unexpected keyword argument 'encoder_hidden_states'`"
4250,b'[Request] NER Scripts on CoNLL 2003 dataset ',2020-05-09T11:10:26Z,2020-07-20T01:07:06Z,"wontfix, Ex: Named Entity Recognition, Documentation",,
4249,b'[docs] fix typo',2020-05-09T10:16:42Z,2020-05-10T18:07:09Z,,,
4248,b'sentiment analysis pipeline provides option to output the original rating than positive/negative tags',2020-05-09T10:16:16Z,2020-07-17T14:43:01Z,wontfix,,
4247,b'removed variable api from pplm example',2020-05-09T07:03:53Z,2020-05-09T07:09:52Z,,,
4246,b'Structuring Dataset for Fine Tuning GPT-2 with Song Lyrics?',2020-05-09T03:06:09Z,2020-09-12T21:50:10Z,wontfix,,
4245,b'Add back option --do_lower_case in SQuAD for uncased models',2020-05-09T02:37:49Z,2020-05-27T01:13:07Z,,,
4244,b'Allow gpt2 to be exported to valid ONNX',2020-05-09T00:39:14Z,2020-05-11T18:55:56Z,,,
4243,b'Distributed eval: SequentialDistributedSampler + gather all results',2020-05-09T00:18:17Z,2020-05-19T02:02:40Z,,,
4242,b'Reformer self attention mask value not converted in apex half precision',2020-05-08T19:04:34Z,2020-05-11T14:53:43Z,reformer,RuntimeError,"RuntimeError: expected scalar type Half but found Float"
4241,b'Reformer self attention mask value not converted in apex half precision',2020-05-08T19:03:02Z,2020-05-11T14:54:15Z,reformer,RuntimeError,"RuntimeError: expected scalar type Half but found Float"
4240,b'RuntimeError: expected device cpu but got device cuda:0',2020-05-08T18:44:21Z,2020-05-19T03:13:33Z,,RuntimeError,"RuntimeError: expected device cpu but got device cuda:0"
4239,b'distilbert-base-uncased',2020-05-08T18:32:20Z,2020-10-18T21:11:09Z,wontfix,OSError,"OSError: Model name 'distilbert-base-uncased' was not found in tokenizers model name list (distilbert-base-uncased, distilbert-base-uncased-distilled-squad, distilbert-base-cased, distilbert-base-cased-distilled-squad, distilbert-base-german-cased, distilbert-base-multilingual-cased). We assumed 'distilbert-base-uncased' was a path, a model identifier, or url to a directory containing vocabulary files named ['vocab.txt'] but couldn't find such vocabulary files at this path or url."
4238,b'[tests] make pipelines tests faster with smaller models',2020-05-08T15:47:43Z,2020-05-14T17:36:03Z,,,
4237,b'Only self-attention returned when output_attentions = True for BERT',2020-05-08T14:46:26Z,2020-07-15T03:26:19Z,wontfix,,
4236,b'Pipelines: use tokenizer.max_len',2020-05-08T14:29:26Z,2020-06-16T13:03:10Z,,,
4235,b'Error importing Reformer model',2020-05-08T14:13:01Z,2020-05-11T12:50:16Z,,ImportError,"ImportError: cannot import name 'ReformerModel' from 'transformers'"
4234,"b'[infra] make a tiny ""distilroberta-base"" to speed up test_pipelines.py and test_examples.py'",2020-05-08T14:10:53Z,2020-05-14T17:36:02Z,Good First Issue,,
4233,b'How can I mute the info from BertTokenizerFast.encode_plus',2020-05-08T14:01:54Z,2020-07-08T04:45:16Z,wontfix,,
4232,b'MarianMTModel: Runtime Errors',2020-05-08T13:09:46Z,2020-05-08T18:26:29Z,marian,,
4231,b'Strange difference between NER results in 2.8.0 and previous versions',2020-05-08T12:12:21Z,2020-07-29T10:56:02Z,wontfix,,
4230,b'example updated to use generation pipeline',2020-05-08T11:27:14Z,2020-05-08T13:45:10Z,model card,,
4229,b'Problem with BertTokenizer using additional_special_tokens',2020-05-08T10:38:28Z,2020-07-25T02:39:01Z,wontfix,ImportError,"ImportError: cannot import name 'AddedToken' from 'tokenizers' (/media/discoD/anaconda3/envs/fast-bert/lib/python3.7/site-packages/tokenizers/__init__.py)"
4228,b'Reformer tokenizer not working',2020-05-08T10:16:58Z,2020-05-08T10:35:11Z,,IndexError,"IndexError: list index out of range"
4227,b'2.9.0 Padding Bug',2020-05-08T09:12:07Z,2020-08-30T23:11:32Z,"wontfix, Core: Tokenization",TypeError,"TypeError: ne() received an invalid combination of arguments - got (NoneType), but expected one of:"
4226,b'Simplify cache vars and allow for TRANSFORMERS_CACHE env',2020-05-08T08:13:10Z,2020-05-11T19:24:03Z,,,
4225,b'[Reformer/Longformer] Add cross-attention layers for Encoder-Decoder setting',2020-05-08T05:31:45Z,2020-10-18T05:15:22Z,wontfix,,
4224,b'Bart now enforces maximum sequence length in Summarization Pipeline',2020-05-08T04:24:15Z,2020-12-05T05:00:52Z,wontfix,IndexError,"IndexError: index out of range in self"
4223,"b'[TPU] Doc, fix xla_spawn.py, only preprocess dataset once'",2020-05-08T01:02:45Z,2020-05-08T18:10:06Z,,,
4222,"b""TPU Trainer's PerDeviceLoader has no len()""",2020-05-08T00:05:42Z,2020-05-10T18:57:55Z,,TypeError,"TypeError: object of type 'PerDeviceLoader' has no len()"
4221,b'feat(trainer): log epoch and final metrics',2020-05-07T23:37:53Z,2020-06-04T03:13:17Z,,,
4220,b'Improvements to the wandb integration',2020-05-07T23:26:01Z,2020-05-12T22:21:45Z,,,
4219,b'Need correction in GPT-2 perplexity equation based on computation',2020-05-07T23:19:43Z,2020-07-14T05:17:21Z,wontfix,,
4218,b'feat(Trainer): log epoch and final metrics',2020-05-07T22:37:47Z,2020-05-07T23:21:31Z,,,
4217,"b'[Pipeline, Generation]  tf generation pipeline bug'",2020-05-07T22:01:33Z,2020-05-08T12:30:06Z,,,
4216,b'[examples/summarization] run_train_tiny.sh: remove unused kwarg',2020-05-07T21:25:05Z,2020-05-07T23:19:07Z,,,
4215,b'Examples readme.md',2020-05-07T19:00:00Z,2020-05-07T19:00:07Z,,,
4214,b'[examples] text_classification/run_pl.sh error',2020-05-07T18:20:14Z,2020-07-27T03:11:06Z,"Help wanted, Examples",AttributeError,"AttributeError: 'NoneType' object has no attribute 'items'"
4213,b'BIG Reorganize examples ',2020-05-07T17:31:55Z,2020-05-07T17:48:45Z,,,
4212,b'GPT2Tokenizer.decode() slow for individual tokens',2020-05-07T17:23:18Z,2020-07-13T18:00:31Z,wontfix,,
4211,b'W&B integration improvements',2020-05-07T16:54:24Z,2020-05-07T23:21:39Z,,,
4210,b'Perplexity in T5/BART',2020-05-07T16:52:04Z,2020-05-07T22:06:37Z,,,
4209,b'Fix for IndexError when Roberta Tokenizer is called on empty text',2020-05-07T16:12:45Z,2020-06-22T15:09:06Z,,,
4208,b'Remove comma that turns int into tuple',2020-05-07T15:42:17Z,2020-05-07T17:51:43Z,,,
4207,"b'BertForTokenClassification, logits on sequence tagging task '",2020-05-07T15:33:50Z,2020-07-13T16:00:33Z,wontfix,,
4206,b'How to get output from BERTNextSentencePrediction by passing One Sentence and Next Sentence getting predicted',2020-05-07T15:13:15Z,2020-07-13T16:00:32Z,wontfix,,
4205,b'Issues with RobertaTokenizer and unicode characters',2020-05-07T15:08:02Z,2020-07-18T05:22:34Z,wontfix,,
4204,b'KeyError with a fine-tuned model',2020-05-07T14:48:55Z,2020-05-08T09:55:50Z,,KeyError,"KeyError: 422"
4203,b'Use with_suffix to change the extension of the path',2020-05-07T14:37:24Z,2020-05-07T15:14:57Z,,,
4202,b'Create README.md',2020-05-07T14:06:04Z,2020-05-07T22:31:23Z,model card,,
4201,b'Ensure fast tokenizer can construct single-element tensor without pad token',2020-05-07T14:02:00Z,2020-05-07T14:02:54Z,,,
4200,b'[Proposal] Small HfAPI Cleanup',2020-05-07T13:57:13Z,2020-07-13T14:00:31Z,wontfix,,
4199,b'[README] Corrected some grammatical mistakes',2020-05-07T13:29:08Z,2020-05-10T13:02:37Z,,,
4198,b'[Benchmark] Memory benchmark utils',2020-05-07T13:17:10Z,2020-05-27T21:22:17Z,,,
4197,b'AutoTokenizer not able to load saved Roberta Tokenizer ',2020-05-07T13:09:16Z,2020-07-25T02:39:04Z,wontfix,OSError,"OSError: file ./my_tokenizer/config.json not found"
4196,b'Model card for spanish electra small',2020-05-07T12:23:44Z,2020-05-08T13:30:16Z,model card,,
4195,b'add XLMRobertaForQuestionAnswering',2020-05-07T11:34:57Z,2020-08-03T14:59:42Z,wontfix,,
4194,"b""Optimize PyTorch's GPT2 computation graph """,2020-05-07T11:08:38Z,2020-07-23T05:26:01Z,wontfix,,
4193,b'Language model training with NSP using run_language_modeling.py',2020-05-07T08:58:02Z,2020-05-07T12:03:38Z,wontfix,,
4192,b'[Reformer] Doctsring: fix examples again',2020-05-07T08:53:52Z,2020-05-07T08:54:49Z,,,
4191,b'[Reformer] Fix example and error message',2020-05-07T08:36:55Z,2020-05-07T08:50:12Z,,,
4190,b'[Reformer] fix docstring',2020-05-07T08:27:53Z,2020-05-07T08:28:32Z,,,
4189,b'Bug: can not use pretrained BERT on multiple GPUs with DataParallel (PyTorch 1.5.0)',2020-05-07T04:24:22Z,2020-05-11T22:35:30Z,,,
4188,b'Fix Albert Attention',2020-05-07T02:09:45Z,2020-07-11T01:38:12Z,wontfix,,
4187,b'How to apply Torchtext convenience classes to prepare data for a Transformer?',2020-05-07T01:12:27Z,2020-07-13T05:00:33Z,wontfix,,
4186,b'Add patience argument to Trainer',2020-05-06T21:48:10Z,2020-11-30T16:04:20Z,,,
4185,b'New model request: MobileBERT from Google ',2020-05-06T21:17:20Z,2020-07-18T07:32:50Z,"wontfix, New model",,
4184,b'Model card for allegro/herbert-klej-cased-tokenizer-v1',2020-05-06T19:59:41Z,2020-05-08T13:42:44Z,model card,,
4183,b'Model card for allegro/herbert-klej-cased-v1',2020-05-06T19:56:16Z,2020-05-08T13:42:29Z,model card,,
4182,"b'Is it possible to get document embeddings using GPT-2? If so, how?'",2020-05-06T19:33:12Z,2020-07-13T16:00:31Z,wontfix,,
4181,b'[Marian] Key-Error for some languages',2020-05-06T17:54:42Z,2020-05-10T17:54:57Z,marian,KeyError,"KeyError: 'Be'"
4180,b'considering empty input text to avoid string index out of range error',2020-05-06T17:10:12Z,2020-07-15T03:25:36Z,wontfix,,
4179,b'Create README.md',2020-05-06T15:39:33Z,2020-05-08T13:25:36Z,model card,,
4178,b'[Model Cards] Add 1010 model cards for Helsinki-NLP',2020-05-06T15:32:45Z,2020-05-11T17:14:50Z,model card,,
4177,b'Not able to import certain packages',2020-05-06T13:53:02Z,2020-05-06T14:05:55Z,,,
4176,b' ONNX conversion script.',2020-05-06T11:36:12Z,2020-05-09T13:33:11Z,,,
4175,b'args.output_dir seems like been ignored',2020-05-06T09:35:13Z,2020-07-12T10:34:54Z,wontfix,,
4174,b'Make ElectraPreTrainedModel importable',2020-05-06T07:30:33Z,2020-05-11T14:26:33Z,,,
4173,b'Include ElectraPreTrainedModel into __init__',2020-05-06T07:23:22Z,2020-05-06T16:00:24Z,,,
4172,"b""ImportError: cannot import name 'AutoModel' from 'transformers'""",2020-05-06T07:06:03Z,2020-05-08T16:25:23Z,,ImportError,"ImportError: cannot import name 'AutoModel'"
4171,b'Encoder/Decoder generation',2020-05-06T06:59:39Z,2020-05-08T06:41:59Z,,,
4170,b'How to position encode a sentence?',2020-05-06T05:14:39Z,2020-07-18T05:22:34Z,wontfix,,
4169,b'Update __init__.py for AlbertMLMHead',2020-05-06T04:27:33Z,2020-07-07T14:48:25Z,wontfix,,
4168,"b""No name 'AlbertMLMHead' in module 'transformers'""",2020-05-06T04:20:26Z,2020-05-11T14:23:55Z,New model,,
4167,b'change order pytorch/tf in readme',2020-05-05T21:02:23Z,2020-05-06T20:31:07Z,,,
4166,b'Tapas',2020-05-05T20:31:08Z,2020-07-25T02:39:09Z,"wontfix, New model",,
4165,b'[RFC] Sampling transform function for generation',2020-05-05T20:03:30Z,2020-06-30T22:30:35Z,,,
4164,b'Add a sampling_transform callback to generation for arbitrary probability-warps',2020-05-05T20:01:54Z,2020-09-06T23:57:51Z,wontfix,,
4163,b'Cannot use camembert for question answering',2020-05-05T19:44:38Z,2020-05-11T17:31:04Z,,IndexError,"IndexError: index out of range in self"
4162,b'Add model card for the NER model',2020-05-05T18:49:36Z,2020-05-06T14:40:56Z,model card,,
4161,b'run_generation.py - use of < redirect for input text file only reads first line - solution requested',2020-05-05T18:45:44Z,2020-06-03T13:46:25Z,,,
4160,b'[Marian] Multilingual models require language codes',2020-05-05T17:49:08Z,2020-05-13T21:29:42Z,marian,,
4159,b'Tokenizer.batch_decode convenience method',2020-05-05T17:43:15Z,2020-05-14T17:50:47Z,,,
4158,b'[Marian] @-@ symbol causes strange generations',2020-05-05T17:31:13Z,2020-05-10T17:54:57Z,marian,,
4157,b'[Marian] Readme parser defaults to porting oldest model',2020-05-05T17:30:17Z,2020-05-06T17:52:13Z,marian,,
4156,b'Removed the use of deprecated Variable API in PPLM example',2020-05-05T17:02:24Z,2020-05-27T15:09:59Z,,,
4155,b'num_samples=0 when using pretrained model',2020-05-05T14:50:50Z,2020-05-05T15:13:34Z,,,
4154,b'Rewritten batch support in pipelines.',2020-05-05T14:42:24Z,2020-05-07T13:52:41Z,,,
4153,b'Embedding index getting out of range while running camemebert model',2020-05-05T14:40:31Z,2020-05-11T17:31:04Z,,IndexError,"IndexError: index out of range in self"
4152,b'[Marian] documentation and AutoModel support',2020-05-05T12:53:00Z,2020-05-10T17:54:58Z,marian,,
4151,b'How to pre-train BART model',2020-05-05T10:00:43Z,2020-10-10T08:13:55Z,"wontfix, Ex: LM (Finetuning), Ex: LM (Pretraining)",,
4150,b'Config File',2020-05-05T08:13:45Z,2020-05-05T21:59:58Z,,,
4149,b'Fine-tuning T5 in Tensorflow',2020-05-05T00:41:44Z,2020-11-14T09:26:56Z,wontfix,,
4148,b'DistilBertForQuestionAnswering returns [UNK]',2020-05-04T23:22:37Z,2020-05-05T12:04:37Z,,,
4147,b'Error in Calculating Sentence Perplexity for GPT-2 model',2020-05-04T18:23:57Z,2020-06-03T13:43:01Z,,,
4146,b'Tpu trainer',2020-05-04T15:35:19Z,2020-05-07T14:34:05Z,,,
4145,b'tokenizer.batch_encoder_plus do not return input_len',2020-05-04T08:35:10Z,2020-07-11T00:49:32Z,wontfix,,
4144,b'Use finetuned-BART large to do conditional generation',2020-05-04T08:31:40Z,2020-06-17T17:51:35Z,,UnicodeDecodeError,"UnicodeDecodeError: 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte"
4143,b'Camembert-large-fquad model card',2020-05-04T08:17:17Z,2020-05-06T14:41:08Z,model card,,
4142,b'New model addition: Blender (Facebook chatbot)',2020-05-04T07:54:13Z,2020-08-22T05:34:30Z,"wontfix, New model",,
4141,"b'\xf0\x9f\x9a\x80 An advice about changing variable name from ""attention_mask"" to ""adder""'",2020-05-04T07:40:39Z,2020-07-24T08:37:33Z,"wontfix, Good First Issue",,
4140,b'How to make run_language_modeling.py work for transformer-xl?',2020-05-04T06:37:49Z,2020-06-03T13:43:53Z,,,
4139,b'Cannot set max_position_embeddings to any desired value in T5Config',2020-05-04T06:33:51Z,2020-05-18T15:31:36Z,,AttributeError,"AttributeError: can't set attribute"
4138,b'[Roberta] fix hard wired pad token id',2020-05-04T00:31:23Z,2020-05-05T22:42:35Z,,,
4137,"b""Error: `cannot import name 'TFBertForMaskedLM'`""",2020-05-03T20:50:04Z,2020-05-03T20:51:55Z,,ImportError,"ImportError: cannot import name 'TFBertForMaskedLM' from 'transformers' (/usr/local/lib/python3.7/site-packages/transformers/__init__.py)"
4136,b'Cannot loading SpanBERT pre-trained model ',2020-05-03T20:17:21Z,2020-07-06T13:33:32Z,wontfix,,
4135,b'Why does ALBERT use einsum in PyTorch implementation while in TF one it does not?',2020-05-03T19:26:15Z,2020-10-07T15:14:53Z,,,
4134,b'jplu/tf-xlm-roberta-large showing random performance',2020-05-03T18:43:57Z,2020-07-10T00:31:06Z,wontfix,,
4133,b'problem about change from pytorch-pretrained-bert to transformers',2020-05-03T15:50:00Z,2020-05-06T03:27:18Z,,,
4132,b'Create README.md',2020-05-03T12:18:59Z,2020-05-08T13:27:30Z,model card,,
4131,b'Make transformers-cli cross-platform',2020-05-03T10:06:05Z,2020-05-26T14:00:51Z,,,
4130,b'GPT-2 past behaves incorrectly when attention_mask is used',2020-05-03T07:36:42Z,2020-05-03T09:08:30Z,,,
4129,b'Parse in tensorflow strings as well as normal strings',2020-05-03T05:11:36Z,2020-07-10T00:31:07Z,wontfix,ValueError,"ValueError: Input is not valid. Should be a string, a list/tuple of strings or a list/tuple of integers."
4128,b'Add decoder specific error message for T5Stack.forward',2020-05-03T03:34:17Z,2020-05-03T10:40:09Z,,,
4127,b'Model output should be dict',2020-05-03T03:31:37Z,2020-05-04T00:00:46Z,,,
4126,b'No decoder specific error message for T5Stack.forward',2020-05-03T03:21:43Z,2020-05-03T10:40:36Z,,`ValueError,"`ValueError: You have to specify either input_ids or inputs_embeds`"
4125,"b'Is it possible to have high perplexity of some individual sentences, as compared to overall testing corpus perplexity?'",2020-05-02T23:20:43Z,2020-07-10T00:31:08Z,wontfix,,
4124,b'convert pytorch_model.pt [pretrained Bert model] to pytorch_model.onnx (ONNX)',2020-05-02T19:30:18Z,2020-07-22T10:52:41Z,wontfix,,
4123,b'Text corrections',2020-05-02T18:44:18Z,2020-05-04T00:08:23Z,,,
4122,b'ValueError: You are attempting to pad samples but the tokenizer you are using (GPT2Tokenizer) does not have one.',2020-05-02T17:32:10Z,2021-01-02T04:26:05Z,"wontfix, Ex: LM (Pretraining)",ValueError,"ValueError: You are attempting to pad samples but the tokenizer you are using (GPT2Tokenizer) does not have one."
4121,b'`eos_token_id` breaks the `T5ForConditionalGeneration`',2020-05-02T16:19:59Z,2020-06-03T16:53:24Z,Ex: Generation,AssertionError,"AssertionError: If batch_idx is not done, final next scores: tensor([-3.5015, -4.4583, -4.5677, -4.5808, -4.7325, -4.8779, -5.1127, -5.2106,"
4120,b'fixed utils_summarization import path',2020-05-02T15:36:51Z,2020-09-05T18:49:37Z,wontfix,,
4119,b'Fix markdown to show the results table properly',2020-05-02T15:32:56Z,2020-05-06T14:38:30Z,model card,,
4118,b'Update run_pl_ner.py',2020-05-02T13:27:18Z,2020-05-02T14:38:22Z,,,
4117,b'Update run_pl_glue.py',2020-05-02T13:26:24Z,2020-05-02T14:38:31Z,,,
4116,b'Updated',2020-05-02T12:53:32Z,2020-05-02T12:54:21Z,,,
4115,b'Create model card',2020-05-02T11:25:31Z,2020-05-02T15:19:32Z,model card,,
4114,b'model card for surajp/albert-base-sanskrit',2020-05-02T10:51:39Z,2020-05-02T15:15:39Z,model card,,
4113,b'[Reformer] Move model card to google model',2020-05-02T08:24:22Z,2020-05-02T08:25:23Z,,,
4112,b'Albert large QA model pretrained from baidu webqa and baidu dureader datasets.',2020-05-02T03:57:16Z,2020-05-02T14:52:12Z,model card,,
4111,b'BERT as encoder and a transformer as a decoder.',2020-05-02T00:51:24Z,2020-06-03T13:39:32Z,Usage,,
4110,b'NER: parse args from .args file or JSON',2020-05-01T22:57:41Z,2020-05-02T14:29:18Z,,,
4109,b'Fix #2941',2020-05-01T21:47:55Z,2020-05-02T15:20:31Z,,,
4108,b'Feature/torchserve interface [WIP]',2020-05-01T20:37:45Z,2020-07-15T03:25:38Z,wontfix,,
4107,b'Fix `RobertaClassificationHead` style consistency.',2020-05-01T20:17:29Z,2020-07-05T15:50:43Z,,,
4106,b'FIXME(Actually test multi input pipelines)',2020-05-01T19:51:15Z,2020-05-05T19:01:33Z,,,
4105,b'model from path 16-bits training:True but float16 false',2020-05-01T19:50:23Z,2020-07-09T03:04:28Z,wontfix,,
4104,b'Fix pytorch lighting examples',2020-05-01T18:09:27Z,2020-07-15T03:25:37Z,wontfix,,
4103,"b""AttributeError: 'NoneType' object has no attribute 'abs' when run example/run_bertology.py""",2020-05-01T17:33:58Z,2020-05-02T17:13:13Z,,AttributeError,"AttributeError: 'NoneType' object has no attribute 'abs'"
4102,b'Added huseinzol05/gpt2-345M-bahasa-cased',2020-05-01T16:15:51Z,2020-05-02T14:51:15Z,model card,,
4101,b'Docs: add XLM-RoBERTa to multi-lingual section',2020-05-01T14:58:16Z,2020-05-01T15:06:59Z,,,
4100,b'Masking in Bert',2020-05-01T14:34:02Z,2020-05-02T19:08:21Z,,,
4099,b'Added GePpeTto card',2020-05-01T14:07:19Z,2020-05-01T15:46:43Z,model card,,
4098,b'[Fix #3963] GPT2 FP16',2020-05-01T13:00:57Z,2020-05-08T16:06:59Z,,,
4097,b'Fix gpt2 fp16',2020-05-01T12:58:52Z,2020-05-01T12:59:04Z,,,
4096,b'Defaults models for different pipelines',2020-05-01T10:10:46Z,2020-05-01T15:20:05Z,,,
4095,b'Fix object is not subscriptable error in BertEncoder (#1188)',2020-05-01T09:56:33Z,2020-07-15T03:25:39Z,wontfix,,
4094,b'Negative dimension when initialising the XLNetModel',2020-05-01T09:36:18Z,2020-05-01T16:06:26Z,,RuntimeError,"RuntimeError: Trying to create tensor with negative dimension -1: [-1, 768]"
4093,b'Fix overwrite_cache behaviour for pytorch lightning examples',2020-05-01T08:44:19Z,2020-05-06T16:24:49Z,,,
4092,b'Finue-tuning T5 model',2020-04-30T22:55:28Z,2020-05-04T17:03:17Z,"Ex: LM (Finetuning), Ex: LM (Pretraining)",,
4091,b'Add ForMultipleChoice for Electra and Albert [WIP]',2020-04-30T21:05:27Z,2020-04-30T21:57:14Z,,,
4090,b'Character level models?',2020-04-30T20:05:20Z,2020-05-17T23:13:16Z,,,
4089,b'GePpeTto!',2020-04-30T16:27:37Z,2020-05-01T15:46:42Z,New model,,
4088,"b'InvalidArgumentError: Incompatible shapes: [5,20] vs. [5,18] [Op:Less]'",2020-04-30T14:42:37Z,2020-05-18T13:51:41Z,,,
4087,b'Added huseinzol05/gpt2-117M-bahasa-cased README.md',2020-04-30T12:42:17Z,2020-05-01T02:21:39Z,model card,,
4086,b'Not able to reproduce same CoLA result as huggingface defualt',2020-04-30T10:28:13Z,2020-07-09T03:04:34Z,wontfix,,
4085,b'Use roberta-med or scibert for fillmask',2020-04-30T09:36:17Z,2020-04-30T22:33:43Z,,,
4084,b'feature-extraction pipeline is second last layer better representation than the last hidden layer',2020-04-30T07:16:32Z,2020-05-02T17:24:54Z,,,
4083,b'Why is masking ID optional?',2020-04-30T01:43:58Z,2020-05-01T15:32:29Z,,,
4082,b'Zero-shot PPLs vary wildly across gpt2 model sizes',2020-04-29T23:50:59Z,2020-07-06T01:07:46Z,wontfix,,
4081,b'Examples link to the master branch which seems to be ahead of the pip install version?',2020-04-29T23:12:33Z,2020-07-06T01:07:47Z,wontfix,,
4080,b'tokenizer.encode_plus stopped returning `attention_mask`',2020-04-29T22:25:24Z,2020-04-29T23:11:28Z,,,
4079,b'Load of pre-trained t5 model from Tf to HuggingFace',2020-04-29T21:53:03Z,2020-06-03T13:38:56Z,,RuntimeError,"RuntimeError: /Users/antonio/Downloads/tf_path/model.ckpt-306400.data-00000-of-00002; No such file or directory</b>"
4078,"b'Using \'ner\' task in pipeline with a non default model gives me entities as ""LABEL-6"" , ""LABEL-8""  instead of ""I-ORG"" and ""I-LOC""'",2020-04-29T20:03:43Z,2020-11-14T09:27:02Z,"wontfix, Core: Pipeline, Ex: Named Entity Recognition",,
4077,b'[docs] bad RST in PretrainedModel.generate docstring',2020-04-29T19:59:59Z,2020-05-14T16:39:57Z,,,
4076,b'Remove double bias in TF Albert',2020-04-29T18:58:16Z,2020-06-08T16:06:31Z,,,
4075,b'How to using vocab other language in run_ner.py?',2020-04-29T17:47:43Z,2020-05-01T13:45:55Z,,,
4074,b'Issue with non-text files and bertabs example',2020-04-29T15:26:41Z,2020-06-03T13:37:58Z,,,
4073,b'Save T5 model as H5 and convert it to model.json to be used in TensorflowJS',2020-04-29T13:37:12Z,2020-07-05T20:07:47Z,wontfix,NotImplementedError,"NotImplementedError: Saving the model to HDF5 format requires the model to be a Functional model or a Sequential model. It does not work for subclassed models, because such models are defined via the body of a Python method, which isn't safely serializable. Consider saving to the Tensorflow SavedModel format (by setting save_format=""tf"") or using `save_weights`."
4072,b'How can I continue finetuning from checkpoint using the NER script?',2020-04-29T13:25:12Z,2020-07-13T09:00:32Z,wontfix,,
4071,b'Native integration with pytorch/serve',2020-04-29T13:12:43Z,2020-09-05T07:35:34Z,wontfix,,
4070,b'No CORS Policy for Write With Transformer Endpoints',2020-04-29T09:22:09Z,2020-05-01T00:50:49Z,Write With Transformer,,
4069,b'Delete batch = tuple(t.to(args.device) for t in batch) for it perform\xe2\x80\xa6',2020-04-29T07:13:41Z,2020-07-15T03:25:40Z,wontfix,,
4068,b'Is there pre-train bert or xlnet from scratch code ?',2020-04-29T07:13:04Z,2020-05-01T00:21:34Z,Ex: LM (Pretraining),,
4067,"b""Why GPT2LMHeadModel's loss mismatches accuracy?""",2020-04-29T04:05:56Z,2020-06-03T13:37:11Z,,,
4066,b'create model_card camembert-base-wikipedia-4gb',2020-04-29T03:45:52Z,2020-05-01T02:16:13Z,model card,,
4065,b'Create model_card camembert-base-ccnet',2020-04-29T03:44:43Z,2020-05-01T02:16:01Z,model card,,
4064,b'Create model_card camembert-base-ccnet-4gb',2020-04-29T03:43:07Z,2020-05-01T02:15:48Z,model card,,
4063,b'Create README.md model_card camembert/camembert-base-oscar-4gb',2020-04-29T03:40:49Z,2020-05-01T02:15:39Z,model card,,
4062,b'Create README.md',2020-04-29T03:31:56Z,2020-05-01T02:15:24Z,model card,,
4061,"b""KeyError ' '- run_ner.py - Transformers 2.8.0""",2020-04-29T03:10:37Z,2020-07-29T10:56:04Z,"wontfix, Ex: Named Entity Recognition",KeyError,"KeyError: ''"
4060,b'Cannot Load bert-base-japanese tokenizer',2020-04-29T03:00:13Z,2020-05-01T01:19:50Z,,OSError,"OSError: Model name 'bert-base-japanese' was not found in tokenizers model name list (bert-base-japanese, bert-base-japanese-whole-word-masking, bert-base-japanese-char, bert-base-japanese-char-whole-word-masking). We assumed 'bert-base-japanese' was a path, a model identifier, or url to a directory containing vocabulary files named ['vocab.txt'] but couldn't find such vocabulary files at this path or url."
4059,b'Dropout training',2020-04-29T02:34:17Z,2020-04-30T15:39:35Z,,,
4058,b'How to run squad for chinese dataset? ',2020-04-29T02:33:16Z,2020-07-05T05:07:46Z,wontfix,,
4057,b'Add AlbertForPreTraining and TFAlbertForPreTraining models.',2020-04-28T20:45:45Z,2020-05-07T23:44:52Z,,,
4056,b'Minor Readme Fixes',2020-04-28T19:58:26Z,2020-04-28T20:42:16Z,model card,,
4055,b'[Naming] lm_labels -> labels ; masked_lm_labels -> masked_labels',2020-04-28T19:19:42Z,2020-06-02T07:51:10Z,,,
4054,b'Changes to fix working for transformer-xl',2020-04-28T18:43:50Z,2020-06-15T12:43:58Z,,,
4053,b'[isort] add known 3rd party to setup.cfg',2020-04-28T17:40:03Z,2020-04-28T21:12:01Z,,,
4052,b'Checking new isort',2020-04-28T17:12:15Z,2020-04-28T21:12:32Z,,,
4051,b'Fix TF input docstrings to refer to tf.Tensor rather than torch.Float\xe2\x80\xa6',2020-04-28T16:52:11Z,2020-04-30T12:28:56Z,,,
4050,b'Remove jitted method so that our models are pickable.',2020-04-28T16:36:52Z,2020-04-29T13:53:20Z,,,
4049,b'p_mask in SQuAD pre-processing',2020-04-28T16:16:35Z,2020-05-14T21:07:53Z,,,
4048,b'Why is the pooler output used for sequence classification (if it does not represent the input semantic well)?',2020-04-28T15:51:16Z,2020-08-22T18:29:48Z,wontfix,,
4047,b'GPT2LMHeadModel Documentation Mismatch for labels',2020-04-28T14:32:08Z,2020-06-01T19:48:55Z,,,
4046,b'[EncoderDecoder Tests] Improve tests',2020-04-28T14:25:34Z,2020-05-04T00:18:37Z,,,
4045,b'[EncoderDecoder] Add working examples to docstring',2020-04-28T14:21:18Z,2020-04-28T14:33:24Z,,,
4044,b'Email Alerts: Run failed: Torch hub integration',2020-04-28T14:19:29Z,2020-07-04T16:36:23Z,wontfix,,
4043,b'daigo/bert-base-japanese-sentiment',2020-04-28T12:59:36Z,2020-04-28T19:35:31Z,model card,,
4042,b'info about loading file None is not informative',2020-04-28T12:35:37Z,2020-07-15T03:25:41Z,wontfix,,
4041,b'[wip] more fp16 test coverage',2020-04-28T12:20:52Z,2020-06-07T16:01:53Z,,,
4040,b'Small cosmetic changes to CamemBERT model card',2020-04-28T12:12:14Z,2020-04-28T19:32:56Z,model card,,
4039,b'Add BPE dropout to tokenizers',2020-04-28T10:55:27Z,2020-07-09T03:04:29Z,wontfix,,
4038,b'GPT-2 models are unpickable',2020-04-28T09:34:18Z,2020-04-29T13:53:20Z,,`TypeError,"`TypeError: can't pickle torch._C.ScriptFunction objects`"
4037,b'torch num_samples=0 error on XLMnet @ run_language_modeling.py',2020-04-28T08:23:41Z,2020-07-20T07:07:07Z,wontfix,ValueError,"ValueError: num_samples should be a positive integer value, but got num_samples=0"
4036,b'BertForSequenceClassification producing same output during evaluation ',2020-04-28T07:53:26Z,2020-05-23T10:38:38Z,,'no_cache',"'no_cache': True,"
4035,b'Model card for roberta-base-squad2-covid',2020-04-28T07:51:33Z,2020-04-28T19:29:31Z,model card,,
4034,"b'\xf0\x9f\x90\x9b Saving TF model : Expected Operation, Variable, or Tensor, got None'",2020-04-28T07:26:33Z,2020-04-28T23:49:22Z,,`TypeError,"`TypeError: Expected Operation, Variable, or Tensor, got None`"
4033,b'Model Card: gaochangkuan  README.md',2020-04-28T07:06:22Z,2020-05-01T02:26:59Z,model card,,
4032,b'[experimental] rename torch_device -> default_device',2020-04-28T02:18:24Z,2020-04-28T12:13:58Z,,,
4031,b'[gitignore] fixtures created by unit tests',2020-04-28T01:48:52Z,2020-05-01T12:35:55Z,,,
4030,b'CDN urls',2020-04-28T00:51:40Z,2020-04-29T00:27:14Z,,,
4029,b'TF2 - how to access intermediate layers of pre-trained bert model?',2020-04-27T23:12:28Z,2020-07-04T03:36:22Z,wontfix,,
4028,b'CamembertForSequenceClassification not initialized from pretrained model',2020-04-27T23:12:12Z,2020-07-09T03:04:33Z,wontfix,,
4027,b'Hoist bert model tester for patrick',2020-04-27T22:53:22Z,2020-04-30T12:29:27Z,,,
4026,b'Training a new language model with custom loss and input representation',2020-04-27T22:47:14Z,2020-07-09T03:04:33Z,wontfix,,
4025,b'Minor fix in Transformers Notebook',2020-04-27T22:47:04Z,2020-04-28T07:12:26Z,,,
4024,b'Fix for #3846',2020-04-27T22:37:53Z,2020-05-13T12:32:58Z,,,
4023,b'TFBert: Out of memory error when acting on a strided slice of input.',2020-04-27T21:20:28Z,2020-07-03T22:56:55Z,wontfix,ResourceExhaustedError,"ResourceExhaustedError: 2 root error(s) found."
4022,b'NER Pipeline with CamemBERT not showing entities',2020-04-27T20:35:35Z,2020-04-28T01:27:28Z,,,
4021,b'T5 Tokenization of unique masked tokens (<extra_id_1>) is incorrect',2020-04-27T19:43:16Z,2020-06-03T15:26:48Z,,,
4020,b'Pass existing tensorboard SummaryWriter to Trainer PR (#4019)',2020-04-27T19:33:12Z,2020-05-04T23:58:25Z,,,
4019,b'Pass existing tensorboard SummaryWriter to Trainer. ',2020-04-27T19:29:12Z,2020-05-04T23:58:53Z,,,
4018,b'String Format should be more pythonic',2020-04-27T16:58:07Z,2020-04-28T13:57:34Z,,,
4017,b'TF version of the trainer',2020-04-27T16:00:41Z,2020-05-06T16:56:52Z,,,
4016,b'Text Generation generated <|endoftext|>',2020-04-27T15:37:37Z,2020-07-09T03:04:29Z,wontfix,,
4015,b'Fast Tokenizers: `batch_encode_plus` error',2020-04-27T14:41:46Z,2020-06-15T21:12:52Z,"Core: Tokenization, Fast Tokenizers","RuntimeError, Notice","RuntimeError: stack expects each tensor to be equal size, but got [34] at entry 0 and [8] at entry 1Notice: This bug only occurs, when input sentences have different lenghts and the `return_tensors=""pt""` is used!"
4014,"b'[Fix common tests on GPU] send model, ids to torch_device'",2020-04-27T14:09:33Z,2020-04-29T13:47:21Z,,,
4013,b'test_lm_head_model_random_*_generate fail on GPU',2020-04-27T14:06:55Z,2020-04-29T13:47:21Z,,,
4012,b'Attribute error while using run_language_modeling.py with Transformer-XL',2020-04-27T12:30:49Z,2020-06-15T12:32:03Z,,**AttributeError,"**AttributeError: 'AdaptiveEmbedding' object has no attribute 'weight'**"
4011,b'Report minimum system requirements for each architecture.',2020-04-27T11:22:35Z,2020-07-03T11:56:56Z,wontfix,,
4010,"b'fairseq-preprocess, Why are the number of rows in src and trg different?'",2020-04-27T09:42:16Z,2020-07-03T10:56:58Z,wontfix,,
4009,b'Implemented lazy line-by-line text data set loading for LM example script',2020-04-27T08:38:46Z,2020-11-21T02:59:21Z,wontfix,,
4008,b'camembert-base-fquad',2020-04-27T08:34:53Z,2020-04-27T22:29:56Z,model card,,
4007,b'Create model card',2020-04-27T08:27:18Z,2020-04-27T22:27:47Z,model card,,
4006,b'Create model card',2020-04-27T08:15:28Z,2020-04-27T22:27:05Z,model card,,
4005,"b'Fast Tokenizers do not work when `return_offsets_mapping=True, return_tensors=""pt""`'",2020-04-27T07:43:38Z,2020-04-28T10:51:28Z,,TypeError,"TypeError: expected Tensor as element 0 in argument 0, but got list"
4004,b'ALBERT with Masked Language Model Input Processing from SavedModel',2020-04-27T06:02:11Z,2020-07-03T06:56:56Z,wontfix,ValueError,"ValueError: Could not find matching function to call loaded from the SavedModel. Got:"
4003,b'Maybe it is a bug for Roberta vocab file.',2020-04-27T05:21:41Z,2020-04-27T15:11:07Z,,,
4002,b'\xf0\x9f\x8c\x9f Transformer Lite',2020-04-27T04:17:44Z,2020-07-03T06:56:57Z,wontfix,,
4001,b'TF BART ?',2020-04-27T04:11:51Z,2020-10-21T11:10:17Z,TensorFlow,,
4000,b'Adding --do_lower_case option',2020-04-27T03:49:21Z,2020-04-29T01:45:03Z,,,
3999,b'How do I convert T5 checkpoint to pytorch bin to utilize fine-tuned model from finetune.py?',2020-04-27T01:10:39Z,2020-05-05T23:57:23Z,,,
3998,b'Question about the output linear weight  ',2020-04-27T00:14:24Z,2020-07-03T02:56:56Z,wontfix,,
3997,b'Toy size models versions for faster experimentation and testing',2020-04-26T23:35:53Z,2020-07-05T04:07:45Z,wontfix,,
3996,b'[DialoGPT] add dialogpt training tips',2020-04-26T22:05:22Z,2020-04-28T12:32:32Z,,,
3995,b'[model_cards] Add model card for Hindi-Bert',2020-04-26T21:36:15Z,2020-04-27T22:25:17Z,model card,,
3994,b'Allow a more backward compatible behavior of max_len_single_sentence and max_len_sentences_pair',2020-04-26T21:13:41Z,2020-04-28T23:14:00Z,,,
3993,b'[Generation] Generation should allow to start with empty prompt',2020-04-26T20:34:56Z,2020-04-28T12:33:16Z,,,
3992,b'RuntimeError: Creating MTGP constants failed. at /opt/conda/conda-bld/pytorch_1549287501208/work/aten/src/THC/THCTensorRandom.cu:35',2020-04-26T20:12:45Z,2020-04-27T06:03:18Z,,,
3991,b'Fix the typos',2020-04-26T19:20:37Z,2020-04-27T15:51:39Z,,,
3990,b'Language generation not possible with roberta?',2020-04-26T18:53:24Z,2020-04-30T12:17:54Z,,,
3989,b'Model cards for KoELECTRA',2020-04-26T17:15:25Z,2020-04-27T22:21:02Z,model card,,
3988,b'[Trainer] Add more schedules to trainer',2020-04-26T16:38:21Z,2020-05-07T15:26:00Z,,,
3987,b'fix output_dir / tokenizer_name confusion',2020-04-26T13:27:51Z,2020-04-27T20:27:39Z,,,
3986,"b""Run Multiple Choice failure with ImportError: cannot import name 'AutoModelForMultipleChoice'""",2020-04-26T13:23:05Z,2020-04-27T16:02:36Z,,ImportError,"ImportError: cannot import name 'AutoModelForMultipleChoice'"
3985,"b""Using the T5 model with huggingface's mask-fill pipeline""",2020-04-26T12:18:26Z,2020-07-20T11:07:07Z,wontfix,,
3984,b'why the accuracy is very low when we test on sentences one by one of the words input?',2020-04-26T10:21:15Z,2020-07-02T10:55:51Z,wontfix,,
3983,b'MLM Loss not decreasing when pretraining Bert from scratch',2020-04-26T10:08:34Z,2020-04-27T08:03:17Z,,,
3982,"b""Can't install transformers from sources using poetry""",2020-04-26T10:01:50Z,2020-07-01T16:14:48Z,wontfix,,
3981,b'Improve split on token',2020-04-26T09:46:25Z,2020-07-15T03:25:42Z,wontfix,,
3980,"b' When I run `import transformers` , it reports an error. But I had no problem with pyrotch-transformers before. Is it a TensorRT problem?'",2020-04-26T09:13:55Z,2020-04-26T12:04:05Z,,,
3979,b'Add modelcard for Hate-speech-CNERG/dehatebert-mono-arabic model',2020-04-26T03:11:09Z,2020-04-27T22:18:55Z,model card,,
3978,b'Fix t5 doc typos',2020-04-26T03:02:57Z,2020-04-27T16:27:16Z,,,
3977,b'xlnet large ',2020-04-26T02:52:36Z,2020-07-05T12:07:46Z,wontfix,,
3976,b'Fixed Style Inconsistency',2020-04-26T02:05:45Z,2020-04-30T12:33:10Z,,,
3975,b'Added support for pathlib.Path objects instead of string paths in from_pretrained() (resolves #3962)',2020-04-25T22:42:25Z,2020-08-22T03:25:19Z,wontfix,,
3974,b'Weights from pretrained model not used in GPT2LMHeadModel',2020-04-25T18:44:57Z,2020-06-03T13:36:20Z,,,
3973,b'Pytorch 1.5.0',2020-04-25T17:33:14Z,2020-05-05T14:23:02Z,,,
3972,b'Sized Fill-in-the-blank or Multi Mask filling with T5',2020-04-25T17:30:56Z,2020-11-07T05:43:01Z,wontfix,,
3971,b'Problem with downloading the XLNetSequenceClassification pretrained xlnet-large-cased',2020-04-25T17:08:40Z,2020-04-26T22:50:42Z,,RuntimeError,"RuntimeError: Trying to create tensor with negative dimension -1: [-1, 1024]`"
3970,b'Fix GLUE TPU script',2020-04-25T17:01:10Z,2020-04-27T20:52:29Z,,,
3969,b'override weights name',2020-04-25T16:35:55Z,2020-07-15T03:25:43Z,wontfix,,
3968,b'Remove boto3 dependency',2020-04-25T14:50:59Z,2020-04-27T15:17:15Z,,,
3967,b'Proposal: saner num_labels in configs.',2020-04-25T13:55:19Z,2020-05-01T15:28:56Z,,,
3966,b'Add CALBERT (Catalan ALBERT) base-uncased model card',2020-04-25T09:19:10Z,2020-04-25T13:16:41Z,model card,,
3965,b'Remove hard-coded pad token id in distilbert and albert',2020-04-25T07:03:25Z,2020-05-12T12:32:45Z,,,
3964,b'Error on dtype in modeling_bertabs.py file',2020-04-25T06:31:34Z,2020-06-04T01:39:05Z,,RuntimeError,"RuntimeError: expected device cpu and dtype Byte but got device cpu and dtype Bool"
3963,"b'run_language_modeling, RuntimeError: expected scalar type Half but found Float'",2020-04-25T06:06:35Z,2020-05-08T16:06:58Z,,RuntimeError,"RuntimeError: Caught RuntimeError in replica 0 on device 0."
3962,b'`BertTokenizer.from_pretrained()` not working with native Python `pathlib` module',2020-04-25T06:05:02Z,2020-08-06T22:12:36Z,"wontfix, Good First Issue",AttributeError,"AttributeError: 'WindowsPath' object has no attribute 'decode'"
3961,b'There are some warnings when I used AdamW and pytorch1.5.',2020-04-25T05:20:37Z,2020-07-28T09:07:50Z,wontfix,,
3960,b'Question regarding glue examples',2020-04-25T02:58:42Z,2020-04-25T14:09:01Z,,,
3959,"b""AttributeError: 'LambdaLR' object has no attribute 'get_last_lr'""",2020-04-25T02:19:04Z,2020-05-19T00:17:36Z,,AttributeError,"AttributeError: 'LambdaLR' object has no attribute 'get_last_lr'"
3958,b'model.multiple_choice_head( ) function for Hugging Face GPT2 models',2020-04-25T02:04:01Z,2020-06-17T20:45:26Z,,,
3957,"b'Allow the creation of ""entity groups"" for NerPipeline #3548'",2020-04-25T01:13:07Z,2020-05-17T07:25:18Z,,,
3956,b'RuntimeError: Error(s) in loading state_dict for BertForTokenClassification',2020-04-24T22:10:10Z,2020-07-01T09:28:08Z,wontfix,RuntimeError,"RuntimeError: Error(s) in loading state_dict for BertForTokenClassification:"
3955,b'Fix #3954 - GPT2 is not traceable',2020-04-24T21:14:14Z,2020-04-28T19:18:57Z,,,
3954,b'GPT2 is not fully torch.jit.trace-able',2020-04-24T21:02:22Z,2020-04-28T19:18:59Z,,,
3953,b'Fix BERT example code for NSP and Multiple Choice',2020-04-24T19:09:46Z,2020-05-29T15:55:56Z,,,
3952,b'BertForSequenceClassification is not optimum',2020-04-24T18:44:42Z,2020-07-01T09:28:09Z,wontfix,,
3951,"b""run_xnli doesn't execute""",2020-04-24T17:53:18Z,2020-04-26T13:22:47Z,,IndexError,"IndexError: Target 2 is out of bounds."
3950,"b'In run_xnli.py, output_dir seems to be used in place of tokenizer_name'",2020-04-24T17:27:54Z,2020-06-25T13:49:30Z,wontfix,OSError,"OSError: Model name '/tmp/debug_xnli/' was not found in tokenizers model name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). We assumed '/tmp/debug_xnli/' was a path, a model identifier, or url to a directory containing vocabulary files named ['vocab.txt'] but couldn't find such vocabulary files at this path or url."
3949,b'After reading the tutorial I can use the BertModel to extract word embedding but how to use it extract the sentence embedding?',2020-04-24T17:25:32Z,2020-06-25T22:31:30Z,wontfix,,
3948,b'Add Type Hints to modeling_utils.py Closes #3911',2020-04-24T17:06:05Z,2020-05-22T23:10:23Z,,,
3947,b'Many tests fails with PyTorch 1.5.0',2020-04-24T16:07:32Z,2020-05-05T14:23:02Z,,,
3946,"b""ImportError: cannot import name 'DefaultDataCollator'""",2020-04-24T14:35:33Z,2020-06-03T13:33:54Z,,,
3945,"b'BertConfig.to_json_file(\'config.json\') saves  ""num_labels""  as  ""_num_labels"" on Google Colab'",2020-04-24T14:22:54Z,2020-04-27T18:55:46Z,,ValueError,"ValueError: Layer #2 (named ""classifier""), weight <tf.Variable 'tf_bert_for_sequence_classification/classifier/kernel:0' shape=(768, 2) dtype=float32, numpy="
3944,b'Trainer: distributed eval',2020-04-24T13:18:21Z,2020-05-20T22:31:36Z,Good First Issue,,
3943,b'How to input hidden state vectors from GPT2Model directly into mc_head of the GPT2DoubleHeads Model?',2020-04-24T12:17:47Z,2020-04-25T01:56:23Z,,,
3942,b'XLNetLMHeadModel: target mapping with num_predict > 1 and labels not working',2020-04-24T09:58:25Z,2020-06-03T13:33:09Z,,RuntimeError,"RuntimeError: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead."
3941,b'Decoding output sequences from TFGPT2Model',2020-04-24T07:10:57Z,2020-05-17T21:17:48Z,,TypeError,"TypeError: only size-1 arrays can be converted to Python scalars"
3940,b'fix resize_token_embeddings to accept padding_idx for xlm-roberta models',2020-04-24T06:06:28Z,2020-04-24T06:36:13Z,,,
3939,b'Continue training args and tqdm in notebooks',2020-04-24T04:50:13Z,2020-05-01T02:14:09Z,,,
3938,b'[Benchmark] Parameter settings of XLM-R on NER tasks',2020-04-24T03:31:47Z,2020-07-02T03:22:43Z,wontfix,,
3937,b'What should I do if I want to change the padding idx in pytorch bert(Huggingface)?',2020-04-24T00:17:25Z,2020-05-05T22:42:35Z,,,
3936,b'Pytorch 1.5 DataParallel',2020-04-24T00:09:36Z,2020-05-31T08:27:20Z,PyTorch,,
3935,b'How to feeding hidden state vectors from one transformer directly into a layer of different transformer',2020-04-23T23:32:56Z,2020-04-24T12:11:20Z,,,
3934,b'[qol] example scripts: parse args from .args file or JSON',2020-04-23T23:10:29Z,2020-05-01T02:40:14Z,,,
3933,b'Add ALBERT to the Tensorflow to Pytorch model conversion cli',2020-04-23T22:15:08Z,2020-05-11T17:10:01Z,,,
3932,b'[ci] Load pretrained models into the default (long-lived) cache',2020-04-23T21:29:18Z,2020-05-01T02:30:16Z,,,
3931,b'Loading a TF pretrained model into BertForSequenceClassification module',2020-04-23T19:16:43Z,2020-05-11T13:07:58Z,,`AttributeError,"`AttributeError: 'BertForSequenceClassification' object has no attribute 'bias'`"
3930,b'How can i finetune an encode-decoder combination?',2020-04-23T18:01:39Z,2020-07-01T09:28:14Z,wontfix,,
3929,b'Add support for LayerNorm before residual connection in transformer',2020-04-23T17:54:01Z,2021-04-27T15:02:33Z,,,
3928,b'Fix TFAlbertForSequenceClassification classifier dropout probability.\xe2\x80\xa6',2020-04-23T17:13:05Z,2020-04-23T17:18:16Z,,,
3927,b'\xe2\x9d\x93 DistilBert test perplexity based on WikiText-2: ppl is too low?',2020-04-23T16:33:13Z,2020-04-24T02:21:48Z,,,
3926,b'Remove 50k limits bug',2020-04-23T15:12:21Z,2020-04-23T15:15:10Z,,,
3925,b'Using the default trainer args',2020-04-23T14:59:50Z,2020-07-27T21:33:57Z,wontfix,,
3924,"b'Change uses of pow(x, 3) to pow(x, 3.0) to resolve #3873'",2020-04-23T14:14:29Z,2020-04-23T18:25:32Z,,,
3923,b'Feat/add model card',2020-04-23T13:45:11Z,2020-04-24T14:24:29Z,model card,,
3922,b'LineByLineTextDataset limits the total number of examples to 50000 documents',2020-04-23T12:54:35Z,2020-04-23T15:15:43Z,,,
3921,b'New run_language_modeling.py does not save vocab.txt and tokenizer_config.json',2020-04-23T12:11:58Z,2020-04-24T13:52:57Z,,,
3920,b'run_language_modeling.py line 251: checking if it is a directory',2020-04-23T12:02:36Z,2020-04-24T00:00:13Z,,TypeError,"TypeError: expected string or bytes-like object"
3919,b'xlm-roberta (large/base) : run_language_modeling.py cannot starting training',2020-04-23T11:53:29Z,2020-04-30T09:55:14Z,,,
3918,b'change scheduler.get_last_lr to get_lr to avoid bug',2020-04-23T08:31:23Z,2020-04-24T01:44:06Z,,,
3917,b'Create README.md',2020-04-23T07:03:29Z,2020-04-24T14:24:01Z,model card,,
3916,b'feat: add logging through Weights & Biases',2020-04-23T06:20:53Z,2020-05-05T02:42:28Z,,*Note,"*Note: I didn't try any fine-tuning and just used same parameters on both in this example.*"
3915,b'New run_language_modeling.py continuing trainng',2020-04-23T06:01:17Z,2020-04-23T15:32:48Z,,,
3914,b'Unknown Device when training GPT2 with TPUs in Colab',2020-04-23T05:39:19Z,2020-05-04T21:44:35Z,,RuntimeError,"RuntimeError: Unknown device"
3913,b'Summarization',2020-04-23T05:36:49Z,2020-04-23T05:41:02Z,,,
3912,"b'ImportError: cannot import name \'DataCollatorForLanguageModeling\'_File ""run_language_modeling.py""'",2020-04-23T05:35:33Z,2020-04-23T15:45:58Z,,ImportError,"ImportError: cannot import name 'DataCollatorForLanguageModeling'"
3911,b'Type Hints for modeling_utils.py',2020-04-23T04:27:03Z,2020-05-22T23:10:23Z,Good First Issue,,
3910,b'GPT2 generations with specific words',2020-04-23T01:08:01Z,2020-06-03T13:32:57Z,,,
3909,b'Shuffle train subset for summarization example',2020-04-22T23:23:07Z,2020-04-24T11:55:35Z,,,
3908,"b""MarianMTModel.from_pretrained('Helsinki-NLP/opus-marian-en-de')""",2020-04-22T22:40:48Z,2020-04-28T22:22:38Z,"seq2seq, translation",,
3907,b'Fix TF optimization classes and apply the changes in the NER TF script',2020-04-22T22:06:58Z,2020-05-11T07:14:48Z,,,
3906,b'[Cleanup] Fix typos in modeling_utils.py',2020-04-22T21:52:01Z,2020-04-27T17:25:54Z,,,
3905,b'BERT TF-Lite conversion not working in TensorFlow 2.2.0',2020-04-22T21:03:13Z,2020-06-28T04:25:10Z,wontfix,,
3904,b'how i pass from an AutoModelWithLMHead to a GPT2DoubleHeadsModel ?',2020-04-22T20:13:49Z,2020-05-11T13:02:11Z,,,
3903,b'T5 shows weird behaviour in a sanity check of its pre-training task.',2020-04-22T19:12:15Z,2020-04-26T21:24:07Z,,,
3902,b'RuntimeError: index out of range at /pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp:237',2020-04-22T19:09:45Z,2020-11-16T03:04:51Z,wontfix,RuntimeError,"RuntimeError: index out of range: Tried to access index 512 out of table with 511 rows. at ../aten/src/TH/generic/THTensorEvenMoreMath.cpp:237"
3901,b'How to only use part of pretrained model ',2020-04-22T18:36:46Z,2020-07-18T05:22:31Z,wontfix,,
3900,b'quick fix wording readme for community models',2020-04-22T17:54:29Z,2020-04-23T18:19:46Z,,,
3899,b'NLQ application',2020-04-22T15:05:51Z,2020-07-17T14:43:01Z,wontfix,,
3898,b'Bump tokenizers version to final 0.7.0',2020-04-22T15:00:04Z,2020-04-22T15:02:30Z,,,
3897,b'How to fine-tune DialoGPT with your own data?',2020-04-22T13:50:23Z,2020-04-28T12:32:32Z,,,
3896,"b""ImportError: cannot import name 'DataCollatorForLanguageModeling' in run_language_modeling.py""",2020-04-22T11:30:01Z,2020-04-22T13:57:07Z,,,
3895,"b""run_bertology: AttributeError: 'NoneType' object has no attribute 'abs'""",2020-04-22T10:54:18Z,2020-07-26T09:24:01Z,"wontfix, Good First Issue",AttributeError,"AttributeError: 'NoneType' object has no attribute 'abs'"
3894,"b""FileNotFoundError: [Errno 2] No such file or directory: 'mnli/dev_matched.tsv'""",2020-04-22T10:34:38Z,2020-04-22T14:07:54Z,,FileNotFoundError,"FileNotFoundError: [Errno 2] No such file or directory: 'mnli/dev_matched.tsv'"
3893,b'Can not import DataCollatorForLanguageModeling',2020-04-22T05:30:29Z,2020-04-22T05:54:59Z,,ImportError,"ImportError: cannot import name 'DataCollatorForLanguageModeling'`"
3892,b'\xe2\x9d\x93 Summarization example : Why no shuffling ?',2020-04-22T02:28:51Z,2020-04-24T11:55:35Z,,,
3891,b'Allow one to return encoder attentions in seq2seq generation',2020-04-21T22:41:28Z,2020-12-19T06:31:48Z,wontfix,,
3890,b'Create model card',2020-04-21T21:27:04Z,2020-04-22T18:56:43Z,model card,,
3889,b'Update comparison table',2020-04-21T21:24:46Z,2020-04-22T18:54:18Z,model card,,
3888,b'encode_for_summarization function did actually add CLS and SEP to separate sentences',2020-04-21T20:20:59Z,2020-06-27T20:53:45Z,wontfix,,
3887,"b""pytorch lightning examples doesn't work in multi gpu's with backend=dp""",2020-04-21T19:29:43Z,2020-10-18T21:11:13Z,wontfix,,
3886,b'How to find a correct place of original word from the list of predicted words from GPT-2 model?',2020-04-21T18:03:18Z,2020-07-02T23:28:41Z,wontfix,,
3885,b'Pretrain From Scratch using Google TPU',2020-04-21T17:55:23Z,2020-07-01T09:28:10Z,wontfix,,
3884,"b""Problem trying to run AlbertForMaskedLM on Colab TPU: TypeError: can't pickle torch._C.ScriptFunction objects when calling xm.send_cpu_data_to_device(model, dev) """,2020-04-21T17:11:13Z,2020-05-01T12:06:53Z,,TypeError,"TypeError: can't pickle torch._C.ScriptFunction objects"
3883,b'No longer able to fine-tune GPT2 using provided examples',2020-04-21T16:28:19Z,2020-04-21T21:30:53Z,,,
3882,b'Create model card for RoBERTa large fine-tuned on wsc',2020-04-21T16:06:56Z,2020-04-24T19:57:02Z,model card,,
3881,b'Fix Torch.hub + Integration test',2020-04-21T14:57:26Z,2020-04-21T18:13:31Z,,,
3880,b'Replace `config.output_attentions` parameter with function argument `output_attentions`',2020-04-21T14:00:23Z,2020-06-09T21:39:07Z,Good First Issue,,
3879,b'Replace `config.output_hidden_states` parameter with function argument `output_hidden_states`',2020-04-21T13:57:38Z,2020-07-24T08:37:18Z,"wontfix, Good First Issue",,
3878,b'When will ELECTRA pretraining from  scratch will be available?',2020-04-21T12:57:23Z,2020-11-14T09:26:59Z,wontfix,,
3877,"b""ImportError: cannot import name 'MODEL_CLASSES' from 'run_glue' """,2020-04-21T12:16:28Z,2020-04-22T01:15:23Z,,ImportError,"ImportError: cannot import name 'MODEL_CLASSES' from 'run_glue' (/home/stud-yantao/Transformer/transformers/examples/run_glue.py)"
3876,b'How to reduce random summary generation of BART Summarization models?',2020-04-21T09:57:49Z,2020-06-28T13:46:31Z,"Discussion, wontfix",,
3875,b'T5 Translation Error ',2020-04-21T09:48:37Z,2020-04-22T05:18:56Z,,,
3874,b'create readme for spentaur/yelp model',2020-04-21T01:47:53Z,2020-04-21T19:31:37Z,model card,,
3873,"b""Call to torch.pow() passing integer as exponent isn't per PyTorch docs""",2020-04-20T23:35:28Z,2020-04-23T18:25:32Z,,,
3872,"b'torchscript tests fail with RuntimeError: normal_ expects std > 0.0, but found std=0'",2020-04-20T23:17:29Z,2020-05-05T14:23:02Z,,,
3871,b'Tokenizer could accept a string tensor',2020-04-20T22:42:07Z,2020-06-27T03:27:18Z,wontfix,,
3870,b'bert summarizer module import error',2020-04-20T22:37:27Z,2020-08-15T19:28:07Z,wontfix,ModuleNotFoundError,"ModuleNotFoundError: No module named '__main__.utils_summarization'; '__main__' is not a package"
3869,"b""ImportError: cannot import name 'HfArgumentParser' from 'transformers'""",2020-04-20T19:30:22Z,2020-04-20T22:22:08Z,,ImportError,"ImportError: cannot import name 'HfArgumentParser' from 'transformers' (/Users/thomas/opt/anaconda3/lib/python3.7/site-packages/transformers/__init__.py)"
3868,"b""unable to load model 'bert', tensor 'input_ids': the model expects 1 dimensions but the model configuration specified 2 dimensions""",2020-04-20T17:34:00Z,2020-04-25T04:20:51Z,Usage,,
3867,b'Tokenization issue with RoBERTa and DistilRoBERTa.',2020-04-20T16:49:12Z,2020-07-04T21:36:22Z,"wontfix, Core: Tokenization",,
3866,b'[examples] fix summarization do_predict',2020-04-20T14:35:39Z,2020-04-20T14:49:57Z,,,
3865,b'Summarisation tuning',2020-04-20T13:06:42Z,2020-04-20T14:39:52Z,,,
3864,b'Add language and license information to model cards',2020-04-20T10:35:24Z,2020-04-28T20:40:22Z,model card,,
3863,b'Cannot convert RoBERTa to tflite model',2020-04-20T06:31:44Z,2020-04-21T18:48:29Z,,,
3862,b'New model added',2020-04-20T02:09:56Z,2020-04-20T21:10:02Z,model card,,
3861,b'How to do parameter sharing between two BERT models',2020-04-20T01:40:10Z,2020-07-01T09:28:14Z,wontfix,,
3860,b'Update README.md',2020-04-19T17:50:49Z,2020-04-20T14:10:57Z,model card,,
3859,b'ValueError: Unable to set proper padding strategy as the tokenizer does not have a padding token. ',2020-04-19T16:38:51Z,2020-05-05T16:26:33Z,,,
3858,b'Write with transformers demo hardware',2020-04-19T12:45:56Z,2020-05-11T14:31:19Z,,,
3857,b'[Pipelines] Encode to max length of input not max length of tokenizer for batch input',2020-04-19T12:37:20Z,2020-04-20T18:39:17Z,,,
3856,b'Bug in optimization_tf   create_optimizer ',2020-04-19T11:04:11Z,2020-08-12T03:03:38Z,wontfix,,
3855,b'Fix Documentation issue in BertForMaskedLM forward',2020-04-19T10:36:18Z,2020-04-21T07:08:21Z,,,
3854,b'Added electra-bahasa README',2020-04-19T08:57:07Z,2020-04-20T21:19:15Z,model card,,
3853,b'How to use fine-tuned BART for prediction?',2020-04-18T22:10:00Z,2020-08-16T16:33:47Z,"Discussion, wontfix",,
3852,b'TFT5: get_input_embeddings() and get_output_embeddings()',2020-04-18T20:41:48Z,2020-06-03T13:31:00Z,,,
3851,b'How properly apply a tokenizer map function to a Tensorflow batched dataset?',2020-04-18T19:02:37Z,2020-06-09T01:01:39Z,,"AttributeError, **Note**","AttributeError: 'Tensor' object has no attribute 'numpy'**Note**: I published a working example on [`Google Colab`][2]."
3850,"b""'pad_to_max_length' in Pipeline should be set to True by default""",2020-04-18T15:23:12Z,2020-04-20T09:38:28Z,,ValueError,"ValueError: The sequences building the batch are not of the same size, no tensor can be built. Set `pad_to_max_length=True` to pad the smaller sequencesup to the larger sequence's length."
3849,b'Bug in run_glue ',2020-04-18T12:57:58Z,2020-04-20T14:15:45Z,,"ImportError, ModuleNotFoundError","ImportError: cannot import name 'TrainingArguments' from 'transformers' (/idiap/user/rkarimi/libs/anaconda3/envs/iborn/lib/python3.7/site-packages/transformers/__init__.py)ModuleNotFoundError: No module named 'transformers.hf_argparser'"
3848,b'Electra for question answering',2020-04-18T02:32:39Z,2020-06-11T11:56:49Z,,,
3847,b'Share more details on fine-tuning GPT-2 on WikiText-2 ?',2020-04-17T21:35:44Z,2020-08-27T05:17:28Z,wontfix,,
3846,"b'Roberta (and BERT) tokenization converts ""do not"" to ""don\'t""'",2020-04-17T20:41:45Z,2020-06-03T13:30:47Z,,,
3845,b'list index out of range error when I execute a command with examples/run_glue.py',2020-04-17T15:55:00Z,2020-04-18T01:14:06Z,,IndexError,"IndexError: list index out of range"
3844,b'[TF T5] Higher tolerance for past testing in TF T5',2020-04-17T15:20:01Z,2020-04-17T15:26:17Z,,,
3843,b'[T5] Higher tolerance for past testing in T5',2020-04-17T15:19:36Z,2020-04-17T15:25:15Z,,,
3842,b'Fix bug in run_*.py scripts: double wrap into DataParallel during eval',2020-04-17T14:54:14Z,2020-04-20T23:37:45Z,,,
3841,b'Reproducing squad score with TFXLMRoberta?',2020-04-17T12:42:17Z,2020-06-23T15:46:36Z,wontfix,,
3840,b'Decoding predictions for masked language modeling task using custom BPE',2020-04-17T11:39:48Z,2020-07-02T03:22:45Z,wontfix,,
3839,b'Different output encode and encode_plus',2020-04-17T06:54:46Z,2020-04-17T07:08:17Z,,,
3838,b'Cutom tokenizer not loaded in AutoTokenizer',2020-04-17T05:24:36Z,2020-06-23T15:46:35Z,wontfix,OSError,"OSError: Model name './MyRobertaConfig/' was not found in tokenizers model name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). We assumed './MyRobertaConfig/' was a path, a model identifier, or url to a directory containing vocabulary files named ['vocab.txt'] but couldn't find such vocabulary files at this path or url."
3837,"b'PretrainedTokenizer cleanup: Typehints, decode_batch'",2020-04-17T04:33:03Z,2020-04-28T11:46:57Z,,,
3836,b'Update camembert-base-README.md ',2020-04-17T02:27:01Z,2020-04-18T00:08:13Z,model card,,
3835,b'Transfo-XL cannot generate long texts. Using run_generation.py to generate texts',2020-04-16T20:25:51Z,2020-06-03T13:29:23Z,,,
3834,b'i want help to create saved model(.pth) from Pytorch Dump(pytorch_model.bin) if possible!',2020-04-16T19:35:16Z,2020-04-17T14:48:21Z,,,
3833,b'Remove tqdm logging when using pipelines.',2020-04-16T16:56:28Z,2020-04-20T20:58:53Z,,,
3832,b'The issue I met when do the NER task for Universal language by using XLM-R',2020-04-16T16:54:39Z,2020-06-23T00:25:25Z,wontfix,,
3831,b'AlbertModel output is not HalfTensor when using apex fp16',2020-04-16T16:23:29Z,2020-04-21T13:39:15Z,,,
3830,b'Faster mask computation ',2020-04-16T15:56:12Z,2020-06-03T13:26:17Z,,,
3829,"b""Can't install transformers in conda environment""",2020-04-16T15:41:34Z,2020-08-01T04:18:41Z,wontfix,,
3828,b'Tanh torch warnings',2020-04-16T15:26:40Z,2020-04-16T19:10:36Z,,,
3827,"b""ModuleNotFoundError: No module named '__main__.utils_summarization'; '__main__' is not a package""",2020-04-16T14:57:22Z,2020-04-16T16:56:36Z,bertabs,`ModuleNotFoundError,"`ModuleNotFoundError: No module named '__main__.utils_summarization'; '__main__' is not a package`"
3826,b'[readability] consolidate examples/summarization/bart  and examples/summarization/t5',2020-04-16T14:53:38Z,2020-06-09T15:14:12Z,"Help wanted, Summarization, Examples",,
3825,b'[readability] Consolidate prune_heads logic to PretrainedModel.',2020-04-16T14:49:51Z,2020-07-08T14:45:23Z,"Help wanted, PyTorch, Good First Issue",,
3824,b'[examples] summarization/bart/finetune.py supports t5',2020-04-16T14:45:18Z,2020-04-16T19:15:20Z,,,
3823,b'lowercasing on LM with cased models',2020-04-16T13:49:18Z,2020-06-26T20:47:27Z,wontfix,,
3822,b'getting random results when running run_glue',2020-04-16T13:42:47Z,2020-06-24T10:37:08Z,wontfix,,
3821,b'Typo fix',2020-04-16T12:53:56Z,2020-04-16T15:04:33Z,,,
3820,b'#3787 Fixing the pip install issue by installing from git',2020-04-16T12:10:05Z,2020-07-15T03:25:45Z,wontfix,,
3819,b'Tokenizers Notebook  Issue',2020-04-16T09:03:34Z,2020-04-16T11:12:44Z,,TypeError,"TypeError: cannot create 'BPE' instances`"
3818,b'What are the GPU RAM requirements of popular models?',2020-04-16T08:32:18Z,2020-04-26T20:41:31Z,,,
3817,"b'[Examples, T5] Change newstest2013 to newstest2014 and clean up '",2020-04-16T07:36:15Z,2020-04-16T18:00:42Z,,,
3816,b'Aborted (core dumped) or Kernel dies',2020-04-16T06:45:39Z,2020-08-16T10:33:46Z,wontfix,,
3815,b'How to speed up getting answers?',2020-04-16T05:45:39Z,2020-06-22T07:01:46Z,wontfix,,
3814,b'A bug in the padding of input examples in the NER fine-tuning example',2020-04-15T23:13:27Z,2020-07-11T00:49:31Z,wontfix,,
3813,b'T5 prediction using fine-tuned model',2020-04-15T22:21:46Z,2020-04-16T19:15:19Z,,,
3812,b'Question Answering support for Albert and Roberta in TF',2020-04-15T21:12:58Z,2020-04-17T14:45:30Z,,,
3811,b'Pre-trained BART performance on XSum lower than expected',2020-04-15T16:34:31Z,2020-06-22T10:01:45Z,wontfix,,
3810,"b""run_glue.py example doesn't work for distilbert models""",2020-04-15T16:05:26Z,2020-04-22T00:11:57Z,,TypeError,"TypeError: an integer is required (got type NoneType)"
3809,b'Roberta Tokenizer crashes when tokenizing  empty string in 2.8.0',2020-04-15T15:46:08Z,2020-06-22T15:09:27Z,"wontfix, Core: Tokenization, Should Fix",IndexError,IndexError: string index out of range
3808,b'typo: fine-grained token-leven',2020-04-15T15:25:20Z,2020-04-16T19:11:24Z,,,
3807,b'isort ignores examples directory',2020-04-15T14:01:35Z,2020-05-01T12:36:35Z,,,
3806,"b'[cleanup] factor out get_head_mask, invert_attn_mask, get_extended_attention_mask'",2020-04-15T11:39:23Z,2020-04-16T13:55:27Z,,,
3805,"b""Using fill-mask pipeline to get the \xe2\x80\x9cscore\xe2\x80\x9d for a result it didn't suggest""",2020-04-15T07:32:59Z,2020-06-21T08:22:14Z,wontfix,,
3804,b'Calculated offsets are wrong in squad.py',2020-04-15T05:06:20Z,2020-06-21T08:22:13Z,wontfix,,
3803,b'Fix bug in max_seq_length for preprocessing in ner example',2020-04-15T04:40:14Z,2020-07-15T03:25:44Z,wontfix,,
3802,b'Fix examples/translation/t5 to use newstest2014 rather than newstest2013',2020-04-15T04:18:24Z,2020-04-16T17:56:51Z,,,
3801,b'Fix bug in GLUE example for models that do not require token_type_ids',2020-04-15T03:49:01Z,2020-04-22T00:14:39Z,,TypeError,"TypeError: an integer is required (got type NoneType)"
3800,b'Trainer',2020-04-15T01:34:46Z,2020-04-22T00:11:57Z,,,
3799,b'Clarification about GPT2LMHeadModel lm_head weights',2020-04-15T00:15:09Z,2020-04-26T19:58:53Z,,,
3798,"b'Error when using run_generation.py to generate texts with long prompts, specifically for models -XLM and Openai-GPT'",2020-04-14T22:21:14Z,2020-06-23T00:25:25Z,wontfix,,
3797,"b'[Config, Serialization] more readable config serialization'",2020-04-14T20:50:21Z,2020-04-18T00:07:19Z,,,
3796,b'Calculated offsets are wrong',2020-04-14T20:48:24Z,2020-04-22T15:03:34Z,,,
3795,b'[Pipelines] Clean pipelines test and remove unnecessary code',2020-04-14T20:08:24Z,2020-04-16T14:21:35Z,,,
3794,b'Getting large alloc error while evaluating bert-base on NER task',2020-04-14T19:10:38Z,2020-06-20T21:22:13Z,wontfix,,
3793,b'[Bert] remove hard-coded pad token id',2020-04-14T18:44:44Z,2020-04-16T13:58:58Z,,,
3792,b'Using run_glue.py on external datasets for fine-tuning a RoBERTa classification model --> Is this possible?',2020-04-14T17:16:02Z,2020-05-01T16:02:54Z,,,
3791,b'XLM tokenizer should encode with bos token',2020-04-14T16:10:51Z,2020-04-17T15:28:56Z,,,
3790,b'Fix token_type_id in BERT question-answering example',2020-04-14T15:12:58Z,2020-04-17T15:14:13Z,,,
3789,b'Is there a classical transformer model in the project?',2020-04-14T13:44:31Z,2020-04-15T18:11:42Z,,,
3788,b'Inconsistencies and possible bugs in different tokenizers',2020-04-14T12:50:29Z,2020-04-17T15:28:55Z,"Core: Tokenization, Should Fix",,
3787,"b'In just fouth blocks of the code of the colab notebook ""01-training notebook"", it just failed.'",2020-04-14T11:39:40Z,2020-04-14T12:23:43Z,,TypeError,"TypeError: cannot create 'BPE' instances`"
3786,b'Why force tokens in Bart decoding',2020-04-14T07:05:03Z,2020-04-15T09:13:38Z,,,
3785,b'How to fine tune EncoderDecoder model for training a new corpus of data ?',2020-04-14T05:02:22Z,2020-08-03T16:49:13Z,"wontfix, Core: Encoder-Decoder",,
3784,b'Convert pytorch-pretrained-bert to new version (transformers)',2020-04-14T03:30:08Z,2020-04-17T14:29:29Z,,,
3783,"b'Longformer, a scalable transformer model for long-document NLP tasks'",2020-04-14T00:35:38Z,2020-06-03T13:24:45Z,,,
3782,b'Importing horovod.tensorflow crashes AlbertTokenizer but not BertTokenizer',2020-04-13T23:22:42Z,2020-10-23T08:08:50Z,wontfix,,
3781,b'Create model card',2020-04-13T23:00:13Z,2020-04-20T21:07:35Z,model card,,
3780,b'language modeling other models',2020-04-13T22:44:20Z,2020-04-13T22:44:29Z,,,
3779,b'Problem when Converting a Fine-tuned Checkpoint from TF to PyTorch using ALBERTxxlargev1 Model',2020-04-13T21:40:39Z,2020-07-18T05:22:35Z,wontfix,AttributeError,"AttributeError: 'AlbertForMaskedLM' object has no attribute 'bias'"
3778,"b'[Generation, EncoderDecoder] Apply Encoder Decoder 1.5GB memory savings to TF as well'",2020-04-13T21:03:11Z,2020-04-14T02:29:29Z,,,
3777,b'[PretrainedTokenizer] Factor out tensor conversion method',2020-04-13T18:38:16Z,2020-04-16T19:02:44Z,,,
3776,b'MBartTokenizer:add language codes',2020-04-13T18:04:54Z,2020-06-11T17:02:34Z,,,
3775,b'OpusNMT/MarianMT Machine Translation Models',2020-04-13T14:48:16Z,2020-05-13T21:29:42Z,seq2seq,,
3774,b'Making Simple whitespace tokenizer and then using that tokenizer to make a language model from scratch?',2020-04-13T13:09:32Z,2020-06-20T10:49:54Z,wontfix,,
3773,"b'Why the first item of the config.json of bert is ""architectures"": [""BertForMaskedLM""]'",2020-04-13T11:14:07Z,2020-04-13T14:02:34Z,,,
3772,"b'[TFT5, Cache] Add cache to TFT5'",2020-04-13T09:19:09Z,2020-04-16T14:14:53Z,,,
3771,b'Cannot find the script',2020-04-13T09:14:11Z,2020-04-13T15:08:36Z,,,
3770,"b""Getting error AttributeError: 'BertOnlyMLMHead' object has no attribute 'bias' when giving TF path""",2020-04-13T08:19:02Z,2020-06-22T15:07:49Z,wontfix,AttributeError,"AttributeError: 'BertOnlyMLMHead' object has no attribute 'bias'"
3769,b'Text generation with Transformer-XL stops at <eos> token.',2020-04-13T03:52:20Z,2020-06-03T13:24:32Z,Ex: Generation,,
3768,b'[PL examples]: fix progress bar bug',2020-04-12T20:08:24Z,2020-07-08T10:54:14Z,wontfix,,
3767,b'Issues in Training GPT-2 Model from Scratch (Text Generation-Identifying Epoch Value-Perplexity Calculation)',2020-04-12T18:44:20Z,2020-06-19T20:22:05Z,wontfix,,
3766,b'Fix shuffling issue for distributed training (#3721)',2020-04-12T17:35:37Z,2020-04-13T14:11:19Z,,,
3765,b' Input format for a BertTokenClassification task',2020-04-12T15:44:29Z,2020-06-19T09:01:35Z,wontfix,,
3764,b'long text classification',2020-04-12T14:31:30Z,2020-07-05T04:07:46Z,wontfix,,
3763,b'[CI] Add CircleCI workflow to build docs for preview',2020-04-12T14:07:58Z,2020-04-17T15:23:19Z,,,
3762,b'PPLM Write With Transformer demo not working ',2020-04-12T12:26:26Z,2020-06-04T22:08:54Z,,,
3761,b'Summarization pipeline fails to initialize',2020-04-12T11:15:17Z,2020-04-13T18:23:02Z,,OSError,"OSError: Can't load 't5-base'. Make sure that:"
3760,b'Quick question difference output of Bert models compared to Electra',2020-04-12T08:37:59Z,2020-04-12T12:33:31Z,,,
3759,b'Why does `examples/translation/t5` test on newstest2013 rather than newstest2014?',2020-04-12T07:33:34Z,2020-04-17T13:05:07Z,,,
3758,b'Pipeline for Text Generation: GenerationPipeline',2020-04-12T06:53:04Z,2020-04-22T13:37:03Z,,,
3757,b'Dealing with class imbalance ',2020-04-11T20:28:28Z,2020-07-25T02:39:00Z,wontfix,,
3756,b'Trace log probs on generation',2020-04-11T17:58:29Z,2020-07-15T03:25:46Z,wontfix,,
3755,b'[Docs] Add DialoGPT',2020-04-11T16:27:13Z,2020-04-16T07:04:33Z,,,
3754,b'Deprecation warning due to invalid escape sequences in Python 3.7',2020-04-11T15:25:47Z,2020-06-17T21:46:59Z,,,
3753,b'How to speed up the transformer inference?',2020-04-11T14:11:51Z,2020-04-13T15:09:57Z,,,
3752,b'uss',2020-04-11T14:01:44Z,2020-04-12T20:17:33Z,,,
3751,b'Etract all last hidden states of the input dequences for Question and answering Bert',2020-04-11T12:48:50Z,2020-06-18T16:35:35Z,wontfix,,
3750,"b""ImportError: cannot import name 'HfArgumentParser' from 'transformers'""",2020-04-11T11:35:54Z,2020-04-14T05:22:29Z,,ImportError,"ImportError: cannot import name 'HfArgumentParser' from 'transformers' (/Users/leotreasure/opt/anaconda3/lib/python3.7/site-packages/transformers/__init__.py)"
3749,b'Question about whitespace filtering in squad data processor ',2020-04-11T08:10:17Z,2020-06-17T09:47:37Z,wontfix,,
3748,b'Slow training time on BERT pretraining on multiple gpu compare to single gpu',2020-04-11T07:41:15Z,2020-04-11T14:45:58Z,,,
3747,b'text generation like lorem ipsum but human readable',2020-04-11T04:48:13Z,2020-08-22T05:34:27Z,wontfix,,
3746,b'Added README huseinzol05/albert-tiny-bahasa-cased',2020-04-11T03:27:31Z,2020-04-11T10:42:07Z,model card,,
3745,b'Add `qas_id` to SquadResult and SquadExample',2020-04-11T01:54:56Z,2020-04-20T20:08:57Z,,,
3744,b'Turning off Verbosity on QA model using Pipeline',2020-04-10T20:20:02Z,2020-04-20T20:58:53Z,,,
3743,b'JIT not compatible with PyTorch/XLA',2020-04-10T19:59:48Z,2020-04-16T15:19:25Z,,TypeError,"TypeError: can't pickle torch._C.ScriptFunction objects"
3742,b'Fix `glue_convert_examples_to_features` API breakage',2020-04-10T19:57:27Z,2020-04-10T20:03:28Z,,,
3741,b'Tokenizer Encode More than 2 inputs',2020-04-10T19:38:14Z,2020-06-17T06:47:33Z,wontfix,,
3740,b'[WIP] EncoderDecoder model that works',2020-04-10T17:04:53Z,2020-04-13T15:21:15Z,,,
3739,b'Seq2seq generation with prefix',2020-04-10T16:57:35Z,2020-07-08T13:58:42Z,wontfix,,
3738,b'[docs] The use of `do_lower_case` in scripts is on its way to depreca\xe2\x80\xa6',2020-04-10T16:32:12Z,2020-04-10T16:34:05Z,,,
3737,b'Seq2Seq: decoder hidden_states shape not tested',2020-04-10T15:12:32Z,2020-06-03T13:22:30Z,,,
3736,b'updated dutch squad model card',2020-04-10T15:00:03Z,2020-04-11T10:45:00Z,,,
3735,b'Pipeline for text generation',2020-04-10T14:55:25Z,2020-04-10T15:41:39Z,,,
3734,"b'[Config, Caching] Remove `output_past` everywhere and replace by `use_cache` argument'",2020-04-10T14:51:34Z,2020-04-14T18:40:29Z,,,
3733,b'How i take an OpenAIGPTDoubleHeadsModel from run_language_modeling.py script? ',2020-04-10T13:12:53Z,2020-06-09T19:09:35Z,wontfix,,
3732,b'Fine tuning XLMRoberta for Question Answering',2020-04-10T10:00:30Z,2020-04-10T17:06:39Z,,,
3731,"b'Loading pipeline(""summarization"") failed'",2020-04-10T08:49:33Z,2020-06-03T13:22:45Z,,AttributeError,"AttributeError: 'NoneType' object has no attribute 'config'"
3730,b'OOM error when resuming training from a checkpoint',2020-04-10T08:45:23Z,2020-07-23T05:26:07Z,wontfix,,
3729,b'exbert links for my albert model cards',2020-04-10T01:52:06Z,2020-04-20T14:54:40Z,model card,,
3728,b'Checking that the LM actually trained ',2020-04-09T21:57:00Z,2020-06-12T07:05:02Z,wontfix,,
3727,b'ValueError: Cannot reshape a tensor - TFBertForSequenceClassification',2020-04-09T18:42:07Z,2020-06-15T23:00:49Z,wontfix,,
3726,b'Separate input_ids and decoder_input_ids in model.generate()',2020-04-09T18:30:39Z,2020-04-10T16:23:08Z,,,
3725,b'Fine-tuning for paraphrasing tasks',2020-04-09T18:10:43Z,2020-07-18T16:22:28Z,"Discussion, wontfix",,
3724,b'Has anyone used run_language_modelling.py to train a gpt 2 from scratch? ',2020-04-09T16:54:36Z,2020-06-16T21:18:37Z,"Discussion, wontfix, Ex: LM (Pretraining)",,
3723,b'How to get multiple answers from the context using BertForQuestionAnswering',2020-04-09T16:12:39Z,2020-06-17T02:47:36Z,wontfix,,
3722,b'Integrate Bert-like model on Flax runtime.',2020-04-09T15:24:56Z,2020-10-19T13:55:42Z,,,
3721,"b""DistributedSampler can't shuffle the dataset""",2020-04-09T11:57:08Z,2020-04-13T14:39:23Z,,,
3720,b'Disable @torch.no_grad() for model.generate() ? ',2020-04-09T11:27:05Z,2020-08-09T16:32:22Z,"wontfix, Ex: Generation",,
3719,b'Unable to load german BERT model',2020-04-09T11:16:31Z,2020-06-20T10:49:52Z,wontfix,TypeError,"TypeError: stat: path should be string, bytes, os.PathLike or integer, not NoneType"
3718,"b""loading from tf_ckp and this showed up: AttributeError: 'BertCrf' object has no attribute 'bias' .""",2020-04-09T10:35:45Z,2020-04-10T05:02:46Z,,,
3717,b'how to use transformers with gpu',2020-04-09T10:26:33Z,2020-04-09T11:48:34Z,,,
3716,b'Shift labels internally within TransfoXLLMHeadModel when called with labels',2020-04-09T10:16:32Z,2020-04-13T16:11:24Z,,,
3715,b'How can i conditional fine-tuning with GPT2?',2020-04-09T09:33:34Z,2020-06-03T13:21:27Z,,,
3714,b'Zero shot multilingual BERT',2020-04-09T09:31:34Z,2020-06-16T02:54:11Z,wontfix,,
3713,b'cannot determine what will be the cardinality of the output after applying glue_convert_examples_to_features [TF  2.2.0rcx]',2020-04-09T08:40:18Z,2020-04-13T11:46:51Z,,,
3712,b'Text Generation with XLNet is very Slow',2020-04-09T07:18:10Z,2020-06-03T13:13:36Z,Ex: Generation,,
3711,"b""TransfoXLLMHead doesn't shift labels internally when called for loss""",2020-04-09T06:59:22Z,2020-04-13T16:11:23Z,,,
3710,b'inconsistent tokenize output',2020-04-09T05:32:03Z,2020-06-15T23:00:51Z,wontfix,,
3709,b'Add model tag',2020-04-09T02:37:18Z,2020-04-24T20:16:45Z,model card,,
3708,b'Add model tag',2020-04-09T02:35:35Z,2020-04-24T20:16:44Z,model card,,
3707,b'Distributed training on multiple GPU nodes is slower than on single GPU node',2020-04-08T21:41:32Z,2020-04-11T04:15:45Z,,,
3706,b'Cleanup fast tokenizers integration',2020-04-08T21:08:22Z,2020-04-18T11:43:58Z,,,
3705,b'Update tokenizers to 0.7.0-rc5',2020-04-08T20:32:06Z,2020-04-10T18:23:49Z,,,
3704,b'Queries about the Notation and Model training of T5 and ELECTRA sentiment classification.',2020-04-08T20:24:02Z,2020-07-10T08:27:12Z,wontfix,,
3703,b'Token-level regression mode added in ForTokenClassification models',2020-04-08T19:00:34Z,2020-07-15T03:25:48Z,wontfix,,
3702,b'Add `run_glue_tpu.py` that trains models on TPUs',2020-04-08T17:17:32Z,2020-04-10T16:53:55Z,,,
3701,b'Problem with https://transformer.huggingface.co/doc/gpt2-xl',2020-04-08T15:55:55Z,2020-06-10T00:02:47Z,wontfix,,
3700,b'Would the weights for the main body of the pertained GPT2Model and pertained GPT2DoubleHeadsModel be identical?',2020-04-08T14:22:31Z,2020-06-10T00:07:06Z,wontfix,,
3699,b'Bug in ElectraForTokenClassification',2020-04-08T13:58:20Z,2020-04-08T18:33:11Z,,AttributeError,"AttributeError: 'ElectraForTokenClassification' object has no attribute 'num_labels'"
3698,b'More doc for model cards',2020-04-08T13:57:34Z,2020-04-08T16:12:53Z,,,
3697,b'Fix force_download of files on Windows',2020-04-08T11:51:31Z,2020-04-09T18:44:57Z,,FileExistsError,"FileExistsError: [WinError 183] Cannot create a file when that file already exists:"
3696,b'Can not set different token and model dir in `run_glue.py`',2020-04-08T11:04:47Z,2020-06-14T12:22:53Z,wontfix,,
3695,b'Deserialize BERT Sequence Lassifier Quantized Model & Inferencing Issue',2020-04-08T10:30:15Z,2020-06-14T12:22:52Z,wontfix,,
3694,b'Extending XLM Roberta for Question Answering',2020-04-08T09:41:27Z,2020-04-08T12:53:55Z,,AttributeError,"AttributeError: 'NoneType' object has no attribute 'seek'"
3693,"b'How can I pinpoints Logs directory to Google Drive while finetuning GPT-2 model, which helps in visualizing data via tensorboard?'",2020-04-08T09:12:14Z,2020-06-14T11:22:54Z,wontfix,,
3692,b'How to use Huggingface pytorch bert to generate the prediction TSV file from the test set of a GLUE task?',2020-04-08T04:53:23Z,2020-06-08T08:50:06Z,wontfix,,
3691,b'cannot import name AddedToken',2020-04-08T04:14:29Z,2020-04-08T06:27:14Z,,ImportError,"ImportError: cannot import name 'AddedToken'"
3690,"b'BertSelfAttention have not Add and Norm layer, why???'",2020-04-08T02:51:15Z,2020-04-10T18:31:15Z,,,
3689,"b""Can't update the train_batch_size and eval_batch_size for the training image in a docker container""",2020-04-08T02:48:17Z,2020-04-08T18:56:56Z,,,
3688,b'Big cleanup of `glue_convert_examples_to_features`',2020-04-08T01:02:34Z,2020-04-10T14:20:18Z,,,
3687,b'Is it possible to use multiprocessing for pipelines?',2020-04-07T23:29:41Z,2020-06-14T00:30:41Z,wontfix,,
3686,b'Bug in variable name in NER',2020-04-07T22:03:46Z,2020-05-05T23:04:59Z,,AttributeError,"AttributeError: 'BertTokenizer' object has no attribute 'num_added_tokens'"
3685,b'Requesting model for TFAlbertForQuestionAnswering',2020-04-07T21:30:50Z,2020-05-07T20:16:32Z,,,
3684,b'Updating the TensorFlow models to work as expected with tokenizers v3.0.0',2020-04-07T19:46:22Z,2020-04-08T20:22:45Z,,,
3683,b'question-answering pipeline error : too many values to unpack (expected 2)',2020-04-07T19:04:29Z,2020-07-09T13:43:59Z,"wontfix, Core: Pipeline",ValueError,"ValueError: too many values to unpack (expected 2)"
3682,"b'[T5, generation] Add decoder caching for T5'",2020-04-07T18:32:37Z,2020-04-09T23:02:50Z,,,
3681,b'updating to new transformer',2020-04-07T18:06:44Z,2020-04-07T18:07:47Z,,,
3680,b'How to use GPT2DoubleHeadsModel?',2020-04-07T17:46:39Z,2020-09-25T14:45:35Z,,,
3679,"b'Update doc for {Summarization,Translation}Pipeline and other tweaks'",2020-04-07T17:45:04Z,2020-04-08T13:45:02Z,,,
3678,b'run_generation.py with empty input',2020-04-07T16:33:47Z,2020-04-28T12:33:16Z,Ex: Generation,,
3677,b'Does anyone have the XLNet (and ALBERT) NER performance on CONLL-2003 ',2020-04-07T15:57:47Z,2020-05-15T01:14:15Z,,,
3676,b'gpt2-medium fine-tuned model.generate joins words and sentences together without space or newline',2020-04-07T15:14:23Z,2020-08-09T16:32:21Z,"wontfix, fp16",,
3675,b'Wrong tokenizer configuration in sentiment-analysis pipeline',2020-04-07T14:41:36Z,2020-06-22T16:34:48Z,"wontfix, Core: Pipeline",,
3674,"b'[Examples, Benchmark] Improve benchmark utils'",2020-04-07T12:58:28Z,2020-04-07T20:25:57Z,,,
3673,b'TypeError while loading the model built from scratch using transformer ',2020-04-07T10:03:57Z,2020-06-27T20:53:47Z,wontfix,TypeError,"TypeError: stat: path should be string, bytes, os.PathLike or integer, not NoneType"
3672,b'How to train BART text summarization with your own data?',2020-04-07T09:09:09Z,2020-04-16T15:04:09Z,,,
3671,b'Loading pre-trained ELECTRA checkpoint to HuggingFace',2020-04-07T09:08:52Z,2020-06-16T21:18:48Z,wontfix,NotImplementedError,"NotImplementedError: Weights may only be loaded based on topology into Models when loading TensorFlow-formatted weights (got by_name=True to load_weights)."
3670,b'Has anyone used the run_language_modeling.py to train a gpt2 in a different language? is it possible?',2020-04-07T07:11:01Z,2020-06-10T22:35:48Z,wontfix,,
3669,b'[examples] Generate argparsers from type hints on dataclasses',2020-04-07T05:25:29Z,2020-04-10T16:21:59Z,,,
3668,"b'\xe2\x9d\x93 In BART, why forcing the first token to BOS ?'",2020-04-07T04:50:11Z,2020-04-09T23:27:08Z,,,
3667,b'Any Ideas on how to generate in bulk with CTRL?',2020-04-07T04:45:19Z,2020-06-10T13:20:54Z,wontfix,,
3666,b'Created README.md for model card ChemBERTa',2020-04-07T04:25:11Z,2020-04-08T13:10:21Z,model card,,
3665,b'Fix mlm',2020-04-07T01:02:58Z,2020-07-09T13:40:25Z,wontfix,,
3664,b'Unable to serialize/save TF2.0 RobertaSequenceClassification model to saved model format',2020-04-07T00:18:36Z,2020-06-10T22:57:53Z,"High-Level feature, TensorFlow, Core: Modeling, Should Fix",TypeError,"TypeError: ('Not JSON Serializable:', RobertaConfig {"
3663,b'Speedup torch summarization tests',2020-04-06T21:32:54Z,2020-04-07T18:01:31Z,,,
3662,b'Create model card  for NLP4H/ms_bert',2020-04-06T19:20:06Z,2020-04-06T20:27:51Z,model card,,
3661,b'fixed TransfoXLLMHeadModel documentation',2020-04-06T17:59:49Z,2020-04-06T22:47:52Z,,,
3660,b'Exception: process 0 terminated with signal SIGKILL',2020-04-06T16:48:43Z,2020-08-05T23:30:04Z,wontfix,Exception,"Exception: process 0 terminated with signal SIGKILL"
3659,b'Add model card',2020-04-06T16:31:03Z,2020-04-06T20:30:03Z,model card,,
3658,b'Add model card',2020-04-06T16:24:02Z,2020-04-06T20:29:52Z,model card,,
3657,b'Weird summarization results - the summary is longer than the input',2020-04-06T16:23:05Z,2020-07-17T14:42:59Z,wontfix,,
3656,b'Add model card',2020-04-06T16:11:37Z,2020-04-06T20:29:45Z,model card,,
3655,b'Add model card',2020-04-06T16:08:19Z,2020-04-06T20:29:36Z,model card,,
3654,b'Create model card',2020-04-06T15:57:19Z,2020-04-06T20:29:26Z,model card,,
3653,b'Create model card',2020-04-06T15:44:22Z,2020-04-06T20:29:03Z,model card,,
3652,b'Create README.md for ktrapeznikov/biobert_v1.1_pubmed_squad_v2',2020-04-06T14:00:49Z,2020-04-06T20:35:30Z,model card,,
3651,b'\xe2\x9d\x93Adding new tokens to pre-trained tokenizer',2020-04-06T14:00:11Z,2020-04-09T06:45:21Z,,,
3650,b'How can I judge whether is in the dictionary?',2020-04-06T13:07:45Z,2020-06-12T14:22:11Z,wontfix,,
3649,b'Add model card for BERTeus',2020-04-06T11:22:03Z,2020-04-06T20:21:26Z,model card,,
3648,b'Chatbot QnA feature for given text corpus',2020-04-06T10:26:41Z,2020-06-16T10:12:50Z,,,
3647,b'Bertabs metrics lower than paper',2020-04-06T10:12:47Z,2020-08-27T05:17:32Z,"wontfix, bertabs",,
3646,b'Allow token regression in ForTokenClassification models',2020-04-06T09:59:22Z,2020-06-12T15:22:11Z,wontfix,,
3645,b'\xe2\x9d\x93 How to run pipeline (summarization) in FP16 mode ?',2020-04-06T09:03:59Z,2020-04-13T01:42:18Z,,,
3644,b'How can I track the performance of my GPT-2 model during finetuning?',2020-04-06T08:25:35Z,2020-04-07T20:04:12Z,,,
3643,b'BioMed Roberta-Base (AllenAI)',2020-04-06T00:57:23Z,2020-04-06T20:12:10Z,model card,,
3642,b'Fix roberta checkpoint conversion script',2020-04-06T00:51:48Z,2020-04-07T16:03:23Z,,,
3641,"b""Can't evaluate official TensorFlow NER model""",2020-04-05T20:34:56Z,2020-04-22T19:37:45Z,,TypeError,"TypeError: unsupported operand type(s) for +: 'NoneType' and 'int'"
3640,b'Wrong Mask LM prediction with BertForMaskedLM',2020-04-05T17:02:12Z,2020-04-16T15:11:41Z,,,
3639,"b""Summarization pipeline - Couldn't reach server at 'https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-large-cnn/modelcard.json'""",2020-04-05T16:30:15Z,2020-04-07T14:20:29Z,,,
3638,b'Translation pipeline bug after 398 characters',2020-04-05T14:18:46Z,2020-04-07T14:41:46Z,,,
3637,b'[TransfoXL] fix argument order of update_mems fn in TF version',2020-04-05T10:20:47Z,2020-04-05T10:33:42Z,,,
3636,"b'[Docs, T5] Fix TF T5 examples docstring'",2020-04-05T10:11:25Z,2020-04-05T10:23:09Z,,,
3635,b'Reinitializing layers in BERT',2020-04-05T09:12:07Z,2020-06-12T03:08:36Z,wontfix,,
3634,b'Custom collate function that pads only to the longest sequence?',2020-04-05T09:10:56Z,2020-06-12T03:08:35Z,wontfix,,
3633,b'Cased model + `--do_lower_case` in documentation?',2020-04-05T06:08:05Z,2020-04-10T16:34:05Z,,,
3632,b'[Bart] Replace config.output_past with use_cache kwarg',2020-04-04T20:45:17Z,2020-04-07T23:08:26Z,,,
3631,b'Fix RoBERTa/XLNet Pad Token in run_multiple_choice.py',2020-04-04T18:22:34Z,2020-04-06T20:52:23Z,,,
3630,b'How to get top 10 possible set of words to calculate Top-K accuracy and MRR?',2020-04-04T17:04:28Z,2020-06-12T20:22:11Z,wontfix,,
3629,b'Create README.md for ktrapeznikov/scibert_scivocab_uncased_squad_v2',2020-04-04T16:33:29Z,2020-04-04T19:18:32Z,model card,,
3628,b'Create README.md for ktrapeznikov/albert-xlarge-v2-squad-v2',2020-04-04T16:05:20Z,2020-04-04T19:18:55Z,model card,,
3627,b'Failing to load saved TFBertModel',2020-04-04T15:21:26Z,2021-01-11T11:58:38Z,wontfix,TypeError,"TypeError: The two structures don't have the same nested structure."
3626,b'ValueError: You have to specify either input_ids or inputs_embeds!',2020-04-04T09:44:01Z,2020-04-05T10:14:43Z,,ValueError,"ValueError: in converted code:"
3625,b'how can i run gpt2 model on tf serving ?',2020-04-04T08:18:16Z,2021-04-26T15:03:00Z,,,
3624,b'Add code to pretrain T5 model from scratch',2020-04-03T22:21:32Z,2020-06-03T13:14:55Z,,,
3623,b'Create model card',2020-04-03T22:10:00Z,2020-04-04T12:20:35Z,model card,,
3622,b'default of weight_decay  for run_language_modeling.py ',2020-04-03T19:48:12Z,2020-06-10T06:19:02Z,wontfix,,
3621,b'fix prepare_for_tokenization in tokenization_roberta.py',2020-04-03T19:36:52Z,2020-07-15T03:25:49Z,wontfix,,
3620,b'Update notebooks',2020-04-03T18:18:49Z,2020-04-06T18:32:40Z,,,
3619,b'Feature Request: Fill Mask more than 1 token',2020-04-03T14:49:09Z,2020-04-03T16:16:41Z,,,
3618,b'Update German Bert model card',2020-04-03T13:54:01Z,2020-04-04T19:28:46Z,model card,,
3617,b'Choosing between adding frequent out of vocab words and doing further pretraining.',2020-04-03T13:14:54Z,2020-06-09T18:18:00Z,wontfix,,
3616,b'Out of memory error while training GPT2-large on 8x32GB Nvidia Volta',2020-04-03T11:28:18Z,2020-08-09T16:32:20Z,"wontfix, Distributed Training / Models",RuntimeError,"RuntimeError: CUDA out of memory. Tried to allocate 190.00 MiB (GPU 2; 31.72 GiB total capacity; 28.71 GiB already allocated; 135.88 MiB free; 1.66 GiB cached)"
3615,b'Mismatch of loss shape in document with output of TransfoXL',2020-04-03T11:07:44Z,2020-04-11T14:12:53Z,,,
3614,b'The tensorflow implementation of T5ForConditionalGeneration runs much slower than the pytorch one. GPU utilization is 30%',2020-04-03T10:52:40Z,2020-10-10T08:13:56Z,wontfix,,
3613,b'Added albert-base-bahasa-cased README and fixed tiny-bert-bahasa-cased README',2020-04-03T10:42:45Z,2020-04-03T13:28:44Z,model card,,
3612,b'training GPT2 from scratch : implement causal attention mask?',2020-04-03T09:36:50Z,2020-06-03T13:08:41Z,,,
3611,b'corrected mistake in polish model cards',2020-04-03T08:46:22Z,2020-04-03T13:07:16Z,,,
3610,"b'How can u make sure that my transformer model should only one GPU, though the serve has multiple GPU cards.'",2020-04-03T08:15:49Z,2020-04-06T06:43:56Z,,,
3609,b'Filling more than 1 masked token at a time',2020-04-03T08:06:22Z,2021-07-07T15:03:48Z,,,
3608,b'RobertaTokenizer corner case with empty string',2020-04-03T07:53:33Z,2020-08-22T05:34:29Z,"wontfix, Core: Tokenization",,
3607,"b'Allow the creation of ""entity groups"" for NerPipeline #3548'",2020-04-03T07:35:38Z,2020-04-25T01:21:52Z,,,
3606,b'Fix typo in FeatureExtractionPipeline docstring',2020-04-03T07:00:01Z,2020-04-08T13:08:57Z,,,
3605,b'\xf0\x9f\x90\x9b Summarization pipeline : T5-base much slower than BART-large',2020-04-03T06:59:18Z,2020-04-09T23:02:50Z,Core: Pipeline,,
3604,b'Update README.md',2020-04-03T04:55:58Z,2020-04-03T13:28:25Z,model card,,
3603,b'Update README.md',2020-04-03T04:49:55Z,2020-04-03T13:27:58Z,model card,,
3602,b'Multilingual BART -',2020-04-03T00:06:18Z,2020-04-10T15:25:39Z,,"`config.normalize_before`, `config.add_final_layer_norm`","`config.normalize_before`: all the `LayerNorm` calls happen before attention calls`config.add_final_layer_norm`: There is one extra layer_norm in the decoder"
3601,"b'[Generate, Test] Split generate test function into beam search, no beam search'",2020-04-02T20:43:57Z,2020-04-06T08:37:05Z,,,
3600,"b""Why isn't there a SequenceClassificationModel for GPT-2 (and some other models)?""",2020-04-02T20:36:30Z,2020-06-09T07:18:05Z,wontfix,,
3599,b'Why is there not a SequenceClassification model for GPT-2?',2020-04-02T20:36:28Z,2020-04-02T21:44:12Z,,,
3598,"b'After enable fp16, torch.save model has error'",2020-04-02T20:28:36Z,2020-06-09T11:18:00Z,wontfix,,
3597,b'CTRL generates French text when I want English texts',2020-04-02T20:16:13Z,2020-06-28T23:46:32Z,"wontfix, Ex: Generation",,
3596,b'batch_encode_plus with pad_to_max_length but no max_length is not padding the output',2020-04-02T19:49:11Z,2020-06-17T22:16:20Z,"wontfix, Core: Tokenization",,
3595,b'[Generation] delete print statement',2020-04-02T19:36:49Z,2020-04-02T19:49:35Z,,,
3594,b'Wrong tokenization for distilbert-base-multilingual-cased',2020-04-02T18:23:05Z,2020-06-09T18:18:02Z,"wontfix, Core: Tokenization",,
3593,b'Transformers and BERT: dealing with possessives and apostrophes when encode',2020-04-02T16:23:17Z,2020-06-09T07:18:02Z,wontfix,,
3592,b'Issues with using SciBERT for Summarizer',2020-04-02T15:22:02Z,2020-04-03T19:21:40Z,,AttributeError,"AttributeError: You tried to generate sequences with a model that does not have a LM Head.Please use another model class (e.g. `OpenAIGPTLMHeadModel`, `XLNetLMHeadModel`, `GPT2LMHeadModel`, `CTRLLMHeadModel`, `T5WithLMHeadModel`, `TransfoXLLMHeadModel`, `XLMWithLMHeadModel`, `BartForConditionalGeneration` )"
3591,b'Cannot load model in tranformers',2020-04-02T14:51:36Z,2020-06-09T18:18:01Z,"wontfix, Version mismatch",,
3590,b'min_length parameter in default pipeline summarization produces output smaller than min_length',2020-04-02T14:49:04Z,2020-04-02T19:31:28Z,,,
3589,b'Evaluation - Output False Positive and False Negative Sentences',2020-04-02T14:28:03Z,2020-06-09T07:18:01Z,wontfix,,
3588,b'added model_cards for polish squad models',2020-04-02T13:35:28Z,2020-04-03T01:40:17Z,model card,,
3587,b'How to fine tune T5 like for translation tasks?',2020-04-02T09:53:09Z,2020-04-04T15:39:08Z,,,
3586,"b'when I run transformers in Docker container, it appeared this error'",2020-04-02T09:12:10Z,2020-04-21T01:39:47Z,,,
3585,b'Reason behind the layers taken for distilbert-multilingual',2020-04-02T08:59:18Z,2020-04-09T18:31:14Z,,,
3584,b'cased -> uncased in BERT GLUE example',2020-04-02T08:03:02Z,2020-04-10T16:34:05Z,,,
3583,b'Dict in the first positional arguments',2020-04-02T05:25:47Z,2020-04-04T00:39:56Z,"TensorFlow, Usage",InvalidArgumentError,"InvalidArgumentError: Index out of range using input dim 1; input has only 1 dims [Op:StridedSlice] name: tf_bert_model/bert/strided_slice/"
3582,b'Does the BART model support Chinese? Having the pre-trained Chinese model?',2020-04-02T02:35:09Z,2020-06-09T07:17:59Z,wontfix,,
3581,b'Different outputs in using convert_roberta_original_pytorch_checkpoint_to_pytorch.py',2020-04-02T02:22:15Z,2020-04-10T18:32:02Z,,,
3580,b'wrong parameters order in TFTransfoXLMainLayer _update_mems call',2020-04-01T22:35:46Z,2020-04-05T10:33:42Z,,,
3579,b'Summarization pipeline max_length parameter seems to just cut the summary rather than generating a complete sentence within the max length',2020-04-01T21:50:09Z,2020-04-02T11:17:46Z,,,
3578,b'[WIP] Adding model parallelism for T5 (should work for other models as well)',2020-04-01T21:27:55Z,2021-04-26T15:03:01Z,Distributed Training / Models,,
3577,b'DistilBert not giving hidden states',2020-04-01T21:15:05Z,2020-04-01T21:22:34Z,,,
3576,b'T5 fine tune for seq2seq generation',2020-04-01T20:48:05Z,2020-04-16T19:15:19Z,,,
3575,b'Create README.md',2020-04-01T20:05:21Z,2020-04-03T01:43:13Z,model card,,
3574,b'[Benchmark] QUAERO French Medical Corpus for Named Entity Recognition',2020-04-01T16:24:44Z,2020-06-07T17:28:35Z,wontfix,,
3573,b'How can I use masked_lm_labels correctly?',2020-04-01T15:42:00Z,2020-06-09T07:18:00Z,wontfix,,
3572,b'BART run run_train.sh RuntimeError: expected device cuda:0 but got device cpu',2020-04-01T14:58:14Z,2020-06-14T11:22:56Z,wontfix,"Exception, RuntimeError","Exception: RuntimeError: expected device cuda:0 but got device cpu"
3571,b'transformers pipeline in GCP cloud functions',2020-04-01T11:32:47Z,2020-05-16T10:54:34Z,Help wanted,,
3570,b'tokenizer cannot load form model on disk',2020-04-01T10:24:03Z,2020-04-01T10:30:33Z,,,
3569,b'Regarding distilbert-multilingual-uncased model',2020-04-01T10:11:08Z,2020-06-07T11:28:38Z,wontfix,,
3568,b'Create README.md',2020-04-01T09:28:12Z,2020-04-03T01:48:32Z,model card,,
3567,b'Add tiny-bert-bahasa-cased model card',2020-04-01T07:33:11Z,2020-04-01T11:15:00Z,model card,,
3566,b'BertJapaneseTokenizer accept options for mecab',2020-04-01T06:57:33Z,2020-04-03T15:12:19Z,,,
3565,b'Language model fine tuning using scibert as the base model',2020-04-01T06:52:21Z,2020-05-07T12:43:38Z,Usage,,
3564,b'Tokenizers: setting bos_token_id = 0 and adding language_pair_codes',2020-04-01T05:30:09Z,2020-06-09T07:18:07Z,"wontfix, Core: Tokenization",,
3563,b'update run_language_modeling.py for high efficiency in Multi GPUs',2020-04-01T03:44:47Z,2020-04-01T04:03:51Z,,,
3562,"b'can not init tokenizers from third party model , on albert model'",2020-04-01T03:40:28Z,2020-04-07T08:42:16Z,,,
3561,b'Evaluation of labelled test set?',2020-04-01T02:56:32Z,2020-04-13T09:13:39Z,,,
3560,b'Mean reduce over last hidden state',2020-04-01T00:23:17Z,2020-06-07T03:14:40Z,wontfix,,
3559,b'How to trace the BertForQuestionAnswering',2020-04-01T00:09:23Z,2020-06-07T03:14:39Z,wontfix,,
3558,b'Metrics are coupled to the run_glue.py tasks.',2020-03-31T23:03:38Z,2020-06-07T09:28:37Z,wontfix,,
3557,b'Create model card',2020-03-31T20:26:06Z,2020-04-01T11:14:24Z,model card,,
3556,"b'[T5, examples] replace heavy t5 models with tiny random models'",2020-03-31T17:13:54Z,2020-04-02T10:34:06Z,,,
3555,b'T5 for summarization: pipeline x T5ForConditionalGeneration different results',2020-03-31T17:00:31Z,2020-03-31T18:29:47Z,,,
3554,b'resize_token_embeddings error for Transformer-XL',2020-03-31T16:10:44Z,2020-06-10T23:03:07Z,,AttributeError,"AttributeError: 'AdaptiveEmbedding' object has no attribute 'weight'"
3553,b'unable to completely load T5 pretrained model; missing/unexpected keys',2020-03-31T15:03:28Z,2020-05-06T11:54:46Z,,,
3552,b'Update README.md',2020-03-31T14:15:49Z,2020-03-31T14:40:35Z,model card,,
3551,b'Recommended preprocessing steps for english sentences in GPT2',2020-03-31T14:05:32Z,2020-04-01T08:50:48Z,,,
3550,"b'[T5, Testst] Add extensive hard-coded integration tests and make sure PT and TF give equal results'",2020-03-31T10:10:25Z,2020-04-01T16:01:34Z,,,
3549,"b""model name '../data/bert_models/chinese_finetuned_lm/pytorch_model.bin' was not found in model name list . Creating an empty model card.""",2020-03-31T09:23:29Z,2020-06-06T12:14:38Z,wontfix,,
3548,"b'How to extract ""contiguous tokens"" from `NerPipeline` results?'",2020-03-31T09:15:19Z,2020-05-17T07:58:02Z,,,
3547,"b'[T5, TF 2.2] change tf t5 argument naming'",2020-03-31T09:03:22Z,2020-04-01T20:04:20Z,,ValueError,"ValueError: The first argument to `Layer.call` must always be passed."
3546,b'Impossible to use T5 11b',2020-03-31T08:33:03Z,2020-03-31T20:22:55Z,Core: Modeling,,
3545,"b'[T5, pipeline] fix bug in warnings'",2020-03-31T07:48:13Z,2020-04-01T19:59:13Z,,,
3544,b'[examples] unit test for run_bart_sum',2020-03-31T05:10:06Z,2020-04-15T22:35:01Z,,,
3543,b'[testing] add timeout_decorator',2020-03-31T04:12:09Z,2020-05-01T13:05:48Z,,,
3542,"b""KeyError: 'answers' error when using BioASQ dataset using Huggingface Transformers""",2020-03-31T03:33:33Z,2020-06-24T19:28:48Z,wontfix,KeyError,"KeyError: 'answers'"
3541,"b""forward() got an unexpected keyword argument 'output_all_encoded_layers'""",2020-03-30T21:31:17Z,2020-04-01T21:47:44Z,Usage,TypeError,TypeError: forward() got an unexpected keyword argument 'output_all_encoded_layers'
3540,"b""Quick Tour TF2.0 error: dataclasses.FrozenInstanceError: cannot assign to field 'label'""",2020-03-30T21:00:51Z,2020-03-31T12:02:50Z,,dataclasses.FrozenInstanceError,"dataclasses.FrozenInstanceError: cannot assign to field 'label'"
3539,b'T5 Summarization',2020-03-30T20:35:43Z,2020-04-01T20:04:20Z,,ValueError,"ValueError: The first argument to `Layer.call` must always be passed."
3538,b'[Docs] Add usage examples for translation and summarization',2020-03-30T20:33:02Z,2020-03-31T13:36:04Z,,,
3537,b'Add model cards',2020-03-30T19:44:09Z,2020-03-31T11:54:45Z,model card,,
3536,b'[Encoder-Decoder] Force models outputs to always have batch_size as their first dim',2020-03-30T19:28:22Z,2020-04-02T13:18:34Z,,,
3535,b'Error on fine-tuning XLM like model on SQUaD like dataset',2020-03-30T19:26:04Z,2020-10-10T08:14:02Z,wontfix,TypeError,"TypeError: forward() got an unexpected keyword argument 'cls_index'"
3534,b'pretrained EsperBERTo',2020-03-30T15:15:24Z,2020-06-06T09:14:38Z,"wontfix, Ex: LM (Pretraining), Usage",,
3533,b'Error when training with distributed training on 4/8 Nvidia v100.',2020-03-30T14:31:53Z,2020-04-02T14:24:23Z,,RuntimeError,"RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by (1) passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`; (2) making sure all `forward` function outputs participate in calculating loss. If you already have done the above two steps, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable). (prepare_for_backward at /pytorch/torch/csrc/distributed/c10d/reducer.cpp:518)"
3532,b'Resizing embedding matrix before sending it to the optimizer.',2020-03-30T14:16:23Z,2020-04-02T19:00:05Z,,,
3531,"b'[T5, docs] remove useless and confusing lm_labels line'",2020-03-30T12:58:36Z,2020-03-31T13:32:26Z,,,
3530,"b'TypeError: sequence item 0: expected str instance, NBProgressBar found'",2020-03-30T12:45:11Z,2020-06-21T17:01:47Z,"wontfix, TensorFlow",TypeError,"TypeError: sequence item 0: expected str instance, NBProgressBar found"
3529,b'[T5] fix lm lables in docstring',2020-03-30T12:06:26Z,2020-03-30T12:26:25Z,,,
3528,b'Unexpected ZeroDivisionError when calling model.prune_heads',2020-03-30T11:16:08Z,2020-06-06T08:14:39Z,wontfix,ZeroDivisionError,"ZeroDivisionError: float division by zero"
3527,b'Bart.generate requires config.output_past=True',2020-03-30T10:24:29Z,2020-04-07T23:08:26Z,,,
3526,b'bug in run_glue.py',2020-03-30T09:07:38Z,2020-06-05T14:11:12Z,wontfix,KeyError,"KeyError: 'flaubert'"
3525,b'Issue loading custom tokenizer for fine-tuning gpt2',2020-03-30T07:38:54Z,2020-06-09T07:18:06Z,"wontfix, Core: Tokenization",,
3524,b'Add shoarora/electra and alectra model cards',2020-03-30T04:00:35Z,2020-03-31T11:58:49Z,model card,,
3523,b'Why GPT2 train loss and topK accuracy both decrease?',2020-03-30T02:49:15Z,2020-03-30T10:55:10Z,,,
3522,"b""why isn't AlbertForMultipulChioce in modeling_albert?""",2020-03-30T01:38:00Z,2020-07-22T10:52:37Z,"wontfix, Core: Modeling",,
3521,b'[T5] make decoder input ids optional for t5 training',2020-03-29T23:48:19Z,2020-03-30T11:45:27Z,,,
3520,b'WIP: haiku bert implementation',2020-03-29T22:57:05Z,2020-07-28T02:34:43Z,wontfix,,
3519,b'Resizing embedding matrix before sending it to the optimizer.',2020-03-29T20:01:30Z,2020-03-30T14:16:29Z,,,
3518,b'Argument \xe2\x80\x9cnever_split\xe2\x80\x9d not working on bert tokenizer',2020-03-29T17:23:36Z,2020-06-08T21:11:18Z,Core: Tokenization,,
3517,b'[Tokenization] fix edge case for bert tokenization',2020-03-29T16:53:12Z,2020-04-07T20:26:32Z,,,
3516,b'[Docs] examples/summarization/bart: Simplify CNN/DM preprocessing steps',2020-03-29T14:46:59Z,2020-03-29T17:25:42Z,,,
3515,b'Isort installed from github branch does not correspond to circle ci isort',2020-03-29T13:46:09Z,2020-03-31T11:05:18Z,,,
3514,b'[Examples] Clean summarization and translation example testing files for T5 and Bart',2020-03-29T13:10:55Z,2020-03-31T15:54:14Z,,,
3513,b'Adding mbart-large-cc25',2020-03-29T12:32:30Z,2020-07-07T17:23:01Z,"Help wanted, Documentation, New model, seq2seq, translation",,
3512,b'how to get activation weights of a pretrained model?',2020-03-29T11:10:47Z,2020-06-05T04:11:16Z,wontfix,,
3511,b'Update the NER TF script',2020-03-29T10:51:14Z,2020-03-30T13:50:13Z,,,
3510,b'reproducing the performance of XLM-ROBERTA on MLQA dataset on the zh language',2020-03-29T10:01:18Z,2020-04-09T13:19:32Z,,,
3509,b'Fix for continuing training',2020-03-29T07:32:00Z,2020-04-02T18:07:08Z,,,
3508,b'[Bart] when output_paste=False BartForConditionalGeneration raises confusing error',2020-03-29T06:49:32Z,2020-04-07T23:08:26Z,seq2seq,ValueError,"ValueError: too many values to unpack (expected 2)"
3507,b'[T5] Add training documenation ',2020-03-29T01:54:49Z,2020-03-30T11:35:55Z,,,
3506,b'No grad feature in model parameters',2020-03-29T01:47:31Z,2020-04-02T17:07:12Z,,,
3505,b'Add clear description of how to train T5',2020-03-29T01:43:54Z,2020-03-29T01:59:45Z,,,
3504,b'[Docs] Update usage doc regarding generate fn',2020-03-29T00:13:49Z,2020-03-31T13:31:47Z,,,
3503,b'Distil-BART?',2020-03-28T23:16:18Z,2020-06-27T21:10:01Z,"Distillation, seq2seq",,
3502,b'Bert Batch Encode Plus adding an extra [SEP]',2020-03-28T17:33:34Z,2020-04-07T20:26:31Z,,,
3501,b'[BART] Update encoder and decoder on set_input_embedding',2020-03-28T16:30:44Z,2020-03-30T16:20:38Z,,,
3500,b'[Wait to merge] [Bart] Rename lm_labels argument to masked_lm_labels',2020-03-28T15:58:49Z,2020-06-28T19:01:46Z,,,
3499,b'masked_lm_loss in BertForMaskedLM model',2020-03-28T15:21:59Z,2020-03-30T09:56:21Z,,`RuntimeError,"`RuntimeError: Assertion 'cur_target >= 0 && cur_target < n_classes' failed.`"
3498,b'XLM-ROBERTA',2020-03-28T15:03:35Z,2020-03-31T11:04:17Z,Core: Modeling,,
3497,b'REALM',2020-03-28T10:43:46Z,2022-01-18T12:24:14Z,"New model, Feature request",,
3496,b'How to load BertforSequenceClassification  models weights into BertforTokenClassification model?',2020-03-28T05:10:31Z,2020-06-09T07:18:04Z,"wontfix, Usage",,
3495,b'CUDA error: CUBLAS_STATUS_ALLOC_FAILED When running language modeling using bert-base-cased',2020-03-28T04:54:58Z,2020-06-17T22:22:10Z,"wontfix, Ex: LM (Finetuning)",RuntimeError,"RuntimeError: CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`"
3494,b'TypeError when using Feature Extraction Pipeline with XLM roberta',2020-03-28T04:00:56Z,2020-03-31T09:09:25Z,,**TypeError,"**TypeError: forward() got an unexpected keyword argument 'training'**"
3493,b'Finetuning GPT-2 ',2020-03-28T01:39:12Z,2020-06-15T23:00:50Z,wontfix,,
3492,b'[model_cards]: use MIT license for all dbmdz models',2020-03-27T21:01:35Z,2020-03-27T22:06:26Z,model card,,
3491,b'cased -> uncased for example cmdline consistency',2020-03-27T19:39:26Z,2020-04-10T16:34:05Z,,,
3490,b'which iterator to use for different hugging face transformer models for solving multiple choice questions?',2020-03-27T19:13:48Z,2020-06-07T20:28:36Z,"Help wanted, Discussion, wontfix",,
3489,b'missing import in BartForConditionalGeneration example',2020-03-27T18:50:47Z,2020-06-07T20:28:35Z,"Need more information, wontfix",,
3488,b'[bart-tiny-random] Put a 5MB model on S3 to allow faster examples test',2020-03-27T17:21:33Z,2020-03-30T16:28:27Z,,,
3487,b'Create model card',2020-03-27T16:33:18Z,2020-03-31T11:59:23Z,model card,,
3486,"b""setup.py succeeds, then can't import transformers""",2020-03-27T16:33:11Z,2020-03-27T16:50:41Z,,ModuleNotFoundError,"ModuleNotFoundError: No module named 'transformers'"
3485,b'Fix circle ci flaky fail of wmt example ',2020-03-27T15:25:43Z,2020-03-27T17:01:29Z,,,
3484,b'Sphinx build for documentation fails when tensorflow is installed',2020-03-27T15:18:53Z,2020-03-27T15:35:04Z,,,
3483,b'Tests for more examples',2020-03-27T15:03:41Z,2020-06-06T08:14:40Z,wontfix,,
3482,b'Correct output shape for Bert NSP models in docs',2020-03-27T14:53:24Z,2020-04-01T19:04:39Z,,,
3481,b'Do I need to pad non-fixed examples or does run_language_modeling.py already takes care of that? ',2020-03-27T14:51:17Z,2020-06-05T16:11:12Z,wontfix,,
3480,b'Add option to choose T5 model size.',2020-03-27T14:33:15Z,2020-03-27T14:57:00Z,,,
3479,b'Add Colab with the evaluation procedure',2020-03-27T13:57:31Z,2020-07-15T03:25:50Z,wontfix,,
3478,b'add summarization and translation to notebook',2020-03-27T12:06:47Z,2020-03-27T15:05:38Z,,,
3477,b'Added CovidBERT-NLI model card',2020-03-27T11:44:09Z,2020-03-31T11:59:50Z,model card,,
3476,"b""Finetuning FlauBERT with hugging face's Transformers : WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 2, distributed training: False, 16-bits training: True""",2020-03-27T11:01:28Z,2020-04-01T19:00:30Z,,KeyError,"KeyError: 'flaubert'"
3475,b'[WIP] General docs polish',2020-03-27T10:48:06Z,2020-06-27T14:11:43Z,,,
3474,b'[examples] fine-tuning `bert-base-finnish-(un)cased-v1` model for Named Entity Recognition',2020-03-27T09:21:15Z,2020-07-15T03:25:50Z,wontfix,,
3473,b'Inversion of a mask in newer pytorch versions',2020-03-27T06:10:20Z,2020-06-09T19:18:01Z,wontfix,RuntimeError,"RuntimeError: Subtraction, the `-` operator, with a bool tensor is not supported. If you are trying to invert a mask, use the `~` or `logical_not()` operator instead."
3472,b'Optimize tokenization (85-92% time reduction)',2020-03-27T06:02:44Z,2020-07-15T03:25:52Z,wontfix,,
3471,b'TFAlbertForMaskedLM Decoding Error',2020-03-27T05:25:06Z,2020-03-29T15:49:05Z,,,
3470,b'Update README.md',2020-03-27T02:58:17Z,2020-03-31T12:01:10Z,model card,,
3469,b'CircleCI ExamplesTests::test_run_squad failing',2020-03-27T01:47:52Z,2020-03-31T15:54:14Z,,,
3468,b'Issue in generating samples for text generation',2020-03-27T01:12:00Z,2020-03-29T15:41:29Z,,,
3467,b'Model Cards: Fix grammar error',2020-03-26T23:27:48Z,2020-03-27T01:33:34Z,,,
3466,b'Docstring cannot be build anymore',2020-03-26T23:05:42Z,2020-03-27T15:23:17Z,,,
3465,b'Add link to 16 POS tags model',2020-03-26T22:58:49Z,2020-03-31T12:00:00Z,model card,,
3464,b'Add text shown in example of usage',2020-03-26T22:49:06Z,2020-03-31T11:59:37Z,model card,,
3463,b'Question Answering pipeline not working',2020-03-26T21:29:10Z,2020-07-13T20:00:31Z,wontfix,KeyError,"KeyError: 'token_type_ids'"
3462,b'SyntaxError when fine-tuning ALBERT on NER',2020-03-26T18:00:46Z,2020-06-09T20:21:44Z,"wontfix, Ex: Named Entity Recognition, Should Fix","SyntaxError, TypeError","SyntaxError: invalid syntaxTypeError: 'NoneType' object is not callable"
3461,b'Add T5 to docs',2020-03-26T17:44:25Z,2020-03-27T14:57:16Z,,,
3460,"b""Error : forward() got an unexpected keyword argument 'inputs_embeds'""",2020-03-26T17:36:46Z,2020-03-29T15:36:43Z,,TypeError,"TypeError: forward() got an unexpected keyword argument 'inputs_embeds'"
3459,b'[Docs] Add better explanation to check `docs` locally.',2020-03-26T17:34:23Z,2020-03-31T13:30:17Z,,,
3458,"b'WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 2, distributed training: False, 16-bits training: True'",2020-03-26T16:52:35Z,2020-03-27T10:59:43Z,,KeyError,"KeyError: 'flaubert'"
3457,"b""ImportError: cannot import name 'TF_MODEL_FOR_TOKEN_CLASSIFICATION_MAPPING'""",2020-03-26T15:58:19Z,2020-03-30T08:50:04Z,,,
3456,b'Fine-tuning with BertForSequenceClassification on custom dataset yields a model that outputs only the label with highest support in training set',2020-03-26T14:31:24Z,2020-06-12T07:22:11Z,"Help wanted, Discussion, wontfix",,
3455,b'Tokenizers: Start cleaning examples a little',2020-03-26T14:10:22Z,2020-04-01T11:13:41Z,,,
3454,b'NER pipeline usage examples',2020-03-26T13:55:45Z,2020-04-01T18:40:24Z,,,
3453,b'Create card for the model: GPT-2-finetuned-covid-bio-medrxiv',2020-03-26T13:43:12Z,2020-03-31T12:01:04Z,model card,,
3452,b'Write With Transformer returning a 502 on gpt2/xl model',2020-03-26T13:34:16Z,2020-03-28T15:29:50Z,,,
3451,b'[examples] SummarizationDataset cleanup',2020-03-26T13:28:41Z,2020-04-07T23:05:58Z,,,
3450,b'Create card for model GPT-2-finetuned-CORD19',2020-03-26T13:07:20Z,2020-03-26T13:10:10Z,model card,,
3449,b'revert unpin isort commit',2020-03-26T11:53:02Z,2020-03-26T17:19:19Z,,,
3448,b'Failure to load checkpoints saved during distributed training',2020-03-26T11:29:28Z,2020-06-02T09:14:58Z,"wontfix, Ex: LM (Finetuning), Ex: LM (Pretraining), Core: Modeling",RuntimeError,"RuntimeError: storage has wrong size: expected 4434893008627221919 got 2359296"
3447,b'Save models after each epoch',2020-03-26T10:54:44Z,2020-09-26T10:44:31Z,"wontfix, Examples",,
3446,b'Special tokens to pre-trained BART model',2020-03-26T10:53:25Z,2020-03-30T15:37:28Z,Core: Tokenization,,
3445,b'run_lm_finetuning on multiple training files',2020-03-26T09:35:33Z,2020-03-26T13:08:26Z,,,
3444,b'Import error in example script `run_language_modeling.py`',2020-03-26T06:12:34Z,2020-03-26T12:10:38Z,,ImportError,"ImportError: cannot import name 'MODEL_WITH_LM_HEAD_MAPPING'"
3443,b'`run_language_modeling` fails with community model (BioClinicalBERT)',2020-03-26T03:01:54Z,2020-06-04T15:42:40Z,"wontfix, Ex: LM (Finetuning), Core: Modeling, Should Fix",RuntimeError,"RuntimeError: cublas runtime error : the GPU program failed to execute at /opt/conda/conda-bld/pytorch_1579022034529/work/aten/src/THC/THCBlas.cu:368"
3442,"b""can't import TFBertModel from transformers""",2020-03-26T01:44:00Z,2020-04-01T18:24:15Z,,ImportError,"ImportError: cannot import name 'TFBertModel' from 'transformers' (/home/cally/.local/lib/python3.7/site-packages/transformers/__init__.py)"
3441,b'Add support for the null answer in `QuestionAnsweringPipeline`',2020-03-26T01:12:03Z,2020-04-17T15:17:22Z,,,
3440,"b""feat: config what's trainable in Bert layers""",2020-03-26T00:36:18Z,2020-07-15T03:25:51Z,wontfix,,
3439,b'Force the return of token type IDs',2020-03-25T21:45:04Z,2020-03-26T08:41:37Z,,,
3438,b'Same probability from fine-tuning custom pre-trained LM',2020-03-25T21:20:58Z,2021-04-26T15:03:03Z,,,
3437,b'[Bug fix] Using loaded checkpoint with --do_predict (instead of random init)',2020-03-25T20:23:47Z,2020-03-30T21:06:09Z,,,
3436,b'TFXLMRoberta impossible to load base and large model with pretrained weight ?',2020-03-25T20:05:42Z,2020-06-20T18:22:13Z,wontfix,,
3435,b'Updated/added model cards',2020-03-25T19:16:16Z,2020-03-25T20:40:04Z,model card,,
3434,b'How to detokenize a BertTokenizer output? ',2020-03-25T14:56:02Z,2020-06-09T11:17:59Z,"Discussion, wontfix, Core: Tokenization",,
3433,b'Extend config with task specific configs.',2020-03-25T14:41:47Z,2020-03-25T20:32:05Z,,,
3432,b'how to use transformers to get all pretraining model names in transformers hub',2020-03-25T13:46:16Z,2020-03-25T17:15:58Z,,,
3431,"b""Error ImportError: cannot import name 'MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING' from 'transformers' (C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python37_64\\lib\\site-packages\\transformers\\__init__.py)""",2020-03-25T12:10:52Z,2020-05-31T12:51:27Z,wontfix,ImportError,"ImportError: cannot import name 'MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING' from 'transformers' (C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\site-packages\transformers\__init__.py)"
3430,b'Problem saving and/or loading fine-tuned model',2020-03-25T11:19:42Z,2020-06-03T13:08:18Z,wontfix,,
3429,b'Confusion in understanding the output of BERTforTokenClassification class from Transformers library',2020-03-25T10:58:34Z,2020-03-25T15:54:10Z,,,
3428,b'Add wmt translation example',2020-03-25T10:47:35Z,2020-03-26T18:08:00Z,,,
3427,b'I want to create a tokenizer which the vocab file in my computer',2020-03-25T10:32:48Z,2020-03-25T12:11:47Z,,OSError,"OSError: Couldn't reach server at '/home/cally/Awake/Code/bert/pre_model/vocab.txt' to download configuration file or configuration file is not a valid JSON file. Please check network or file content here: /home/cally/Awake/Code/bert/pre_model/vocab.txt."
3426,"b""Expected object of device type cuda but got device type cpu for argument #1 'self' in call to _th_index_selec""",2020-03-25T05:19:36Z,2020-06-07T19:28:38Z,wontfix,,
3425,b'Update model card huseinzol05/bert-base-bahasa-cased',2020-03-25T04:21:07Z,2020-03-26T11:50:28Z,model card,,
3424,b'Where is the code of Bart fine-tuning?Thanks',2020-03-25T01:54:34Z,2020-04-16T15:03:10Z,,,
3423,b'Experiment w/ dataclasses (including Py36)',2020-03-25T00:14:53Z,2020-03-25T15:10:21Z,,,
3422,b'[BART] add bart-large-xsum weights',2020-03-24T22:32:45Z,2020-03-29T14:51:13Z,,,
3421,b'Added BioBERT-NLI model card',2020-03-24T22:22:01Z,2020-03-25T01:15:56Z,model card,,
3420,b'Reading files takes for ever in language modeling',2020-03-24T21:04:01Z,2020-07-18T06:22:26Z,wontfix,,
3419,b'Adds translation pipeline',2020-03-24T20:44:25Z,2020-03-26T12:50:59Z,,,
3418,b'Unused function in squad metrics',2020-03-24T19:56:42Z,2020-05-30T21:48:05Z,wontfix,,
3417,b'Fix XLNet batch generation bug',2020-03-24T19:41:54Z,2020-06-03T11:24:16Z,,,
3416,b'XLNet model on S3 not set up correctly? ',2020-03-24T19:28:52Z,2020-03-24T19:47:32Z,,,
3415,b'Problem with running Transformer Notebook: How to train a language model',2020-03-24T17:08:19Z,2020-03-24T21:59:45Z,,ImportError,"ImportError: cannot import name 'CONFIG_MAPPING'"
3414,b'Add custom rules for sampling from GPT-2 Generator',2020-03-24T15:41:24Z,2020-03-24T15:59:06Z,,,
3413,"b""Add t5 to pipeline(task='summarization')""",2020-03-24T14:26:20Z,2020-03-26T10:03:14Z,,,
3412,"b"" cannot import name 'MODEL_FOR_TOKEN_CLASSIFICATION_MAPPING'""",2020-03-24T14:07:12Z,2020-03-24T14:44:31Z,,,
3411,b'Add t5 summarization example',2020-03-24T11:53:46Z,2020-03-26T17:17:56Z,,,
3410,b'Added precisions in SciBERT-NLI model card',2020-03-24T11:46:42Z,2020-03-24T15:01:57Z,model card,,
3409,b'Add right model and tokenizer path in example',2020-03-24T11:03:31Z,2020-03-24T15:30:13Z,model card,,
3408,b'[model_cards] \xf0\x9f\x87\xb9\xf0\x9f\x87\xb7 Add new BERTurk models',2020-03-24T10:22:58Z,2020-03-24T15:29:07Z,model card,,
3407,b'AdamW in HuggingFace is different from AdamW in Pytorch',2020-03-24T08:32:20Z,2020-05-31T05:52:20Z,wontfix,,
3406,b'Model cards for CS224n SQuAD2.0 models',2020-03-24T07:41:35Z,2020-03-24T15:28:34Z,model card,,
3405,b'Glue test processors and predictions',2020-03-24T06:13:30Z,2020-05-21T13:18:58Z,,,
3404,b'[Bart]example---BartForConditionalGeneration',2020-03-24T03:04:38Z,2020-03-24T05:08:17Z,,**TypeError,"**TypeError: generate() got an unexpected keyword argument 'attention_mask'**"
3403,b'[examples] Use AutoModels in more examples',2020-03-23T23:47:37Z,2020-03-24T00:11:16Z,,,
3402,b'[WIP] seq2seq example',2020-03-23T22:24:08Z,2020-06-03T13:07:48Z,,,
3401,b'added_tokens.json is used for splitting texts',2020-03-23T20:47:55Z,2020-06-02T09:15:00Z,"wontfix, Core: Tokenization, Should Fix",,
3400,b'[Bart: example] drop columns that are exclusively pad_token_id from input_ids',2020-03-23T20:10:45Z,2020-03-26T23:33:55Z,,,
3399,b'Trying to train a GPT2 from scratch',2020-03-23T15:52:26Z,2020-03-24T16:42:07Z,,,
3398,b'[Bart] Fix: put dummy_inputs on correct device',2020-03-23T14:35:28Z,2020-03-26T22:42:10Z,,,
3397,b'Supported language information by model',2020-03-23T14:15:19Z,2020-06-09T13:17:59Z,wontfix,,
3396,b'Cannnot Import from transformers',2020-03-23T14:04:05Z,2020-05-30T05:48:08Z,"wontfix, Installation",,
3395,b'\xf0\x9f\x9a\x80 Feature request Multimodal BERT Models',2020-03-23T12:40:32Z,2020-06-20T20:22:16Z,wontfix,,
3394,b'Add comparison table with new models',2020-03-23T11:32:41Z,2020-03-23T16:10:24Z,model card,,
3393,b'Create README.md',2020-03-23T11:30:50Z,2020-03-31T12:00:36Z,model card,,
3392,b'Add comparison table with older brother in family',2020-03-23T11:27:53Z,2020-03-23T16:11:21Z,model card,,
3391,b'Create card for the model',2020-03-23T11:18:26Z,2020-03-23T16:10:42Z,model card,,
3390,b'adding --fp16 to run_language_modeling and increase batch size but cuda out of memory error',2020-03-23T11:00:33Z,2020-04-17T16:14:25Z,,,
3389,b'\xf0\x9f\x90\x9bBugs in run_tf_ner.py',2020-03-23T10:50:16Z,2020-03-26T15:18:22Z,,,
3388,b'Lazy text dataset loading for language modelling with PyTorch',2020-03-23T10:40:11Z,2020-05-20T12:11:16Z,,,
3387,b'Finetuning of T5 on SQuAD 1.1 including code examples',2020-03-23T09:48:15Z,2020-03-30T11:39:38Z,,,
3386,"b""Model conversion from PyTorch to TF2 doesn't work properly for ALBERT""",2020-03-23T07:22:37Z,2020-06-12T03:54:28Z,"wontfix, Core: Modeling",AssertionError,"AssertionError: Error, model absolute difference is >2e-2: 17.709423065185547"
3385,b'minor website fix',2020-03-23T01:25:12Z,2020-06-02T09:14:59Z,"wontfix, Should Fix",,
3384,b'gpt2 - convert examples to features(tensorflow 2)',2020-03-23T00:54:18Z,2020-03-29T14:07:03Z,,KeyError,"KeyError: None"
3383,b'Clean Encoder-Decoder models with Bart/T5-like API and add generate possibility',2020-03-22T23:58:59Z,2020-04-28T13:11:10Z,,,
3382,"b'When I used the add_special_tokens function in the BertTokenizer, it assigns 2 different tokens with the same ID. Is this done on purpose?'",2020-03-22T23:36:43Z,2020-06-07T19:28:37Z,"Need more information, wontfix",,
3381,b'[BART] test_dummy_inputs fails on GPU',2020-03-22T16:48:22Z,2020-03-26T22:42:10Z,,RuntimeError,"RuntimeError: Expected object of device type cuda but got device type cpu for argument #3 'index' in call to _th_index_select"
3380,"b""Can't save DistilBert model.""",2020-03-22T15:42:06Z,2020-06-05T04:11:09Z,wontfix,,
3379,b'Data Processor should not include in the package',2020-03-22T15:40:36Z,2020-05-29T03:27:21Z,wontfix,,
3378,b'test_resize_tokens_embeddings does not inspect `get_output_embeddings`',2020-03-22T14:56:23Z,2020-06-23T00:25:24Z,"Help wanted, wontfix",,
3377,"b""RobertaTokenizer doesn't have 'batch_encode_plus'""",2020-03-22T14:15:09Z,2020-05-31T00:48:06Z,"wontfix, Version mismatch",AttributeError,"AttributeError: 'RobertaTokenizer' object has no attribute 'batch_encode_plus'"
3376,b'Added scibert-nli model card',2020-03-22T13:26:17Z,2020-03-23T15:55:42Z,model card,,
3375,b'Add camembert integration tests',2020-03-22T12:39:00Z,2020-03-24T09:18:38Z,,,
3374,b'closed',2020-03-22T04:27:40Z,2020-03-22T08:30:42Z,,,
3373,b'Add example code for CRF heads ',2020-03-22T04:23:53Z,2020-05-29T03:27:22Z,wontfix,,
3372,b'BERT pretrained checkpoints',2020-03-22T04:12:28Z,2020-03-23T18:50:17Z,,,
3371,"b'[Bart/Memory] Two separate, smaller decoder attention masks'",2020-03-22T02:47:04Z,2020-03-27T01:34:15Z,,,
3370,b'[Seq2Seq Generation] Call encoder before expanding input_ids',2020-03-21T20:45:10Z,2020-03-26T22:41:20Z,,,
3369,b'[Bart/Memory] SelfAttention only returns weights if config.output_attentions',2020-03-21T19:53:38Z,2020-03-26T22:42:39Z,,Now,"Now: `SelfAttention` returns (output, None) if `config.output_attentions=False` and the memory can be freed "
3368,b'Why does huggingface bert pooler hack make mixed precission training stable?',2020-03-21T19:33:57Z,2020-05-30T05:48:09Z,wontfix,,
3367,b'[Generate] Add bad words list argument to the generate function',2020-03-21T14:25:13Z,2020-03-31T16:42:32Z,,,
3366,"b""GPT2TokenizerFast does not preserve special tokens' ids after a save and load.""",2020-03-21T00:00:57Z,2020-06-06T14:14:39Z,"wontfix, Core: Tokenization, Should Fix, Fast Tokenizers",,
3365,b'fixes lr_scheduler warning',2020-03-20T21:41:43Z,2020-03-20T22:03:51Z,,,
3364,b'Generate all possible sentences using a fine-tuned GPT-2 model',2020-03-20T21:33:29Z,2020-03-21T09:53:10Z,,,
3363,b'Added total_save_limit feature similar to run_langauge_modeling.py',2020-03-20T20:23:43Z,2020-04-02T23:57:53Z,,,
3362,"b'New model, new model cards'",2020-03-20T16:57:46Z,2020-03-20T22:01:02Z,model card,,
3361,b'TF Camembert not improving over epochs',2020-03-20T15:40:19Z,2020-05-26T19:55:58Z,wontfix,,
3360,b'RuntimeError: CUDA out of memory. Tried to allocate 786.00 MiB (GPU 0; 14.73 GiB total capacity; 13.33 GiB already allocated; 575.88 MiB free; 13.38 GiB reserved in total by PyTorch)',2020-03-20T13:40:24Z,2020-06-03T13:07:07Z,,RuntimeError,"RuntimeError: CUDA out of memory. Tried to allocate 786.00 MiB (GPU 0; 14.73 GiB total capacity; 13.33 GiB already allocated; 575.88 MiB free; 13.38 GiB reserved in total by PyTorch)"
3359,"b""Some community models are broken and can't be downloaded""",2020-03-20T13:11:55Z,2020-03-24T16:33:19Z,,,
3358,b'Downloading mlm-17-1280 community model',2020-03-20T12:46:40Z,2020-03-23T22:13:05Z,,,
3357,b'License information by model',2020-03-20T10:31:05Z,2020-04-28T20:40:23Z,,,
3356,b'Update run_language_modeling.py to handle writes on networked filesystem better',2020-03-20T05:49:14Z,2020-07-09T13:39:38Z,wontfix,,
3355,b'Bug? NaN loss after training for a while using for BERT Encoded sentences.',2020-03-19T21:31:19Z,2020-05-26T09:14:02Z,wontfix,,
3354,b'Export ALBERT main layer in TensorFlow',2020-03-19T17:12:01Z,2020-03-19T17:53:06Z,,,
3353,b'Handle pinned version of isort',2020-03-19T13:30:03Z,2020-03-20T22:00:05Z,,,
3352,b'Add model cards for huseinzol05/bert-base-bahasa-cased',2020-03-19T12:35:13Z,2020-03-19T19:08:20Z,model card,,
3351,b'Reformer',2020-03-19T11:37:06Z,2020-05-07T08:17:02Z,,,
3350,b'Reproducing SQuAD v1.1 with xlnet-base cased?',2020-03-19T11:13:20Z,2020-05-25T11:58:47Z,wontfix,,
3349,b'Create model card for bert-small-finetuned-squadv2',2020-03-19T10:51:12Z,2020-03-19T19:07:57Z,model card,,
3348,b'Create card for BERT-Mini finetuned on SQuAD v2',2020-03-19T10:40:04Z,2020-03-19T19:07:40Z,model card,,
3347,b'Create card for BERT-Tiny fine-tuned on SQuAD v2',2020-03-19T10:20:16Z,2020-03-19T19:07:23Z,model card,,
3346,b'Created card for spanbert-finetuned-squadv1',2020-03-19T09:46:25Z,2020-03-19T19:06:37Z,model card,,
3345,b'Fix input ids can be none attn mask',2020-03-19T08:43:25Z,2020-03-19T08:55:18Z,,,
3344,b'Fix wrong link for the notebook file',2020-03-19T08:10:28Z,2020-03-19T16:22:48Z,,,
3343,b'Update 01-training-tokenizers.ipynb (typo issue)',2020-03-19T07:34:47Z,2020-03-19T22:21:50Z,,,
3342,b'No Module named Transformers',2020-03-19T04:58:41Z,2020-05-29T03:27:23Z,"wontfix, Installation",ModuleNotFoundError,"ModuleNotFoundError: No module named 'transformers'"
3341,b'Simpler Error message when loading config/model with .from_pretrained()',2020-03-19T04:14:03Z,2020-03-19T22:23:03Z,,,
3340,b'Create README.md',2020-03-19T03:08:39Z,2020-04-24T20:13:59Z,model card,,
3339,b'Create README.md',2020-03-19T03:08:31Z,2020-04-24T20:14:04Z,model card,,
3338,b'Create README.md',2020-03-19T03:07:42Z,2020-03-19T03:44:14Z,,,
3337,b'Create README.md',2020-03-19T03:06:50Z,2020-03-19T03:43:51Z,,,
3336,b'Create README.md',2020-03-19T03:05:50Z,2020-03-19T03:45:03Z,,,
3335,b'Fix #3305: run_ner only possible on ModelForTokenClassification models',2020-03-19T02:11:37Z,2020-03-19T20:41:30Z,,,
3334,b'transformers.PreTrainedTokenizer.tokenize does lower case work all the time and discards space and tab. Want this changed.',2020-03-19T02:02:11Z,2020-05-26T09:14:01Z,"wontfix, Core: Tokenization",,
3333,b'Finetuning T5 Model',2020-03-18T23:57:19Z,2020-03-30T11:35:55Z,Usage,,
3332,"b""run_tf_ner.py doesn't work with unlabelled test data""",2020-03-18T20:36:13Z,2020-06-03T08:31:31Z,wontfix,ZeroDivisionError,"ZeroDivisionError: Weights sum to zero, can't be normalized"
3331,b'Add model cards for FinBERT.',2020-03-18T20:00:08Z,2020-03-19T19:06:01Z,model card,,
3330,b'Added model cards for SciBERT models uploaded under AllenAI org',2020-03-18T19:32:53Z,2020-03-18T19:45:11Z,model card,,
3329,b'CUDA Error when running run_language_modeling.py',2020-03-18T16:07:28Z,2020-05-27T12:23:56Z,wontfix,,
3328,b'Create README.md',2020-03-18T14:34:09Z,2020-03-18T15:37:18Z,model card,,
3327,b'improve doctstring for tf and pt generate() method',2020-03-18T12:16:10Z,2020-03-18T12:24:10Z,,,
3326,b'add link to blog post',2020-03-18T11:42:21Z,2020-03-18T12:24:28Z,,,
3325,b'Cubla Error on DistilBert',2020-03-18T10:32:47Z,2020-07-26T05:24:03Z,wontfix,,
3324,"b""Error loading finetuned bert model AttributeError: 'NoneType' object has no attribute 'endswith'""",2020-03-18T08:02:45Z,2020-06-02T09:14:57Z,wontfix,"SyntaxError, AttributeError","SyntaxError: invalid syntaxAttributeError: 'NoneType' object has no attribute 'endswith' "
3323,"b""[Bart/Memory] don't create lm_head""",2020-03-18T06:53:22Z,2020-03-26T22:40:40Z,,,
3322,b'[BART] torch 1.0 compatibility',2020-03-18T05:11:31Z,2020-03-19T15:56:55Z,,,
3321,b'Init card for model',2020-03-18T03:28:54Z,2020-03-18T11:55:28Z,model card,,
3320,b'TF BERT not FP16 compatible?',2020-03-18T03:20:45Z,2020-11-19T09:17:29Z,,tensorflow.python.framework.errors_impl.InvalidArgumentError,"tensorflow.python.framework.errors_impl.InvalidArgumentError: cannot compute AddV2 as input #1(zero-based) was expected to be a half tensor but is a float tensor [Op:AddV2] name: tf_bert_for_question_answering/bert/embeddings/add/"
3319,"b'[BART] cleanup: remove redundant kwargs, improve docstrings'",2020-03-18T02:53:41Z,2020-03-19T15:16:52Z,,,
3318,b'pipelines.ipynb mask should be [MASK]',2020-03-18T02:49:13Z,2020-03-19T09:33:56Z,,,
3317,b'output value of  XLNetModel changes for the same input ',2020-03-17T23:27:32Z,2020-03-19T17:31:22Z,,,
3316,b'TextClassificationPipeline does not work with pretrained BERT model',2020-03-17T21:13:27Z,2020-03-26T14:36:09Z,,ValueError,"ValueError: operands could not be broadcast together with shapes (1,8,768) (1,8) "
3315,b'how does masked_lm_labels  work ?',2020-03-17T21:02:22Z,2020-05-30T11:48:06Z,wontfix,,
3314,b'Mismatch in the accuracy figures',2020-03-17T19:20:38Z,2020-03-17T19:53:34Z,,,
3313,b'KeyError in GLUE data tokenization with RoBERTA',2020-03-17T16:57:58Z,2020-03-26T08:41:36Z,,KeyError,"KeyError: 'token_type_ids'"
3312,"b""GPT2Tokenizer doesn't include BOS or EOS token""",2020-03-17T16:34:27Z,2020-03-17T16:43:58Z,,,
3311,b'GPT2 -- build_inputs_with_special_tokens lacking BOS and EOS tokens.',2020-03-17T16:33:57Z,2020-11-29T11:50:19Z,wontfix,,
3310,b'Add sample softmax possibility to TransfoXL model for TransfoXL training',2020-03-17T15:14:48Z,2020-05-29T03:27:24Z,wontfix,,
3309,b'Create model card for CodeBERTaPy',2020-03-17T14:20:30Z,2020-03-17T16:29:12Z,,,
3308,b'Loading DistilBertModel with AutoModel gives 12 layers  ',2020-03-17T10:37:42Z,2020-05-23T11:23:39Z,wontfix,,
3307,b'Make sacremoses dependency optional due to GPL license.',2020-03-17T10:07:01Z,2020-09-18T23:52:59Z,wontfix,,
3306,b'Create README.md',2020-03-17T08:14:43Z,2020-03-17T13:05:12Z,model card,,
3305,b'Update examples/ner/run_ner.py to use AutoModel',2020-03-17T02:55:05Z,2020-03-17T16:30:11Z,,,
3304,b'Error in loading  albert-base-v2',2020-03-17T02:29:22Z,2020-05-23T20:46:12Z,wontfix,,
3303,b'Error in loading Albert model',2020-03-17T02:21:56Z,2020-05-23T06:23:23Z,wontfix,,
3302,b'[BART] Delete redundant unit test',2020-03-17T01:39:28Z,2020-03-17T03:09:11Z,,,
3301,"b""Add model card for Google AI's BERT Miniatures""",2020-03-16T23:21:18Z,2020-03-17T01:51:46Z,model card,,
3300,"b""ImportError: cannot import name 'BartForConditionalGeneration'""",2020-03-16T19:33:30Z,2020-03-16T19:45:00Z,,,
3299,b'add camembert for Question answering for examples',2020-03-16T18:40:20Z,2020-03-16T18:42:13Z,,,
3298,b'[generate] do_sample default back to False',2020-03-16T12:10:41Z,2020-03-17T14:52:37Z,,,
3297,b'Getting output of any hidden layer ',2020-03-16T10:32:27Z,2020-03-19T15:43:01Z,wontfix,,
3296,b'Installation error: can not find Rust compiler',2020-03-16T10:03:55Z,2020-03-16T16:31:31Z,,,
3295,b'Create CodeBERTaJS model card',2020-03-16T09:15:58Z,2020-03-16T16:23:02Z,model card,,
3294,b'BertForPreTraining should compute only <MASKED> prediction_scores',2020-03-16T08:13:16Z,2020-05-26T09:13:58Z,wontfix,,
3293,b'Create model card for spanbert-finetuned-squadv2',2020-03-16T08:02:08Z,2020-03-16T16:32:47Z,model card,,
3292,b'NER Pipeline returns null',2020-03-16T02:54:03Z,2020-03-26T14:04:45Z,,,
3291,"b""a lot of examples in doc can't run successful""",2020-03-16T02:41:07Z,2020-03-25T10:37:25Z,,,
3290,b'[WIP] Lightning glue example',2020-03-16T00:30:16Z,2020-03-17T15:46:43Z,,,
3289,b'GPT-2 attention_mask reshaping uses input_ids first dimension',2020-03-15T18:31:12Z,2020-03-19T08:55:17Z,,,
3288,b'Dockerhub images huggingface/transformers_cpu for version 2.5.1 has version 2.5.0 installed',2020-03-15T14:44:06Z,2020-03-18T15:48:39Z,,,
3287,b'Unexpected output from feature extraction pipeline',2020-03-15T13:55:11Z,2020-03-16T16:10:43Z,,,
3286,b'Adding LM Head to Transfo-XL and first step to fixing problem with Adaptive Embeddings in TransfoXL',2020-03-15T12:19:54Z,2020-03-18T13:24:28Z,,,
3285,b'Is there a way to evaluate GPT-2 model during fine-tuning process for accuracy and fluency?',2020-03-15T10:28:14Z,2020-03-19T15:38:23Z,,,
3284,b'Return token span from NerPipeline',2020-03-15T10:27:24Z,2020-05-25T20:05:54Z,wontfix,,
3283,"b'What is the most effective way to use BERT , ROBERTA , GPT-2 architectures as frozen feature extractors ?'",2020-03-15T09:06:20Z,2020-06-02T09:15:03Z,"Discussion, wontfix",,
3282,"b'Install error , Win10,anaconda3,python3.5,pytorch'",2020-03-15T08:19:29Z,2020-03-19T04:38:33Z,,,
3281,"b'how to use TFBertModel to load a Bert, which the path is from own computer.'",2020-03-15T05:50:31Z,2020-03-25T10:37:00Z,,,
3280,b'how to finetune with PreTrainedEncoderDecoder',2020-03-15T05:33:30Z,2020-06-02T09:15:01Z,"wontfix, Core: Encoder-Decoder",,
3279,b'[BART] Remove unused kwargs',2020-03-14T23:09:38Z,2020-03-16T03:00:44Z,,,
3278,b'[BART] generation_mode as a kwarg not a class attribute',2020-03-14T22:24:56Z,2020-03-16T16:47:54Z,,,
3277,b'Add missing token classification for XLM',2020-03-14T17:31:27Z,2020-03-26T14:22:13Z,,,
3276,b'Model fail to revert to generation_mode=False after generation',2020-03-14T15:35:35Z,2020-04-16T15:08:04Z,,,
3275,b'Cannot Achieve Reproducability with Tensorflow Transformer Models',2020-03-14T15:23:34Z,2020-07-04T21:36:21Z,wontfix,,
3274,b'Tremendous slowdown in multi-node distributed training',2020-03-14T15:10:06Z,2020-03-20T06:00:08Z,,,
3273,b'add XLMForTokenClassification',2020-03-14T14:58:12Z,2020-03-14T15:45:40Z,,,
3272,"b'how can i distill xlm-roberta model , just like distill roberta model , any suggestion ?thanks a lot  '",2020-03-14T14:36:00Z,2020-05-20T15:43:25Z,wontfix,,
3271,b'Finetuning before feature extraction',2020-03-14T11:46:24Z,2020-05-07T08:50:03Z,,,
3270,b'train model from scratch with big data',2020-03-14T09:50:27Z,2020-05-20T10:43:24Z,wontfix,,
3269,"b'when I install transformers, it appeared this error'",2020-03-14T04:08:15Z,2020-03-25T10:36:50Z,,,
3268,b'add gpt2-xl for tf',2020-03-13T20:09:44Z,2020-03-13T20:40:36Z,,,
3267,b'removing torch.cuda.empty_cache() from TF function',2020-03-13T18:22:04Z,2020-03-19T22:25:31Z,,,
3266,b'[BART] FP16 testing fixes',2020-03-13T16:44:08Z,2020-03-13T23:48:28Z,,,
3265,b'[BART] test_generate_fp16 fails after PR#3140',2020-03-13T15:43:57Z,2020-03-13T23:48:28Z,,,
3264,b'Clean special token init in modeling_....py',2020-03-13T14:28:24Z,2020-03-20T20:41:05Z,,,
3263,b'Create camembert-base-README.md',2020-03-13T12:11:16Z,2020-03-13T13:35:55Z,model card,,
3262,b'TFAlbertMainLayer cannot be imported from the transformers library.',2020-03-13T10:19:35Z,2020-03-19T17:53:06Z,,,
3261,b'Update examples/ner/run_ner.py',2020-03-13T09:35:07Z,2020-03-16T07:48:13Z,,,
3260,"b""Model name 'distilbert-base-german-cased' was not found in model name list.""",2020-03-13T08:45:46Z,2020-03-13T15:36:54Z,,`OSError,"`OSError: Model name 'distilbert-base-german-cased' was not found in model name list. We assumed 'https://s3.amazonaws.com/models.huggingface.co/bert/distilbert-base-german-cased/config.json' was a path, a model identifier, or url to a configuration file named config.json or a directory containing such a file but couldn't find any such file at this path or url.`"
3259,b'Great job advice',2020-03-13T02:25:17Z,2020-03-13T08:46:15Z,,,
3258,b'very slow performance on transformer 2.5.0 versus 2.3.0',2020-03-13T01:39:01Z,2020-06-02T09:15:02Z,wontfix,,
3257,b'ELECTRA',2020-03-12T22:51:13Z,2020-04-03T18:10:55Z,,,
3256,b'Implement Electra',2020-03-12T21:27:03Z,2020-03-13T08:51:57Z,,,
3255,b'add BART to README',2020-03-12T18:30:19Z,2020-03-12T23:38:05Z,,,
3254,b'Bump psutil from 5.6.3 to 5.6.6 in /examples/distillation',2020-03-12T18:16:07Z,2020-03-13T01:14:58Z,dependencies,,
3253,b'bug in run_glue',2020-03-12T16:24:37Z,2020-03-13T13:54:28Z,wontfix,ImportError,ImportError: cannot import name 'glue_compute_metrics'
3252,b'batch_encode_plus cannot work properly',2020-03-12T16:16:47Z,2020-03-12T18:00:34Z,,NameError,"NameError: name 'inp_wsrq1ids' is not defined"
3251,"b""Why is the seq_len dimension hard coded to be the first dimension of BERT's input?""",2020-03-12T16:11:46Z,2020-04-13T02:26:05Z,,,
3250,b'UnicodeDecodeError when loading BART from fairseq checkpoint',2020-03-12T15:41:33Z,2020-05-20T11:57:08Z,,,
3249,b'Using FP16 on BartModel',2020-03-12T15:10:15Z,2020-03-13T23:48:27Z,,,
3248,b'[model_cards] polbert: simplify usage example with pipelines',2020-03-12T14:04:49Z,2020-03-12T14:05:41Z,,,
3247,b'Improved Error message when loading config/model with .from_pretrained()',2020-03-12T13:03:18Z,2020-03-16T08:48:31Z,,,
3246,b'How do you do inference in production?',2020-03-12T10:06:11Z,2020-04-30T02:49:18Z,,,
3245,b'pad error in BertTokenizer.batch_encode_plus ',2020-03-12T08:03:44Z,2020-05-26T09:14:00Z,wontfix,,
3244,b'Get word seperator char for tokenization',2020-03-12T07:42:07Z,2020-07-25T08:38:43Z,"wontfix, High-Level feature, Core: Tokenization",,
3243,b'seems TFBertForSequenceClassification cannot load tf1.x model?',2020-03-12T07:40:53Z,2020-03-19T16:10:04Z,,,
3242,b'Update examples/ner/run_ner.py',2020-03-12T06:31:00Z,2020-03-13T09:27:10Z,,,
3241,b'simplify polbert usage example with pipelines',2020-03-12T04:57:03Z,2020-03-12T14:05:33Z,model card,,
3240,b'Minor Bug Fix for Running Roberta on Glue',2020-03-12T04:39:52Z,2020-03-19T16:08:31Z,,,
3239,b'Minor Bug Fix for Running Roberta on Glue',2020-03-12T04:24:04Z,2020-03-12T04:36:57Z,,,
3238,b'add output_past option to BERT class ',2020-03-12T02:25:53Z,2020-07-15T03:25:53Z,wontfix,,
3237,b'How to encode a batch of sequence?',2020-03-12T02:11:04Z,2020-03-12T17:49:15Z,,RuntimeError,"RuntimeError: Could not infer dtype of dict"
3236,b'[WIP] Add BART for summarization training with CNN/DM using pytorch-lightning',2020-03-12T01:08:11Z,2020-03-25T01:00:25Z,,,
3235,b'Directories not found when saving checkpoints',2020-03-11T22:09:42Z,2020-07-15T03:25:54Z,wontfix,,
3234,b'[model_cards] \xf0\x9f\x87\xb9\xf0\x9f\x87\xb7 Add new (cased) DistilBERTurk model',2020-03-11T21:57:18Z,2020-03-11T22:40:39Z,,,
3233,b'Bart: update example for #3140 compatibility',2020-03-11T21:36:41Z,2020-03-12T14:36:37Z,,,
3232,"b""[TorchHub]Repo's layout is not compatible with TorchHub anymore since 2.0""",2020-03-11T21:09:16Z,2020-05-11T16:09:02Z,wontfix,ModuleNotFoundError,"ModuleNotFoundError: No module named 'transformers'"
3231,b'train dev test split with BERT',2020-03-11T18:15:02Z,2020-05-17T20:17:32Z,wontfix,,
3230,b'Create README.md for bio+discharge summary BERT',2020-03-11T16:02:57Z,2020-03-11T16:37:00Z,model card,,
3229,b'Add Bio+ Clinical BERT model card',2020-03-11T15:57:18Z,2020-03-11T16:36:34Z,model card,,
3228,b'Support T5 Generation',2020-03-11T14:50:02Z,2020-03-19T22:18:24Z,,,
3227,b'An Error report about pipeline',2020-03-11T14:23:50Z,2020-03-26T08:41:37Z,"Core: Pipeline, Version mismatch",KeyError,KeyError: 'token_type_ids'
3226,b'Strange behaviour after using BertTokenizer.add_tokens()',2020-03-11T13:40:18Z,2020-03-19T15:23:41Z,,,
3225,b'Complete merge Seq-2-Seq generation into default generation',2020-03-11T13:38:30Z,2020-03-14T14:08:59Z,,,
3224,b'Problem with PreTrainedTokenizerFast and return_offsets_mapping',2020-03-11T11:04:27Z,2020-06-20T20:22:15Z,"wontfix, Core: Pipeline, Fast Tokenizers",TypeError,"TypeError: expected Tensor as element 0 in argument 0, but got list"
3223,b'torch.distributed.barrier() have NCCL error',2020-03-11T08:46:53Z,2020-03-11T08:53:28Z,,"RuntimeError, subprocess.CalledProcessError","RuntimeError: NCCL error in: ../torch/lib/c10d/ProcessGroupNCCL.cpp:450, unhandled system error, NCCL version 2.5.6subprocess.CalledProcessError: Command '['/opt/conda/bin/python', '-u', 'run_glue.py', '--local_rank=1', '--model_type', 'bert', '--model_name_or_path', 'bert-base-uncased', '--task_name', 'MRPC', '--do_train', '--do_eval', '--do_lower_case', '--data_dir', '.glue_data/MRPC', '--max_seq_length', '128', '--per_gpu_eval_batch_size=8', '--per_gpu_train_batch_size=8', '--learning_rate', '2e-5', '--num_train_epochs', '3.0', '--output_dir', '/tmp/MRPC/']' returned non-zero exit status 1."
3222,b'Why the pre-trained models will be downloaded each times?',2020-03-11T08:36:27Z,2020-03-25T01:48:42Z,,,
3221,b'Model card for dkleczek/bert-base-polish-uncased-v1',2020-03-11T07:26:58Z,2020-03-11T13:12:48Z,model card,,
3220,b'How to tokenize word to characeter',2020-03-11T06:44:37Z,2020-05-26T09:13:59Z,"wontfix, Core: Tokenization",,
3219,b'Typo in warning message',2020-03-11T00:34:32Z,2020-03-19T13:49:26Z,,,
3218,b'Create README.md',2020-03-11T00:03:22Z,2020-03-11T13:10:25Z,model card,,
3217,b'Create README.md',2020-03-10T23:38:43Z,2020-03-11T00:00:15Z,,,
3216,b'Create README.md',2020-03-10T23:38:36Z,2020-03-11T13:10:58Z,model card,,
3215,b'Create README.md',2020-03-10T23:37:11Z,2020-03-11T13:10:14Z,model card,,
3214,b'Create README.md',2020-03-10T23:33:21Z,2020-03-11T13:03:32Z,model card,,
3213,b'fix typo in docstring demonstrating invocation of PreTrainedEncoderDecoder.from_pretrained',2020-03-10T22:41:05Z,2020-03-19T13:47:55Z,,,
3212,b'Update README.md',2020-03-10T21:59:35Z,2020-03-11T13:03:22Z,model card,,
3211,b'Update README.md',2020-03-10T21:56:27Z,2020-03-11T13:03:08Z,model card,,
3210,b'Update README.md',2020-03-10T21:54:34Z,2020-03-11T13:02:57Z,model card,,
3209,b'Update README.md',2020-03-10T21:52:02Z,2020-03-11T13:02:20Z,model card,,
3208,b'Error loading pretrained bert-base-multilingual-cased',2020-03-10T20:02:53Z,2020-03-12T15:10:08Z,,RuntimeError,"RuntimeError: Error(s) in loading state_dict for BertModel:"
3207,b'Pipeline for Question Answering: How to return multiple correct answers output?',2020-03-10T17:27:13Z,2020-05-19T01:06:26Z,"wontfix, Core: Pipeline, High-Level feature, Ex: Question Answering",,
3206,b'More details about DistilBERT experiment setting.',2020-03-10T17:13:46Z,2020-03-23T17:06:41Z,,,
3205,b'where is the position emdeddings in bert for training a new model from scratch ?',2020-03-10T13:35:16Z,2020-05-16T17:44:04Z,wontfix,,
3204,"b""UnboundLocalError: local variable 'tokenizer' referenced before assignment""",2020-03-10T12:25:23Z,2020-05-25T15:58:49Z,wontfix,`UnboundLocalError,"`UnboundLocalError: local variable 'tokenizer' referenced before assignmen"
3203,b'Attention mask always returns array of ones for CamembertTokenizer.batch_encode_plus',2020-03-10T10:54:02Z,2020-03-11T09:03:21Z,Core: Tokenization,,
3202,b'Update README.md',2020-03-10T04:11:01Z,2020-03-10T14:59:35Z,model card,,
3201,b'Update README.md',2020-03-10T04:06:28Z,2020-03-10T14:59:18Z,model card,,
3200,"b""TF GPT2 Language model can't be created with from_pretrained() for specific shortcut name""",2020-03-10T01:35:11Z,2020-03-13T20:40:36Z,Core: Modeling,TypeError,"TypeError: stat: path should be string, bytes, os.PathLike or integer, not NoneType"
3199,b'Model card for albert-base-v2-squad2',2020-03-09T23:28:16Z,2020-03-09T23:37:16Z,model card,,
3198,b'XLM-R Tokenizer now passes common tests + Integration tests',2020-03-09T22:43:53Z,2020-03-18T13:52:51Z,,,
3197,b'[model upload] Support for organizations',2020-03-09T21:30:24Z,2020-03-09T21:33:58Z,,,
3196,b'Create README.md',2020-03-09T20:22:56Z,2020-03-10T14:58:15Z,model card,,
3195,"b'Error reported when fine tuning on my dataset using \'\'run_language_modeling.py""'",2020-03-09T19:55:22Z,2020-05-16T12:44:04Z,"wontfix, Ex: LM (Finetuning)",,
3194,b'[fix] Bart CNN Example: model.to(device)',2020-03-09T18:19:05Z,2020-03-09T19:09:36Z,,,
3193,b'Where is the default download address for pre-trained weight',2020-03-09T17:35:47Z,2020-03-09T17:52:49Z,,,
3192,b'Provide comprehensive guide & best-practices for run_language_modeling.py',2020-03-09T17:29:51Z,2020-08-08T21:35:56Z,"wontfix, Ex: LM (Finetuning), Ex: LM (Pretraining)",,
3191,b'Add integration tests lm generate torch tf',2020-03-09T14:03:25Z,2020-03-10T10:29:18Z,,,
3190,b'fix repetition penalty mask in tf',2020-03-09T14:01:14Z,2020-03-09T15:29:58Z,,,
3189,b'I want to import the model path on my owm computer?',2020-03-09T13:58:01Z,2020-03-09T15:17:07Z,,,
3188,b'Beam search sometimes fails this assert error',2020-03-09T12:50:21Z,2020-05-18T11:17:35Z,wontfix,AssertionError,"AssertionError: [(tensor(-19.8421), tensor(26), tensor(0)), (tensor(-20.9710), tensor(26), tensor(0)), (tensor(-30.5064), tensor(5), tensor(0)), (0, 0, 0), (0, 0, 0), (0, 0, 0), (0, 0, 0), (0, 0, 0), (0, 0, 0), (tensor(-17.4236), tensor(11), tensor(9)), (tensor(-26.3645), tensor(16), tensor(9)), (tensor(-23.9410), tensor(16), tensor(9)), (0, 0, 0), (0, 0, 0), (0, 0, 0), (tensor(-58.0648), tensor(0), tensor(15))], 3, 5"
3187,b'Knowing the specific data set used for DistilBertForQuestionAnswering',2020-03-09T12:37:31Z,2020-05-15T22:12:27Z,"wontfix, Ex: Question Answering, Distillation",,
3186,b'CPU/GPU memory benchmarking utilities - Remove support for python 3.5 (now only 3.6+)',2020-03-09T12:06:04Z,2020-03-17T14:17:12Z,,,
3185,b'Tokenizers v3.0.0',2020-03-09T10:34:57Z,2020-04-06T22:29:16Z,,,
3184,b'`Failed to build tokenizers` when installing 2.5.1 version.',2020-03-09T08:52:34Z,2020-03-09T13:55:56Z,,,
3183,b'About the examples document of bert with SQuAD 2.0',2020-03-09T08:52:02Z,2020-03-09T14:54:22Z,,,
3182,b'How can i use pipeline with pretrained automodel ?',2020-03-09T07:55:42Z,2020-03-09T14:18:31Z,,,
3181,b'The implementation of GPT2 masked attention mechanism will cause errors when the model was trained after some iterations.',2020-03-09T03:22:27Z,2020-05-16T12:44:05Z,"wontfix, Core: Modeling",,
3180,b'NER - pl example',2020-03-08T23:37:09Z,2020-03-10T00:43:39Z,,,
3179,b'I can not import transformers',2020-03-08T15:43:08Z,2020-03-08T17:36:10Z,,,
3178,b'Pretraining QA corpora from scratch with sentence pairs',2020-03-08T12:19:56Z,2020-05-14T13:49:11Z,wontfix,,
3177,b'Distilgpt2 finetuning and text generation',2020-03-08T11:29:48Z,2020-06-03T13:00:36Z,,,
3176,b'GLUE test set predictions',2020-03-08T09:45:03Z,2020-05-21T13:17:45Z,,,
3175,b'Updated `Tokenw   ise` in print statement to `Token wise`',2020-03-08T09:41:42Z,2020-03-08T14:55:31Z,,,
3174,b'How can I assign a specific gpu when using examples/run_language_modeling.py?',2020-03-08T06:08:46Z,2020-05-14T22:18:52Z,wontfix,,
3173,b'Get the CNN/Daily Mail Data for BART',2020-03-07T20:10:12Z,2020-05-14T03:57:29Z,wontfix,,
3172,b'Quick tour TF 2.0 ',2020-03-07T15:19:43Z,2020-03-07T15:39:55Z,,NameError,"NameError: name 'TFBertForSequenceClassification' is not defined"
3171,b'Do we have a whole-word-masked version of BERT?',2020-03-07T14:38:05Z,2020-03-07T15:44:10Z,,,
3170,b'Semantic Code Retrieval using Transformers',2020-03-07T13:00:42Z,2020-05-22T05:57:54Z,wontfix,,
3169,b'issues while modifying modeling_roberta.py file',2020-03-07T05:20:36Z,2020-03-08T14:09:51Z,,,
3168,b'Can we use GPT-2 sentence embedding for classification tasks?',2020-03-07T03:25:17Z,2020-08-16T19:33:44Z,"Discussion, wontfix",,
3167,b'padding and attention mask does not work as intended in batch input in GPT2 language model ',2020-03-07T01:35:20Z,2020-03-08T20:21:16Z,,,
3166,b'[BERT] Implementation of the sliding window for long sequences',2020-03-07T01:19:03Z,2020-05-13T03:17:51Z,wontfix,,
3165,b'Reason for speedup',2020-03-07T00:13:36Z,2020-05-16T16:44:04Z,wontfix,,
3164,b'wrong configuration of ALBERT xlarge',2020-03-06T20:37:06Z,2020-05-13T16:17:52Z,wontfix,,
3163,b'Inference is slow with ',2020-03-06T18:18:02Z,2020-05-13T03:17:59Z,wontfix,,
3162,b'Does this project have this function ?',2020-03-06T17:33:11Z,2020-06-13T00:22:11Z,wontfix,,
3161,b'urgent - ROBERTA on WSC',2020-03-06T16:51:22Z,2020-05-13T03:17:54Z,wontfix,,
3160,b'[Bart] add imports to examples',2020-03-06T16:12:53Z,2020-03-06T16:15:34Z,,,
3159,b'NER: some issues in PyTorch Lightning example',2020-03-06T15:37:51Z,2020-05-15T04:12:27Z,wontfix,,
3158,b'[Bart] _prepare_decoder_inputs should use large negative',2020-03-06T15:36:16Z,2020-03-06T21:06:37Z,,,
3157,b'[model_cards]Add albert chinese model',2020-03-06T14:26:49Z,2020-03-06T22:23:32Z,model card,,
3156,b'Partially fix space only input without special tokens added int the output ',2020-03-06T10:30:58Z,2020-03-12T09:02:18Z,Core: Tokenization,,
3155,b'i want to browse and store image in data base in tkinter can u give me suggessions?',2020-03-06T08:39:32Z,2020-03-06T08:40:59Z,,,
3154,b'seed parameter for model generate()',2020-03-06T06:10:13Z,2020-03-06T10:30:27Z,,,
3153,b'Fresh macOS install errors out on import',2020-03-06T02:43:44Z,2020-05-12T09:41:08Z,wontfix,RuntimeError,"RuntimeError: dictionary changed size during iteration"
3152,b'BART.generate: possible to reduce time/memory?',2020-03-06T02:20:13Z,2020-06-06T16:14:39Z,wontfix,,
3151,b'How to train a distilled gpt2',2020-03-06T01:40:27Z,2020-03-12T16:38:32Z,,,
3150,b'Padding changes model outputs (even with attention_mask)',2020-03-05T23:47:03Z,2020-05-15T22:12:28Z,"wontfix, Core: Modeling",,
3149,b'fix missed BartForMaskedLM renaming',2020-03-05T23:36:17Z,2020-03-05T23:45:08Z,,,
3148,b'refactored beam search according to torch implementation',2020-03-05T23:00:23Z,2020-03-06T21:01:47Z,,,
3147,b'Pass kwargs to configuration',2020-03-05T21:15:10Z,2020-03-05T22:16:58Z,,,
3146,b'Create README.md for mrm8488/bert-multi-uncased-finetuned-xquadv1',2020-03-05T18:36:04Z,2020-03-06T22:20:21Z,model card,,
3145,b'[Bart] FP16 Support',2020-03-05T17:58:33Z,2020-03-05T21:14:36Z,,,
3144,b'[Question]: Why does model.__call__ return the loss too?',2020-03-05T17:50:09Z,2020-05-12T00:34:07Z,"Discussion, wontfix, Core: Modeling",,
3143,b'Correct missing keys',2020-03-05T16:58:33Z,2020-03-05T22:01:55Z,,,
3142,b'Missing `missing_keys` when loading from saved base model checkpoint',2020-03-05T15:58:05Z,2020-03-05T22:01:55Z,Core: Modeling,,
3141,b'GPU memory getting out of bound ',2020-03-05T15:27:23Z,2020-05-13T03:17:57Z,"wontfix, Core: Modeling",,
3140,b'Merge bart generate into default generate',2020-03-05T15:16:25Z,2020-03-11T12:21:54Z,,,
3139,b'Remove excess line breaks in DeepPavlov model cards',2020-03-05T14:59:45Z,2020-03-06T22:19:37Z,model card,,
3138,b'Add model cards for DeepPavlov models',2020-03-05T14:19:52Z,2020-03-05T14:34:44Z,model card,,
3137,b'Refactor BartModel so that input checks are handled within enc/dec',2020-03-05T13:50:02Z,2020-03-06T12:06:35Z,,,
3136,"b""links of the model's pretrained weights""",2020-03-05T13:16:14Z,2020-03-09T12:05:55Z,,,
3135,b'Refactoring and bug fixing beam search generate',2020-03-05T12:13:36Z,2020-03-05T21:12:57Z,,,
3134,b'Cant import my pretrained bert model from NVIDIA/DeepLearningExamples/',2020-03-05T11:52:25Z,2020-05-17T05:44:04Z,"wontfix, Migration",,
3133,b'BART: move boilerplate code inside encoder/decoder',2020-03-05T11:45:20Z,2020-03-06T12:06:35Z,,,
3132,b'[hf_api] Get the public list of all the models on huggingface',2020-03-05T04:52:32Z,2020-03-06T12:05:53Z,,,
3131,"b""Converting tf weights:  AttributeError: 'GPT2Model' object has no attribute 'zeLoss'""",2020-03-05T02:29:25Z,2020-06-26T20:47:28Z,"wontfix, Migration",AttributeError,"AttributeError: 'GPT2Model' object has no attribute 'zeLoss'"
3130,b'Performance Issue about pretrained bert migration from tensorflow to pytorch',2020-03-05T01:50:35Z,2020-05-15T22:12:29Z,"wontfix, Migration",,
3129,b'Load pretrained roberta model from fairseq?',2020-03-05T01:19:07Z,2020-03-06T20:26:38Z,,,
3128,b'Add Summarization to Pipelines',2020-03-04T23:21:23Z,2020-03-17T22:04:22Z,,,
3127,b'Create README.md',2020-03-04T17:13:01Z,2020-03-04T18:57:24Z,model card,,
3126,"b""BartTokenizer and 'bart-large-cnn' out of sync""",2020-03-04T16:46:26Z,2020-05-10T19:37:09Z,"wontfix, seq2seq",,
3125,b'NER tutorial: run_tf_ner.py reports an entity number not matching the one in the .txt files',2020-03-04T16:40:19Z,2020-05-18T11:17:36Z,"wontfix, TensorFlow, Ex: Named Entity Recognition",,
3124,b'[Broken Proposal] CircleCI runs tests with torch=1.0.0',2020-03-04T16:37:23Z,2020-05-15T17:12:23Z,wontfix,,
3123,b'fix sklearn release circle ci [temporary]',2020-03-04T16:09:26Z,2020-03-04T16:25:24Z,,,
3122,b'include tf gpt2 tests for attn mask and past variable',2020-03-04T13:36:04Z,2020-03-04T17:03:47Z,,,
3121,b'A better way to process extended_attention_mask in BertModel.forward()',2020-03-04T12:04:04Z,2020-05-10T18:58:40Z,Core: Modeling,,
3120,b'Making past and mems variables have batch size as their first output dimension.',2020-03-04T11:12:26Z,2020-05-10T11:37:08Z,wontfix,,
3119,"b""rename variables named 'word' to 'token' in generate fn""",2020-03-04T09:43:44Z,2020-03-04T17:01:17Z,,,
3118,b'Add beam search to generation tf 2 0',2020-03-04T09:42:39Z,2020-03-04T22:28:01Z,,,
3117,b'BART FP16',2020-03-04T02:52:27Z,2020-03-05T21:14:36Z,,RuntimeError,"RuntimeError: Expected object of scalar type Float but got scalar type Half for argument #2 'mat2' in call to _th_bmm"
3116,b'Skipping outputs',2020-03-03T23:27:53Z,2020-03-09T17:48:59Z,,,
3115,b'fix: passing config as Layer trainable param',2020-03-03T23:08:24Z,2020-03-04T20:59:10Z,,,
3114,b'Rename BartForMaskedLM -> BartForConditionalGeneration',2020-03-03T22:42:13Z,2020-03-05T22:41:19Z,,,
3113,b'model cards for both aubmindlab/bert-base-arabert models',2020-03-03T22:34:12Z,2020-03-04T17:04:40Z,model card,,
3112,b'Adds failing tests for the fast tokenizers',2020-03-03T21:43:06Z,2020-07-21T12:11:37Z,"wontfix, Core: Tokenization, Tests",,
3111,b'Create README.md',2020-03-03T21:38:37Z,2020-03-04T17:06:06Z,model card,,
3110,"b'BartForSequenceClassification: fix num_labels, add test'",2020-03-03T20:24:41Z,2020-03-03T20:54:30Z,,,
3109,b'[WIP] Add tests that ensure that copied functions remain in sync',2020-03-03T19:57:01Z,2020-05-15T17:12:22Z,wontfix,,
3108,b'BART: <mask> token ID is outside vocab bounds',2020-03-03T17:02:14Z,2020-03-04T11:44:17Z,"wontfix, Core: Modeling, seq2seq",RuntimeError,"RuntimeError: index out of range: Tried to access index 50264 out of table with 50263 rows. at /pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp:418"
3107,b'BertTokenizer.save_pretrained() ignores do_lower_case',2020-03-03T16:16:21Z,2020-05-10T12:37:07Z,"wontfix, Core: Tokenization",,
3106,b'fix beam_search behavior when sampling',2020-03-03T15:56:19Z,2020-03-04T14:30:52Z,,,
3105,b'Change back pipeline signatures',2020-03-03T15:31:24Z,2020-03-06T22:26:19Z,,,
3104,b'BART -- RuntimeError: expected device cuda:0 but got device cpu',2020-03-03T14:47:49Z,2020-03-03T20:26:16Z,seq2seq,RuntimeError,"RuntimeError: expected device cuda:0 but got device cpu"
3103,b'Support keras JSON/HDF5 serialization of main layers',2020-03-03T14:14:22Z,2020-03-06T11:59:14Z,,,
3102,b'bert-base-arabic model card',2020-03-03T13:31:39Z,2020-03-03T14:29:29Z,model card,,
3101,b'Keras layers should override get_config to be JSON-serializable',2020-03-03T13:30:02Z,2020-03-06T11:59:14Z,,NotImplementedError,"NotImplementedError: Layers with arguments in `__init__` must override `get_config`."
3100,"b'Fix QA models binding for Flaubert, XLNet and XLM.'",2020-03-03T13:05:20Z,2020-03-06T18:04:30Z,,,
3099,"b""Don't crash if fine-tuned model doesn't end with a number""",2020-03-03T12:13:03Z,2020-03-03T13:59:48Z,,,
3098,"b""why BertModel' object has no attribute 'bias'""",2020-03-03T09:20:15Z,2020-03-04T18:30:34Z,,AttributeError,"AttributeError: 'BertModel' object has no attribute 'bias'"
3097,b'Some question about training BERT after change the Vocab.txt size',2020-03-03T07:16:38Z,2020-05-09T21:12:13Z,wontfix,RuntimeError,"RuntimeError: Error(s) in loading state_dict for BertForMaskedLM:"
3096,b'BART BartForSequenceClassification example',2020-03-03T05:01:57Z,2020-03-03T20:54:30Z,seq2seq,AttributeError,"AttributeError: 'BartForSequenceClassification' object has no attribute 'num_labels'"
3095,b'Getting different topk results when using past + attention mask for more than 1 sentence',2020-03-03T04:09:59Z,2020-03-03T14:56:30Z,,,
3094,b'[Bart] dont call .forward',2020-03-03T03:42:35Z,2020-03-03T20:14:13Z,,,
3093,"b""wrong 'label2id' and 'id2label' in config when loading from pretrained""",2020-03-03T01:11:21Z,2020-03-05T22:16:57Z,,,
3092,b'\xc4\x8b in gpt2',2020-03-02T23:43:19Z,2020-03-05T18:41:08Z,"Discussion, Core: Tokenization",,
3091,b'Fast tokenizers fail when the input is just spaces',2020-03-02T22:24:21Z,2020-04-22T14:01:04Z,Core: Tokenization,IndexError,"IndexError: list index out of range"
3090,b'Cuda error during evaluation - CUBLAS_STATUS_NOT_INITIALIZED ',2020-03-02T22:20:34Z,2020-03-04T17:04:38Z,,RuntimeError,"RuntimeError: CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`"
3089,b'add models cards for camembert-base-fquad camembert-base-squad',2020-03-02T21:47:31Z,2020-03-02T22:07:14Z,model card,,
3088,"b""Fast tokenizers can't `encode_plus` a list of ids; slow tokenizers can""",2020-03-02T21:32:23Z,2020-05-09T01:12:13Z,"wontfix, Core: Tokenization",,
3087,b'Error when runnig run_tf_ner.py',2020-03-02T19:35:30Z,2020-03-02T20:52:49Z,,ImportError,"ImportError: cannot import name 'GradientAccumulator'"
3086,b'Disabling Eager Mode Prevents Loading Pre-Trained BERT',2020-03-02T19:18:22Z,2020-06-03T12:43:07Z,"wontfix, TensorFlow, Core: Modeling",,
3085,b'TF GPU CI',2020-03-02T18:32:28Z,2020-03-02T20:45:26Z,,,
3084,b'Update README.md',2020-03-02T18:32:08Z,2020-03-02T18:35:36Z,model card,,
3083,b'Memory error : load 200GB file in run_language_model.py',2020-03-02T17:36:03Z,2020-05-25T15:58:48Z,"Discussion, wontfix, High-Level feature, Core: Tokenization",,
3082,b'Summarization Examples: add Bart CNN Evaluation',2020-03-02T16:13:33Z,2020-03-03T20:30:01Z,,,
3081,b'Why there is no TransfoXLForSequenceClassification class?',2020-03-02T16:02:50Z,2020-05-12T12:46:11Z,"Discussion, wontfix, Core: Modeling",,
3080,b'Docker Hub automatic tests failing',2020-03-02T15:58:43Z,2020-03-03T15:33:20Z,"Tests, Should Fix",,
3079,b'Bart CUDA not working',2020-03-02T15:12:24Z,2020-03-02T16:35:47Z,,RuntimeError,"RuntimeError: expected device cuda:0 but got device cpu"
3078,b'correct greedy generation when doing beam search',2020-03-02T14:04:13Z,2020-03-02T17:00:10Z,,,
3077,b'fix n_gpu count when no_cuda flag is activated',2020-03-01T20:30:20Z,2020-03-02T15:20:22Z,,,
3076,b'XLNet multiple sentence modeling token type ids',2020-03-01T18:34:48Z,2020-05-08T22:06:09Z,"Help wanted, Discussion, wontfix, Core: Modeling",,
3075,b'Training TFBertForSequenceClassification with custom X and Y data',2020-03-01T17:30:45Z,2020-06-09T11:17:58Z,"wontfix, Usage",,
3074,b'Only enable some labels in BERT fine-tuned NER',2020-03-01T14:45:37Z,2020-05-29T09:13:23Z,"wontfix, Usage",,
3073,b'Finetuned BERT model does not seem to predict right labels/work properly?',2020-03-01T14:41:53Z,2020-05-08T22:06:08Z,wontfix,,
3072,b' Chinese BERT model can be used represented by words instead of character',2020-03-01T11:45:30Z,2020-03-02T09:11:33Z,,,
3071,b'Predict the next word in sentence context from the list of possible words in Russian',2020-03-01T10:15:54Z,2020-03-02T08:29:36Z,Usage,,
3070,"b""load_tf_weights_in_bert : 'BertModel' object has no attribute 'bias'""",2020-03-01T05:50:22Z,2020-03-02T15:23:50Z,Core: Modeling,AttributeError,"AttributeError: 'BertModel' object has no attribute 'bias'"
3069,b'No Causal Attention Masking in GPT-2 LM Finetuning Script',2020-03-01T04:29:04Z,2020-03-01T23:19:38Z,,,
3068,b'Problem with using pretrained BertTokenizer for Korean',2020-02-28T22:42:22Z,2020-05-08T22:06:11Z,"wontfix, Core: Tokenization",,
3067,b'No speed diference when doing prediction between BERT and ALBERT',2020-02-28T21:43:08Z,2020-05-21T20:40:46Z,"Discussion, wontfix",,
3066,b'Documentation and code mismatch in BertForMaskedLM forward method',2020-02-28T20:59:04Z,2020-04-21T07:08:21Z,Documentation,,
3065,"b""XLM-RoBERTa can't add new tokens.""",2020-02-28T18:45:11Z,2020-05-09T11:12:14Z,"wontfix, Core: Tokenization",,
3064,b'Add LM capabilities to TFTransfoXLLMHead',2020-02-28T17:52:28Z,2020-03-19T15:32:22Z,Core: Modeling,,
3063,b'Add generate() functionality to TF 2.0',2020-02-28T14:37:10Z,2020-03-03T14:42:15Z,,,
3062,b'Should weight distribution change more when fine-tuning transformers-based classifier?',2020-02-28T09:03:16Z,2020-05-24T13:46:08Z,"Discussion, wontfix",,
3061,b'Bad word list for text generation',2020-02-28T04:54:12Z,2020-03-31T16:42:31Z,High-Level feature,,
3060,b'How to init a subclass of BertForTokenClassification',2020-02-28T03:08:00Z,2020-05-05T07:09:48Z,wontfix,TypeError,"TypeError: __init__() missing 1 required positional argument: 'config'"
3059,b'Bart-CNN',2020-02-28T02:28:56Z,2020-03-02T15:35:54Z,,,
3058,b'Fast tokenizers calculate wrong offsets when special characters are present',2020-02-28T01:19:57Z,2020-04-22T13:56:05Z,Core: Tokenization,,
3057,"b""Fast tokenizers don't properly tokenize special tokens""",2020-02-28T01:08:46Z,2020-04-22T13:58:10Z,Core: Tokenization,,
3056,b'(Gross WIP) Bart-CNN',2020-02-27T21:29:05Z,2020-02-28T02:29:10Z,,,
3055,b'Pipeline doc',2020-02-27T20:39:37Z,2020-03-02T19:07:11Z,,,
3054,b'Bart: Use bool attention_mask for encoder',2020-02-27T19:21:57Z,2020-03-04T14:37:44Z,,,
3053,b'Changes to NER examples for PLT and TPU',2020-02-27T17:16:54Z,2020-02-27T21:45:33Z,,,
3052,b'Is ALBERT the right implement from paper ?  ',2020-02-27T16:45:36Z,2020-02-28T03:59:05Z,,,
3051,b'Adding Docker images for transformers + notebooks',2020-02-27T16:01:57Z,2020-03-04T16:45:58Z,,,
3050,b'Should be able to turn off logging',2020-02-27T14:57:39Z,2020-11-02T15:37:32Z,wontfix,,
3049,b'Regarding attention received by the distilbert model',2020-02-27T13:34:07Z,2020-05-04T22:49:51Z,wontfix,,
3048,b'Set specific hidden_size for ClassificationHead',2020-02-27T13:07:14Z,2020-05-04T22:49:50Z,wontfix,,
3047,b'[WIP] deleted special tokens as attributes from model config',2020-02-27T13:04:44Z,2020-03-05T22:21:55Z,,,
3046,b'[WIP] add generate tests to more models',2020-02-27T11:10:46Z,2020-03-05T22:27:01Z,,,
3045,b'[docs] Provide a barebones GPT-2 colab notebook',2020-02-27T07:02:44Z,2020-08-29T19:17:49Z,"wontfix, Documentation, Good First Issue",,
3044,b'How can I use the this result?',2020-02-27T07:01:17Z,2020-02-28T08:12:41Z,Usage,,
3043,b'Issue with Makefile',2020-02-27T06:33:20Z,2020-05-04T07:33:57Z,wontfix,,
3042,b'Fix spelling of strictly in error messages',2020-02-27T05:54:01Z,2020-02-27T15:22:36Z,,,
3041,b'Fix batch_encode_plus',2020-02-27T04:37:21Z,2020-02-27T14:56:48Z,,,
3040,b'Knowledge distillation from internal representation GPT2',2020-02-27T03:30:06Z,2020-05-08T22:06:10Z,wontfix,,
3039,b'Paragraph re-ranking using MS MARCO dataset',2020-02-27T00:43:56Z,2020-05-07T11:57:09Z,"wontfix, Usage",,
3038,"b""AttributeError: 'Model2Model' object has no attribute 'prepare_model_kwargs' in 2.5.1""",2020-02-27T00:28:28Z,2020-05-04T01:33:57Z,"wontfix, Core: Modeling",AttributeError,"AttributeError: 'Model2Model' object has no attribute 'prepare_model_kwargs'"
3037,b'Wrong logic in `batch_encode_plus()`',2020-02-27T00:03:09Z,2020-02-27T14:56:48Z,,,
3036,b'Cannot use `transformers.GradientAccumulator` with `tf.function`',2020-02-26T19:30:52Z,2020-05-12T00:34:10Z,"wontfix, TensorFlow",,
3035,b'Force pad_token_id to be set before padding for standard tokenizer',2020-02-26T17:48:27Z,2020-03-02T15:53:56Z,,,
3034,b'fix several typos in Distil* readme',2020-02-26T17:01:22Z,2020-02-26T17:39:55Z,,,
3033,b'Fix attn mask gpt2 when using past',2020-02-26T16:32:38Z,2020-02-26T17:04:38Z,,,
3032,b'Loading custom weights for BERT in pytorch',2020-02-26T15:21:09Z,2020-02-26T15:50:13Z,Core: CLI,,
3031,b'Forward pass with GPT2 using both past and attention_mask as an input leads to dimension error',2020-02-26T14:39:57Z,2020-02-26T17:04:38Z,,,
3030,b'run_tf_glue with AdamW optimizer and distributed training',2020-02-26T14:37:22Z,2020-05-03T16:33:57Z,wontfix,,
3029,b'How to add data to  pretrained model.',2020-02-26T12:01:15Z,2020-05-10T07:47:12Z,"wontfix, Usage",,
3028,"b""AttributeError: 'Tensor' object has no attribute 'size'""",2020-02-26T10:44:28Z,2020-02-26T20:34:29Z,TensorFlow,AttributeError,"AttributeError: 'Tensor' object has no attribute 'size'"
3027,b'This class and module cannot be found',2020-02-26T06:42:31Z,2020-02-27T08:14:03Z,"Core: Tokenization, Installation",ImportError,"ImportError: No module named 'tokenizers'"
3026,"b""language_modeling.py doesn't continue from last global step""",2020-02-26T06:12:59Z,2020-05-03T18:33:57Z,"wontfix, Ex: LM (Finetuning), Should Fix",,
3025,b'Why do I run example/run_ner.py with no output',2020-02-26T02:45:35Z,2020-06-25T23:35:32Z,"Need more information, wontfix",,
3024,b'Fix (non-slow) tests on GPU (torch)',2020-02-26T01:26:29Z,2020-02-26T16:59:26Z,,,
3023,b'BART : host `bart-large-cnn`',2020-02-26T01:20:25Z,2020-03-03T20:30:00Z,"New model, Core: Encoder-Decoder, seq2seq",,
3022,b'Make format consistent with that of PreTrainedTokenizer',2020-02-25T22:57:50Z,2020-05-02T23:39:08Z,wontfix,,
3021,b'Can GPT2LMHeadModel do batch inference with variable sentence lengths?',2020-02-25T22:05:02Z,2020-02-26T13:11:23Z,,,
3020,b'[ci] Run all tests on (self-hosted) GPU',2020-02-25T21:12:03Z,2020-02-29T02:11:09Z,,,
3019,b'Delete Model2Model',2020-02-25T20:49:43Z,2020-02-26T16:36:28Z,,,
3018,b'[WIP] Updates to simplify PLT example and use new features',2020-02-25T20:10:49Z,2020-02-27T06:30:07Z,,,
3017,b'[WIP] Update to use new pytorch-lightning features ',2020-02-25T20:08:45Z,2020-02-25T20:09:05Z,,,
3016,b'Use my own pretrained BERT model',2020-02-25T19:45:47Z,2020-05-03T00:39:10Z,"wontfix, Usage",,
3015,b'Latest version of transformers available via conda?',2020-02-25T18:33:16Z,2020-06-07T01:14:39Z,"wontfix, Installation, Should Fix",,
3014,b'Add integration tests for xlm roberta modelling and xlm roberta tokenzier',2020-02-25T17:42:40Z,2020-02-25T21:51:26Z,,,
3013,b'Generation with gpt-2',2020-02-25T17:03:26Z,2020-02-25T17:23:25Z,wontfix,,
3012,b'batch_encode_plus with pad_to_max_length is not padding the output',2020-02-25T16:59:18Z,2020-02-26T06:22:19Z,"Core: Tokenization, Version mismatch",,
3011,b'Add models special tokens to its pretrained configs',2020-02-25T15:41:04Z,2020-03-05T22:26:49Z,,,
3010,b'Possible improvement of padding logic in generate',2020-02-25T15:23:39Z,2020-02-27T13:14:34Z,,,
3009,"b'add crf layer to BERT, RoBERTa, XLMR'",2020-02-25T11:09:18Z,2020-03-22T04:24:54Z,,,
3008,b'[WIP] Remove tokenizers dependency',2020-02-25T10:40:46Z,2020-04-07T17:59:55Z,,,
3007,b'[Benchmark] Pipeline for question answering',2020-02-25T09:47:00Z,2020-05-13T03:17:56Z,"wontfix, Core: Pipeline",,
3006,b'[FIX] not training when epoch is small',2020-02-25T08:10:01Z,2020-03-19T15:21:22Z,,,
3005,b'Output of pipeline feature extraction',2020-02-25T07:39:31Z,2020-05-04T22:49:48Z,"wontfix, Core: Pipeline",,
3004,b'BART : How can I train and evaluate BART on CNN/DM dataset',2020-02-25T07:36:09Z,2020-03-25T01:00:25Z,,,
3003,b'Test correct tokenizers after default switch',2020-02-24T23:38:43Z,2020-02-24T23:45:54Z,,,
3002,b'Tokenizer Fast False by default',2020-02-24T23:29:32Z,2020-02-24T23:30:58Z,,,
3001,b'Changing the loss function in BERT',2020-02-24T21:40:15Z,2020-02-27T15:51:18Z,Usage,,
3000,b'unk_token not set when loading TransformerXLTokenizer.from_pretrained() from a save_pretrained() ',2020-02-24T21:27:52Z,2020-05-02T06:39:12Z,"wontfix, Core: Tokenization",ValueError,"ValueError: No <unkown> token in vocabulary"
2999,b'Create README.md for the new model fine tuned for Spanish POS tagging',2020-02-24T19:32:07Z,2020-02-24T19:33:50Z,,,
2998,b'kwargs are passed to both model and configuration in AutoModels',2020-02-24T19:19:21Z,2020-02-24T19:19:40Z,,,
2997,b'add explaining example to XLNet LM modeling',2020-02-24T19:05:10Z,2020-02-24T20:42:39Z,,,
2996,b'Trying to Use AlbertTokenizer With my own custom Vocab file',2020-02-24T18:24:18Z,2020-07-09T03:04:31Z,"wontfix, Core: Tokenization",,
2995,b'No optimizer steps when gradient_accumulation_steps smaller than epoch_iterator length',2020-02-24T15:32:01Z,2020-03-19T15:21:22Z,Ex: Sequence Classification,,
2994,b'Tf qa pipelines test',2020-02-24T15:12:53Z,2020-03-09T20:51:11Z,,,
2993,b'Too many bugs in Version 2.5.0',2020-02-24T13:44:38Z,2020-03-10T10:54:00Z,"Core: Tokenization, Installation, Should Fix",,
2992,b'Train TFXLNetForSequenceClassification model failed.',2020-02-24T13:28:26Z,2020-02-25T12:04:56Z,,ValueError,"ValueError: in converted code:"
2991,b'run_ner.py / bert-base-multilingual-cased can output empty tokens',2020-02-24T12:48:45Z,2020-03-27T14:59:55Z,,,
2990,b'run_ner.py example',2020-02-24T08:47:15Z,2020-02-24T09:44:46Z,Ex: Named Entity Recognition,,
2989,b'Documentation',2020-02-24T02:26:36Z,2020-02-25T23:43:37Z,,,
2988,b'Speed up GELU computation with torch.jit',2020-02-23T23:55:07Z,2020-04-03T19:20:22Z,,,
2987,b'Add preprocessing step for transfo-xl tokenization to avoid tokenizing words followed by punction to <unk>',2020-02-23T23:13:13Z,2020-02-24T20:11:11Z,,,
2986,b'How to generate BERT/Roberta word/sentence embedding?',2020-02-23T22:27:10Z,2020-02-25T08:56:38Z,"Discussion, Usage",,
2985,b'`AutoModel.from_pretrained` sends config kwargs to model',2020-02-23T21:34:01Z,2020-02-24T19:19:39Z,"Core: Modeling, Should Fix",TypeError,"TypeError: __init__() got an unexpected keyword argument 'output_attention'"
2984,b'add_ctags_to_git_ignore',2020-02-23T21:32:33Z,2020-02-23T21:55:33Z,,,
2983,b'NER support for Albert in run_ner.py and NerPipeline',2020-02-23T20:39:26Z,2020-02-27T15:22:56Z,,,
2982,b'Change masking to direct labeling for TPU support.',2020-02-23T19:35:40Z,2020-02-25T19:47:44Z,,,
2981,b'Strange bug when Finetuning own pretrained model (with an even stranger solution)',2020-02-23T17:05:18Z,2020-06-15T23:00:52Z,"wontfix, Should Fix",ValueError,"ValueError: loaded state dict contains a parameter group that doesn't match the size of optimizer's group "
2980,b'Cannot install Transformers version >2.3.0 with pip on CentOS',2020-02-23T16:17:03Z,2020-05-07T15:57:10Z,"Core: Tokenization, Installation",,
2979,b'Question about output pipeline(feature-extraction)',2020-02-23T14:38:18Z,2020-02-23T14:41:27Z,,,
2978,b'unreadable codes in for utils_glue',2020-02-23T11:50:36Z,2020-02-23T16:20:09Z,,,
2977,b'Fix for case of multi-gpu',2020-02-23T08:46:02Z,2020-04-30T08:54:57Z,wontfix,,
2976,b'XLMRobertaTokenizer vocab size',2020-02-23T05:33:53Z,2020-06-29T15:58:04Z,"wontfix, Core: Tokenization, Should Fix",,
2975,b'GPT2 always has largest attention on first token?',2020-02-23T05:29:32Z,2020-02-25T23:21:48Z,,,
2974,b'New CLI using Typer',2020-02-22T23:37:31Z,2020-04-24T20:02:50Z,,,
2973,b'Testing that batch_encode_plus is the same as encode_plus',2020-02-22T22:44:08Z,2020-02-24T17:09:47Z,,,
2972,b'Getting the output of the from the forward function of the GPT-2',2020-02-22T22:17:33Z,2020-05-02T06:39:11Z,"Help wanted, wontfix, Core: Modeling",,
2971,b'fix _update_memory fn call in transformer-xl',2020-02-22T21:44:20Z,2020-02-24T22:50:25Z,,,
2970,b'Bug in transfo_xl function call',2020-02-22T21:16:06Z,2020-02-25T09:26:11Z,Core: Modeling,,
2969,b'Bart: fix layerdrop and caching shapes for generation',2020-02-22T21:09:05Z,2020-02-22T21:25:04Z,,,
2968,"b'Delete untested, broken Model2LSTM'",2020-02-22T20:26:56Z,2020-02-23T16:28:49Z,,,
2967,b'missing name entity recognition link',2020-02-22T20:02:50Z,2020-02-25T19:06:58Z,,,
2966,b'Warning on `add_special_tokens`',2020-02-22T15:06:07Z,2020-02-24T13:42:55Z,,,
2965,b'Correct `special_tokens_mask` when `add_special_tokens=False`',2020-02-22T15:05:10Z,2020-02-23T14:50:40Z,,,
2964,b'[DOCS] fix hardcoded path in examples readme',2020-02-22T12:38:18Z,2020-02-22T16:12:39Z,,,
2963,"b""Length of special_tokens_mask doesn't align with the input_ids""",2020-02-22T01:20:33Z,2020-02-23T14:50:40Z,"Core: Tokenization, Should Fix",,
2962,b'[WIP] Proposal for Migrating to Typer for CLI and Examples',2020-02-22T00:25:43Z,2020-04-24T20:03:42Z,,,
2961,b'Fix max_length not taken into account when using pad_to_max_length on fast tokenizers',2020-02-21T23:58:50Z,2020-02-22T14:27:48Z,,,
2960,"b""Python Tokenizer batch_encode_plus doesn't pad input if asked to do so.""",2020-02-21T23:55:37Z,2020-02-24T17:09:47Z,"Core: Tokenization, Should Fix",,
2959,b'Switching from argparse to Typer',2020-02-21T22:52:28Z,2020-04-24T20:01:54Z,Discussion,,
2958,b'Remove double bias',2020-02-21T22:09:41Z,2020-02-21T22:10:19Z,,,
2957,b'Bias in `BertLMPredictionHead ` is added twice',2020-02-21T21:44:29Z,2020-02-21T21:50:27Z,,,
2956,b'adding support for commonsense qa for multiple choice question answering',2020-02-21T21:16:21Z,2020-05-02T16:39:08Z,wontfix,,
2955,b'Only use F.gelu for torch >=1.4.0',2020-02-21T21:01:01Z,2020-02-21T21:10:22Z,,,
2954,b'Distillation throws CUDA out of memory even with available GPU memory',2020-02-21T20:35:32Z,2020-05-20T22:43:23Z,"Need more information, wontfix, Distillation",,
2953,b'Migrating from `pytorch-pretrained-bert` to `pytorch-transformers` issue regarding model() output',2020-02-21T20:33:08Z,2020-02-21T22:10:10Z,,IndexError,"IndexError: index 10 is out of bounds for dimension 0 with size 7"
2952,"b""RuntimeError: Expected tensor for argument #1 'indices' to have scalar type Long; but got torch.cuda.IntTensor instead (while checking arguments for embedding)""",2020-02-21T16:46:51Z,2020-04-29T20:09:47Z,"wontfix, PyTorch",RuntimeError,"RuntimeError: Expected tensor for argument #1 'indices' to have scalar type Long; but got torch.cuda.IntTensor instead (while checking arguments for embedding)"
2951,b'Update modelcard of bert-base-german-cased',2020-02-21T15:46:41Z,2020-02-22T16:08:44Z,model card,,
2950,b'BertTokenizerFast ignores `pad_to_max_length`',2020-02-21T15:17:09Z,2020-02-24T15:30:59Z,"Core: Tokenization, Should Fix",,
2949,b'Change the model type after fine-tuning?',2020-02-21T14:35:02Z,2020-05-02T17:39:11Z,"wontfix, Core: Modeling, Usage",,
2948,b'Some questions about change the BertForSequenceClassification',2020-02-21T13:56:07Z,2020-04-29T20:09:48Z,"wontfix, Core: Modeling",,
2947,b'Fast tokenizers padding and prefix space',2020-02-21T12:57:00Z,2020-02-24T22:39:28Z,"Core: Tokenization, Should Fix",,
2946,b'On masked-lm labels and computing the loss',2020-02-21T11:55:26Z,2020-02-24T07:28:02Z,,,
2945,b'Labels are now added to model config under id2label and label2id',2020-02-21T11:32:57Z,2020-02-21T13:53:06Z,,,
2944,b'output padding different to zero in embedding layer',2020-02-21T10:53:06Z,2020-02-21T14:12:43Z,"Core: Modeling, Usage",,
2943,b'fp16 is not compatible with the current activation code when pytorch is less than 1.4.0',2020-02-21T03:58:21Z,2020-02-21T08:45:56Z,,,
2942,b'Create README.md for xlnet_large_squad',2020-02-21T02:51:32Z,2020-02-21T13:54:42Z,model card,,
2941,"b'pipeline(""sentiment-analysis"")() can\'t handle more than 2 sentences'",2020-02-20T23:57:24Z,2020-05-02T15:20:32Z,Core: Pipeline,ValueError,"ValueError: operands could not be broadcast together with shapes (10,2) (10,) "
2940,b'BERT model breaks during FP16 Apex training on the latest update (2.5.0) - due to gelu function',2020-02-20T22:53:34Z,2020-02-21T21:10:22Z,"PyTorch, Should Fix",RuntimeError,"RuntimeError: ""GeluCUDAKernelImpl"" not implemented for 'Half'"
2939,b'Add standardized get_vocab method to tokenizers',2020-02-20T20:44:37Z,2020-02-22T17:09:02Z,,,
2938,b'OpenAIGPTDoubleHeadsModel throws CUDA OOM with large number of candidates',2020-02-20T20:42:21Z,2020-04-30T04:55:15Z,"wontfix, Core: Modeling",,
2937,b'Small fix: default args for torch-lightning',2020-02-20T20:19:47Z,2020-02-20T20:31:18Z,,,
2936,b'New tokenizers issue in NER demo',2020-02-20T19:49:46Z,2020-06-21T12:22:13Z,"wontfix, Core: Tokenization, Should Fix",,
2935,b'Optimized squad.py multi-threading',2020-02-20T18:16:27Z,2020-04-29T20:09:34Z,wontfix,,
2934,b'Update README.md',2020-02-20T17:32:38Z,2020-02-22T16:06:42Z,model card,,
2933,b'Fix for fast tokenizers save_pretrained compatibility with Python.',2020-02-20T17:22:31Z,2020-02-24T23:20:43Z,,,
2932,b'[WIP] Add a trainer tool class to make the TF2 model training easier',2020-02-20T15:40:21Z,2020-04-25T14:01:01Z,,,
2931,"b'Fix spell: EsperBERTo, not EspertBERTo'",2020-02-20T13:54:53Z,2020-02-20T15:02:08Z,,,
2930,b'Add local_files_only parameter to pretrained items',2020-02-20T13:02:18Z,2020-02-24T19:58:16Z,,,
2929,b'Getting the same results when evaluating Model2Model with different encoder inputs.',2020-02-20T12:57:06Z,2020-02-26T09:09:27Z,"Core: Encoder-Decoder, seq2seq",,
2928,b'Make RobertaForMaskedLM implementation identical to fairseq',2020-02-20T12:55:31Z,2020-02-21T15:58:30Z,Core: Modeling,,
2927,b'What does ## mean in the bert vocab?',2020-02-20T11:13:07Z,2020-02-20T11:37:47Z,Core: Tokenization,,
2926,b'Masked LM implementation details',2020-02-20T09:38:10Z,2020-03-05T02:06:55Z,"TensorFlow, Core: Modeling",,
2925,b'DistilRoberta Model fine tuning on Squad dataset',2020-02-20T09:15:41Z,2020-05-07T12:40:17Z,"PyTorch, Distillation",RuntimeError,"RuntimeError: transform: failed to synchronize: cudaErrorAssert: device-side assert triggered"
2924,b'Update modeling_tf_utils.py',2020-02-20T09:06:38Z,2020-02-21T16:28:33Z,Documentation,,
2923,b'Loading tensorflow first and then loading transformers errors',2020-02-20T08:44:42Z,2020-04-27T18:11:20Z,"wontfix, TensorFlow",tensorflow.python.framework.errors_impl.InternalError,"tensorflow.python.framework.errors_impl.InternalError: Blas GEMM launch failed : a.shape=(15, 768), b.shape=(768, 768), m=15, n=768, k=768 [Op:MatMul] name: tf_bert_for_sequence_classification/bert/encoder/layer_._0/attention/self/query/Tensordot/MatMul/"
2922,b'Tokenizer fast warnings',2020-02-20T08:42:04Z,2020-02-20T16:55:04Z,,,
2921,b'Expose all constructor parameters for BertTokenizerFast',2020-02-20T08:10:05Z,2020-02-20T16:53:32Z,,,
2920,b'Error arises when using pipeline with community model',2020-02-20T04:22:31Z,2020-04-29T20:09:39Z,"wontfix, Core: Tokenization",TypeError,"TypeError: "
2919,b'Fast tokenizers ignore `add_special_tokens=False`',2020-02-20T00:10:06Z,2020-04-22T14:41:24Z,Core: Tokenization,,
2918,b'Fast Tokenizers save pretrained should return the list of generated file paths.',2020-02-19T23:47:36Z,2020-02-19T23:58:05Z,,,
2917,b'Breaking-change behavior in BERT tokenizer when stripping accents',2020-02-19T23:45:40Z,2020-07-01T09:28:14Z,"wontfix, Core: Tokenization",,
2916,b'How to train a LM with a custom Dataset?',2020-02-19T23:07:28Z,2020-02-20T16:02:42Z,Ex: LM (Pretraining),,
2915,b'How to train with variable number of candidates for multiple choice selection?',2020-02-19T22:27:39Z,2020-04-29T20:09:46Z,"Help wanted, wontfix, Usage",,
2914,b'Add syntax highlighting to the BibTeX in README',2020-02-19T21:49:41Z,2020-02-20T15:06:16Z,,,
2913,b'make RobertaForMaskedLM implementation identical to fairseq',2020-02-19T20:30:08Z,2020-02-20T12:50:08Z,,,
2912,b'Override build_inputs_with_special_tokens for fast tokenizers',2020-02-19T20:25:44Z,2020-02-19T21:09:52Z,,,
2911,"b'missing ""para"" attribute in ARC dataset for multiple choice question answering model'",2020-02-19T18:50:31Z,2020-02-19T23:55:16Z,,,
2910,"b""`PreTrainedTokenizerFast.build_inputs_with_special_tokens` doesn't add the special tokens""",2020-02-19T18:22:00Z,2020-02-19T21:14:47Z,Core: Tokenization,,
2909,b'Add slow generate tests for pretrained lm models',2020-02-19T17:23:23Z,2020-02-24T16:51:58Z,,,
2908,b'Model I am using Roberta',2020-02-19T15:06:47Z,2020-02-19T16:41:00Z,,,
2907,b'Help needed with interpretation of the MLP class',2020-02-19T13:44:07Z,2020-07-05T12:07:47Z,"Help wanted, wontfix, Core: Modeling",,
2906,b'documentation for TF models mentions non-existent methods',2020-02-19T13:44:01Z,2020-02-21T16:28:33Z,Documentation,,
2904,b'squad_convert_example_to_features does not work with CamembertTokenizer',2020-02-19T13:03:12Z,2020-02-26T09:10:58Z,Core: Tokenization,,
2903,b'Update to include example of LM',2020-02-19T11:26:28Z,2020-02-20T15:58:00Z,,,
2902,b'Convert BERT to RoBERTa',2020-02-19T10:27:18Z,2020-02-21T16:44:42Z,"Core: Modeling, Usage",,
2901,b'Pre-trained BERT-LM missing LM Head - returns random token predictions',2020-02-19T09:54:26Z,2020-02-28T04:52:56Z,"Core: Modeling, New model",,
2900,b'pull from original',2020-02-19T08:39:22Z,2020-02-19T08:40:10Z,,,
2899,"b""RobertaTokenizer different than fairseq for 'world'""",2020-02-19T03:41:03Z,2020-02-19T03:58:02Z,Core: Tokenization,,
2898,b'Language modeling example script missing the next sentence predicion',2020-02-19T00:31:52Z,2020-02-21T16:40:42Z,Ex: LM (Pretraining),,
2897,"b""save_pretrained doesn't work with GPT2FastTokenizer""",2020-02-18T23:30:53Z,2020-02-26T18:42:46Z,Core: Tokenization,,
2896,"b""BertModel' object missing 'save_pretrained' attribute""",2020-02-18T21:35:52Z,2020-02-18T21:53:57Z,,AttributeError,"AttributeError: 'BertModel' object has no attribute 'save_pretrained'"
2895,"b""Enable 'from transformers import AlbertMLMHead'""",2020-02-18T18:54:02Z,2020-02-18T23:21:44Z,,,
2894,"b'Allow import of model components, e.g. `from transformers import TFAlbertMLMHead`'",2020-02-18T18:41:14Z,2020-02-18T23:21:40Z,,,
2893,b'Pipeline Loading Models and Tokenizers',2020-02-18T18:40:40Z,2020-03-05T08:48:42Z,Core: Pipeline,,
2892,b'Create README.md',2020-02-18T18:30:49Z,2020-02-19T15:51:17Z,model card,,
2891,b'Fix InputExample docstring',2020-02-18T17:24:42Z,2020-02-20T20:25:16Z,,,
2890,b'Support for torch-lightning in NER examples',2020-02-18T17:22:01Z,2020-02-20T16:50:06Z,,,
2889,"b""Getting: AttributeError: 'BertTokenizer' object has no attribute 'encode'""",2020-02-18T16:33:43Z,2020-06-25T23:19:47Z,"wontfix, PyTorch, Core: Tokenization",AttributeError,"AttributeError: 'BertTokenizer' object has no attribute 'encode'"
2888,b'[WIP] Adapt lm generate fn for seq 2 seq models',2020-02-18T14:55:53Z,2020-03-05T22:30:10Z,,,
2887,b'Regarding attention size returned by the model',2020-02-18T13:30:43Z,2020-02-21T16:33:52Z,"TensorFlow, Core: Modeling",,
2886,b'Load Pretrained Model Error in Inherit Class',2020-02-18T08:18:39Z,2020-04-29T20:09:43Z,"wontfix, PyTorch, Core: Modeling",,
2885,b'Improve special_token_id logic in run_generation.py and add tests',2020-02-17T20:37:46Z,2020-02-21T17:10:00Z,,,
2884,b'Evaluation and Inference added to run_glue.py',2020-02-17T18:20:23Z,2020-04-29T20:09:42Z,"wontfix, Ex: Sequence Classification",,
2883,b'Create README.md in the right path for bert-spanish-cased-finetuned-ner',2020-02-17T14:23:42Z,2020-02-17T15:58:44Z,model card,,
2882,b'No prediction for some words (BERT NER) when run on GPU',2020-02-17T13:35:38Z,2020-05-04T04:33:56Z,"wontfix, PyTorch, Ex: Named Entity Recognition",,
2881,b'update .gitignore to ignore .swp files created when using vim',2020-02-17T13:27:28Z,2020-02-17T13:36:49Z,,,
2880,b'Transformers \xe3\x81\xa8 Simpletransfomrers\xe3\x82\x92\xe4\xbd\xbf\xe3\x81\xa3\xe3\x81\x9fAlebert\xe3\x81\xa7\xe3\x81\xaeNER\xe3\x81\xae\xe5\xaf\xbe\xe5\xbf\x9c',2020-02-17T06:51:56Z,2020-02-17T06:54:21Z,,,
2879,b'[model_cards] \xf0\x9f\x87\xb9\xf0\x9f\x87\xb7 Add new (cased) BERTurk model',2020-02-17T00:19:09Z,2020-02-17T14:54:47Z,model card,,
2878,b'FileNotFoundError when python runs setup.py for sentencepiece',2020-02-16T19:58:41Z,2020-02-19T08:14:41Z,,,
2877,b'Error with run_language_modeling.py training from scratch',2020-02-16T18:18:32Z,2020-05-13T11:17:51Z,"wontfix, Ex: LM (Pretraining), PyTorch",RuntimeError,"RuntimeError: CUDA error: device-side assert triggered"
2876,b'Create bert-spanish-cased-finedtuned-ner.md',2020-02-16T14:20:10Z,2020-02-16T16:58:57Z,model card,,
2875,b'Update README.md',2020-02-16T12:11:02Z,2020-02-16T15:09:35Z,model card,,
2874,b'How  to run TFBERT model in disable_eager_execution() mode ',2020-02-16T08:49:50Z,2020-02-18T13:42:17Z,,,
2873,"b'how to get ""xlnet-base-cased-pytorch_model.bin"" original \'last modified\' date?'",2020-02-16T06:34:16Z,2020-02-16T08:33:14Z,,,
2872,b'Explanation of the results derived from fine tuning',2020-02-16T06:19:11Z,2020-05-10T11:37:07Z,wontfix,,
2871,b'RoBERTa has a token_type layer (just a cosmetic issue)',2020-02-16T06:01:47Z,2020-06-21T17:01:46Z,"wontfix, Core: Modeling",,
2870,b'distilberttokenizer.encode_plus() token_type_ids are non-default',2020-02-16T04:09:15Z,2020-04-15T14:16:18Z,,,
2869,"b""ValueError: too many dimensions 'str'""",2020-02-16T04:07:45Z,2020-02-18T20:59:51Z,,ValueError,"ValueError: too many dimensions 'str'"
2868,b'How can I run NER on ALBERT?',2020-02-15T23:32:10Z,2020-05-13T03:17:55Z,"wontfix, Ex: Named Entity Recognition",,
2867,b'from_pretrained making internet connection if internet turned on',2020-02-15T17:24:24Z,2020-02-24T19:58:16Z,High-Level feature,,
2866,b'How to get the matrix that is used to combine output from multiple number of attention heads?',2020-02-14T23:29:29Z,2020-04-29T20:09:44Z,"Help wanted, wontfix",,
2865,b'UserWarning: The number of elements in the out tensor of shape [1] is 1 ',2020-02-14T20:14:22Z,2020-04-26T09:31:06Z,"wontfix, PyTorch",,
2864,b'Update model card: new performance chart',2020-02-14T18:38:02Z,2020-02-14T18:39:24Z,,,
2863,"b""What does the variable 'present' represent?""",2020-02-14T16:37:24Z,2020-02-14T21:10:07Z,,,
2862,b'PreTrainedTokenizer returns potentially incorrect attention mask',2020-02-14T16:29:08Z,2020-04-25T23:16:01Z,"Discussion, wontfix, Core: Tokenization",,
2861,b'DistilBERT distilbert-base-cased failed to load',2020-02-14T15:35:13Z,2020-02-17T17:48:17Z,,OSError,"OSError: Model name 'distilbert-base-cased' was not found in tokenizers model name list (distilbert-base-uncased, distilbert-base-uncased-distilled-squad, distilbert-base-german-cased, distilbert-base-multilingual-cased). We assumed 'distilbert-base-cased' was a path, a model identifier, or url to a directory containing vocabulary files named ['vocab.txt'] but couldn't find such vocabulary files at this path or url."
2860,b'Post-padding affects the Bert embedding output',2020-02-14T11:49:47Z,2020-02-18T21:55:14Z,,,
2859,b'Added model card for bert-base-multilingual-uncased-sentiment',2020-02-14T10:58:17Z,2020-02-14T14:31:16Z,model card,,
2858,b'is right?',2020-02-14T10:52:44Z,2020-02-19T08:29:13Z,Ex: LM (Pretraining),,
2857,b'Fix typos',2020-02-14T09:41:09Z,2020-02-14T14:11:09Z,,,
2856,b'Fix typo',2020-02-14T08:46:21Z,2020-02-14T14:07:44Z,,,
2855,b'Fix typo',2020-02-14T04:53:55Z,2020-02-14T14:13:08Z,,,
2854,"b""Create model card for 'distill-bert-base-spanish-wwm-cased-finetuned-spa-squad2-es'""",2020-02-14T03:59:28Z,2020-02-14T04:04:53Z,,,
2853,b'[pipeline] Alias NerPipeline as TokenClassificationPipeline',2020-02-14T01:15:04Z,2020-02-14T14:18:11Z,,,
2852,b'Update with additional information',2020-02-14T00:50:37Z,2020-02-14T02:54:43Z,,,
2851,"b""Create model card for the newly released 'nlpaueb/bert-base-greek-uncased-v1'""",2020-02-14T00:18:07Z,2020-02-14T00:27:23Z,model card,,
2850,b'Adding usage examples for common tasks',2020-02-13T21:41:00Z,2020-02-25T18:48:25Z,,,
2849,b'PreTrainedEncoderDecoder does not work for LSTM ',2020-02-13T20:27:40Z,2020-02-23T16:28:49Z,"Core: Modeling, seq2seq, Should Fix",AttributeError,"AttributeError: 'LSTM' object has no attribute 'config'"
2848,b'Add `masked_lm_labels` argument to `TFAlbertForMaskedLM`',2020-02-13T18:55:18Z,2020-06-09T01:50:25Z,"wontfix, TensorFlow",,
2847,b'BART/T5 seq2seq example',2020-02-13T15:52:57Z,2020-03-03T20:30:00Z,seq2seq,,
2846,"b'Error reported when running \'\'run_language_modeling.py"" file'",2020-02-13T15:39:53Z,2020-04-22T19:40:54Z,"Need more information, wontfix, Core: Modeling, Version mismatch",,
2845,b'Skip flaky test_tf_question_answering',2020-02-13T13:53:22Z,2020-02-18T21:14:51Z,,,
2844,b'Attempt to increase timeout for circleci slow tests',2020-02-13T13:26:59Z,2020-02-13T14:11:04Z,,,
2843,b'Model card: Literary German BERT',2020-02-13T13:12:54Z,2020-02-13T20:43:45Z,model card,,
2842,b'when will add XLMRobertaForQuestionAnswering package',2020-02-13T12:39:42Z,2020-02-19T13:31:45Z,,,
2841,b'cannot find model in model name list',2020-02-13T12:32:49Z,2020-02-14T03:31:23Z,"Core: Tokenization, Usage, Version mismatch",,
2840,b'[WIP] Add patience argument to run_language_modeling script',2020-02-13T10:40:47Z,2020-05-06T21:13:42Z,,,
2839,b'Fine-tuning the model using classification tasks',2020-02-13T10:36:46Z,2020-04-25T23:16:00Z,"wontfix, Ex: Sequence Classification, Usage",,
2838,b'A small model for CTRL ',2020-02-13T01:25:24Z,2020-05-07T01:31:15Z,"wontfix, Distillation",,
2837,b'Pretrained TFAlbertForMaskedLM returns seemingly random token predictions',2020-02-13T00:52:34Z,2020-04-22T20:45:43Z,"wontfix, TensorFlow, Core: Modeling",,
2836,b'Getting value of [UNK] labels',2020-02-12T23:59:58Z,2020-04-22T19:40:43Z,"wontfix, Core: Tokenization, Ex: Named Entity Recognition",,
2835,b'Failing slow RobertaModelIntegrationTest',2020-02-12T22:44:51Z,2020-04-22T19:40:53Z,"wontfix, Core: Modeling, Tests",,
2834,b'Failing slow AutoModelTest/BertForPreTraining',2020-02-12T22:43:56Z,2020-04-22T19:40:53Z,"wontfix, Core: Pipeline, Tests",,
2833,b'add model_card flaubert-base-uncased-squad',2020-02-12T22:24:48Z,2020-02-13T22:19:14Z,model card,ValueError,"ValueError: too many values to unpack (expected 2)"
2832,"b""'distilbert-base-cased-distilled-squad' was not found error""",2020-02-12T21:34:02Z,2020-02-13T16:32:24Z,,OSError,"OSError: "
2831,b'Installation Error - Failed building wheel for tokenizers',2020-02-12T19:52:28Z,2020-07-09T03:04:30Z,"wontfix, Core: Tokenization, Installation","""error, ""ERROR","""error: can not find Rust Compiler""""ERROR: Failed building wheel for tokenizers"""
2830,b'Reusing states for sequential decoding in BERTForMaskedLM',2020-02-12T18:28:29Z,2020-03-02T16:34:44Z,"PyTorch, Core: Encoder-Decoder, seq2seq",,
2829,b'BERT generating prediction in 120sec approx using squad 2.0 in prediction.json',2020-02-12T17:28:08Z,2020-04-19T18:29:10Z,wontfix,,
2828,b'[WIP] Create a Trainer class to handle TF2 model training',2020-02-12T16:13:10Z,2020-02-20T14:02:23Z,,,
2827,b'OOM risk in RobertaTokenizer/GPT2Tokenizer',2020-02-12T15:57:14Z,2020-04-19T23:29:13Z,"wontfix, Core: Tokenization",,
2826,b'Why only use the hidden state of last token of last layer is used for predicting the next word?',2020-02-12T14:28:32Z,2020-04-19T15:29:10Z,wontfix,,
2825,b'binarized_data.py in distillation uses incorrect type casting',2020-02-12T14:14:08Z,2020-02-17T20:01:43Z,Distillation,,
2824,b'GPT-2 language model: multiplying decoder-transformer output with token embedding or another weight matrix',2020-02-12T13:21:34Z,2020-02-12T17:08:26Z,"PyTorch, Core: Modeling",,
2823,b'Update run_tf_squad.py',2020-02-12T12:00:18Z,2020-02-12T12:41:33Z,,,
2822,b'bugs in xlnet XLNetLMHeadModel',2020-02-12T11:35:36Z,2020-02-24T20:42:38Z,"PyTorch, Core: Modeling, Should Fix",,
2821,b'CUDA out of memory issue in the middle of training in run_language_modeling.py (say after 1000 steps).',2020-02-12T09:13:20Z,2020-02-14T08:21:55Z,"Need more information, Ex: LM (Finetuning)",,
2820,"b""ImportError: cannot import name 'GradientAccumulator'""",2020-02-12T07:48:51Z,2020-04-13T01:54:15Z,"wontfix, Version mismatch",ImportError,"ImportError: cannot import name 'GradientAccumulator'"
2819,b'Create card for model bert-base-spanish-wwm-cased-finetuned-spa-squad2-es.md',2020-02-12T00:46:18Z,2020-02-12T01:07:16Z,model card,,
2818,b'Albert multilingual',2020-02-12T00:15:09Z,2020-04-19T16:29:10Z,"Need more information, wontfix, New model",,
2817,b'GPT2LMHeadModel with variable length batch input',2020-02-11T22:13:55Z,2020-04-22T19:40:44Z,"Help wanted, wontfix, PyTorch",,
2816,b'Proposal: Update examples to utilize a new format.',2020-02-11T20:07:41Z,2020-02-18T15:08:10Z,,,
2815,b'Add more specific testing advice to Contributing.md',2020-02-11T19:42:14Z,2020-02-11T22:20:10Z,,,
2814,b'Repository with recipes how to pretrain model from scratch on my own data',2020-02-11T16:04:52Z,2020-12-12T20:44:41Z,"wontfix, Documentation",,
2813,b'PreTrainedModel.generate do_sample default argument is wrong in the documentation',2020-02-11T15:27:09Z,2020-02-12T16:40:24Z,"PyTorch, Core: Modeling, Documentation",,
2812,b'How can I finetune the BERTModel on my own corpus?',2020-02-11T15:20:13Z,2020-03-03T22:50:59Z,"Ex: LM (Finetuning), Core: Modeling, Usage",,
2811,b'How to use a batch size bigger than zero in Bert Sequence Classification',2020-02-11T13:33:01Z,2020-02-12T02:00:18Z,,,
2810,b'How to get longer output for summary?',2020-02-11T10:59:34Z,2020-04-18T23:35:41Z,"wontfix, Usage, Summarization",,
2809,b'Fix typo in src/transformers/data/processors/squad.py',2020-02-11T06:56:49Z,2020-02-11T16:22:26Z,,,
2808,"b'Multiple Choice BERT, SWAG task, failure to test '",2020-02-11T05:44:36Z,2020-04-18T20:35:43Z,"wontfix, PyTorch, Ex: Multiple Choice, Usage",ValueError,"ValueError: For swag testing, the input file does not contain a label column. It can not be tested in current codesetting! "
2807,"b""get_activation('relu') provides a simple mapping from strings in configs to activation functions""",2020-02-11T04:48:49Z,2020-02-13T13:28:34Z,,,
2806,"b""TFBertModel.from_pretrained('neuralmind/bert-base-portuguese-cased') -> TypeError""",2020-02-10T23:17:15Z,2020-04-23T12:41:35Z,"Need more information, TensorFlow, Usage",,
2805,b'[model_cards] Add new German Europeana BERT models',2020-02-10T22:25:57Z,2020-02-11T15:49:39Z,model card,,
2804,b'Fix a few issues regarding the language modeling script',2020-02-10T21:53:17Z,2020-02-12T18:23:15Z,,,
2803,b'Support DeepSpeed for language modeling finetuning',2020-02-10T19:07:24Z,2020-04-18T00:50:34Z,"wontfix, Ex: LM (Finetuning), Ex: LM (Pretraining), External",,
2802,b'FlauBERT lang embeddings only when n_langs > 1',2020-02-10T17:20:09Z,2020-02-10T18:24:05Z,,,
2801,"b""Can't load pre-trained Flaubert model""",2020-02-10T16:31:11Z,2020-04-18T20:35:45Z,"Need more information, wontfix, Core: CLI, Core: Tokenization","OSError, SyntaxError","OSError: Model name 'flaubert-base-cased' was not found in tokenizers model name list (flaubert-large-cased, flaubert-base-uncased, flaubert-small-cased, flaubert-base-cased). We assumed 'flaubert-base-cased' was a path, a model identifier, or url to a directory containing vocabulary files named ['merges.txt', 'vocab.json'] but couldn't find such vocabulary files at this path or url.SyntaxError: invalid syntax"
2800,"b""CircleCI doesn't run slow tests""",2020-02-10T15:32:00Z,2020-02-10T16:16:59Z,,,
2799,b'Add model readme for bert-base-german-cased',2020-02-10T15:15:20Z,2020-02-10T15:27:30Z,model card,,
2798,b'Reduce the CamemBERT dimensions',2020-02-10T14:53:44Z,2020-02-18T06:26:04Z,"wontfix, Usage",AttributeError,"AttributeError: 'tuple' object has no attribute 'dim'"
2797,b'Add model readme for deepset/roberta-base-squad2',2020-02-10T14:07:15Z,2020-02-10T20:21:49Z,model card,,
2796,b'output padding different to zero in hidden layers with attention mask',2020-02-10T13:41:25Z,2020-04-18T00:50:34Z,"wontfix, PyTorch, Core: Modeling, Usage",,
2795,b'Probably a bug in XLMRobertaTokenizer',2020-02-10T13:13:12Z,2020-03-18T13:52:50Z,Core: Tokenization,,
2794,b'You must specify an aggregation method to update a MirroredVariable in Replica Context.',2020-02-10T12:59:07Z,2020-04-18T00:50:32Z,"wontfix, TensorFlow, Usage",,
2793,b'Fix circleci cuInit error on Tensorflow >= 2.1.0.',2020-02-10T12:38:11Z,2020-02-11T10:48:43Z,Tests,,
2792,b'tiny issue with distilbertconfig docs',2020-02-10T09:28:24Z,2020-02-10T18:46:36Z,"Documentation, Usage",,
2791,b'Create BERT-of-Theseus model card',2020-02-10T01:38:30Z,2020-02-10T14:58:41Z,model card,,
2790,"b""Is there any way that I can directly feed the hidden output of the embedding layer into each of the transformer's layer?""",2020-02-09T19:06:39Z,2020-02-19T13:40:13Z,"PyTorch, Usage",,
2789,b'Is there any way that I can extract the hidden output from the self-attention layer?',2020-02-09T18:43:09Z,2020-04-29T20:09:45Z,"Help wanted, wontfix, PyTorch, Usage",,
2788,b'SQuAD preprocessing not working for roberta (wrong p_mask)',2020-02-09T08:32:36Z,2020-05-14T21:07:53Z,"Core: Pipeline, Ex: Question Answering, Usage, Should Fix",KeyError,"KeyError: 0"
2787,b'Distillation code loss functions',2020-02-09T05:21:33Z,2020-04-19T22:29:10Z,"wontfix, PyTorch, Core: Modeling, Usage, Distillation",,
2786,"b""SequenceSummary: config.summary_activation = 'relu' would be ignored""",2020-02-08T22:11:47Z,2020-02-13T13:28:34Z,"Ex: Sequence Classification, Core: Modeling",,
2785,b'Create README.md',2020-02-08T21:32:17Z,2020-02-10T22:28:00Z,model card,,
2784,b'ERROR:CUDA out of memory when using GPT2 tour',2020-02-08T10:13:12Z,2020-02-10T04:49:46Z,,,
2783,b'Features proposals to simplify training Tensorflow model',2020-02-08T10:05:55Z,2020-05-21T01:43:23Z,"wontfix, High-Level feature, TensorFlow",,
2782,b'RoBERTaMultiChoice does not work with `roberta-large`',2020-02-08T08:50:47Z,2020-02-08T09:33:52Z,,,
2781,b'Flaky TF pipelines test on CircleCI',2020-02-08T00:28:04Z,2020-04-18T00:50:31Z,"wontfix, Core: Pipeline, TensorFlow, Tests",,
2780,"b'Pipelines- if initial model download is interrupted, everything is ruined'",2020-02-07T23:40:37Z,2020-04-17T17:50:52Z,"wontfix, Core: Pipeline, Ex: Named Entity Recognition","RuntimeError, OSError","RuntimeError: unexpected EOF. The file might be corrupted.OSError: Unable to load weights from pytorch checkpoint file. If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True. "
2779,b'configuration from custom config file not working ',2020-02-07T21:51:11Z,2020-02-10T16:10:12Z,Usage,OSError,"OSError: Model name './lm/gpt2-xl/lm/my_config.json' was not found in model name list. We assumed 'https://s3.amazonaws.com/models.huggingface.co/bert/./lm/gpt2-xl/lm/my_config.json/config.json' was a path, a model identifier, or url to a configuration file named config.json or a directory containing such a file but couldn't find any such file at this path or url."
2778,b'Preserve spaces in GPT-2 tokenizers',2020-02-07T20:27:21Z,2020-02-13T18:29:44Z,,,
2777,b'distilbert-base-cased',2020-02-07T19:22:21Z,2020-02-07T20:28:14Z,,,
2776,b'Pipeline for text classification',2020-02-07T17:15:01Z,2020-05-07T01:31:16Z,"wontfix, Core: Pipeline, Usage",,
2775,b'Using fast tokenizers with pipelines',2020-02-07T17:13:41Z,2020-08-08T06:35:58Z,"wontfix, Core: Pipeline, Core: Tokenization",,
2774,b'embedding index getting out of range while running gpt2-xl model ',2020-02-07T16:53:29Z,2020-02-12T10:16:43Z,"PyTorch, Core: Modeling, Ex: Generation, Usage",,
2773,b'How to load a pretrained TF model using AutoModel?',2020-02-07T10:55:18Z,2020-02-07T15:18:15Z,,,
2772,b'How to generate different suggestions with GPT2 or XLNet like Write With Transformers?',2020-02-07T09:03:01Z,2020-02-07T10:01:57Z,,,
2771,b'export to onnx issue',2020-02-07T06:53:51Z,2020-04-14T23:25:54Z,"wontfix, External",,
2770,b'The prediction output is random',2020-02-07T05:13:27Z,2020-02-07T12:51:49Z,,,
2769,"b'Model download: tf-xlm-roberta-large ""tf_model.h5"" file missing '",2020-02-07T02:58:48Z,2020-02-07T15:38:28Z,,,
2768,b'why take the first hidden state for sequence classification (DistilBertForSequenceClassification)',2020-02-07T02:44:19Z,2020-04-14T23:25:55Z,"wontfix, Core: Modeling, Usage",,
2767,b'Adapter-BERT is missing in transformers library?',2020-02-07T01:26:13Z,2020-04-29T20:09:41Z,wontfix,,
2766,b'Fix documentation in ProjectedAdaptiveLogSoftmax',2020-02-07T01:04:05Z,2020-02-07T15:14:59Z,,,
2765,b'Add option to `cached_path` to automatically extract archives',2020-02-06T23:10:13Z,2020-02-10T13:05:16Z,,,
2764,b'[examples] rename run_lm_finetuning to run_language_modeling',2020-02-06T20:10:31Z,2020-02-07T14:15:29Z,,,
2763,b'Add albert-base-v3 to pretrained models?',2020-02-06T19:47:23Z,2020-02-10T20:52:07Z,,,
2762,b'Add contributors snapshot',2020-02-06T19:18:01Z,2020-02-06T20:25:49Z,,,
2761,b'[docs] Add menu w/ links to other pages on hf.co',2020-02-06T18:49:53Z,2020-02-06T20:30:03Z,,,
2760,"b'build: add poetry, an alternative to setup.py with dependency versions tracked'",2020-02-06T16:43:36Z,2020-04-29T20:09:35Z,wontfix,,
2759,"b'Loss is calculated on all tokens, including padding, in the LM fine-tuning example'",2020-02-06T15:22:16Z,2020-02-07T08:00:49Z,,,
2758,b'TFRoberta output with attention_mask changes in version 2.3.0 vs 2.4.1',2020-02-06T15:17:38Z,2020-04-14T23:25:56Z,"wontfix, TensorFlow, Core: Modeling, Usage",AssertionError,"AssertionError: "
2757,b'Cannot reproduce SQUAD Example',2020-02-06T12:14:54Z,2020-02-07T06:54:58Z,,,
2756,b'BERT decoder: Fix failure with the default attention mask.',2020-02-06T11:56:21Z,2020-02-11T20:19:23Z,,RuntimeError,"RuntimeError: expected device cpu and dtype Float but got device cpu and dtype Long"
2755,b'Multi-text files support for run_lm_finetuning',2020-02-06T09:13:47Z,2020-04-14T23:25:57Z,"wontfix, Ex: LM (Finetuning)",,
2754,b'Changed vocabulary save function. Variable name was inconsistent',2020-02-06T09:06:30Z,2020-02-06T21:40:08Z,,UnboundLocalError,"UnboundLocalError: local variable 'vocab_file' referenced before assignment"
2753,b'Saving tokenizer vocabulary throws error when passing file name instead of directory.',2020-02-06T08:56:58Z,2020-02-07T03:18:59Z,,UnboundLocalError,"UnboundLocalError: local variable 'vocab_file' referenced before assignment"
2752,b'Fix multi-gpu evaluation in run_glue.py example',2020-02-06T07:23:04Z,2020-02-06T21:38:56Z,,,
2751,b'Sentence pair classification',2020-02-06T03:10:58Z,2020-04-08T11:22:53Z,"wontfix, Ex: Sequence Classification, Usage",,
2750,"b""default output of BertModel.from_pretrained('bert-base-uncased')""",2020-02-05T22:55:13Z,2020-04-22T19:40:54Z,"wontfix, Usage",,
2749,b'Upgrade run_generation',2020-02-05T21:17:28Z,2020-02-21T17:22:59Z,,,
2748,b'TFAlbertModelTest::test_pt_tf_model_equivalence  -> Fatal Python Error on Mac',2020-02-05T19:50:59Z,2020-04-18T04:35:41Z,"wontfix, PyTorch, TensorFlow, Tests",,
2747,b'Arxiv README',2020-02-05T18:44:18Z,2020-02-05T20:26:29Z,,,
2746,b'Added CamembertForQuestionAnswering',2020-02-05T17:42:23Z,2020-02-21T17:01:03Z,,,
2745,b'Add BartModel',2020-02-05T17:10:03Z,2020-02-20T23:11:14Z,seq2seq,,
2744,b'Albert language model fine tuning not running run_lm_finetuning.py',2020-02-05T12:19:43Z,2020-06-03T06:43:08Z,"wontfix, Ex: LM (Finetuning), PyTorch, Usage",,
2743,b'PreTrainedEncoderDecoder keeps giving me the same next token',2020-02-05T11:54:32Z,2020-02-05T15:37:51Z,,,
2742,b'do_lower_case strips accents!',2020-02-05T11:47:20Z,2020-04-12T12:12:45Z,wontfix,,
2741,b'XLM-Roberta mask filling error',2020-02-05T09:28:07Z,2020-03-18T13:52:50Z,,RuntimeError,"RuntimeError: cublas runtime error : library not initialized at ../aten/src/THC/THCGeneral.cpp:216"
2740,b'T5 ',2020-02-05T08:09:37Z,2020-05-11T16:16:34Z,wontfix,UnboundLocalError,"UnboundLocalError: local variable 'extended_attention_mask' referenced before assignment"
2739,b'Development Infrastructure for ML Projects',2020-02-05T05:46:31Z,2020-02-05T23:11:29Z,,,
2738,b'Fix GPT2 config set to trainable',2020-02-04T22:12:23Z,2020-02-05T18:55:42Z,,,
2737,"b'Version 2.4.1 breaks run_lm_finetuning.py, version 2.3.0 runs fine'",2020-02-04T21:53:39Z,2020-02-04T22:10:39Z,,"RuntimeError, IndexError","RuntimeError: cuda runtime error (710) : device-side assert triggered at /pytorch/aten/src/THCUNN/generic/ClassNLLCriterion.cu:110IndexError: Target -1 is out of bounds."
2736,"b""TensorFlow XLM doesn't accept NumPy arrays for the attention mask""",2020-02-04T20:24:15Z,2020-04-23T18:13:25Z,,,
2735,b'test_attention_weights cleanup',2020-02-04T19:01:18Z,2020-02-04T21:38:53Z,,,
2734,b'pass langs parameter to certain XLM models',2020-02-04T18:18:19Z,2020-02-04T22:12:43Z,,,
2733,b'Save model wrapped in Keras',2020-02-04T17:26:38Z,2020-11-29T11:50:02Z,wontfix,NotImplementedError,"NotImplementedError: "
2732,b'Error for run_lm_finetuning.py (CUDA error: device-side assert triggered)',2020-02-04T15:56:50Z,2020-02-04T22:35:19Z,,RuntimeError,"RuntimeError: CUDA error: device-side assert triggered"
2731,b'Masked LM and TFBertForSequenceClassification',2020-02-04T15:29:46Z,2020-04-11T15:44:20Z,wontfix,,
2730,b'QuickStart code error',2020-02-04T15:19:56Z,2020-02-26T16:36:28Z,,`RuntimeError,"`RuntimeError: The size of tensor a (8) must match the size of tensor b (768) at non-singleton dimension 3`"
2729,"b""Attention Mask for TFXLM Model doesn't work""",2020-02-04T14:47:32Z,2020-05-13T03:17:53Z,wontfix,AttributeError,"AttributeError: 'tuple' object has no attribute 'as_list'"
2728,b'RuntimeError: expected dtype Float but got dtype Long - run_lm_finetuning.py',2020-02-04T11:49:18Z,2020-02-05T17:49:26Z,,"`RuntimeError, RuntimeError","`RuntimeError: expected dtype Float but got dtype LongRuntimeError: expected dtype Float but got dtype Long"
2727,b'XLM Roberta token_type_ids bug with batch_encode_plus',2020-02-04T11:19:12Z,2020-03-18T13:52:50Z,,RuntimeError,"RuntimeError: index out of range: Tried to access index 1 out of table with 0 rows. at ../aten/src/TH/generic/THTensorEvenMoreMath.cpp:418"
2726,"b'convert_tokens_to_ids(self, tokens)\xe4\xb8\xad\xe7\x9a\x84ids.append(self.vocab[token])\xef\xbc\x8cKeyError'",2020-02-04T09:41:20Z,2020-04-14T13:25:51Z,wontfix,KeyError,"KeyError: 'persuading'"
2725,b'add TinyBERT?',2020-02-04T02:01:08Z,2020-05-30T21:48:04Z,wontfix,,
2724,b'sequence labeling for sentences and not tokens',2020-02-03T23:58:53Z,2020-04-19T23:29:12Z,wontfix,,
2723,b'Improved testing',2020-02-03T23:32:31Z,2020-02-04T23:05:36Z,,,
2722,b'Bert and Roberta models cannot be converted to TFLite',2020-02-03T21:27:32Z,2020-04-11T03:44:21Z,wontfix,ConverterError,"ConverterError: See console for info."
2721,b'Is transformers ovewriting tokenizer?',2020-02-03T15:31:48Z,2020-02-03T17:24:08Z,,,
2720,b'Add READMEs to Tensorflow versions of CamemBERT and XLM-RoBERTa',2020-02-03T12:28:27Z,2020-02-03T14:04:36Z,,,
2719,b'Error when running run_lm_finetuning.py',2020-02-03T09:48:39Z,2020-02-04T05:09:45Z,,RuntimeError,"RuntimeError: Assertion `cur_target >= 0 && cur_target < n_classes' failed.  at C:\w\1\s\windows\pytorch\aten\src\THNN/generic/ClassNLLCriterion.c:97"
2718,b'DistilBertForMaskedLM is not passing ignore_index to loss fct nn.CrossEntropyLoss',2020-02-03T09:47:38Z,2020-02-04T02:31:35Z,,,
2717,b'error while training distilbert multilingual model',2020-02-03T06:30:39Z,2020-04-18T08:35:41Z,wontfix,ValueError,"ValueError: Error when checking model target: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 8 array(s), for inputs ['output_1', 'output_2', 'output_3', 'output_4', 'output_5', 'output_6', 'output_7', 'output_8'] but instead got the following list of 1 arrays: [<tf.Tensor 'ExpandDims:0' shape=(None, 1) dtype=int64>]"
2716,b'Added README.md to Swedish BERT models from National Library of Sweden',2020-02-02T22:12:12Z,2020-02-03T14:09:35Z,,,
2715,b'Optimize causal mask using torch.where',2020-02-02T21:11:03Z,2020-04-07T20:19:19Z,,,
2714,b'How to add Dense layer on top of TFBertForSequenceClassification model?',2020-02-02T20:21:48Z,2020-02-02T20:42:54Z,,"```AttributeError, AttributeError","```AttributeError: 'list' object has no attribute 'shape'```AttributeError: The layer has never been called and thus has no defined input shape."
2713,b'Weights of FlaubertForQuestionAnswering not initialized from pretrained model',2020-02-02T15:35:47Z,2020-02-11T22:11:53Z,,,
2712,b'a problem occur when I train Chinese distilgpt2 model',2020-02-02T14:36:33Z,2020-04-09T15:26:08Z,wontfix,**ValueError,**ValueError: too many values to unpack (expected 2)**
2711,"b""TypeError: apply_gradients() missing 1 required positional argument: 'clip_norm'""",2020-02-02T13:46:22Z,2020-02-11T00:45:06Z,,TypeError,"TypeError: apply_gradients() missing 1 required positional argument: 'clip_norm'"
2710,b'Removed unused fields in DistilBert TransformerBlock',2020-02-02T09:10:58Z,2020-02-20T21:08:22Z,,,
2709,b'DistributedDataParallel for multi-gpu single-node runs in run_lm_finetuning.py',2020-02-02T07:19:49Z,2020-02-02T12:41:43Z,,,
2708,"b""Can't pickle local object using the finetuning example.""",2020-02-01T22:48:47Z,2020-04-16T17:42:11Z,wontfix,AttributeError,"AttributeError: Can't pickle local object 'get_linear_schedule_with_warmup.<locals>.lr_lambda'"
2707,b'Fix typo in examples/utils_ner.py',2020-02-01T16:02:26Z,2020-02-01T16:10:58Z,,,
2706,b'Load from tf2.0 checkpoint fail',2020-02-01T15:24:18Z,2020-04-10T17:18:08Z,wontfix,NotImplementedError,"NotImplementedError: Weights may only be loaded based on topology into Models when loading TensorFlow-formatted weights (got by_name=True to load_weights)"
2705,b'What is the input for TFBertForSequenceClassification?',2020-02-01T10:20:29Z,2020-02-02T09:51:19Z,,`AttributeError,"`AttributeError: The layer has never been called and thus has no defined output shape.`"
2704,b'How to make transformers examples use GPU?',2020-02-01T04:02:21Z,2020-02-02T04:42:38Z,,,
2703,b'run_lm_finetuning.py on bert-base-uncased with wikitext-2-raw does not work',2020-02-01T03:03:08Z,2020-02-01T03:57:43Z,,RuntimeError,"RuntimeError: CUDA error: device-side assert triggered"
2702,"b'DistilBERT does not support token type ids, but the tokenizers produce them'",2020-02-01T01:03:37Z,2020-04-20T00:29:10Z,wontfix,TypeError,"TypeError: forward() got an unexpected keyword argument 'token_type_ids'"
2701,b'Store Model cards in the repo',2020-01-31T22:13:22Z,2020-01-31T23:39:10Z,,,
2700,b'Add TF2 version of FlauBERT',2020-01-31T16:56:50Z,2020-03-16T13:29:22Z,,,
2699,b'CLI script to gather environment info',2020-01-31T16:47:31Z,2020-02-01T15:38:15Z,,,
2698,b'Typo on markdown link in README.md',2020-01-31T15:40:24Z,2020-01-31T15:58:50Z,,,
2697,b'Albert language model fine tuning not running run_lm_finetuning.py ',2020-01-31T15:00:27Z,2020-04-07T15:55:15Z,wontfix,,
2696,b'Missing `do_sample` argument for run_generation example',2020-01-31T14:53:27Z,2020-02-07T14:51:11Z,,,
2695,"b""get_linear_schedule_with_warmup method can't be found in optimization.py""",2020-01-31T14:37:51Z,2020-01-31T16:52:47Z,,,
2694,b'AutoModel fails to load FlauBERT with `output_hidden_states`',2020-01-31T13:38:43Z,2020-04-07T21:55:40Z,wontfix,TypeError,"TypeError: __init__() got an unexpected keyword argument 'output_hidden_states'"
2693,b'Input file format for examples/run_lm_finetuning.py',2020-01-31T10:41:34Z,2020-07-18T05:22:37Z,wontfix,,
2692,"b""Regarding distlbert uncased model's size""",2020-01-31T09:54:32Z,2020-04-11T07:44:19Z,wontfix,,
2691,b'how can i finetune BertTokenizer?',2020-01-31T09:23:11Z,2020-04-25T15:15:57Z,wontfix,,
2690,b'Hardware requirements for BERT QA inference',2020-01-30T22:01:55Z,2020-01-31T09:50:13Z,,,
2689,b'Correct PyTorch distributed training command in examples/README.md',2020-01-30T21:55:23Z,2020-01-30T23:41:25Z,,,
2688,b'Config: reference array of architectures',2020-01-30T21:48:08Z,2020-01-31T00:27:00Z,,,
2687,b'Issue about pipeline of sentiment-analysis',2020-01-30T17:11:41Z,2020-02-04T06:58:13Z,,"RuntimeError, OSError","RuntimeError: unexpected EOF, expected 75086145 more bytes. The file might be corrupted.OSError: Unable to load weights from pytorch checkpoint file. If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True. "
2686,b'Add layerdrop to Flaubert',2020-01-30T16:04:38Z,2020-01-30T17:05:02Z,,,
2685,"b'German Bert tokenizer does not recognize (some) special characters (!,?,...)'",2020-01-30T14:28:12Z,2020-04-06T18:48:41Z,wontfix,,
2684,b'distilbert_multilingual_cased model for multiple language',2020-01-30T14:21:17Z,2020-02-03T14:46:15Z,,,
2683,b'TFCamembertModel',2020-01-30T13:33:03Z,2020-04-07T16:56:09Z,wontfix,,
2682,b'Issue with my profile on the upload/share models webpage',2020-01-30T11:11:18Z,2020-01-30T12:17:38Z,,,
2681,b'How to add a fc classification head to BertForQA to make a MTL-BertForQA model?',2020-01-30T09:46:53Z,2020-04-07T03:55:16Z,wontfix,,
2680,b'Does loss  function in the run_tf_ner.py takes logits or probabilities?',2020-01-30T09:34:45Z,2020-03-26T18:30:41Z,,,
2679,b'Add classifier dropout in ALBERT',2020-01-30T08:22:23Z,2020-01-30T14:52:36Z,,,
2678,b'Bug in consecutive creation of tokenizers with different parameters',2020-01-29T20:00:45Z,2020-01-29T21:52:53Z,,,
2677,b'Flaubert',2020-01-29T19:26:00Z,2020-01-30T15:04:19Z,,,
2676,b'Trouble fine tuning Huggingface GPT-2 on Colab \xe2\x80\x94 Assertion error',2020-01-29T17:22:25Z,2020-01-31T20:55:08Z,,,
2675,b'Best weights/models after fine-tuning gpt2',2020-01-29T16:58:10Z,2020-01-31T08:31:10Z,,,
2674,b'Integrate fast tokenizers library inside transformers',2020-01-29T16:30:28Z,2020-02-19T16:35:41Z,,,
2673,b'Fine tuning XLMRoberta for Question Answering',2020-01-29T15:48:42Z,2020-02-04T17:10:06Z,,,
2672,b'bert-base-uncased have weird result on Squad 2.0 ',2020-01-29T15:18:32Z,2020-01-31T09:36:37Z,,,
2671,b'is SOP(sentence order prediction) implemented?',2020-01-29T05:39:18Z,2020-02-03T06:50:28Z,,,
2670,b'Remove unnecessary `del` in run_tf_glue.py example',2020-01-29T01:30:41Z,2020-01-29T19:01:17Z,,,
2669,b'models and tokenizers trained with pytorch_pretrained_bert are not compatible with transformers',2020-01-28T23:30:08Z,2020-02-11T14:24:05Z,,RuntimeError,"RuntimeError: Error(s) in loading state_dict for OpenAIGPTLMHeadModel:"
2668,b'How to get .ckpt files for tensorflow  DistilBERT model',2020-01-28T21:16:56Z,2020-04-07T14:55:45Z,wontfix,,
2667,b'Calling AlbertTokenizer.from_pretrained() with the path to a single file or url is deprecated',2020-01-28T15:31:00Z,2020-04-11T03:44:20Z,wontfix,RuntimeError,"RuntimeError: Internal: /sentencepiece/src/sentencepiece_processor.cc(73) [model_proto->ParseFromArray(serialized.data(), serialized.size())] "
2666,b'Multiple token IDs for same token',2020-01-28T14:21:25Z,2020-04-04T16:38:32Z,wontfix,,
2665,b'standardize CTRL BPE files - upload models to S3',2020-01-28T12:51:59Z,2020-02-19T19:41:45Z,,,
2664,b'Updates to the templates',2020-01-28T12:21:07Z,2020-01-28T15:41:12Z,,,
2663,b'Add check to verify existence of pad_token_id',2020-01-28T10:45:25Z,2020-01-29T22:44:59Z,,,
2662,"b"" 'Embedding' object has no attribute 'shape'""",2020-01-28T08:59:30Z,2020-02-07T16:47:13Z,,,
2661,b'[Umberto] model shortcuts',2020-01-27T23:04:37Z,2020-01-31T02:05:54Z,,,
2660,b'PPLM with Tensorflow',2020-01-27T22:18:19Z,2020-03-28T14:38:39Z,,,
2659,b'[FIX] #2658 Inconsistent values returned by batch_encode_plus and enc\xe2\x80\xa6',2020-01-27T20:57:45Z,2020-04-25T04:15:59Z,wontfix,,
2658,b'Inconsistent values returned by batch_encode_plus and encode_plus ',2020-01-27T19:47:02Z,2020-02-24T17:09:47Z,,,
2657,b'Add `return_special_tokens_mask` to `batch_encode_plus()`',2020-01-27T17:11:40Z,2020-04-04T14:38:30Z,wontfix,,
2656,b'Using Transformers for a Sequence with Multiple Variables at Each Step',2020-01-27T16:20:13Z,2020-04-03T17:30:04Z,wontfix,,
2655,b'Fix AutoModelForQuestionAnswering for Roberta',2020-01-27T16:19:49Z,2020-01-27T17:12:47Z,,,
2654,b'Add keyword arguments to batch_encode_plus() to match encode_plus()',2020-01-27T16:15:47Z,2020-02-24T17:09:47Z,,,
2653,b'Fix token_type_ids for XLM-R',2020-01-27T15:04:46Z,2020-01-27T16:08:32Z,,,
2652,b'Fix importing unofficial TF models with extra optimizer weights',2020-01-27T14:50:31Z,2020-02-07T15:25:32Z,,,
2651,b'XLNET SQuAD2.0 Fine-Tuning - What May Have Changed?',2020-01-26T20:48:28Z,2020-05-08T22:06:06Z,wontfix,"'NoAns_exact', 'NoAns_f1', 'NoAns_total'","'NoAns_exact': 90.51303616484441,'NoAns_f1': 90.51303616484441,'NoAns_total': 5945,"
2650,b'loss function error when running run_lm_finetuning.py file',2020-01-26T17:48:35Z,2020-04-03T18:30:02Z,wontfix,RuntimeError,"RuntimeError: CUDA error: device-side assert triggered"
2649,b'Using a Model without any pretrained data',2020-01-26T17:07:32Z,2020-01-27T16:11:02Z,,,
2648,"b'run_lm_finetuning.py for GPT2 throw error ""Using pad_token, but it is not set yet.""'",2020-01-26T15:21:28Z,2020-01-28T15:33:30Z,,TypeError,"TypeError: fill_() received an invalid combination of arguments - got (NoneType), but expected one of:"
2647,b'Question Answering with Japanese',2020-01-26T12:33:15Z,2020-04-09T14:26:09Z,wontfix,,
2646,"b""glue.py: AttributeError: 'numpy.str_' object has no attribute 'text_a'""",2020-01-26T11:02:12Z,2020-01-26T13:29:50Z,,,
2645,b'How to load locally saved tensorflow DistillBERT model',2020-01-26T07:10:25Z,2020-01-28T08:35:38Z,,NotImplementedError,"NotImplementedError: When subclassing the `Model` class, you should implement a `call` method."
2644,b'XLNet run_squad.py IndexError: tuple index out of range',2020-01-26T01:47:24Z,2020-01-28T14:02:31Z,,IndexError,"IndexError: tuple index out of range"
2643,b'BERT LOSS FUNCTION',2020-01-25T22:44:39Z,2020-07-17T14:42:53Z,wontfix,,
2642,b'Scrambled dimensions on output of forward pass',2020-01-25T20:05:07Z,2020-08-08T09:35:59Z,wontfix,RuntimeError,"RuntimeError: size of dimension does not match previous size, operand 1, dim 0"
2641,"b""ImportError: cannot import name 'TFDistilBertModel'""",2020-01-25T12:44:24Z,2020-01-26T06:24:11Z,,ImportError,"ImportError: cannot import name 'TFDistilBertModel'"
2640,"b'batch_encode_plus not working for GPT2, OpenAI, TransfoXL when returning PyTorch tensors'",2020-01-25T10:51:55Z,2020-01-29T22:44:59Z,,,
2639,"b""AttributeError: 'Tensor' object has no attribute 'transpose'""",2020-01-25T10:35:07Z,2020-04-03T17:30:03Z,wontfix,AttributeError,"AttributeError: 'Tensor' object has no attribute 'transpose'"
2638,b'Get Warning Message: Unable to convert output to tensors format pt',2020-01-25T02:18:46Z,2020-04-01T23:15:50Z,wontfix,,
2637,b'Add AutoModelForPreTraining',2020-01-24T22:53:06Z,2020-01-27T19:27:08Z,,,
2636,b'Gradient checkpointing with GPT2DoubleHeadsModel',2020-01-24T22:43:46Z,2020-05-31T02:48:08Z,wontfix,,
2635,b'Improving generation',2020-01-24T22:28:52Z,2020-02-21T17:24:09Z,,,
2634,b'AutoModels Documentation',2020-01-24T21:37:07Z,2020-01-24T21:37:31Z,,,
2633,"b""Details on T5's current integration status """,2020-01-24T18:55:32Z,2020-03-31T21:32:45Z,wontfix,,
2632,b'Add FlauBERT: Unsupervised Language Model Pre-training for French',2020-01-24T14:58:03Z,2020-01-29T19:17:55Z,,,
2631,b'CamembertTokenizer cannot be pickled',2020-01-24T14:03:43Z,2020-01-24T15:40:53Z,,TypeError,"TypeError: can't pickle SwigPyObject objects"
2630,b'Pad token for GPT2 and OpenAIGPT models',2020-01-24T13:27:29Z,2020-03-04T16:38:25Z,,,
2629,b'Question about Architecture of BERT for QA',2020-01-24T11:41:00Z,2020-04-01T01:32:42Z,wontfix,,
2628,b'Albert on QQP inference',2020-01-24T11:11:10Z,2020-03-31T12:32:42Z,wontfix,,
2627,b'Why does the hidden state of the same input token change every time I call the same GPT2 model?',2020-01-23T23:41:04Z,2020-01-24T13:55:20Z,,,
2626,b'BertModel output the same embedding during Evaluation',2020-01-23T22:12:07Z,2020-02-14T22:50:34Z,,,
2625,b'Pipeline error when creating a model without a model card json file (on Windows)',2020-01-23T21:33:23Z,2020-04-06T21:53:25Z,"wontfix, Core: Pipeline",,
2624,b'How to merge TFDistilBertForSequenceClassification with another tf.Keras model',2020-01-23T21:24:44Z,2020-03-30T23:04:12Z,wontfix,AttributeError,"AttributeError: Layer tf_distil_bert_for_sequence_classification has no inbound nodes."
2623,b'QA pipeline run-time error when there is no answer',2020-01-23T19:52:29Z,2020-04-06T21:53:26Z,"wontfix, Core: Pipeline","ValueError, KeyError","ValueError: Wrong shape for input_ids (shape torch.Size([0])) or attention_mask (shape torch.Size([0]))KeyError: 255"
2622,b'tokenizer.add_tokens not working',2020-01-23T17:45:35Z,2020-01-24T14:50:39Z,,`RuntimeError,"`RuntimeError: The size of tensor a (30524) must match the size of tensor b (30522) at non-singleton dimension 2`"
2621,b'Documentation markup for model descriptions',2020-01-23T14:21:02Z,2020-03-30T17:04:12Z,wontfix,,
2620,"b""Document which heads are pretrained and which aren't""",2020-01-23T14:15:42Z,2020-04-01T18:15:50Z,wontfix,,
2619,b'Adding scibert in the list of pre-trained models?',2020-01-23T12:57:24Z,2020-01-23T14:51:00Z,,,
2618,b'summarization codes ',2020-01-23T11:12:05Z,2020-03-30T13:04:12Z,wontfix,,
2617,b'TF Models have no attribute .train() or .eval()',2020-01-23T09:02:08Z,2020-05-29T15:51:31Z,wontfix,,
2616,b'Adaptive Attention Span for Transformers',2020-01-22T23:01:30Z,2020-03-30T00:01:22Z,wontfix,,
2615,b'Question answering pipeline fails with long context',2020-01-22T22:10:10Z,2020-03-29T23:01:21Z,"wontfix, Core: Pipeline",IndexError,"IndexError: index 0 is out of bounds for axis 0 with size 0"
2614,"b'Missing module ""startlette"" when calling transformers-cli'",2020-01-22T17:09:17Z,2020-02-10T15:07:17Z,Core: Pipeline,,
2613,b'XLnet memory usage for long sequences',2020-01-22T17:08:22Z,2020-03-29T18:01:23Z,wontfix,,
2612,b'Error in fine tuning Roberta for QA',2020-01-22T17:05:33Z,2020-03-30T15:04:13Z,wontfix,tensorflow.python.framework.errors_impl.InvalidArgumentError,"tensorflow.python.framework.errors_impl.InvalidArgumentError: indices[0,11] = 1 is not in [0, 1) [Op:ResourceGather] name: tf_roberta_for_question_answering/tf_roberta_model/roberta/embeddings/token_type_embeddings/embedding_lookup/"
2611,b'Finetuning my language model',2020-01-22T15:35:44Z,2020-02-04T11:49:59Z,,,
2610,"b'run_ner.py huge discrepancy between eval and predict (or ""dev"" and ""test"" evaluation modes)'",2020-01-22T14:21:45Z,2020-03-29T16:01:24Z,wontfix,,
2609,b'Bad Results with Albert',2020-01-22T10:33:08Z,2020-03-31T10:32:44Z,wontfix,,
2608,b'Bug in the command line tool: os.DirEntry not supported in Python 3.5',2020-01-22T10:25:48Z,2020-03-24T17:44:05Z,"wontfix, Core: CLI",,
2607,"b""Fix inconsistency between T5WithLMHeadModel's doc and it's behavior""",2020-01-22T06:37:05Z,2020-01-24T15:28:21Z,,,
2606,"b'Upload CLI: on Windows, uniformize paths/urls separators'",2020-01-21T22:11:07Z,2020-03-23T13:46:23Z,"wontfix, Core: CLI",,
2605,b'glue.py when using mrpc and similar data does not work',2020-01-21T21:50:30Z,2020-03-29T18:01:22Z,wontfix,,
2604,b'Can not upload BertTokenizer.from_pretrained() from an AWS S3 bucket',2020-01-21T19:13:57Z,2020-03-29T23:01:22Z,wontfix,,
2603,b'XLNet: Incorrect segment id for CLS token',2020-01-21T16:31:15Z,2020-03-28T18:25:49Z,wontfix,,
2602,b'Edit a way to get `projected_context_layer`',2020-01-21T15:40:46Z,2020-02-03T16:25:53Z,,,
2601,"b""unexpected keyword argument 'encoder_hidden_states' when using PreTrainedEncoderDecoder""",2020-01-21T15:23:36Z,2020-03-28T16:25:53Z,wontfix,TypeError,"TypeError: forward() got an unexpected keyword argument 'encoder_hidden_states'"
2600,b'Trouble fine tuning multiple choice',2020-01-21T10:14:47Z,2020-06-09T07:18:03Z,wontfix,,
2599,"b'Xlnet, Alberta, Roberta are not finetuned for CoLA task'",2020-01-21T08:00:12Z,2020-01-26T04:49:32Z,,,
2598,b'load  tf2 roberta model meet error',2020-01-21T02:11:32Z,2020-01-21T08:26:41Z,,ValueError,"ValueError: Layer #0 (named ""roberta""), weight <tf.Variable 'tf_roberta_model_5/roberta/embeddings/word_embeddings/weight:0' shape=(30522, 768) dtype=float32, numpy="
2597,b'Transfer Learning on Text Summarization Model',2020-01-21T02:09:58Z,2020-03-30T15:08:46Z,seq2seq,,
2596,b'changing the attention head size in MultiBert',2020-01-20T21:15:59Z,2020-03-19T01:03:25Z,,,
2595,b'RAM leakage when trying to retrieve the hidden states from the GPT-2 model.',2020-01-20T18:19:42Z,2020-01-23T23:36:25Z,,,
2594,b'edited a way to get  at AlbertAttention.forward',2020-01-20T16:00:24Z,2020-01-21T15:28:17Z,,,
2593,b'Added custom model dir to PPLM train',2020-01-20T15:47:24Z,2020-03-27T17:15:40Z,wontfix,,
2592,"b'RuntimeError: The expanded size of the tensor (449) must match the existing size (2) at non-singleton dimension 2.  Target sizes: [4, 2, 449].  Tensor sizes: [1, 2] while using ALBERT'",2020-01-20T11:33:05Z,2020-04-26T16:42:37Z,wontfix,RuntimeError,"RuntimeError: The expanded size of the tensor (449) must match the existing size (2) at non-singleton dimension 2.  Target sizes: [4, 2, 449].  Tensor sizes: [1, 2]`"
2591,b'What is the f1 score of Squad v2.0 on bert-base? I only got f1 score 74.78.',2020-01-20T09:03:45Z,2020-01-22T05:03:12Z,,,
2590,"b'run_glue.py, CoLA : MCC goes to 0, in some hyperparameter cases'",2020-01-20T06:22:58Z,2020-03-27T09:15:43Z,wontfix,,
2589,"b'run_lm_finetuning.py regenerates examples cache when restored from a checkpoint, is this intended?'",2020-01-19T19:11:54Z,2020-03-28T04:25:49Z,wontfix,"FileNotFoundError, RuntimeError","FileNotFoundError: [Errno 2] No such file or directory: 'C:\\data\\.\\output\\checkpoint-200\\_cached_lm_512_dataset.txt'RuntimeError: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 11.00 GiB total capacity; 8.61 GiB already allocated; 10.74 MiB free; 8.69 GiB reserved in total by PyTorch)"
2588,b'how can i download the model manually?',2020-01-19T14:22:18Z,2020-03-27T13:15:43Z,wontfix,,
2587,b'The accuracy of XLNet',2020-01-19T12:26:54Z,2020-03-28T04:25:50Z,wontfix,,
2586,"b""PyTorch 1.2 has released API 'torch.nn.Transformer'\xef\xbc\x8cso it's better to modify the source code with the official  python API""",2020-01-19T09:54:05Z,2020-03-27T13:15:44Z,wontfix,,
2585,"b""Attibute Error\xef\xbc\x9a\xe2\x80\x98NoneType\xe2\x80\x99 object has no attribute 'seek' and OSError""",2020-01-19T05:27:33Z,2020-03-27T18:15:47Z,wontfix,,
2584,"b""what's the structure of the model saved after fine-tuning ?""",2020-01-19T03:29:09Z,2020-01-22T04:14:29Z,,,
2583,b'How to start a server and client to get feature vectors',2020-01-19T01:26:31Z,2020-03-26T03:36:22Z,wontfix,,
2582,b'XLM-Roberta checkpoint redundant weight',2020-01-18T22:11:31Z,2020-01-19T00:17:57Z,,,
2581,b'Invalid argument:  assertion failed: [Condition x == y did not hold element-wise:] [x (loss/output_1_loss/SparseSoftmaxCrossEntropyWithLogits/Shape_1:0) = ] [32 1] [y (loss/output_1_loss/SparseSoftmaxCrossEntropyWithLogits/strided_slice:0) = ] [32 128]',2020-01-18T20:54:22Z,2020-03-27T18:15:46Z,wontfix,InvalidArgumentError,"InvalidArgumentError: 2 root error(s) found."
2580,b'glue_convert_examples_to_features in  glue.py runs to  errors',2020-01-18T18:00:14Z,2020-01-20T17:49:46Z,,TypeError,"TypeError: object of type '_OptionsDataset' has no len()"
2579,b'Added saving to custom dir in PPLM train',2020-01-18T13:17:56Z,2020-01-20T15:37:05Z,,,
2578,"b""GPT2TokenizerFast object has no attribute 'with_pre_tokenizer'""",2020-01-18T12:35:43Z,2020-05-26T19:55:57Z,wontfix,AttributeError,"AttributeError: 'Tokenizer' object has no attribute 'with_pre_tokenizer'"
2577,b'always occur error:AssertionError',2020-01-18T08:13:02Z,2020-03-25T17:57:27Z,wontfix,,
2576,b'fill_mask helper',2020-01-18T05:30:40Z,2020-01-30T23:15:42Z,,,
2575,b'Fix examples/run_tf_ner.py label encoding error #2559 ',2020-01-18T04:46:45Z,2020-03-29T06:01:18Z,wontfix,,
2574,b'is RoBERTa-base.json in s3  wrong?',2020-01-18T03:36:36Z,2020-01-18T04:06:34Z,,,
2573,"b""Is RoBERTa's pair of sequences tokenizer correct with double </s>""",2020-01-18T02:34:35Z,2020-01-18T04:02:34Z,,,
2572,b'Bert TPU fine-tuning works on Colab but not in GCP',2020-01-18T01:59:57Z,2020-03-27T18:15:45Z,wontfix,NotFoundError,"NotFoundError: '_MklMatMul' is neither a type of a primitive operation nor a name of a function registered in binary running on n-aa2fcfb7-w-0. One possible root cause is the client and server binaries are not built with the same version. Please make sure the operation or function is registered in the binary running in this process. [Op:Identity]"
2571,"b""Why isn't BERT doing wordpiece tokenization?""",2020-01-17T23:40:00Z,2020-01-22T00:20:34Z,,,
2570,b'[run_lm_finetuning] Train from scratch',2020-01-17T23:06:36Z,2020-01-21T21:57:39Z,,,
2569,b'Add lower bound to tqdm for tqdm.auto',2020-01-17T21:58:48Z,2020-01-17T23:29:12Z,,ModuleNotFoundError,"ModuleNotFoundError: No module named 'tqdm.auto'"
2568,b'Finetuning ALBERT using examples/run_lm_finetuning.py',2020-01-17T21:12:38Z,2020-01-22T00:22:53Z,,,
2567,b'Bert perform way worse than simple LSTM+Glove',2020-01-17T21:01:35Z,2020-01-29T06:11:32Z,,,
2566,b'question about tokenizer changes original sequence length',2020-01-17T18:39:22Z,2020-01-17T21:07:43Z,,,
2565,b'Optionally convert output of FeatureExtraction pipeline to list',2020-01-17T18:37:34Z,2020-04-17T17:50:50Z,"wontfix, Core: Pipeline",,
2564,b'Fix glue processor failing on tf datasets',2020-01-17T17:59:59Z,2020-01-20T16:46:44Z,,,
2563,b'Fix typo in examples/run_squad.py',2020-01-17T11:44:56Z,2020-01-17T16:22:52Z,,,
2562,b'Architectures for Dialogue',2020-01-17T10:11:12Z,2020-05-23T14:47:04Z,wontfix,,
2561,"b'Model upload and sharing - delete, update, rename....'",2020-01-17T09:36:25Z,2020-01-27T23:47:11Z,,,
2560,"b""why this implementation didn't apply residual and layer norm?""",2020-01-17T06:37:01Z,2020-02-27T16:53:34Z,,,
2559,b'Prediction on NER Tensorflow 2',2020-01-17T02:36:02Z,2020-01-22T05:52:17Z,,,
2558,"b""solve the exception: [AttributeError: 'bool' object has no attribute 'mean']""",2020-01-17T02:22:24Z,2020-02-14T08:56:39Z,,,
2557,b'Fix BasicTokenizer to respect `never_split` parameters',2020-01-16T23:16:10Z,2020-01-17T19:57:57Z,,,
2556,b'Quantized model not preserved when imported using from_pretrained()',2020-01-16T19:54:48Z,2020-03-28T06:25:49Z,wontfix,,
2555,b'Fix output name',2020-01-16T19:45:49Z,2020-03-23T20:07:59Z,wontfix,,
2554,b'CTRL tokenizer has no special tokens to indicate EOS',2020-01-16T19:40:51Z,2020-02-21T17:10:00Z,,,
2553,b'Model not learning when using albert-base-v2 -- ALBERT',2020-01-16T19:19:12Z,2020-03-29T15:01:22Z,wontfix,,
2552,b'fix #2549',2020-01-16T18:20:12Z,2020-07-09T13:46:27Z,wontfix,,
2551,"b""EnvironmentError OSError: Couldn't reach server""",2020-01-16T18:12:02Z,2020-01-17T16:39:42Z,,OSError,"OSError: Couldn't reach server at 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json' to download pretrained model configuration file.```"
2550,b'fast gpt2 inference',2020-01-16T17:41:04Z,2020-05-13T03:17:58Z,wontfix,,
2549,b'unsupported operand type error in tokenizer.batch_encode_plus',2020-01-16T17:34:20Z,2020-03-31T01:04:12Z,wontfix,TypeError,"TypeError: unsupported operand type(s) for -: 'list' and 'int'"
2548,b'SQuAD convert_examples_to_features skipping doc tokens when they exceed max_seq_length',2020-01-16T16:33:18Z,2020-03-23T20:08:06Z,wontfix,,
2547,b'AlbertDoublehHeadsModel',2020-01-16T13:31:30Z,2020-03-23T16:08:04Z,wontfix,,
2546,b'Unable to generate ALBERT embeddings of size 128',2020-01-16T11:34:34Z,2020-01-16T12:00:24Z,,,
2545,"b'modified method simple_accuracy(), solve the exception'",2020-01-16T10:15:45Z,2020-01-17T02:20:55Z,,,
2544,b'modified method simple_accuracy()',2020-01-16T10:02:59Z,2020-01-16T10:14:15Z,,,
2543,b'modified method simple_accuracy()',2020-01-16T09:53:01Z,2020-01-16T10:02:12Z,,,
2542,b'Dynamic Quantization on ALBERT (pytorch) ',2020-01-16T05:13:25Z,2020-03-24T07:36:54Z,wontfix,AttributeError,"AttributeError: 'function' object has no attribute 't'"
2541,b'squad convert example to features potential bug',2020-01-16T04:41:04Z,2020-03-23T20:08:03Z,wontfix,,
2540,b'[PyTorch 1.4] Fix failing torchscript test for xlnet',2020-01-16T01:22:43Z,2020-01-16T12:17:00Z,,,
2539,b'Finetuning TFDistilBertForQuestionAnswering on SQuAD',2020-01-16T00:38:41Z,2020-03-24T20:36:57Z,wontfix,ValueError,"ValueError: Failed to find data adapter that can handle input: (<class 'list'> containing values of types {""<class 'transformers.data.processors.squad.SquadFeatures'>""}), <class 'NoneType'>"
2538,b':lipstick: super',2020-01-15T23:34:22Z,2020-01-16T12:17:16Z,,,
2537,b'[Question] Help needed to understand how torch.distributed.barrier() works',2020-01-15T22:33:50Z,2020-01-16T15:10:50Z,,,
2536,b'Universal Sentence Encoder',2020-01-15T22:15:28Z,2020-11-07T05:42:56Z,wontfix,,
2535,b'Tokenizer.from_pretrained: fetch all possible files remotely',2020-01-15T21:12:23Z,2020-01-16T21:47:20Z,,,
2534,b'DistilBERT accuracies on the glue test set.',2020-01-15T15:04:10Z,2020-03-22T15:49:36Z,wontfix,,
2533,b'Gradient accumulation',2020-01-15T14:56:44Z,2020-03-22T15:49:35Z,wontfix,,
2532,b'Automatic testing of examples in documentation',2020-01-15T14:30:13Z,2020-01-23T14:38:46Z,,,
2531,b'Serving improvements',2020-01-15T11:10:32Z,2020-01-20T15:56:24Z,,,
2530,b'SentencePiece Error with AlbertTokenizer using google pretrained chinese model',2020-01-15T09:08:56Z,2020-03-22T14:49:36Z,wontfix,RuntimeError,"RuntimeError: Internal: /sentencepiece/src/sentencepiece_processor.cc(73) [model_proto->ParseFromArray(serialized.data(), serialized.size())] "
2529,"b'Updating the issue template, directing general question to SO'",2020-01-15T08:45:30Z,2020-03-30T14:10:02Z,"Discussion, wontfix",,
2528,b'[Question] Add extra sublayer for each layer of Transformer',2020-01-15T08:38:03Z,2020-03-22T10:49:32Z,wontfix,,
2527,b'How to get the output in other layers from Bert?',2020-01-15T07:49:19Z,2020-01-16T11:29:53Z,,,
2526,"b'modified method simple_accuracy(), before:(preds == labels).mean() This\xe2\x80\xa6'",2020-01-15T03:39:04Z,2020-01-16T09:50:44Z,,,
2525,b'Error when running demo script in T5Model',2020-01-15T01:16:50Z,2020-03-22T03:49:32Z,wontfix,TypeError,"TypeError: forward() got an unexpected keyword argument 'lm_labels'"
2524,b'Will you release the pre-train script for T5?',2020-01-15T00:48:54Z,2020-03-22T02:49:32Z,wontfix,,
2523,"b""Tokenizer encoding functions don't support 'left' and 'right' values for `pad_to_max_length`""",2020-01-14T22:34:20Z,2020-01-14T23:33:56Z,,,
2522,b'https://s3.amazonaws.com/models.huggingface.co/xxx/pytorch_model.bin failed or can not open at xxx/.cache/xxxxxxxxxx',2020-01-14T16:20:18Z,2020-01-20T05:18:04Z,,,
2521,b'Bias should be resized with the weights',2020-01-14T15:14:00Z,2020-01-14T18:43:46Z,,,
2520,b'Descriptions of shared models and interaction with contributors',2020-01-14T14:19:47Z,2020-04-03T02:13:23Z,wontfix,,
2519,b'Does calling fit() method on TFBertForSequenceClassification change the weights of internal pre-trained bert?',2020-01-14T13:57:38Z,2020-03-21T20:19:20Z,wontfix,,
2518,b'Type of Training file needed for finetuning',2020-01-14T11:05:32Z,2020-03-25T05:14:28Z,wontfix,,
2517,b'Save only Bert Model after training a Sequence Classification Task/ LM finetuning Task. ',2020-01-14T10:08:40Z,2020-01-16T05:24:38Z,,,
2516,b'update',2020-01-14T03:12:48Z,2020-01-14T12:46:35Z,,,
2515,b'How to use transformers to  convert  batch sentences into word vectors???',2020-01-14T02:55:29Z,2020-01-15T08:40:32Z,,,
2514,b'T5 Masked LM -- pre-trained model import?',2020-01-13T19:15:59Z,2020-03-20T20:20:59Z,wontfix,,
2513,b'Error in AlbertForMaskedLM with add_tokens and model.resize_token_embeddings',2020-01-13T17:01:07Z,2020-03-27T16:15:43Z,wontfix,,
2512,"b""Getting started with the new 'FeatureExtractionPipeline' feature""",2020-01-13T16:41:54Z,2020-02-23T14:23:00Z,Core: Pipeline,,
2511,b'Saving full tensor output of hidden states instead of truncated output in lm_finetuning.py script',2020-01-13T16:30:21Z,2020-01-16T02:44:07Z,,,
2510,"b""ModuleNotFoundError: No module named 'model_bertabs' AND RuntimeError: CUDA error: device-side assert triggered""",2020-01-13T13:09:33Z,2020-03-27T12:15:43Z,wontfix,[1]ModuleNotFoundError,"[1]ModuleNotFoundError: No module named 'model_bertabs' when run convert_bertabs_original_pytorch_checkpoint.py"
2509,b'fix xlm roberta tokenizer mask id',2020-01-13T08:03:58Z,2020-02-22T21:39:36Z,,,
2508,b'XLMRobertaTokenizer is a wrong tokenizer for XLMRoberta',2020-01-13T07:41:12Z,2020-03-18T13:52:50Z,,,
2507,"b'update probabilitiy to probability, misspelled the word'",2020-01-13T07:32:28Z,2020-01-14T13:09:52Z,,,
2506,b'Discrepancy in results ( BertModel) between pytorch_pretrained_bert and transformers',2020-01-13T06:30:28Z,2020-01-16T17:05:40Z,,,
2505,"b""AttributeError: 'BertForTokenClassification' object has no attribute 'named_configeters'""",2020-01-13T06:16:10Z,2020-01-16T05:58:15Z,,,
2504,b'BertTokenizerFast.encode() ignores max_length',2020-01-12T20:24:38Z,2020-03-21T10:19:20Z,wontfix,,
2503,b'BERT and cross entropy',2020-01-12T17:08:46Z,2020-06-16T06:23:12Z,wontfix,,
2502,b'Perform MultiLingual Name Matching ',2020-01-12T15:09:13Z,2020-03-19T15:54:08Z,wontfix,,
2501,b'[announcement] Community effort for storing models metrics in one place. Anyone can help to gather results',2020-01-12T12:23:04Z,2020-03-22T10:49:33Z,,,
2500,"b'mistake, closing'",2020-01-12T12:16:31Z,2020-01-12T12:21:14Z,,,
2499,b'Trouble fine tuning distilbertmodel',2020-01-11T00:20:33Z,2020-03-18T17:40:02Z,wontfix,RuntimeError,"RuntimeError: Assertion `cur_target >= 0 && cur_target < n_classes' failed.  at /pytorch/aten/src/THNN/generic/ClassNLLCriterion.c:97"
2498,"b""RuntimeError: Expected object of backend CUDA but got backend CPU for argument #3 'index'""",2020-01-10T22:51:24Z,2020-04-17T17:50:51Z,"wontfix, Core: Pipeline",,
2497,b'How to load tf1 BERT checkpoints and sentencepiece model from local folder?',2020-01-10T22:42:11Z,2020-03-23T16:08:04Z,wontfix,,
2496,b'Using Model2Model with Albert',2020-01-10T22:09:35Z,2020-03-27T07:15:43Z,wontfix,TypeError,"TypeError: forward() got an unexpected keyword argument 'lm_labels'"
2495,"b""T5: move rp_bucket to relative_attention_bias' device""",2020-01-10T20:09:52Z,2020-01-10T21:18:55Z,,,
2494,"b""AutoModels: model_type is defined in config.json, not hardcoded in model's name""",2020-01-10T19:44:31Z,2020-01-14T18:59:15Z,,,
2493,b'GPT2 text generation produces different results w and w/o `past`',2020-01-10T15:08:56Z,2020-01-12T00:20:53Z,,,
2492,b'Configuration Documentation',2020-01-10T14:51:54Z,2020-01-14T13:09:11Z,,,
2491,b'Masked tokens are -1 not -100?',2020-01-10T14:20:04Z,2020-01-10T14:25:54Z,,,
2490,"b""UnicodeDecodeError: 'charmap' codec can't decode byte 0x90 in position 13: character maps to <undefined>""",2020-01-10T13:18:18Z,2020-03-17T14:16:13Z,wontfix,UnicodeDecodeError,"UnicodeDecodeError: 'charmap' codec can't decode byte 0x90 in position 13: character maps to <undefined>**"
2489,b'Model trained on Wikipedia Articles',2020-01-10T10:32:33Z,2020-03-20T10:20:57Z,wontfix,,
2488,b'NER Pipeline Issue',2020-01-10T10:31:30Z,2020-05-27T23:27:59Z,"Core: Pipeline, Ex: Named Entity Recognition",,
2487,"b'""config.json"" does not include correct ""id2label"" and ""label2id"" after finetuning on NER task'",2020-01-10T10:29:34Z,2020-02-21T13:53:06Z,,,
2486,b'Finding the right keras loss and metric for SQuAD',2020-01-10T10:22:15Z,2020-02-21T13:48:05Z,,,
2485,b'Adds UmBERTo: an Italian Language Model trained with Whole Word Masking',2020-01-10T10:01:11Z,2020-01-31T01:23:26Z,,,
2484,b'Import issues in run_squad_w_distillation',2020-01-10T05:01:58Z,2020-01-14T05:05:15Z,,ValueError,"ValueError: attempted relative import beyond top-level package"
2483,b'Removing pretrained layers?',2020-01-09T21:09:16Z,2020-01-11T17:34:39Z,,,
2482,b'model.generate should support past as an input',2020-01-09T20:59:51Z,2020-03-16T22:16:11Z,wontfix,,
2481,b'[closed] cls token in XLM',2020-01-09T20:27:42Z,2020-01-21T21:26:05Z,,,
2480,b'BERT add_token function not modify bias size',2020-01-09T19:55:25Z,2020-03-23T20:08:05Z,wontfix,,
2479,b'Implement Layer-wise Relevance Propagation (LRP) for prediction explanation',2020-01-09T19:53:07Z,2020-03-27T21:15:43Z,wontfix,,
2478,"b""ImportError: No module named 'transformers'""",2020-01-09T19:20:00Z,2020-03-21T13:19:20Z,wontfix,,
2477,"b'TFDistilBERT ValueError when loading a saved model and running model.predict(), same with any sequence classification model in tensorflow'",2020-01-09T18:34:17Z,2020-03-17T23:40:38Z,wontfix,,
2476,b'DistilBertTokenizer defaults to tokenize_chinese_chars=True',2020-01-09T18:32:24Z,2020-03-16T19:58:41Z,wontfix,,
2475,b'help...',2020-01-09T17:14:38Z,2020-03-20T10:20:58Z,wontfix,,
2474,"b""ALBERT tokenizer : local variable 'tokenizer' referenced before assignment""",2020-01-09T16:03:20Z,2020-03-16T19:58:42Z,wontfix,UnboundLocalError,"UnboundLocalError: local variable 'tokenizer' referenced before assignment"
2473,b'Using Transformer Library for code prediction',2020-01-09T12:47:25Z,2020-03-13T23:05:30Z,,,
2472,b'Pytorch T5 does not run on GPU',2020-01-09T12:36:16Z,2020-08-22T02:07:51Z,wontfix,RuntimeError,"RuntimeError: Expected object of device type cuda but got device type cpu for argument #3 'index' in call to _th_index_select"
2471,b'Using T5',2020-01-09T11:39:13Z,2020-03-23T20:08:04Z,wontfix,,
2470,b'How pipeline can use a  ner finetuned model from a local directory ?',2020-01-09T11:35:27Z,2020-03-23T12:08:04Z,"wontfix, Core: Pipeline",OSError,"OSError: Model name './1-out/checkpoint-24924/' was not found in model name list (xlm-roberta-base, ...)"
2469,b'Add PRETRAINED_INIT_CONFIGURATION to DistilBERT tokenizer',2020-01-09T11:15:05Z,2020-01-10T10:42:22Z,,,
2468,b'Error in BertForMaskedLM with add_tokens',2020-01-09T09:17:26Z,2020-01-17T06:41:15Z,,RuntimeError,"RuntimeError: The size of tensor a (119569) must match the size of tensor b (119547) at non-singleton dimension 2"
2467,b'Add japanese',2020-01-09T06:39:19Z,2020-01-09T06:39:40Z,,,
2466,b'GPT-2 XL PyTorch Quantization for use on a Cloud Server',2020-01-09T05:57:28Z,2020-03-24T11:36:57Z,wontfix,,
2465,b'Fix Tokenizer.from_pretrained `raise OSError`',2020-01-09T05:03:34Z,2020-01-09T11:54:30Z,,,
2464,"b'How to run the ""run_lm_finetuning.py"" with my own corpus?'",2020-01-09T04:11:46Z,2020-03-16T05:35:35Z,wontfix,,
2463,b'How to use GPU to do inference ?',2020-01-09T02:07:54Z,2020-01-09T03:20:53Z,,,
2462,b'TF2 version of Multilingual DistilBERT throws an exception  [TensorFlow 2]',2020-01-08T22:52:20Z,2020-02-14T21:43:11Z,,ValueError,"ValueError: Error when checking model target: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 8 array(s), but instead got the following list of 1 arrays: [<tf.Tensor 'ExpandDims:0' shape=(None, 1) dtype=int64>]..."
2461,"b""For Hugging Face transformer's hidden_states output, is the first hidden state tensor that is returned the out of the embeddings?""",2020-01-08T17:13:24Z,2020-01-09T16:31:12Z,,,
2460,b'Fine-tuning pretrained BERT model using own dataset but with same training task',2020-01-08T16:03:56Z,2020-03-17T08:54:57Z,wontfix,,
2459,b'Update pipelines.py',2020-01-08T15:42:49Z,2020-01-13T15:02:54Z,Core: Pipeline,,
2458,b'Update QA pipeline',2020-01-08T15:34:22Z,2020-01-08T15:40:36Z,,,
2457,b'New SQuAD API for distillation script',2020-01-08T15:16:13Z,2020-01-10T10:42:54Z,,,
2456,b'Adding usage example with Tensorflow',2020-01-08T14:50:05Z,2020-03-24T13:36:52Z,wontfix,,
2455,b'ROBERTa model wrong padding for token_type_ids field if return_tensors=True',2020-01-08T14:15:54Z,2020-03-31T01:04:13Z,wontfix,"RuntimeError, ValueError","RuntimeError: CUDA error: device-side assert triggeredValueError: Attempt to convert a value (tensor([[-1, -1, -1, -1, -1, -1,  0],"
2454,b'Add XLM-RoBERTa model for TF2',2020-01-08T13:26:11Z,2020-01-29T16:47:50Z,,,
2453,b'Installation of Transformers without Sacremoses',2020-01-08T13:23:45Z,2020-03-17T23:40:37Z,wontfix,,
2452,b'Remove redundant hidden states',2020-01-08T12:56:06Z,2020-02-04T15:59:33Z,,,
2451,b'Add check for token_type_ids before tensorizing',2020-01-08T12:33:33Z,2020-01-15T17:31:44Z,,,
2450,b'Error when running run_generation.py',2020-01-08T10:33:11Z,2020-01-14T12:19:57Z,,"PermissionError, OSError","PermissionError: [WinError 32] The process cannot access the file because it is being used by another process: 'C:\\Users\\GPT2\\.cache\\torch\\transformers\\tmpy2recb0u' -> 'C:\\Users\\GPT2\\.cache\\torch\\transformers\\f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71'OSError: Couldn't reach server at '{}' to download vocabulary files."
2449,b'Evaluation not working on distilbert-base-uncased-distilled-squad',2020-01-08T08:52:12Z,2020-01-09T06:25:50Z,,"""NoAns_exact"", ""NoAns_f1"", ""NoAns_total"", TypeError","""NoAns_exact"": 84.0874684608915,""NoAns_f1"": 84.0874684608915,""NoAns_total"": 5945TypeError: forward() got an unexpected keyword argument 'token_type_ids'"
2448,b'Tokenizer methods and padding',2020-01-08T07:59:20Z,2020-03-25T17:57:28Z,wontfix,,
2447,b'Reproducibility problem with DistilBERT paper ',2020-01-08T07:38:07Z,2020-01-09T03:14:16Z,,,
2446,b'RuntimeError: index out of range: Tried to access index 512 out of table with 511 rows.',2020-01-08T06:03:41Z,2020-07-15T03:26:18Z,wontfix,,
2445,b'Error occurs in XLMRobertaModel when token_type_ids is given.',2020-01-08T05:59:18Z,2020-01-08T08:28:43Z,,RuntimeError,"RuntimeError: index out of range: Tried to access index 1 out of table with 0 rows. at /pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp:418"
2444,b'Update',2020-01-08T05:31:03Z,2020-01-08T05:40:42Z,,,
2443,b'porting XLM-Roberta to tensorflow 2.0',2020-01-08T03:46:51Z,2020-01-13T06:15:26Z,,,
2442,b'loss_fct = CrossEntropyLoss(ignore_index=-1) for BERT/RoBERTa MaksedLM',2020-01-08T02:55:12Z,2020-03-15T14:35:38Z,wontfix,,
2441,b'is pytorch-pretrained-bert still being maintained in the future?',2020-01-07T23:57:14Z,2020-05-14T13:02:12Z,wontfix,,
2440,b'DistilBertForSequenceClassification returning nans',2020-01-07T23:30:09Z,2020-01-10T23:35:31Z,,,
2439,b'Generating text with fine-tuned TFGPT2LMHeadModel in python. ',2020-01-07T16:35:18Z,2020-01-07T19:11:13Z,,,
2438,b'Fix typograpical errors',2020-01-07T14:48:37Z,2020-01-07T16:21:23Z,,,
2437,b'Add CamemBERT model for TF2',2020-01-07T14:41:00Z,2020-01-29T17:06:14Z,,,
2436,b'Added repetition penalty to PPLM example',2020-01-07T14:11:31Z,2020-01-11T04:00:08Z,,,
2435,b'update the config.is_decoder=True before initialize the decoder',2020-01-07T14:02:30Z,2020-03-24T04:36:52Z,wontfix,,
2434,b'spelling correction',2020-01-07T13:59:04Z,2020-01-07T16:25:26Z,,,
2433,b'make test problem',2020-01-07T13:38:07Z,2020-01-07T14:06:49Z,,,
2432,b'Fix misleading RoBERTa token type ids',2020-01-07T12:47:48Z,2020-01-14T22:47:29Z,,,
2431,b'How can I fine-tune XLM for sentence classification?',2020-01-07T12:46:36Z,2020-04-26T23:31:06Z,wontfix,,
2430,b'T5_INPUTS_DOCSTRING correct!?',2020-01-07T12:31:51Z,2020-03-14T13:03:16Z,wontfix,,
2429,b'It occurs error when python run_lm_finetuning.py ',2020-01-07T11:55:09Z,2020-03-14T17:03:12Z,wontfix,**AttributeError,"**AttributeError: 'BertTokenizer' object has no attribute 'add_special_tokens_single_sequence'**"
2428,b'Padding part output in BERT NER task is not [PAD]?',2020-01-07T09:47:37Z,2020-03-14T10:03:15Z,wontfix,,
2427,b'ALBERT model does not work as expected',2020-01-07T09:43:53Z,2020-03-14T10:03:14Z,wontfix,,
2426,b'Make doc regarding masked indices more clear',2020-01-07T09:14:43Z,2020-01-07T16:37:28Z,,,
2425,b'Tokenize whole sentence vs. tokenize words in sentence then concat',2020-01-07T08:12:50Z,2020-01-08T01:16:42Z,,,
2424,"b'convert tf ckpt to pytorch_model.bin, load back model(TFBertModel), will loss params'",2020-01-07T07:01:38Z,2020-05-20T15:43:26Z,wontfix,,
2423,b'[DistillBERT] tokenizer issue of multilingual-cased',2020-01-07T05:51:35Z,2020-03-16T15:10:49Z,wontfix,,
2422,b'Is any possible for load local model ?',2020-01-07T03:13:42Z,2020-01-09T01:43:17Z,,,
2421,b'[Albert] SentencePiece Error with AlbertTokenizer',2020-01-07T00:48:48Z,2020-03-17T23:40:36Z,wontfix,RuntimeError,"RuntimeError: Internal: unk is not defined."
2420,"b""Bug Transformers 2.3.0 - ValueError: invalid literal for int() with base 10: 'pytorch'""",2020-01-07T00:45:35Z,2020-04-25T04:16:14Z,wontfix,ValueError,"ValueError: invalid literal for int() with base 10: 'pytorch'"
2419,b'Is there a way to reduce the vocabulary size?',2020-01-07T00:30:35Z,2020-03-14T02:09:42Z,wontfix,,
2418,b'Unclear documentation for indice masking',2020-01-06T15:50:24Z,2020-03-24T04:36:56Z,wontfix,RuntimeError,"RuntimeError: Assertion `cur_target >= 0 && cur_target < n_classes' failed.  at /pytorch/aten/src/THNN/generic/ClassNLLCriterion.c:97"
2417,b'Albert to torchscript is not working',2020-01-06T13:42:13Z,2020-03-14T14:03:13Z,wontfix,,
2416,b'Fixed answer structure for QAPipeline',2020-01-06T13:20:45Z,2020-01-13T16:48:44Z,Core: Pipeline,,
2415,b'greedy beam search generates same sequence N times',2020-01-06T12:32:26Z,2020-03-02T17:00:10Z,,,
2414,b'Serializing XLMRobertaTokenizer',2020-01-06T11:58:45Z,2020-01-21T15:18:25Z,,,
2413,"b'How to use transformers-cli serve , how to set up on the server side?'",2020-01-06T10:24:52Z,2020-01-16T02:11:24Z,Core: Pipeline,,
2412,b'Update Mish activation function to use torchscript JIT',2020-01-06T10:23:32Z,2020-03-13T12:09:39Z,wontfix,,
2411,"b'What is the difference between T5Model, T5WithLMHeadModel, T5PreTrainedModel?'",2020-01-06T07:01:32Z,2020-03-13T08:09:42Z,wontfix,,
2410,b'Typo in XLM moses pipeline.',2020-01-06T01:58:20Z,2020-01-07T14:20:53Z,,,
2409,b'Error in pipeline() when model left as None',2020-01-05T22:06:03Z,2020-03-13T16:09:42Z,"wontfix, Core: Pipeline",TypeError,"TypeError: string indices must be integers"
2408,"b""Can't download models or model config""",2020-01-05T16:31:42Z,2020-01-08T15:48:27Z,,,
2407,b'[cli] Add support for T5 model conversion',2020-01-05T14:31:40Z,2020-03-18T10:39:58Z,wontfix,,
2406,"b""BERT's Embedding/Vocab Size in Code is Different from Provided Pretrained Config""",2020-01-05T13:57:49Z,2020-01-12T13:54:58Z,,RuntimeError,"RuntimeError: Error(s) in loading state_dict for BertModel:"
2405,b'weird resize during the initialization in the PreTrainedModel',2020-01-05T12:35:10Z,2020-03-14T15:03:13Z,wontfix,,
2404,b'Pretrained Model not available',2020-01-05T12:23:29Z,2020-03-22T04:49:32Z,wontfix,,
2403,b'Add support for Albert and XLMRoberta for the Glue example',2020-01-05T10:14:16Z,2020-01-07T13:55:55Z,,,
2402,b'BertForTokenClassification can not from_pretrained the fine-tuned model?',2020-01-05T03:08:27Z,2020-01-09T03:18:20Z,,,
2401,b'Batch size affecting output.',2020-01-04T13:44:46Z,2020-03-19T12:54:07Z,wontfix,,
2400,b'fix #2399 an ImportError in official example',2020-01-04T13:20:20Z,2020-01-05T17:50:21Z,,,
2399,b'import Error from official example caused by fastprogress',2020-01-04T12:40:06Z,2020-01-05T17:50:22Z,,ImportError,"ImportError: cannot import name 'master_bar' from 'fastprogress' (/usr/local/lib/python3.7/dist-packages/fastprogress/__init__.py)"
2398,b'Distilbert predicting mask',2020-01-04T12:31:15Z,2020-01-05T16:53:55Z,,,
2397,b'unable to use distilbert multilingual model',2020-01-03T23:38:14Z,2020-03-14T15:03:14Z,wontfix,OSError,"OSError: Model name 'https://s3.amazonaws.com/models.huggingface.co/bert/distilbert-base-multilingual-cased-config.json' was not found in tokenizers model name list (distilbert-base-uncased, distilbert-base-uncased-distilled-squad). We assumed 'https://s3.amazonaws.com/models.huggingface.co/bert/distilbert-base-multilingual-cased-config.json' was a path or url to a directory containing vocabulary files named ['vocab.txt'] but couldn't find such vocabulary files at this path or url."
2396,b'Model2Model quickstart attention_mask dimensionality problem',2020-01-03T23:13:43Z,2020-03-09T17:50:36Z,wontfix,RuntimeError,"RuntimeError: The size of tensor a (8) must match the size of tensor b (768) at non-singleton dimension 3"
2395,b'ALBERT pretrained models uses wrong type of GELU activation',2020-01-03T21:17:01Z,2020-02-18T11:48:20Z,,,
2394,b'Pretrained model installation issue',2020-01-03T16:19:53Z,2020-03-11T20:30:33Z,wontfix,,
2393,b'run_squad_w_distillation update',2020-01-03T16:15:58Z,2020-03-14T12:03:13Z,wontfix,,
2392,b'Unable to download community models',2020-01-03T14:55:43Z,2020-01-15T15:43:59Z,,OSError,"OSError: Model name 'bert-base-cased-finetuned-conll03-english' was not found in model name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-japanese, bert-base-japanese-whole-word-masking, bert-base-japanese-char, bert-base-japanese-char-whole-word-masking, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). We assumed 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-finetuned-conll03-english/config.json' was a path or url to a configuration file named config.json or a directory containing such a file but couldn't find any such file at this path or url."
2391,b'What dataset was used for the NER results reported in the docs for bert/roberta-large-cased and distilbert-base-uncased models?',2020-01-03T01:11:23Z,2020-03-10T03:00:32Z,wontfix,,
2390,b'Pipelines support',2020-01-02T14:22:35Z,2020-01-09T13:28:49Z,Core: Pipeline,RuntimeError,"RuntimeError: Expected object of device type cuda but got device type cpu for argument #3 'index' in call to _th_index_select"
2389,b'update the config.is_decoder=True before initialize the decoder',2020-01-02T13:38:11Z,2020-01-07T13:27:22Z,,,
2388,"b""Can't load finetuned model properly.""",2020-01-02T12:56:04Z,2020-03-13T12:09:43Z,wontfix,,
2387,b'Pre-trained model returns different outputs(random outputs)',2020-01-02T12:17:56Z,2020-03-15T14:35:37Z,wontfix,,
2386,b'Different usage between BertModel and AlbertModel',2020-01-02T09:39:05Z,2020-01-14T01:55:45Z,,,
2385,b'The method os.rename() in file_utils.py  make a permissionError',2020-01-02T02:37:06Z,2020-01-04T02:51:20Z,,,
2384,b'Releasing file lock',2020-01-01T22:51:12Z,2020-01-14T12:19:02Z,,PermissionError,PermissionError: [WinError 32] The process cannot access the file because it is being used by another process: 'C:\\Users\\dimag\\.cache\\torch\\transformers\\tmpnhzxze8u' -> 'C:\\Users\\dimag\\.cache\\torch\\transformers\\26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084'
2383,b'clarification on output',2020-01-01T22:05:59Z,2020-01-01T22:33:41Z,,,
2382,b'Proposition to include community models in readme',2020-01-01T19:44:20Z,2020-01-05T17:37:12Z,,,
2381,b'how to use distilledgpt2',2020-01-01T15:49:02Z,2020-03-09T13:00:34Z,wontfix,,
2380,b'errors encountered with run_lm_finetuning.py',2020-01-01T15:29:54Z,2020-05-14T18:59:32Z,wontfix,"ValueError, RuntimeError","ValueError: num_samples should be a positive integeral value, but got num_samples=0RuntimeError: index out of range: Tried to access index 512 out of table with 511 rows. at /opt/conda/conda-bld/pytorch_1565272279342/work/aten/src/TH/generic/THTensorEvenMoreMath.cpp:237"
2379,b'finetune transformer',2020-01-01T10:11:01Z,2020-03-08T11:39:52Z,wontfix,,
2378,b'added pad_to_max_length option to batch_encode_plus',2019-12-31T17:48:44Z,2020-03-13T16:09:39Z,wontfix,,
2377,b'Text generation on GPU: Moved the encoded_prompt to correct device',2019-12-31T12:27:11Z,2020-01-06T14:11:13Z,,,
2376,b'Classification of sentence pair with two different languages',2019-12-31T12:23:22Z,2020-03-07T13:57:18Z,wontfix,,
2375,b'Is the position of the scheduler.step() correct?',2019-12-31T08:02:55Z,2019-12-31T08:23:46Z,,,
2374,b'Fine-tuning BertAbs on new dataset?',2019-12-31T03:09:31Z,2020-03-03T15:36:35Z,wontfix,,
2373,b'RuntimeError: The size of tensor a (30524) must match the size of tensor b (30522) at non-singleton dimension 2 --- run_lm_finetuning.py',2019-12-31T01:32:45Z,2020-03-14T03:09:42Z,wontfix,RuntimeError,"RuntimeError: The size of tensor a (30524) must match the size of tensor b (30522) at non-singleton dimension 2"
2372,"b'What is the ""could not find answer"" warning in squad.py'",2019-12-30T22:31:58Z,2020-03-14T14:03:12Z,wontfix,,
2371,"b'Encounter an ""index out of range problem""'",2019-12-30T12:30:39Z,2019-12-30T14:35:02Z,,RuntimeError,"RuntimeError: index out of range: Tried to access index 512 out of table with 511 rows. at C:\w\1\s\tmp_conda_3.6_111945\conda\conda-bld\pytorch_1572952852006\work\aten\src\TH/generic/THTensorEvenMoreMath.cpp:418"
2370,b'Pipelines: add PoS support',2019-12-30T11:08:48Z,2020-03-06T20:54:02Z,"wontfix, Core: Pipeline",,
2369,b'few changes due to the torch version inconsistency in summarization example',2019-12-30T10:37:17Z,2020-03-11T13:30:29Z,wontfix,,
2368,b'Clarification regarding past/layer_past in GPT-2',2019-12-30T07:29:42Z,2020-03-13T08:51:08Z,wontfix,,
2367,b'Load the google bert model(ckpt) from TFBertForPreTraining error',2019-12-30T06:17:59Z,2020-03-06T08:25:19Z,wontfix,,
2366,b'How Can I load the google bert model(ckpt)? ',2019-12-30T04:35:31Z,2020-03-06T08:25:18Z,wontfix,,
2365,"b""upgrading new transformer doesn't work""",2019-12-29T21:23:57Z,2019-12-31T02:06:32Z,,ImportError,"ImportError: cannot import name 'BertTokenizer' from 'transformers' (unknown location)"
2364,b'How to fine-tune PreTrainedEncoderDecoder on new dataset?',2019-12-29T18:17:14Z,2020-05-21T07:52:56Z,wontfix,,
2363,b'Finetuning on several tasks ',2019-12-29T16:47:10Z,2020-03-06T01:29:45Z,wontfix,RuntimeError,"RuntimeError: Error(s) in loading state_dict for BertForSequenceClassification:"
2362,b'Why albert has a print statement during forward?',2019-12-29T15:10:34Z,2020-03-13T15:09:46Z,wontfix,,
2361,b'Improve logging message in feature conversion functions',2019-12-29T12:36:56Z,2020-01-06T13:54:37Z,,,
2360,"b""CTRL - RuntimeError: Expected object of backend CUDA but got backend CPU for argument #3 'index'""",2019-12-29T08:55:27Z,2019-12-29T09:20:15Z,,"`RuntimeError, RuntimeError","`RuntimeError: Expected object of backend CUDA but got backend CPU for argument #3 'index'`RuntimeError: Expected object of backend CUDA but got backend CPU for argument #3 'index'"
2359,b'Confusion about the target_mapping parameter of the xlnet model',2019-12-29T08:20:11Z,2020-03-05T09:29:45Z,wontfix,,
2358,b'Quickstart BERT Example: Assertion Error',2019-12-29T07:28:52Z,2019-12-29T07:46:54Z,,,
2357,b'GLUE benchmark score for XLNet_base_cased?',2019-12-29T03:32:27Z,2020-03-13T15:09:44Z,wontfix,,
2356,b'GPT2 should not store/compute cached activations during finetuning',2019-12-28T16:34:21Z,2020-03-11T13:30:30Z,wontfix,,
2355,b'transformers command not found after installing transformers using pip',2019-12-28T07:35:02Z,2020-01-02T18:20:48Z,,,
2354,"b'[debug] Debug Heisenbug, the old school way.'",2019-12-28T05:01:15Z,2019-12-29T15:07:22Z,,,
2353,b'[http] Tweak http user-agent',2019-12-28T04:46:07Z,2019-12-29T15:06:51Z,,,
2352,b'Cli tweaks',2019-12-28T04:22:14Z,2019-12-28T14:40:01Z,,,
2351,b'GLUE Benchmark Hyperparameters',2019-12-28T00:12:42Z,2020-03-13T15:09:43Z,wontfix,,
2350,b'Trouble fine tuning BERT language model ',2019-12-27T23:16:19Z,2020-01-30T07:54:24Z,,RuntimeError,"RuntimeError: Assertion `cur_target >= 0 && cur_target < n_classes' failed.  at /pytorch/aten/src/THNN/generic/ClassNLLCriterion.c:97"
2349,b'Enforce target version for black.',2019-12-27T21:49:14Z,2020-01-05T17:52:15Z,,,
2348,b'CamembertForQuestionAnswering',2019-12-27T21:26:47Z,2020-03-04T19:53:58Z,wontfix,,
2347,b'revise T5 code to support one step decoding during generation',2019-12-27T19:33:44Z,2020-03-11T13:30:31Z,wontfix,,
2346,b'Why does the BertForQuestionAnswering sample code duplicate the [CLS] token?',2019-12-27T15:56:48Z,2020-03-13T15:09:43Z,wontfix,,
2345,b'Feature Request: Pipeline for Query/Document relevance',2019-12-27T15:44:38Z,2020-05-13T03:17:52Z,"wontfix, Core: Pipeline",,
2344,b'How to run bert without checkpoints',2019-12-27T14:35:26Z,2020-03-03T16:24:02Z,wontfix,,
2343,b'How to finetune PreTrainedEncoderDecoder',2019-12-27T12:34:52Z,2020-03-09T05:39:52Z,wontfix,,
2342,b'Tokenizers as optional dependency',2019-12-27T12:15:53Z,2020-03-11T13:30:32Z,wontfix,,
2341,"b'""Reformer: The Efficient Transformer"" looks awesome. I\'d love to see it in the library.'",2019-12-27T06:29:22Z,2020-05-22T19:00:26Z,wontfix,,
2340,b'Bert cross attention',2019-12-27T05:37:53Z,2020-01-14T14:12:17Z,,,
2339,"b'read each lines, require less memory'",2019-12-27T04:51:17Z,2020-01-31T05:27:11Z,,,
2338,"b""Summarization ROGUE scores don't equal that of the paper ...""",2019-12-26T23:46:57Z,2020-03-03T02:24:05Z,wontfix,,
2337,b'Dropout rates to be updated in all ALBERT v2 configs',2019-12-26T21:04:17Z,2020-02-28T22:30:13Z,wontfix,,
2336,"b'TypeError: Expected Operation, Variable, or Tensor, got None while saving tensorflow model'",2019-12-26T18:56:32Z,2020-03-02T20:53:48Z,wontfix,TypeError,"TypeError: Expected Operation, Variable, or Tensor, got None"
2335,b'XLNet and RoBERTa embeddings',2019-12-26T18:31:55Z,2020-03-02T20:53:49Z,wontfix,,
2334,b'relativeattentionbias.weight in block 0 EncDecAttention of T5 Model not in original tf model. Where do we get it from?',2019-12-26T17:45:11Z,2020-03-29T21:01:21Z,wontfix,,
2333,"b""Add 'keep_accents' flag to basic tokenizer""",2019-12-26T17:22:47Z,2020-03-22T20:49:28Z,wontfix,,
2332,"b""What does 'output of the embeddings' mean?""",2019-12-26T16:53:34Z,2020-01-08T17:11:44Z,,,
2331,b'Learning Rate is not being updated by the Scheduler',2019-12-26T16:49:16Z,2020-03-09T05:39:51Z,wontfix,,
2330,b'BERT adapted to time series',2019-12-26T16:43:13Z,2020-03-02T17:43:40Z,wontfix,,
2329,b'refactoring the code',2019-12-26T14:26:41Z,2019-12-26T14:28:50Z,,,
2328,b'Refactoring the code ',2019-12-26T13:51:51Z,2019-12-26T14:16:41Z,,,
2327,b'load_and_cache_examples crashes on windows',2019-12-26T13:39:19Z,2020-03-02T14:59:19Z,wontfix,,
2326,b'run_generation.py gives TypeError when using xlnet due to empty dict being passed as token',2019-12-26T13:01:22Z,2020-04-12T22:12:48Z,wontfix,TypeError,"TypeError: unhashable type: 'dict'"
2325,b'How to make FP16 quantization on gpt/xl?',2019-12-26T12:27:49Z,2020-01-02T16:49:59Z,,`ValueError,`ValueError: Message tensorflow.GraphDef exceeds maximum protobuf size of 2GB: 6234365906`
2324,b'Typo in serving.py',2019-12-26T11:21:48Z,2019-12-26T11:38:07Z,,,
2323,b'Where does the pre-trained bert model gets cached in my system by default? ',2019-12-26T09:31:29Z,2020-01-01T20:11:42Z,,,
2322,"b'I am getting repetitive output when running ""python run_generation.py""'",2019-12-26T07:46:52Z,2020-02-25T07:34:53Z,wontfix,,
2321,b'Bert Decoder using is_decoder and encoder_hidden_states',2019-12-26T06:53:34Z,2020-03-13T12:09:44Z,wontfix,RuntimeError,"RuntimeError: expected device cpu and dtype Float but got device cpu and dtype Bool"
2320,"b'how to do a simple multi-classifier by bert 2.0,training set ,and label set all lines'",2019-12-26T03:34:55Z,2020-03-02T06:19:41Z,wontfix,,
2319,"b""help: couldn't find such vocabulary files at this path or url""",2019-12-26T03:21:04Z,2019-12-26T06:51:07Z,,,
2318,b'How can I read my bert model by using transformers?',2019-12-26T01:54:30Z,2019-12-26T13:10:03Z,,,
2317,b'Fix beam search when sampling in language generation',2019-12-25T23:21:18Z,2020-03-03T15:07:08Z,,,
2316,b'Delete [dev] behind pip install -e .',2019-12-25T22:54:33Z,2019-12-27T07:51:23Z,,,
2315,b'Add hint to install pytest-xdist',2019-12-25T22:50:31Z,2019-12-26T20:51:01Z,,,
2314,b'Is there a uncased gpt2?',2019-12-25T21:42:18Z,2020-01-06T15:00:52Z,,,
2313,b'Add dropout to WordpieceTokenizer and BPE',2019-12-25T17:33:46Z,2020-03-17T23:40:30Z,wontfix,,
2312,b'Correct tokenization for special and added tokens',2019-12-25T16:36:29Z,2019-12-25T19:52:31Z,,,
2311,b'Can I use BERT / gpt-2 for text generation',2019-12-25T13:31:38Z,2020-03-02T12:33:58Z,wontfix,,
2310,b'revert erroneous fix #2276',2019-12-25T06:30:18Z,2019-12-25T11:43:23Z,,,
2309,b'Bug: Tokenization of Special Tokens',2019-12-25T04:47:16Z,2019-12-25T20:01:11Z,,,
2308,b'pytorch_pretrained_bert giving different scores for BertForNextSentencePrediction ',2019-12-25T03:12:11Z,2019-12-30T08:13:04Z,,,
2307,"b""What's the exact name of BERT large in results ( GermEval 2014)?""",2019-12-25T01:10:20Z,2020-03-01T01:59:59Z,wontfix,,
2306,b'Non-Deterministic Behavior in BertTokenizer',2019-12-25T00:45:25Z,2020-03-17T21:52:35Z,,,
2305,b'[CLS] token /  is used as the aggregate sequence representation for classification tasks',2019-12-25T00:27:26Z,2020-03-02T14:59:19Z,wontfix,,
2304,b'Why are you getting just the last encoder states in the summarization code?',2019-12-24T22:25:13Z,2020-03-01T00:00:00Z,wontfix,,
2303,b'fix repetition penalty error in modeling_utils.py',2019-12-24T16:19:18Z,2019-12-25T21:39:20Z,,,
2302,b'Repetition penalty work falsely in case the logit of the token is negativ',2019-12-24T16:18:14Z,2019-12-25T23:22:38Z,,,
2301,b'Can I use run_lm_finetuning.py for training models in an uncovered language?',2019-12-24T13:27:03Z,2020-02-14T22:26:14Z,,,
2300,"b'run_ner.py RobertaForTokenClassification.from_pretrained ""size mismatch for classifier.bias""'",2019-12-24T12:27:36Z,2019-12-31T10:42:43Z,,RuntimeError,"RuntimeError: Error(s) in loading state_dict for RobertaForTokenClassification:"
2299,b'Model2Model inference',2019-12-24T08:58:54Z,2020-02-29T09:59:59Z,wontfix,,
2298,"b'Why cosine similarity of BERT, ALBERT, Robert is so big, almost near 1.0?'",2019-12-24T08:28:37Z,2020-03-08T11:39:51Z,wontfix,,
2297,"b'RunTimeError in ""run_summarization"": expected device cuda:0 and dtype byte but got device cuda: 0 and dtype Bool'",2019-12-24T07:53:37Z,2019-12-31T01:01:17Z,,,
2296,b'A question about BERT position embedding.',2019-12-24T03:26:42Z,2019-12-24T05:24:07Z,,,
2295,b'How do you handle large documents?',2019-12-24T02:06:55Z,2020-03-13T22:09:42Z,wontfix,,
2294,b'Is there any efficient way to convert BERT outputs to fit token-level tasks?',2019-12-24T01:54:33Z,2019-12-28T16:16:51Z,,,
2293,b'Train custom NER model with new Pipeline',2019-12-23T23:39:26Z,2020-03-13T14:09:45Z,"wontfix, Core: Pipeline",,
2292,b'Add cached past for language generation',2019-12-23T22:10:35Z,2019-12-27T09:33:27Z,,,
2291,b'Fix F841 flake8 warning',2019-12-23T21:42:39Z,2019-12-25T21:37:43Z,,,
2290,b'duplicated line for repeating_words_penalty_for_language_generation',2019-12-23T20:51:17Z,2019-12-27T09:29:07Z,,,
2289,b'fix bug in prepare inputs for language generation for xlm for effective batch_size > 1',2019-12-23T20:46:49Z,2019-12-25T21:30:47Z,,,
2288,b'Improve handling of optional imports',2019-12-23T20:30:53Z,2019-12-23T21:28:47Z,,,
2287,b'Do Hugging Face GPT-2 Transformer Models Automatically Does the Absolute Position Embedding for Users?',2019-12-23T18:42:50Z,2020-01-06T13:53:24Z,,,
2286,b'Typo in tokenization_utils.py',2019-12-23T17:15:01Z,2019-12-27T09:50:47Z,,,
2285,b'BertTokenizer custom UNK unexpected behavior',2019-12-23T15:12:29Z,2020-02-28T15:28:26Z,wontfix,,
2284,b'[ALBERT]: Albert base model itself consuming 32 GB GPU memory.. ',2019-12-23T14:48:14Z,2020-03-14T12:03:12Z,wontfix,,
2283,b'Loading sciBERT failed',2019-12-23T14:20:53Z,2019-12-23T15:06:45Z,,`-OSError,"`-OSError: Model name 'pretrain/scibert-uncased' was not found in model name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). We assumed 'pretrain/scibert-uncased/config.json' was a path or url to a configuration file named config.json or a directory containing such a file but couldn't find any such file at this path or url.`"
2282,b'Maybe some parameters are error in document for distributed training ?',2019-12-23T13:08:03Z,2019-12-24T02:59:17Z,,,
2281,b'Add Dutch pre-trained BERT model',2019-12-23T12:40:56Z,2020-01-28T02:00:35Z,,,
2280,b'Do anyone have solution for this',2019-12-23T10:50:38Z,2019-12-23T10:51:23Z,,,
2279,b'Help with finetune BERT pretraining ',2019-12-23T10:35:49Z,2020-02-14T22:40:36Z,,,
2278,b'where is the script of a second step of  knwoledge distillation on SQuAD 1.0?',2019-12-23T09:13:26Z,2020-02-28T10:28:27Z,wontfix,,
2277,b'Does the calling order need to be changed?',2019-12-23T08:30:10Z,2020-02-28T00:51:34Z,wontfix,,
2276,b'fix error due to wrong argument name to Tensor.scatter()',2019-12-23T07:19:41Z,2019-12-23T11:19:49Z,,,
2275,"b'Gpt2/xl Broken on ""Write With Transformer"" site'",2019-12-23T05:26:28Z,2020-01-22T19:38:40Z,,,
2274,"b""AttributeError: 'GPT2LMHeadModel' object has no attribute 'generate'""",2019-12-23T03:05:15Z,2020-03-13T13:09:42Z,wontfix,AttributeError,"AttributeError: 'GPT2LMHeadModel' object has no attribute 'generate'"
2273,b'adding special tokens after truncating in run_lm_finetuning.py ',2019-12-22T23:28:49Z,2020-03-09T13:00:33Z,wontfix,,
2272,b'Run_tf_ner.py error on TPU ',2019-12-22T23:19:29Z,2020-02-28T07:26:09Z,wontfix,`AttributeError,"`AttributeError: 'device_map' not accessible within a TPU context.`"
2271,b'Improve setup and requirements',2019-12-22T19:39:04Z,2019-12-24T10:21:20Z,,,
2270,b'Remove support for Python 2',2019-12-22T17:23:35Z,2019-12-22T22:04:38Z,,,
2269,b'Bad F1 Score for run_squad.py on SQuAD2.0',2019-12-22T13:22:27Z,2019-12-22T14:33:05Z,,,
2268,b'Improve repository structure',2019-12-22T13:00:05Z,2019-12-22T15:41:54Z,,,
2267,b'Does Pre-Trained Weights Work Internally in pytorch? ',2019-12-22T10:26:42Z,2020-02-22T18:51:15Z,wontfix,,
2266,b'Imports likely broken in examples',2019-12-22T10:24:04Z,2020-03-20T15:20:59Z,wontfix,,
2265,b'Only the Bert model is currently supported',2019-12-22T09:24:32Z,2020-01-03T04:10:02Z,,,
2264,b'Fix doc link in README',2019-12-22T07:12:24Z,2019-12-23T11:31:01Z,,,
2263,b'BertModel sometimes produces the same output during evaluation',2019-12-22T07:00:45Z,2020-03-30T23:04:13Z,wontfix,,
2262,b'How to do_predict on run_glue?',2019-12-22T02:45:02Z,2020-04-26T19:31:06Z,wontfix,,
2261,"b""AlbertTokenizer behavior doesn't match docs""",2019-12-22T00:59:49Z,2020-03-03T16:24:03Z,wontfix,ValueError,"ValueError: 102 is not in list"
2260,b'Fixing incorrect link in model docstring',2019-12-21T21:02:07Z,2020-01-15T23:25:39Z,,,
2259,"b'problem in the doc, in the  ""Quick Start"" GPT2 example'",2019-12-21T20:20:17Z,2020-02-27T01:31:18Z,wontfix,,
2258,b'run_ner.py load checkpoint issue',2019-12-21T19:27:54Z,2020-04-25T16:15:59Z,wontfix,ValueError,"ValueError: invalid literal for int() with base 10: 'pytorch_dump_folder'`"
2257,b'HuggingFace transformers documentation webpage is blank?',2019-12-21T19:09:36Z,2019-12-23T18:38:29Z,,,
2256,"b'Untrainable dense layer in TFBert. ""WARNING:tensorflow:Gradients do not exist for variables [\'tf_bert_model/bert/pooler/dense/kernel:0\', \'tf_bert_model/bert/pooler/dense/bias:0\'] when minimizing the loss.""'",2019-12-21T18:53:37Z,2019-12-22T05:54:42Z,,,
2255,b'Implement some Python best practices',2019-12-21T14:53:02Z,2019-12-22T15:31:12Z,,,
2254,b'adding positional embeds masking to TFRoBERTa',2019-12-21T14:27:11Z,2019-12-21T14:33:23Z,,,
2253,b'bias weights not used in T5Model',2019-12-21T11:02:53Z,2020-01-08T16:06:37Z,,,
2252,b'Documentation link broken',2019-12-21T10:42:14Z,2019-12-23T11:31:01Z,,,
2251,"b""AttributeError: 'Sst2Processor' object has no attribute 'tfds_map'""",2019-12-21T08:31:11Z,2020-03-13T15:09:45Z,wontfix,AttributeError,"AttributeError: 'Sst2Processor' object has no attribute 'tfds_map'"
2250,b'Four tests fail when running the full test suite',2019-12-21T07:41:53Z,2020-01-08T08:40:56Z,,"FileNotFoundError, OSError, NameError, AssertionError","FileNotFoundError: [Errno 2] No such file or directory: '/var/folders/pq/hzv7wgqs5fq0hf1bwzy4mlzr0000gn/T/transformers_test/5b4c66df217ea00b14f607787de616bbff332ae36147a92cd94219160006685a'OSError: Model name 'albert-base-uncased' was not found in model name list (albert-xxlarge-v2, albert-large-v1, albert-xlarge-v1, albert-base-v2, albert-base-v1, albert-large-v2, albert-xlarge-v2, albert-xxlarge-v1). We assumed 'https://s3.amazonaws.com/models.huggingface.co/bert/albert-base-uncased/config.json' was a path or url to a configuration file named config.json or a directory containing such a file but couldn't find any such file at this path or url.NameError: name 'XLMModel' is not definedAssertionError: False is not true"
2249,b'bert\xef\xbc\x88+lstm\xef\xbc\x89+crf',2019-12-21T06:24:43Z,2020-08-14T09:39:37Z,wontfix,,
2248,b'Extract features aligned to tokens from a BertForQuestionAnswering model',2019-12-21T05:00:37Z,2019-12-21T20:44:54Z,,,
2247,b'NER pipeline missing start/end',2019-12-21T02:46:41Z,2020-03-13T14:09:44Z,"wontfix, Core: Pipeline",,
2246,b'Recently added pipelines tests should be marked as slow',2019-12-20T20:28:32Z,2020-02-26T19:49:54Z,wontfix,,
2245,b'Training dataset is not available',2019-12-20T20:07:56Z,2020-04-25T21:15:57Z,wontfix,,
2244,b'Fix Camembert and XLM-R `decode` method- Fix NER pipeline alignement',2019-12-20T19:45:09Z,2019-12-20T21:10:40Z,,,
2243,b'fixing xlm-roberta tokenizer max_length and automodels',2019-12-20T18:02:20Z,2019-12-20T18:34:09Z,,,
2242,b'BertTokenizer / CamemBERTokenizer `decode` behaviour ?',2019-12-20T15:19:10Z,2019-12-30T10:11:44Z,,TypeError,"TypeError: unsupported operand type(s) for -: 'str' and 'int'"
2241,b'How to load the finetuned model for retraining from checkpoints in run_squad.py?',2019-12-20T14:33:32Z,2020-02-27T07:04:32Z,wontfix,OSError,"OSError: Model name '../data/tmp/attempt_with_dataset_v3/checkpoint-5000/' was not found in tokenizers model name list (distilbert-base-uncased, distilbert-base-uncased-distilled-squad, distilbert-base-german-cased, distilbert-base-multilingual-cased). We assumed '../data/tmp/attempt_with_dataset_v3/checkpoint-5000/' was a path or url to a directory containing vocabulary files named ['vocab.txt'] but couldn't find such vocabulary files at this path or url."
2240,b'TFDistilBertModelTest.test_pt_tf_model_equivalence thrown while merging after PR',2019-12-20T14:25:51Z,2020-04-14T23:25:51Z,wontfix,,
2239,b'HANS evaluation',2019-12-20T13:38:01Z,2020-01-16T12:28:26Z,,,
2238,b'Readme installation/test order can lead to confusion when running example unit tests',2019-12-20T11:19:33Z,2020-01-01T20:13:50Z,,,
2237,b'Fix out-of-date comments in Transformers examples directory',2019-12-20T10:55:06Z,2019-12-27T09:51:09Z,,,
2236,b'Removing redundant model weights',2019-12-20T10:55:05Z,2020-02-25T16:06:59Z,wontfix,,
2235,b'add example for Model2Model in quickstart',2019-12-20T10:14:34Z,2019-12-20T14:12:33Z,,,
2234,b'Supoort loading model weights from a single file.',2019-12-20T07:41:59Z,2020-02-18T14:55:39Z,wontfix,,
2233,b'The code used to be clean...',2019-12-20T06:59:42Z,2019-12-22T06:25:31Z,,,
2232,b'Keep even the first of the special tokens intact while lowercasing',2019-12-20T01:05:39Z,2019-12-20T16:29:45Z,,,
2231,b'[http] customizable requests user-agent',2019-12-19T23:30:04Z,2019-12-20T09:28:10Z,,,
2230,"b""what is the most efficient way to store all hidden layers' weights?""",2019-12-19T19:41:00Z,2020-02-24T20:38:46Z,wontfix,,
2229,b'Minor/basic text fixes',2019-12-19T17:37:00Z,2019-12-19T21:23:19Z,,,
2228,b'Trouble loading Albert model',2019-12-19T16:47:23Z,2019-12-19T18:34:59Z,,RuntimeError,"RuntimeError: Internal: C:\projects\sentencepiece\src\sentencepiece_processor.cc(73) [model_proto->ParseFromArray(serialized.data(), serialized.size())] "
2227,"b'Add ""Train on Valohai"" buttons to README'",2019-12-19T15:12:55Z,2020-03-11T13:30:34Z,wontfix,,
2226,b'[REVIEW] Updated a out-of-date comment in run_lm_finetuning.py',2019-12-19T14:24:42Z,2019-12-19T14:27:08Z,,,
2225,b'[REVIEW] Updated comments in run_lm_finetuning.py',2019-12-19T14:02:39Z,2019-12-19T14:13:11Z,,,
2224,"b'[REVIEW] Removed duplicate XLMConfig, XLMForQuestionAnswering and XLMTokenizer in run_squad.py'",2019-12-19T13:49:37Z,2019-12-19T14:50:57Z,,,
2223,b'Need pretrained XLNET on Squad which can be loaded from_pre_trained',2019-12-19T12:21:10Z,2020-02-24T13:38:49Z,wontfix,,
2222,b'Can we remove force_download=True from tests?',2019-12-19T08:36:39Z,2019-12-21T10:54:31Z,,,
2221,b'Updated typo on the link',2019-12-19T08:14:33Z,2019-12-19T14:36:45Z,,,
2220,b'tokenizer of bert-base-uncased gives an incorrect split',2019-12-19T01:40:57Z,2019-12-20T16:29:44Z,,,
2219,"b""When i run the script run_tf_ner.py, i got ValueError: Expected floating point type, got <dtype: 'int32'>.""",2019-12-19T01:10:09Z,2020-02-24T13:38:48Z,wontfix,ValueError,"ValueError: Expected floating point type, got <dtype: 'int32'>."
2218,b'corrected typo in example for t5 model input argument',2019-12-19T00:36:25Z,2019-12-19T14:34:56Z,,,
2217,b'Support running tests in parallel',2019-12-18T21:58:06Z,2019-12-21T10:54:24Z,,,
2216,b'Error while loading Pretrained Enocder and Decoder transformers ',2019-12-18T18:52:37Z,2020-02-25T12:51:25Z,wontfix,ValueError,"ValueError: Unrecognized model identifier in /models/m2m/. Should contains one of 'bert', 'openai-gpt', 'gpt2', 'transfo-xl', 'xlnet', 'xlm', 'roberta, 'ctrl'"
2215,b'Return overflowing tokens if max_length is not given',2019-12-18T16:55:21Z,2020-02-23T17:52:32Z,wontfix,,
2214,b'XLM run_squad errors with size mismatch',2019-12-18T15:07:40Z,2019-12-18T16:23:52Z,,RuntimeError,"RuntimeError: Error(s) in loading state_dict for XLMForQuestionAnswering:"
2213,b'T5 - Finetuning of an EncoderDecoder Model',2019-12-18T13:10:32Z,2020-02-23T17:52:31Z,wontfix,,
2212,b'Fine-tuning TF models on Colab TPU',2019-12-18T08:20:39Z,2020-04-14T07:25:51Z,wontfix,,
2211,b'Fast tokenizers',2019-12-18T00:06:40Z,2019-12-27T09:24:30Z,,,
2210,b'training a new BERT tokenizer model',2019-12-17T23:51:52Z,2020-01-13T18:38:37Z,,,
2209,b'```glue_convert_examples_to_features``` for sequence labeling tasks',2019-12-17T19:28:06Z,2019-12-17T19:41:38Z,,,
2208,b'```glue_convert_examples_to_features``` for sequence labeling tasks',2019-12-17T19:17:04Z,2020-02-24T13:38:47Z,wontfix,,
2207,b'Fix segmentation fault',2019-12-17T19:02:42Z,2019-12-17T20:54:06Z,,,
2206,b'Transformers Encoder and Decoder Inference',2019-12-17T17:02:57Z,2019-12-18T18:12:07Z,,TypeError,"TypeError: forward() missing 1 required positional argument: 'decoder_input_ids'"
2205,b'Segmentation fault when GPT2-chinese import transformers',2019-12-17T16:41:27Z,2019-12-18T15:42:54Z,,,
2204,b'PRs error which occurs many times in the last days',2019-12-17T16:36:26Z,2019-12-18T14:36:11Z,,,
2203,b'fix: wrong architecture count in README',2019-12-17T16:18:20Z,2019-12-21T13:31:45Z,,,
2202,b'weights not initialised in pre-trained Roberta',2019-12-17T14:31:54Z,2019-12-19T20:49:47Z,,,
2201,"b""[WAITING YOUR REVIEW] Issue #2196: now it's possible to save PreTrainedEncoderDecoder objects correctly to file system""",2019-12-17T09:33:25Z,2019-12-20T22:21:25Z,,,
2200,b'run_ner.py example fails',2019-12-17T08:38:51Z,2019-12-17T11:32:17Z,,json.decoder.JSONDecodeError,"json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)"
2199,b'How to add traditional features for transformers?',2019-12-17T08:38:09Z,2019-12-18T09:04:56Z,,,
2198,b'How to output labels for GLUE test set',2019-12-17T08:14:56Z,2020-02-17T05:18:01Z,wontfix,,
2197,b'XLNet fine-tuning speed (Multi-label classification)',2019-12-17T05:46:55Z,2020-02-22T11:13:16Z,wontfix,,
2196,b'Error while saving Pretrained model for Encoder and decoder',2019-12-17T04:10:13Z,2019-12-17T17:11:01Z,,AssertionError,"AssertionError: Saving path should be a directory where the model and configuration can be saved"
2195,"b'Fixing checks test pr, will be closed'",2019-12-17T04:05:25Z,2019-12-17T04:07:39Z,,,
2194,b'Improve TextDataset building/tokenization (6x faster; Enable large dataset file usage)',2019-12-17T02:47:00Z,2020-03-11T13:30:33Z,wontfix,,
2193,b'Fine-tuning GPT2 or BERT and adding new vocabulary?',2019-12-16T22:11:33Z,2019-12-17T14:50:06Z,,,
2192,b'Bug fix: PyTorch loading from TF and vice-versa',2019-12-16T21:33:19Z,2019-12-16T21:58:45Z,,,
2191,b'Numpy compatibility for sentence piece',2019-12-16T20:32:02Z,2019-12-20T21:08:09Z,,,
2190,b'Adding Finnish BERT.',2019-12-16T17:24:32Z,2019-12-18T01:35:26Z,,,
2189,b'Add support for XLM-RoBERTa',2019-12-16T16:13:45Z,2019-12-20T12:26:39Z,,,
2188,b'About QuestionAnswering on SQuAD2.0 Dataset',2019-12-16T12:46:21Z,2020-02-21T14:15:48Z,wontfix,,
2187,b'Output diverging on different GPUs using same prompt?',2019-12-16T04:36:08Z,2020-02-21T21:15:49Z,wontfix,,
2186,b'summarization code is incomplete',2019-12-15T20:52:08Z,2020-02-22T00:15:48Z,wontfix,,
2185,b'RuntimeError: CUDA error: device-side assert triggered when using Roberta',2019-12-15T19:28:02Z,2020-03-13T11:09:44Z,wontfix,RuntimeError,"RuntimeError: CUDA error: device-side assert triggered"
2184,"b'T5Tokenizer: Using cls_token, but it is not set yet.'",2019-12-15T19:25:08Z,2020-02-22T11:13:15Z,wontfix,,
2183,b'Unit of the prediction scores of a language model',2019-12-15T16:16:37Z,2020-02-22T15:13:14Z,wontfix,,
2182,b'sts-b task score is far worse than other GLUE tasks',2019-12-15T13:35:50Z,2020-02-20T15:04:35Z,wontfix,,
2181,b'Conda version is not the latest',2019-12-15T08:32:30Z,2020-02-21T19:15:49Z,wontfix,,
2180,b'Pretty sure patch in Pull Request #1313 is incorrect',2019-12-15T08:07:51Z,2019-12-16T21:44:45Z,,,
2179,b'Should I always use bert as a teacher to distillation distilbert as a student?',2019-12-15T04:29:22Z,2019-12-18T12:34:08Z,,,
2178,b'Tokenize with offsets',2019-12-15T03:34:40Z,2020-05-03T06:06:56Z,wontfix,,
2177,b':zip: #2106 tokenizer.tokenize speed improvement (3-8x) by caching added_tokens in a Set',2019-12-14T14:45:11Z,2019-12-21T13:31:21Z,,,
2176,b'run_squad.py for SQuAD2.0 have bad f1 score',2019-12-14T13:14:53Z,2019-12-26T06:49:04Z,,,
2175,b'merge new version',2019-12-14T12:16:23Z,2019-12-14T12:17:43Z,,,
2174,b'RobertaTokenizer token type issue',2019-12-14T09:29:15Z,2020-02-19T11:11:49Z,wontfix,,
2173,b'run_squad with roberta',2019-12-14T01:16:59Z,2019-12-21T13:33:17Z,,,
2172,b'[cli] Upload is now compatible with folders',2019-12-13T21:10:25Z,2019-12-13T21:39:09Z,,,
2171,"b'Small run_squad nit: eliminate trailing ""_"" in ""best_predictions_.json"" when no prefix'",2019-12-13T20:20:08Z,2020-02-18T21:11:48Z,wontfix,,
2170,b'BertForSequenceClassification() model TF to pytorch conversion',2019-12-13T19:46:59Z,2020-03-20T11:20:53Z,wontfix,,
2169,b'How to structure input data for training TFGPT2LMHeadModel using model.fit() in TF2.0?',2019-12-13T18:54:09Z,2020-02-22T18:13:15Z,wontfix,,
2168,"b""CUDA error at 'cublasSgemm' when using the pretrained BERT""",2019-12-13T18:08:19Z,2020-02-21T11:15:48Z,wontfix,RuntimeError,"RuntimeError: CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling `cublasSgemm( handle, opa, opb, m, n, k, &alpha, a, lda, b, ldb, &beta, c, ldc)"
2167,"b'using run_squad.py for predict and specifying config_name as path, config.json not found'",2019-12-13T15:26:21Z,2019-12-13T21:57:17Z,,OSError,"OSError: file /home/.../pipeline_results/session_id_1234/output/workdirs/0.0.0/config.json' not found                                                                                                                                                                    During handling of the above exception, another exception occurred: "
2166,b'How to do the further pretraining ?',2019-12-13T15:08:17Z,2020-02-19T05:31:12Z,wontfix,,
2165,b'Model parallelism + Adapters',2019-12-13T15:01:19Z,2020-03-11T13:30:35Z,wontfix,,
2164,b'[SMALL BREAKING CHANGE] Cleaning up configuration classes - Adding Model Cards',2019-12-13T13:38:24Z,2019-12-17T08:10:17Z,,,
2163,b'PreTrainedEncoderDecoder on tensorflow',2019-12-13T12:19:17Z,2020-02-11T15:28:44Z,wontfix,,
2162,b'pad_to_max_length param is not supported in PreTrainedTokenizer.encode ',2019-12-13T10:23:11Z,2019-12-16T09:53:09Z,,,
2161,b'Adding model type to config.json',2019-12-13T10:18:03Z,2020-01-16T11:54:06Z,,,
2160,b'[WIP] Add UniLM model',2019-12-13T08:54:20Z,2020-06-16T09:05:30Z,,,
2159,b'Low ROUGE scores for BertSum',2019-12-13T06:28:31Z,2020-07-04T16:36:22Z,"wontfix, seq2seq",,
2158,b'gpt-2 implement issue',2019-12-13T01:58:54Z,2019-12-26T09:34:02Z,,,
2157,b'How to find the  corresponding download models from Amazon?',2019-12-13T01:34:56Z,2020-03-04T19:53:59Z,wontfix,,
2156,b'End-Task Distillation with DistilBERT',2019-12-12T23:36:16Z,2020-02-21T22:15:49Z,wontfix,,
2155,b'Special Tokens are Split by BPE',2019-12-12T20:58:51Z,2019-12-16T14:21:22Z,,,
2154,"b""AlBERT UnicodeDecodeError: 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte""",2019-12-12T16:39:02Z,2019-12-12T17:27:42Z,,,
2153,b'BertAbs decoder_input_ids',2019-12-12T06:33:39Z,2020-03-07T20:57:14Z,wontfix,,
2152,b'RoBERTa/GPT-2 tokenization: Why we call all_special_tokens for each token in split_all_tokens?',2019-12-12T02:00:58Z,2019-12-15T23:58:02Z,,,
2151,"b""RoBERTa tokenization: Why do we call 'all_special_tokens' in each tokenize loop?""",2019-12-12T01:48:37Z,2019-12-12T01:57:17Z,,,
2150,"b""RoBERTa tokenization: Why do we call 'all_special_tokens' in each tokenize loop?""",2019-12-12T01:48:37Z,2019-12-12T01:53:47Z,,,
2149,"b':bug: #2120 in model.from_pretrained, PosixPath crashes at ""albert"" check'",2019-12-11T23:47:22Z,2019-12-20T22:23:19Z,,,
2148,b'Fix encode plus',2019-12-11T20:18:47Z,2019-12-12T06:24:48Z,,,
2147,b'Recommended way for creating distillBERT container and serving ',2019-12-11T18:07:39Z,2020-01-22T10:31:35Z,,,
2146,b'doc: fix pretrained models table',2019-12-11T16:57:49Z,2019-12-11T17:19:22Z,,,
2145,b'the docs pretrained models is missing',2019-12-11T16:33:17Z,2019-12-11T20:26:00Z,,,
2144,b'Allowing from_pretrained to load from url directly',2019-12-11T16:21:44Z,2019-12-12T06:43:41Z,,,
2143,b'Fix typo in examples/run_glue.py args declaration.',2019-12-11T15:15:02Z,2019-12-12T16:16:20Z,,,
2142,b'master branch examples/run_squad.py: missing --predict_file argparse argument',2019-12-11T14:57:24Z,2020-02-17T16:26:03Z,wontfix,,
2141,b'Fine-tuning distilled GPT-2',2019-12-11T14:48:51Z,2019-12-11T23:19:17Z,,,
2140,b'return_tokens_mapped_to_origin not working',2019-12-11T13:19:11Z,2020-03-13T21:09:42Z,wontfix,,
2139,b'About Summarization ',2019-12-11T09:37:11Z,2020-03-03T14:24:00Z,wontfix,,
2138,b'encode_plus not returning attention_mask and not padding',2019-12-11T08:31:29Z,2020-07-09T17:42:35Z,,,
2137,b'Tokenization in C++',2019-12-11T08:18:14Z,2020-01-13T17:07:54Z,,,
2136,b'is the tokenization broken for bert?',2019-12-11T03:58:41Z,2019-12-11T08:45:10Z,,,
2135,b'Is there support for TensorflowJs?',2019-12-11T02:54:16Z,2019-12-11T23:53:29Z,,,
2134,b'closes #1960 Add saving and resuming functionality for remaining examples',2019-12-11T02:53:11Z,2019-12-21T14:03:54Z,,,
2133,b'Refactor functionality of run_squad and squad_utils into XXXForQuestionAnswering',2019-12-11T00:23:36Z,2020-02-16T21:20:05Z,wontfix,,
2132,b'`bert-base-uncased` tokenizer broke around special tokens in v2.2.1',2019-12-10T23:53:11Z,2019-12-11T00:47:44Z,,,
2131,b'[AB-219] Progress bar',2019-12-10T20:46:59Z,2019-12-10T21:06:15Z,,,
2130,b'[BREAKING CHANGE] Setting all ignored index to the PyTorch standard',2019-12-10T20:45:57Z,2019-12-21T13:55:41Z,,,
2129,b'Progress indicator improvements when downloading pre-trained models.',2019-12-10T19:40:23Z,2019-12-10T21:18:56Z,,,
2128,b'In which directory the downloaded roberta-base models will be stored on linux server conda environment',2019-12-10T19:25:10Z,2020-03-08T20:39:51Z,wontfix,,
2127,b'Where is extract_features.py and run_classifier.py ?',2019-12-10T17:14:27Z,2019-12-13T15:09:01Z,,,
2126,b'Model2Model: RuntimeError: expected device cpu and dtype Float but got device cpu and dtype Bool',2019-12-10T16:31:41Z,2019-12-10T16:59:59Z,,RuntimeError,"RuntimeError: expected device cpu and dtype Float but got device cpu and dtype Bool"
2125,b'DistilmBERT training/distillation dataset',2019-12-10T15:39:55Z,2020-02-21T22:15:48Z,wontfix,,
2124,b'Is there a way to evaluate models during training in Multi-gpu setting',2019-12-10T14:21:25Z,2020-02-16T10:20:10Z,wontfix,,
2123,"b'Transformers for Tabular data extraction - e.g., wikitables'",2019-12-10T14:02:06Z,2021-04-26T15:03:04Z,,,
2122,b'Remove misplaced summarization documentation',2019-12-10T13:53:28Z,2019-12-10T14:13:34Z,,,
2121,"b'""Write With Transformer"" interface returning 502 on gpt2/xl model'",2019-12-10T09:15:28Z,2019-12-11T06:22:24Z,,,
2120,"b""BertModel.from_pretrained() doesn't accept pathlib.PosixPath anymore""",2019-12-10T07:39:41Z,2020-02-17T00:20:04Z,wontfix,TypeError,"TypeError: argument of type 'PosixPath' is not iterable"
2119,b'Finetune and generate text with BertForMaskedLM',2019-12-10T04:49:10Z,2020-02-15T05:31:42Z,wontfix,,
2118,b'Could convert_pytorch_checkpoint_to_tf2.py convert any pytorch model to tf2?',2019-12-10T04:08:39Z,2020-02-15T10:57:10Z,wontfix,,
2117,b'Encoder-decoders in Transformers ',2019-12-10T03:38:26Z,2020-06-20T10:49:53Z,wontfix,TypeError,"TypeError: forward() got an unexpected keyword argument 'encoder_hidden_states'"
2116,"b""Couldn't reach server at '{}' to download vocabulary files.""",2019-12-10T02:52:54Z,2019-12-10T02:59:18Z,,"TimeoutError, urllib3.exceptions.NewConnectionError, urllib3.exceptions.MaxRetryError, requests.exceptions.ConnectionError, OSError","TimeoutError: [Errno 110] Connection timed outurllib3.exceptions.NewConnectionError: <urllib3.connection.VerifiedHTTPSConnection object at 0x7f912c2cc550>: Failed to establish a new connection: [Errno 110] Connection timed outurllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='s3.amazonaws.com', port=443): Max retries exceeded with url: /models.huggingface.co/bert/bert-base-chinese-vocab.txt (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7f912c2cc550>: Failed to establish a new connection: [Errno 110] Connection timed out',))requests.exceptions.ConnectionError: HTTPSConnectionPool(host='s3.amazonaws.com', port=443): Max retries exceeded with url: /models.huggingface.co/bert/bert-base-chinese-vocab.txt (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7f912c2cc550>: Failed to establish a new connection: [Errno 110] Connection timed out',))OSError: Couldn't reach server at '{}' to download vocabulary files."
2115,b'[WIP] Add MMBT Model to Transformers Repo',2019-12-10T02:41:18Z,2019-12-21T14:26:09Z,,,
2114,b'Split models to multiple GPUs',2019-12-09T19:28:37Z,2019-12-14T13:08:44Z,,,
2113,b'Running run_lm_finetuning.py within python',2019-12-09T17:43:39Z,2019-12-10T16:22:14Z,,,
2112,b'XLM model masked word prediction Double Language ',2019-12-09T16:47:25Z,2020-02-14T17:31:44Z,wontfix,,
2111,b'Could not run run_ner.py based on XLNET model',2019-12-09T14:53:15Z,2020-02-14T16:31:42Z,wontfix,,
2110,"b""unable to load the downloaded BERT model  offline in local  machine . could not  find config.json and  Error no file named ['pytorch_model.bin', 'tf_model.h5', 'model.ckpt.index'] |""",2019-12-09T12:58:52Z,2020-02-17T03:20:05Z,wontfix,,
2109,b'Error in TFBertForSequenceClassification',2019-12-09T12:43:48Z,2019-12-16T21:58:45Z,,AssertionError,"AssertionError: classifier.weight not found in PyTorch model"
2108,b'I am running bert fine tuning with cnnbase model but my project stops at loss.backward() without any prompt in cmd.',2019-12-09T10:36:09Z,2019-12-12T15:59:56Z,,,
2107,b'create encoder attention mask from shape of hidden states',2019-12-09T10:22:43Z,2019-12-10T09:07:57Z,,,
2106,b'RobertaTokenizer runs slowly after add _tokens',2019-12-09T09:58:35Z,2020-02-19T16:11:48Z,wontfix,,
2105,b'Some bug in using eval_all_checkpoints',2019-12-09T04:36:55Z,2020-03-11T13:30:37Z,wontfix,,
2104,b'Having trouble reproducing SQuAD 2.0 results using ALBERT v2 models',2019-12-09T03:07:46Z,2020-03-20T06:20:59Z,wontfix,,
2103,b'Is there any way to treat the whitespace characters same as other characters when tokenizing?',2019-12-09T02:34:43Z,2019-12-15T09:02:10Z,,,
2102,b'How to pretrain BERT whole word masking (wwm) model?',2019-12-08T23:11:42Z,2020-03-13T18:09:42Z,wontfix,,
2101,"b':bug: #2096 in tokenizer.decode, adds a space after special tokens for string format'",2019-12-08T22:58:20Z,2019-12-13T21:41:46Z,,,
2100,"b""Unclear how to decode a model's output""",2019-12-08T22:32:32Z,2019-12-09T18:21:18Z,,,
2099,b'which special token is used to predict the score in roberta?',2019-12-08T14:24:20Z,2019-12-11T13:50:24Z,,,
2098,b'Understanding output of models and relation to token probability',2019-12-08T09:35:25Z,2019-12-18T15:42:16Z,,,
2097,b'about the special tokens',2019-12-08T09:07:46Z,2019-12-13T03:35:13Z,,,
2096,b'The added tokens do not work as expected',2019-12-08T06:28:59Z,2020-02-11T23:16:58Z,wontfix,,
2095,"b""Can't get gradients from TF TransformerXL model forward pass""",2019-12-07T10:50:00Z,2020-02-12T13:21:33Z,wontfix,,
2094,b'How to save a model as a BertModel',2019-12-07T10:11:43Z,2020-02-15T13:57:06Z,wontfix,,
2093,b'Remove pytest dependency.',2019-12-07T08:56:57Z,2019-12-07T12:46:15Z,,,
2092,"b'When I use albertModel, it prints the following repeatedly.'",2019-12-07T08:38:18Z,2019-12-09T09:04:15Z,,,
2091,b'Error msg when running on the colab',2019-12-07T04:30:05Z,2020-02-14T18:31:45Z,wontfix,,
2090,b'AssertionError in official example ',2019-12-07T02:19:38Z,2019-12-07T02:58:40Z,,AssertionError,"AssertionError: "
2089,b'Use run_lm-finetuning on tpu',2019-12-06T18:42:17Z,2020-02-11T20:47:26Z,wontfix,,
2088,b'Help with converting fine-tuned PT model to TF checkpoint',2019-12-06T16:45:55Z,2019-12-23T04:24:27Z,,**AssertionError,"**AssertionError: cls.seq_relationship.weight not found in PyTorch model**"
2087,b'How can I get similarity matching ?',2019-12-06T15:40:21Z,2020-02-13T22:34:37Z,wontfix,,
2086,"b'""Only evaluate when single GPU otherwise metrics may not average well""'",2019-12-06T14:16:34Z,2020-02-13T22:34:38Z,wontfix,,
2085,b'Write With Transformer: PPLM document is stuck',2019-12-06T13:51:14Z,2019-12-06T17:45:25Z,,,
2084,b'CUDA out of memory for 8x V100 GPU',2019-12-06T13:36:24Z,2020-02-14T18:31:45Z,wontfix,,
2083,b'ALBERT how to obtain the embedding matrix?',2019-12-06T13:26:34Z,2020-02-11T14:47:23Z,wontfix,,
2082,"b""ImportError: cannot import name 'WarmupLinearSchedule'""",2019-12-06T11:49:58Z,2020-02-29T10:00:00Z,wontfix,,
2081,b'handle string with only whitespaces as empty',2019-12-06T09:33:59Z,2019-12-12T07:20:44Z,,,
2080,b'Encoding special tokens',2019-12-06T09:21:17Z,2019-12-09T10:45:13Z,,,
2079,b'How to average sub-words embeddings to obtain word embeddings?',2019-12-06T05:46:02Z,2020-04-12T13:12:46Z,wontfix,,
2078,b'[cli] Uploads: add progress bar',2019-12-06T00:31:40Z,2019-12-06T16:56:24Z,,,
2077,b'corrected documentation for past tensor shape for ctrl and gpt2 model',2019-12-06T00:28:10Z,2019-12-06T11:14:49Z,,,
2076,b'Text Generation in Hebrew',2019-12-05T22:27:37Z,2020-02-10T23:19:17Z,wontfix,,
2075,b'Check link validity',2019-12-05T20:27:06Z,2019-12-12T07:09:12Z,,,
2074,b'Check the validity of download links',2019-12-05T20:20:49Z,2019-12-05T20:22:32Z,,,
2073,b'How to structure text data to finetune distilGPT2 using tf.keras.model.fit()?',2019-12-05T18:55:21Z,2019-12-05T19:37:43Z,,IndexError,"IndexError: list index out of range"
2072,b'Accessing roberta embeddings',2019-12-05T18:43:47Z,2019-12-10T16:16:21Z,,,
2071,"b""The generation script could fail when there's a double space in the prompt""",2019-12-05T18:27:35Z,2019-12-06T11:16:10Z,,,
2070,b'XLMWithLMHeadModel forwarding questions',2019-12-05T15:20:04Z,2020-02-10T16:19:19Z,wontfix,,
2069,b'clean up PT <=> TF conversion',2019-12-05T14:20:24Z,2019-12-10T14:34:09Z,,,
2068,"b""Nicer error message when Bert's input is missing batch size""",2019-12-05T13:39:26Z,2019-12-06T11:06:43Z,,,
2067,b'Save model for tensorflow serving',2019-12-05T13:23:23Z,2019-12-05T13:39:35Z,,AttributeError,"AttributeError: 'MultiLabelClassificationModel' object has no attribute 'state_dict'"
2066,b'CPU RAM out of memory when detach from GPU',2019-12-05T12:38:05Z,2020-02-10T13:19:19Z,wontfix,,
2065,b'Fixing camembert tokenization',2019-12-05T12:30:27Z,2019-12-05T12:45:45Z,,,
2064,b'[ Structure of LM vocab trained from scratch ]',2019-12-05T11:58:59Z,2019-12-20T14:05:14Z,,,
2063,b'special_tokens_mask value was unused and calculated twice',2019-12-05T08:06:26Z,2019-12-12T07:21:46Z,,,
2062,"b""TypeError: argument of type 'PosixPath' is not iterable (in modeling_utils.py)""",2019-12-05T07:16:11Z,2019-12-05T07:52:18Z,,TypeError,"TypeError: argument of type 'PosixPath' is not iterable "
2061,"b""BertForSequenceClassification' object has no attribute 'bias""",2019-12-05T05:53:01Z,2020-02-17T13:26:10Z,wontfix,,
2060,b'Pr for pplm',2019-12-05T05:08:34Z,2019-12-05T14:20:08Z,,,
2059,b'How to run a batch of data through BERT model?',2019-12-05T01:29:33Z,2019-12-05T01:52:08Z,,,
2058,"b'Automatically allocates memory in GPU, always OOM when create TFALBERT model'",2019-12-05T00:53:21Z,2020-02-10T15:19:18Z,wontfix,OSError,"OSError: Unable to open file (file signature not found)"
2057,b'`distilroberta-base` link missing',2019-12-04T23:35:36Z,2019-12-05T00:44:30Z,,,
2056,"b""cannot import name 'get_linear_schedule_with_warmup' from 'transformers.optimization'""",2019-12-04T23:17:27Z,2019-12-05T04:34:36Z,,,
2055,b'Remove dependency on pytest for running tests',2019-12-04T20:45:18Z,2019-12-06T18:57:38Z,,,
2054,b'Find dot product of query and key vectors ',2019-12-04T19:31:23Z,2020-02-11T10:47:23Z,wontfix,,
2053,"b'Crosslingual classification with XLM, loss does not converge'",2019-12-04T18:14:38Z,2020-02-11T03:47:22Z,wontfix,,
2052,"b'Missing ""do_lower_case"" action for special token (e.g. mask_token)'",2019-12-04T17:54:53Z,2020-02-11T23:47:24Z,wontfix,,
2051,b'Fix bug which lowercases special tokens',2019-12-04T16:01:52Z,2019-12-06T21:15:54Z,,,
2050,b'[CamemBert] About SentencePiece training',2019-12-04T13:41:08Z,2019-12-04T15:15:05Z,,,
2049,"b""ModuleNotFoundError: No module named 'git'""",2019-12-04T11:33:40Z,2020-01-01T20:18:47Z,,ModuleNotFoundError,"ModuleNotFoundError: No module named 'git'"
2048,b'Changing the number of hidden layers for BERT',2019-12-04T10:29:53Z,2020-02-12T00:47:22Z,wontfix,,
2047,b'Tokenization in quickstart guide fails',2019-12-04T09:12:17Z,2019-12-06T21:15:54Z,,,
2046,b'Add NER TF2 example.',2019-12-04T08:44:38Z,2019-12-06T11:12:23Z,,,
2045,b'Remove dead code in tests.',2019-12-04T07:21:47Z,2019-12-05T13:41:57Z,,,
2044,b'CLI for authenticated file sharing',2019-12-04T05:56:16Z,2019-12-05T08:44:08Z,,,
2043,b'Missing xlm-mlm-100-1280',2019-12-04T01:34:47Z,2020-02-10T14:19:19Z,wontfix,,
2042,"b""UnboundLocalError: local variable 'extended_attention_mask' referenced before assignment""",2019-12-04T01:29:14Z,2019-12-05T17:43:35Z,,UnboundLocalError,"UnboundLocalError: local variable 'extended_attention_mask' referenced before assignment"
2041,b'How do I load a pretrained file offline?',2019-12-04T01:14:32Z,2019-12-04T07:47:02Z,,,
2040,b'XLM-R Support',2019-12-03T19:34:09Z,2019-12-04T12:16:57Z,,,
2039,b'Meaning of run_lm_finetuning.py output',2019-12-03T19:18:29Z,2020-02-10T17:19:17Z,wontfix,,
2038,b'run_squad with xlm: Dataparallel has no attribute config. ',2019-12-03T18:12:12Z,2019-12-18T14:53:38Z,,AttributeError,"AttributeError: 'DataParallel' object has no attribute 'config'"
2037,b'how to select best model in run_glue',2019-12-03T14:09:52Z,2020-02-08T15:01:12Z,wontfix,,
2036,b'error',2019-12-03T11:10:48Z,2019-12-05T05:48:02Z,,json.decoder.JSONDecodeError,"json.decoder.JSONDecodeError: Unterminated string starting at: line 1 column 4194293 (char 4194292)"
2035,b'Doubts on modeling_gpt2.py',2019-12-03T10:43:43Z,2020-02-21T06:15:50Z,wontfix,,
2034,b'Updated examples/README and parser for run_summarization_finetuning',2019-12-03T09:44:24Z,2019-12-17T01:11:32Z,,,
2033,b'run_lm_finetuning.py script CLM inputs and labels preparing',2019-12-03T07:16:51Z,2019-12-03T09:14:42Z,,,
2032,b'Any workaround to extend the embeddings on TFGPT2DoubleHeadsModel?',2019-12-03T04:58:59Z,2020-02-10T15:19:17Z,wontfix,,
2031,b'Typo in modeling_albert.py for mask_token',2019-12-03T04:07:14Z,2019-12-05T13:06:37Z,,,
2030,"b""cannot import name 'WEIGHTS_NAME'""",2019-12-02T23:43:40Z,2019-12-03T15:03:16Z,,ImportError,"ImportError: cannot import name 'WEIGHTS_NAME' from 'transformers' (unknown location)"
2029,b'gpt-2 generation examples',2019-12-02T20:44:19Z,2019-12-04T01:51:43Z,,,
2028,b'[CamemBERT] Potential error in the docs',2019-12-02T17:00:42Z,2019-12-03T08:14:10Z,,,
2027,b'Tokenization differs for different intepreter instances',2019-12-02T14:20:38Z,2019-12-17T09:03:45Z,,,
2026,b'Does GPT2LMHeadModel need <|startoftext|> and <|endoftext|> tokens?',2019-12-02T14:17:38Z,2019-12-03T14:37:00Z,,,
2025,b'How to convert a tf2 pre-trained model to pytorch model?',2019-12-02T13:45:14Z,2020-02-09T01:58:56Z,wontfix,,
2024,"b'[ALBERT] : ValueError: Layer #1 (named ""predictions"") expects 11 weight(s), but the saved weights have 10 element(s).'",2019-12-02T13:04:31Z,2020-02-19T11:11:50Z,wontfix,ValueError,"ValueError: Layer #1 (named ""predictions"") expects 11 weight(s), but the saved weights have 10 element(s)."
2023,b'Is it possible to fine-tune models on TPUs using TensorFlow?',2019-12-02T12:39:52Z,2020-08-08T02:47:59Z,wontfix,,
2022,b'How to convert the ALBERT tfhub model to pytorch model?',2019-12-02T11:25:46Z,2019-12-04T02:18:39Z,,,
2021,b'save as tensorflow saved model format and how to inference?',2019-12-02T11:25:06Z,2020-02-09T11:58:55Z,wontfix,TypeError,"TypeError: Expected at most 0 positional arguments (and the rest keywords, of ['attention_mask', 'input_ids', 'token_type_ids']), got ({'input_ids': <tf.Tensor: id=130383, shape=(64, 128), dtype=int32, numpy="
2020,b'Camenbert length Tokenizer not equal config vocab_size',2019-12-02T10:30:44Z,2020-02-10T13:19:18Z,wontfix,,
2019,"b""[CamemBert] Tokenizer function add_tokens doesn't work""",2019-12-02T10:18:30Z,2019-12-05T12:45:45Z,,,
2018,"b""FileNotFoundError: [Errno 2] No such file or directory: 'data/dump.txt'""",2019-12-02T08:29:57Z,2020-02-07T15:52:28Z,wontfix,,
2017,b'How to use GPT-2 text generator in spanish',2019-12-01T20:49:26Z,2019-12-02T14:25:59Z,,,
2016,b'GPT-2 finetuning with run_lm_finetuning.py script',2019-12-01T20:36:00Z,2019-12-03T20:51:48Z,,,
2015,b'[CamemBERT] Add CamembertForQuestionAnswering',2019-12-01T19:57:18Z,2020-04-13T12:29:26Z,wontfix,,
2014,b'Mark tests in TFAutoModelTest as slow.',2019-12-01T17:26:55Z,2019-12-03T15:03:49Z,,,
2013,"b'What is the real parameters to weight the triple loss (L_{ce}, L_{mlm}, L_{cos}) in DistilBert?'",2019-12-01T16:49:05Z,2019-12-02T15:37:37Z,,,
2012,b'How to output the vectors of the last four layers of BERT_Model.',2019-12-01T08:58:54Z,2019-12-04T06:22:32Z,,,
2011,b'typo fix on the docs as per Pytorch v1.1+',2019-12-01T08:38:39Z,2019-12-05T11:39:05Z,,,
2010,b'Changing the docs as per Pytorch v1.1+',2019-12-01T08:34:51Z,2020-02-02T11:46:07Z,wontfix,,
2009,b'Reason for using einsum in xlnet?',2019-12-01T05:30:39Z,2020-02-06T07:13:36Z,wontfix,,
2008,b'Expand run_lm_finetuning.py to all models',2019-11-30T22:19:57Z,2020-11-01T16:29:42Z,wontfix,,
2007,b'fixed XLNet attention output for both attention streams whenever target_mapping is provided',2019-11-30T15:11:14Z,2019-12-05T11:32:40Z,,,
2006,"b""[ALBERT]: 'AlbertForMaskedLM' object has no attribute 'bias'""",2019-11-30T14:15:25Z,2019-12-03T18:25:14Z,,AttributeError,"AttributeError: 'AlbertForMaskedLM' object has no attribute 'bias'"
2005,b'tf.keras.mixed_precision.experimental.Policy',2019-11-30T11:01:39Z,2019-11-30T11:02:56Z,,,
2004,b'Can we use tf.keras.mixed_precision.experimental.set_policy ?',2019-11-30T10:55:30Z,2020-02-05T20:13:36Z,wontfix,,
2003,b'Where I could find the vocab.json for XLNet',2019-11-30T09:01:24Z,2019-12-03T12:18:38Z,,,
2002,b'Always use SequentialSampler during evaluation',2019-11-30T00:22:57Z,2019-12-03T15:15:40Z,,,
2001,b'GPT2: how to construct batch for Language Modeling',2019-11-29T20:28:13Z,2020-02-13T00:21:36Z,wontfix,,
2000,b'Wrong tokenization in Transformer-XL documentation',2019-11-29T15:15:02Z,2020-02-24T20:11:11Z,,,
1999,b'Training masked language model with Tensorflow',2019-11-29T11:49:41Z,2020-02-21T18:15:48Z,wontfix,tensorflow.python.framework.errors_impl.InvalidArgumentError,"tensorflow.python.framework.errors_impl.InvalidArgumentError: Dimensions must be equal, but are 10 and 119547 for 'loss/output_1_loss/mul' (op: 'Mul') with input shapes: [?,10], [?,?,119547]."
1998,b'Added Camembert to available models',2019-11-29T11:34:08Z,2019-11-29T19:17:03Z,,,
1997,b'How to get a spiece.model from customize chinese vocab.txt in Albert xlnet ?',2019-11-29T11:14:24Z,2020-05-21T20:40:45Z,wontfix,,
1996,b'ALBERT is missing from AutoClasses',2019-11-29T10:57:56Z,2019-11-30T15:01:03Z,,,
1995,b'Add ALBERT to AutoClasses',2019-11-29T10:53:52Z,2019-11-29T16:25:38Z,,,
1994,b'XLnet output_attentions=True raises an exception',2019-11-29T10:39:43Z,2019-12-05T11:45:51Z,,AttributeError,"AttributeError: 'tuple' object has no attribute 'permute'"
1993,b'Why is the weight of linear layer tied to the input embeddings in OpenAIGPTLMHeadModel?',2019-11-29T07:54:12Z,2020-02-08T18:01:12Z,wontfix,,
1992,b'Worse F1 on squad2 with finetune+distil distilroberta-base than just finetune',2019-11-29T07:32:14Z,2020-02-04T09:10:59Z,wontfix,,
1991,"b""Facing AttributeError: 'DataParallel' object has no attribute 'resize_token_embeddings'""",2019-11-29T07:13:45Z,2019-12-04T13:53:45Z,,AttributeError,"AttributeError: 'DataParallel' object has no attribute 'resize_token_embeddings'"
1990,"b'When training QA models, albert-xxlarge-v2 uses much more GPU mem than Bert-large'",2019-11-29T07:04:21Z,2020-02-09T03:58:55Z,wontfix,,
1989,b'Will you add XLNet text-generation feature ?',2019-11-29T05:29:10Z,2020-02-11T05:47:23Z,wontfix,,
1988,b'Possible error in the HuggingFace Transformers documentation?',2019-11-29T01:36:44Z,2020-02-04T14:23:23Z,wontfix,,
1987,b'Saving and resuming',2019-11-29T01:29:39Z,2019-12-09T21:24:36Z,,,
1986,b'Fine Tuning Bert for Q&A',2019-11-29T00:22:47Z,2020-02-04T19:23:20Z,wontfix,,
1985,b'run_squad.py for tf',2019-11-28T22:48:53Z,2019-12-03T15:10:32Z,,,
1984,b'[WIP] Squad refactor',2019-11-28T22:36:08Z,2019-12-10T10:07:27Z,,,
1983,b'add special tokens',2019-11-28T21:07:44Z,2019-12-03T04:08:08Z,,,
1981,b'Transformers for WebNLG tasks ',2019-11-28T16:25:02Z,2020-02-08T11:01:12Z,wontfix,,
1980,b'update all tf.shape and tensor.shape to shape_list',2019-11-28T14:53:31Z,2019-11-29T14:40:51Z,,,
1979,b'AlbertForQuestionAnswering ',2019-11-28T14:48:05Z,2019-11-29T09:26:37Z,,,
1978,b'Modify position_embeddings from pre_trained model',2019-11-28T12:12:26Z,2019-12-05T14:50:08Z,,,
1977,"b""'convert_tf_checkpoint_to_pytorch.py' file is missing""",2019-11-28T07:13:52Z,2019-11-29T06:14:06Z,,,
1976,b'Merge pull request #1 from huggingface/master',2019-11-28T02:14:28Z,2019-11-28T02:15:04Z,,,
1975,b'How can we view different versions of documentation?',2019-11-27T23:14:14Z,2019-12-05T18:45:03Z,,,
1974,b'Albert Hyperparameters for Fine-tuning SQuAD 2.0',2019-11-27T22:03:22Z,2020-02-25T16:07:00Z,wontfix,,
1973,b'Changes to S3 Roberta / RobertaForSequenceClassification',2019-11-27T21:36:35Z,2020-02-02T22:44:27Z,wontfix,,
1972,b'How to persist cloud-based transformers',2019-11-27T18:08:52Z,2020-02-03T16:55:29Z,wontfix,,
1971,b'add add_special_tokens=True for input examples',2019-11-27T16:50:49Z,2019-11-27T17:05:25Z,,,
1970,b'Bert Tensor Dimensions',2019-11-27T16:14:42Z,2020-02-10T14:19:18Z,wontfix,,
1969,b'Implemented concurrent encoding and converting of sequences for data binarization',2019-11-27T16:14:08Z,2020-03-11T13:30:36Z,wontfix,,
1968,b'AlbertPreTrainedModel class is not available in release v2.2.0',2019-11-27T14:39:34Z,2019-12-05T11:14:32Z,,,
1967,"b""Trouble running 'bert-base-multilingual-cased'""",2019-11-27T13:29:37Z,2020-02-02T10:50:37Z,wontfix,RuntimeError,"RuntimeError: index out of range: Tried to access index 2 out of table with 1 rows. at ../aten/src/TH/generic/THTensorEvenMoreMath.cpp:418"
1966,"b'Fix issue: #1962, input shape problem'",2019-11-27T10:07:20Z,2019-11-27T15:38:12Z,,,
1965,b'XLMForTokenClassification',2019-11-27T09:34:29Z,2020-02-02T13:44:24Z,wontfix,,
1964,b'How to increase model saving checkpoint from 50 to 1000?',2019-11-27T07:44:21Z,2020-04-04T15:38:35Z,wontfix,,
1963,b'Did the underlying pre-trained models change somehow?',2019-11-27T06:43:02Z,2019-11-27T17:30:06Z,,InvalidArgumentError,"InvalidArgumentError: 2 root error(s) found."
1962,"b""TFBertModel ValueError: Tried to convert 'dims' to a tensor and failed. Error: Cannot convert a partially known TensorShape to a Tensor""",2019-11-27T06:31:58Z,2019-11-27T15:38:12Z,,ValueError,"ValueError: Tried to convert 'dims' to a tensor and failed. "
1961,"b""Problems with running 'run_lm_finetuning.py' with bert""",2019-11-27T02:35:53Z,2020-02-10T21:19:18Z,wontfix,,
1960,b'Improving model saving and resuming',2019-11-27T02:02:54Z,2019-12-21T14:03:53Z,,,
1959,b'update Roberta checkpoint conversion',2019-11-27T00:17:15Z,2019-12-17T23:12:23Z,,,
1958,b'run_ner.py --do_predict inference mode errors. Right data format?',2019-11-26T22:43:27Z,2020-02-03T08:55:29Z,wontfix,IndexError,"IndexError: list index out of range"
1957,b'Do we need to add [CLS] and [SEP] for BertForMaskedLM ?',2019-11-26T22:29:03Z,2019-11-27T16:54:08Z,,,
1956,b' get_linear_schedule_with_warmup Scheduler',2019-11-26T19:17:42Z,2019-11-27T14:08:12Z,,ImportError,"ImportError: cannot import name 'get_linear_schedule_with_warmup' from 'pytorch_transformers' (/Users/hyunjindominiquecho/opt/anaconda3/lib/python3.7/site-packages/pytorch_transformers/__init__.py)"
1955,b'run_squad.py crashes during do_eval',2019-11-26T18:46:43Z,2020-02-05T17:24:13Z,wontfix,"KeyError, subprocess.CalledProcessError","KeyError: 1000000000subprocess.CalledProcessError: Command '['/opt/conda/bin/python', '-u', './examples/run_squad.py', '--local_rank=5', '--model_type', 'bert', '--model_name_or_path', 'bert-large-uncased-whole-word-masking', '--do_eval', '--do_lower_case', '--train_file', '/data/data/SQUAD/train-v1.1.json', '--predict_file', '/data/data/SQUAD/dev-v1.1.json', '--learning_rate', '3e-5', '--num_train_epochs', '2', '--max_seq_length', '384', '--doc_stride', '128', '--output_dir', 'models/wwm_uncased_finetuned_squad_supp/', '--per_gpu_eval_batch_size=6', '--per_gpu_train_batch_size=6', '--save_steps', '500']' returned non-zero exit status 1."
1954,b'BertForMultipleChoice',2019-11-26T18:14:01Z,2020-02-02T17:44:24Z,wontfix,,
1953,b'the output type of TFBertModel is weird',2019-11-26T17:14:07Z,2020-04-05T12:07:52Z,wontfix,AttributeError,"AttributeError: 'list' object has no attribute 'shape'"
1952,b'suggest to track repo w/ https rather than ssh',2019-11-26T15:31:52Z,2019-11-27T16:02:29Z,,,
1951,b'Benchmark not replicable',2019-11-26T15:26:43Z,2020-02-01T17:20:09Z,wontfix,,
1950,b'word or sentence embedding from BERT model',2019-11-26T13:54:48Z,2020-08-03T11:12:05Z,wontfix,,
1949,b'Can i train my own text corpus',2019-11-26T11:05:01Z,2019-11-26T11:21:19Z,,,
1948,b'Should I use `attention_mask`?',2019-11-26T10:33:51Z,2019-11-29T13:02:13Z,,,
1947,"b""Expected object of scalar type Byte but got scalar type Bool for argument #2 'mask'""",2019-11-26T09:30:56Z,2019-11-27T00:12:16Z,,,
1946,b'Fixed typo',2019-11-26T04:06:23Z,2019-11-26T14:01:33Z,,,
1945,b'When using the Bert model',2019-11-26T03:54:55Z,2019-11-27T02:07:02Z,,,
1944,b'tokenization progress made more sensible via tqdm',2019-11-25T22:09:05Z,2020-03-11T13:30:38Z,wontfix,,
1943,b'git@github.com: Permission denied (publickey) when fetching',2019-11-25T21:49:17Z,2019-11-27T23:01:29Z,,,
1942,b'Wrong paraphrase in the TF2/PyTorch README example.',2019-11-25T19:46:09Z,2019-12-12T18:54:17Z,,,
1941,b'NER - sciBERT weights not initialized.',2019-11-25T17:14:35Z,2020-02-01T10:20:08Z,wontfix,,
1940,b'Add TF2 NER example',2019-11-25T17:03:00Z,2019-12-04T08:28:44Z,,,
1939,b'Abruptly model training was stopped',2019-11-25T16:58:49Z,2020-02-01T15:20:08Z,wontfix,,
1938,b'Load output file from fine-tuned bert language model',2019-11-25T16:09:59Z,2019-11-28T16:05:12Z,,,
1937,b'access to the vocabulary',2019-11-25T15:53:33Z,2019-11-25T19:07:19Z,,,
1936,"b'how to output specific layer of TFBertForSequenceClassification, or add layer?'",2019-11-25T12:32:44Z,2019-11-27T07:40:12Z,,,
1935,"b'attention_mask added, not multiplied ... is this correct?'",2019-11-25T08:40:38Z,2019-12-11T10:49:49Z,,,
1934,"b'Download model too slow, is there any way'",2019-11-25T08:13:18Z,2019-11-27T02:08:38Z,,,
1933,b'Can I use HF XLNet to make a Model that Predicts Backwards?',2019-11-25T01:21:07Z,2020-01-31T04:35:07Z,wontfix,,
1932,b'Using GPT-2 XL',2019-11-24T23:36:35Z,2019-11-25T20:42:27Z,,OSError,"OSError: Model name 'gpt2-xl' was not found in tokenizers model name list (gpt2, gpt2-medium, gpt2-large, distilgpt2). We assumed 'gpt2-xl' was a path or url to a directory containing vocabulary files named ['vocab.json', 'merges.txt'] but couldn't find such vocabulary files at this path or url."
1931,b'Using model output by transformers (v2.0) in older versions (0.4.0 or 1.0.0) ',2019-11-24T06:09:02Z,2020-02-01T07:20:07Z,wontfix,,
1930,b'BERT bertviz',2019-11-24T02:05:47Z,2019-11-25T01:11:06Z,,,
1929,b'configuration of the optimizer ',2019-11-23T23:38:21Z,2020-01-31T04:35:06Z,wontfix,,
1928,b'Split on punc should receive never_split list',2019-11-23T16:27:51Z,2019-12-05T10:44:34Z,,,
1927,b'Mask probability in run_lm_finetuning.py',2019-11-23T13:36:53Z,2019-11-23T16:42:27Z,,,
1926,b'How to process ARC dataset with HuggingFace GPT2',2019-11-23T12:36:30Z,2020-01-29T13:39:16Z,wontfix,,
1925,b'Need a Restore training mechenisim in run_lm_finetuning.py',2019-11-23T08:39:41Z,2019-11-26T02:52:04Z,,,
1924,"b""TypeError: convert_examples_to_features() got an unexpected keyword argument 'sequence_a_is_doc'""",2019-11-23T08:06:14Z,2020-02-01T15:20:07Z,wontfix,,
1923,b'Step restarts from step 0 when reload from an existing checkpoint?',2019-11-23T07:28:12Z,2019-11-23T07:54:30Z,,,
1922,b'update',2019-11-23T01:12:27Z,2019-12-05T10:21:37Z,,,
1921,b'FileNotFoundError when running run_squad.py',2019-11-23T01:09:09Z,2019-11-23T02:24:45Z,,FileNotFoundError,"FileNotFoundError: [Errno 2] No such file or directory: '/train-v1.1.json'"
1920,b'CTRLTokenizer not consistent with the fastBPE tokenizer used in Salesforce/CTRL',2019-11-22T20:39:30Z,2019-12-05T10:18:55Z,,,
1919,b'Fix typo in documentation. toto -> to',2019-11-22T19:57:08Z,2019-11-23T15:55:17Z,,,
1918,b'Minor bug fixes on run_ner.py',2019-11-22T18:35:49Z,2019-11-25T21:48:04Z,,,
1917,b'run_squad.py not running',2019-11-22T17:23:03Z,2019-11-22T20:22:03Z,,,
1916,b'Truncating GPT2 past',2019-11-22T15:18:37Z,2020-02-15T00:31:42Z,wontfix,,
1915,b'Any plan to include BART and T5? ',2019-11-22T14:16:22Z,2019-11-24T11:27:42Z,,,
1914,b'How to perform common sense reasoning task with GPT-2?',2019-11-22T12:51:14Z,2020-01-28T14:14:11Z,wontfix,,
1913,b'Some Questions about XLNet ',2019-11-22T09:58:01Z,2020-01-28T10:14:10Z,wontfix,,
1912,b'XLNet is getting slower when enabling mems',2019-11-22T04:17:22Z,2020-01-31T17:35:05Z,wontfix,,
1911,b'Fix GPT2 docstring from #1906',2019-11-22T02:56:42Z,2019-11-25T16:32:02Z,,,
1910,b'Bart Tokenizer treat symbols in a word as a new word. ',2019-11-21T17:55:05Z,2019-12-05T10:33:41Z,,,
1909,"b""Passing inputs to TFGPT2LMHeadModel results in error: 'TensorSliceDataset' object has no attribute 'shape'""",2019-11-21T16:04:57Z,2020-01-31T16:35:05Z,wontfix,AttributeError,"AttributeError: 'TensorSliceDataset' object has no attribute 'shape'"
1908,b'Training transformer XL from scratch with my own dataset',2019-11-21T15:59:39Z,2019-11-23T08:55:57Z,,,
1907,b'lm_fine-tuning on small dataset of 3 documents',2019-11-21T14:07:44Z,2019-11-22T15:06:45Z,,,
1906,b'Documentation error in GPT2Tokenizer',2019-11-21T14:00:46Z,2019-11-25T16:32:01Z,,,
1905,b'run_summarization_finetuning.py',2019-11-21T13:37:14Z,2020-02-01T17:20:09Z,wontfix,,
1904,"b'Typo in Documentation for GPT2LM Output ""past""'",2019-11-21T12:06:45Z,2020-01-01T13:36:58Z,,,
1903,b'Valohai integration',2019-11-21T10:02:44Z,2019-12-03T15:13:02Z,,,
1902,b'Add CamemBERT models to modeling_auto',2019-11-21T09:45:21Z,2019-11-25T15:21:04Z,,,
1901,b'Methods get_input_embeddings() and set_input_embeddings() appear in documentation but not available.',2019-11-21T09:39:57Z,2019-11-23T16:42:21Z,,,
1900,b'Can GPT2DoubleHeadsModel be used for regular next token prediction task without adjusting its head?',2019-11-21T09:25:20Z,2019-11-22T12:33:00Z,,,
1899,b'Classify entities - run_ner script ',2019-11-21T03:19:44Z,2020-01-27T03:54:42Z,wontfix,,
1898,b'Is the usage of scheduler described in README correct?',2019-11-21T02:18:06Z,2020-01-27T03:54:43Z,wontfix,,
1897,b'Distilling GPT2 with gives OOM',2019-11-21T01:37:16Z,2020-01-27T13:54:42Z,wontfix,RuntimeError,"RuntimeError: Expected tensor for argument #1 'indices' to have scalar type Long; but got torch.cuda.IntTensor instead (while checking arguments for embedding)"
1896,b'Tokenizing/Loading Data for GPT-2 (1 example per line)',2019-11-21T01:10:06Z,2020-01-27T19:54:42Z,wontfix,,
1895,b'Update Squ',2019-11-21T00:50:16Z,2020-01-07T21:03:34Z,,,
1894,b'`overwrite_cache` argument in `run_lm_finetuning.py` not used at all',2019-11-20T23:09:55Z,2019-11-25T14:34:14Z,,,
1893,b'Cleanup TPU bits from `run_glue.py`',2019-11-20T22:51:57Z,2019-11-20T22:54:35Z,,,
1892,b'error on bert.fit for Squad dataset',2019-11-20T19:47:26Z,2020-02-10T10:19:19Z,"Need more information, wontfix",,
1891,b'fixes issue with unrecognized arguments for AdamW',2019-11-20T19:29:37Z,2019-12-05T09:15:26Z,,,
1890,b'Correction for the tuple problem',2019-11-20T19:05:27Z,2019-12-05T09:14:54Z,,,
1889,b'explain how to successfully run examples in readme and doc',2019-11-20T19:00:29Z,2019-11-21T19:41:20Z,,,
1888,"b'Is there a straightforward way to classify documents at the sentence level, while using surrounding sentences for context?'",2019-11-20T17:01:37Z,2019-11-21T17:49:21Z,,,
1887,b'Using GPU for gpt2-xl',2019-11-20T16:44:04Z,2019-12-05T09:12:22Z,,,
1886,b'save_pretrained on CamembertTokenizer',2019-11-20T15:19:29Z,2019-12-05T09:05:48Z,,OSError,"OSError: Not found: ""None"": No such file or directory Error #2"
1885,b'GPT2 Tokenizer Special Token ID Bug',2019-11-20T14:52:08Z,2019-11-20T17:08:02Z,,,
1884,b'Wrong definition of the `logging_steps` parameter at the `run_lm_finetuning.py`',2019-11-20T14:23:19Z,2020-01-26T15:13:33Z,wontfix,,
1883,b'F score 0 in combining RoBERTa and BiLSTM ',2019-11-20T12:03:02Z,2020-02-11T02:47:23Z,wontfix,,
1882,b'XLNetForSequenceClassification and CLS token',2019-11-20T10:23:25Z,2019-11-20T10:32:27Z,,,
1881,b'convert list to set in tokenize().split_on_tokens()',2019-11-20T07:08:40Z,2019-12-05T09:11:34Z,,,
1880,"b""question about 'add_prefix_space' of encode method""",2019-11-20T02:27:01Z,2019-11-21T18:58:27Z,,,
1879,b'run_squad hangs for small max_seq_length',2019-11-20T01:45:41Z,2019-11-20T14:43:25Z,,,
1878,b'Is this a bug? ',2019-11-20T00:27:57Z,2019-11-20T18:07:49Z,,,
1877,b'Can the HuggingFace GPT2DoubleHeadsModel either for regular language modelling or solving multiple-choice questions? or is it only for solving multiple-choice questions?',2019-11-19T23:36:58Z,2019-11-21T09:22:32Z,,,
1876,b'Mean does not exist in TF2',2019-11-19T23:15:25Z,2019-11-29T08:26:33Z,,,
1875,b'Clarifications about the Quick-tour of the fine-tuning scripts?',2019-11-19T22:56:11Z,2019-11-20T23:25:52Z,,,
1874,b'Disparitry with Fairseq Roberta implementation for predicting the mask token',2019-11-19T20:49:34Z,2020-02-25T20:58:17Z,Core: Modeling,,
1873,b'German DistilBERT',2019-11-19T19:04:27Z,2019-11-29T08:25:47Z,,,
1871,b'Understanding feature creation ',2019-11-19T16:48:46Z,2020-02-09T07:58:56Z,wontfix,ValueError,"ValueError: num_samples should be a positive integer value, but got num_samples=0`"
1870,b'XLNet for Token classification',2019-11-19T11:54:05Z,2019-12-05T08:54:09Z,,,
1869,b'Pre-training a smaller version of BERT on own data',2019-11-19T10:36:37Z,2019-12-05T08:51:45Z,,,
1868,b'XLNet for Token classification',2019-11-19T10:02:07Z,2019-11-19T10:12:07Z,,,
1867,b'How to fine-tune BERT on a large training dataset?',2019-11-19T09:28:30Z,2020-01-26T00:43:40Z,wontfix,,
1866,b'BertForTokenClassification for NER . what is the conclusion of  this output ?',2019-11-19T09:23:23Z,2020-02-04T21:23:21Z,wontfix,,
1865,b'Run bert for multi-classification but loss never decrease',2019-11-19T07:24:20Z,2020-01-26T04:13:33Z,wontfix,,
1864,"b'tensorflow2.0 does not has mean, but reduce mean'",2019-11-19T04:46:32Z,2019-12-05T10:33:26Z,,```AttributeError,"```AttributeError: module 'tensorflow' has no attribute 'mean'```"
1863,b'How do I train OpenAIGPTDoubleHeadsModel from scratch?',2019-11-19T02:43:37Z,2019-12-05T10:37:03Z,,,
1862,b'save all 12 layers outputs for each token',2019-11-19T00:05:57Z,2019-12-05T08:51:13Z,,,
1861,b'Better TensorFlow 2 examples',2019-11-18T16:14:17Z,2020-04-05T16:07:46Z,wontfix,,
1860,b'[WIP] Add support for CamembertForTokenClassification',2019-11-18T13:18:09Z,2019-11-21T09:56:08Z,,,
1859,b'Adds CamemBERT to Model architectures list',2019-11-18T09:07:21Z,2019-11-18T14:23:15Z,,,
1858,b'Conversion of [Model]ForSequenceClassification logits to probabilities',2019-11-17T23:19:48Z,2020-01-24T03:24:39Z,wontfix,,
1857,b'XLM Masked Word Prediction',2019-11-17T16:43:32Z,2019-11-21T17:24:27Z,,,
1856,b'multitask learning ',2019-11-17T14:35:28Z,2019-12-05T10:25:09Z,,,
1855,b'Some questions about the abstractive summarization code.',2019-11-17T14:05:35Z,2019-11-20T23:10:52Z,,,
1854,b'run glue problem',2019-11-17T10:29:30Z,2019-11-19T01:19:23Z,,,
1853,"b'typo ""deay"" -> ""decay""'",2019-11-17T09:09:44Z,2019-11-18T16:50:07Z,,,
1852,b'XLNet model params for Question answering ',2019-11-16T20:14:07Z,2020-02-12T03:21:33Z,wontfix,RuntimeError,RuntimeError: CUDA error: device-side assert triggered
1851,b'The acc of RACE is always low by roberta model',2019-11-16T16:20:58Z,2020-03-27T10:15:42Z,wontfix,,
1850,"b""Can't run convert_roberta_original_pytorch_checkpoint_to_pytorch.py""",2019-11-16T16:04:13Z,2020-02-26T19:49:55Z,wontfix,tarfile.CompressionError,"tarfile.CompressionError: unknown compression type 'pt'"
1849,b'run_finetuning resize token embeddings',2019-11-16T14:49:39Z,2019-11-25T20:06:33Z,,,
1848,b'CUDA runtime error (59) : device-side assert triggered ',2019-11-16T14:37:49Z,2019-11-26T15:51:45Z,,,
1847,"b'Update modeling_utils.py by adding ""DUMMY_INPUTS"" after ""logger"" variable.'",2019-11-16T14:34:21Z,2019-12-04T12:35:50Z,,,
1846,b'fix summary_type value of SequenceSummary',2019-11-16T09:48:56Z,2019-12-04T12:28:40Z,,,
1845,b'summary_type value  of SequenceSummary is incorrect',2019-11-16T09:46:44Z,2019-12-04T12:29:04Z,,,
1844,b'Rebase and merge Louismartin/camembert',2019-11-16T05:10:53Z,2019-11-16T05:11:08Z,,,
1843,"b'""This tokenizer does not make use of special tokens."" warning'",2019-11-15T20:06:32Z,2019-11-19T17:08:19Z,,,
1842,b'xlm-mlm-17-1280 model masked word prediction',2019-11-15T17:41:37Z,2019-11-21T17:23:39Z,,,
1841,b'BPE error when fine-tuning a CTRL model',2019-11-15T16:27:41Z,2020-01-21T19:07:33Z,wontfix,,
1840,b'Sampling sequence generator for transformers',2019-11-15T09:59:45Z,2019-12-21T13:27:36Z,,,
1839,b'Add support for Japanese BERT models by cl-tohoku',2019-11-15T08:42:48Z,2019-12-11T23:32:28Z,,,
1838,b'resize_token_embeddings not implemented for TFBert* ',2019-11-15T00:07:39Z,2020-04-25T23:15:59Z,wontfix,,
1837,"b""Error started happening today: ImportError: cannot import name 'get_linear_schedule_with_warmup'""",2019-11-14T22:39:42Z,2019-12-05T10:52:19Z,,>ImportError,">ImportError: cannot import name 'get_linear_schedule_with_warmup'"
1836,b'Model parallelism support?',2019-11-14T21:33:37Z,2020-01-22T15:48:59Z,wontfix,,
1835,b'Parallell compute failing for finetuning GPT2 using GPU',2019-11-14T18:36:04Z,2019-11-27T20:21:52Z,,RuntimeError,"RuntimeError: parallel_for failed: no kernel image is available for execution on the device"
1834,b'Where is Model2Model PreTrainedEncoderDecoder in run_summerization_finetune',2019-11-14T18:09:24Z,2020-03-09T03:39:51Z,wontfix,,
1833,b'Token indices sequence length is longer than the specified maximum sequence length for this model',2019-11-14T15:39:05Z,2019-11-14T21:04:50Z,,,
1832,b'replace LambdaLR scheduler wrappers by function',2019-11-14T14:41:10Z,2019-11-14T21:10:31Z,,,
1831,b'sum() is replaced by itertools.chain.from_iterable()',2019-11-14T14:10:19Z,2019-11-14T21:11:55Z,,,
1830,b'GPT2 tokenizer is so slow because of sum()',2019-11-14T12:47:18Z,2019-11-14T21:14:47Z,,,
1829,b'NotImplementedError when using TFDistilBertModel',2019-11-14T11:51:44Z,2019-11-14T13:57:02Z,,NotImplementedError,"NotImplementedError: in converted code:"
1828,b'GPT2 tokenizer is so slow because of regex.findall',2019-11-14T09:55:57Z,2019-11-14T10:28:03Z,,,
1827,b'How to get all layers(12) hidden states of BERT?',2019-11-14T08:21:18Z,2019-12-04T12:24:20Z,,,
1826,b'Regarding Fine-Tuning for Abstractive Summarization',2019-11-14T06:48:28Z,2020-01-20T07:33:12Z,wontfix,,
1825,b'\xe5\x90\x88\xe5\xb9\xb6',2019-11-14T06:46:28Z,2019-11-14T21:24:29Z,,,
1824,b'Issue testing run_squad.py example',2019-11-14T05:06:19Z,2019-11-14T05:41:29Z,,">ValueError, ValueError",">ValueError: For training, each question should have exactly 1 answer.ValueError: For training, each question should have exactly 1 answer."
1823,b'how to use convert_pytorch_checkpoint_to_tf2.py ',2019-11-14T02:48:06Z,2019-11-18T05:56:45Z,,,
1822,b'CamemBERT',2019-11-14T01:04:36Z,2019-11-16T05:13:44Z,,,
1821,b'Generated text makes no sense. Trying to auto-generate sentences like https://transformer.huggingface.co/doc/distil-gpt2',2019-11-14T00:17:50Z,2020-01-20T09:33:12Z,wontfix,,
1820,b'Is there a way of finetuning DistilGPT2?',2019-11-13T21:45:56Z,2020-01-22T10:48:59Z,wontfix,,
1819,b'CUDA out of memory on loss.backward when fine-tuning GPT2 (117M)',2019-11-13T20:48:47Z,2020-02-08T03:07:15Z,wontfix,RuntimeError,"RuntimeError: CUDA out of memory. Tried to allocate 198.00 MiB (GPU 0; 5.93 GiB total capacity; 4.64 GiB already allocated; 54.94 MiB free; 233.05 MiB cached)"
1818,b'Trouble running fine tuned language model script',2019-11-13T17:08:14Z,2019-11-14T15:51:19Z,,RuntimeError,"RuntimeError: Expected object of scalar type Byte but got scalar type Bool for argument #2 'mask'"
1817,b'Using multiple inputs for GPT-2',2019-11-13T14:57:05Z,2020-01-19T16:12:42Z,wontfix,,
1816,b'Best way to fine tune GPT-2 in order to create a custom text generator?',2019-11-13T09:36:25Z,2020-01-24T02:24:45Z,wontfix,,
1815,b'computing self-attention for tokens in a sentence',2019-11-13T03:38:46Z,2020-02-09T21:58:56Z,wontfix,,
1814,b'Sample a constant number of tokens for masking in LM finetuning',2019-11-13T02:13:10Z,2020-03-11T14:30:30Z,wontfix,,
1813,b'LM Fine-tuning for XLNET?',2019-11-12T22:09:11Z,2020-01-27T17:13:44Z,wontfix,,
1812,b'Update conversion script to convert XLM-R',2019-11-12T21:51:37Z,2019-11-14T21:17:58Z,,,
1811,b'Fix special tokens addition in decoder #1807',2019-11-12T20:32:11Z,2019-11-14T21:17:25Z,,,
1810,"b""NameError: name 'DUMMY_INPUTS' is not defined - From TF to PyTorch""",2019-11-12T20:23:55Z,2019-12-04T12:35:04Z,,"NameError, UnicodeDecodeError","NameError: name 'DUMMY_INPUTS' is not definedUnicodeDecodeError: 'utf-8' codec can't decode byte 0x89 in position 0: invalid start byte"
1809,b'Why do language modeling heads not have activation functions?',2019-11-12T19:19:03Z,2019-11-13T15:15:28Z,,,
1808,b'XLMForSequenceClassification - help with zero-shot cross-lingual classification',2019-11-12T18:47:47Z,2019-12-04T12:15:25Z,,,
1807,b'Whether it belongs to the bug of class trainedtokenizer decode?',2019-11-12T15:25:40Z,2019-11-14T21:18:29Z,,,
1806,b'Extracting First Hidden States',2019-11-12T14:37:43Z,2019-11-12T14:53:29Z,,,
1805,b'RuntimeError: CUDA error: device-side assert triggered',2019-11-12T14:10:39Z,2020-01-24T02:24:46Z,wontfix,,
1804,b'fix multi-gpu eval in torch examples',2019-11-12T11:04:00Z,2019-11-14T21:24:05Z,,,
1803,b'fix run_squad.py during fine-tuning xlnet on squad2.0',2019-11-12T08:29:19Z,2019-12-21T12:38:48Z,,"""NoAns_exact"", ""NoAns_f1"", ""NoAns_total""","""NoAns_exact"": 84.0874684608915,""NoAns_f1"": 84.0874684608915,""NoAns_total"": 5945"
1802,b'pip cannot install transformers with python version 3.8.0',2019-11-12T08:23:08Z,2020-05-17T17:17:32Z,wontfix,,
1801,b'run_glue.py RuntimeError: module must have its parameters and buffers on device cuda:0 (device_ids[0]) but found one of them on device: cuda:3',2019-11-12T05:22:52Z,2020-06-06T08:14:41Z,wontfix,RuntimeError,"RuntimeError: module must have its parameters and buffers on device cuda:0 (device_ids[0]) but found one of them on device: cuda:3`"
1800,b'Exact and F1 score do not increase when fine-tunes XLM on the SQuAD dataset',2019-11-12T03:47:56Z,2020-01-19T00:12:50Z,wontfix,,
1799,b'Exact and F1 score do not increase when fine-tunes XLM on the SQuAD dataset',2019-11-12T03:43:09Z,2020-04-11T22:44:18Z,wontfix,,
1798,b'Add an LSTM and CNN layer on top of BERT embeddings for sentiment analysis task',2019-11-12T03:28:27Z,2020-01-19T00:12:48Z,wontfix,,
1797,b'TF: model forwards can take an inputs_embeds param',2019-11-12T03:27:37Z,2019-11-12T16:29:22Z,,,
1796,b'Fix GPT2LMHeadModel.from_pretrained(from_tf=True)',2019-11-12T01:36:51Z,2020-03-11T14:30:31Z,wontfix,,
1795,b'RuntimeError: Connection timed out in Single node Multi GPU training',2019-11-12T00:59:29Z,2020-01-31T20:35:05Z,wontfix,RuntimeError,RuntimeError: Connection timed out
1794,b'Confused by GPT2DoubleHeadsModel example',2019-11-11T21:21:26Z,2019-11-13T20:23:31Z,,,
1793,b'MNLI: BERT No Training Progress',2019-11-11T21:17:32Z,2020-01-19T00:12:47Z,wontfix,,
1792,b'DistilBERT for token classification',2019-11-11T16:35:53Z,2019-11-14T21:47:54Z,,,
1791,b'token indices sequence length is longer than the specified maximum sequence length ',2019-11-11T14:05:26Z,2019-12-04T12:22:39Z,,,
1790,b'transformers vs pytorch_pretrained_bert giving different scores for BertForNextSentencePrediction',2019-11-11T11:08:57Z,2019-12-04T13:47:37Z,,,
1789,b'BertForMultipleChoice QuickTour issue with weights?',2019-11-11T10:34:28Z,2019-11-12T07:52:50Z,,RuntimeError,"RuntimeError: shape '[-1, 16]' is invalid for input of size 1"
1788,b'BertForNextSentencePrediction is giving high score for non similar sentences .',2019-11-11T10:32:53Z,2020-05-02T19:39:11Z,wontfix,,
1787,b'Invalid argument with CTRLModel',2019-11-11T08:21:15Z,2019-12-04T12:03:34Z,,OSError,"OSError: [Errno 22] Invalid argument"
1786,b'a BertForMaskedLM.from_pretrained error',2019-11-11T02:44:37Z,2019-11-29T04:36:21Z,,,
1785,"b'""Write with Transformer"" source code?'",2019-11-10T23:55:08Z,2019-11-11T14:16:01Z,Write With Transformer,,
1784,b'Unclear documentation for special_tokens_mask',2019-11-10T22:26:55Z,2019-11-11T15:15:57Z,,,
1783,b'How to measure similarity of words?',2019-11-10T10:28:08Z,2020-01-20T00:33:17Z,wontfix,,
1782,b'    model = GPT2LMHeadModel.from_pretrained(args.model_path) try loads in json format',2019-11-10T08:19:58Z,2019-11-14T15:40:31Z,,UnicodeDecodeError,"UnicodeDecodeError: 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte"
1781,b'Dose the file /examples/run_lm_finetuning.py provide a demo to pre-train a BERT',2019-11-10T07:34:47Z,2019-12-04T11:58:40Z,,,
1780,b'Problems when restoring the pretrain weights for TFbert',2019-11-10T05:23:18Z,2020-02-02T10:44:24Z,wontfix,,
1779,b'Multi GPU dataparallel crash',2019-11-10T05:10:57Z,2019-11-13T12:31:20Z,,**RuntimeError,"**RuntimeError: Gather got an input of invalid size: got [2, 2, 12, 1024, 64], but expected [2, 3, 12, 1024, 64] (gather at /opt/conda/conda-**bld/pytorch_1565272279342/work/torch/csrc/cuda/comm.cpp:226)"
1778,b'from_pretrained: convert DialoGPT format',2019-11-09T16:32:54Z,2019-11-28T15:08:38Z,,,
1777,b'Could you support albert?',2019-11-09T14:38:21Z,2019-11-09T16:09:59Z,,,
1776,b'Extracting the output layer of HuggingFace GPT2DoubleHeadsModel',2019-11-09T10:13:44Z,2019-11-09T13:23:19Z,,,
1775,b'pip install transformers not downloading gpt2-xl',2019-11-08T18:33:42Z,2019-11-08T19:21:00Z,,OSError,"OSError: Model name 'gpt2-xl' was not found in model name list (gpt2, gpt2-medium, gpt2-large, distilgpt2). We assumed 'gpt2-xl' was a path or url to a configuration file named config.json or a directory containing such a file but couldn't find any such file at this path or url."
1774,"b'For HuggingFace GPT2DoubleHeadsModel, is there a way to directly provide a hidden state for an input? '",2019-11-08T16:22:57Z,2020-01-17T15:57:04Z,wontfix,,
1773,b'[WIP] BertAbs summarization',2019-11-08T14:56:19Z,2019-12-10T01:37:56Z,,,
1772,b'Fix run_squad.py',2019-11-08T14:34:23Z,2020-03-11T14:30:32Z,wontfix,,
1771,b'Error when Fine-tuning XLM on SQuA',2019-11-08T10:26:29Z,2020-01-15T11:34:22Z,wontfix,AttributeError,"AttributeError: 'XLMTokenizer' object has no attribute 'do_lower_case'"
1770,b'Only init encoder_attention_mask if stack is decoder',2019-11-08T10:23:10Z,2019-11-28T15:06:56Z,,,
1769,b'[XLM-R] by Facebook AI Research',2019-11-08T08:07:35Z,2020-03-16T15:10:48Z,wontfix,,
1768,"b'BERT: Uncased vocabulary has 30,552 tokens whereas cased has 28,996 tokens. Why this difference?'",2019-11-08T07:03:08Z,2020-01-14T08:39:44Z,wontfix,,
1767,b'GPT2 - XL',2019-11-08T05:03:41Z,2019-11-13T04:41:13Z,,OSError,"OSError: file gpt2-xl not found"
1766,b'Added additional training utils for run_squad.py',2019-11-07T22:42:14Z,2019-12-21T11:42:25Z,,,
1765,b'Fix run_bertology.py',2019-11-07T22:28:08Z,2019-11-08T21:28:41Z,,,
1764,b'Bug-fix: Roberta Embeddings Not Masked',2019-11-07T18:05:01Z,2019-12-21T11:13:28Z,,,
1763,b'Fixed training for TF XLNet',2019-11-07T17:51:28Z,2019-12-03T17:28:08Z,,,
1762,b'Perplexity for (not-stateful) Transformer - Why is it still fair to compare to RNN?',2019-11-07T17:35:22Z,2020-01-13T18:57:49Z,wontfix,,
1761,b'Roberta Positional Embeddings Not Masked',2019-11-07T14:48:01Z,2020-01-06T18:06:33Z,wontfix,,
1760,"b""'RuntimeError: CUDA error' is occured when encoding text with pre-trained model on cuda""",2019-11-07T12:59:24Z,2019-11-12T01:30:31Z,,RuntimeError,"RuntimeError: CUDA error: device-side assert triggered"
1759,"b""NameError: name 'DUMMY_INPUTS' is not defined""",2019-11-07T10:08:09Z,2020-01-19T21:12:43Z,wontfix,,
1758,"b'the BertModel have the class ""BertForTokenClassification"", why XLNetModel don\'t have the class'",2019-11-07T08:39:47Z,2020-01-13T09:57:49Z,wontfix,,
1757,b'Is it possible fine tune XLNet?',2019-11-07T07:57:32Z,2020-01-19T00:12:49Z,wontfix,,
1756,b'Is it okay to define and use new model by using only a part of full GPT model blocks? Or anyone tried to do so?',2019-11-07T07:18:14Z,2019-12-02T14:51:38Z,,,
1755,b'How to add weighted CrossEntropy loss in sequence classification task?',2019-11-07T06:50:18Z,2019-12-02T14:50:03Z,,,
1754,b'Out of Memory Error (OOM) only during evaluation phase of run_lm_finetuning.py and run_glue.py',2019-11-06T22:30:20Z,2020-03-16T00:35:34Z,wontfix,,
1753,b'Added Mish Activation Function',2019-11-06T22:16:25Z,2019-11-28T14:24:08Z,,,
1752,b'Subtokens in BPE in GPT2',2019-11-06T21:20:03Z,2019-11-06T23:13:08Z,,,
1751,b'input token embedding issues',2019-11-06T18:57:53Z,2020-01-12T19:39:03Z,wontfix,,
1750,b'How to calculate memory requirements of different GPT models? ',2019-11-06T18:07:58Z,2020-01-14T13:39:44Z,wontfix,,
1749,b'gpt2 generation crashes when using `past` for some output lengths',2019-11-06T16:43:41Z,2019-11-14T15:10:46Z,,RuntimeError,"RuntimeError: index out of range: Tried to access index 1024 out of table with 1023 rows. at /Users/distiller/project/conda/conda-bld/pytorch_1570710797334/work/aten/src/TH/generic/THTensorEvenMoreMath.cpp:418"
1748,b'Released OpenAI GPT-2 1.5B model',2019-11-06T16:31:05Z,2019-11-06T16:37:37Z,,,
1747,b'How to use gpt-2-xl with run_generation.py',2019-11-06T16:11:15Z,2020-01-12T17:39:03Z,wontfix,,
1746,b'Fixing models inputs_embeds ',2019-11-06T15:57:37Z,2019-11-06T19:03:48Z,,,
1745,b'How to stop wordpiece-tokenizing in BertTokenizer?',2019-11-06T14:07:05Z,2019-11-06T19:56:09Z,,,
1744,b'F1 socre is zero while loss is about 0.12xx when using run_ner.py to fine tuning bert model',2019-11-06T13:57:22Z,2019-11-25T16:08:00Z,,,
1743,b'Unlimited sequence length in Bert for QA',2019-11-06T10:00:11Z,2019-11-06T10:02:56Z,,,
1742,b'Out of Memory (OOM) when repeatedly running large models',2019-11-06T06:15:26Z,2019-11-18T09:09:04Z,,RuntimeError,"RuntimeError: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 0; 15.75 GiB total capacity; 14.24 GiB already allocated; 8.88 MiB free; 476.01 MiB cached)"
1741,b'GPT-2 XL',2019-11-05T18:31:20Z,2019-11-05T18:31:59Z,,,
1740,b'Fix CTRL past',2019-11-05T16:28:01Z,2019-11-27T22:28:31Z,,,
1739,b'[WIP] Adding Google T5 model',2019-11-05T15:45:24Z,2019-12-14T08:40:44Z,,,
1738,b'BART',2019-11-05T13:57:01Z,2019-11-05T15:20:40Z,,,
1737,b'Documentation: Updating docblocks in optimizers.py',2019-11-05T11:06:38Z,2019-11-05T22:31:30Z,,,
1736,b'Fix TFXLNet',2019-11-05T10:27:07Z,2019-12-21T11:42:06Z,,,
1735,b'Do not use GPU when importing transformers',2019-11-05T10:23:09Z,2019-12-05T10:56:48Z,,,
1734,b'add progress bar to convert_examples_to_features',2019-11-05T08:34:36Z,2019-11-05T10:34:22Z,,,
1733,b'\xf0\x9f\x8c\x9fNew model addition: VL-BERT',2019-11-05T03:04:19Z,2020-05-08T22:06:07Z,wontfix,,
1732,b'TFBertForSequenceClassification.from_pretrained ERROR',2019-11-05T03:00:03Z,2019-11-05T10:49:25Z,,UnicodeDecodeError,"UnicodeDecodeError: 'utf-8' codec can't decode byte 0x89 in position 0: invalid start byte"
1731,b'Threads running on evaluation?',2019-11-05T02:18:15Z,2019-11-05T04:58:39Z,,,
1730,"b""resize_token_embeddings doesn't work as expected for BertForMaskedLM""",2019-11-05T00:51:24Z,2020-05-22T19:57:47Z,wontfix,RuntimeError,"RuntimeError: Error(s) in loading state_dict for BertForMaskedLM:"
1729,b'Roberta embeddings comparison',2019-11-04T22:56:55Z,2020-02-10T19:19:17Z,wontfix,,
1728,b'glue_convert_examples_to_features not working if no task is provided',2019-11-04T21:36:24Z,2020-01-10T22:39:28Z,wontfix,,
1727,"b'loss is nan, for training on MNLI dataset'",2019-11-04T21:26:16Z,2020-04-04T16:38:33Z,wontfix,,
1726,b'Exceeding max sequence length in Roberta',2019-11-04T18:35:47Z,2019-12-04T12:19:53Z,,RuntimeError,"RuntimeError: index out of range at /opt/conda/conda-bld/pytorch_1550796191843/work/aten/src/TH/generic/THTensorEvenMoreMath.cpp:191"
1725,b'GPT2 text generation repeat',2019-11-04T16:49:58Z,2019-11-07T16:07:06Z,,,
1724,b'Fix encode_plus',2019-11-04T16:09:17Z,2019-11-27T16:14:49Z,,,
1723,b'Fix #1623',2019-11-04T15:22:58Z,2019-11-05T07:36:31Z,,,
1722,"b'BUG for XLNet: Low GPU usage and High CPU usage, very low running speed!'",2019-11-04T15:06:29Z,2020-01-31T04:35:05Z,wontfix,,
1721,b'Add common getter and setter for input_embeddings & output_embeddings',2019-11-04T11:30:25Z,2019-11-04T15:21:53Z,,,
1720,b'run_generation.py Runtime error',2019-11-04T11:23:25Z,2020-01-13T09:57:48Z,wontfix,RuntimeError,"RuntimeError: cannot reshape tensor of 0 elements into shape [-1, 0]_"
1719,b'Can we fine tune GPT2 using multiple inputs?',2019-11-04T09:33:50Z,2019-11-06T16:28:36Z,,,
1718,"b'Hello, how to upload a .ckpt file in TFBertForSequenceClassification?'",2019-11-04T08:11:44Z,2020-03-12T10:40:36Z,wontfix,,
1717,b'Retaining unknown token behaver consistency in tokenizer for BERT and XLNET',2019-11-04T08:05:13Z,2019-11-05T10:02:24Z,,,
1716,b'add qa and result',2019-11-04T05:04:28Z,2019-12-05T09:20:49Z,,,
1715,b'Retaining unknown token behaver consistency in tokenizer for BERT and XLNET',2019-11-04T04:22:37Z,2019-11-04T07:16:09Z,,,
1714,b'How to train from scratch',2019-11-03T21:03:54Z,2019-11-06T19:45:28Z,,,
1713,"b""How to mask lm_labels and compute loss? --- Finetune gpt2: masking the lm_labels with '-1' and padding increase the perplexity a lot! """,2019-11-03T20:52:53Z,2020-01-24T21:24:39Z,wontfix,,
1712,b'Scheduler documentation blocks subtly wrong',2019-11-03T20:45:11Z,2019-11-05T22:45:56Z,,,
1711,"b""transformers module doesn't work with torch compiled on Cuda 10.0?""",2019-11-03T19:58:42Z,2019-11-03T20:05:55Z,,,
1710,"b'CTRL does not react to the ""seed"" argument'",2019-11-03T17:06:29Z,2019-11-05T02:20:04Z,,,
1709,b'Fixing mode in evaluate during training',2019-11-03T10:46:43Z,2019-11-05T09:55:34Z,,,
1708,b'The id of the word obtained by tokenizer.encode does not correspond to the id of the word in vocab.txt',2019-11-03T09:31:11Z,2020-01-11T23:47:08Z,wontfix,,
1707,b'bugs with run_summarization script',2019-11-03T09:08:05Z,2020-01-10T07:39:28Z,wontfix,,
1706,b'Regression Loss',2019-11-03T03:17:08Z,2020-01-11T23:47:07Z,wontfix,,
1705,b'unable to import from utils_squad ',2019-11-02T22:40:52Z,2019-11-14T08:57:59Z,,ModuleNotFoundError,"ModuleNotFoundError: No module named 'utils_squad'"
1704,"b'loss is nan, for training on MNLI dataset'",2019-11-02T18:58:34Z,2019-11-02T23:49:25Z,,,
1703,b'Add `model.train()` line to ReadMe training example ',2019-11-02T18:31:35Z,2019-11-04T16:54:31Z,,,
1702,b'Interpretation of output from fine-tuned BERT for Masked Language Modeling.',2019-11-02T04:11:46Z,2019-12-04T11:59:04Z,,,
1701,"b'When I uesd gpt2, I got a error'",2019-11-02T03:05:05Z,2019-11-02T13:18:52Z,,json.decoder.JSONDecodeError,"json.decoder.JSONDecodeError: Unterminated string starting at: line 1 column 295508 (char 295507)`"
1700,b'Best practices for Bert passage similarity. Perhaps further processing Bert vectors; Bert model inside another model ',2019-11-02T02:54:49Z,2020-01-08T03:51:07Z,wontfix,,
1699,b'Why do I get 13 hidden layers?',2019-11-02T02:47:08Z,2019-11-02T16:39:05Z,,,
1698,b'add_tokens() leading to wrong behavior',2019-11-02T01:11:33Z,2019-11-26T23:28:15Z,,,
1697,b'PPLM (squashed)',2019-11-01T20:36:31Z,2019-12-03T15:14:03Z,,,
1696,b'Invalid argument error with TFRoberta on GLUE',2019-11-01T18:08:12Z,2020-02-27T01:31:19Z,wontfix,tensorflow.python.framework.errors_impl.InvalidArgumentError,"tensorflow.python.framework.errors_impl.InvalidArgumentError:  indices[28,20] = 1 is not in [0, 1)"
1695,b'model forwards can take an inputs_embeds param',2019-11-01T16:29:21Z,2019-11-05T14:55:29Z,,,
1694,b'solves several bugs in the summarization codes ',2019-11-01T16:16:30Z,2019-12-21T11:39:33Z,,,
1693,b'TFXLNet Incompatible shapes in relative attention',2019-11-01T16:10:45Z,2020-01-13T18:57:50Z,wontfix,tensorflow.python.framework.errors_impl.InvalidArgumentError,"tensorflow.python.framework.errors_impl.InvalidArgumentError:  Incompatible shapes: [512,512,32,12] vs. [512,1023,32,12]"
1692,b'TFXLNet int32 to float promotion error',2019-11-01T15:37:32Z,2019-12-21T11:42:05Z,,TypeError,"TypeError: in converted code:"
1691,b'ALbert Model implementation is finished on squad qa task. but some format is different with huggingface.(specify on albert qa task)',2019-11-01T15:21:03Z,2019-11-01T16:29:59Z,,,
1690,b'T5',2019-11-01T15:14:12Z,2019-11-01T15:49:35Z,,,
1689,"b""Can't export TransfoXLModel model""",2019-11-01T12:03:24Z,2020-01-11T09:47:07Z,wontfix,"torch.jit.frontend.UnsupportedNodeError, KeyError","torch.jit.frontend.UnsupportedNodeError: GeneratorExp aren't supported:KeyError: 'triu'"
1688,b'fine tuning bert and roberta_base model',2019-11-01T11:17:38Z,2019-11-01T13:14:10Z,,,
1687,b'request for a Bert_base uncase model.bin file',2019-11-01T09:51:23Z,2019-11-01T13:13:19Z,,,
1686,b'OpenAIGPTDoubleHeadsModel Not working (even with the official example...)',2019-11-01T04:09:12Z,2019-11-05T08:38:07Z,,RuntimeError,"RuntimeError: Invalid index in gather at /pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp:657"
1685,b'Unpickling errors when running examples',2019-10-31T21:01:30Z,2019-11-01T01:19:55Z,,UnpicklingError,"UnpicklingError: invalid load key, '<'."
1684,b'Access denied to pretrained GPT2 model',2019-10-31T21:00:47Z,2019-12-31T20:41:01Z,wontfix,UnpicklingError,"UnpicklingError: invalid load key, '<'."
1683,b'Add ALBERT to the library',2019-10-31T18:17:08Z,2019-11-26T18:08:14Z,,,
1682,b'xnli benchmark',2019-10-31T16:31:24Z,2019-11-27T16:07:23Z,,,
1681,b'Wrong Roberta special tokens in releases on GitHub',2019-10-31T16:21:38Z,2019-10-31T20:23:38Z,,,
1680,b'Error when creating RobertTokenizer for distilroberta-base',2019-10-31T10:26:20Z,2019-10-31T12:54:15Z,,OSError,"OSError: Model name 'distilroberta-base' was not found in tokenizers model name list (roberta-base, roberta-large, roberta-large-mnli). We assumed 'distilroberta-base' was a path or url to a directory containing vocabulary files named ['vocab.json', 'merges.txt'] but couldn't find such vocabulary files at this path or url."
1679,b'Fix https://github.com/huggingface/transformers/issues/1673',2019-10-31T09:09:24Z,2019-11-01T21:02:25Z,,,
1678,b'Download assets directly to the specified cache_dir',2019-10-31T08:39:19Z,2019-12-21T11:07:51Z,,,
1677,b'i want to use bert pre-trained modle in a text classification problem which the text with Multi-label. But\xef\xbc\x8cthere are some problems .',2019-10-31T04:42:14Z,2020-01-06T09:13:43Z,wontfix,,
1676,b'\xf0\x9f\x8c\x9f BART',2019-10-31T02:41:24Z,2020-02-20T23:11:14Z,,,
1675,b'Any example of how to do multi-class classification with TFBertSequenceClassification',2019-10-31T00:14:58Z,2020-03-17T14:16:14Z,wontfix,,
1674,b'possible issues with run_summarization_finetuning.py',2019-10-30T20:20:52Z,2020-01-10T07:39:29Z,wontfix,,
1673,"b'BertModel.from_pretrained is failing with ""HTTP 407 Proxy Authentication Required"" during model weight download when running behing a proxy'",2019-10-30T12:53:40Z,2019-11-01T21:02:27Z,,"`OSError, OSError, urllib3.exceptions.MaxRetryError, requests.exceptions.ProxyError","`OSError: Tunnel connection failed: 407 Proxy Authentication Required` . This could be the symptom of `proxies` parameter not being passed through the `request` package commands.OSError: Tunnel connection failed: 407 Proxy Authentication Requiredurllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='s3.amazonaws.com', port=443): Max retries exceeded with url: /models.huggingface.co/bert/bert-base-cased-config.json (Caused by ProxyError('Cannot connect to proxy.', OSError('Tunnel connection failed: 407 Proxy Authentication Required',)))requests.exceptions.ProxyError: HTTPSConnectionPool(host='s3.amazonaws.com', port=443): Max retries exceeded with url: /models.huggingface.co/bert/bert-base-cased-config.json (Caused by ProxyError('Cannot connect to proxy.', OSError('Tunnel connection failed: 407 Proxy Authentication Required',)))"
1672,b'Is HuggingFace TransfoXLLMHeadModels trainable from scratch?',2019-10-30T12:42:17Z,2020-01-11T08:47:07Z,wontfix,,
1671,b'Quick Tour TF2.0 Training Script has Control Flow Error when Replacing TFBERT with TFRoberta',2019-10-30T11:06:16Z,2020-01-10T20:39:27Z,wontfix,TypeError,"TypeError: You are attempting to use Python control flow in a layer that was not declared to be dynamic. Pass `dynamic=True` to the class constructor."
1670,b'Templates and explanation for adding a new model and example script',2019-10-30T10:40:38Z,2019-10-30T16:05:59Z,,,
1669,b'How to load trained model of distilbert',2019-10-30T09:55:08Z,2020-01-06T15:13:38Z,wontfix,,
1668,b'Fixed training for TF XLM',2019-10-30T01:35:20Z,2019-10-30T16:08:00Z,,,
1667,b'Added FP16 support to benchmarks.py',2019-10-30T01:16:05Z,2019-10-31T21:24:38Z,,,
1666,b'Question: Token sequence length longer maximum sequence length',2019-10-29T23:41:39Z,2020-02-03T14:47:37Z,wontfix,,
1665,b'Allowing PR#1455 to be merged in the master ',2019-10-29T21:19:22Z,2019-10-30T09:24:28Z,,,
1664,"b""Moving model from GPU -> CPU doesn't work""",2019-10-29T21:15:40Z,2019-10-30T14:44:32Z,,,
1663,b'Problem with restoring GPT-2 weights',2019-10-29T18:21:21Z,2020-03-18T09:40:01Z,wontfix,,
1662,b'Tokenizer.tokenize return none on some utf8 string in current pypi version',2019-10-29T17:07:58Z,2019-11-05T15:44:08Z,,,
1661,b'BERT multi heads attentions',2019-10-29T15:21:03Z,2020-01-04T17:41:56Z,wontfix,,
1660,b'How to fine-tune CTRL?',2019-10-29T12:43:44Z,2019-12-30T04:02:27Z,wontfix,,
1659,b'How is the interactive GPT-2 implemented?',2019-10-29T04:55:57Z,2020-01-07T22:40:47Z,wontfix,,
1658,b'How to fine tune xlm-mlm-100-128 model.',2019-10-29T00:56:22Z,2020-01-03T10:17:06Z,wontfix,,
1657,b'[WIP] Raise error if larger sequences',2019-10-29T00:02:12Z,2020-01-04T00:41:52Z,wontfix,,
1656,b'Parallel data preprocessing for distillation',2019-10-28T23:20:20Z,2020-01-06T20:13:39Z,wontfix,,
1655,b'Missing a line in examples/distillation/README.md',2019-10-28T23:09:44Z,2019-10-30T18:25:52Z,,,
1654,b'Can I load a CTRL model that was fine-tuned using the Salesforce code?',2019-10-28T21:18:17Z,2019-11-01T13:47:48Z,,,
1653,b'No way to control ID of special chars e.g. mask IDs',2019-10-28T20:28:44Z,2020-01-10T17:39:28Z,wontfix,,
1652,"b""Missing required argument 'mode' in run_ner.""",2019-10-28T19:43:11Z,2020-01-10T17:39:34Z,wontfix,,
1651,b'How to set local_rank argument in run_squad.py',2019-10-28T15:51:47Z,2019-10-29T10:22:15Z,,,
1650,b'Custom language text generation',2019-10-28T05:05:36Z,2019-12-30T07:09:18Z,wontfix,,
1649,b'ALBERT',2019-10-28T03:56:45Z,2019-10-28T09:51:04Z,,,
1648,b'Changing LM loss function',2019-10-28T01:15:11Z,2020-01-10T17:39:33Z,wontfix,,
1647,b'distilroberta-base unavailable in pip install transformers',2019-10-28T00:00:42Z,2019-10-28T13:36:34Z,,,
1646,b'Undefined behavior',2019-10-27T20:10:51Z,2020-01-10T17:39:31Z,wontfix,,
1645,b'Error while importing RoBERTa model',2019-10-27T19:37:31Z,2019-10-29T21:00:18Z,,`RuntimeError,"`RuntimeError: Error(s) in loading state_dict for RobertaModel: Missing key(s) in state_dict: ""decoder.sentence_encoder.layers.0.self_attn.k_proj.weight"", ""decoder.sentence_encoder.layers.0.self_attn.k_proj.bias"", ""decoder.sentence_encoder.layers.0.self_attn.v_proj.weight"", ""decoder.sentence_encoder.layers.0.self_attn.v_proj.bias"", ""decoder.sentence_encoder.layers.0.self_attn.q_proj.weight"", ""decoder.sentence_encoder.layers.0.self_attn.q_proj.bias"", ""decoder.sentence_encoder.layers.1.self_attn.k_proj.weight"", ""decoder.sentence_encoder.layers.1.self_attn.k_proj.bias"", ""decoder.sentence_encoder.layers.1.self_attn.v_proj.weight"", ""decoder.sentence_encoder.layers.1.self_attn.v_proj.bias"", ""decoder.sentence_encoder.layers.1.self_attn.q_proj.weight"", ""decoder.sentence_encoder.layers.1.self_attn.q_proj.bias"", ""decoder.sentence_encoder.layers.2.self_attn.k_proj.weight"", ""decoder.sentence_encoder.layers.2.self_attn.k_proj.bias"", ""decoder.sentence_encoder.layers.2.self_attn.v_proj.weight"", ""decoder.sentence_encoder.layers.2.self_attn.v_proj.bias"", ""decoder.sentence_encoder.layers.2.self_attn.q_proj.weight"", ""decoder.sentence_encoder.layers.2.self_attn.q_proj.bias"", ""decoder.sentence_encoder.layers.3.self_attn.k_proj.weight"", ""decoder.sentence_encoder.layers.3.self_attn.k_proj.bias"", ""decoder.sentence_encoder.layers.3.self_attn.v_proj.weight"", ""decoder.sentence_encoder.layers.3.self_attn.v_proj.bias"", ""decoder.sentence_encoder.layers.3.self_attn.q_proj.weight"", ""decoder.sentence_encoder.layers.3.self_attn.q_proj.bias"", ""decoder.sentence_encoder.... Unexpected key(s) in state_dict: ""decoder.sentence_encoder.layers.0.self_attn.in_proj_weight"", ""decoder.sentence_encoder.layers.0.self_attn.in_proj_bias"", ""decoder.sentence_encoder.layers.1.self_attn.in_proj_weight"", ""decoder.sentence_encoder.layers.1.self_attn.in_proj_bias"", ""decoder.sentence_encoder.layers.2.self_attn.in_proj_weight"", ""decoder.sentence_encoder.layers.2.self_attn.in_proj_bias"", ""decoder.sentence_encoder.layers.3.self_attn.in_proj_weight"", ""decoder.sentence_encoder.layers.3.self_attn.in_proj_bias"", ""decoder.sentence_encoder.layers.4.self_attn.in_proj_weight"", ""decoder.sentence_encoder.layers.4.self_attn.in_proj_bias"", ""decoder.sentence_encoder.layers.5.self_attn.in_proj_weight"", ""decoder.sentence_encoder.layers.5.self_attn.in_proj_bias"", ""decoder.sentence_encoder.layers.6.self_attn.in_proj_weight"", ""decoder.sentence_encoder.layers.6.self_attn.in_proj_bias"", ""decoder.sentence_encoder.layers.7.self_attn.in_proj_weight"", ""decoder.sentence_encoder.layers.7.self_attn.in_proj_bias"", ""decoder.sentence_encoder.layers.8.self_attn.in_proj_weight"", ""decoder.sentence_encoder.layers.8.self_attn.in_proj_bias"", ""decoder.sentence_encoder.layers.9.self_attn.in_proj_weight"", ""decoder.sentence_encoder.layers.9.self_attn.in_proj_bias"", ""decoder.sentence_encoder.layers.10.self_attn.in_proj_weight"", ""decoder.sentence_encoder.layers.10.self_attn.in_proj_bias"", ""decoder.sentence_encoder.layers.11.self_attn.in_proj_weight"", ""decoder.sentence_encoder.layers.11.self_attn.in_proj_bi...`"
1644,b'Maximum length of out put generated in run_generation.py is of length (1021 ) despite changing position id length and length parameter',2019-10-27T19:01:36Z,2020-01-07T00:36:55Z,wontfix,,
1643,b'how to use  BertForMaskedLM',2019-10-27T17:25:59Z,2020-01-02T18:22:11Z,wontfix,,
1642,b'How to compute loss with HuggingFace transformers?',2019-10-27T12:35:19Z,2020-01-02T16:22:13Z,wontfix,,
1641,b'How to use custom built Torchtext vocabulary with HuggingFace TransfoXLLMHeadModel?',2019-10-27T08:59:51Z,2020-01-02T10:55:15Z,wontfix,,
1640,b'Why DistilBertTokenizer and BertTokenizer are creating different number of features?? ',2019-10-27T07:43:27Z,2019-10-27T10:53:06Z,,,
1639,b'Add Transformer-XL fine-tuning support.',2019-10-26T10:25:48Z,2020-01-10T17:39:30Z,wontfix,,
1638,b'how can I pre-training my own model from the existed model or from scratch',2019-10-26T05:09:24Z,2020-01-11T05:47:05Z,wontfix,,
1637,"b'Installation error :Command ""python setup.py egg_info"" failed with error code 1'",2019-10-26T00:43:53Z,2019-10-31T03:39:07Z,,pip._internal.exceptions.InstallationError,"pip._internal.exceptions.InstallationError: Command ""python setup.py egg_info"" failed with error code 1 in /short/oe7/uk1594/tmp/pip-install-5c0k51ol/sentencepiece/"
1636,"b""AttributeError: 'CTRLTokenizer' object has no attribute 'control_codes'""",2019-10-25T22:08:36Z,2019-10-31T19:04:06Z,,AttributeError,"AttributeError: 'CTRLTokenizer' object has no attribute 'control_codes'"
1635,b'Training DistilBert - RuntimeError: index out of range at /pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp:237',2019-10-25T20:02:20Z,2019-10-25T22:23:15Z,,RuntimeError,"RuntimeError: index out of range: Tried to access index 61578 out of table with 30521 rows. at /pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp:237"
1634,b'How to initialize AdamW optimizer in HuggingFace transformers?',2019-10-25T13:58:20Z,2019-10-29T09:33:01Z,,,
1633,b'Fix for mlm evaluation in run_lm_finetuning.py',2019-10-25T10:31:29Z,2019-10-28T14:18:59Z,,,
1632,"b'Loading from ckpt is not possible for bert, neither tf to pytorch conversion works in 2.1.1'",2019-10-25T10:09:29Z,2019-12-31T11:32:06Z,wontfix,,
1631,"b""cannot import name 'RobertaForTokenClassification'""",2019-10-25T09:06:51Z,2020-01-03T03:22:12Z,wontfix,ImportError,"ImportError: cannot import name 'RobertaForTokenClassification'"
1630,b'rename _has_sklearn to _sklearn_available',2019-10-25T07:18:02Z,2019-12-26T11:25:46Z,wontfix,,
1629,b'Perm Mask in XLNet',2019-10-25T07:06:54Z,2019-12-31T08:32:06Z,wontfix,,
1628,b'run_tf_glue works with all tasks',2019-10-24T21:44:57Z,2019-10-30T16:04:04Z,,,
1627,"b'Loading pretrained RobertaForSequenceClassification fails, size missmatch error'",2019-10-24T21:21:45Z,2019-10-25T04:24:39Z,,RuntimeError,"RuntimeError: Error(s) in loading state_dict for RobertaForSequenceClassification:"
1626,b'What is currently the best way to add a custom dictionary to a neural machine translator that uses the transformer architecture?',2019-10-24T17:48:10Z,2020-01-04T09:41:58Z,wontfix,,
1625,b'Update run_ner.py example with RoBERTa',2019-10-24T17:38:31Z,2019-10-24T18:32:49Z,,,
1624,b'Add support for resumable downloads for HTTP protocol.',2019-10-24T15:25:04Z,2019-11-27T16:11:07Z,,,
1623,b'--cache_dir argument in run_lm_finetuning.py not used at all',2019-10-24T14:21:52Z,2019-11-05T07:36:33Z,,,
1622,b'Fine-tuning BERT using Next sentence prediction loss',2019-10-24T13:37:11Z,2019-10-24T13:47:35Z,,,
1621,b'tokenization slow',2019-10-24T12:34:02Z,2020-01-13T17:16:06Z,wontfix,,
1620,"b""'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte""",2019-10-24T12:12:28Z,2019-10-25T08:57:52Z,,,
1619,"b""AttributeError: 'BertForPreTraining' object has no attribute 'classifier'""",2019-10-24T09:34:18Z,2019-12-31T07:32:06Z,wontfix,AttributeError,"AttributeError: 'BertPreTrainingHeads' object has no attribute 'classifier'"
1618,b'Format problem when training DistilBert',2019-10-24T09:25:43Z,2019-10-25T19:51:21Z,,RuntimeError,"RuntimeError: Expected object of scalar type Byte but got scalar type Bool for argument #2 'mask'"
1617,b'Add T5 model',2019-10-24T09:25:37Z,2020-01-03T21:41:57Z,wontfix,,
1616,b'run_generation.py example for a batch',2019-10-24T09:16:15Z,2019-12-30T10:04:12Z,wontfix,,
1615,b'CUDA error: device-side assert triggered(pretrained_model.cuda())',2019-10-24T09:12:43Z,2019-10-25T08:58:13Z,,RuntimeError,RuntimeError: CUDA error: device-side assert triggered
1614,b'Slight different output between transformers and pytorch-transformers',2019-10-24T06:56:02Z,2020-01-10T17:39:29Z,wontfix,,
1613,b'Roberta token classification',2019-10-24T05:12:36Z,2019-10-24T18:30:47Z,,,
1612,"b'add model & config address in appendix, and add link to appendix.md i\xe2\x80\xa6'",2019-10-24T03:05:16Z,2019-10-26T05:53:22Z,,,
1611,b'How can I get the probability of a word which fits the masked place?',2019-10-24T02:16:47Z,2019-10-24T04:31:34Z,,,
1610,b'Update setup.py',2019-10-23T12:41:51Z,2019-12-21T11:38:16Z,,,
1609,b'Can the prefix for GPT-2 conditional sampling be very long (longer than context window size)?',2019-10-23T12:38:33Z,2019-12-29T13:31:19Z,wontfix,,
1608,"b'Error raised by ""tmp_eval_loss += tmp_eval_loss.item()"" when using multi-gpu'",2019-10-23T12:30:24Z,2019-10-30T16:14:08Z,,,
1607,b'failed to download pretrained weights',2019-10-23T09:49:37Z,2019-10-23T15:04:34Z,,,
1606,b'Show pretrained model and config file download address directly in README.md & doc',2019-10-23T07:29:51Z,2019-11-04T04:51:31Z,,,
1605,"b'Support for gpt2-medium, gpt2-large and distilgpt2 in pytorch-pretrained-bert 0.6.2'",2019-10-23T05:36:54Z,2019-12-29T17:31:19Z,wontfix,,
1604,b'Versioning in documentation',2019-10-22T22:04:21Z,2019-10-30T16:03:15Z,,,
1603,b'[scripts] Proposal: add a specific device flag',2019-10-22T19:34:25Z,2020-01-11T13:47:02Z,wontfix,,
1602,b'Fix architectures count',2019-10-22T19:11:22Z,2019-10-22T19:13:48Z,,,
1601,b'Clean roberta model & all tokenizers now add special tokens by default (breaking change)',2019-10-22T18:20:01Z,2019-10-30T16:00:40Z,,,
1600,b'None in openAi-gpt tokenization',2019-10-22T16:43:20Z,2019-12-28T19:42:14Z,wontfix,,
1599,b'Issue in Cost Function',2019-10-22T15:31:49Z,2019-10-22T19:23:22Z,,,
1598,"b'changing ""out_features"" of final linear layer'",2019-10-22T13:00:48Z,2019-11-08T07:45:49Z,,,
1597,"b""_tokenize() got an unexpected keyword argument 'add_prefix_space' in CTRL""",2019-10-22T12:24:31Z,2019-10-22T15:27:13Z,,,
1596,b'How to use BERT for ENTITY extraction from a Sequence without classification in the NER task ?',2019-10-22T09:51:24Z,2019-12-07T01:56:37Z,,,
1595,b'Using HuggingFace TransfoXLLMHeadModel() with custom Torchtext vocabulary',2019-10-22T08:26:27Z,2020-01-02T10:55:16Z,wontfix,RuntimeError,"RuntimeError: Trying to create tensor with negative dimension -171129: [-171129, 1]"
1594,b'Make benchmark more flexible (TF or PT)',2019-10-22T07:43:02Z,2019-10-22T17:59:15Z,,,
1593,b'Fix AdamW import error for <1.2',2019-10-22T07:35:08Z,2020-04-07T18:00:16Z,,,
1592,b'Consider do_lower_case in PreTrainedTokenizer',2019-10-22T07:06:00Z,2019-11-27T16:05:19Z,,,
1591,b'Error when trying to reuse hidden states in CTRL',2019-10-22T03:01:07Z,2019-11-07T03:58:59Z,,RuntimeError,"RuntimeError: The size of tensor a (13) must match the size of tensor b (7) at non-singleton dimension 3"
1590,b'[WIP] Fixes for TF Roberta (and other models WIP)',2019-10-22T02:53:33Z,2019-11-06T09:02:23Z,,tensorflow.python.framework.errors_impl.InvalidArgumentError,"tensorflow.python.framework.errors_impl.InvalidArgumentError:  Incompatible shapes: [128,128,16,12] vs. [128,255,16,12]"
1589,b'Fix architectures count',2019-10-22T00:30:00Z,2019-12-21T11:36:29Z,,,
1588,b'Using HuggingFace pre-trained transformer to tokenize and generate iterator for a different text than the one it was trained on',2019-10-21T18:59:42Z,2019-12-28T10:42:14Z,wontfix,,
1587,b'Sequence to sequence with GPT model ',2019-10-21T08:31:25Z,2019-10-22T19:39:07Z,,,
1586,b'Add special tokens to documentation for bert examples to resolve issue: #1561',2019-10-21T04:59:55Z,2019-12-21T11:36:11Z,,,
1585,b'AdamW requires torch>=1.2.0',2019-10-21T03:52:13Z,2020-04-05T12:07:51Z,wontfix,,
1584,b'Add special tokens to documentation for bert examples to resolve issue: #1561',2019-10-21T03:51:09Z,2019-10-21T04:00:17Z,,,
1583,b'Question answering for SQuAD with XLNet',2019-10-21T02:25:08Z,2019-12-27T03:31:41Z,wontfix,,
1582,b'How does arg --vocab_transform help in extract_distilbert.py?',2019-10-21T01:01:33Z,2019-12-30T19:04:12Z,wontfix,,
1581,b'Is there a computation/speed advantage to batching inputs into `TransformerModel` to reduce its number calls',2019-10-20T23:07:27Z,2019-12-26T23:31:41Z,wontfix,,
1580,b'Gradient norm clipping should be done right before calling the optimiser',2019-10-20T21:36:20Z,2019-10-22T11:59:20Z,,,
1579,b'seq2seq with gpt2',2019-10-20T17:40:48Z,2019-10-21T07:16:55Z,,,
1578,b'distilled gpt2 to be added to run_generation and run_lm_fintuning',2019-10-20T15:37:12Z,2019-10-22T17:46:39Z,,,
1577,b'Add feature #1572 which gives support for multiple candidate sequences',2019-10-20T15:27:47Z,2019-10-31T18:41:54Z,,,
1576,b'evaluating on race dataset with checkpoints fine tuned on roberta with fairseq',2019-10-20T13:37:25Z,2020-01-27T15:54:42Z,wontfix,,
1575,b'use gpt2 as a seq2seq model',2019-10-20T12:04:19Z,2019-10-21T07:16:46Z,,,
1574,b'Why the output is same within a batch use BertForSequenceClassification?',2019-10-20T07:54:24Z,2019-12-28T17:42:14Z,wontfix,,
1573,b'GPT2 attention mask and output masking',2019-10-20T05:31:35Z,2019-10-22T15:27:25Z,,,
1572,b'Can we generate multiple possible sentences using GPT?',2019-10-20T04:46:35Z,2019-10-31T18:54:28Z,,,
1571,"b'Pytorch Transformers no longer loads SciBert weights, getting `UnicodeDecodeError`. Worked in pytorch_pretrained_bert'",2019-10-19T22:11:59Z,2019-10-20T00:57:37Z,,UnicodeDecodeError,"UnicodeDecodeError: 'utf-8' codec can't decode byte 0x8b in position 1: invalid start byte"
1570,b'Fix Roberta on TPU',2019-10-19T21:38:10Z,2019-12-27T14:20:08Z,wontfix,,
1569,b'TFRobertaForSequenceClassification fails on TPU on Transformers >2.0.0',2019-10-19T21:30:37Z,2019-12-25T23:05:39Z,wontfix,InvalidArgumentError,"InvalidArgumentError:  Compilation failure: Detected unsupported operations when trying to compile graph tf_roberta_for_sequence_classification_roberta_cond_true_122339[] on XLA_TPU_JIT: PrintV2 (No registered 'PrintV2' OpKernel for XLA_TPU_JIT devices compatible with node {{node PrintV2}}"
1568,b'Fix hanging when loading pretrained models',2019-10-19T20:20:02Z,2019-10-21T12:31:58Z,,,
1567,b'Added mixed precision (AMP) to inference benchmark',2019-10-19T09:02:26Z,2019-10-28T17:30:14Z,,,
1566,b'error load bert model \xef\xbc\x9anot found model file',2019-10-19T08:56:17Z,2020-02-10T10:19:21Z,wontfix,OSError,"OSError: Error no file named ['pytorch_model.bin', 'tf_model.h5', 'model.ckpt.index'] found in directory ./uncased_L-12_H-768_A-12_transformers or `from_tf` set to False"
1565,b'How to add the output word vector of bert to my model',2019-10-19T03:14:58Z,2020-02-23T10:52:32Z,wontfix,,
1564,b'ALBERT: will it be supported?',2019-10-19T02:42:16Z,2019-10-23T22:55:06Z,,,
1563,b'The implementation of grad clipping is not correct when gradient accumulation is enabled',2019-10-19T00:12:56Z,2019-11-04T15:06:25Z,,,
1562,b'training BERT on coreference resolution ',2019-10-18T14:32:57Z,2019-12-24T15:40:48Z,wontfix,,
1561,b'[CLS] & [SEP] tokens missing in documentation',2019-10-18T14:12:41Z,2019-10-21T13:18:45Z,,,
1560,b'Finetuning OpenAI GPT-2 for another language.',2019-10-18T11:54:33Z,2020-01-05T09:37:23Z,wontfix,,
1559,b'Compatibility between DistilBert and Bert models',2019-10-18T11:05:16Z,2019-12-24T12:40:49Z,wontfix,,
1558,b'unable to parse E:/litao/bert/bert-base-cased\\config.json as a URL or as a local path',2019-10-18T10:06:42Z,2019-12-24T11:40:49Z,wontfix,,
1557,b'Tuning BERT on our own data set for multi-class classification problem',2019-10-18T06:53:23Z,2019-12-24T08:40:49Z,wontfix,,
1556,"b""Does the function of  'evaluate()' change the result?""",2019-10-18T05:10:50Z,2019-10-24T12:50:53Z,,,
1555,b'Sample a constant number of tokens for masking in LM finetuning',2019-10-18T01:54:02Z,2019-11-13T14:49:05Z,,,
1554,b'GPT2 not in modeltype',2019-10-18T00:24:40Z,2020-02-09T14:58:56Z,wontfix,KeyError,"KeyError: 'gpt2'"
1553,b'Add speed log to examples/run_squad.py',2019-10-17T21:44:08Z,2019-11-05T10:13:13Z,,,
1552,"b""There is not space after generating an 'special token' and the next word using gpt2.""",2019-10-17T20:09:33Z,2020-02-23T10:52:31Z,wontfix,,
1551,b'[FIX] fix repetition penalty in `examples/run_generation.py`',2019-10-17T18:14:20Z,2019-10-17T18:47:15Z,,,
1550,b'training BERT from scratch for native language PT-BR? Without init weight',2019-10-17T17:30:58Z,2019-12-23T21:18:26Z,wontfix,,
1549,b'Fix token order in xlnet preprocessing for SQuAD',2019-10-17T15:14:28Z,2019-11-04T14:37:16Z,,,
1548,b'[2.2] - Command-line interface - Pipeline class',2019-10-17T15:04:10Z,2019-12-20T14:28:30Z,,,
1547,b'Is it possible/is there a plan to enable continued pretraining?',2019-10-17T14:46:56Z,2020-02-09T22:58:56Z,wontfix,,
1546,b'Q / Note: BERT Masked-LM fails to predict last token in sequence if it is not punctuation',2019-10-17T13:19:11Z,2019-12-27T19:20:11Z,wontfix,,
1545,b'Adding new tokens to uncased tokenizers - case insensitivity is lost',2019-10-17T12:57:13Z,2019-12-28T06:42:14Z,wontfix,,
1544,b'the num_labels in run_squad',2019-10-17T11:34:51Z,2019-12-25T18:05:40Z,wontfix,,
1543,b'Where is pytorch-pretrained-BERT?',2019-10-17T07:46:13Z,2019-12-05T10:27:31Z,,,
1542,b'Running CTRL Model On Google Colab Environment',2019-10-17T07:12:27Z,2019-10-20T12:02:25Z,,,
1541,b'Type of model for each GLUE task',2019-10-17T04:41:25Z,2019-11-11T20:40:26Z,,,
1540,b'Should the option to run on TPU in run_glue.py use some sort of xla data parallelizer ? ',2019-10-17T00:38:51Z,2019-12-24T00:18:26Z,wontfix,,
1539,b'A couple of noob-to-transformers questions',2019-10-17T00:29:34Z,2020-02-21T11:15:47Z,wontfix,,
1538,b' Fine-tune RoBERTa on WikiText-2',2019-10-16T22:52:42Z,2020-04-08T10:22:19Z,wontfix,,
1537,"b'Behavior of Masked-LM BERT, dependence on masked token'",2019-10-16T17:56:06Z,2019-10-17T13:03:34Z,,,
1536,b'Penalize high confident false negative classifications?',2019-10-16T12:37:36Z,2019-12-25T08:05:40Z,wontfix,,
1535,b'Why the output of DistilBertModel is inconsistent with BertModel?!',2019-10-16T12:30:57Z,2019-10-19T19:08:09Z,,,
1534,b'run_ner.py file with Distill Bert',2019-10-16T11:45:53Z,2019-10-25T14:57:35Z,,,
1533,b'Add vocabulary gives sequence length warning',2019-10-16T11:34:30Z,2020-02-09T22:58:57Z,wontfix,,
1532,"b""'BertForSequenceClassification' is not defined   'DUMMY_INPUTS' is not defined""",2019-10-16T08:53:40Z,2019-10-16T15:06:34Z,,"NameError, ImportError","NameError: name 'BertForSequenceClassification' is not definedImportError: cannot import name 'BertForSequenceClassification' from 'transformers' "
1531,b'why xlnet requires a long prompt for short inputs while Bert does not ? ',2019-10-16T06:05:51Z,2019-12-22T07:05:09Z,wontfix,,
1530,b'Plan to support UniLM ?',2019-10-16T02:57:26Z,2019-10-16T23:34:47Z,,,
1529,b'Hight CPU and low GPU on XLNet',2019-10-16T01:56:28Z,2020-03-24T09:36:57Z,wontfix,,
1528,b'Question about hidden states in GPT2',2019-10-15T21:57:18Z,2019-10-28T19:45:40Z,,,
1527,b'Training GPT or GPT-2 from scratch',2019-10-15T18:24:10Z,2020-02-09T21:58:57Z,wontfix,,
1526,"b""Alignment of tokens - 'extract_features_aligned_to_words' from fairseq roberta?""",2019-10-15T16:02:44Z,2019-12-21T16:36:31Z,wontfix,,
1525,b'Understanding run_glue in distributed mode',2019-10-15T14:26:41Z,2019-10-16T07:55:11Z,,,
1524,b'Question on AllenNLP vocabulary and huggingface BERT out of sync',2019-10-15T14:08:10Z,2019-12-21T16:36:29Z,wontfix,,
1523,b'Why the codes of training BERT from scratch are deprecated ',2019-10-15T11:30:11Z,2019-12-21T16:36:30Z,wontfix,,
1522,b'When to support Albert?',2019-10-15T09:21:25Z,2019-10-17T11:01:02Z,,,
1521,b'Downloading model in distributed mode',2019-10-15T08:14:12Z,2019-10-15T08:40:44Z,,,
1520,b'Changelog',2019-10-15T07:48:21Z,2019-10-15T13:45:05Z,,,
1519,b'Accuracy drop in finetuning roBERTa',2019-10-15T06:16:20Z,2019-12-21T07:36:29Z,wontfix,,
1518,b'Predefined token classification ',2019-10-15T01:02:57Z,2019-12-21T02:36:29Z,wontfix,,
1517,b'Unable to import TF models',2019-10-14T22:58:46Z,2019-10-15T03:26:58Z,,NameError,"NameError: name 'TFBertForSequenceClassification' is not defined"
1516,b'Fused optimizer and gradient clipper using apex',2019-10-14T20:31:54Z,2020-03-11T14:30:35Z,wontfix,,
1515,b'Main and train for CTRL model',2019-10-14T16:24:33Z,2019-12-20T17:20:32Z,wontfix,,
1514,"b'/pytorch/aten/src/THC/THCTensorScatterGather.cu:100: void THCudaTensor_gatherKernel(TensorInfo<Real, IndexType>, TensorInfo<Real, IndexType>, TensorInfo<long, IndexType>, int, IndexType) [with IndexType = unsigned int, Real = float, Dims = 3]: block: [4,0,0], thread: [319,0,0] Assertion `indexValue >= 0 && indexValue < src.sizes[dim]` failed.'",2019-10-14T15:48:49Z,2020-02-18T18:11:49Z,wontfix,,
1513,b'Force einsum to run in fp16',2019-10-14T15:14:03Z,2019-10-15T08:25:01Z,,,
1512,b'Fix import error in script to convert faisreq roberta checkpoints',2019-10-14T08:41:21Z,2019-10-14T15:40:33Z,,,
1511,b'Run squad with all model lq',2019-10-14T06:32:17Z,2019-10-14T12:35:45Z,,,
1510,b'CalledProcessError',2019-10-14T03:34:01Z,2020-05-09T08:12:14Z,wontfix,CalledProcessError,"CalledProcessError: Command 'cd /content/transformers"
1509,b'remove leftover usage of DUMMY_INPUTS',2019-10-13T23:10:47Z,2019-10-15T08:24:14Z,,NameError,"NameError: name 'DUMMY_INPUTS' is not defined"
1508,"b'Added performance enhancements (XLA, AMP) to examples'",2019-10-13T13:17:50Z,2019-10-15T07:57:19Z,,,
1507,b'GPU Usage?',2019-10-12T18:33:13Z,2020-01-12T13:39:03Z,wontfix,,
1506,b'Seq2Seq model with HugginFace',2019-10-12T12:43:14Z,2019-10-30T16:46:25Z,,,
1505,"b""Fixed the sample code in the title 'Quick tour'.""",2019-10-12T11:20:22Z,2019-10-15T07:50:37Z,,,
1504,"b'Fine-tuning with run_squad.py, Transformers 2.1.1 & PyTorch 1.3.0 Data Parallel Error'",2019-10-12T08:12:54Z,2020-06-24T19:28:48Z,wontfix,"_""RuntimeError","_""RuntimeError: module must have its parameters and buffers on device cuda:0 (device_ids[0]) but found one of them on device: cuda:1""_"
1503,b'What is the best way to handle sequences > max_len for tasks like abstract summarization?',2019-10-12T00:40:50Z,2020-02-17T13:26:11Z,wontfix,,
1502,b'the working example code to use BertForQuestionAnswering ',2019-10-12T00:10:47Z,2019-10-14T14:14:52Z,,,
1501,b'Issue with XLNet pretrained model',2019-10-11T21:23:35Z,2019-10-20T05:09:48Z,,TypeError,"TypeError: forward() got an unexpected keyword argument 'input_ids'"
1500,b'How to load a different domain BERT-based pre-trained model?',2019-10-11T20:11:49Z,2019-12-20T03:20:34Z,wontfix,OSError,"OSError: Model name '/data/ftm/xgb_regr/FinBERT/pred/FinBERT-Pre2K_128MSL-250K' was not found in model name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). We assumed '/data/ftm/xgb_regr/FinBERT/pred/FinBERT-Pre2K_128MSL-250K' was a path or url to a configuration file named config.json or a directory containing such a file but couldn't find any such file at this path or url."
1499,b'model.to(args.device) in run_glue.py taking around 10 minutes. Is this normal?',2019-10-11T17:22:36Z,2019-10-11T20:14:35Z,,,
1498,b'Merge pull request #1 from huggingface/master',2019-10-11T12:27:23Z,2019-10-11T12:27:48Z,,,
1497,b'Merge pull request #1 from huggingface/master',2019-10-11T12:01:13Z,2019-10-11T12:01:21Z,,,
1496,b'Merge pull request #1 from huggingface/master',2019-10-11T12:00:13Z,2019-10-11T12:00:21Z,,,
1495,b'Merge pull request #1 from huggingface/master',2019-10-11T11:33:19Z,2019-10-11T11:33:27Z,,,
1494,b'Merge pull request #1 from huggingface/master',2019-10-11T11:26:00Z,2019-10-11T11:26:42Z,,,
1493,b'FR: Tokenizer function that can handle arbitrary number of sequences',2019-10-11T11:19:02Z,2019-12-17T21:53:32Z,wontfix,,
1492,b'Add new BERT models for German (cased and uncased)',2019-10-11T08:25:29Z,2019-10-11T11:01:52Z,,,
1491,"b'RuntimeError: unexpected EOF, expected 7491165 more bytes. The file might be corrupted.'",2019-10-11T05:46:58Z,2019-10-14T12:37:48Z,,RuntimeError,"RuntimeError: unexpected EOF, expected 7491165 more bytes. The file might be corrupted."
1490,b'Is encode_plus supposed to pad to max_length? ',2019-10-11T05:42:45Z,2019-10-15T20:12:25Z,,,
1489,b'Excessively Long text_b Raises Unnecessary Warnings in `encode_plus`',2019-10-11T00:56:24Z,2019-12-17T01:50:32Z,wontfix,,
1488,b'GLUE on TPU',2019-10-10T23:35:02Z,2019-10-11T14:33:00Z,,,
1487,b'convert int to str before adding to a str',2019-10-10T20:28:01Z,2019-10-10T23:20:40Z,,,
1486,b'Can you please share the pre-processed text dump of the bookcorpus and wikipediacorpus?',2019-10-10T17:55:17Z,2019-10-11T13:00:28Z,,,
1485,b'improve final answer extraction in utils_squad.py',2019-10-10T15:13:19Z,2020-01-27T01:13:33Z,wontfix,,
1484,b'Error while fine-tuning model for GPT2',2019-10-10T14:45:57Z,2019-10-10T21:32:25Z,,AttributeError,"AttributeError: 'GPT2Tokenizer' object has no attribute 'build_inputs_with_special_tokens'"
1483,b'Create new',2019-10-10T12:19:40Z,2019-10-10T13:26:03Z,,,
1482,b'Integration of TF 2.0 models with other Keras modules',2019-10-10T11:18:29Z,2019-10-11T14:25:43Z,,,
1481,b'Does run_lm_finetuning.py finetune the entire BERT / Xlnet architecture',2019-10-10T09:54:44Z,2019-12-18T20:15:48Z,wontfix,,
1480,b'Fixing CTRL tokenizer - Update error messages - XLM-MLM in run_generation',2019-10-10T08:15:21Z,2019-10-10T09:56:20Z,,,
1479,"b""How can I get the transformers' parameters?""",2019-10-10T07:42:15Z,2019-10-11T09:37:34Z,,,
1478,b'bert-large-uncased-whole-word-masking-finetuned-squad or BertForQuestionAnswering?',2019-10-10T06:30:43Z,2019-12-24T22:40:49Z,wontfix,ValueError,"ValueError: not enough values to unpack (expected 3, got 2)"
1477,"b'Much slower for inference, even when traced?'",2019-10-10T04:51:45Z,2019-10-10T23:14:28Z,,,
1476,b'RuntimeError: Error(s) in loading state_dict for BertModel:',2019-10-10T01:53:09Z,2019-12-16T03:03:10Z,wontfix,,
1475,b'data loader for varying length input',2019-10-09T21:06:22Z,2019-10-09T21:06:32Z,,,
1474,"b""'LayerNorm' object has no attribute 'cls'""",2019-10-09T19:57:59Z,2019-12-22T08:05:10Z,wontfix,AttributeError,"AttributeError: 'LayerNorm' object has no attribute 'cls'"
1473,b'Bug in CTRL generation',2019-10-09T18:43:13Z,2019-10-10T14:24:12Z,,,
1472,b'Bug when finetuning model on Squad',2019-10-09T17:25:55Z,2020-01-24T05:24:39Z,wontfix,KeyError,"KeyError: 1000000000"
1471,b'Write with Transformer: Changing settings on Mobile?',2019-10-09T13:30:43Z,2019-10-13T19:57:44Z,,,
1470,b'Plan for Albert?',2019-10-09T13:30:10Z,2019-10-09T13:43:04Z,,,
1469,b'How much GPU memory is needed to run run_squad.py',2019-10-09T11:38:08Z,2019-10-11T06:15:25Z,,,
1468,b'Scores using BertForNextSentencePrediction are not Interpretable.',2019-10-09T09:21:14Z,2019-12-15T10:21:10Z,wontfix,,
1467,b'Hf master',2019-10-09T07:57:38Z,2019-10-09T07:58:09Z,,,
1466,b' RuntimeError: storage has wrong size: expected -1451456236095606723 got 1024',2019-10-09T06:26:22Z,2019-12-15T16:03:10Z,wontfix,,
1465,b'Multilabel Classification with TFBertForSequenceClassification',2019-10-08T23:02:21Z,2020-03-30T05:04:14Z,wontfix,,
1464,b'How is it possible to furthur tune gpt-2(or gpt) in a seq2seq manner?',2019-10-08T21:46:07Z,2019-12-16T20:50:40Z,wontfix,,
1463,b'bert ids',2019-10-08T21:31:21Z,2019-10-08T22:59:22Z,,,
1462,b'Visualizing the Inner Workings of Attention ',2019-10-08T21:23:59Z,2019-12-14T22:21:09Z,wontfix,,
1461,b'How can I use a TensorFlow 2.0 model for Named-Entity-Recognition (NER)? (using TFBertForTokenClassification )',2019-10-08T20:40:04Z,2019-12-21T10:36:31Z,wontfix,,
1460,b'`decoder` without bias in BertLMPredictionHead',2019-10-08T19:44:09Z,2019-10-08T22:19:47Z,,,
1459,b'Imports for Roberta conversion appear to be outdated',2019-10-08T18:42:53Z,2019-12-19T21:40:47Z,wontfix,"UnicodeDecodeError, ImportError","UnicodeDecodeError: 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byteImportError: cannot import name 'BertEncoder'"
1458,b'how to get word embedding vector in GPT-2',2019-10-08T15:55:00Z,2019-10-09T13:59:05Z,,,
1457,b'when running run_squad.py it is showing no progress . stuck after feature building',2019-10-08T15:38:53Z,2019-10-08T16:16:59Z,,,
1456,"b""questions on checkpoint and 'training_args.bin' in run_lm_finetuning.py""",2019-10-08T15:04:21Z,2019-12-16T14:03:11Z,wontfix,,
1455,b'[WIP] Add PretrainedEncoderDecoder class',2019-10-08T14:24:14Z,2019-10-30T15:54:18Z,,,
1454,b'Change tensorboard imports to use built-in tensorboard if available',2019-10-08T13:32:50Z,2019-10-10T09:56:56Z,,,
1453,"b""DistilBert for Tensorflow doesn't work""",2019-10-08T11:21:08Z,2019-10-08T23:56:00Z,,,
1452,b'xlm-mlm-100-1280 model is not available for download',2019-10-08T08:43:37Z,2019-12-14T09:53:23Z,wontfix,,
1451,b'nn.Transformer',2019-10-08T00:13:35Z,2019-12-16T16:47:39Z,wontfix,,
1450,"b""Installation example #2 fails: cannot import name 'glue_compute_metrics'""",2019-10-07T23:44:59Z,2019-10-08T13:17:29Z,,,
1449,"b""Can't replicate Language Model finetuning""",2019-10-07T22:15:46Z,2019-10-08T20:13:17Z,,,
1448,b'Contribution guidelines',2019-10-07T21:19:25Z,2019-10-08T14:55:34Z,,,
1447,b'Provide requirements.txt for development dependencies',2019-10-07T15:55:38Z,2019-10-07T16:49:26Z,,,
1446,b'integer representation ambuiguty in tokenizer',2019-10-07T15:48:46Z,2019-10-07T18:55:59Z,,,
1445,b'Performance degradation with new version of this library (inference)',2019-10-07T15:36:35Z,2019-10-07T18:06:36Z,,,
1444,b'XLNet - Finetuning - Layer-wise LR decay',2019-10-07T15:29:35Z,2019-10-09T14:05:59Z,,,
1443,b'RuntimeError: cublas runtime error : resource allocation failed',2019-10-07T11:57:34Z,2020-04-14T23:25:52Z,wontfix,"`RuntimeError, RuntimeError","`RuntimeError: CUDA error: out of memory`RuntimeError: cublas runtime error : resource allocation failed at /pytorch/aten/src/THC/THCGeneral.cpp:216"
1442,b'TFBertForSequenceClassification - Feeding List of InputExamples',2019-10-07T11:33:40Z,2019-12-13T15:53:31Z,wontfix,,
1441,"b'TF2 Mixed Precision, XLA, Distribution'",2019-10-07T10:10:14Z,2019-10-23T09:05:27Z,,,
1440,b'BLUE 2',2019-10-07T10:05:46Z,2019-12-21T08:36:25Z,wontfix,,
1439,b'Input length is not equal to output length?',2019-10-07T09:35:56Z,2019-10-08T13:15:03Z,,,
1438,b'fix pytorch-transformers migration description in README',2019-10-07T09:01:20Z,2019-10-07T09:02:24Z,,,
1437,b'how to do next word prediction in xlnet?',2019-10-07T04:46:21Z,2019-10-07T11:34:35Z,,,
1436,b'Which model should I use for machine translation?',2019-10-06T23:37:04Z,2020-05-19T10:54:02Z,wontfix,,
1435,b'GPT2 Tokenizer ',2019-10-06T19:21:58Z,2019-10-07T13:54:12Z,,,
1434,b'Remove unnecessary use of FusedLayerNorm in XLNet',2019-10-06T17:35:20Z,2019-10-15T07:44:19Z,,,
1433,b'Fix some typos in README',2019-10-06T17:17:14Z,2019-10-07T03:40:53Z,,,
1432,"b'How to return  bert self attention, so that i can do visualization??'",2019-10-06T14:52:51Z,2019-12-16T08:03:11Z,wontfix,,
1431,b'Fine-tune specific layers',2019-10-06T10:48:13Z,2019-10-07T13:00:24Z,,,
1430,"b""AttributeError: 'BertOnlyMLMHead' object has no attribute 'bias'""",2019-10-06T10:01:44Z,2019-10-06T10:38:16Z,,AttributeError,"AttributeError: 'BertOnlyMLMHead' object has no attribute 'bias'"
1429,b'Checkpoint rotation',2019-10-06T06:57:52Z,2019-10-09T12:48:41Z,,,
1428,b'Problem with word prediction with GPT2',2019-10-05T23:34:10Z,2019-10-06T23:04:21Z,,,
1427,"b""Replace TensorboardX with Pytorch's built in SummaryWriter""",2019-10-05T18:41:30Z,2019-10-12T16:28:36Z,,,
1426,b'GPU Benchmarking + Accumulated Optimizer for TF2',2019-10-05T13:54:18Z,2019-12-21T17:36:31Z,wontfix,,
1425,b'ELECTRA Model',2019-10-04T20:22:05Z,2020-04-17T15:30:12Z,,,
1424,b'Training on GLUE using TPUs',2019-10-04T18:24:55Z,2020-01-27T19:30:18Z,,,
1423,b'Problem loading trained keras model',2019-10-04T16:05:17Z,2020-02-10T10:19:18Z,wontfix,,
1422,b'Option to upload a trained model from gpt-2-simple to use with Write With Transformer',2019-10-03T23:15:01Z,2020-01-14T06:39:44Z,"wontfix, Write With Transformer",,
1421,b'Rbert - follow-up to #1301 - more robust configuration class loading',2019-10-03T22:23:34Z,2020-03-11T14:30:33Z,wontfix,,
1420,b'ALBERT Model Incoming?',2019-10-03T18:57:34Z,2019-10-03T19:18:54Z,,,
1419,b'question for one parameter matrix in transformers/GPT2',2019-10-03T18:02:43Z,2019-10-04T18:32:51Z,,,
1418,b'DistillBert Documentation Code Example fixes',2019-10-03T16:30:20Z,2019-10-03T19:51:34Z,,,
1417,b'How to replicate Arxiv-NLP but for different subject?',2019-10-03T15:27:36Z,2019-12-13T15:53:30Z,wontfix,,
1416,b'How to install transformers with pytorch only?',2019-10-03T15:19:44Z,2019-10-07T11:26:25Z,,AttributeError,"AttributeError: module 'tensorflow' has no attribute '__version__'"
1415,b'run_glue.py - Import Error',2019-10-03T15:06:57Z,2019-10-03T20:01:17Z,,ImportError,"ImportError: cannot import name 'glue_compute_metrics'"
1414,b'Instruction for Using XLM Text  Generations',2019-10-03T13:23:10Z,2019-12-16T16:47:38Z,wontfix,,
1413,b'Adding New Vocabulary Tokens to the Models',2019-10-03T12:56:28Z,2019-10-03T19:21:54Z,,,
1412,b'How to use model.fit in GPT2 TF Model',2019-10-03T09:32:54Z,2019-12-10T16:23:11Z,wontfix,```ValueError,"```ValueError: Error when checking model target: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 13 array(s), but instead got the following list of 1 arrays: [<tf.Tensor 'IteratorGetNext:1' shape=(None, 750) dtype=int64>]...```"
1411,b'Update run_glue.py',2019-10-03T08:31:17Z,2019-10-03T15:31:12Z,,,
1410,b'migrate BertForQuestionAnswering from pytorch-pretrained-bert not produce the same result',2019-10-03T08:11:07Z,2019-12-16T09:03:09Z,wontfix,,
1409,b'Evaluation result.txt path changing #1286',2019-10-03T04:53:09Z,2019-10-09T01:14:34Z,,,
1408,b'Batched BertForNextSentencePrediction with variable length sentences',2019-10-03T00:03:15Z,2019-10-04T23:05:25Z,,,
1407,b'GPT-2 Training on non-english text',2019-10-02T23:32:58Z,2020-08-11T20:49:47Z,wontfix,,
1406,b'Distil update',2019-10-02T20:32:53Z,2019-10-03T14:27:12Z,,,
1405,b'Re-order XLNet attention head outputs for better perf',2019-10-02T18:41:42Z,2019-10-11T10:10:59Z,,,
1404,b'How to speedup BERT eval',2019-10-02T18:01:45Z,2019-10-02T19:30:46Z,,,
1403,b'Is it possible to modify the parameters in GPT-2?',2019-10-02T17:32:42Z,2019-10-03T17:53:51Z,,,
1402,b'Defining Models in TF 2.0 and Extending Them',2019-10-02T17:13:51Z,2019-12-08T18:38:10Z,wontfix,,
1401,b'XLM add new models',2019-10-02T15:15:45Z,2019-10-02T21:53:02Z,,,
1400,b'Fix typo: initialy -> initially',2019-10-02T15:02:36Z,2019-10-02T15:04:19Z,,,
1399,b'Generate Variable Length Text With GPT2',2019-10-02T00:25:26Z,2019-10-02T23:54:52Z,,,
1398,b'Fixed typo in docs README',2019-10-02T00:22:06Z,2019-10-09T01:52:42Z,,,
1397,b'remove token type inputs from roberta - fix #1234',2019-10-01T23:57:13Z,2019-10-10T09:57:27Z,,,
1396,b'Fix syntax typo in README.md',2019-10-01T18:58:13Z,2019-10-01T18:59:32Z,,,
1395,b'Masking of special tokens in masked LM finetuning.',2019-10-01T15:36:21Z,2019-12-10T21:23:14Z,wontfix,,
1394,b'Change gpt2 language model loss function',2019-10-01T15:26:59Z,2019-12-12T20:06:55Z,wontfix,,
1393,b'With GPT-2 is it possible to get previous word prediction?',2019-10-01T12:39:24Z,2019-10-07T13:50:31Z,,,
1392,"b""Bert's keyword argument 'output_all_encoded_layers' does not exist anymore?""",2019-10-01T09:56:29Z,2019-10-02T11:34:30Z,,,
1391,b'Built-in pretrained models location',2019-10-01T08:45:30Z,2019-10-01T10:06:17Z,,,
1390,b'\xe2\x9d\x93 How to use cached hidden states in run_generation ?',2019-10-01T08:11:34Z,2019-12-08T01:04:14Z,wontfix,,
1389,b'Fix compatibility issue with PyTorch 1.2',2019-10-01T07:44:23Z,2019-10-15T07:43:22Z,,,
1388,b'Add Roberta SQuAD model',2019-10-01T04:50:09Z,2019-12-10T23:51:04Z,,,
1387,"b""TFTransfoXLLMHeadModel doesn't accept lm_labels parameter""",2019-09-30T23:19:16Z,2019-12-07T00:13:00Z,wontfix,TypeError,"TypeError: call() got an unexpected keyword argument 'lm_labels'"
1386,b'Add RoBERTa question answering & Update SQuAD runner to support RoBERTa',2019-09-30T22:26:33Z,2019-12-20T22:18:15Z,,,
1385,b'[multiple-choice] Simplify and use tokenizer.encode_plus',2019-09-30T20:10:23Z,2019-10-04T21:40:52Z,,,
1384,b'Quality of life enhancements in encoding + patch MLM masking',2019-09-30T18:43:32Z,2019-10-09T15:18:25Z,,,
1383,b'Adding CTRL',2019-09-30T18:25:55Z,2019-10-09T15:31:06Z,,,
1382,b'Issue with `decode` in the presence of special tokens',2019-09-30T14:18:37Z,2019-10-09T09:26:43Z,,TypeError,"TypeError: replace() argument 1 must be str, not None"
1381,b'how to train RoBERTa from scratch',2019-09-30T14:09:17Z,2019-12-14T12:53:21Z,wontfix,,
1380,b'Confusing tokenizer result on single word',2019-09-30T04:38:54Z,2019-12-07T23:04:12Z,wontfix,,
1379,b'TransfoXLCorpus requires pytorch to tokenize files',2019-09-30T00:23:32Z,2019-12-06T01:27:50Z,wontfix,NameError,"NameError: name 'torch' is not defined"
1378,b'TFDistilBertForSequenceClassification - TypeError: len is not well defined for symbolic Tensors during model.fit()',2019-09-29T18:44:18Z,2019-10-08T23:54:54Z,,TypeError,"TypeError: in converted code:"
1377,b'Error when calculate tokens_id and Mask LM',2019-09-29T15:28:18Z,2019-12-05T16:27:50Z,wontfix,,
1376,b'Is it save the best model when used example like run_glue?',2019-09-29T13:21:15Z,2019-09-30T09:12:04Z,,,
1375,"b""cannot import name 'TFBertForSequenceClassification'""",2019-09-29T12:43:32Z,2019-09-29T17:44:42Z,,,
1374,b'Fix run_glue.py on QNLI part',2019-09-29T12:39:19Z,2019-12-05T13:27:47Z,wontfix,,
1373,b'Fixed critical css font-family issues',2019-09-29T11:51:32Z,2019-10-03T23:04:03Z,,,
1372,b'Simplify code by using six.string_types',2019-09-29T08:40:51Z,2019-12-21T11:30:15Z,,,
1371,b'Make activation functions available from modeling_utils (PyTorch)',2019-09-29T07:40:10Z,2020-03-04T15:31:18Z,wontfix,,
1370,b'considerd to add albert?',2019-09-29T02:21:47Z,2020-01-12T08:39:03Z,wontfix,,
1369,b'Update README.md',2019-09-28T23:37:05Z,2019-09-30T18:48:02Z,,,
1368,b'Tried to import  TFBertForPreTraining in google colab',2019-09-28T23:34:22Z,2019-12-06T14:13:02Z,wontfix,ImportError,"ImportError: cannot import name 'TFBertForPreTraining'"
1367,"b'Model does not train when using new BertModel, but does with old BertModel'",2019-09-28T20:04:12Z,2019-12-15T03:21:09Z,wontfix,,
1366,b'fix redundant initializations of Embeddings in RobertaEmbeddings',2019-09-28T16:29:23Z,2019-09-28T16:52:30Z,,,
1365,"b""Why add the arguments 'head_mask' and when to use this arguments""",2019-09-28T13:40:38Z,2019-12-04T14:38:47Z,wontfix,,
1364,b'Is there any plan for Roberta in SQuAD?',2019-09-28T12:56:27Z,2019-09-29T06:17:59Z,,,
1363,"b""Why the RoBERTa's max_position_embeddings size is 512+2=514?""",2019-09-28T11:52:45Z,2019-12-04T16:38:46Z,wontfix,,
1362,b'fix link',2019-09-28T08:22:16Z,2019-09-28T08:26:43Z,,,
1361,b'distil-finetuning in run_squad',2019-09-27T21:47:59Z,2019-10-04T21:23:16Z,,,
1360,b'Chunking Long Documents for Classification Tasks',2019-09-27T20:03:14Z,2019-09-28T12:45:30Z,,,
1359,b'Update run_lm_finetuning.py',2019-09-27T18:19:34Z,2019-09-27T20:58:20Z,,,
1358,b'How to contribute to \xe2\x80\x9cWrite with transformer\xe2\x80\x9d?',2019-09-27T17:59:29Z,2020-01-29T13:39:17Z,"wontfix, Write With Transformer",,
1357,b'Support for SuperGLUE fine-tune/eval?',2019-09-27T17:33:54Z,2019-12-03T18:56:11Z,wontfix,,
1356,b'GPT and BERT pretrained models in French',2019-09-27T16:01:31Z,2020-04-07T10:55:16Z,wontfix,,
1355,b'Fix tensorflow_dataset glue support',2019-09-27T15:21:40Z,2019-09-27T20:59:54Z,,,
1354,b'run_tf_glue.py breaks when changing to a glue dataset different from mrpc',2019-09-27T15:03:28Z,2019-10-01T07:57:20Z,,,
1353,b'Fix some typos',2019-09-27T14:56:01Z,2019-09-27T21:00:35Z,,,
1352,b'wwm-bert lm_finetune ',2019-09-27T14:02:51Z,2019-12-03T14:56:12Z,wontfix,,
1351,b'SQUAD: V2 referenced at top of Readme; V1 referenced in usage instructions',2019-09-27T13:38:08Z,2019-09-27T13:42:07Z,,ValueError,"ValueError: For training, each question should have exactly 1 answer."
1350,b'Custom models: MixUp Transformers with TF.Keras code',2019-09-27T12:22:13Z,2019-12-16T13:03:09Z,wontfix,,
1349,b'Just some typos',2019-09-27T10:09:50Z,2019-09-27T11:08:01Z,,,
1348,b'Urgent: RoBERTa-Large-MNLI does not work for 2-way classification anymore',2019-09-27T09:20:06Z,2020-02-09T23:58:58Z,wontfix,RuntimeError,"RuntimeError: Error(s) in loading state_dict for RobertaForSequenceClassification:"
1347,"b""Use PyTorch's GELU activation""",2019-09-27T08:53:51Z,2019-12-03T09:56:14Z,wontfix,,
1346,b'Add small  note about the output of hidden states (closes #1332)',2019-09-27T08:03:43Z,2019-09-27T08:30:08Z,,,
1345,b'Ram utilisation of DistilBERT',2019-09-27T07:46:33Z,2019-12-03T08:56:11Z,wontfix,,
1344,b'Errors when using fp16 with traced models',2019-09-27T00:25:19Z,2019-12-03T00:56:13Z,wontfix,`RuntimeError,"`RuntimeError: expected device cuda:0 and dtype Float but got device cuda:0 and dtype Half"
1343,b'RobertaTokenizer documentation is off with the new transformers library',2019-09-26T20:11:49Z,2019-09-26T20:49:17Z,,,
1342,"b""AttributeError: 'RobertaTokenizer' object has no attribute 'add_special_tokens_sentences_pair'""",2019-09-26T18:19:40Z,2019-09-26T20:29:32Z,,`AttributeError,`AttributeError: 'RobertaTokenizer' object has no attribute 'add_special_tokens_sentences_pair'`
1341,b'Examples in Colab',2019-09-26T17:49:19Z,2019-12-04T23:38:46Z,wontfix,,
1340,b'Size mismatch when loading pretrained model',2019-09-26T16:52:53Z,2019-09-26T17:56:33Z,,RuntimeError,"RuntimeError: Error(s) in loading state_dict for RobertaModel:"
1339,b'Why is the vocabulary of token_type_ids and input_ids shared? ',2019-09-26T15:25:33Z,2019-12-02T17:20:28Z,wontfix,,
1338,b'Extending `examples/` to TensorFlow',2019-09-26T14:29:20Z,2019-12-08T16:38:10Z,wontfix,,
1337,b'faster dataset building',2019-09-26T13:56:25Z,2019-09-27T08:35:13Z,,,
1336,b'Completed the documentation with TF2',2019-09-26T11:44:50Z,2019-09-26T11:45:41Z,,,
1335,b'Optimize XLNet model to generate embedding of long documents',2019-09-25T20:44:48Z,2019-09-26T15:49:08Z,,,
1334,b'Typo in modeling_bert file',2019-09-25T13:21:30Z,2019-09-27T06:59:47Z,,,
1333,b'[FIX] fix run_generation.py to work with batch_size > 1',2019-09-25T12:57:05Z,2019-10-31T18:29:59Z,,,
1332,b'pytorch-transformers returns output of 13 layers?',2019-09-25T09:51:36Z,2019-09-27T08:30:08Z,,,
1331,b'Is the UI code for https://transformer.huggingface.co open source? ',2019-09-25T04:18:43Z,2019-09-26T13:51:43Z,,,
1330,b'Loading errors for BERT base on GPU with PyTorch 0.4.1 ',2019-09-25T03:15:34Z,2019-09-25T03:23:47Z,,,
1329,b'GLUE Script for Tensorflow',2019-09-25T02:05:50Z,2019-09-27T08:43:56Z,,,
1328,b'Sequence Classification pooled output vs last hidden state',2019-09-24T20:30:19Z,2019-09-30T17:42:44Z,,,
1327,b'Pytorch/TF2 determinism',2019-09-24T19:04:02Z,2019-09-24T20:49:58Z,,,
1326,b'RuntimeError: expected scalar type Half but found Float',2019-09-24T14:46:25Z,2020-02-10T10:19:20Z,wontfix,RuntimeError,"RuntimeError: expected scalar type Half but found Float (data<c10::Half> at /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorMethods.h:1821)"
1325,b'[Proposal] GLUE processors included in library',2019-09-24T13:48:05Z,2019-09-24T19:40:11Z,,,
1324,b'A Micro BERT',2019-09-24T12:04:24Z,2019-12-04T05:38:47Z,wontfix,,
1323,b'How to build a Text-to-Feature Extractor based on Fine-Tuned BERT Model',2019-09-24T09:35:17Z,2019-09-26T12:14:00Z,,,
1322,"b""parameter never_split not added in BasicTokenizer's tokenize""",2019-09-24T08:48:21Z,2019-11-30T09:12:11Z,wontfix,,
1321,"b'Using pytorch-transformer to reimplement the ""Attention is all you need"" paper'",2019-09-23T20:51:36Z,2019-09-30T19:28:00Z,,,
1320,b'Why does padding affect the embedding results for XLNet? Pre-padding returns different embeddings than post-padding. Which one should be used?',2019-09-23T18:56:10Z,2020-01-12T23:39:03Z,wontfix,,
1319,b'BertForQuestionAnswering output to predict text',2019-09-23T14:31:51Z,2019-10-17T02:32:39Z,,,
1318,b'A sequence with no special tokens has been passed to the RoBERTa model. This model  requires special tokens in order to work. Please specify add_special_tokens=True in  your encoding.',2019-09-23T11:02:30Z,2019-12-20T12:20:31Z,wontfix,,
1317,b'BertTokenizer provides wrong encode function for Japanese BERT',2019-09-23T08:07:27Z,2019-09-26T10:18:25Z,,,
1316,b'How to predict missing word [MASK] using Robert',2019-09-23T02:41:12Z,2019-09-24T03:43:51Z,,,
1315,b'Remove unnecessary use of FusedLayerNorm',2019-09-23T00:33:16Z,2019-09-26T06:50:03Z,,,
1314,b'How to preprocess my own data to use RoBERTa of Multiple GPUs',2019-09-22T18:46:43Z,2019-12-09T00:38:10Z,wontfix,,
1313,"b""Add option to use a 'stop token'""",2019-09-22T13:42:37Z,2019-10-03T22:43:57Z,,,
1312,"b'In BertForSequenceClassification, why is loss initialised in every forward?'",2019-09-22T08:45:51Z,2020-04-14T23:25:53Z,wontfix,,
1311,b'RoBERTa : add_special_tokens=True',2019-09-22T07:29:04Z,2019-12-02T07:20:26Z,wontfix,,
1310,b'Redundant sep_token_extra option for RoBERTa Fine-tuning',2019-09-22T03:49:35Z,2019-09-27T23:37:00Z,,,
1309,b'Best loss',2019-09-21T20:19:37Z,2019-09-28T20:40:52Z,,,
1308,b'Planned support for new Grover 1.5B models?',2019-09-21T14:15:12Z,2019-11-29T18:57:22Z,wontfix,,
1307,b'mask_tokens sometimes masks special tokens ',2019-09-20T22:01:49Z,2019-12-06T15:12:59Z,wontfix,,
1306,b'Which model is best to used for language model rescoring for ASR',2019-09-20T16:23:09Z,2019-12-15T12:21:09Z,wontfix,,
1305,b'Dataset format and Best Practices For Language Model Fine-tuning',2019-09-20T15:12:26Z,2019-12-01T14:07:47Z,wontfix,,
1304,b'max_len_single_sentence should be max_len - 2 for RoBERTa',2019-09-20T13:20:46Z,2019-09-27T21:03:01Z,,,
1303,"b""Getting an unexpected EOF when trying to download 'bert-large-uncased-whole-word-masking-finetuned-squad' model.""",2019-09-20T13:09:18Z,2020-02-01T10:20:09Z,wontfix,,
1302,b'Rectified Adam + LARS',2019-09-20T13:00:54Z,2020-02-03T09:55:35Z,wontfix,,
1301,b'RBERT implementation',2019-09-20T11:39:13Z,2019-10-15T07:40:45Z,,,
1300,b'\xe2\x9d\x93 Why the criterion of XLNet LMHeadModel use ignore_index = -1 ?',2019-09-20T07:53:20Z,2019-10-03T23:36:03Z,,,
1299,b'What is the best CPU inference acceleration solution for BERT now?',2019-09-20T02:50:55Z,2019-11-20T01:42:25Z,wontfix,,
1298,b'fix annotation',2019-09-20T02:09:26Z,2019-09-20T14:59:36Z,,,
1297,b'add support for file I/O',2019-09-19T15:20:38Z,2019-10-03T21:13:33Z,,,
1296,b'Added ValueError for duplicates in list of added tokens',2019-09-19T14:46:32Z,2019-10-03T21:06:18Z,,,
1295,"b""Where are BERT's pretrained Embeddings loaded?""",2019-09-19T12:48:50Z,2019-09-20T11:44:10Z,,,
1294,b'Delete n_special reference in docstring',2019-09-19T08:37:27Z,2019-09-23T13:54:56Z,,,
1293,"b""cannot import name 'XLNetForMultipleChoice' but python can import""",2019-09-19T06:48:40Z,2019-09-30T13:51:31Z,,,
1292,b'Fine Tuning GPT2 on wikitext-103-raw',2019-09-19T06:00:03Z,2020-01-22T11:49:01Z,wontfix,,
1291,b'traced_model ',2019-09-19T01:12:59Z,2019-12-06T14:13:01Z,wontfix,RuntimeError,"RuntimeError: Tracer cannot infer type of (tensor([[[-0.9993,  0.2632, -0.6305,  ..., -0.3520, -1.2041, -1.5944],"
1290,b'MemoryError on run_lm_finetuning.py',2019-09-18T22:05:45Z,2019-11-24T22:27:52Z,wontfix,,
1289,b'Adding Adapters',2019-09-18T20:45:15Z,2019-12-21T11:29:01Z,wontfix,,
1288,b'Typo with LM Fine tuning script',2019-09-18T20:42:47Z,2019-10-01T22:46:07Z,,,
1287,b'TransfoXLLMHeadModel compatibility with pytorch 1.1.0',2019-09-18T20:20:54Z,2019-10-01T22:42:50Z,,,
1286,b'Evaluation result.txt path suggestion',2019-09-18T16:41:42Z,2019-12-09T07:48:00Z,wontfix,,
1285,b'GPT2 Tokenizer Decoding Adding Space',2019-09-18T15:42:20Z,2019-09-26T10:11:05Z,,,
1284,b'Fix fp16 masking in PoolerEndLogits',2019-09-18T13:33:52Z,2019-10-01T22:40:22Z,,,
1283,b'Is training from scratch possible now?',2019-09-18T09:04:55Z,2019-11-28T10:13:25Z,wontfix,,
1282,b'start_position=0 in utils_squad.py when span is impossible',2019-09-18T08:53:09Z,2019-09-18T09:11:02Z,,,
1280,b'FineTuning using single sentence document',2019-09-18T04:40:48Z,2020-01-02T19:22:12Z,wontfix,,
1279,b'connection limit of pregenerate_training_data.py',2019-09-18T03:16:47Z,2019-11-24T04:27:52Z,wontfix,,
1278,"b""'Default process group is not initialized' Error""",2019-09-18T01:50:39Z,2019-11-24T14:27:52Z,wontfix,AssertionError,"AssertionError: Default process group is not initialized"
1277,b'No language embedding weights in pre-trained xlm models. ',2019-09-17T22:40:23Z,2019-09-20T19:50:55Z,,,
1276,"b'Write with Transformer: Please, add an autosave to browser cache!'",2019-09-17T20:56:43Z,2019-11-25T13:31:03Z,"wontfix, Write With Transformer",,
1275,b'Implement fine-tuning BERT on CoNLL-2003 named entity recognition task',2019-09-17T14:13:16Z,2019-10-15T07:35:25Z,,,
1274,"b'Fixes #1263, add tokenization_with_offsets, gets tokens with offsets in the original text'",2019-09-17T14:13:00Z,2020-03-11T14:30:36Z,wontfix,,
1273,"b""ModuleNotFoundError: No module named 'pytorch_transformers.modeling' using convert_pytorch_checkpoint_to_tf.py""",2019-09-17T05:32:40Z,2019-11-23T05:55:02Z,wontfix,ModuleNotFoundError,"ModuleNotFoundError: No module named 'pytorch_transformers.modeling'"
1272,b'How long does it take? (BERT Model Finetuning using Masked ML objective)',2019-09-16T21:10:39Z,2019-11-22T22:36:53Z,wontfix,,
1271,b'get NaN loss when I run the example code run_squad.py',2019-09-16T18:00:39Z,2019-09-18T12:23:12Z,,RuntimeError,"RuntimeError: Function 'MulBackward0' returned nan values in its 0th output."
1270,b'BERT returns different embedding for same sentence',2019-09-16T11:31:00Z,2019-09-19T14:20:50Z,,,
1269,b'could you add an option to transfer variables from float32 to float16 in GPT2 model to reduce model size and accelerate the inference speed ',2019-09-16T01:56:39Z,2019-11-24T09:27:53Z,wontfix,,
1268,b'How to use pytorch-transformers for transfer learning?',2019-09-15T17:42:59Z,2019-11-22T17:36:53Z,wontfix,,
1267,b'Accuracy not increasing with BERT Large model',2019-09-15T14:17:07Z,2019-12-19T18:07:36Z,wontfix,,
1266,b'Fine-tune distilbert-base-uncased under run_glue',2019-09-15T14:04:06Z,2019-09-27T21:49:09Z,,,
1265,b'different results shown each time when I run the example code for BertForMultipleChoice',2019-09-15T07:20:06Z,2019-11-24T09:27:52Z,wontfix,,
1264,b'Error running openai-gpt on ROCstories',2019-09-14T15:51:23Z,2019-09-18T08:07:56Z,,"TypeError, RuntimeError","TypeError: __init__() got an unexpected keyword argument 'num_special_tokens'RuntimeError: The size of tensor a (78) must match the size of tensor b (16) at non-singleton dimension 1"
1263,b'Offsets in original text from tokenizers',2019-09-13T23:29:36Z,2019-11-24T08:27:54Z,wontfix,,
1262,"b""run_generation.py  'encode' error for gpt2 and xlnet""",2019-09-13T15:24:33Z,2019-09-15T15:40:50Z,,AttributeError,"AttributeError: 'NoneType' object has no attribute 'encode'"
1261,b'SequenceSummary / quenstion regarding summary types',2019-09-13T13:41:31Z,2019-11-24T14:27:53Z,wontfix,,
1260,b'XLNet tokenizer returns empty list instead of string for some indexes',2019-09-13T13:41:29Z,2019-10-01T23:09:18Z,,,
1259,b'Cannot install the library ',2019-09-12T22:32:17Z,2019-11-19T00:26:23Z,wontfix,,
1258,b'fix padding_idx of RoBERTa model',2019-09-12T22:20:31Z,2019-09-27T23:10:58Z,,,
1257,b'Training time increased from 45 min per epoch to 6 hours per epoch in colab ',2019-09-12T16:14:34Z,2019-11-19T11:26:23Z,wontfix,,
1256,b'Could you please implement a Adafactor optimizer? :)',2019-09-12T14:41:24Z,2020-08-27T08:58:14Z,,,
1255,b'examples/lm_finetuning/simple_lm_finetuning.py crashes with cublas runtime error',2019-09-12T13:22:43Z,2019-09-18T11:11:50Z,,RuntimeError,"RuntimeError: cublas runtime error : resource allocation failed at /pytorch/aten/src/THC/THCGeneral.cpp:216"
1254,b'Write With Transformer adding spaces?',2019-09-12T13:19:04Z,2019-11-25T13:31:04Z,"wontfix, Write With Transformer",,
1253,b'Running XLNet on Squad',2019-09-12T03:01:41Z,2019-11-24T15:27:52Z,wontfix,,
1252,b'Max encoding length + corresponding tests',2019-09-11T16:24:33Z,2019-09-18T13:46:26Z,,,
1251,b'Why you need DistilBertModel class?',2019-09-11T14:35:28Z,2019-09-17T19:00:26Z,,,
1250,b'R-BERT implementation',2019-09-11T08:09:31Z,2019-11-24T08:27:52Z,wontfix,,
1249,"b'fixed: hard coding for max and min number will out of range in fp16, which will cause nan.'",2019-09-11T07:49:49Z,2019-09-11T13:53:29Z,,,
1248,b'model_type for gpt',2019-09-11T07:16:52Z,2019-11-17T16:03:09Z,wontfix,,
1247,b'KnowBert',2019-09-11T01:23:01Z,2020-01-26T00:43:39Z,wontfix,,
1246,b'breaking change',2019-09-10T18:44:41Z,2019-11-24T08:27:55Z,wontfix,,
1245,b'Different performance between pip install vs. download zip code',2019-09-10T18:21:52Z,2019-09-11T18:07:36Z,,,
1244,b'unconditional generation with run_generation.py',2019-09-10T17:55:56Z,2019-11-17T21:20:07Z,wontfix,,
1243,b'Can pytorch-transformers be used to get XLM sentence embeddings for multiple languages?',2019-09-10T17:36:29Z,2019-11-16T20:03:10Z,wontfix,,
1242,b'Special tokens / XLNet',2019-09-10T16:41:32Z,2019-11-25T11:31:03Z,wontfix,,
1241,"b""Fixing typo in gpt2 for doc site's class link""",2019-09-10T16:13:17Z,2019-09-10T20:14:19Z,,,
1240,b'ModuleNotFoundError in distillation/scripts/binarized_data.py',2019-09-10T15:45:49Z,2019-09-11T14:21:49Z,,ModuleNotFoundError,"ModuleNotFoundError: No module named 'examples.distillation'"
1239,b'how to finetuning with roberta-large',2019-09-10T12:10:08Z,2019-09-10T14:14:09Z,,,
1238,b'BLUE',2019-09-10T11:37:11Z,2019-10-07T10:06:26Z,,,
1237,b'Issue in fine-tuning distilbert on Squad 1.0',2019-09-10T10:16:31Z,2019-12-23T21:18:27Z,wontfix,,
1236,b'Roberta for squad',2019-09-10T06:35:53Z,2019-11-16T06:53:19Z,wontfix,,
1235,b'Quick questions about details',2019-09-10T04:21:50Z,2019-11-16T04:53:18Z,wontfix,,
1234,b'\xe2\x9d\x93 How to finetune `token_type_ids` of RoBERTa ?',2019-09-10T02:05:18Z,2020-01-03T01:22:12Z,wontfix,,
1233,b'Fix to prevent crashing on assert len(tokens_b)>=1',2019-09-10T00:01:36Z,2019-09-10T20:15:48Z,,,
1232,"b""Can't reproduce XNLI zero-shot results from MBERT in Chinese""",2019-09-09T21:13:54Z,2019-09-10T04:18:17Z,,,
1231,b'Unable to load DistilBertModel after training',2019-09-09T18:51:26Z,2019-09-09T20:40:43Z,,OSError,"OSError: file ser_dir/sm_training_1/pytorch_model.bin not found"
1230,b'How to deal with oov tokens with pretrained models',2019-09-09T16:03:44Z,2019-09-10T17:04:07Z,,,
1229,b'changes in evaluate function in run_lm_finetuning.py',2019-09-09T14:38:06Z,2019-09-10T20:16:45Z,,,
1228,b'Trying to fix the head masking test',2019-09-09T13:04:23Z,2019-09-10T09:55:27Z,,,
1227,b'class DistilBertForMultiLabelSequenceClassification()',2019-09-09T11:04:40Z,2019-11-15T12:03:33Z,wontfix,,
1226,b'Question on the position embedding of DistilBERT',2019-09-09T01:24:12Z,2019-09-09T19:31:30Z,,,
1225,b'Bert output last hidden state',2019-09-08T21:51:29Z,2019-11-16T15:53:18Z,wontfix,,
1224,b'Remove duplicate hidden_states of the last layer in BertEncoder in modeling_bert.py',2019-09-08T17:32:34Z,2019-11-15T07:03:33Z,wontfix,,
1223,"b'[RuntimeError: sizes must be non-negative]  : XLnet, Large and Base'",2019-09-08T06:16:46Z,2019-09-17T08:25:16Z,,,
1222,b'Citing DistilBERT',2019-09-08T05:07:49Z,2019-09-09T17:50:19Z,,,
1221,"b'Hi there, is bert-large-uncased-whole-word-masking-finetuned-squad trained for Squad 1.0 or 2.0?'",2019-09-07T23:47:24Z,2019-11-14T17:29:19Z,wontfix,,
1220,"b'RuntimeError: Gather got an input of invalid size: got [2, 3, 12, 256, 64], but expected [2, 4, 12, 256, 64] (gather at /opt/conda/conda-bld/pytorch_1544199946412/work/torch/csrc/cuda/comm.cpp:227)'",2019-09-07T18:48:26Z,2020-04-25T19:15:59Z,wontfix,RuntimeError,"RuntimeError: Gather got an input of invalid size: got [2, 3, 12, 256, 64], but expected [2, 4, 12, 256, 64] (gather at /opt/conda/conda-bld/pytorch_1544199946412/work/torch/csrc/cuda/comm.cpp:227)"
1219,b'fix tokenize(): potential bug of splitting pretrained tokens with newly added tokens',2019-09-07T08:29:28Z,2020-03-15T10:35:31Z,wontfix,,
1218,b'How to set the weight decay in other layers after BERT output?',2019-09-07T00:04:41Z,2019-11-16T16:53:18Z,wontfix,,
1217,b'Fixing head masking test',2019-09-06T21:28:21Z,2019-09-24T11:51:47Z,,,
1216,"b'Is there any sample code for fine-tuning BERT on sequence labeling tasks, e.g., NER on CoNLL-2003?'",2019-09-06T18:38:07Z,2019-12-28T12:42:13Z,wontfix,,
1215,b'Cut off sequences of length greater than max_length= 512 for roberta',2019-09-06T16:41:04Z,2019-11-13T00:56:13Z,wontfix,,
1214,b'Better examples',2019-09-06T16:15:20Z,2019-09-09T07:26:37Z,,,
1213,b'Fine-tuned RoBERTa models on CPU',2019-09-06T11:00:35Z,2019-11-12T12:56:13Z,wontfix,,
1212,b'LSTM returns nan after using the pretrained BERT embedding as input ',2019-09-06T09:41:04Z,2019-09-06T14:39:49Z,,,
1211,b'How to fine tune small dataset?',2019-09-06T08:12:17Z,2019-11-16T02:53:18Z,wontfix,,
1210,b'Finetuning distilbert-base-uncased',2019-09-06T06:31:40Z,2019-12-06T09:51:51Z,,,
1209,b'run_squad.py predictions',2019-09-06T02:16:05Z,2019-11-12T05:31:10Z,wontfix,,
1208,b'How to set the token_type_ids in XLNet correctly?',2019-09-05T14:52:03Z,2019-09-09T07:22:22Z,,,
1207,b'convert_roberta_checkpoint_to_pytorch.py 514 max position?',2019-09-05T13:13:03Z,2019-11-11T14:31:10Z,wontfix,,
1206,b'the best way to cut the upper layers',2019-09-05T10:40:45Z,2019-10-11T17:15:04Z,,,
1205,b'Fix typo',2019-09-05T10:25:23Z,2019-09-05T19:44:17Z,,,
1204,"b""Can't trace any model with pytorch-transformers 1.2""",2019-09-05T09:21:19Z,2019-11-11T11:09:54Z,wontfix,"RuntimeError, torch.jit.TracingCheckError","RuntimeError: r ASSERT FAILED at /pytorch/aten/src/ATen/core/jit_type.h:142, please report a bug to PyTorch. (expect at /pytorch/aten/src/ATen/core/jit_type.h:142)torch.jit.TracingCheckError: Tracing failed sanity checks!"
1203,b'[2.0] TF 2.0 support',2019-09-05T08:23:57Z,2019-09-26T10:11:03Z,,,
1202,b'Learning word-pieces garble the predictions',2019-09-05T06:40:10Z,2019-11-11T08:31:12Z,wontfix,,
1201,b'[2.0] - Split configuration and modeling files',2019-09-04T22:29:39Z,2019-09-05T19:16:59Z,,,
1200,b'Distributed device ordinal question',2019-09-04T22:21:59Z,2019-11-10T23:24:33Z,wontfix,,
1199,b'Fixing TransformerXL bool issue #1169',2019-09-04T20:38:44Z,2019-09-05T19:17:02Z,,,
1198,b'How to fine-tune xlnet on SQuAD with the parameter setting provided in the paper?',2019-09-04T19:38:25Z,2019-12-03T16:56:12Z,wontfix,,
1197,b'Fix loading of question answering bert from tf weights.',2019-09-04T13:24:07Z,2019-12-06T19:12:57Z,wontfix,,
1196,b'RoBERTa/GPT2 tokenization',2019-09-04T12:44:52Z,2019-09-26T10:11:05Z,,,
1195,b'[2.0] Reodering arguments for torch jit #1010 and future TF2.0 compatibility',2019-09-04T10:50:17Z,2019-09-09T12:42:51Z,,,
1194,b'How to finetune DistilBERT on custom data?',2019-09-04T10:43:04Z,2019-11-11T16:31:10Z,wontfix,,
1193,b'how to get distilbert-base-uncased-distilled-squad?',2019-09-04T09:45:42Z,2019-11-30T21:12:11Z,wontfix,,
1192,b'Finetuning BertModel to extract textual features for VQA shows bad results',2019-09-04T09:30:37Z,2019-09-05T14:40:45Z,,,
1191,"b""how to use 'spiece.model' to create the xlnet_tokenizer""",2019-09-04T07:15:54Z,2019-11-15T08:03:33Z,wontfix,,
1190,b'Fix reference of import in XLM tokenization',2019-09-04T01:52:36Z,2019-09-04T10:50:50Z,,,
1189,b'Roberta tokenizer fails on certain unicode characters',2019-09-04T00:53:45Z,2019-09-05T23:55:54Z,,,
1188,b'BertEncoder head_mask not subscript-able error when not passed',2019-09-03T23:56:41Z,2019-12-19T18:07:35Z,wontfix,,
1187,b'Using do_eval from run_glue.py uses the cached result',2019-09-03T15:18:52Z,2019-11-12T21:56:12Z,wontfix,,
1186,b'[README] link to Write With Transformer',2019-09-03T14:30:03Z,2019-09-05T16:33:47Z,,,
1185,"b""XLnet output attentions doesn't work""",2019-09-03T11:32:20Z,2019-09-03T12:46:48Z,,`IndexError,"`IndexError: tuple index out of range`"
1184,b'Convert RoBERTa to TF checkpoint',2019-09-03T06:50:00Z,2019-11-10T00:40:23Z,wontfix,,
1183,"b""'DistilBertModel' object has no attribute 'init_weights'""",2019-09-03T06:20:34Z,2019-09-09T01:33:40Z,,AttributeError,"AttributeError: 'DistilBertModel' object has no attribute 'init_weights'"
1182,"b""Updated GLUE script. New feature: Binary mask creation from the tokenizer's encoding.""",2019-09-03T02:00:13Z,2019-09-26T10:11:06Z,,,
1181,b'DistilBERT Loss Function Choice and further query on extending to GPT2.',2019-09-02T14:52:11Z,2019-09-10T15:41:08Z,,,
1180,b'DistilBERT baseline',2019-09-02T14:25:41Z,2019-09-03T23:48:21Z,,,
1179,b'DistilBERT training is killed because OOM',2019-09-02T13:45:52Z,2019-10-11T13:07:16Z,,,
1178,b'added tokens may split a normal token into halves',2019-09-02T13:24:43Z,2019-09-04T07:57:00Z,,,
1177,b'How to install previous versions of pytorch-transformers ',2019-09-02T10:16:43Z,2019-11-08T22:30:30Z,wontfix,,
1176,b'merge',2019-09-02T04:08:47Z,2019-09-02T04:10:47Z,,,
1175,b'May I get the details of Bert pre-train procedure?',2019-09-02T03:17:59Z,2019-09-02T07:31:46Z,,,
1174,b'Fix byte-level BPE decoding error when using added tokens',2019-09-02T00:29:11Z,2019-09-02T07:01:16Z,,,
1173,"b""Write with Transformer doesn't show 774M model?""",2019-09-01T14:26:48Z,2019-09-01T14:35:22Z,,,
1172,b'apex fp16 FusedLayerNorm type issues',2019-09-01T11:45:28Z,2019-09-02T00:36:27Z,,RuntimeError,"RuntimeError: expected scalar type Half but found Float (data<c10::Half> at /home/madvillain/miniconda3/envs/ash3/lib/python3.6/site-packages/torch/include/ATen/core/TensorMethods.h:1386)"
1171,"b""Can't get GPT2tokenizer to load correctly""",2019-09-01T07:01:02Z,2019-09-01T22:32:17Z,,AttributeError,AttributeError: 'GPT2Tokenizer' object has no attribute 'max_len_single_sentence'```
1170,b'How to use BERT or word embedding for e-commerce product classification. ',2019-08-31T23:23:18Z,2019-12-08T10:38:10Z,wontfix,,
1169,b'Attribute errors with pytorch_transformers tests',2019-08-31T20:35:49Z,2019-12-04T12:36:56Z,,,
1168,b'How to add new pre-trained model pytorch-transformers',2019-08-31T20:09:23Z,2019-11-06T22:13:03Z,wontfix,,
1167,"b""ImportError: cannot import name 'DistilBertModel'""",2019-08-31T18:43:53Z,2019-09-05T14:51:18Z,,,
1166,b'Roberta for NER task',2019-08-31T18:02:17Z,2019-12-23T15:13:25Z,,,
1165,b'Dependency errors when trying to use gpt2 using pytorch hub.',2019-08-31T10:28:14Z,2019-09-06T20:40:12Z,,,
1164,b'distillation: fix ModuleNotFoundError error in token counts script',2019-08-31T10:24:35Z,2019-09-02T00:00:08Z,,,
1163,b'[Help] how to make a constrained text generation',2019-08-31T08:02:09Z,2019-11-08T00:52:12Z,wontfix,,
1162,b'XLNet bias fix on resize embeddings (cf #1124)',2019-08-31T04:53:22Z,2019-09-02T21:14:04Z,,,
1161,b'Large Memory Layers',2019-08-30T17:23:07Z,2019-11-05T18:59:59Z,wontfix,,
1160,b'--seed does not change the fintuning results of the xlnet model',2019-08-30T17:09:28Z,2019-09-03T08:00:14Z,,,
1159,b'Problem with optimizers after migration',2019-08-30T14:59:43Z,2019-11-08T00:52:11Z,wontfix,,
1158,b'regarding #1026 pull request ',2019-08-30T13:48:25Z,2019-08-30T14:30:33Z,,,
1157,b'How to load pretraind XLM model',2019-08-30T13:43:59Z,2019-08-31T19:46:09Z,,TypeError,"TypeError: cannot unpack non-iterable NoneType object"
1156,b'About distilled the SQuAD?',2019-08-30T09:24:11Z,2019-11-05T10:59:58Z,wontfix,,
1155,b'Update apex fp16 implementation',2019-08-30T05:08:00Z,2019-08-30T21:29:12Z,,,
1154,b'fix: hard coding for max number',2019-08-30T04:17:14Z,2019-08-30T21:23:09Z,,,
1153,b'[WIP] Refactor Tokenizers creation to support in-memory initialization',2019-08-29T21:49:11Z,2019-12-06T19:12:57Z,wontfix,,
1152,b'fix adding special tokens',2019-08-29T20:48:39Z,2019-08-30T21:21:59Z,,TypeError,"TypeError: can only concatenate list (not ""tuple"") to list"
1151,b'Idea to improve DistilBERT',2019-08-29T20:20:22Z,2019-12-27T02:31:43Z,wontfix,,
1150,b'What is the relationship between `run_lm_finetuning.py` and the scripts in `lm_finetuning`?',2019-08-29T18:15:45Z,2019-12-30T15:04:14Z,wontfix,,
1149,b'Closing bracket is missing in token_counts.py for DistilBERT',2019-08-29T16:23:57Z,2019-08-29T19:31:42Z,,,
1148,b'Documentation auto-deploy',2019-08-29T16:15:25Z,2019-08-30T11:28:17Z,,,
1147,b'GPT2-large fails to load the tokenizer',2019-08-29T15:31:08Z,2019-08-29T20:43:35Z,,AttributeError,"AttributeError: 'NoneType' object has no attribute 'add_special_tokens'"
1146,b'Attention values occasionally exceed 1 in BertModel',2019-08-29T13:40:30Z,2020-01-10T17:39:32Z,"wontfix, Core: Modeling",,
1145,b'How to finetune GPT2',2019-08-29T12:32:40Z,2019-11-16T20:03:11Z,wontfix,,
1144,b'where can i assign step in function lr_lambda of Class WramupLinearSchedule?',2019-08-29T10:45:10Z,2019-11-04T19:54:44Z,wontfix,,
1143,b'Why still using old implementation of apex fp16',2019-08-29T10:44:12Z,2019-08-30T21:29:18Z,,,
1142,b'FP16_Optimizer is not an Optimizer when fp_16',2019-08-29T10:03:42Z,2019-09-02T07:01:50Z,,,
1141,b'Small modification of comment in the run_glue.py example',2019-08-29T07:55:20Z,2019-08-29T12:43:31Z,,,
1140,"b""Can't Using Binarization Script for DistilBERT""",2019-08-29T07:20:18Z,2019-08-30T16:05:24Z,,ValueError,"ValueError: attempted relative import beyond top-level package"
1139,b'Need multiple capabilities',2019-08-29T06:39:31Z,2019-08-30T21:20:12Z,,,
1138,b'loss explosion',2019-08-29T04:45:34Z,2019-08-29T06:25:11Z,,,
1137,b'Cannot import DistilBert classes',2019-08-28T18:41:09Z,2019-08-28T18:50:09Z,,,
1136,b'swap order of optimizer.step() and scheduler.step()',2019-08-28T17:21:40Z,2019-08-28T20:00:53Z,,,
1135,b'distilbert: fix number of hidden_size',2019-08-28T16:10:27Z,2019-08-28T20:00:13Z,,,
1134,b'Schedulers cause memory accumulation across folds in cross-validation?',2019-08-28T15:45:53Z,2019-12-04T11:56:55Z,,,
1133,b'GPT2 Tokenizer decoding fails when the added tokens include a space',2019-08-28T13:19:20Z,2019-09-02T07:01:32Z,,KeyError,"KeyError: ' '"
1132,b'How to split consecutive numbers?',2019-08-28T10:07:06Z,2019-10-28T02:04:23Z,wontfix,,
1131,b'mems output in XLNet',2019-08-28T09:29:10Z,2019-08-28T20:41:48Z,,,
1130,b'Output of BertModel does not match fixed feature vectors extracted from the last hidden layer  ',2019-08-28T09:25:08Z,2019-11-17T01:03:10Z,wontfix,,
1129,b'Fine-tuning (BERT & RoBERTa) base outperforms large',2019-08-28T09:18:14Z,2019-11-09T16:40:24Z,wontfix,,
1128,"b""cannot import name 'RobertaConfig""",2019-08-28T08:26:49Z,2019-08-28T08:31:54Z,,,
1127,b'DistilBERT',2019-08-28T07:34:11Z,2019-08-28T14:43:10Z,,,
1126,b'Bert initialization',2019-08-28T02:01:59Z,2019-09-02T06:53:22Z,,,
1125,"b""UnicodeDecodeError: 'charmap' codec can't decode byte 0x81 in position 1176: character maps to <undefined>""",2019-08-27T20:37:09Z,2019-09-01T23:18:49Z,,,
1124,b'XLNet resize embedding size ERROR',2019-08-27T18:52:10Z,2019-09-02T21:14:56Z,,RuntimeError,"RuntimeError: The size of tensor a (32003) must match the size of tensor b (32000) at non-singleton dimension 2"
1123,b'Extracting Features Example',2019-08-27T15:21:11Z,2019-12-12T11:48:51Z,wontfix,,
1122,b'PyTorch library dependency',2019-08-27T14:59:49Z,2019-11-27T22:18:52Z,wontfix,AttributeError,"AttributeError: module 'torch.nn.functional' has no attribute 'one_hot'"
1121,b'Using pretrained XLNET for long sentences',2019-08-27T13:23:54Z,2019-11-02T14:51:36Z,wontfix,,
1120,b'Change attention mask dtype to be bool. Fix #1119',2019-08-27T11:19:12Z,2019-08-27T13:01:01Z,,,
1119,b'Tons of warnings on use of TransfoXLModel. masked_fill_ input dtype torch.uint8 should be changed to torch.bool',2019-08-27T11:11:29Z,2019-08-27T13:01:02Z,,,
1118,b'Documentation fix #1117',2019-08-27T09:22:50Z,2019-08-27T12:58:51Z,,,
1117,b'Wrong parameter name in documentation',2019-08-27T09:20:09Z,2019-08-27T12:58:52Z,,,
1116,b'Delete nonexistent parameter from documentation fix #1115',2019-08-27T09:11:09Z,2019-08-27T12:56:44Z,,,
1115,b'No parameter which is presented in documentation',2019-08-27T09:03:33Z,2019-08-27T12:56:46Z,,,
1114,b'Does RoBERTa needs input_type_ids as Bert ?',2019-08-27T07:52:14Z,2019-08-27T08:10:17Z,,,
1113,b'[Help] How to do mean/max pooling to get sentence embedding?',2019-08-27T03:46:58Z,2019-08-27T04:48:47Z,,,
1112,b'Implement the QuickStart but got an error when  using BertForMaskedLM to predict a masked token',2019-08-27T03:33:10Z,2019-08-27T04:27:27Z,,,
1111,b'Can we get a 1.1.1 release so that AutoRoberta is included?',2019-08-26T22:52:25Z,2019-11-02T13:51:38Z,wontfix,,
1110,"b'Torch.hub now based on AutoModels - Updating AutoModels with AutoModelWithLMHead, Sequence Classification and Question Answering'",2019-08-26T20:10:43Z,2019-08-30T21:08:58Z,,,
1109,b'keeping encoder fixed from pretrained model but changing classifier',2019-08-26T18:50:00Z,2019-08-26T22:47:31Z,,RuntimeError,"RuntimeError: Error(s) in loading state_dict for BertRUBIForSequenceClassification:"
1108,b'using BERT as pretraining with custom classifier',2019-08-26T18:33:26Z,2019-11-02T00:51:36Z,wontfix,RuntimeError,"RuntimeError: Error(s) in loading state_dict for BertRUBIForSequenceClassification:"
1107,b'Changing the _read_tsv method in class DataProcessor',2019-08-26T18:19:39Z,2019-11-08T00:52:10Z,wontfix,,
1106,b'sample_text.txt is broken (404 ERROR)',2019-08-26T16:49:01Z,2019-08-27T12:50:29Z,,,
1105,"b""How to get pooler state's (corresponds to CLS token) attention vector?""",2019-08-26T12:30:41Z,2019-08-29T13:32:25Z,,,
1104,b'TensorFlow 2.0 - Testing with a few Bert architectures',2019-08-26T11:42:28Z,2019-09-24T13:46:29Z,,,
1103,b'Roberta semantic similarity',2019-08-26T11:19:29Z,2019-09-17T07:35:03Z,,,
1102,b'Wrong documentation example for RoBERTa',2019-08-26T10:59:20Z,2019-08-26T22:13:12Z,,,
1101,b'evaluate bert on Senteval dataset',2019-08-26T09:51:09Z,2019-11-01T12:30:01Z,wontfix,,
1100,b'Writing predictions in a separate output file',2019-08-26T01:50:17Z,2019-08-29T07:39:35Z,,,
1099,b'Missing RobertaForMultipleChoice',2019-08-26T01:26:10Z,2019-11-01T11:30:01Z,wontfix,,
1098,b'Support multiprocessing when loading pretrained weights',2019-08-25T22:46:37Z,2019-11-03T01:03:29Z,wontfix,,
1097,b'modifying config',2019-08-25T18:26:51Z,2019-11-01T22:30:03Z,wontfix,,
1096,"b""Temporary fix for RoBERTa's mismatch of vocab size and embedding size - issue #1091""",2019-08-25T16:39:12Z,2019-08-27T12:21:11Z,,,
1095,"b'some words not in xlnet vocabulary ,especially name '",2019-08-25T03:10:49Z,2019-09-05T04:24:11Z,,,
1094,b'Performing MRPC task after Fine Tuning',2019-08-24T19:25:53Z,2019-08-26T11:22:56Z,,,
1093,b'fused_layer_norm_cuda.cpython-36m-x86_64-linux-gnu.so: undefined symbol: _ZN2at7getTypeERKNS_6TensorE',2019-08-24T09:36:18Z,2019-08-27T10:29:51Z,,ImportError,"ImportError: /opt/conda/lib/python3.6/site-packages/fused_layer_norm_cuda.cpython-36m-x86_64-linux-gnu.so: undefined symbol: _ZN2at7getTypeERKNS_6TensorE"
1092,b'Added cleaned configuration properties for tokenizer with serialization - improve tokenization of XLM',2019-08-24T01:52:48Z,2019-08-30T21:15:41Z,,,
1091,b'Problem with mask token id in RoBERTa vocab',2019-08-23T18:54:38Z,2019-11-02T13:51:36Z,wontfix,,
1090,"b""No such file or directory: '..\\\\VERSION'""",2019-08-23T13:32:42Z,2020-01-02T07:31:21Z,wontfix,,
1089,"b""change layernorm code to pytorch's native layer norm""",2019-08-23T10:18:23Z,2019-08-30T12:49:09Z,,,
1088,"b'\xe2\x9d\x93 Why in `run_squad.py` using XLNet, CLS token is not set at the end ?'",2019-08-23T01:53:29Z,2019-10-29T12:20:31Z,wontfix,,
1087,b'Decode now calls private property instead of public method',2019-08-22T21:26:43Z,2019-08-28T20:22:11Z,,,
1086,b'ProjectedAdaptiveLogSoftmax log_prob computation dimensions error',2019-08-22T15:39:22Z,2019-10-28T16:23:05Z,wontfix,RuntimeError,"RuntimeError: The size of tensor a (15) must match the size of tensor b (1000) at non-singleton dimension 1"
1085,b'RuntimeError: Creating MTGP constants failed. at /opt/conda/conda-bld/pytorch_1533739672741/work/aten/src/THC/THCTensorRandom.cu:34',2019-08-22T13:06:15Z,2019-10-28T21:22:57Z,wontfix,RuntimeError,"RuntimeError: Creating MTGP constants failed. at /opt/conda/conda-bld/pytorch_1533739672741/work/aten/src/THC/THCTensorRandom.cu:34"
1084,b'Xlnet for multi-label classification',2019-08-22T12:18:30Z,2019-10-29T01:22:58Z,wontfix,,
1083,b'hwo to get RoBERTaTokenizer  vocab.json and also merge file',2019-08-22T11:16:10Z,2019-10-29T17:20:31Z,wontfix,,
1082,b'Getting tokenization ERROR while running run_generation.py ',2019-08-22T05:44:55Z,2019-10-28T20:22:58Z,wontfix,,
1081,b'Fix distributed barrier hang',2019-08-22T04:44:10Z,2019-08-23T14:53:54Z,,,
1080,b'51 lm',2019-08-22T04:28:15Z,2019-08-22T04:28:29Z,,,
1079,"b'Fix ""No such file or directory"" for SQuAD v1.1'",2019-08-22T03:19:22Z,2019-08-23T06:12:13Z,,FileNotFoundError,FileNotFoundError: [Errno 2] No such file or directory: 'squad/squad-debug/null_odds_.json'
1078,b'Index misplacement of Vocab.txt BUG BUG BUG',2019-08-22T03:14:15Z,2019-08-22T08:40:02Z,,,
1077,b'Pruning changes so that deleted heads are kept on save/load',2019-08-22T01:39:48Z,2019-09-01T07:42:16Z,,,
1076,b'can this project select the specific version of BERT?',2019-08-22T01:29:33Z,2019-08-22T01:45:54Z,,,
1075,b'reraise EnvironmentError in modeling_utils.py',2019-08-21T22:35:14Z,2019-08-23T10:42:40Z,,,
1074,"b""Shortcut to special tokens' ids - fix GPT2 & RoBERTa tokenizers - improved testing for GPT/GPT-2""",2019-08-21T22:16:22Z,2019-08-30T21:18:58Z,,,
1073,b'Unable to get hidden states and attentions BertForSequenceClassification ',2019-08-21T18:09:22Z,2019-08-29T00:05:25Z,,,
1072,b'Missing tf variables in convert_pytorch_checkpoint_to_tf.py',2019-08-21T17:23:19Z,2019-08-22T20:05:11Z,,,
1071,b'Support for Tensorflow (& or Keras)',2019-08-21T13:47:35Z,2019-11-11T06:24:30Z,wontfix,,
1070,b'Fix the gpt2 quickstart example',2019-08-21T12:01:33Z,2019-08-30T12:19:52Z,,,
1069,b'ru language',2019-08-21T10:03:14Z,2019-10-02T11:05:53Z,,,
1068,b'LM fine-tuning for non-english dataset (hindi)',2019-08-21T09:32:06Z,2019-10-28T11:22:58Z,wontfix,IndexError,"IndexError: too many indices for tensor of dimension 2"
1067,b'Fix bug in run_openai_gpt.py file.',2019-08-21T07:12:39Z,2019-12-06T19:12:58Z,wontfix,,
1066,b'`run_squad.py` not using the dev cache',2019-08-21T04:07:17Z,2019-10-27T05:30:38Z,wontfix,,
1065,b'Has anyone reproduced RoBERTa scores on Squad dataset?',2019-08-21T00:56:55Z,2019-11-23T03:55:02Z,wontfix,,
1064,b'Adding gpt-2 large (774M parameters) model',2019-08-21T00:32:43Z,2019-08-21T01:05:57Z,,,
1063,"b""Can't load the RobertaTokenizer from AutoTokenizer.from_pretrained interface""",2019-08-20T22:30:47Z,2019-08-21T00:17:50Z,,,
1062,"b""Example in OpenAIGPTDoubleHeadsModel can't run""",2019-08-20T21:10:51Z,2019-08-21T00:08:50Z,,,
1061,b'GPT2 774M weights released!',2019-08-20T16:13:29Z,2019-08-22T17:11:36Z,,,
1060,b'Fix typo. configuratoin -> configuration',2019-08-20T12:43:20Z,2019-08-20T15:39:07Z,,,
1059,b'Better use of spacy tokenizer in open ai and xlm tokenizers',2019-08-20T12:18:24Z,2019-08-20T23:53:49Z,,,
1058,b'Initialising XLMTokenizer ',2019-08-20T09:59:41Z,2019-10-26T11:07:29Z,wontfix,,
1057,"b'Add a few of typos corrections, bugs fixes and small improvements'",2019-08-20T09:03:54Z,2019-08-20T23:54:06Z,,,
1056,b'Swap of optimizer.step and scheduler.step for lm finetuning examples',2019-08-20T08:01:50Z,2019-08-20T10:42:25Z,,,
1055,b'Fix #1015 (tokenizer defaults to use_lower_case=True when loading from trained models)',2019-08-19T20:09:00Z,2019-08-20T23:20:47Z,,,
1054,b'simple example of BERT input features : position_ids and head_mask ',2019-08-19T19:21:28Z,2019-08-20T11:01:17Z,,,
1053,b'reproducing bert results on snli and mnli ',2019-08-19T12:29:22Z,2019-10-25T13:58:35Z,wontfix,,
1052,b'Fix RobertaEmbeddings',2019-08-19T04:52:08Z,2019-12-06T19:13:00Z,wontfix,,
1051,b'BUG: run_openai_gpt.py load ROCStories data error',2019-08-19T01:44:55Z,2019-08-21T07:17:29Z,,,
1050,b'Error in converting tensorflow checkpoints to pytorch ',2019-08-18T13:27:47Z,2019-10-25T11:58:38Z,wontfix,**NotFoundError,"**NotFoundError: Unsuccessful TensorSliceReader constructor:** Failed to find any matching files for pretrained_bert/model.ckpt"
1049,b'BUG: run_openai_gpt.py bug of GPTTokenizer and GPTDoubleHeadsModel',2019-08-18T13:08:40Z,2019-08-21T07:14:50Z,,TypeError,"TypeError: __init__() got an unexpected keyword argument 'num_special_tokens'"
1048,b'Very bad performances with BertModel on sentence classification',2019-08-18T10:00:15Z,2019-08-20T20:34:59Z,,,
1047,b'Issue using Roberta',2019-08-18T02:49:57Z,2019-08-19T13:12:26Z,,,
1046,b'Update README after RoBERTa addition',2019-08-17T16:48:30Z,2019-08-17T17:18:39Z,,,
1045,b'mnli results for BERT',2019-08-17T13:02:20Z,2019-10-23T14:09:33Z,wontfix,,
1044,b'Correct truncation for RoBERTa in 2-input GLUE',2019-08-16T20:02:18Z,2019-08-16T20:30:39Z,,,
1043,b'Unable to load custom tokens',2019-08-16T17:53:55Z,2019-08-16T18:37:00Z,,,
1042,b'fix #1041',2019-08-16T15:46:35Z,2019-08-16T15:53:11Z,,,
1041,"b'Issue in running run_glue.py in Roberta, XLNet, XLM in latest release'",2019-08-16T14:28:22Z,2019-08-16T15:53:30Z,,AttributeError,"AttributeError: 'XLMTokenizer' object has no attribute 'vocab'"
1040,b'Fix bug of multi-gpu training in lm finetuning',2019-08-16T04:22:50Z,2019-08-20T15:13:44Z,,,
1039,b'Minor bug in evaluation phase in example code for SQUAD',2019-08-15T23:21:01Z,2019-08-16T16:28:07Z,,,
1038,b'Adding new tokens to GPT tokenizer',2019-08-15T21:05:05Z,2019-08-15T21:40:03Z,,,
1037,"b'wrong generation of training sentence pairs. method: get_corpus_line, in finetune_on_pregenerated.py'",2019-08-15T18:17:38Z,2019-10-26T13:07:35Z,wontfix,,
1036,b'Customize BertTokenizer and Feature Extraction from Bert Model',2019-08-15T12:32:55Z,2019-10-30T03:12:57Z,wontfix,,
1035,b'Merge pull request #1 from huggingface/master',2019-08-15T05:56:53Z,2019-08-18T09:03:59Z,,,
1034,b'Getting embedding from XLM in differnet languages',2019-08-14T20:20:55Z,2019-08-20T23:54:07Z,,,
1033,b'GPT2 Tokenizer got an expected argument `skip_special_tokens`',2019-08-14T18:46:54Z,2019-08-16T21:26:55Z,,TypeError,"TypeError: decode() got an unexpected keyword argument 'skip_special_tokens'"
1032,b'GPT2 Tokenizer got an expected argument `skip_special_tokens`',2019-08-14T18:35:35Z,2019-08-14T18:48:50Z,,``TypeError,"``TypeError: decode() got an unexpected keyword argument 'skip_special_tokens'``"
1031,b'Efficient data loading functionality',2019-08-14T16:50:02Z,2019-10-20T17:44:00Z,wontfix,,
1030,b'Tokenizer not found after conversion from TF checkpoint to PyTorch',2019-08-14T11:42:04Z,2019-10-25T16:58:36Z,wontfix,AttributeError,"AttributeError: 'NoneType' object has no attribute 'encode'`"
1029,"b'if cutoffs=[], convert_transfo_xl_checkpoint_to_pytorch.py has a bug'",2019-08-14T09:27:38Z,2019-11-08T09:30:31Z,wontfix,AttributeError,"AttributeError: 'ProjectedAdaptiveLogSoftmax' object has no attribute  'cluster_weight'"
1028,b'add data utils',2019-08-14T09:17:51Z,2019-08-14T09:19:58Z,,,
1027,b'Re-implemented tokenize() iteratively in PreTrainedTokenizer.',2019-08-14T09:17:39Z,2019-08-20T23:46:04Z,,,
1026,"b'loads the tokenizer for each checkpoint, to solve the reproducability\xe2\x80\xa6'",2019-08-14T09:02:57Z,2019-08-30T12:15:37Z,,,
1025,b'puzzling issue regarding evaluation phase',2019-08-14T08:35:41Z,2019-10-20T10:44:00Z,wontfix,,
1024,b'fail to download vocabulary behind proxy server',2019-08-14T07:01:15Z,2019-08-15T02:24:40Z,,,
1023,b'fix issue #824',2019-08-13T15:26:36Z,2019-08-19T13:31:47Z,,,
1022,"b'""mask_padding_with_zero"" for xlnet'",2019-08-13T09:12:35Z,2019-08-21T03:17:37Z,,,
1021,"b'When I set fp16_opt_level == O2 or O3,  I can not use multiple GPU'",2019-08-13T06:49:51Z,2019-10-25T14:58:35Z,wontfix,,
1020,b'Intended Behaviour for Impossible (out-of-span) SQuAD Features',2019-08-13T01:50:21Z,2019-10-19T02:46:14Z,wontfix,,
1019,b'Fine-tuning approach for Bert and GPT2 classifiers',2019-08-12T22:26:46Z,2019-08-14T13:28:21Z,,,
1018,b'Add LM-only finetuning script for GPT modules',2019-08-12T20:54:20Z,2019-08-15T05:25:39Z,,,
1017,b'the execution order of `scheduler.step()` and `optimizer.step()`',2019-08-12T20:14:25Z,2019-08-21T20:22:24Z,,,
1016,b'inconsistent between class name (Pretrained vs PreTrained)',2019-08-12T19:53:05Z,2019-10-19T15:46:16Z,wontfix,,
1015,b'Logic issue with evaluating cased models in `run_squad.py`',2019-08-12T19:02:47Z,2019-08-20T23:20:48Z,,,
1014,"b""BertTokenizer.save_vocabulary() doesn't work as docstring described""",2019-08-12T18:58:22Z,2019-08-20T23:54:07Z,,,
1013,b'XLNet / sentence padding',2019-08-12T17:53:43Z,2019-10-27T02:07:31Z,wontfix,,
1012,b'inconsistency of the model (XLNet) output  / related to #475 #735',2019-08-12T13:05:40Z,2019-12-16T08:03:10Z,wontfix,,
1011,b'run_classifier.py missing from examples dir',2019-08-12T12:17:39Z,2019-10-19T00:46:15Z,wontfix,,
1010,b'Order of inputs of forward function problematic for jit with Classification models',2019-08-12T10:48:10Z,2019-09-26T15:41:02Z,,,
1009,"b'GPT2 Sentence Probability: Necessary to Prepend ""<|endoftext|>""?'",2019-08-12T07:46:31Z,2019-08-15T10:01:39Z,,,
1008,b'How can I use only one layer transformer via this repository?',2019-08-12T03:06:25Z,2019-08-14T04:05:08Z,,,
1007,b'can somebody share an example of how to use GPT2 model for multiclass classification problem with fine tuning Language model ?',2019-08-11T19:17:18Z,2019-10-17T20:11:05Z,wontfix,,
1006,b'Update README.md',2019-08-11T13:37:30Z,2019-08-12T13:53:06Z,,,
1005,"b""Can't get attribute 'Corpus' on <module '__main__' from 'convert_transfo_xl_checkpoint_to_pytorch.py'>""",2019-08-11T11:15:20Z,2019-08-14T07:52:48Z,,AttributeError,"AttributeError: Can't get attribute 'Corpus' on <module '__main__' from 'convert_transfo_xl_checkpoint_to_pytorch.py'>"
1004,b'Refactoring old run_swag.py',2019-08-11T08:04:12Z,2019-09-18T19:42:52Z,,,
1003,"b""Can't GPT-2 set special_tokens? (or unk tokens)""",2019-08-10T14:12:38Z,2019-10-25T13:58:37Z,wontfix,,
1002,b'How to make a new line when using gpt2 to generate lyrics?',2019-08-10T13:40:35Z,2019-10-21T14:47:27Z,wontfix,,
1001,b'How do a put a different classifier on top of BertForSequenceClassification?',2019-08-10T07:19:49Z,2019-08-17T10:58:19Z,,,
1000,b'Running on GPU?',2019-08-09T21:12:58Z,2019-08-09T21:53:46Z,,,
999,b'Multi_Head Attention in BERT different from Transformer?',2019-08-09T20:20:13Z,2019-08-12T22:29:31Z,,,
998,b'Running the pytorch.distributed.launch example of Glue hangs at evaluation',2019-08-09T18:03:56Z,2019-10-27T05:30:37Z,wontfix,,
997,b'Is XLA feature existed in current repo?',2019-08-09T15:19:29Z,2019-08-09T22:26:43Z,,,
996,b'Small typo fix in logger',2019-08-09T10:38:48Z,2019-08-09T23:36:45Z,,,
995,b'BERT with sequence pairs & padding',2019-08-09T09:46:46Z,2019-08-12T09:53:13Z,,,
994,b'Pretrained GPT2 mdoels does not load unk special symbol',2019-08-08T20:53:10Z,2019-08-10T20:01:27Z,,,
993,b'RuntimeError:  Invalid index in gather at ../aten/src/TH/generic/THTensorEvenMoreMath.cpp:469  (GPT2DoubleHeadsModel)',2019-08-08T17:27:18Z,2019-08-10T18:04:43Z,,RuntimeError,"RuntimeError: Invalid index in gather at ../aten/src/TH/generic/THTensorEvenMoreMath.cpp:469"
992,b'Any idea how to use pytorch-transformers for Entity Linking?',2019-08-08T12:36:28Z,2019-10-14T13:11:53Z,wontfix,,
991,b'Supress long sequence tokenization warning',2019-08-08T12:05:12Z,2019-08-12T13:35:14Z,,,
990,b'bert-base-multilingual-uncased vocabulary not consecutive',2019-08-08T10:08:18Z,2019-10-26T10:07:29Z,wontfix,,
989,"b'Using hidden states from BERT (Similar to using precomputed hidden states in GPT2 model ""past"" argument)'",2019-08-08T09:35:04Z,2019-10-25T13:58:36Z,wontfix,,
988,b'seq2seq model with transformer',2019-08-08T09:20:43Z,2019-10-21T07:16:34Z,wontfix,,
987,b'Generative finetuning',2019-08-07T21:43:53Z,2019-08-28T14:51:51Z,,,
986,b'Potential bug with gradient clipping when using gradient accumulation in examples',2019-08-07T14:59:58Z,2019-12-09T17:47:56Z,wontfix,,
985,b'Unable to read pre-trained model using BertModel.from_pretrained ',2019-08-07T14:31:51Z,2019-11-02T12:51:36Z,wontfix,,
984,b'docs: correct number of layers for various xlm models',2019-08-07T14:24:43Z,2019-08-20T09:04:55Z,,,
983,b'Worse performance of gpt2 than gpt',2019-08-07T12:29:32Z,2020-01-20T19:33:19Z,wontfix,,
982,b'How to predict masked whole word which was tokenized as sub-words for bert-base-multilingual-cased',2019-08-07T11:06:23Z,2019-10-17T12:25:58Z,wontfix,,
981,b'The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False.',2019-08-07T09:49:29Z,2019-08-07T15:23:07Z,,,
980,b'n/a',2019-08-07T01:32:05Z,2019-08-07T01:32:37Z,,,
979,b'n/a',2019-08-06T22:41:56Z,2019-08-06T22:42:09Z,,,
978,b'RuntimeError: bool value of Tensor with more than one value is ambiguous',2019-08-06T19:34:53Z,2019-08-06T20:53:35Z,,,
977,b'Fixed typo in migration guide',2019-08-06T18:20:57Z,2019-08-07T08:08:45Z,,,
976,b'Issue: Possibly wrong documentation about labels in BERT classifier',2019-08-06T14:09:39Z,2019-08-08T14:39:08Z,,,
975,b'Inconsistant output between pytorch-transformers and pytorch-pretrained-bert',2019-08-06T10:11:20Z,2019-10-12T22:40:24Z,wontfix,,
974,b'Support longer sequences with BertForSequenceClassification',2019-08-06T09:57:56Z,2019-11-25T14:31:03Z,wontfix,,
973,b' Fix examples of loading pretrained models in docstring',2019-08-06T03:33:52Z,2019-08-07T08:08:23Z,,,
972,b'XLNetForQuestionAnswering - weight pruning',2019-08-05T19:16:51Z,2019-10-30T01:12:55Z,wontfix,,
971,b'Brackets are not aligned in the DocString of Bert.',2019-08-05T17:54:46Z,2019-08-08T14:37:16Z,,,
970,b'How to use   GPT2LMHeadModel  for conditional generation',2019-08-05T16:27:06Z,2019-08-05T19:20:44Z,,,
969,b'Finetune GPT2 ',2019-08-05T16:03:31Z,2019-08-05T16:16:10Z,,,
968,b'Error when running run_squad.py in colab',2019-08-05T14:23:42Z,2019-10-11T15:44:11Z,wontfix,"RuntimeError, subprocess.CalledProcessError","RuntimeError: Address already in usesubprocess.CalledProcessError: Command '['/usr/bin/python3', '-u', './examples/run_squad.py', '--local_rank=0', '--model_type', 'bert', '--model_name_or_path', 'bert-large-uncased-whole-word-masking', '--do_train', '--do_eval', '--do_lower_case', '--train_file', 'SQUAD_DIR/train-v1.1.json', '--predict_file', 'SQUAD_DIR/dev-v1.1.json', '--learning_rate', '3e-5', '--num_train_epochs', '2', '--max_seq_length', '384', '--doc_stride', '128', '--output_dir', 'wwm_uncased_finetuned_squad/', '--per_gpu_eval_batch_size=3', '--per_gpu_train_batch_size=3']' returned non-zero exit status 1."
967,b' Unable to load weights properly from tf checkpoint',2019-08-05T08:59:44Z,2019-12-10T10:23:12Z,wontfix,,
966,"b""AttributeError: module 'tensorflow.python.training.training' has no attribute 'list_variables'""",2019-08-05T08:12:07Z,2019-10-11T09:44:11Z,wontfix,,
965,b'How to output a vector',2019-08-05T04:00:22Z,2019-08-08T08:01:47Z,,,
964,"b'RoBERTa: model conversion, inference, tests \xf0\x9f\x94\xa5'",2019-08-05T02:07:45Z,2019-08-15T15:11:11Z,,,
963,b'Update modeling_bert.py',2019-08-05T00:58:25Z,2019-08-07T08:09:05Z,,,
962,b'Update modeling_xlnet.py',2019-08-05T00:57:27Z,2019-08-07T08:09:21Z,,,
961,"b""Deep learning NLP models for children's story understanding?""",2019-08-04T17:11:22Z,2019-12-08T17:54:42Z,,,
960,b'Fixing unused weight_decay argument',2019-08-04T16:32:05Z,2019-08-07T08:09:55Z,,,
959,b'Use the fine-tuned model for another task',2019-08-04T13:58:58Z,2019-10-14T15:11:52Z,wontfix,,
958,b'Fixed small typo',2019-08-04T06:06:56Z,2019-08-07T08:10:20Z,,,
957,b'total training steps and tokenization in run_glue',2019-08-03T17:41:25Z,2019-08-08T00:29:31Z,,,
956,b'Tokenizer added special token attributes missing',2019-08-03T08:47:04Z,2019-08-16T06:45:36Z,,AttributeError,"AttributeError: 'XLNetTokenizer' object has no attribute 'custom_token'"
955,b'Fix comment typo',2019-08-03T04:18:55Z,2019-08-07T08:11:26Z,,,
954,"b""Bert model instantiated from BertForMaskedLM.from_pretrained('bert-base-uncased') and BertForMaskedLM(BertConfig.from_pretrained('bert-base-uncased')) give different results """,2019-08-03T03:11:00Z,2019-10-12T04:44:10Z,wontfix,,
953,b'How to add some parameters in gpt-2 (in attention layer) and initialize the original gpt-2 parameters with pre-trained model and the new introduced parameters randomly?',2019-08-02T17:17:05Z,2019-11-08T07:30:35Z,wontfix,,
952,b'Add 117M and 345M as aliases for pretrained models',2019-08-02T16:18:19Z,2019-08-21T18:39:24Z,,,
951,b'run_swag.py should use AdamW',2019-08-02T14:58:50Z,2019-08-30T12:14:45Z,,,
950,b'CONFIG_NAME and WEIGHTS_NAME are missing in modeling_transfo_xl.py',2019-08-02T13:59:39Z,2019-08-05T15:26:40Z,,ImportError,"ImportError: cannot import name 'CONFIG_NAME'"
949,b'<model>ForQuestionAnswering loading non-deterministic weights',2019-08-02T13:15:04Z,2019-08-05T15:24:57Z,,,
948,b'How to train BertModel',2019-08-02T07:23:21Z,2019-08-05T16:19:44Z,,,
947,b'[XLNet] Parameters to reproduce SQuAD scores',2019-08-02T05:20:52Z,2020-01-26T04:13:34Z,wontfix,,
946,b'Using memory states with XLNet / TransfoXL',2019-08-01T19:22:06Z,2019-10-12T21:40:25Z,wontfix,RuntimeError,"RuntimeError: invalid argument 0: Sizes of tensors must match except in dimension 0. Got 14 and 32 in dimension 1 at /pytorch/aten/src/THC/generic/THCTensorMath.cu:71"
945,b'_convert_id_to_tokens for XLNet not working',2019-08-01T19:13:25Z,2019-10-11T17:44:10Z,wontfix,TypeError,"TypeError: in method 'SentencePieceProcessor_IdToPiece', argument 2 of type 'int'"
944,b'Missing lines in Readme examples?',2019-08-01T15:21:50Z,2019-08-05T15:17:15Z,,,
943,b'Is pytorch-transformers useful for training from scratch on a custom dataset?',2019-08-01T13:30:17Z,2019-10-27T08:30:36Z,wontfix,,
942,b'Using BERT for predicting masked token',2019-08-01T10:16:20Z,2019-10-12T07:44:10Z,wontfix,,
941,b'Updated model token sizing to replace removed parameter `num_special_\xe2\x80\xa6',2019-08-01T10:13:17Z,2019-12-06T19:12:59Z,wontfix,,
940,b'Unexpectedly preprocess when multi-gpu using',2019-08-01T08:44:49Z,2020-01-11T07:47:05Z,wontfix,,
939,b'Chinese BERT broken',2019-08-01T08:38:55Z,2019-10-11T14:58:29Z,wontfix,,
938,b'Performance dramatically drops down after replacing pytorch-pretrained-bert with pytorch-transformers',2019-08-01T08:36:01Z,2019-08-05T14:29:04Z,,,
937,b'Wrong refactoring of mandatory parameters for run_squad.py',2019-08-01T08:11:17Z,2019-10-11T15:44:10Z,wontfix,,
936,b'XLNet large low accuracy',2019-08-01T05:48:23Z,2019-10-11T13:44:12Z,wontfix,,
935,b'run_glue : Evaluating in every grad_accumulation_step if flag eval during training is true',2019-08-01T05:35:10Z,2019-08-01T05:44:07Z,,,
934,b'Feature Request : run_swag with XLNet and XLM ',2019-08-01T02:42:08Z,2019-08-05T12:47:07Z,,,
933,b'link to `swift-coreml-transformers`',2019-08-01T01:10:02Z,2019-08-01T13:50:31Z,,,
932,"b'pip install error: ""regex_3/_regex.c:48:10: fatal error: Python.h: No such file or directory""'",2019-08-01T00:09:39Z,2019-08-01T00:43:59Z,,,
931,b'Updating run_swag script for new pytorch_transformers setup',2019-07-31T22:03:44Z,2019-10-11T13:44:11Z,wontfix,ImportError,"ImportError: cannot import name 'WEIGHTS_NAME'"
930,b'Fixing a broken link in the README.md',2019-07-31T14:18:39Z,2019-07-31T14:22:42Z,,,
929,"b""AttributeError: 'NoneType' object has no attribute 'split'""",2019-07-31T09:42:09Z,2019-08-05T12:44:00Z,,AttributeError,AttributeError: 'NoneType' object has no attribute 'split'
928,b'ERNIE 2.0 ?',2019-07-31T04:25:52Z,2019-10-07T04:32:07Z,wontfix,,
927,b'`do_wordpiece_only` argument',2019-07-31T00:08:19Z,2019-10-10T09:44:41Z,wontfix,,
926,b'Feature request: roBERTa',2019-07-30T16:45:02Z,2019-07-30T17:48:14Z,,,
925,b'Torchscipt mode for BertForPreTraining',2019-07-30T14:29:39Z,2019-08-01T13:48:44Z,,TypeError,"TypeError: __init__() got an unexpected keyword argument 'torchscript'"
924,b'[RuntimeError: sizes must be non-negative] in run_squad.py using xlnet large model',2019-07-30T11:09:44Z,2019-08-05T12:39:03Z,,,
923,"b""Don't save model without training (example/run_squad.py bug)""",2019-07-30T10:40:43Z,2019-08-18T09:02:26Z,,,
922,"b""TypeError: 'NoneType' object is not callable""",2019-07-30T07:48:06Z,2019-10-11T18:44:10Z,wontfix,TypeError,TypeError: 'NoneType' object is not callable
921,b'Issues in visualizing a fine tuned model',2019-07-30T05:48:15Z,2019-10-13T15:00:30Z,wontfix,,
920,b'Unigram frequencies in GPT-2 or XLnet?',2019-07-30T03:21:39Z,2019-08-08T06:33:54Z,,,
919,b'Code snippet on docs page using old import',2019-07-29T22:48:31Z,2019-08-05T11:45:34Z,,,
918,b'Export to Tensorflow not properly implemented',2019-07-29T14:46:13Z,2019-08-05T10:55:30Z,,,
917,b'XLNet: Sentence probability/perplexity',2019-07-29T05:57:27Z,2019-08-27T13:20:30Z,,,
916,b'Avoid i/o in class __init__ methods',2019-07-28T11:31:25Z,2019-10-04T09:52:40Z,wontfix,,
915,b'Wrong layer names for selecting parameters groups (run_openai_gpt.py)',2019-07-28T10:01:17Z,2019-10-04T01:04:32Z,wontfix,,
914,"b""Using new pretrained model with it's own vocab.txt file. """,2019-07-28T01:53:51Z,2019-11-08T08:30:31Z,wontfix,,
913,b'Best practices for combining large pretrained models with smaller models?',2019-07-28T01:42:10Z,2019-10-03T03:01:35Z,wontfix,,
912,b'adding vocabulary in OpenAI GPT2 tokenizer issue',2019-07-27T07:57:04Z,2019-07-30T00:07:26Z,,,
911,b'Small fixes',2019-07-26T19:30:11Z,2019-07-26T19:36:22Z,,,
910,b'Adding AutoTokenizer and AutoModel classes that automatically detect architecture - Clean up tokenizers',2019-07-26T17:27:32Z,2019-08-05T17:17:48Z,,,
909,b'[develop] Convenience args.{train/dev}_file arguments.',2019-07-26T16:52:30Z,2019-09-16T13:48:54Z,,,
908,b'Cannot inherit from BertPretrainedModel  anymore after migrating to pytorch-transformers',2019-07-26T16:36:25Z,2019-07-26T19:36:22Z,,,
907,b'Fix convert to tf',2019-07-26T13:35:08Z,2019-08-05T10:55:05Z,,,
906,b'cuda out of memory',2019-07-26T08:12:11Z,2020-02-09T21:58:58Z,wontfix,RuntimeError,"RuntimeError: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 0; 10.92 GiB total capacity; 6.34 GiB already allocated; 28.50 MiB free; 392.76 MiB cached)`"
905,"b""Bugfix for encoding error during GPT2Tokenizer.from_pretrained('local\xe2\x80\xa6""",2019-07-25T20:50:23Z,2019-08-13T17:19:59Z,,UnicodeDecodeError,UnicodeDecodeError: 'ascii' codec can't decode byte 0xc2 in position 840: ordinal not in range(128)
904,b'AssertionError while using DataParallelModel',2019-07-25T19:40:06Z,2019-10-11T16:44:10Z,wontfix,,
903,b'why the acc of chinese model(bert) is just 0.438',2019-07-25T11:40:18Z,2019-10-24T06:42:25Z,wontfix,,
902,b'Torchscript Trace slower with C++ runtime environment.',2019-07-25T11:06:57Z,2019-10-14T06:00:30Z,wontfix,,
901,b'bug: it is broken to use tokenizer path',2019-07-25T09:44:26Z,2019-07-26T19:36:25Z,,json.decoder.JSONDecodeError,"json.decoder.JSONDecodeError: Expecting value: line 1 column 2 (char 1)"
900,b'SpanBERT support',2019-07-25T09:02:22Z,2019-11-18T09:20:06Z,wontfix,,
899,b'Fixed import to use torchscript flag.',2019-07-25T08:56:42Z,2019-07-25T13:03:22Z,,,
898,b'fp16 is still has the problem',2019-07-25T07:02:21Z,2019-08-01T12:17:57Z,,,
897,b'Fix FileNotFoundError when running on SQuAD-v1.1',2019-07-25T06:10:57Z,2019-07-25T13:03:04Z,,,
896,b'fix multi-gpu training bug when using fp16',2019-07-25T05:15:19Z,2019-07-26T17:31:02Z,,,
895,b'fix a bug of saving added tokens',2019-07-25T04:50:27Z,2019-07-25T13:02:02Z,,,
894,b'Sequence length more than 512',2019-07-25T03:11:39Z,2019-09-30T13:09:30Z,wontfix,,
893,b'make save_pretrained do the right thing with added tokens',2019-07-24T23:56:03Z,2019-07-25T12:58:49Z,,,
892,b'How to add new special token',2019-07-24T23:20:38Z,2019-07-27T01:18:00Z,,,
891,b'BERT: run_squad.py falling over after eval',2019-07-24T23:19:46Z,2019-07-25T13:00:20Z,,FileNotFoundError,"FileNotFoundError: [Errno 2] No such file or directory: '/content/SQuAD_for_bert/models/bert_base_uncased_finetuned_script/null_odds_.json'`"
890,b'PreTrainedTokenizer.from_pretrained should be more general',2019-07-24T20:56:49Z,2019-09-30T16:09:31Z,wontfix,,
889,b'Increased number of hidden states returned from transformers in latest release',2019-07-24T19:57:32Z,2019-07-25T13:45:41Z,,,
888,b'Update docs for parameter rename',2019-07-24T18:30:21Z,2019-07-25T13:01:23Z,,,
887,b'No gradient clipping in AdamW',2019-07-24T16:03:40Z,2019-10-11T11:44:10Z,wontfix,,
886,b'BERT uncased model outputs a tuple instead of a normal pytorch tensor ',2019-07-24T14:46:30Z,2019-07-24T14:53:46Z,,AttributeError,"AttributeError: 'tuple' object has no attribute 'log_softmax'"
885,b'Can lm_finetuning be used with non-english data ? ',2019-07-24T14:11:50Z,2019-07-25T12:36:37Z,,,
884,b'Customized BertForTokenClassification Model',2019-07-24T11:16:09Z,2020-04-26T12:31:08Z,wontfix,,
883,b'Upgrade to new FP16',2019-07-24T10:13:11Z,2019-07-27T14:18:57Z,,,
882,b'fix squad v1 error (na_prob_file should be None)',2019-07-24T08:12:44Z,2019-07-25T13:00:00Z,,,
881,b'can not convert_tf_checkpoint_to_pytorch',2019-07-24T07:35:32Z,2019-07-24T08:12:30Z,,ValueError,"ValueError: Couldn't find 'checkpoint' file or checkpoints in given directory /home/zxr/summary/bertsum/src/ckpt_to_bin/uncased_L-12_H-768_A-12"
880,b'Printing Iteration every example problem',2019-07-24T07:07:29Z,2019-10-04T01:04:33Z,wontfix,,
879,b'fix #878',2019-07-24T07:04:57Z,2019-08-20T23:16:01Z,,,
878,b'Fail to load pre-trained tokens.',2019-07-24T07:02:30Z,2019-10-20T13:44:00Z,wontfix,,
877,b'error when tried to migrate from pretrained-bert to transformers.',2019-07-23T22:18:56Z,2019-07-24T16:02:59Z,,AttributeError,AttributeError: 'int' object has no attribute 'view'`
876,b'How to use BERT for finding similar sentences or similar news? ',2019-07-23T22:16:01Z,2021-04-26T15:03:07Z,,,
875,b'XLNet bidirectional input pipeline requires batch size at least 2',2019-07-23T18:18:01Z,2019-09-28T20:00:09Z,wontfix,RuntimeError,"RuntimeError: shape '[x, y, z]' is invalid for input of size 0"
874,b'Fine-tuning model and Generation',2019-07-23T17:43:02Z,2019-11-15T08:03:34Z,wontfix,,
873,b'Add nn.Identity replacement for old PyTorch',2019-07-23T15:53:06Z,2019-07-23T16:16:36Z,,,
872,b'Updating schedules for state_dict saving/loading',2019-07-23T13:59:43Z,2019-07-23T14:03:07Z,,,
871,b'fp16 is not work',2019-07-23T11:52:50Z,2019-07-23T15:59:05Z,,,
870,b'How to load a fine-tuned model pytorch_model.bin produced by run_bert_swag.py',2019-07-23T09:04:06Z,2019-07-24T07:49:45Z,,RuntimeError,"RuntimeError: storage has wrong size: expected 4357671300540823961 got 589824."
869,"b"" module 'torch.nn' has no attribute 'Identity'""",2019-07-23T08:20:57Z,2019-07-23T16:16:36Z,,AttributeError,"AttributeError: module 'torch.nn' has no attribute 'Identity'"
868,b'fp16 is broken',2019-07-23T08:18:21Z,2019-07-23T15:46:14Z,,RuntimeError,"RuntimeError: Incoming model is an instance of torch.nn.parallel.DistributedDataParallel. Parallel wrappers should only be applied to the model(s) AFTER"
867,b'XLnet sentence vector',2019-07-23T02:06:26Z,2019-10-11T10:44:13Z,wontfix,,
866,b'Rework how PreTrainedModel.from_pretrained handles its arguments',2019-07-22T20:42:34Z,2019-07-23T16:05:30Z,,,
865,b'Using Fp16 half precision makes Bert prediction slower.',2019-07-22T20:01:12Z,2019-10-11T13:44:10Z,wontfix,,
864,b'Fixed PreTrainedModel.from_pretrained(...) not passing cache_dir to PretrainedConfig.from_pretrained(...)',2019-07-22T19:17:26Z,2019-07-23T13:28:06Z,,,
863,"b""PreTrainedModel.from_pretrained(...) doesn't pass cache_dir to PretrainedConfig.from_pretrained(...)""",2019-07-22T19:15:00Z,2019-07-23T16:18:01Z,,,
862,b'Bert encodings',2019-07-22T17:43:30Z,2019-10-11T10:44:11Z,wontfix,AttributeError,"AttributeError: 'list' object has no attribute 'split'"
861,b'Deleting models',2019-07-22T16:32:13Z,2019-07-22T16:54:05Z,,,
860,b'read().splitlines() -> readlines()',2019-07-22T12:49:29Z,2019-07-23T13:24:26Z,,,
859,b'Bug of BertTokenizer',2019-07-22T12:25:06Z,2019-07-24T00:51:17Z,,,
858,b'CLS segment_id for BERT',2019-07-22T12:23:08Z,2019-07-23T14:46:03Z,,,
857,b'XLMForMaskedLM',2019-07-22T09:57:54Z,2019-09-28T18:00:08Z,wontfix,,
856,b'manually download models',2019-07-22T05:31:09Z,2019-10-05T11:43:30Z,wontfix,,
855,"b""modeling_xlnet.py lines 798  torch.eisum('i,d->id', pos_seq, inv_freq)""",2019-07-22T04:55:53Z,2019-12-01T03:07:47Z,wontfix,,
854,b'Get the different result at BertModel',2019-07-22T04:31:33Z,2019-09-28T16:00:10Z,wontfix,,
853,b'Error loading converted pytorch checkpoint',2019-07-22T04:01:29Z,2019-07-24T02:22:51Z,,,
852,"b'UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.'",2019-07-22T02:55:22Z,2019-10-07T19:54:30Z,wontfix,,
851,b'problem when calling resize_token_embeddings',2019-07-22T02:08:29Z,2019-10-11T10:44:14Z,wontfix,RuntimeError,"RuntimeError: The size of tensor a (21215) must match the size of tensor b (21128) at non-singleton dimension 2"
850,b'Confused about the prune heads operation.',2019-07-21T14:21:07Z,2019-07-24T07:35:26Z,,,
849,"b""can't find utils_glue""",2019-07-21T13:38:55Z,2019-09-27T14:29:41Z,wontfix,,
848,b'adaptive softmax in transformer-xl',2019-07-21T13:22:04Z,2019-09-28T16:00:09Z,wontfix,,
847,b'typos',2019-07-21T12:40:09Z,2019-07-23T13:28:32Z,,,
846,b'XLNET completely wrong and random output',2019-07-20T23:25:57Z,2020-02-09T14:58:55Z,wontfix,,
845,b'fixed version issues in run_openai_gpt',2019-07-20T10:43:37Z,2019-07-23T13:29:32Z,,,
844,b'Fixed typo',2019-07-20T08:50:11Z,2019-07-21T15:05:37Z,,,
843,b'Issue ',2019-07-20T03:51:49Z,2019-07-20T07:16:12Z,wontfix,,
842,b'16 GB dataset for finetuning fail on reduce_memory ',2019-07-20T01:28:11Z,2019-09-25T02:04:30Z,wontfix,,
841,b'Detaching Variables',2019-07-20T00:24:35Z,2019-09-28T16:00:09Z,wontfix,,
840,"b""AttributeError: 'BertModel' object has no attribute '_load_from_state_dict'""",2019-07-19T21:08:44Z,2019-08-03T11:15:10Z,,,
839,b'How to restore a training?',2019-07-19T19:58:37Z,2019-09-30T18:09:31Z,wontfix,,
838,b'Standardized head for Question Answering',2019-07-19T15:03:36Z,2019-11-25T10:31:02Z,wontfix,,
837,b'run_openai_gpt.py issues with Adamw',2019-07-19T14:13:50Z,2019-09-28T23:24:33Z,wontfix,,
836,b'BertForNextSentencePrediction labels',2019-07-19T14:06:03Z,2019-08-09T22:47:19Z,,,
835,b'How to use the pretrain script with only token classification task ?',2019-07-19T07:20:16Z,2019-08-09T22:46:51Z,,,
834,b'git pull pytorch-transformers??',2019-07-19T06:07:16Z,2019-07-19T15:07:27Z,,,
833,"b""missing 1 required positional argument: 'num_classes' in 'from_pretrained'""",2019-07-19T06:00:59Z,2019-08-09T01:19:30Z,,TypeError,"TypeError: __init__() missing 1 required positional argument: 'num_classes'"
832,b'Training with wrong GPU count',2019-07-19T02:27:13Z,2019-08-09T22:46:41Z,,,
831,b'finetune_on_pregenerate Loss.backwards() throw an error ',2019-07-19T00:42:46Z,2019-07-23T12:12:48Z,,,
830,b'AdamW does not have args warmup and t_total',2019-07-19T00:39:41Z,2019-09-28T14:00:07Z,wontfix,,
829,b'RoBERTa support',2019-07-18T22:05:10Z,2019-12-04T13:50:39Z,,,
828,b'CUDA error: invalid configuration argument when not using DataParallel',2019-07-18T20:38:46Z,2019-07-19T14:22:11Z,,RuntimeError,"RuntimeError: CUDA error: invalid configuration argument"
827,b'xlnet input_mask and attention_mask type error',2019-07-18T19:25:01Z,2019-07-23T12:06:56Z,,RuntimeError,"RuntimeError: expected backend CUDA and dtype Float but got backend CUDA and dtype Byte"
826,b'Providing older documentation',2019-07-18T17:09:06Z,2019-07-19T07:37:13Z,,,
825,b'Chinese BERT broken probably after `pytorch-transformer` release',2019-07-18T16:49:13Z,2019-07-26T08:19:31Z,,,
824,b'Bertology example is probably broken',2019-07-18T16:15:13Z,2019-09-21T15:50:04Z,wontfix,,
823,b'Updating simple_lm_finetuning.py for FP16 training',2019-07-18T15:19:54Z,2019-10-12T16:40:24Z,wontfix,,
822,"b""XLNet-large-cased on Squad 2.0: can't replicate results""",2019-07-18T14:09:31Z,2019-10-27T02:07:30Z,wontfix,,
821,"b""Couldn't reach server """,2019-07-18T12:03:44Z,2019-11-02T13:51:37Z,wontfix,,
820,b'RuntimeError: Creating MTGP constants failed',2019-07-18T10:29:49Z,2019-11-16T06:53:18Z,wontfix,RuntimeError,"RuntimeError: **Creating MTGP constants failed.** at /opt/conda/conda-bld/pytorch_1556653099582/work/aten/src/THC/THCTensorRandom.cu:33"
819,b'Output of BertModel does not match the last hidden layer from fixed feature vectors',2019-07-18T07:16:38Z,2019-10-30T03:12:56Z,wontfix,,
818,b'GPT sentence log loss: average or summed loss?',2019-07-18T07:09:05Z,2019-07-18T21:26:17Z,,,
817,b'from pytorch-pretrained-bert to pytorch-transformers\xef\xbc\x8csome problem',2019-07-18T06:32:50Z,2019-09-23T23:29:11Z,wontfix,TypeError,"TypeError: forward() got an unexpected keyword argument 'output_all_encoded_layers'"
816,b'typos',2019-07-18T06:17:43Z,2019-07-18T13:54:06Z,,,
815,b'Update Readme link for Fine Tune/Usage section',2019-07-18T05:51:44Z,2019-07-18T16:30:33Z,,,
814,b'Is there any plan of developing softmax-weight function for using 12 hidden BERT layer? ',2019-07-18T02:03:24Z,2019-09-24T16:29:11Z,wontfix,,
813,b'How to use BertModel ?',2019-07-18T00:41:46Z,2019-10-01T08:09:04Z,wontfix,,
812,b'do I need to add sep and cls token in each sequence ?',2019-07-18T00:22:57Z,2019-07-19T00:26:41Z,,,
811,"b""Fix openai-gpt ROCStories example's issues with AdamW optimizer""",2019-07-17T21:54:47Z,2019-07-23T20:57:16Z,,,
810,b'SEG_ID constants for XLNet misleading/off',2019-07-17T20:45:40Z,2019-07-18T21:36:07Z,,,
809,b'Problem loading finetuned XLNet model',2019-07-17T20:30:57Z,2019-07-25T11:38:08Z,,,
808,b'GPT2 model does not have attention mask',2019-07-17T18:36:10Z,2019-08-20T23:54:07Z,,,
807,"b""AttributeError: 'tuple' object has no attribute 'softmax'""",2019-07-17T17:53:07Z,2019-07-17T19:07:58Z,,AttributeError,"AttributeError: 'tuple' object has no attribute 'softmax'"
806,b'Fix a path so that a test can run on Windows',2019-07-17T16:10:39Z,2019-08-20T23:14:41Z,,,
805,"b'Where is ""run_bert_classifier.py""?'",2019-07-17T14:57:53Z,2019-07-17T15:34:03Z,,,
804,b'Answers to Bullet/List Items by bert',2019-07-17T13:31:49Z,2019-09-22T14:23:28Z,wontfix,,
803,b'AssertionError in BERT-Quickstart example',2019-07-17T11:21:44Z,2019-07-17T12:23:03Z,,,
802,b'fp16+xlnet did not gain any speed increase ',2019-07-17T09:30:23Z,2019-11-21T15:49:12Z,wontfix,,
801,b'import sys twice',2019-07-17T09:12:19Z,2019-07-17T10:31:27Z,,,
800,b'attention_mask at run_squad.py',2019-07-17T06:30:54Z,2019-07-17T11:51:20Z,,,
799,b'Error while adding new tokens to GPT2 tokenizer',2019-07-17T06:22:02Z,2019-08-05T17:17:48Z,,TypeError,"TypeError: 'NoneType' object is not iterable"
798,b'[bug]BertAdam change to AdamW in example',2019-07-17T05:59:02Z,2019-07-19T07:10:04Z,,,
797,b'fix some errors for distributed lm_finetuning',2019-07-17T01:18:50Z,2019-07-18T21:32:34Z,,,
796,b'Minor documentation updates',2019-07-16T21:45:23Z,2019-07-17T10:26:35Z,,,
795,b'XLNet-large-cased: hyper-parameters for fine-tuning on SST-2',2019-07-16T21:42:09Z,2019-09-30T11:09:30Z,wontfix,,
794,b'Adding additional model loading functionality',2019-07-16T18:40:23Z,2019-08-30T12:04:25Z,,,
793,b'BertModel docstring missing pooled_output',2019-07-16T18:08:51Z,2019-07-16T19:36:02Z,,,
792,b'Issue running run_transfo_xl.py',2019-07-16T18:03:38Z,2019-07-16T19:22:26Z,,ValueError,"ValueError: too many values to unpack (expected 2)"
791,b'RestructuredText table for pretrained models.',2019-07-16T16:00:17Z,2019-08-05T14:18:01Z,,,
790,b'XLNet Embeddings',2019-07-16T10:27:47Z,2019-08-27T12:16:04Z,,,
789,b'XLNet text generation ability : inference is slow',2019-07-16T00:23:34Z,2020-06-03T13:06:43Z,Ex: Generation,,
788,b'bert-large config file',2019-07-15T13:52:49Z,2019-09-20T15:02:12Z,wontfix,,
787,b'How to use Bert QA model for predictions?',2019-07-15T00:50:48Z,2019-09-23T00:33:57Z,wontfix,,
786,b'New documentation for pytorch-transformers',2019-07-12T09:50:14Z,2019-07-13T10:08:58Z,,,
785,b'Implementation of 15% words masking would cause the drop of performance in short text ',2019-07-12T07:50:50Z,2019-09-17T08:45:53Z,wontfix,,
784,b'[bug] from_pretrained  error with from_tf',2019-07-12T05:22:08Z,2019-07-13T20:59:18Z,,,
783,b'how to get the word vector from bert pretrain model ?',2019-07-12T01:53:41Z,2019-07-12T06:39:14Z,,,
782,b'Why the activation function is tanh in BertPooler',2019-07-12T01:23:47Z,2019-07-13T21:04:20Z,,,
781,b'Clean up input embeddings resizing and weights tying',2019-07-11T22:05:10Z,2019-07-12T09:10:26Z,,,
780,b'Fail to run finetune_on_pregenerated.py',2019-07-11T22:01:23Z,2019-09-16T22:45:52Z,wontfix,KeyError,"KeyError: 'Ad'"
779,b'Should close the SummaryWriter after using it',2019-07-11T20:12:59Z,2019-07-13T21:01:51Z,,,
778,b'Order of tokens in vocabulary of German model',2019-07-11T14:02:16Z,2019-09-22T09:23:28Z,wontfix,,
777,b'Working GLUE Example for XLNet (STS-B) ',2019-07-11T13:43:37Z,2019-07-11T13:43:47Z,,,
776,b'Working GLUE Example for XLNet (STS-B)',2019-07-11T13:41:50Z,2019-07-11T13:42:28Z,,,
775,b'fix typo in readme: extract_classif.py ==> extract_features.py',2019-07-11T11:38:03Z,2019-07-16T12:26:02Z,,,
774,b'XLNet text generation ability',2019-07-11T02:57:50Z,2019-07-13T21:04:09Z,,,
773,"b'Sphinx doc, XLM Checkpoints'",2019-07-10T23:05:25Z,2019-07-11T13:46:40Z,,,
772,"b""Cannot load 'bert-base-german-cased'""",2019-07-10T22:48:48Z,2019-07-11T10:27:16Z,,,
771,b'Performance dramatically drops down without training. ',2019-07-10T19:19:38Z,2019-07-13T21:06:28Z,,,
770,b'How can I load a fine-tuned model? ',2019-07-10T16:03:27Z,2019-09-18T21:38:28Z,wontfix,,
769,b'XLNet tensor at wrong device issuse',2019-07-10T04:56:24Z,2019-07-13T21:11:05Z,,RuntimeError,"RuntimeError: Expected object of backend CUDA but got backend CPU for argument #3 'index'"
768,b'GPT-2 language model decoding method',2019-07-09T15:55:04Z,2019-07-13T21:13:39Z,,,
767,b'Documentation',2019-07-09T14:53:16Z,2019-07-09T14:56:35Z,,,
766,b'Fine tune Xlnet',2019-07-09T13:23:07Z,2019-09-22T07:23:28Z,wontfix,,
765,b'Is is possible to fine-tune GPT2 on downstream tasks currently?',2019-07-09T00:16:41Z,2019-10-11T10:44:12Z,wontfix,,
764,b'Adding extra inputs when fine-tuning BERT',2019-07-08T15:55:58Z,2019-09-18T00:32:10Z,wontfix,,
763,"b""''bert-large-uncased-whole-word-masking-finetuned-squad' CAN'T be reached.""",2019-07-08T09:24:16Z,2019-09-14T21:13:20Z,wontfix,,
762,b'randrange() error when running pregenerate_training_data.py code in lm_finetuning',2019-07-08T04:45:29Z,2019-09-13T06:02:46Z,wontfix,ValueError,"ValueError: empty range for randrange() (198,198, 0)"
761,b'Help loading BioBERT weights',2019-07-07T23:23:36Z,2019-10-03T04:01:36Z,wontfix,,
760,b'Simple LM finetuning falls with RunTime Error: CUDA out of memory',2019-07-05T20:41:42Z,2019-09-27T22:08:21Z,wontfix,,
759,b'Release 0.7: pytorch-pretrained-bert => pytorch-transformers',2019-07-05T10:32:52Z,2019-07-09T15:06:04Z,,,
758,b'Release 0.7 - Add doc',2019-07-04T15:09:27Z,2019-07-05T09:22:04Z,,,
757,b'Release 0.7 - Add a real doc',2019-07-04T15:06:54Z,2019-07-04T15:07:28Z,,,
756,b'Invalid Syntax Error trying to run pregenerate_training_data.py',2019-07-04T14:00:56Z,2019-09-13T17:02:47Z,wontfix,,
755,b'TorchScript trace comparison with different sizes',2019-07-03T22:05:09Z,2019-08-07T18:46:07Z,,,
754,b'Get Attention Values for Pretrained Model',2019-07-03T21:45:56Z,2019-09-10T07:29:51Z,wontfix,,
753,"b'`bert-base-uncased` works for CoLA, `bert-large-uncased` always predicts one class'",2019-07-03T21:23:12Z,2019-09-08T22:59:08Z,wontfix,,
752,b'how to set the init learning rate when use bertAdam?',2019-07-03T06:57:04Z,2019-09-08T07:39:32Z,wontfix,,
751,b'Slower and more memory hungry than the TensorFlow BERT?',2019-07-03T00:30:47Z,2019-11-14T21:03:33Z,wontfix,,
750,b'Incorrect training loss scaling factor in examples/run_classifier.py?',2019-07-02T20:39:55Z,2019-09-14T22:13:20Z,wontfix,,
749,"b""Attribute Error : 'BertModel' object has no attribute 'bert'""",2019-07-02T17:45:04Z,2019-09-13T11:02:46Z,wontfix,AttributeError,"AttributeError: 'BertModel' object has no attribute 'bert'"
748,b'Release 0.7 - Add Torchscript capabilities',2019-07-02T14:42:54Z,2019-07-03T20:52:04Z,,,
747,b'BERT pretraining routine',2019-07-02T10:14:06Z,2019-07-13T21:26:32Z,,,
746,b'GPT2Tokenizer for Hindi Data',2019-07-02T09:22:14Z,2019-10-27T08:30:37Z,wontfix,,
745,b'fix evaluation bug',2019-07-01T21:58:39Z,2019-07-05T10:00:05Z,,,
744,b'Recommended multilingual bert cased model returns similar embeddings ',2019-07-01T13:24:04Z,2019-09-06T21:38:58Z,wontfix,,
743,b'Cannot reproduce results from version 0.4.0',2019-06-30T14:22:07Z,2019-07-01T03:50:41Z,,,
742,"b'When not loading a pretrained model, all layers are initialized with copies of the same weights'",2019-06-29T14:12:38Z,2019-06-29T14:20:03Z,,,
741,b'Using BertForNextSentencePrediction and GPT2LMHeadModel in a GAN setup.',2019-06-29T13:55:58Z,2019-07-03T11:40:31Z,,,
740,b'How to get perplexity score of a sentence using anyone of the given Language Models?',2019-06-29T11:30:25Z,2019-09-04T12:19:30Z,wontfix,,
739,"b'where is ""pytorch_model.bin""?'",2019-06-28T15:09:50Z,2019-09-03T17:19:30Z,wontfix,,
738,b'BertTokenizer never_split issue',2019-06-28T04:05:43Z,2019-09-06T09:36:14Z,wontfix,,
737,"b""gpt-2 model doesn't output hidden states of all layers.""",2019-06-28T01:30:28Z,2019-09-04T14:17:13Z,wontfix,,
736,b'Question regarding crossentropy loss function for BERTMaskedLM ',2019-06-27T23:21:13Z,2019-09-10T07:29:52Z,wontfix,,
735,b'BERT encoding layer produces same output for all inputs during evaluation ',2019-06-27T17:12:22Z,2019-07-09T15:44:50Z,,,
734,b'Erroneous Code',2019-06-27T06:34:24Z,2019-09-02T08:06:27Z,wontfix,,
733,b'Added option to use multiple workers to create training data',2019-06-26T23:19:00Z,2019-07-05T10:04:31Z,,,
732,b'GPT & GPT2: binary classification fails',2019-06-26T19:00:32Z,2019-09-01T22:06:28Z,wontfix,,
731,b'merge',2019-06-26T18:38:58Z,2019-06-28T15:07:56Z,,,
730,b'bertForNextSentencePrediction',2019-06-26T12:43:42Z,2019-06-29T07:06:01Z,,,
729,b'Grover generator support',2019-06-26T08:49:43Z,2019-09-01T10:02:23Z,wontfix,,
728,b'UnicodeDecodeError:',2019-06-26T08:39:21Z,2019-09-01T14:06:26Z,wontfix,UnicodeDecodeError,"UnicodeDecodeError: 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte"
727,b'Poor Training and evaluation accuracy even with low loss',2019-06-26T03:57:51Z,2019-09-06T17:38:58Z,wontfix,,
726,b'Examples does not work with apex optimizers',2019-06-26T02:12:21Z,2019-07-16T09:51:24Z,,,
725,b'BERT Input size reduced to half in forward function ',2019-06-25T16:52:21Z,2019-06-26T03:15:16Z,,,
724,b'fixing bugs in load_rocstories_dataset in run_openai_gpt.py',2019-06-25T14:31:38Z,2019-07-12T14:14:25Z,,,
723,b'Update Adam optimizer to follow pytorch convention for betas parameter (#510)',2019-06-25T08:29:30Z,2019-06-28T15:28:26Z,,,
722,b'low accuracy when fine tuning for the MRPC task with large model',2019-06-24T23:48:54Z,2019-08-31T17:02:23Z,wontfix,,
721,b'Usual loss when pretraining?',2019-06-24T15:13:35Z,2019-09-09T19:29:52Z,wontfix,,
720,"b""Import Error: cannot import name 'warmup_linear'""",2019-06-24T14:00:40Z,2019-08-05T08:59:54Z,,ImportError,"ImportError: cannot import name 'warmup_linear'"
719,b'Embedding and predictions in one forward pass',2019-06-24T07:31:48Z,2019-06-25T08:05:04Z,,,
718,b'Incorrect docstring for BertForMaskedLM',2019-06-23T17:49:03Z,2019-06-28T15:08:51Z,,,
717,b'BPE vocab',2019-06-22T23:38:38Z,2019-08-29T12:07:59Z,wontfix,,
716,b'Add tie_weights to XLNetForSequenceClassification',2019-06-22T22:15:23Z,2019-06-28T15:10:33Z,,,
715,b'Include a reference for LM finetuning',2019-06-22T14:08:02Z,2019-06-22T19:29:20Z,,,
714,b'Correct a broken link on README',2019-06-22T11:36:35Z,2019-06-22T19:29:41Z,,,
713,b'TypeError: expand_as() takes 1 positional argument but 5 were given',2019-06-22T08:41:11Z,2019-08-28T11:00:01Z,wontfix,,
712,b'BERT Tokenizer not working! Failed to load the bert-base-uncased model.',2019-06-21T19:41:48Z,2019-10-14T19:11:55Z,wontfix,AttributeError,"AttributeError: 'NoneType' object has no attribute 'tokenize'"
711,b'PyTorch-Transformers 1.0 - w. XLNet and XLM model - Standard API - Torchscript compatibility',2019-06-21T16:40:18Z,2019-07-16T09:51:23Z,,,
710,b'A way to increase input length limitation?',2019-06-21T12:33:32Z,2019-06-25T08:06:53Z,,,
709,b'layer_norm_eps',2019-06-21T11:37:50Z,2019-06-21T14:27:01Z,,,
708,b'Future attention masking in GPT/GPT-2?',2019-06-20T22:00:07Z,2019-08-13T03:02:26Z,,,
707,b'Update run_squad.py',2019-06-20T19:15:44Z,2019-07-05T10:02:36Z,,,
706,b'Update run_squad.py',2019-06-20T18:59:30Z,2019-06-20T19:12:02Z,,,
705,b'Implementing XLNet in pytorch',2019-06-20T04:26:24Z,2019-09-04T10:44:30Z,,,
704,b'Adjust s3 german Bert file storage',2019-06-19T16:43:09Z,2019-06-28T15:10:59Z,,,
703,"b'""Received \'killed\' signal"" during the circleci python3 build after submitting PR'",2019-06-19T15:18:33Z,2019-06-25T08:06:17Z,,,
702,b'Add an argument --model_size to convert_gpt2_checkpoint_to_pytorch.py',2019-06-19T15:04:07Z,2019-08-30T12:03:06Z,wontfix,,
701,b'Low SQuADv2 F1 & EM Score',2019-06-19T14:59:55Z,2019-08-29T04:00:01Z,wontfix,,
700,b'Add an argument --model_size to convert_gpt2_checkpoint_to_pytorch.py',2019-06-19T14:49:43Z,2019-06-19T15:03:27Z,,,
699,b'Fine tuning GPT-2 for LM objective function',2019-06-19T07:21:08Z,2019-10-13T22:00:31Z,wontfix,,
698,b'convert_gpt2_checkpoint_to_pytorch dimensions assertion error',2019-06-18T17:27:40Z,2019-06-19T15:14:32Z,,AssertionError,"AssertionError: (torch.Size([2304]), (3072,))`"
697,b'Updating examples',2019-06-18T14:30:41Z,2019-06-20T07:58:25Z,,,
696,b'Split config weights',2019-06-18T09:21:40Z,2019-06-18T09:42:58Z,,,
695,b'BERT output not deterministic',2019-06-17T23:07:59Z,2019-08-24T07:46:35Z,wontfix,,
694,b'Release 0.6.3',2019-06-17T10:18:31Z,2019-06-17T14:27:26Z,,,
693,b'Have no GPU to train language modelling',2019-06-17T03:28:51Z,2019-06-18T12:57:48Z,,,
692,b'Include a reference on in-domain LM pre-training for BERT',2019-06-16T07:24:09Z,2019-08-28T21:00:01Z,wontfix,,
691,"b'import class ""GPT2MultipleChoiceHead""'",2019-06-15T13:19:56Z,2019-06-15T21:12:56Z,,,
690,b'Transformer XL ProjectedAdaptiveLogSoftmax output fix',2019-06-15T02:13:41Z,2019-06-15T21:14:11Z,,,
689,b'Failing to run pregenerate_training_data.py & finetune_on_pregenerated.py',2019-06-14T21:09:57Z,2019-09-17T13:45:53Z,wontfix,AttributeError,"AttributeError: 'NoneType' object has no attribute 'vocab'"
688,"b'Add German Bert model to code, update readme'",2019-06-14T15:59:19Z,2019-06-15T21:13:42Z,,,
687,b'Updating tests and doc',2019-06-14T15:18:23Z,2019-06-14T15:23:46Z,,,
686,b'How to use GPT2 to predict and fit a word into an existing sentence?',2019-06-14T10:59:11Z,2019-08-15T13:30:35Z,wontfix,,
685,b'Add method to directly load TF Checkpoints for Bert models',2019-06-14T08:10:40Z,2019-06-27T02:47:17Z,,,
684,b'Implementation of 15% words masking in pretraining',2019-06-14T01:24:19Z,2019-09-16T13:44:28Z,wontfix,,
683,b'Fp16',2019-06-13T21:12:21Z,2019-06-13T21:12:28Z,,,
682,"b""Can't find gpt2 vocab file. """,2019-06-13T17:19:41Z,2019-06-13T18:08:45Z,,,
681,b'Can BertForMaskedLM be used to predict out-of-vocabulary words?',2019-06-13T15:43:47Z,2019-08-16T16:23:23Z,wontfix,,
680,b'Limit on the input text length?',2019-06-13T09:17:14Z,2019-06-15T00:03:27Z,,RuntimeError,"RuntimeError: index out of range at ../aten/src/TH/generic/THTensorEvenMoreMath.cpp:193"
679,b'Why the output of models are random.',2019-06-13T08:53:25Z,2019-06-14T20:22:07Z,,,
678,b'Transformer XL ProjectedAdaptiveLogSoftmax bug (maybe?)',2019-06-12T20:59:35Z,2019-06-16T03:34:02Z,,,
677,b'Download the model without executing a Python script',2019-06-12T15:57:14Z,2019-06-15T00:03:10Z,,,
676,b'Importing TF checkpoint as BertForTokenClassificiation',2019-06-12T10:30:10Z,2019-06-27T02:45:53Z,,AttributeError,"AttributeError: 'BertForTokenClassification' object has no attribute 'bias'"
675,b'[hotfix] Fix frozen pooler parameters in SWAG example.',2019-06-11T22:14:32Z,2019-06-12T08:01:22Z,,,
674,b'Gradual unfreezing and discriminative fine-tuning for BERT',2019-06-11T19:43:10Z,2019-08-18T09:23:05Z,wontfix,,
673,b'LM fine-tuning without NSP',2019-06-11T19:16:23Z,2019-08-23T14:46:34Z,wontfix,,
672,b'Add vocabulary and model config to the finetune output',2019-06-11T11:52:25Z,2019-06-14T15:02:47Z,,,
671,"b""BERT  what's different with step and t_total""",2019-06-11T06:54:23Z,2019-06-18T06:10:57Z,,,
670,b'warmup for BertAdam',2019-06-11T05:46:15Z,2019-06-12T02:42:44Z,,,
669,b'`get_final_text` bug when dealing with chinese sentence',2019-06-10T08:14:17Z,2019-08-16T09:58:55Z,wontfix,,
668,b'apply Whole Word Masking technique',2019-06-10T03:21:08Z,2019-06-11T09:29:11Z,,,
667,"b'when I use bert-large-uncased to load bert,runtimeError occured,but base-uncase is ok'",2019-06-10T02:45:39Z,2019-08-17T10:10:43Z,wontfix,RuntimeError,"RuntimeError: $ Torch: invalid memory size -- maybe an overflow? at ..\aten\src\TH\THGeneral.cpp:188"
666,b'GPT2 generating repetitive text',2019-06-08T09:01:22Z,2019-09-15T14:13:22Z,wontfix,,
665,b'GPT-2 medium and large release?',2019-06-08T02:04:14Z,2019-06-08T20:34:31Z,,,
664,b'Padding in GPT-2  ',2019-06-07T02:09:05Z,2019-08-31T09:56:03Z,wontfix,,
663,b'Accumulation',2019-06-06T15:12:33Z,2019-06-06T15:12:45Z,,,
662,"b'MRPC / SQuAD stuck in ""Running training""'",2019-06-06T11:19:44Z,2019-06-11T15:52:46Z,,,
661,b'How to load a existing model',2019-06-06T08:01:35Z,2019-08-31T09:56:04Z,wontfix,,
660,b'Recommended batch size and epochs for finetuning on large data',2019-06-06T03:00:42Z,2019-08-12T04:20:31Z,wontfix,,
659,b'Whole Word Masking Models update',2019-06-04T16:41:02Z,2019-08-26T11:47:25Z,wontfix,,
658,b'SQuAD 1.1 very low evaluation score when using `--fp16`',2019-06-04T14:10:49Z,2019-08-12T08:20:31Z,wontfix,,
657,b'How to use different learning rates in the classifier example.',2019-06-04T09:02:16Z,2019-08-05T09:58:34Z,wontfix,,
656,b'Use of GPT for multilingual LM',2019-06-03T08:33:13Z,2019-08-09T08:58:00Z,wontfix,,
655,b'Finish torchhub interfaces',2019-06-01T21:47:05Z,2019-06-14T15:02:09Z,,,
654,b'use of special tokens in gpt2?',2019-05-31T18:23:42Z,2019-08-07T00:44:32Z,wontfix,,
653,b'Different Results from version 0.4.0 to version 0.5.0',2019-05-31T09:12:52Z,2019-05-31T12:02:55Z,,,
652,b'RuntimeError: CUDA error: device-side assert triggered',2019-05-31T05:53:27Z,2019-08-05T06:42:05Z,wontfix,RuntimeError,"RuntimeError: CUDA error: device-side assert triggered"
651,b'Add GPT* compatibility to torchhub',2019-05-31T05:11:17Z,2019-05-31T12:44:53Z,,,
650,b'default in __init__s for classification BERT models',2019-05-30T19:48:06Z,2019-05-30T19:53:14Z,,,
649,"b'fine-tuning BERT, next sentence prediction loss is not decreasing'",2019-05-30T13:07:23Z,2019-08-05T13:57:57Z,wontfix,,
648,b'[Dropout] why  there is no dropout for the dev and eval?',2019-05-30T13:00:50Z,2019-07-02T03:32:39Z,,,
647,b'No softmax activation in BertForTokenClassification',2019-05-30T11:16:34Z,2019-05-30T11:25:49Z,,,
646,b'Fix link in README',2019-05-30T05:02:12Z,2019-06-14T14:57:46Z,,,
645,"b""BertAdam's get_lr() not return correct learning rate""",2019-05-29T08:34:48Z,2019-08-04T09:18:22Z,wontfix,,
644,b'RuntimeError: cublas runtime error : an internal operation failed at /pytorch/aten/src/THC/THCBlas.cu:258',2019-05-29T02:51:41Z,2019-08-09T01:00:59Z,wontfix,RuntimeError,"RuntimeError: cublas runtime error : an internal operation failed at /pytorch/aten/src/THC/THCBlas.cu:258"
643,"b""FileNotFoundError: [Errno 2] No such file or directory: 'uncased_L-12_H-768_A-12\\\\pytorch_model.bin'""",2019-05-28T17:57:49Z,2019-06-19T15:49:34Z,,,
642,b'Performing optimization on CPU',2019-05-28T15:55:17Z,2019-09-11T11:48:09Z,wontfix,,
641,b'The prediction accuracy for the masked token is ZERO when using the pretrained model. Does it make sense?',2019-05-28T15:54:39Z,2019-05-30T13:08:25Z,,,
640,b'Support latest multi language bert fine tune',2019-05-27T09:30:15Z,2019-06-14T14:57:03Z,,,
639,"b""Isn't it too few activations?""",2019-05-24T15:44:16Z,2019-07-30T16:52:02Z,wontfix,,
638,b'GPT-2 Tokenizer error!',2019-05-24T07:30:29Z,2019-05-29T16:31:07Z,,,
637,b'run_squad.py F1 and EM score are differ from tensorflow version ',2019-05-24T05:34:50Z,2019-07-30T10:52:02Z,wontfix,,
636,b'Training Dataset of GPT-2',2019-05-23T20:20:45Z,2019-08-24T16:25:42Z,wontfix,,
635,b'GPT2 - support data type torch.DoubleTensor for Position embedding',2019-05-23T09:19:24Z,2019-08-15T13:30:31Z,wontfix,,
634,b'convert_tf_checkpoint_to_pytorch get different result?',2019-05-23T08:50:45Z,2019-07-29T12:38:47Z,wontfix,,
633,b'bert->onnx ->caffe2 weird error',2019-05-23T06:59:07Z,2019-07-29T07:38:47Z,wontfix,RuntimeError,"RuntimeError: [enforce fail at pow_op.h:100] A.sizes() == B.sizes(). [4, 512, 768] vs []. Dimension mismatch - did you forget to set broadcast=1?"
632,"b""run_classifier.py:TypeError: forward() missing 1 required positional argument: 'input_ids'""",2019-05-22T13:55:40Z,2019-09-01T08:02:24Z,wontfix,,
631,b'from_pretrained',2019-05-22T11:23:23Z,2019-07-28T15:05:05Z,wontfix,,
630,b'Update run_squad.py',2019-05-22T09:31:20Z,2019-06-14T14:56:26Z,,,
629,b'Is loss.mean() needed?',2019-05-22T06:22:41Z,2019-07-30T07:52:01Z,wontfix,,
628,"b'IndexError: Dimension out of range (expected to be in range of [-1, 0], but got 1)'",2019-05-21T16:57:57Z,2019-05-21T17:02:05Z,,,
627,b'BERT QnA is not matching correct answer when document is in QnA format',2019-05-21T10:54:34Z,2019-08-24T12:25:40Z,wontfix,,
626,b'How to use run_squad.py to produce multiple answers for a question?',2019-05-21T10:03:32Z,2019-09-27T17:29:41Z,wontfix,,
625,"b'Tried to visualize the CLS Token embeddings after fine-tuning on SST-2 using t-SNE, but no clear clustered visualizations of positive and negative sentences !'",2019-05-20T16:20:45Z,2019-07-27T15:44:30Z,wontfix,,
624,b'tokenization_gpt2.py - on python 2 you can use backports.functools_lru_cache package from pypi',2019-05-20T13:17:14Z,2019-07-26T14:07:35Z,wontfix,,
623,b'Integration with a retriever Model',2019-05-20T04:05:35Z,2019-07-27T09:44:29Z,wontfix,,
622,"b'In run_classifier.py, is ""warmup_proportion"" a fraction or percentage?'",2019-05-20T00:43:26Z,2019-08-02T08:51:57Z,wontfix,,
621,b'Question on duplicated sentence',2019-05-19T06:01:58Z,2019-07-26T18:44:29Z,wontfix,,
620,b'Convert pytorch models back to tensorflow',2019-05-18T21:20:32Z,2019-07-05T10:01:18Z,,,
619,"b'Custom data, gradient explosion, accuracy is 0'",2019-05-18T19:39:59Z,2019-07-24T19:58:27Z,wontfix,,
618,b'Loss function of run_classifier.py takes in 2 inputs of different dimensions. ',2019-05-18T10:37:57Z,2019-05-18T10:58:15Z,,,
617,b'How to get the softmax probabilities from the TransfoXLLMModel',2019-05-17T10:51:10Z,2019-07-23T12:14:13Z,wontfix,,
616,b'TransfoXLModel and TransforXLLMModel have the same example',2019-05-17T09:40:14Z,2019-07-23T12:14:12Z,wontfix,,
615,"b""Couldn't import '''BertPreTrainedModel'''""",2019-05-17T08:30:10Z,2019-07-26T14:07:34Z,wontfix,,
614,b'global grad norm clipping (#581)',2019-05-15T17:01:26Z,2019-07-21T18:11:01Z,wontfix,,
613,b'Learning from scratch not working',2019-05-15T15:22:40Z,2019-07-21T16:11:04Z,wontfix,,
612,b'How to use the fine tuned model for classification (CoLa) task?',2019-05-15T14:18:16Z,2019-08-15T12:36:12Z,wontfix,,
611,b'extract_features',2019-05-15T12:35:46Z,2019-08-02T16:51:55Z,wontfix,UnicodeDecodeError,"UnicodeDecodeError: 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte"
610,b't_total',2019-05-15T08:56:33Z,2019-05-15T12:32:33Z,,ZeroDivisionError,"ZeroDivisionError: float division by zero"
609,b't_total',2019-05-15T08:51:35Z,2019-05-15T08:52:39Z,,,
608,"b'when using multiple GPUs, `loss.mean()` may have subtle bias '",2019-05-14T14:57:04Z,2019-07-21T09:49:51Z,wontfix,,
607,b'How to check the vocab size of bert large and bert small?',2019-05-12T22:20:02Z,2019-07-18T23:42:28Z,wontfix,,
606,b'How can we import cased bert model?',2019-05-12T22:18:30Z,2019-07-18T23:42:27Z,wontfix,,
605,b'why use  self.apply(self.init_bert_weights) in inhiritance class\xef\xbc\x9f',2019-05-12T22:16:25Z,2019-07-18T23:42:26Z,wontfix,,
604,"b'Fixing issue ""Training beyond specified \'t_total\' steps with schedule \'warmup_linear\'"" reported in #556'",2019-05-11T22:34:28Z,2019-06-14T14:49:25Z,,,
603,b'Using BERT as feature extractor',2019-05-11T14:18:43Z,2019-09-12T02:33:40Z,wontfix,,
602,b'Different GPT-2 outputs with mixed precision vs single precision',2019-05-11T00:38:21Z,2019-08-30T19:56:02Z,wontfix,,
601,b'How to reduce embedding size from 768?',2019-05-10T10:03:17Z,2019-09-09T10:59:08Z,wontfix,,
600,b'Fine tuning time did not change much after freezing layers ',2019-05-10T09:48:26Z,2019-07-16T12:07:50Z,wontfix,,
599,b'BERT tokenizer - set special tokens ',2019-05-10T08:38:43Z,2019-07-28T12:05:05Z,wontfix,,
598,b'Updating learning rate with special warm up in examples',2019-05-09T15:17:42Z,2019-05-10T11:48:13Z,,,
597,"b'GPT-2 (medium size model, special_tokens, fine-tuning, attention) + repo code coverage metric '",2019-05-08T20:38:56Z,2019-06-14T14:47:33Z,,,
596,b'[Question] Cross-lingual sentence representations',2019-05-08T12:42:27Z,2019-05-10T10:25:53Z,,,
595,b'Unclear error message when unable to cache the model',2019-05-07T12:12:06Z,2019-10-14T12:23:40Z,,,
594,b'size mismatch for lm_head.decoder.weight',2019-05-07T10:05:22Z,2019-07-13T11:22:01Z,wontfix,RuntimeError,"RuntimeError: Error(s) in loading state_dict for GPT2LMHeadModel:"
593,"b""Embedding' object has no attribute 'shape' """,2019-05-07T09:29:43Z,2019-08-03T07:35:30Z,wontfix,,
592,b'Can the use of [SEP] reduce the information extraction between the sentences?',2019-05-07T05:13:00Z,2019-05-20T00:06:08Z,,,
591,b'What is the use of [SEP]?',2019-05-07T04:12:16Z,2019-05-07T05:12:07Z,,,
590,b'Fix for computing t_total in examples',2019-05-06T15:27:41Z,2019-05-13T15:47:00Z,,,
589,"b""Can't save converted checkpoint""",2019-05-06T13:34:01Z,2019-07-12T14:50:30Z,wontfix,,
588,b'installation error ',2019-05-06T12:31:47Z,2019-05-10T09:51:47Z,,,
587,b'From which layer is fine tuning starting in BERT?',2019-05-06T08:37:21Z,2019-07-24T14:21:04Z,wontfix,,
586,b'Padding Token in Transformer XL',2019-05-06T06:52:29Z,2019-07-12T09:08:25Z,wontfix,,
585,b'Make the epsilon of LayerNorm configurable.',2019-05-05T16:28:31Z,2019-05-08T14:56:39Z,,,
584,b'The number of train examples in STS-B is only 5749',2019-05-04T22:47:35Z,2019-07-10T23:46:51Z,wontfix,,
583,b'BERT + PyTorch + XLA',2019-05-04T05:49:35Z,2019-07-12T08:08:26Z,wontfix,,
582,b'Add GPT-2 Bigger Model',2019-05-04T00:00:49Z,2019-07-16T18:17:11Z,wontfix,,
581,b'BertAdam gradient clipping is not global',2019-05-03T20:56:14Z,2019-07-09T21:29:42Z,wontfix,,
580,b'Bert for passage reranking',2019-05-03T17:22:30Z,2019-05-07T16:06:22Z,,AttributeError,"AttributeError: 'BertForPreTraining' object has no attribute 'bias'"
579,b'Resetting current_random_doc and current_doc',2019-05-03T17:03:20Z,2019-07-12T08:08:27Z,wontfix,,
578,"b'""Easy"" path for classifier training / pre-training'",2019-05-03T13:16:00Z,2019-07-21T14:11:08Z,"Discussion, wontfix",,
577,b'GPT2 lm_labels masking using (-1) throws an index out of range',2019-05-03T13:05:39Z,2019-08-20T22:44:43Z,wontfix,RuntimeError,"RuntimeError: index out of range at /Users/soumith/mc3build/conda-bld/pytorch_1549593514549/work/aten/src/TH/generic/THTensorEvenMoreMath.cpp:191"
576,"b'key error when using run_classifier.py in predict mode, expecting label?'",2019-05-03T11:19:02Z,2019-07-09T12:29:42Z,wontfix,,
575,b'Different BERT representations when text is with and without single quotes',2019-05-03T09:30:37Z,2019-05-03T19:41:22Z,,,
574,b'understanding of the output from TransfoXLModel ',2019-05-02T17:37:39Z,2019-07-08T18:23:21Z,wontfix,,
573,"b""GPT2 doesn't accept inputs of varying tokens length (despite the padding at the end)""",2019-05-02T17:05:16Z,2019-10-13T22:00:30Z,wontfix,RuntimeError,"RuntimeError: index out of range at /Users/soumith/mc3build/conda-bld/pytorch_1549593514549/work/aten/src/TH/generic/THTensorEvenMoreMath.cpp:191**"
572,b'BERT pre-training using only domain specific text',2019-05-02T11:39:02Z,2019-07-08T12:23:23Z,wontfix,,
571,b'Fix documentation typo',2019-05-02T11:26:17Z,2019-05-08T14:06:14Z,,,
570,b'Create optimizer only when args.do_train is True',2019-05-02T11:14:44Z,2019-05-08T14:07:51Z,,,
569,b'License of the pretrained models',2019-05-01T23:14:21Z,2019-05-02T18:39:25Z,,,
568,b'Fine-tuning Bert',2019-05-01T16:34:17Z,2019-08-09T03:58:01Z,wontfix,,
567,b'about pytorch 1.1.0 rerlease',2019-05-01T09:48:27Z,2019-07-08T12:23:21Z,wontfix,,
566,b'Bug in run_classifier.py fp16 learning rate',2019-05-01T07:01:53Z,2019-05-10T11:48:13Z,,,
565,b'Results of Fine-tuned model changes in every run',2019-05-01T02:52:00Z,2019-07-07T05:04:23Z,wontfix,,
564,b'Fix #537',2019-05-01T02:48:38Z,2019-05-01T09:18:47Z,,,
563,b'performance does not change but loss decrease',2019-04-30T22:11:46Z,2019-07-07T00:04:23Z,wontfix,,
562,b'Small fix to remove shifting of lm labels during pre process of RocStories.',2019-04-30T20:56:45Z,2019-05-01T09:20:17Z,,,
561,b'Training Transformer XL from scratch',2019-04-30T20:30:27Z,2019-07-07T10:37:33Z,wontfix,,
560,"b'Improvements to GPT-2 (special_tokens, fine-tuning, medium model) + repo code coverage metric'",2019-04-30T09:06:36Z,2019-05-13T14:29:27Z,,,
559,b'the size of words and the size of lables do not match',2019-04-30T05:01:05Z,2019-07-06T09:04:23Z,wontfix,,
558,b'can one run squad using gpt2?',2019-04-29T21:15:43Z,2019-07-05T22:18:14Z,wontfix,,
557,b'Expanding vocab size for GTP2 pre-trained model.',2019-04-29T19:32:02Z,2019-07-06T18:04:23Z,wontfix,RuntimeError,"RuntimeError: index out of range at /Users/soumith/mc3build/conda-bld/pytorch_1549593514549/work/aten/src/TH/generic/THTensorEvenMoreMath.cpp:191"
556,"b""Training beyond specified 't_total' steps with schedule 'warmup_linear'. Learning rate set to 0.0. Please set 't_total' of BertAdam correctly.""",2019-04-29T17:53:52Z,2019-07-27T11:44:29Z,wontfix,,
555,b'Transformer XL from Pytorch model',2019-04-29T12:47:17Z,2019-07-05T13:18:13Z,wontfix,,
554,"b'ValueError: For training, each question should have exactly 1 answer.'",2019-04-28T21:28:03Z,2019-07-22T13:11:04Z,wontfix,,
553,b'How to get back input and predictions as string',2019-04-28T20:04:47Z,2019-07-04T21:54:23Z,wontfix,,
552,b'should loss_scale be multiplied to the loss explicitly?',2019-04-28T09:25:38Z,2019-07-04T13:54:23Z,wontfix,,
551,b'Pad inputs to multiple of 8',2019-04-28T08:25:06Z,2019-07-04T09:54:30Z,wontfix,,
550,b'Fix GPT2 crash on special quotes in Python 3',2019-04-28T05:25:20Z,2019-05-01T09:21:15Z,,,
549,b'CUDA out of memory issue when training ',2019-04-28T02:51:37Z,2019-07-04T09:54:29Z,wontfix,,
548,b'how to ensemble different checkpoints?',2019-04-27T23:19:03Z,2019-07-04T00:21:53Z,wontfix,,
547,b'How to get masked word prediction probabilities',2019-04-27T22:54:10Z,2019-10-09T16:29:28Z,wontfix,,
546,b'Import Error',2019-04-27T21:38:46Z,2019-07-21T10:49:51Z,wontfix,,
545,b'move pytroch_pretrained_bert cache folder under same path as torch',2019-04-27T18:00:36Z,2019-05-08T14:55:28Z,,,
544,"b""TypeError: '<' not supported between instances of 'NoneType' and 'int'""",2019-04-26T17:33:37Z,2019-07-05T08:18:16Z,wontfix,TypeError,"TypeError: '<' not supported between instances of 'NoneType' and 'int'"
543,b'How to train our own domain-specific data instead of using pre-training models?',2019-04-26T16:01:00Z,2019-07-16T09:35:30Z,wontfix,,
542,b'Clarifying attention mask',2019-04-26T14:32:15Z,2019-05-01T14:52:49Z,,,
541,b'Any way to reduce the model size to <250mb?',2019-04-26T08:19:28Z,2019-07-02T17:04:39Z,wontfix,,
540,b'no to_json_file(file) in BERT',2019-04-26T08:05:27Z,2019-05-02T03:00:02Z,,,
539,"b""Can we use 'bert-base-uncased' to question_answer just for start, rather rather than run_squad pretraining?""",2019-04-26T07:15:29Z,2019-04-27T08:03:55Z,,,
538,b'key error in BertQuestionAsnwering predict?',2019-04-26T06:58:11Z,2019-04-26T07:06:41Z,,KeyError,KeyError: 1000000088
537,b'New GPT2 tokenizer no longer encodes Unicode characters properly in Python 3',2019-04-26T05:24:02Z,2019-10-18T15:11:06Z,wontfix,,
536,b'Fix missing warmup_linear in run_classifier.py example',2019-04-25T18:57:45Z,2019-04-25T18:58:58Z,,,
535,b'gpt2 fine tuning sources',2019-04-25T18:21:04Z,2019-10-18T03:11:06Z,wontfix,,
534,b'How many datasets does Bert use in pretraining process?',2019-04-25T16:15:58Z,2019-07-08T12:23:22Z,"Discussion, wontfix",,
533,b'Docs for new learning rate code',2019-04-25T14:16:52Z,2019-04-25T19:02:36Z,,,
532,b'[Feature request] Support configurable BertLayerNorm epsilon',2019-04-25T14:06:52Z,2019-04-25T19:12:49Z,,,
531,b'fixed new LR API in examples',2019-04-25T12:41:44Z,2019-04-25T19:01:19Z,,,
530,b'GPT2 training and generating on text longer than 1024',2019-04-25T00:40:51Z,2019-07-01T02:02:54Z,wontfix,,
529,"b""Why classifier fine-tuning don't save best model based on the evaluation on dev dataset""",2019-04-24T13:17:15Z,2019-06-24T01:51:14Z,wontfix,,
528,"b""__init__() got an unexpected keyword argument 'do_basic_tokenize'""",2019-04-24T12:54:48Z,2019-06-30T16:02:54Z,"Need more information, wontfix",,
527,b'Update example files so that tr_loss is not affected by args.gradient\xe2\x80\xa6',2019-04-24T12:09:45Z,2019-04-30T09:12:55Z,,,
526,b'Will BERT weights for SQuAD be released?',2019-04-24T08:23:43Z,2019-04-24T08:36:11Z,,,
525,b'Should I use weight_decay or weight_decay_rate?',2019-04-24T06:11:06Z,2019-06-30T08:02:54Z,wontfix,,
524,b'Mixed up isNextSentence label in simple_lm_finetuning.py script? ',2019-04-23T17:38:44Z,2019-04-24T20:32:18Z,"Need more information, Discussion",,
523,"b""ImportError: cannot import name 'WEIGHTS_NAME' from 'pytorch_pretrained_bert.file_utils'""",2019-04-23T13:05:37Z,2019-04-25T19:50:58Z,,ImportError,"ImportError: cannot import name 'WEIGHTS_NAME' from 'pytorch_pretrained_bert.file_utils' (/mnt/Data/miniconda3/lib/python3.7/site-packages/pytorch_pretrained_bert/file_utils.py)"
522,b'extending of Transformer-XL for new tasks ',2019-04-23T11:28:26Z,2019-06-30T13:02:54Z,wontfix,,
521,"b""Model type in convert_tf_checkpoint_to_pytorch and 'squad' mapping""",2019-04-23T11:22:15Z,2019-10-07T18:54:27Z,wontfix,,
520,"b'unable to load finetuned LM ""No file bert_config.json""'",2019-04-23T10:37:42Z,2019-04-25T19:51:50Z,,,
519,b'No GPT2 model',2019-04-23T10:17:05Z,2019-04-23T11:19:12Z,Need more information,,
518,b'Fix training schedules in examples to match new API',2019-04-23T09:18:42Z,2019-04-25T19:01:04Z,,,
517,b'More SEPs',2019-04-23T03:51:06Z,2019-06-29T09:11:34Z,"Discussion, wontfix",RuntimeError,"RuntimeError: Creating MTGP constants failed. at /opt/conda/conda-bld/pytorch_1535491974311/work/aten/src/THC/THCTensorRandom.cu:34"
516,b'Same loss values but different eval result',2019-04-22T17:19:40Z,2019-06-29T09:11:32Z,"Need more information, Discussion, wontfix",,
515,b'Fix --reduce_memory in finetune_on_pregenerated',2019-04-22T13:04:16Z,2019-04-23T08:30:23Z,,,
514,b'ADD ERNIE',2019-04-22T09:57:29Z,2019-05-13T06:51:38Z,Help wanted,,
513,b'How many epochs are necessary for finetuning BERT?',2019-04-22T06:47:37Z,2019-06-29T09:11:33Z,"Discussion, wontfix",,
512,b'Fix indentation weirdness in GPT-2 example.',2019-04-21T17:22:49Z,2019-04-23T08:29:02Z,,,
511,b'error when trying to use multilingual model for fine tuning',2019-04-21T13:29:39Z,2019-09-14T02:54:19Z,wontfix,AttributeError,"AttributeError: 'NoneType' object has no attribute 'vocab'"
510,b'Adam optimiser not following Pytorch conventions',2019-04-20T23:33:42Z,2019-06-28T16:00:15Z,"Help wanted, wontfix",,
509,b'How to read a checkpoint and continue training?',2019-04-20T14:54:14Z,2019-10-21T19:47:29Z,"Need more information, wontfix",,
508,b'Fix python syntax in examples/run_gpt2.py',2019-04-19T03:32:48Z,2019-04-23T08:52:41Z,,,
507,b'GPT-2 FineTuning on Cloze/ ROC',2019-04-18T23:16:47Z,2019-05-03T21:22:29Z,Discussion,,
506,b'Hubconf',2019-04-17T22:37:40Z,2019-04-24T18:59:22Z,,,
505,b'Generating text with Transformer XL',2019-04-17T21:13:58Z,2019-04-17T21:42:51Z,,,
504,b'Init BertForTokenClassification from from_pretrained',2019-04-17T20:23:25Z,2019-04-24T18:59:21Z,,,
503,b'Fix possible risks of bpe on special tokens',2019-04-17T16:29:10Z,2019-06-17T06:16:59Z,wontfix,,
502,b'How to obtain attention values for each layer ',2019-04-17T10:51:15Z,2019-06-23T13:14:13Z,"Discussion, wontfix",,
501,b'Test a fine-tuned BERT-QA model',2019-04-17T10:10:27Z,2019-04-18T20:04:27Z,Help wanted,,
500,b'Updating network handling',2019-04-17T09:59:31Z,2019-04-17T13:22:15Z,,,
499,b'error when do python3 run_squad.py',2019-04-17T09:29:28Z,2019-04-23T09:21:24Z,Need more information,ImportError,"ImportError: No module named pytorch_pretrained_bert.file_utils"
498,b'Gpt2 tokenization',2019-04-17T08:22:40Z,2019-04-17T09:06:42Z,,,
497,"b""UnboundLocalError: local variable 'special_tokens_file' referenced before assignment""",2019-04-16T23:33:31Z,2019-04-17T09:06:58Z,,UnboundLocalError,"UnboundLocalError: local variable 'special_tokens_file' referenced before assignment"
496,"b'[run_gpt2.py] temperature should be a float, not int'",2019-04-16T22:23:30Z,2019-04-17T09:08:55Z,,,
495,b'Fix gradient overflow issue during attention mask',2019-04-16T18:42:47Z,2019-04-17T09:10:37Z,,,
494,b'Fix indentation for unconditional generation',2019-04-16T18:12:58Z,2019-04-17T09:11:36Z,,,
493,b'how to use extracted features in extract_features.py?',2019-04-16T13:25:02Z,2019-07-12T15:50:30Z,"Discussion, wontfix",,
492,"b""no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']""",2019-04-16T06:03:22Z,2019-04-17T08:16:25Z,,,
491,b'pretrained GPT-2 checkpoint gets only 31% accuracy on Lambada',2019-04-16T02:04:24Z,2019-05-29T23:39:38Z,Discussion,,
490,b'Clean up GPT and GPT-2 losses computation',2019-04-15T14:14:41Z,2019-04-15T14:14:51Z,,,
489,"b'Better serialization for Tokenizer and Config classes (BERT, GPT, GPT-2 and Transformer-XL)'",2019-04-15T10:06:25Z,2019-04-16T06:49:55Z,,,
488,b'fixed BertForMultipleChoice model init and forward pass',2019-04-15T08:40:10Z,2019-04-25T19:04:17Z,,,
487,b'BERT multilingual for zero-shot classification',2019-04-14T11:39:34Z,2019-06-23T09:14:14Z,"Help wanted, Discussion, wontfix",,
486,b'Difference between this repo and bert-as-service',2019-04-13T18:44:59Z,2019-06-23T09:14:14Z,"Discussion, wontfix",,
485,"b""UnboundLocalError: local variable 'i' referenced before assignment when using fine_tuning code""",2019-04-13T09:58:13Z,2019-04-17T07:58:35Z,,UnboundLocalError,UnboundLocalError: local variable 'i' referenced before assignment
484,b'KeyError: in convert_tokens_to_ids()',2019-04-13T09:04:53Z,2019-04-18T20:19:48Z,Need more information,,
483,"b""Perplexity number of wikitext-103 on gpt-2 don't match the paper""",2019-04-12T21:10:39Z,2019-08-31T15:02:23Z,"Discussion, wontfix",,
482,b'Suggestion: exception handling for out-of-vocab in pretrained model',2019-04-12T20:58:40Z,2019-04-17T11:33:10Z,Need more information,,
481,b'BERT does mask-answering or sequence prediction or both???',2019-04-12T18:54:42Z,2019-06-23T10:14:14Z,"Discussion, wontfix",,
480,b'Extend the BertForSequenceClassification docs to mention the special CLS token.',2019-04-12T18:32:46Z,2019-04-15T08:57:25Z,,,
479,b'Using GPT2 to implement GLTR',2019-04-12T18:23:43Z,2019-06-23T15:14:15Z,"Discussion, wontfix",,
478,b'Added a helpful error for users with single-document corpuses',2019-04-12T14:12:44Z,2019-04-15T08:55:58Z,,,
477,b'Getting Sentence level log probabilities using this model',2019-04-12T07:56:56Z,2019-06-23T10:14:14Z,"Discussion, wontfix",,
476,b'cannot run squad script ',2019-04-12T03:37:33Z,2019-09-01T11:02:24Z,"Need more information, wontfix",,
475,b'Non-Determinism Behavior that cannot reproduce result when evaluate on each epoch',2019-04-12T00:59:40Z,2019-04-23T08:51:16Z,"Help wanted, Need more information",,
474,b'Fix tsv read error in Windows',2019-04-11T19:47:52Z,2019-04-15T08:56:49Z,,,
473,b'GPT as a Language Model',2019-04-11T18:02:25Z,2019-07-13T19:22:00Z,wontfix,,
472,b'Compilation terminated',2019-04-11T09:58:42Z,2019-04-11T10:03:03Z,,,
471,b'modeling_openai.py bug report',2019-04-11T07:57:17Z,2019-04-11T09:43:20Z,,,
470,b'how to correctly do classifying?',2019-04-10T18:08:05Z,2019-06-17T15:54:30Z,"Help wanted, Need more information, Discussion, wontfix",,
469,b'Why can`t we just use cached pytorch-model without internet',2019-04-10T17:08:57Z,2019-06-23T16:14:14Z,wontfix,,
468,b'GPT-2 fine tunning',2019-04-10T08:01:01Z,2019-06-29T17:11:34Z,wontfix,,
467,b'Update README.md',2019-04-09T21:46:03Z,2019-04-11T19:53:24Z,,ValueError,"ValueError: Training is currently the only implemented execution option. Please set `do_train`."
466,b'Mismatch in pre-processed wikitext-103 corpus and using pre-trained tokenizer for TransfoXLLMHeadModel',2019-04-09T20:20:15Z,2019-04-16T06:49:58Z,,,
465,b'Errors when using Apex',2019-04-09T15:17:26Z,2019-06-17T15:54:29Z,"Help wanted, Need more information, wontfix",RuntimeError,"RuntimeError: cuda runtime error (77) : an illegal memory access was encountered at /opt/conda/conda-bld/pytorch_1549636813070/work/aten/src/THC/THCGeneral.cpp:405"
464,b'How to get vocab.txt and bert_config.json as output of fine tuning?',2019-04-09T10:03:22Z,2019-04-11T14:44:24Z,,,
463,b'Vocab changes in lm_finetuning in BERT',2019-04-09T00:41:19Z,2019-10-03T04:01:35Z,"Discussion, wontfix",,
462,b'fix run_gpt2.py',2019-04-09T00:24:04Z,2019-04-11T19:54:47Z,,,
461,b'Pooler weights not being updated for Multiple Choice models? ',2019-04-08T22:29:35Z,2019-06-12T17:40:47Z,,,
460,b'run_classifier on CoLA fails with illegal memory access',2019-04-08T18:01:30Z,2019-06-28T18:57:48Z,wontfix,RuntimeError,"RuntimeError: cuda runtime error (700) : an illegal memory access was encountered at /tmp/pip-req-build-k527kqpu/aten/src/ATen/native/cuda/Embedding.cu:340`"
459,b'Question about BertForQuestionAnswering model',2019-04-08T15:47:53Z,2019-04-09T02:48:50Z,,,
458,b'Suggestion: add warning when using BertForSequenceClassification without special [CLS] token',2019-04-08T11:35:06Z,2019-04-16T11:07:02Z,Discussion,,
457,b'Load Biobert pre-trained weights into Bert model with Pytorch bert hugging face run_classifier.py code',2019-04-08T07:08:40Z,2019-06-17T14:54:29Z,"Discussion, wontfix",'UnicodeDecodeError,"'UnicodeDecodeError: 'utf-8' codec can't decode byte 0x8b in position 1: invalid start byte' "
456,b'max_seq_length for squad',2019-04-08T06:08:01Z,2019-04-11T14:30:33Z,,,
455,b'LM fine tuning on top of a custom model',2019-04-07T07:58:04Z,2019-04-11T14:58:08Z,Discussion,,
454,b'getting sequence embeddings for pair of sentences',2019-04-05T13:28:40Z,2019-04-11T14:26:46Z,,,
453,b'What\xe2\x80\x98s op-for-op meaning?',2019-04-05T09:07:31Z,2019-04-11T14:26:12Z,,,
452,b'Pregenerating data requires multiple documents',2019-04-05T08:14:02Z,2019-06-18T15:21:05Z,"Discussion, wontfix",,
451,b'Help: cannot load pretrain models from .pytorch_pretrained_bert folder',2019-04-04T19:11:46Z,2019-07-08T14:23:21Z,wontfix,,
450,b'Understanding pre-training and fine-tuning',2019-04-04T13:55:47Z,2019-06-17T14:54:30Z,"Discussion, wontfix",,
449,b'Convert_tf_checkpoint_to_pytorch for bert-joint-baseline',2019-04-04T10:05:52Z,2019-06-18T06:54:29Z,"Help wanted, Discussion, wontfix",,
448,b'pretrain for chinese dataset',2019-04-04T08:26:30Z,2019-04-11T13:47:07Z,,,
447,b'Dynamic max_seq_length implementation?',2019-04-03T23:07:41Z,2019-04-11T13:45:07Z,,,
446,"b""How to select a certain layer as token's representation?""",2019-04-03T20:57:32Z,2019-04-11T13:43:34Z,Discussion,,
445,b'Learning rate schedules improvement + extension',2019-04-03T15:36:29Z,2019-04-23T08:27:39Z,,,
444,b'if crf needed when do ner?  ',2019-04-03T08:49:24Z,2019-06-09T10:47:44Z,"Discussion, wontfix",,
443,b'How do you train custom corpus with bert?',2019-04-03T05:24:05Z,2019-06-09T13:47:42Z,"Need more information, wontfix",,
442,b'Unable to incrementally train BERT with custom training',2019-04-02T20:07:32Z,2019-06-09T10:47:42Z,"Discussion, wontfix",,
441,b'Fix bug in run_squad.py',2019-04-02T18:10:29Z,2019-05-02T09:03:58Z,,,
440,b'How can i use bert for finding word embeddings',2019-04-02T16:18:34Z,2019-06-22T14:07:43Z,"Discussion, wontfix",,
439,b'DistributedDataParallel Not Working',2019-04-02T16:18:27Z,2019-06-09T10:47:43Z,"Need more information, wontfix",,
438,"b""convert_tf_checkpoint_to_pytorch 'BertPreTrainingHeads' object has no attribute 'squad'""",2019-04-02T12:31:25Z,2019-04-03T09:01:12Z,,,
437,b'Fix links in README',2019-04-02T09:24:16Z,2019-04-02T09:40:47Z,,,
436,"b""BertTokenizer.from_pretrained('bert-base-multilingual-cased') does not recognize Korean""",2019-04-01T22:47:06Z,2019-04-03T09:01:12Z,,,
435,b'Fixes to the TensorFlow conversion tool',2019-04-01T19:20:50Z,2019-04-02T08:41:41Z,,,
434,b'Model not training at all in Google Colab',2019-04-01T14:39:11Z,2019-06-09T09:47:43Z,"Help wanted, Discussion, wontfix",,
433,b'how to do the pre training the model form scratch?',2019-04-01T13:57:23Z,2019-04-03T07:59:14Z,,,
432,b'Predictions from BertForSequenceClassification model keep changing across runs',2019-04-01T11:40:58Z,2019-06-13T13:31:03Z,"Need more information, wontfix",,
431,b'How to fine tune Transformer-XL on own dataset?',2019-04-01T11:11:52Z,2019-07-14T20:42:27Z,"Discussion, wontfix",,
430,b'Fix typo in example code',2019-03-30T17:21:39Z,2019-04-02T08:41:56Z,,,
429,b'GPT2Tokenizer <|endoftext|>',2019-03-30T14:56:28Z,2019-04-03T17:55:52Z,Discussion,,
428,b'Cannot find Synthetic self-training in this repository.',2019-03-30T11:12:04Z,2019-04-03T07:42:52Z,,,
427,b'fix sample_doc',2019-03-30T05:58:09Z,2019-04-03T09:26:59Z,,,
426,b'instantiate loss_fct once',2019-03-29T13:26:34Z,2019-04-03T09:24:28Z,,,
425,"b""fix lm_finetuning's link""",2019-03-29T08:09:46Z,2019-03-29T08:14:11Z,,,
424,b'Difference between base and large tokenizer?',2019-03-28T18:41:19Z,2019-06-14T09:31:09Z,"Discussion, wontfix",,
423,b'making unconditional generation work',2019-03-28T17:18:40Z,2019-04-15T09:01:54Z,,,
422,"b'BertForTokenClassification for NER, mask labels '",2019-03-28T17:11:21Z,2019-04-03T07:38:31Z,,,
421,b'pytorch model to tensorflow checkpoint',2019-03-28T15:13:52Z,2019-06-09T09:47:44Z,wontfix,,
420,b'Advantage of BertAdam over Adam?',2019-03-28T15:03:16Z,2019-04-03T10:44:50Z,Discussion,,
419,b'bug in examples/run_squad.py line 88 & 90',2019-03-28T09:08:20Z,2019-04-03T07:25:41Z,,,
418,b'can I fine-tuning pretrained gpt2 model on my corpus?',2019-03-28T07:58:42Z,2019-03-30T06:09:13Z,,,
417,b'Is there any pre-training example code?',2019-03-28T06:54:15Z,2019-03-28T07:59:04Z,,,
416,b'Distributed Training Gets Stuck',2019-03-28T00:40:50Z,2019-06-09T08:40:38Z,"Help wanted, Need more information, wontfix",,
415,"b'For sequence classification, is this model using the wrong token?'",2019-03-27T19:32:56Z,2019-03-28T08:10:32Z,,,
414,b'Help with implementing strides into features for multi-label classifier',2019-03-27T06:36:34Z,2019-06-02T12:49:47Z,"Help wanted, Discussion, wontfix",,
413,b'Bert Pretrained model has no modules nor parameters',2019-03-26T21:51:14Z,2019-03-27T00:33:41Z,,,
412,"b'Possible error in ""pytorch-pretrained-BERT/examples/run_gpt2.py"" unconditional'",2019-03-26T21:45:31Z,2019-04-16T06:45:31Z,,,
411,b'Why average the loss when training on multi-GPUs',2019-03-26T13:23:18Z,2019-03-27T11:48:20Z,Discussion,,
410,b'something wrong in example',2019-03-26T13:15:19Z,2019-06-03T08:37:46Z,"Need more information, wontfix",,
409,b'Remove padding_idx from position_embeddings and token_type_embeddings',2019-03-26T13:04:38Z,2019-03-27T11:30:04Z,,,
408,b'slow training speed even 20 steps',2019-03-26T09:05:07Z,2019-06-02T12:49:48Z,wontfix,,
407,b'AllenNLP TransformerXL',2019-03-25T23:33:58Z,2019-06-02T12:49:45Z,"Discussion, wontfix",,
406,b'error when trying to get embeddings after fine tuning',2019-03-25T18:11:36Z,2019-03-27T09:15:49Z,,RuntimeError,"RuntimeError: Error(s) in loading state_dict for BertEmbeddings:"
405,b'embeddings after fine tuning',2019-03-25T17:46:58Z,2019-07-14T07:22:00Z,"Discussion, wontfix",,
404,b'Fix Language Modeling Loss',2019-03-24T21:20:33Z,2019-04-03T09:35:31Z,,,
403,b'[Question]Embedding Generate Problem',2019-03-24T16:13:03Z,2019-07-28T15:05:06Z,"Help wanted, Discussion, wontfix",,
402,b'gpt2 tokenizer issue with ValueError: chr() arg not in range(256) in Python 2.X',2019-03-24T07:57:09Z,2019-06-02T12:49:46Z,"Help wanted, wontfix",,
401,b'How can I generate new text after having fine-tuned BERT on a custom dataset ? ',2019-03-23T16:20:21Z,2019-06-14T09:31:09Z,"Discussion, wontfix",,
400,b'how to freeze bert model and just train a classifier?',2019-03-23T13:56:18Z,2019-03-27T11:37:10Z,,,
399,b'Is the GPT-2 pretrained model language agnostic?',2019-03-22T20:45:26Z,2019-03-25T15:36:55Z,,,
398,b'Multi GPU',2019-03-22T00:41:21Z,2019-03-27T11:13:45Z,,,
397,b'Allow do_lower_case regardless of do_basic_tokenize',2019-03-21T14:56:09Z,2020-06-14T11:22:55Z,"Help wanted, wontfix",,
396,b' add tqdm to the process of eval in examples/run_swag.py',2019-03-21T13:04:31Z,2019-03-27T11:03:26Z,,,
395,"b""AttributeError: 'BertOnlyMLMHead' object has no attribute 'seq_relationship'""",2019-03-21T05:57:02Z,2019-06-02T12:49:43Z,"Need more information, wontfix",AttributeError,"AttributeError: 'BertOnlyMLMHead' object has no attribute 'seq_relationship'"
394,b'Minor change in README',2019-03-21T05:04:13Z,2019-03-27T11:03:00Z,,,
393,"b""AttributeError: 'BertForPreTraining' object has no attribute 'shape'""",2019-03-20T23:48:16Z,2019-10-15T03:11:53Z,"Need more information, wontfix",AttributeError,"AttributeError: 'BertForPreTraining' object has no attribute 'shape'"
392,b'Add full language model fine-tuning',2019-03-20T17:48:29Z,2019-03-27T11:02:37Z,,,
391,b'Reproduce the results on CoLA ',2019-03-20T04:22:10Z,2019-06-11T03:40:19Z,"Discussion, wontfix",,
390,"b""'NoneType' object with constructor""",2019-03-19T13:16:16Z,2019-04-27T07:24:25Z,,,
389,b'Fix cosine schedule',2019-03-18T14:18:05Z,2019-04-03T09:21:44Z,,,
388,"b""Added remaining GLUE tasks to 'run_classifier.py'""",2019-03-17T12:38:27Z,2019-03-28T08:06:53Z,,,
387,b'run_squad.py cannot predict only',2019-03-17T09:01:40Z,2019-05-25T10:39:15Z,"Help wanted, wontfix",,
386,b'Shared tokenizer interface',2019-03-17T05:51:57Z,2019-06-23T22:14:11Z,wontfix,,
385,b'pre-training a BERT from scratch',2019-03-17T00:45:26Z,2019-09-28T22:00:08Z,"Discussion, wontfix",,
384,b'Incrementally Train BERT with minimum QnA records - to get improved results',2019-03-16T10:54:15Z,2019-05-25T10:39:14Z,"Discussion, wontfix",,
383,b'pull from original',2019-03-16T03:04:18Z,2019-03-16T22:33:43Z,,,
382,b'fp16 overflow in GPT-2',2019-03-15T18:16:07Z,2019-04-17T09:11:10Z,,,
381,b'Added missing imports.',2019-03-15T11:50:18Z,2019-03-15T11:54:41Z,,,
380,b'typo in annotation',2019-03-14T09:32:56Z,2019-03-14T14:56:41Z,,,
379,b'typo',2019-03-14T09:03:52Z,2019-03-14T09:17:19Z,,,
378,"b'Add absolute imports to GPT, GPT-2, Transfo-XL and and fix empty nbest_predictions.json'",2019-03-14T08:57:56Z,2019-03-14T09:00:48Z,,,
377,b'Empty nbest_predictions.json for run_squad.py',2019-03-13T21:15:05Z,2019-03-14T09:00:48Z,,,
376,b'run_lm_finetuning generates short training cases',2019-03-13T16:05:30Z,2019-03-27T12:01:26Z,,,
375,b'How to input the fine-tuned model?',2019-03-13T15:16:41Z,2019-06-12T20:12:02Z,"Help wanted, wontfix",,
374,b'handle ImportError exception when used from projects outside',2019-03-13T12:56:00Z,2019-03-14T09:00:48Z,,,
373,b'performance degraded when using paddings between queries and contexts.',2019-03-13T08:14:02Z,2019-04-03T02:12:03Z,Discussion,,
372,"b'a single sentence classification task, should the max length of sentence limited to half of 512, that is to say 256'",2019-03-13T01:47:10Z,2019-07-03T17:21:53Z,"Discussion, wontfix",,
371,"b'Simplify code, delete redundancy line'",2019-03-13T01:45:46Z,2019-03-14T08:03:33Z,,,
370,b'What is Synthetic Self-Training?',2019-03-12T20:40:50Z,2019-06-05T08:50:40Z,"Discussion, wontfix",,
369,b'BertForQuestionAnswering: How to split  output between query hidden state and context hidden state',2019-03-12T18:46:25Z,2019-06-14T09:31:10Z,"Help wanted, Discussion, wontfix",,
368,"b'When i fine tune the BERT on my serve, it always says Segmentation fault? '",2019-03-12T09:24:10Z,2019-07-17T10:07:46Z,"Need more information, wontfix",,
367,b'how does the run_squad.py deal with non-answerable questions ',2019-03-12T07:38:40Z,2019-03-14T02:22:20Z,"Help wanted, Discussion",,
366,b'Vocabularly file not available for Squad predictions',2019-03-11T21:21:06Z,2019-05-18T14:44:35Z,"Help wanted, wontfix",,
365,b'BERT accuracy reduced after providing custom training..The answer is also not correct',2019-03-11T13:36:58Z,2019-05-18T14:44:34Z,"Help wanted, Need more information, wontfix",,
364,b'Potential redundancy in run_classifier.py example script',2019-03-11T04:44:38Z,2019-05-18T14:44:36Z,"Discussion, wontfix",,
363,"b'Separator token for custom QA input (multi paragraph, longer than 512)'",2019-03-10T02:56:29Z,2019-05-17T09:11:31Z,"Discussion, wontfix",,
362,b'Make the hyperlink of NVIDIA Apex clickable',2019-03-09T14:39:38Z,2019-03-11T08:08:52Z,,,
361,b'Correct line number in README for classes',2019-03-09T00:28:56Z,2019-03-11T08:08:28Z,,,
360,b'Ranking predictions with BertForQuestionAnswering',2019-03-08T17:26:03Z,2019-05-18T09:44:38Z,"Discussion, wontfix",,
359,b'Update run_gpt2.py',2019-03-08T16:59:32Z,2019-03-11T08:07:56Z,,,
358,"b""add 'padding_idx=0' for BertEmbeddings""",2019-03-07T12:03:08Z,2019-03-11T08:06:55Z,,,
357,b'Use Dropout Layer in OpenAIGPTMultipleChoiceHead',2019-03-07T09:15:06Z,2019-03-11T08:06:28Z,,,
356,b'How to add input mask to GPT?',2019-03-06T21:41:43Z,2019-03-07T07:49:18Z,,,
355,b'[Question] Best choice for Sentence Compression model? ',2019-03-06T19:54:37Z,2019-05-13T08:16:41Z,"Discussion, wontfix",,
354,b'Dropout Layer in OpenAIGPTMultipleChoiceHead not used',2019-03-06T15:56:16Z,2019-03-11T08:06:28Z,,,
353,"b""can't load the model""",2019-03-06T14:24:23Z,2019-04-17T13:27:03Z,Need more information,,
352,b'How to incrementally do fine tune train ',2019-03-06T12:11:20Z,2019-05-22T08:43:48Z,"Discussion, wontfix",,
351,b'Little training has no impact',2019-03-06T10:09:43Z,2019-05-17T12:11:31Z,"Help wanted, Need more information, wontfix",,
350,b'Bert Uncased Large giving very low results with SQUAD v1.1 dataset',2019-03-06T09:30:23Z,2019-03-19T04:28:35Z,,,
349,b'Unable to train (fine-tuning) BERT with small training set',2019-03-06T02:44:27Z,2019-05-12T13:18:48Z,"Need more information, wontfix",ZeroDivisionError,"ZeroDivisionError: division by zero_"
348,b'output data',2019-03-05T19:50:04Z,2019-03-06T08:49:06Z,,,
347,b'Processor for SST-2 task',2019-03-05T19:39:29Z,2019-03-06T08:48:27Z,,,
346,b'MRPC Score Lower than Expected',2019-03-05T18:25:44Z,2019-03-05T19:21:07Z,,,
345,"b'Not able to import RandomSampler, Getting error ""ImportError: cannot import name \'RandomSampler\'""?'",2019-03-05T09:24:19Z,2019-03-06T06:26:18Z,,,
344,b'BertEmbedding not initialized with `padding_idx=0`',2019-03-05T06:23:48Z,2019-03-11T08:07:41Z,,,
343,b'Tokenizer defaults lowercase even when bert_model is cased',2019-03-05T04:11:14Z,2019-05-12T13:18:47Z,wontfix,,
342,b'Usage example needs [CLS] and [SEP] added post-tokenization',2019-03-05T00:00:37Z,2019-03-05T19:05:47Z,,,
341,b'catch exception if pathlib not install',2019-03-04T22:31:41Z,2019-03-06T08:48:02Z,,,
340,b'optimizer.zero_grad() in run_openai_gpt.py?',2019-03-03T23:51:47Z,2019-03-06T10:44:01Z,,,
339,b'Why the weights are not intialized ?',2019-03-03T06:14:33Z,2019-03-06T09:46:24Z,,,
338,b'Fix top k generation for k != 0',2019-03-03T05:57:09Z,2019-03-06T08:47:33Z,,,
337,b'Allow tokenization of sequences > 512 for caching',2019-03-03T00:30:43Z,2019-03-06T08:45:50Z,,,
336,b'F1 and EM scores output for run_squad.py',2019-03-02T22:42:27Z,2019-03-09T20:57:26Z,Help wanted,,
335,b'Feature Request: GPT2 fine tuning',2019-03-01T17:05:14Z,2019-03-06T09:43:41Z,,,
334,b'pip install [--editable] . ---> Error',2019-03-01T08:45:48Z,2019-03-06T09:42:17Z,,pip._vendor.pyparsing.ParseException,"pip._vendor.pyparsing.ParseException: Expected stringEnd (at char 11), (line:1, col:12)"
333,b'Add lm and next sentence accuracy for run_lm_finetuning example',2019-03-01T08:34:15Z,2019-03-06T09:41:27Z,,,
332,b'Train with custom data on bert question answering',2019-02-28T08:42:09Z,2019-12-13T13:06:56Z,"Help wanted, wontfix",,
331,b'Can BERT do the next-word-predict task? As it is bidirectional.',2019-02-28T08:36:02Z,2019-03-01T02:05:43Z,,,
330,b'Can we fine tune our model on Chinese corpus',2019-02-28T06:43:35Z,2019-03-06T09:40:43Z,,,
329,b'run_lm_finetuning - ZeroDivisionError',2019-02-28T05:05:55Z,2019-05-12T14:18:40Z,"Need more information, wontfix",,
328,b'PyTorch Huggingface BERT-NLP for Named Entity Recognition',2019-02-27T20:57:13Z,2019-07-16T09:51:24Z,Need more information,,
327,b'Issue#324: warmup linear fixes',2019-02-27T18:10:13Z,2019-03-06T08:44:57Z,,,
326,b'run_classifier with evaluation job only',2019-02-27T04:24:42Z,2019-03-06T09:29:50Z,,UnboundLocalError,"UnboundLocalError: local variable 'tr_loss' referenced before assignment"
325,b'add BertTokenizer flag to skip basic tokenization',2019-02-27T04:20:13Z,2019-03-06T08:37:12Z,,,
324,b'warmup_linear for BertAdam and OpenAIAdam',2019-02-26T16:14:45Z,2019-03-06T09:28:38Z,,,
323,b'What should be the label of sub-word units in Token Classification with Bert',2019-02-26T16:06:34Z,2019-03-06T09:28:10Z,,,
322,b'Single sentence corpus in run_lm_finetuning?',2019-02-26T12:53:33Z,2019-03-06T09:26:53Z,,,
321,b'how to load classification model and predict?',2019-02-26T12:18:20Z,2019-03-06T09:26:00Z,,,
320,b'what is the batch size we can use for SQUAD task?',2019-02-26T08:56:20Z,2019-03-03T00:21:19Z,,,
319,"b""run_classifier.py: TypeError: __init__() got an unexpected keyword argument 'cache_dir'""",2019-02-26T00:02:34Z,2019-02-26T20:52:30Z,,TypeError,"TypeError: __init__() got an unexpected keyword argument 'cache_dir'"
318,b'TransfoXLLMHeadModel output interpretation',2019-02-24T06:52:50Z,2019-03-06T09:25:23Z,,,
317,b'anyone notice large difference of using fp16 ?',2019-02-23T17:49:23Z,2019-02-23T22:56:11Z,,,
316,b'update documentation for gpt-2',2019-02-22T23:41:10Z,2019-02-24T08:38:30Z,,,
315,"b""run_classifier.py :  TypeError: join() argument must be str or bytes, not 'PosixPath'""",2019-02-22T21:40:50Z,2019-03-06T09:20:02Z,,TypeError,"TypeError: join() argument must be str or bytes, not 'PosixPath'"
314,b'Issue with apex import on MAC ',2019-02-22T07:13:33Z,2019-02-22T17:18:09Z,,AttributeError,"AttributeError: dlsym(RTLD_DEFAULT, THCudaHalfTensor_normall): symbol not found"
313,b'run_lm_finetuning',2019-02-22T04:42:39Z,2019-05-12T13:18:46Z,"Need more information, wontfix",,
312,b'Problems converting TF BioBERT model to PyTorch',2019-02-22T01:47:02Z,2019-02-26T00:24:17Z,,,
311,"b""Shouldn't GPT2 use Linear instead of Conv1D?""",2019-02-21T11:09:22Z,2019-03-06T09:16:59Z,,,
310,"b""Few small nits in GPT-2's README code examples""",2019-02-21T09:16:38Z,2019-02-21T09:23:28Z,,,
309,"b'Tests error: Issue with python3 compatibility, on zope interface implementation'",2019-02-21T08:41:35Z,2019-03-07T03:07:16Z,Help wanted,,
308,b'It seems the eval speed of transformer-xl is not faster than bert-base-uncased.',2019-02-21T04:22:02Z,2019-02-27T03:31:13Z,,,
307,b'Update README.md',2019-02-21T03:25:41Z,2019-02-21T08:25:19Z,,,
306,b'Issue happens while using convert_tf_checkpoint_to_pytorch  ',2019-02-21T02:33:50Z,2019-07-22T17:14:13Z,wontfix,AttributeError,"AttributeError: 'BertForPreTraining' object has no attribute 'global_step'"
305,b'Update run_openai_gpt.py',2019-02-20T18:59:49Z,2019-02-20T20:24:07Z,,,
304,b'Can I do a code reference in implementing my code?',2019-02-20T18:24:41Z,2019-02-21T08:45:41Z,,,
303,b'Example Code in README fails.',2019-02-20T14:57:26Z,2019-02-20T15:43:19Z,,,
302,b'typo',2019-02-20T13:13:12Z,2019-02-20T13:14:03Z,,,
301,b'`train_dataset` and `eval_dataset` in run_openai_gpt.py',2019-02-20T03:03:52Z,2019-02-20T07:57:14Z,,FileNotFoundError,"FileNotFoundError: [Errno 2] No such file or directory: ''"
300,b'bert.pooler.dense initialization ',2019-02-20T02:56:09Z,2019-02-20T07:56:09Z,,,
299,b'Tests failure',2019-02-20T01:11:52Z,2019-03-06T09:11:51Z,,,
298,b'Transformer-XL: Convert lm1b model to PyTorch',2019-02-19T22:49:57Z,2019-05-12T13:18:45Z,"Help wanted, wontfix",tensorflow.python.framework.errors_impl.DataLossError,"tensorflow.python.framework.errors_impl.DataLossError: Unable to open table file /mnt/transformer-xl/tf/sota/pretrained_xl/tf_lm1b/model/checkpoint: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator?"
297,b'Sudden catastrophic classification output during NER training',2019-02-19T22:09:33Z,2019-02-21T14:48:27Z,,,
296,b'How to change config parameters when loading the model with `from_pretrained`',2019-02-19T18:27:01Z,2019-02-20T02:43:02Z,,,
295,b'fix broken link in readme',2019-02-19T12:35:19Z,2019-02-19T13:00:29Z,,,
294,b'Extract Features for GPT2 and Transformer-XL',2019-02-19T10:36:26Z,2019-02-20T07:59:58Z,,,
293,b'Minor README typos corrected',2019-02-18T20:29:09Z,2019-02-19T08:00:02Z,,,
292,b'Fix typo in `GPT2Model` code sample',2019-02-18T17:28:31Z,2019-02-18T20:07:40Z,,,
291,b'Too much info @ stdout',2019-02-18T16:30:54Z,2019-03-06T09:05:27Z,,,
290,b'Typo/formatting fixes in README',2019-02-18T11:30:27Z,2019-02-18T20:25:46Z,,,
289,b'HugginFace or HuggingFace?',2019-02-18T11:27:20Z,2019-03-06T09:02:34Z,,,
288,b'forgot to add regex to requirements.txt :(',2019-02-18T10:56:21Z,2019-02-18T11:00:12Z,,,
287,b'Gpt2',2019-02-18T10:12:43Z,2019-02-18T10:38:06Z,,,
286,b'Update activation function docstring',2019-02-16T20:18:08Z,2019-02-17T14:30:47Z,,,
285,b'Anyone tried this model to write a next sentence?',2019-02-16T13:45:00Z,2019-05-12T13:18:40Z,"Discussion, wontfix",,
284,"b""Error in Apex's FusedLayerNorm""",2019-02-15T14:01:48Z,2019-02-20T15:46:50Z,,RuntimeError,"RuntimeError: a Tensor with 2482176 elements cannot be converted to Scalar (item at /pytorch/aten/src/ATen/native/Scalar.cpp:9)"
283,b'unicode',2019-02-15T11:21:08Z,2019-02-16T13:49:30Z,,TypeError,"TypeError: write() argument 1 must be unicode, not str"
282,b'Fix some bug about SQuAD code',2019-02-15T08:01:56Z,2019-02-15T09:06:51Z,,,
281,b'Conversion of gpt-2 small model',2019-02-15T07:52:25Z,2019-02-18T10:43:00Z,,,
280,b'Have you eval the inference speed of transformer-xl?',2019-02-15T03:39:14Z,2019-02-27T03:42:59Z,,,
279,b'DataParallel imbalanced memory usage',2019-02-14T06:06:38Z,2019-03-06T09:01:27Z,,,
278,b'PAD symbols change the output',2019-02-14T00:57:02Z,2019-02-14T07:32:29Z,,,
277,b'80min training time to fine-tune BERT-base on the SQuAD dataset instead of 24min?',2019-02-13T15:48:43Z,2019-03-06T09:00:50Z,,,
276,b'Argument do_lower_case is repeated in run_lm_finetuning.py',2019-02-13T15:29:35Z,2019-02-13T15:33:05Z,,**argparse.ArgumentError,**argparse.ArgumentError: argument --do_lower_case: conflicting option string: --do_lower_case**
275,b'--do_lower_case is duplicated in parser args',2019-02-13T14:30:58Z,2019-02-13T15:32:22Z,,,
274,"b'Help: how to get index/symbol from last_hidden, on text8?'",2019-02-13T09:50:51Z,2019-03-06T09:00:37Z,,,
273,b'Update to fifth release',2019-02-13T09:16:53Z,2019-02-13T09:19:09Z,,,
272,b'Facing issue in Run Fine tune LM',2019-02-13T02:45:55Z,2019-03-06T09:00:20Z,,,
271,b'Transformer-XL: wrong encoding in the vocab',2019-02-13T01:52:38Z,2019-05-12T13:18:43Z,"Help wanted, wontfix",,
270,b'Transformer-XL: hidden states are nan',2019-02-12T21:50:32Z,2019-02-13T09:27:14Z,,,
269,b'Get hidden states from all layers of Transformer-XL?',2019-02-12T16:25:18Z,2019-02-13T09:33:48Z,,,
268,b'fixed a minor bug in README.md',2019-02-12T12:05:01Z,2019-02-13T09:19:26Z,,,
267,b'Missing files for Transformer-XL examples',2019-02-12T09:15:46Z,2019-02-12T09:25:24Z,,,
266,b'Tokenization Incorrect',2019-02-12T06:30:57Z,2019-02-13T09:27:37Z,,,
265,b'Variance Sources',2019-02-11T13:01:38Z,2019-02-12T10:06:10Z,,,
264,"b""RuntimeError: cuda runtime error while running run_classifier.py with 'bert-large-uncased' bert model""",2019-02-11T11:05:22Z,2019-03-06T08:57:51Z,,,
263,b'potential bug in extract_features.py',2019-02-08T22:11:49Z,2019-02-11T15:45:45Z,,,
262,b'speed becomes slow',2019-02-07T19:02:12Z,2019-05-12T13:18:42Z,"Help wanted, Need more information, wontfix",,
261,b'removing unused argument eval_batch_size from LM finetuning #256',2019-02-07T09:08:39Z,2019-02-08T09:36:04Z,,,
260,b'pretrained model(s) in onnx format',2019-02-07T00:55:12Z,2019-07-29T03:38:47Z,"Help wanted, wontfix",,
259,b'please add option to load fine-tuned file to CPU if trained on GPU',2019-02-06T23:16:04Z,2019-02-08T09:37:56Z,,,
258,b'Fix the undefined variable in squad example',2019-02-06T18:36:53Z,2019-02-06T19:33:19Z,,,
257,b'Minor redundancy in model defintion?',2019-02-06T12:44:46Z,2019-03-06T08:56:01Z,,,
256,b'does run_lm_finetuning.py actually use --eval_batch_size?',2019-02-06T01:57:28Z,2019-03-06T08:55:00Z,,,
255,b'Error while using Apex',2019-02-05T23:49:01Z,2019-02-06T08:07:21Z,,ModuleNotFoundError,"ModuleNotFoundError: No module named 'fused_layer_norm_cuda'"
254,b'Python 2',2019-02-05T23:08:41Z,2019-02-11T13:19:27Z,,,
253,b'Merge pull request #1 from huggingface/master',2019-02-05T21:22:07Z,2019-02-05T21:41:14Z,,,
252,b'BERT tuning all parameters? ',2019-02-05T18:39:37Z,2019-02-06T08:07:13Z,,,
251,b'Only keep the active part mof the loss for token classification',2019-02-04T16:48:04Z,2019-02-05T15:33:46Z,,,
250,b'Fix squad answer start and end position',2019-02-03T13:06:05Z,2019-02-05T15:16:20Z,,,
249,b'Fine tuning Bert for Question answering ',2019-02-03T06:08:52Z,2019-02-06T08:07:50Z,,,
248,b'fix prediction on run-squad.py example',2019-02-01T18:25:24Z,2019-02-05T15:00:53Z,,FileNotFoundError,FileNotFoundError: [Errno 2] No such file or directory: '../../BERT_work/Squad/pytorch_model.bin'
247,b'Multilabel classification and diverging loss',2019-02-01T15:48:37Z,2019-02-11T17:03:15Z,,,
246,b'Accurate SQuAD answer start and end position',2019-02-01T07:19:39Z,2019-02-03T12:38:24Z,,,
245,b'can you do a new release + pypi',2019-01-31T19:15:10Z,2019-02-11T15:13:17Z,,,
244,b'Avoid confusion of inplace LM masking',2019-01-31T10:45:58Z,2019-02-01T11:17:50Z,,,
243,b'seems there is a bug in fine tuning language model',2019-01-31T09:37:11Z,2019-01-31T10:37:32Z,,,
242,b'Fix argparse type error',2019-01-30T20:11:10Z,2019-02-01T11:14:56Z,,```TypeError,"```TypeError: %o format: an integer is required, not dict```"
241,"b""Tokenization doesn't seem to match BERT paper""",2019-01-30T18:33:15Z,2019-01-30T23:56:06Z,,,
240,b'Minor update in README',2019-01-30T18:20:29Z,2019-02-01T11:14:05Z,,,
239,b'cannot load BERTAdam when restoring from BioBert',2019-01-30T16:21:09Z,2019-03-06T08:54:27Z,,AttributeError,"AttributeError: 'Parameter' object has no attribute 'BERTAdam'"
238,b'padded positions are ignored when embedding position ids',2019-01-30T06:43:13Z,2019-05-12T13:18:44Z,"Discussion, wontfix",,
237,b'How can I change vocab size for pretrained model?',2019-01-30T05:10:37Z,2019-02-05T16:14:19Z,,RuntimeError,RuntimeError: index out of range at /pytorch/aten/src/TH/generic/THTensorMath.cpp:352
236,b'Preprocessing necessary for lengthier text',2019-01-29T15:56:30Z,2019-01-29T16:03:27Z,,,
235,b'Training BERT behind a proxy server',2019-01-29T13:47:29Z,2019-02-05T16:13:25Z,,requests.exceptions.ConnectionError,"requests.exceptions.ConnectionError: HTTPSConnectionPool(host='s3.amazonaws.com', port=443): Max retries exceeded with url: /models.huggingface.co/bert/bert-large-uncased-vocab.txt (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fbb55377828>: Failed to establish a new connection: [Errno 110] Connection timed out'))"
234,b'Fine tuning for evaluation',2019-01-29T13:36:53Z,2019-03-06T12:18:20Z,,,
233,b'What is get_lr() meaning in the optimizer.py',2019-01-28T13:19:06Z,2019-02-05T16:12:33Z,,,
231,b'Why is the output bias computed separately?',2019-01-27T17:22:32Z,2019-01-28T10:27:25Z,,,
230,b'Cleaning `~/.pytorch_pretrained_bert`',2019-01-26T23:18:42Z,2019-01-28T10:27:04Z,,,
229,"b'Is BERT suitable for seq2seq tasks, such as machine translation? '",2019-01-26T11:01:42Z,2019-01-28T10:28:41Z,,,
228,b'Freezing base transformer weights',2019-01-26T09:09:36Z,2019-01-26T09:45:04Z,,,
227,"b""RuntimeError: Expected object of backend CUDA but got backend CPU for argument #3 'index'""",2019-01-25T14:45:34Z,2019-01-25T15:42:59Z,,RuntimeError,"RuntimeError: Expected object of backend CUDA but got backend CPU for argument #3 'index'"
226,b'Logical error in the run_lm_finetuning?',2019-01-25T11:51:02Z,2019-01-25T14:35:21Z,,,
225,b'max sentence length',2019-01-24T03:16:17Z,2019-01-24T05:11:14Z,,,
224,b'how to add new vocabulary?',2019-01-24T02:42:38Z,2019-01-24T05:13:10Z,,,
223,b'Feat/9',2019-01-24T02:22:07Z,2019-01-24T02:29:34Z,,,
222,b'ConnectionError returned if Internet network is not stable',2019-01-24T02:01:16Z,2019-01-28T10:29:08Z,,,
221,b'Using BERT with custom QA dataset',2019-01-23T10:26:54Z,2019-01-28T10:30:41Z,,,
220,b'Questions Answering Example',2019-01-23T08:19:56Z,2019-01-23T08:32:15Z,,,
219,b'How can I get the confidence score for the classification task',2019-01-23T07:21:51Z,2019-01-23T07:35:25Z,,,
218,b'Fix learning rate problems in run_classifier.py',2019-01-22T22:43:30Z,2019-02-05T14:40:44Z,,,
217,b'Loading fine_tuned BertModel fails due to prefix error',2019-01-22T21:55:45Z,2019-01-24T09:42:14Z,,,
216,b'Training classifier does not work for more than two classes',2019-01-22T18:14:52Z,2019-01-23T13:38:42Z,,RuntimeError,"RuntimeError: CUDA error: device-side assert triggered"
215,b'Loading fine tuned BertForMaskedLM',2019-01-21T17:13:54Z,2019-01-23T22:12:30Z,,,
214,b'SQuAD output layer and the computation loss',2019-01-21T09:34:44Z,2019-01-28T10:31:02Z,,,
213,b'will examples update the parameters of bert model?',2019-01-21T07:04:08Z,2019-02-05T16:10:45Z,,,
212,"b""Pytorch-Bert: Why this command: pip install pytorch-pretrained-bert doesn't work for me""",2019-01-20T09:44:52Z,2019-02-05T16:10:10Z,,,
211,b'How convert pytorch to tf checkpoint?',2019-01-19T16:02:47Z,2019-05-21T14:38:19Z,"Help wanted, wontfix",,
210,"b'error: the following arguments are required: --bert_model, --output_dir'",2019-01-19T15:32:43Z,2019-01-20T04:28:03Z,,,
209,b'Missing softmax in BertForQuestionAnswering after linear layer?',2019-01-19T06:55:30Z,2019-01-19T08:26:35Z,,,
208,b'Merge run_squad.py and run_squad2.py',2019-01-19T02:30:58Z,2019-02-05T15:15:04Z,,,
207,"b""AttributeError: 'NoneType' object has no attribute 'start_logit'""",2019-01-18T20:55:47Z,2019-02-05T16:09:16Z,,AttributeError,"AttributeError: 'NoneType' object has no attribute 'start_logit'"
206,b'Classifier example not training on CoLa data',2019-01-18T15:36:20Z,2019-02-05T16:08:59Z,,,
205,b'What is the meaning of Attention Mask',2019-01-18T14:04:11Z,2019-01-18T22:26:00Z,,,
204,b'Two to Three mask word prediction at the same sentence is very complex',2019-01-18T05:52:40Z,2019-01-22T16:50:03Z,,,
203,"b""Add some new layers from BertModel and then 'grad' error occurs""",2019-01-18T02:19:58Z,2019-01-23T16:34:28Z,,,
202,b'training new BERT seems not working',2019-01-18T00:30:24Z,2019-03-06T12:16:55Z,,,
201,"b""run_squad2 Don't save model if do not train""",2019-01-17T16:43:59Z,2019-01-18T08:28:11Z,,,
200,b'Adding Transformer-XL pre-trained model',2019-01-17T08:38:31Z,2019-02-11T12:36:14Z,,,
199,b'(very) minor update to README',2019-01-16T20:05:53Z,2019-01-16T22:51:53Z,,,
198,"b""HTTPSConnectionPool(host='s3.amazonaws.com', port=443): Max retries exceeded with url: /models.huggingface.co/bert/bert-base-uncased.tar.gz (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x000002456AF21710>: Failed to establish a new connection: [WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond',))""",2019-01-16T06:55:31Z,2019-01-16T08:34:45Z,,,
197,b'seems meet the GPU memory leak problem',2019-01-16T03:05:33Z,2019-01-23T16:12:53Z,,,
196,b'TODO statement on Question/Answering Model',2019-01-15T01:56:48Z,2019-01-16T12:23:14Z,,,
195,b'Potentially redundant learning rate scheduling',2019-01-14T09:10:06Z,2019-02-05T16:07:58Z,,,
194,"b""run_classifier.py doesn't save any configurations and I can't load the trained model. """,2019-01-14T07:16:07Z,2019-01-14T09:19:59Z,,,
193,b'Fix importing unofficial TF models',2019-01-14T04:50:00Z,2019-01-14T08:44:02Z,,,
192,b'Documentation Fixes',2019-01-13T15:56:58Z,2019-01-17T08:41:09Z,,,
191,b'lm_finetuning compatibility with Python 3.5',2019-01-13T13:02:01Z,2019-01-14T08:40:08Z,,,
190,b'Fix documentation (missing backslashes)',2019-01-13T13:00:20Z,2019-01-14T08:39:04Z,,,
189,b'[bug fix] args.do_lower_case is always True',2019-01-13T11:51:18Z,2019-01-14T08:38:38Z,,,
188,b'Weight Decay Fix Original Paper',2019-01-12T20:22:45Z,2019-01-14T01:08:36Z,,,
187,"b'issue is, that ##string will repeats at intermediate, it collapses all index for mask words'",2019-01-11T11:35:06Z,2019-01-14T09:16:36Z,,,
186,b'BertOnlyMLMHead is a duplicate of BertLMPredictionHead',2019-01-11T10:35:36Z,2019-01-14T09:14:56Z,,,
185,"b""got an unexpected keyword argument 'cache_dir'""",2019-01-11T10:08:56Z,2019-01-14T09:11:06Z,,TypeError,"TypeError: __init__() got an unexpected keyword argument 'cache_dir'`"
184,b'Python 3.5 + Torch 1.0 does not work',2019-01-11T09:43:43Z,2019-01-14T09:10:02Z,,,
183,b'Adding OpenAI GPT pre-trained model',2019-01-11T08:03:57Z,2019-02-11T12:35:49Z,,,
182,b'add do_lower_case arg and adjust model saving for lm finetuning.',2019-01-11T07:37:13Z,2019-01-11T07:50:13Z,,,
181,b'All about the training speed in classification job ',2019-01-11T06:27:39Z,2019-01-14T09:09:04Z,,,
180,b'Weights not initialized from pretrained model',2019-01-11T06:03:47Z,2019-01-14T09:05:33Z,,,
179,b'Fix it to run properly even if without `--do_train` param.',2019-01-10T12:51:50Z,2019-01-10T22:39:11Z,,,
178,b'Can we use BERT for Punctuation Prediction?',2019-01-10T07:25:30Z,2019-01-14T09:05:22Z,,,
177,b'run_lm_finetuning.py does not define a do_lower_case argument',2019-01-10T05:01:17Z,2019-01-14T09:04:46Z,,,
176,b'Add [CLS] and [SEP] tokens in Usage',2019-01-09T09:41:58Z,2019-01-10T00:35:07Z,,,
175,"b'RuntimeError: Dimension out of range (expected to be in range of [-1, 0], but got 1)'",2019-01-09T07:26:46Z,2019-01-14T09:15:11Z,,RuntimeError,"RuntimeError: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
174,b'Added Squad 2.0',2019-01-08T23:25:40Z,2019-01-10T22:40:46Z,,,
173,"b""What 's the mlm accuracy of pretrained model? """,2019-01-08T07:08:35Z,2019-01-08T10:07:23Z,,,
172,b'Never split some texts.',2019-01-08T03:02:09Z,2019-01-09T12:44:10Z,,,
171,b'LayerNorm initialization',2019-01-07T07:51:47Z,2019-01-07T11:44:47Z,,,
170,b'How to pretrain my own data with this pytorch code?',2019-01-07T07:22:53Z,2019-01-07T12:29:44Z,,,
169,b'Update modeling.py to fix typo',2019-01-06T22:13:44Z,2019-01-07T11:41:57Z,,,
168,b'Cannot reproduce the result of run_squad 1.1',2019-01-06T06:34:47Z,2019-01-07T12:30:56Z,,,
167,b'Question about hidden layers from pretained model',2019-01-05T07:09:20Z,2019-01-07T12:28:19Z,,,
166,b'Fix error when `bert_model` param is path or url.',2019-01-05T02:43:06Z,2019-01-07T11:40:56Z,,,
165,b'fixed model names in help string',2019-01-04T15:49:46Z,2019-01-07T11:28:21Z,,,
164,b'pretrained model ',2019-01-04T14:20:49Z,2019-01-07T12:28:07Z,,,
163,b'TypeError: Class advice impossible in Python3',2019-01-04T11:23:43Z,2019-01-07T05:48:58Z,,TypeError,TypeError: Class advice impossible in Python3.  Use the @implementer class decorator instead.
162,b'BertTokenizer\xec\x97\x90\xec\x84\x9c do_lower_case\xec\x97\x90 \xea\xb4\x80\xea\xb3\x84\xec\x97\x86\xec\x9d\xb4',2019-01-04T08:03:59Z,2019-01-04T08:05:16Z,,,
161,b'Predict Mode: Weights of BertForQuestionAnswering not initialized from pretrained model',2019-01-03T13:56:02Z,2019-01-07T12:27:12Z,,,
160,b'Weights of BertForQuestionAnswering not initialized from pretrained model',2019-01-03T12:24:32Z,2019-01-03T12:38:58Z,,RuntimeError,"RuntimeError: $ Torch: not enough memory: you tried to allocate 0GB. Buy new RAM! at /pytorch/aten/src/TH/THGeneral.cpp:201"
159,b'Allow do_eval to be used without do_train and to use the pretrained model in the output folder',2019-01-03T11:12:34Z,2019-01-07T11:31:07Z,,,
158,"b""AttributeError: 'BertForPreTraining' object has no attribute 'global_step'""",2019-01-03T10:10:51Z,2019-01-04T09:59:42Z,,AttributeError,"AttributeError: 'BertForPreTraining' object has no attribute 'global_step'"
157,b'Is it feasible to set num_workers>=1 in DataLoader to quickly load data?',2019-01-02T13:54:25Z,2019-01-07T12:25:56Z,,,
156,b'Adding new pretrained model to the help of the `bert_model` argument.',2019-01-02T13:02:27Z,2019-01-07T11:27:17Z,,,
155,b'Why not the mlm use the information of adjacent sentences?',2018-12-30T13:08:53Z,2019-01-07T12:25:24Z,,,
154,"b'the run_squad report ""for training,each question should exactly have 1 answer"" when I tried to fintune bert on squad2.0'",2018-12-30T11:33:29Z,2018-12-30T11:48:50Z,,,
153,b'Did you suport squad2.0',2018-12-30T11:25:55Z,2019-01-14T09:03:50Z,,,
152,b'Squad 2.0',2018-12-29T04:21:56Z,2019-01-08T23:12:16Z,,,
151,b'Using large model with fp16 enable causes the server down',2018-12-28T16:32:05Z,2019-01-07T12:24:34Z,,,
150,b'BertLayerNorm not loaded in CPU mode',2018-12-28T09:55:05Z,2019-06-21T10:20:02Z,wontfix,RuntimeError,"RuntimeError: input must be a CUDA tensor (layer_norm_affine at apex/normalization/csrc/layer_norm_cuda.cpp:120)"
149,b'Speedup using NVIDIA Apex ',2018-12-27T23:17:42Z,2018-12-29T10:42:51Z,,,
148,b'Embeddings from BERT for original tokens',2018-12-27T06:48:23Z,2018-12-28T09:17:16Z,,,
147,b'Does the final hidden state contains the <CLS> for Squad2.0',2018-12-26T02:05:34Z,2018-12-26T02:48:04Z,,,
146,b'BertForQuestionAnswering: Predicting span on the question?',2018-12-24T12:51:49Z,2018-12-28T09:20:49Z,,,
145,b'Correct the  wrong note',2018-12-22T12:31:58Z,2019-01-07T11:23:06Z,,,
144,b'Some questions in Loss Function for MaskedLM',2018-12-22T12:24:24Z,2019-01-07T12:19:19Z,,,
143,b'bug in init_bert_weights',2018-12-21T08:29:40Z,2019-01-07T12:18:49Z,,,
142,b'change in run_classifier.py',2018-12-21T05:58:14Z,2019-01-07T11:21:58Z,,,
141,b'loading saved model when n_classes != 2',2018-12-20T21:56:20Z,2019-01-07T11:21:13Z,,,
140,b'Not able to use FP16 in pytorch-pretrained-BERT. Getting error **Runtime error: Expected scalar type object Half but got scalar type Float for argument #2 target**',2018-12-20T18:46:30Z,2019-01-07T12:18:36Z,,,
139,b'Not able to use FP16 in pytorch-pretrained-BERT',2018-12-20T18:46:14Z,2018-12-28T09:23:34Z,,,
138,b'Problem loading finetuned model for squad',2018-12-20T17:27:40Z,2019-01-07T12:17:58Z,,,
137,b'run_squad.py without GPU.. Without CUPY',2018-12-20T14:53:46Z,2018-12-21T09:33:02Z,,RuntimeError,"RuntimeError: CUDA environment is not correctly set up"
136,"b""It's possible to avoid download the pretrained model?""",2018-12-20T14:00:03Z,2018-12-20T14:08:10Z,,,
135,b'Problem loading a finetuned model.',2018-12-20T13:52:44Z,2019-01-07T11:20:43Z,,,
134,b'Fixing various class documentations.',2018-12-20T12:13:41Z,2019-01-07T11:06:07Z,,,
133,b'lower accuracy on OMD(Obama-McCain Debate twitter sentiment dataset)',2018-12-20T07:27:11Z,2019-01-07T12:11:22Z,,,
132,b'NONE',2018-12-20T05:42:29Z,2018-12-28T13:56:36Z,,,
131,"b'bert-base-multilingual-cased, do lower case problem'",2018-12-19T12:40:38Z,2019-01-07T12:08:47Z,,,
130,b'Use entry-points instead of scripts',2018-12-19T03:49:28Z,2018-12-19T09:18:25Z,,,
129,"b""BERT + CNN classifier doesn't work after migrating from 0.1.2 to 0.4.0""",2018-12-19T01:57:22Z,2018-12-20T00:20:48Z,,,
128,b'Add license to source distribution',2018-12-19T01:43:46Z,2018-12-19T09:15:54Z,,,
127,b'raises value error for bert tokenizer for long sequences',2018-12-18T14:51:40Z,2018-12-19T09:29:18Z,,,
126,b'Benchmarking Prediction Speed',2018-12-18T13:21:51Z,2019-05-12T13:18:41Z,"Discussion, wontfix",,
125,b'Warning/Assert when embedding sequences longer than positional embedding size',2018-12-18T10:36:23Z,2019-01-07T11:46:41Z,,,
124,b'Add example for fine tuning BERT language model',2018-12-18T09:48:12Z,2019-01-07T11:03:51Z,,,
123,b'big memory occupied',2018-12-18T03:13:11Z,2018-12-18T08:04:38Z,,,
122,b'_load_from_state_dict() takes 7 positional arguments but 8 were given',2018-12-17T05:38:40Z,2019-01-07T11:46:27Z,,,
121,b'High accuracy for CoLA task',2018-12-16T11:39:56Z,2018-12-17T06:41:06Z,,,
120,"b""RuntimeError: Expected object of type torch.LongTensor but found type torch.cuda.LongTensor for argument #3 'index'""",2018-12-15T18:43:53Z,2018-12-15T20:45:37Z,,RuntimeError,"RuntimeError: Expected object of type torch.LongTensor but found type torch.cuda.LongTensor for argument #3 'index'"
119,b'Minor README fix ',2018-12-14T19:11:14Z,2018-12-14T22:29:48Z,,,
118,b'Segmentation fault (core dumped)',2018-12-14T03:24:58Z,2018-12-22T14:26:56Z,,,
117,b'logging.basicConfig overrides user logging',2018-12-13T17:58:02Z,2018-12-14T13:46:51Z,,,
116,b'Change to use apex for better fp16 and multi-gpu support',2018-12-12T01:37:25Z,2018-12-13T11:32:38Z,,,
115,b'How to run a saved model?',2018-12-11T20:58:38Z,2018-12-14T14:43:43Z,,,
114,b'What is the best dataset structure for BERT?',2018-12-11T16:28:00Z,2018-12-11T20:57:45Z,,,
113,b'fix compatibility with python 3.5.2',2018-12-11T12:29:26Z,2018-12-13T11:15:15Z,,AttributeError,"AttributeError: 'PosixPath' object has no attribute 'rfind'"
112,b'Fourth release',2018-12-11T11:00:12Z,2018-12-14T14:15:47Z,,,
111,b'update: add from_state_dict for PreTrainedBertModel',2018-12-11T10:33:42Z,2018-12-12T02:04:42Z,,,
110,"b""Pretrained Tokenizer Loading Fails: 'PosixPath' object has no attribute 'rfind'""",2018-12-11T00:48:11Z,2018-12-11T10:28:47Z,,AttributeError,"AttributeError: 'PosixPath' object has no attribute 'rfind'"
109,"b""UnicodeDecodeError: 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte""",2018-12-11T00:04:09Z,2018-12-11T01:17:00Z,,UnicodeDecodeError,"UnicodeDecodeError: 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte"
108,b'Does max_seq_length specify the maxium number of words',2018-12-10T15:14:22Z,2018-12-11T12:11:10Z,,,
107,b'Fix optimizer to work with horovod',2018-12-10T10:10:34Z,2018-12-11T10:18:01Z,,,
106,b'Picking max_sequence_length in run_classifier.py CoLA task',2018-12-10T09:04:47Z,2018-12-10T15:14:47Z,,RuntimeError,"RuntimeError: Creating MTGP constants failed. at /jet/tmp/build/aten/src/THC/THCTensorRandom.cu:34"
105,b'weights initialized two times',2018-12-09T07:06:52Z,2018-12-09T21:17:51Z,,,
104,b'BERT for classification example training files',2018-12-08T15:16:50Z,2018-12-08T15:19:17Z,,,
103,b'Words after tokenization replaced with #',2018-12-08T11:56:57Z,2018-12-11T10:33:23Z,,,
102,b'How to modify the model config?',2018-12-08T08:38:37Z,2018-12-09T21:19:31Z,,RuntimeError,"RuntimeError: index out of range at /Users/soumith/code/builder/wheel/pytorch-src/aten/src/TH/generic/THTensorMath.cpp:352"
101,b'Adding --do_lower_case for all uncased BERTs examples',2018-12-07T19:41:50Z,2018-12-09T20:29:32Z,,,
100,b'Squad dataset has multiple answers to a question.',2018-12-07T16:02:00Z,2018-12-08T11:57:22Z,,,
99,b'run_squad.py stuck on batch size greater than 1',2018-12-07T13:44:09Z,2018-12-14T14:43:02Z,,,
98,b'Problem about convert TF model and pretraining',2018-12-07T13:42:59Z,2018-12-14T14:42:40Z,,AttributeError,"AttributeError: 'Parameter' object has no attribute 'adam_m'"
97,b'RuntimeError: cuda runtime error (59) : device-side assert triggered',2018-12-07T01:39:03Z,2018-12-07T07:25:30Z,,RuntimeError,"RuntimeError: cuda runtime error (59) : device-side assert triggered at /pytorch/aten/src/THC/generated/../generic/THCTensorMathPointwise.cu:266"
96,b'BertForMultipleChoice and Swag dataset example.',2018-12-06T18:34:05Z,2018-12-13T11:05:12Z,,,
95,b'Not updating the BERT embeddings during the fine tuning process',2018-12-06T14:40:33Z,2018-12-09T21:13:11Z,,,
94,b'Fixing the commentary of the `SquadExample` class.',2018-12-06T12:16:55Z,2018-12-09T20:27:31Z,,,
93,b'Zoeliao/dev',2018-12-06T02:15:01Z,2018-12-06T02:56:19Z,,,
92,b'Bert uncased and Bert large giving much lower results than Bert cased base',2018-12-05T19:13:24Z,2018-12-09T21:07:49Z,,,
91,b'run_classifier.py improvements',2018-12-05T17:22:31Z,2018-12-11T10:12:05Z,,,
90,b'Fine tuned to Multi-choice dataset?',2018-12-05T14:01:41Z,2018-12-10T10:10:16Z,,,
89,b'bert-base-multilingual-cased - Text bigger than 512',2018-12-05T10:11:21Z,2018-12-09T21:04:53Z,,,
88,b'Error when calculating loss and running backward',2018-12-04T13:30:58Z,2018-12-05T03:41:38Z,,RuntimeError,"RuntimeError: cublas runtime error : the GPU program failed to execute at /opt/conda/conda-bld/pytorch_1532581333611/work/aten/src/THC/THCBlas.cu:411"
87,b'Readme file links',2018-12-04T12:46:36Z,2018-12-05T15:41:05Z,,,
86,b'code in run_squad.py line 263',2018-12-04T11:08:09Z,2018-12-06T01:30:36Z,,,
85,b'How to use pre-trained SQUAD model? ',2018-12-04T03:13:30Z,2018-12-14T14:42:04Z,,,
84,b'elementwise_mean -> mean (thinking ahead to pytorch 1.0)',2018-12-03T23:59:40Z,2018-12-04T00:00:07Z,,,
83,b'Error while runing example',2018-12-03T20:21:12Z,2018-12-05T00:12:48Z,,RuntimeError,"RuntimeError: cuda runtime error (59) : device-side assert triggered at /opt/conda/conda-bld/pytorch_1533672544752/work/aten/src/THC/generic/THCTensorMath.cu:26"
82,"b""AttributeError: 'tuple' object has no attribute 'backward'""",2018-12-03T16:06:20Z,2018-12-04T07:27:06Z,,AttributeError,"AttributeError: 'tuple' object has no attribute 'backward'"
81,b'There is some problem in supporting continuously training',2018-12-03T12:00:09Z,2018-12-09T21:01:02Z,,,
80,b'How can I apply BERT to a cloze task?',2018-12-03T10:58:43Z,2018-12-09T20:57:24Z,,,
79,b'numpy.core._internal.AxisError: axis 1 is out of bounds for array of dimension 1',2018-12-03T07:56:56Z,2018-12-03T08:37:11Z,,,
78,"b""TypeError: object of type 'WindowsPath' has no len()""",2018-12-02T12:03:51Z,2018-12-02T15:30:43Z,,,
77,b'Correct assignement for logits in classifier example',2018-12-02T11:38:51Z,2018-12-02T12:01:05Z,,,
76,b'Wrong signature in model call in run_classifier.py example (?)',2018-12-01T19:34:40Z,2018-12-02T12:02:34Z,,,
75,b'Point typo fix',2018-12-01T00:07:09Z,2018-12-01T00:15:44Z,,,
74,b'Update finetuning example in README adding --do_lower_case',2018-12-01T00:06:52Z,2018-12-01T00:15:31Z,,,
73,b'Third release',2018-11-30T22:10:22Z,2018-11-30T22:10:31Z,,,
72,b'Fix internal hyperlink typo',2018-11-30T21:13:47Z,2018-11-30T22:33:53Z,,,
71,b'run_squad script gets stuck',2018-11-30T18:39:54Z,2018-11-30T19:47:07Z,,,
70,b'fix typo in input for masked lm loss function',2018-11-30T15:56:00Z,2018-11-30T17:23:47Z,,,
69,b'cannot access to pretrained vocab file on S3',2018-11-30T13:57:03Z,2018-11-30T14:44:24Z,,OSError,OSError: HEAD request failed for url https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt with status code 404
68,b'Accuracy on classification task is lower than the official tensorflow version',2018-11-30T06:30:56Z,2018-11-30T22:56:45Z,,,
67,"b""`TypeError: object of type 'NoneType' has no len()` when tuning on squad  """,2018-11-30T05:48:04Z,2018-11-30T13:24:02Z,,TypeError,"TypeError: object of type 'NoneType' has no len()"
66,b'speedup by truncating unused part',2018-11-29T14:56:39Z,2018-11-30T13:27:48Z,,,
65,b'3 sentences as input for BertForSequenceClassification?',2018-11-29T09:18:21Z,2018-11-30T22:58:16Z,,,
64,b'Feature extraction for sequential labelling',2018-11-29T03:33:09Z,2020-01-19T00:12:47Z,"Discussion, wontfix",,
63,b'Unseen Vocab',2018-11-28T22:38:57Z,2018-11-30T22:31:36Z,,,
62,b'Specify a model from a specific directory for extract_features.py',2018-11-28T17:04:39Z,2018-11-30T22:30:12Z,,ValueError,"ValueError: Can't find a vocabulary file at path <THEDIRECTORYPATHISPECIFIED> ..."
61,b'BERTConfigs in example usages in `modeling.py` are not OK (?)',2018-11-28T14:53:01Z,2018-11-30T22:29:24Z,,`ValueError,"`ValueError: The hidden size (512) is not a multiple of the number of attention heads (6)`"
60,b'Updated quick-start example with `BertForMaskedLM`',2018-11-28T13:54:01Z,2018-11-28T13:59:09Z,,,
59,b'not good when I use BERT for seq2seq model in keyphrase generation',2018-11-28T08:44:24Z,2018-11-29T07:47:06Z,,,
58,b'Bug fix in examples;correct t_total for distributed training;run pred\xe2\x80\xa6',2018-11-27T09:10:10Z,2018-11-28T11:39:46Z,,,
57,b'Missing function convert_to_unicode in tokenization.py',2018-11-26T21:50:15Z,2018-11-26T22:33:47Z,,,
56,b'[Feature request ] Add support for the new cased version of the multilingual model',2018-11-26T10:56:18Z,2018-11-30T22:28:32Z,,,
55,b'Loss calculation error',2018-11-25T03:48:17Z,2018-11-26T08:52:00Z,,,
54,b'example in BertForSequenceClassification() conflicts with the api ',2018-11-24T07:27:50Z,2018-11-26T08:54:47Z,,,
53,b'Multi-GPU training vs Distributed training',2018-11-24T00:49:45Z,2018-11-26T09:03:23Z,,,
52,"b""UnicodeDecodeError: 'charmap' codec can't decode byte 0x90 in position 3920: character maps to <undefined>""",2018-11-22T15:42:08Z,2018-11-23T11:21:56Z,,UnicodeDecodeError,"UnicodeDecodeError: 'charmap' codec can't decode byte 0x90 in position 3920: character maps to <undefined>"
51,b'Missing options/arguments in run_squad.py for BERT Large',2018-11-21T15:10:45Z,2018-11-26T08:57:23Z,,,
50,b'pytorch_pretrained_bert/convert_tf_checkpoint_to_pytorch.py  error',2018-11-21T10:36:49Z,2018-11-21T11:10:58Z,,attributeError,attributeError: 'BertForPreTraining' object has no attribute 'global_step'
49,b'Multilingual Issue',2018-11-21T09:32:32Z,2018-11-21T09:39:41Z,,,
48,b'example for is next sentence',2018-11-21T03:16:00Z,2018-11-21T09:40:15Z,,,
47,b'Fine-Tuned BERT-base on Squad v1. ',2018-11-20T17:04:09Z,2018-11-21T09:02:04Z,,,
46,b'Assertion `srcIndex < srcSelectDimSize` failed.',2018-11-20T12:50:41Z,2018-11-21T09:02:51Z,,RuntimeError,"RuntimeError: cublas runtime error : resource allocation failed at /opt/conda/conda-bld/pytorch_1532584813488/work/aten/src/THC/THCGeneral.cpp:333"
45,b'Issue of `bert_model` arg in `run_classify.py`',2018-11-20T09:48:09Z,2018-11-20T13:07:14Z,,,
44,b'Race condition when prepare pretrained model in distributed training ',2018-11-20T09:40:25Z,2018-11-26T09:23:03Z,,EOFError,"EOFError: Compressed file ended before the end-of-stream marker was reached"
43,b'grad is None in squad example',2018-11-20T08:38:03Z,2018-11-20T23:04:28Z,,AttributeError,"AttributeError: 'NoneType' object has no attribute 'data'"
42,"b""Fixed UnicodeDecodeError: 'ascii' codec can't decode byte 0xc2""",2018-11-20T04:09:44Z,2018-11-20T09:09:50Z,,,
41,b'Typo in README',2018-11-20T03:52:35Z,2018-11-20T09:02:15Z,,,
40,b'update pip package name',2018-11-19T17:50:54Z,2018-11-19T19:54:47Z,,,
39,b'Command-line interface Document Bug',2018-11-19T16:42:56Z,2018-11-20T09:03:06Z,,,
38,b'truncated normal initializer',2018-11-19T16:35:08Z,2018-11-26T09:42:42Z,,,
37,b'using BERT as a language Model',2018-11-19T15:26:20Z,2018-11-20T09:06:07Z,,,
36,b'How to detokenize a BertTokenizer output?',2018-11-19T04:39:04Z,2018-11-20T09:07:39Z,,,
35,b'issues with accents on convert_ids_to_tokens()',2018-11-18T20:41:24Z,2018-11-19T08:39:56Z,,,
34,b'Can not find vocabulary file for Chinese model',2018-11-18T14:33:58Z,2018-11-19T03:17:31Z,,ValueError,ValueError: Can't find a vocabulary file at path 'chinese_L-12_H-768_A-12'. To load the vocabulary from a Google pretrained model use `tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)`
33,b'[Bug report] Ineffective no_decay when using BERTAdam',2018-11-18T08:28:52Z,2018-11-20T09:07:58Z,,,
32,b'Fix ineffective no_decay bug when using BERTAdam',2018-11-18T08:21:37Z,2018-11-20T09:11:47Z,,,
31,b'BERT model for Machine Translation',2018-11-18T02:10:15Z,2018-11-18T08:48:33Z,,,
30,b'[Feature request] Add example of finetuning the pretrained models on custom corpus',2018-11-17T15:19:58Z,2018-11-17T22:03:43Z,,,
29,b'First release',2018-11-17T11:19:41Z,2018-11-17T11:21:49Z,,,
28,b'speed is very slow',2018-11-17T06:51:54Z,2018-11-17T22:02:38Z,,,
27,b'how to load checkpoint?',2018-11-17T06:23:28Z,2018-11-17T06:50:57Z,,,
26,b'Checkpoints not saved',2018-11-16T18:50:27Z,2018-11-17T22:02:08Z,,,
25,b'can you push the run-pretraining and create_pretraining_data codes?',2018-11-16T08:15:33Z,2018-11-17T21:57:19Z,,,
24,b'[Feature request] Port SQuAD 2.0 support',2018-11-15T23:47:04Z,2018-11-17T21:57:07Z,,,
23,b'ValueError while using --optimize_on_cpu',2018-11-15T16:53:12Z,2018-11-17T21:56:46Z,,ValueError,"ValueError: ('The argument is not a tensor', 'None')"
22,b'adding `no_cuda` flag',2018-11-15T10:33:03Z,2018-11-17T22:05:24Z,,AttributeError,"AttributeError: 'Namespace' object has no attribute 'no_cuda'"
21,b'Fix some glitches in extract_features.py',2018-11-15T07:49:20Z,2018-11-17T22:07:20Z,,,
20,b'model loading the checkpoint error',2018-11-14T08:13:34Z,2018-11-15T21:02:04Z,,RuntimeError,"RuntimeError: Error(s) in loading state_dict for BertModel:"
19,b'will you push the pytorch code for the pre-training process?',2018-11-14T06:30:59Z,2018-11-17T21:55:41Z,,,
18,b'include the output layer in the model using the pretrained weights',2018-11-13T16:15:03Z,2018-11-17T22:08:09Z,,,
17,b'activation function in BERTIntermediate',2018-11-13T15:47:46Z,2018-11-13T16:00:10Z,,,
16,b'Excluding AdamWeightDecayOptimizer internal variables from restoring',2018-11-13T15:13:18Z,2018-11-13T15:19:29Z,,,
15,b'activation function in BERTIntermediate',2018-11-13T15:09:33Z,2018-11-13T15:17:39Z,,,
14,b'fixed typo',2018-11-12T01:18:24Z,2018-11-12T07:36:04Z,,,
13,b'Bug in run_classifier.py',2018-11-10T17:16:01Z,2018-11-10T17:45:28Z,,,
12,b'py2 code',2018-11-10T13:23:31Z,2018-11-10T15:06:35Z,,,
11,b'Swapped to_seq_len/from_seq_len in comment',2018-11-09T06:13:08Z,2018-11-09T08:31:25Z,,,
10,b'Is there a plan to have a FP16 for GPU so to have larger batch size or longer text documents support ?',2018-11-09T02:23:34Z,2018-11-12T16:06:47Z,,,
9,b'Crash at the end of training',2018-11-08T22:01:57Z,2018-11-09T08:17:26Z,,RuntimeError,"RuntimeError: dimension specified as 0 but tensor has no dimensions"
8,b'fixed small typos in the README.md',2018-11-08T18:24:27Z,2018-11-08T20:00:02Z,,,
7,b'Develop',2018-11-07T22:34:18Z,2018-11-07T22:36:46Z,,,
6,b'Failure during pytest (and solution for python3)',2018-11-06T08:23:29Z,2018-11-07T23:43:42Z,,,
5,b'MRPC hyperparameters question',2018-11-06T05:30:36Z,2018-11-07T23:42:51Z,,,
4,b'Fix typo in subheader BertForQuestionAnswering',2018-11-05T23:04:03Z,2018-11-05T23:34:18Z,,,
3,b'run_squad questions',2018-11-05T21:35:51Z,2018-11-07T22:37:09Z,,,
2,b'Port tokenization for the multilingual model',2018-11-05T21:35:36Z,2018-11-10T21:27:46Z,,,
1,b'Create DataParallel model if several GPUs',2018-11-03T14:10:20Z,2018-11-03T14:10:35Z,,,
