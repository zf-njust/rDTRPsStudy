{"error": null, "response": {"classes": [{"cls_lc": [[31, 0], [104, 70]], "cls_var_ln": {}, "cls_var_occur": {}, "funcs": [{"docstring": {"func": "Constructs BertConfig.", "long_descr": null, "ret": null}, "fn_lc": [[34, 2], [80, 46]], "fn_var_ln": {"attention_probs_dropout_prob": [[77, 4], [77, 37]], "hidden_act": [[74, 4], [74, 19]], "hidden_dropout_prob": [[76, 4], [76, 28]], "hidden_size": [[71, 4], [71, 20]], "initializer_range": [[80, 4], [80, 26]], "intermediate_size": [[75, 4], [75, 26]], "max_position_embeddings": [[78, 4], [78, 32]], "num_attention_heads": [[73, 4], [73, 28]], "num_hidden_layers": [[72, 4], [72, 26]], "type_vocab_size": [[79, 4], [79, 24]], "vocab_size": [[70, 4], [70, 19]]}, "fn_var_occur": {"attention_probs_dropout_prob": [["self", "attention_probs_dropout_prob", "attention_probs_dropout_prob"]], "hidden_act": [["self", "hidden_act", "hidden_act"]], "hidden_dropout_prob": [["self", "hidden_dropout_prob", "hidden_dropout_prob"]], "hidden_size": [["self", "hidden_size", "hidden_size"]], "initializer_range": [["self", "initializer_range", "initializer_range"]], "intermediate_size": [["self", "intermediate_size", "intermediate_size"]], "max_position_embeddings": [["self", "max_position_embeddings", "max_position_embeddings"]], "num_attention_heads": [["self", "num_attention_heads", "num_attention_heads"]], "num_hidden_layers": [["self", "num_hidden_layers", "num_hidden_layers"]], "type_vocab_size": [["self", "type_vocab_size", "type_vocab_size"]], "vocab_size": [["self", "vocab_size", "vocab_size"]]}, "name": "__init__", "params": {"attention_probs_dropout_prob": "", "hidden_act": "", "hidden_dropout_prob": "", "hidden_size": "", "initializer_range": "", "intermediate_size": "", "max_position_embeddings": "", "num_attention_heads": "", "num_hidden_layers": "", "self": "", "type_vocab_size": "", "vocab_size": ""}, "params_descr": {"attention_probs_dropout_prob": "The dropout ratio for the attention\nprobabilities.", "hidden_act": "The non-linear activation function (function or string) in the\nencoder and pooler.", "hidden_dropout_prob": "The dropout probability for all fully connected\nlayers in the embeddings, encoder, and pooler.", "hidden_size": "Size of the encoder layers and the pooler layer.", "initializer_range": "The stdev of the truncated_normal_initializer for\ninitializing all weight matrices.", "intermediate_size": "The size of the \"intermediate\" (i.e., feed-forward)\nlayer in the Transformer encoder.", "max_position_embeddings": "The maximum sequence length that this model might\never be used with. Typically set this to something large just in case\n(e.g., 512 or 1024 or 2048).", "num_attention_heads": "Number of attention heads for each attention layer in\nthe Transformer encoder.", "num_hidden_layers": "Number of hidden layers in the Transformer encoder.", "self": "", "type_vocab_size": "The vocabulary size of the `token_type_ids` passed into\n`BertModel`.", "vocab_size": "Vocabulary size of `inputs_ids` in `BertModel`."}, "params_occur": {"attention_probs_dropout_prob": [["self", "attention_probs_dropout_prob", "attention_probs_dropout_prob"]], "hidden_act": [["self", "hidden_act", "hidden_act"]], "hidden_dropout_prob": [["self", "hidden_dropout_prob", "hidden_dropout_prob"]], "hidden_size": [["self", "hidden_size", "hidden_size"]], "initializer_range": [["self", "initializer_range", "initializer_range"]], "intermediate_size": [["self", "intermediate_size", "intermediate_size"]], "max_position_embeddings": [["self", "max_position_embeddings", "max_position_embeddings"]], "num_attention_heads": [["self", "num_attention_heads", "num_attention_heads"]], "num_hidden_layers": [["self", "num_hidden_layers", "num_hidden_layers"]], "self": [["self", "vocab_size", "vocab_size"], ["self", "hidden_size", "hidden_size"], ["self", "num_hidden_layers", "num_hidden_layers"], ["self", "num_attention_heads", "num_attention_heads"], ["self", "hidden_act", "hidden_act"], ["self", "intermediate_size", "intermediate_size"], ["self", "hidden_dropout_prob", "hidden_dropout_prob"], ["self", "attention_probs_dropout_prob", "attention_probs_dropout_prob"], ["self", "max_position_embeddings", "max_position_embeddings"], ["self", "type_vocab_size", "type_vocab_size"], ["self", "initializer_range", "initializer_range"]], "type_vocab_size": [["self", "type_vocab_size", "type_vocab_size"]], "vocab_size": [["self", "vocab_size", "vocab_size"]]}, "params_p": {"args": [], "attention_probs_dropout_prob": [["float", 0.5129561272298571], ["int", 0.30136489946082445], ["bool", 0.09766902536141477]], "hidden_act": [["float", 0.5129561272298571], ["int", 0.30136489946082445], ["bool", 0.09766902536141477]], "hidden_dropout_prob": [["float", 0.5129561272298571], ["int", 0.30136489946082445], ["bool", 0.09766902536141477]], "hidden_size": [["int", 0.7509040742961989], ["Union[Tuple[int, int], int]", 0.10631243920015128], ["bool", 0.07344061155620758]], "initializer_range": [["float", 0.5129561272298571], ["int", 0.30136489946082445], ["bool", 0.09766902536141477]], "intermediate_size": [["int", 0.7509040742961989], ["Union[Tuple[int, int], int]", 0.10631243920015128], ["bool", 0.07344061155620758]], "kwargs": [], "max_position_embeddings": [["int", 0.7568093871084198], ["float", 0.15696251663017888], ["Optional[int]", 0.08622809626140121]], "num_attention_heads": [["int", 0.6601362084487119], ["Optional[int]", 0.13708515609053545], ["Type[tensorflow.keras.layers.Layer]", 0.13423605741548508]], "num_hidden_layers": [["int", 0.6601362084487119], ["Optional[int]", 0.13708515609053545], ["Type[tensorflow.keras.layers.Layer]", 0.13423605741548508]], "self": [], "type_vocab_size": [["Optional[int]", 0.21197789522800536], ["Optional[float]", 0.21084577437156501], ["float", 0.19914452186002982], ["int", 0.1966713663552441], ["bool", 0.09068022109257787]], "vocab_size": [["int", 0.7509040742961989], ["Union[Tuple[int, int], int]", 0.10631243920015128], ["bool", 0.07344061155620758]]}, "q_name": "BertConfig.__init__", "ret_exprs": [], "ret_type": "", "variables": {"attention_probs_dropout_prob": "", "hidden_act": "", "hidden_dropout_prob": "", "hidden_size": "", "initializer_range": "", "intermediate_size": "", "max_position_embeddings": "", "num_attention_heads": "", "num_hidden_layers": "", "type_vocab_size": "", "vocab_size": ""}, "variables_p": {"attention_probs_dropout_prob": [["str", 0.30000000000000004], ["Dict[str, Any]", 0.1], ["dict", 0.1], ["int", 0.1]], "hidden_act": [["str", 0.30000000000000004], ["Dict[str, Any]", 0.1], ["dict", 0.1], ["int", 0.1]], "hidden_dropout_prob": [["str", 0.30000000000000004], ["Dict[str, Any]", 0.1], ["dict", 0.1], ["int", 0.1]], "hidden_size": [["int", 0.6011954792961447], ["Dict[str, int]", 0.10190101176311737], ["List[int]", 0.09120963028687405]], "initializer_range": [["float", 0.275026791442021], ["bytes", 0.16664164394406855], ["int", 0.14659483487872477], ["str", 0.09178005349327625], ["List[str]", 0.06675711227256084]], "intermediate_size": [["int", 0.6011954792961447], ["Dict[str, int]", 0.10190101176311737], ["List[int]", 0.09120963028687405]], "max_position_embeddings": [["int", 0.9089230495381873], ["float", 0.0910769504618129]], "num_attention_heads": [["int", 0.8807443998015739], ["Dict[float, float]", 0.11925560019842625]], "num_hidden_layers": [["int", 0.8807443998015739], ["Dict[float, float]", 0.11925560019842625]], "type_vocab_size": [["int", 0.9999999999999998]], "vocab_size": [["int", 0.6011954792961447], ["Dict[str, int]", 0.10190101176311737], ["List[int]", 0.09120963028687405]]}}, {"docstring": {"func": "Constructs a `BertConfig` from a Python dictionary of parameters.", "long_descr": null, "ret": null}, "fn_lc": [[83, 2], [88, 17]], "fn_var_ln": {"config": [[85, 4], [85, 10]]}, "fn_var_occur": {"config": [["config", "BertConfig", "vocab_size", "None"], ["config", "__dict__", "key", "value"]]}, "name": "from_dict", "params": {"cls": "", "json_object": ""}, "params_descr": {"cls": "", "json_object": ""}, "params_occur": {"cls": [], "json_object": [["six", "iteritems", "json_object"]]}, "params_p": {"args": [], "cls": [["dict", 0.5949486509960319], ["str", 0.28294262160400174], ["Optional[bool]", 0.05806907635680815]], "json_object": [["Type[T]", 0.9999999990738335], ["List[str]", 2.68083730489297e-10], ["Optional[float]", 1.5871057911221547e-10], ["int", 7.506302729120645e-11], ["dict", 5.464256752721376e-11], ["Optional[int]", 4.636659599989202e-11]], "kwargs": [], "self": []}, "q_name": "BertConfig.from_dict", "ret_exprs": ["return config"], "ret_type": "", "ret_type_p": [["Dict[str, Any]", 0.29851462141805407], ["dict", 0.09927647502580833]], "variables": {"config": ""}, "variables_p": {"config": [["Union[Literal, Literal]", 0.11528118165837391], ["Dict[str, Dict[str, Any]]", 0.08772339729148965], ["Optional[Dict[str, str]]", 0.08618493790954176]]}}, {"docstring": {"func": "Constructs a `BertConfig` from a json file of parameters.", "long_descr": null, "ret": null}, "fn_lc": [[91, 2], [95, 42]], "fn_var_ln": {"text": [[94, 6], [94, 10]]}, "fn_var_occur": {"text": [["text", "reader", "read"], ["cls", "from_dict", "json", "loads", "text"]]}, "name": "from_json_file", "params": {"cls": "", "json_file": ""}, "params_descr": {"cls": "", "json_file": ""}, "params_occur": {"cls": [["cls", "from_dict", "json", "loads", "text"]], "json_file": [["tf", "gfile", "GFile", "json_file", "reader"]]}, "params_p": {"args": [], "cls": [["str", 0.5975841940357735], ["bytes", 0.10096731702246071], ["Dict[str, Any]", 0.09217840021564641]], "json_file": [["str", 0.35953492775629836]], "kwargs": [], "self": []}, "q_name": "BertConfig.from_json_file", "ret_exprs": ["return cls.from_dict(json.loads(text))"], "ret_type": "", "ret_type_p": [["bool", 0.19408705812799437], ["str", 0.11346333715215434], ["Callable", 0.10324846970829532], ["list", 0.08872048703411152]], "variables": {"text": ""}, "variables_p": {"text": [["str", 0.20896320580434424], ["Type[Dict[str, Any]]", 0.12073901406174649], ["Pattern[str]", 0.11062000340176079], ["Dict[str, Union[Any, Any, Any, Any]]", 0.09770761926090316], ["List[Tuple[str, Any]]", 0.09445489527859305], ["Type[str]", 0.09438831213916651]]}}, {"docstring": {"func": "Serializes this instance to a Python dictionary.", "long_descr": null, "ret": null}, "fn_lc": [[97, 2], [100, 17]], "fn_var_ln": {"output": [[99, 4], [99, 10]]}, "fn_var_occur": {"output": [["output", "copy", "deepcopy", "self", "__dict__"]]}, "name": "to_dict", "params": {"self": ""}, "params_descr": {"self": ""}, "params_occur": {"self": [["output", "copy", "deepcopy", "self", "__dict__"]]}, "params_p": {"args": [], "kwargs": [], "self": []}, "q_name": "BertConfig.to_dict", "ret_exprs": ["return output"], "ret_type": "", "ret_type_p": [["str", 0.40127107527095274], ["int", 0.10234088109596855], ["Dict[str, str]", 0.09681286476284566]], "variables": {"output": ""}, "variables_p": {"output": [["Tuple[List[int], List[int]]", 0.26926140610181404], ["str", 0.19030592189076578], ["List[Dict[str, Any]]", 0.10267909035953528], ["Tuple[int, int, int, int, int, int]", 0.09639985545003876]]}}, {"docstring": {"func": "Serializes this instance to a JSON string.", "long_descr": null, "ret": null}, "fn_lc": [[102, 2], [104, 70]], "fn_var_ln": {}, "fn_var_occur": {}, "name": "to_json_string", "params": {"self": ""}, "params_descr": {"self": ""}, "params_occur": {"self": [["json", "dumps", "self", "to_dict", "indent", "sort_keys", "True"]]}, "params_p": {"args": [], "kwargs": [], "self": []}, "q_name": "BertConfig.to_json_string", "ret_exprs": ["return json.dumps(self.to_dict(), indent=2, sort_keys=True) + \"n\""], "ret_type": "", "ret_type_p": [["str", 0.4018341874666681], ["bytes", 0.09376458651875881]], "variables": {}, "variables_p": {}}], "name": "BertConfig", "q_name": "BertConfig", "variables": {}, "variables_p": {}}, {"cls_lc": [[107, 0], [261, 31]], "cls_var_ln": {}, "cls_var_occur": {}, "funcs": [{"docstring": {"func": "Constructor for BertModel.", "long_descr": null, "ret": null}, "fn_lc": [[131, 2], [232, 76]], "fn_var_ln": {"all_encoder_layers": [[205, 8], [205, 31]], "attention_mask": [[200, 8], [200, 22]], "attention_probs_dropout_prob": [[159, 6], [159, 41]], "batch_size": [[162, 4], [162, 14]], "config": [[156, 4], [156, 10]], "embedding_output": [[184, 8], [184, 29]], "embedding_table": [[174, 37], [174, 52]], "first_token_tensor": [[227, 8], [227, 26]], "hidden_dropout_prob": [[158, 6], [158, 32]], "input_mask": [[166, 6], [166, 16]], "input_shape": [[161, 4], [161, 15]], "pooled_output": [[228, 8], [228, 26]], "seq_length": [[163, 4], [163, 14]], "sequence_output": [[218, 6], [218, 26]], "token_type_ids": [[169, 6], [169, 20]]}, "fn_var_occur": {"all_encoder_layers": [["self", "all_encoder_layers", "transformer_model", "input_tensor", "self", "embedding_output", "attention_mask", "attention_mask", "hidden_size", "config", "hidden_size", "num_hidden_layers", "config", "num_hidden_layers", "num_attention_heads", "config", "num_attention_heads", "intermediate_size", "config", "intermediate_size", "intermediate_act_fn", "get_activation", "config", "hidden_act", "hidden_dropout_prob", "config", "hidden_dropout_prob", "attention_probs_dropout_prob", "config", "attention_probs_dropout_prob", "initializer_range", "config", "initializer_range", "do_return_all_layers", "True"], ["self", "sequence_output", "self", "all_encoder_layers"]], "attention_mask": [["attention_mask", "create_attention_mask_from_input_mask", "input_ids", "input_mask"], ["self", "all_encoder_layers", "transformer_model", "input_tensor", "self", "embedding_output", "attention_mask", "attention_mask", "hidden_size", "config", "hidden_size", "num_hidden_layers", "config", "num_hidden_layers", "num_attention_heads", "config", "num_attention_heads", "intermediate_size", "config", "intermediate_size", "intermediate_act_fn", "get_activation", "config", "hidden_act", "hidden_dropout_prob", "config", "hidden_dropout_prob", "attention_probs_dropout_prob", "config", "attention_probs_dropout_prob", "initializer_range", "config", "initializer_range", "do_return_all_layers", "True"]], "attention_probs_dropout_prob": [["config", "attention_probs_dropout_prob"], ["self", "all_encoder_layers", "transformer_model", "input_tensor", "self", "embedding_output", "attention_mask", "attention_mask", "hidden_size", "config", "hidden_size", "num_hidden_layers", "config", "num_hidden_layers", "num_attention_heads", "config", "num_attention_heads", "intermediate_size", "config", "intermediate_size", "intermediate_act_fn", "get_activation", "config", "hidden_act", "hidden_dropout_prob", "config", "hidden_dropout_prob", "attention_probs_dropout_prob", "config", "attention_probs_dropout_prob", "initializer_range", "config", "initializer_range", "do_return_all_layers", "True"]], "batch_size": [["batch_size", "input_shape"], ["input_mask", "tf", "ones", "shape", "batch_size", "seq_length", "dtype", "tf", "int32"], ["token_type_ids", "tf", "zeros", "shape", "batch_size", "seq_length", "dtype", "tf", "int32"]], "config": [["config", "copy", "deepcopy", "config"], ["config", "hidden_dropout_prob"], ["config", "attention_probs_dropout_prob"], ["self", "embedding_output", "self", "embedding_table", "embedding_lookup", "input_ids", "input_ids", "vocab_size", "config", "vocab_size", "embedding_size", "config", "hidden_size", "initializer_range", "config", "initializer_range", "word_embedding_name", "use_one_hot_embeddings", "use_one_hot_embeddings"], ["self", "embedding_output", "embedding_postprocessor", "input_tensor", "self", "embedding_output", "use_token_type", "True", "token_type_ids", "token_type_ids", "token_type_vocab_size", "config", "type_vocab_size", "token_type_embedding_name", "use_position_embeddings", "True", "position_embedding_name", "initializer_range", "config", "initializer_range", "max_position_embeddings", "config", "max_position_embeddings", "dropout_prob", "config", "hidden_dropout_prob"], ["self", "all_encoder_layers", "transformer_model", "input_tensor", "self", "embedding_output", "attention_mask", "attention_mask", "hidden_size", "config", "hidden_size", "num_hidden_layers", "config", "num_hidden_layers", "num_attention_heads", "config", "num_attention_heads", "intermediate_size", "config", "intermediate_size", "intermediate_act_fn", "get_activation", "config", "hidden_act", "hidden_dropout_prob", "config", "hidden_dropout_prob", "attention_probs_dropout_prob", "config", "attention_probs_dropout_prob", "initializer_range", "config", "initializer_range", "do_return_all_layers", "True"], ["self", "pooled_output", "tf", "layers", "dense", "first_token_tensor", "config", "hidden_size", "activation", "tf", "tanh", "kernel_initializer", "create_initializer", "config", "initializer_range"]], "embedding_output": [["self", "embedding_output", "self", "embedding_table", "embedding_lookup", "input_ids", "input_ids", "vocab_size", "config", "vocab_size", "embedding_size", "config", "hidden_size", "initializer_range", "config", "initializer_range", "word_embedding_name", "use_one_hot_embeddings", "use_one_hot_embeddings"], ["self", "embedding_output", "embedding_postprocessor", "input_tensor", "self", "embedding_output", "use_token_type", "True", "token_type_ids", "token_type_ids", "token_type_vocab_size", "config", "type_vocab_size", "token_type_embedding_name", "use_position_embeddings", "True", "position_embedding_name", "initializer_range", "config", "initializer_range", "max_position_embeddings", "config", "max_position_embeddings", "dropout_prob", "config", "hidden_dropout_prob"], ["self", "all_encoder_layers", "transformer_model", "input_tensor", "self", "embedding_output", "attention_mask", "attention_mask", "hidden_size", "config", "hidden_size", "num_hidden_layers", "config", "num_hidden_layers", "num_attention_heads", "config", "num_attention_heads", "intermediate_size", "config", "intermediate_size", "intermediate_act_fn", "get_activation", "config", "hidden_act", "hidden_dropout_prob", "config", "hidden_dropout_prob", "attention_probs_dropout_prob", "config", "attention_probs_dropout_prob", "initializer_range", "config", "initializer_range", "do_return_all_layers", "True"]], "embedding_table": [["self", "embedding_output", "self", "embedding_table", "embedding_lookup", "input_ids", "input_ids", "vocab_size", "config", "vocab_size", "embedding_size", "config", "hidden_size", "initializer_range", "config", "initializer_range", "word_embedding_name", "use_one_hot_embeddings", "use_one_hot_embeddings"]], "first_token_tensor": [["first_token_tensor", "tf", "squeeze", "self", "sequence_output", "axis"], ["self", "pooled_output", "tf", "layers", "dense", "first_token_tensor", "config", "hidden_size", "activation", "tf", "tanh", "kernel_initializer", "create_initializer", "config", "initializer_range"]], "hidden_dropout_prob": [["config", "hidden_dropout_prob"], ["self", "embedding_output", "embedding_postprocessor", "input_tensor", "self", "embedding_output", "use_token_type", "True", "token_type_ids", "token_type_ids", "token_type_vocab_size", "config", "type_vocab_size", "token_type_embedding_name", "use_position_embeddings", "True", "position_embedding_name", "initializer_range", "config", "initializer_range", "max_position_embeddings", "config", "max_position_embeddings", "dropout_prob", "config", "hidden_dropout_prob"], ["self", "all_encoder_layers", "transformer_model", "input_tensor", "self", "embedding_output", "attention_mask", "attention_mask", "hidden_size", "config", "hidden_size", "num_hidden_layers", "config", "num_hidden_layers", "num_attention_heads", "config", "num_attention_heads", "intermediate_size", "config", "intermediate_size", "intermediate_act_fn", "get_activation", "config", "hidden_act", "hidden_dropout_prob", "config", "hidden_dropout_prob", "attention_probs_dropout_prob", "config", "attention_probs_dropout_prob", "initializer_range", "config", "initializer_range", "do_return_all_layers", "True"]], "input_mask": [["input_mask", "None"], ["input_mask", "tf", "ones", "shape", "batch_size", "seq_length", "dtype", "tf", "int32"], ["attention_mask", "create_attention_mask_from_input_mask", "input_ids", "input_mask"]], "input_shape": [["input_shape", "get_shape_list", "input_ids", "expected_rank"], ["batch_size", "input_shape"], ["seq_length", "input_shape"]], "pooled_output": [["self", "pooled_output", "tf", "layers", "dense", "first_token_tensor", "config", "hidden_size", "activation", "tf", "tanh", "kernel_initializer", "create_initializer", "config", "initializer_range"]], "seq_length": [["seq_length", "input_shape"], ["input_mask", "tf", "ones", "shape", "batch_size", "seq_length", "dtype", "tf", "int32"], ["token_type_ids", "tf", "zeros", "shape", "batch_size", "seq_length", "dtype", "tf", "int32"]], "sequence_output": [["self", "sequence_output", "self", "all_encoder_layers"], ["first_token_tensor", "tf", "squeeze", "self", "sequence_output", "axis"]], "token_type_ids": [["token_type_ids", "None"], ["token_type_ids", "tf", "zeros", "shape", "batch_size", "seq_length", "dtype", "tf", "int32"], ["self", "embedding_output", "embedding_postprocessor", "input_tensor", "self", "embedding_output", "use_token_type", "True", "token_type_ids", "token_type_ids", "token_type_vocab_size", "config", "type_vocab_size", "token_type_embedding_name", "use_position_embeddings", "True", "position_embedding_name", "initializer_range", "config", "initializer_range", "max_position_embeddings", "config", "max_position_embeddings", "dropout_prob", "config", "hidden_dropout_prob"]]}, "name": "__init__", "params": {"config": "", "input_ids": "", "input_mask": "", "is_training": "", "scope": "", "self": "", "token_type_ids": "", "use_one_hot_embeddings": ""}, "params_descr": {"config": "`BertConfig` instance.", "input_ids": "int32 Tensor of shape [batch_size, seq_length].", "input_mask": "(optional) int32 Tensor of shape [batch_size, seq_length].", "is_training": "bool. true for training model, false for eval model. Controls\nwhether dropout will be applied.", "scope": "(optional) variable scope. Defaults to \"bert\".", "self": "", "token_type_ids": "(optional) int32 Tensor of shape [batch_size, seq_length].", "use_one_hot_embeddings": "(optional) bool. Whether to use one-hot word\nembeddings or tf.embedding_lookup() for the word embeddings."}, "params_occur": {"config": [["config", "copy", "deepcopy", "config"], ["config", "hidden_dropout_prob"], ["config", "attention_probs_dropout_prob"], ["self", "embedding_output", "self", "embedding_table", "embedding_lookup", "input_ids", "input_ids", "vocab_size", "config", "vocab_size", "embedding_size", "config", "hidden_size", "initializer_range", "config", "initializer_range", "word_embedding_name", "use_one_hot_embeddings", "use_one_hot_embeddings"], ["self", "embedding_output", "embedding_postprocessor", "input_tensor", "self", "embedding_output", "use_token_type", "True", "token_type_ids", "token_type_ids", "token_type_vocab_size", "config", "type_vocab_size", "token_type_embedding_name", "use_position_embeddings", "True", "position_embedding_name", "initializer_range", "config", "initializer_range", "max_position_embeddings", "config", "max_position_embeddings", "dropout_prob", "config", "hidden_dropout_prob"], ["self", "all_encoder_layers", "transformer_model", "input_tensor", "self", "embedding_output", "attention_mask", "attention_mask", "hidden_size", "config", "hidden_size", "num_hidden_layers", "config", "num_hidden_layers", "num_attention_heads", "config", "num_attention_heads", "intermediate_size", "config", "intermediate_size", "intermediate_act_fn", "get_activation", "config", "hidden_act", "hidden_dropout_prob", "config", "hidden_dropout_prob", "attention_probs_dropout_prob", "config", "attention_probs_dropout_prob", "initializer_range", "config", "initializer_range", "do_return_all_layers", "True"], ["self", "pooled_output", "tf", "layers", "dense", "first_token_tensor", "config", "hidden_size", "activation", "tf", "tanh", "kernel_initializer", "create_initializer", "config", "initializer_range"]], "input_ids": [["input_shape", "get_shape_list", "input_ids", "expected_rank"], ["self", "embedding_output", "self", "embedding_table", "embedding_lookup", "input_ids", "input_ids", "vocab_size", "config", "vocab_size", "embedding_size", "config", "hidden_size", "initializer_range", "config", "initializer_range", "word_embedding_name", "use_one_hot_embeddings", "use_one_hot_embeddings"], ["attention_mask", "create_attention_mask_from_input_mask", "input_ids", "input_mask"]], "input_mask": [["input_mask", "None"], ["input_mask", "tf", "ones", "shape", "batch_size", "seq_length", "dtype", "tf", "int32"], ["attention_mask", "create_attention_mask_from_input_mask", "input_ids", "input_mask"]], "is_training": [], "scope": [["tf", "variable_scope", "scope", "default_name"]], "self": [["self", "embedding_output", "self", "embedding_table", "embedding_lookup", "input_ids", "input_ids", "vocab_size", "config", "vocab_size", "embedding_size", "config", "hidden_size", "initializer_range", "config", "initializer_range", "word_embedding_name", "use_one_hot_embeddings", "use_one_hot_embeddings"], ["self", "embedding_output", "embedding_postprocessor", "input_tensor", "self", "embedding_output", "use_token_type", "True", "token_type_ids", "token_type_ids", "token_type_vocab_size", "config", "type_vocab_size", "token_type_embedding_name", "use_position_embeddings", "True", "position_embedding_name", "initializer_range", "config", "initializer_range", "max_position_embeddings", "config", "max_position_embeddings", "dropout_prob", "config", "hidden_dropout_prob"], ["self", "all_encoder_layers", "transformer_model", "input_tensor", "self", "embedding_output", "attention_mask", "attention_mask", "hidden_size", "config", "hidden_size", "num_hidden_layers", "config", "num_hidden_layers", "num_attention_heads", "config", "num_attention_heads", "intermediate_size", "config", "intermediate_size", "intermediate_act_fn", "get_activation", "config", "hidden_act", "hidden_dropout_prob", "config", "hidden_dropout_prob", "attention_probs_dropout_prob", "config", "attention_probs_dropout_prob", "initializer_range", "config", "initializer_range", "do_return_all_layers", "True"], ["self", "sequence_output", "self", "all_encoder_layers"], ["first_token_tensor", "tf", "squeeze", "self", "sequence_output", "axis"], ["self", "pooled_output", "tf", "layers", "dense", "first_token_tensor", "config", "hidden_size", "activation", "tf", "tanh", "kernel_initializer", "create_initializer", "config", "initializer_range"]], "token_type_ids": [["token_type_ids", "None"], ["token_type_ids", "tf", "zeros", "shape", "batch_size", "seq_length", "dtype", "tf", "int32"], ["self", "embedding_output", "embedding_postprocessor", "input_tensor", "self", "embedding_output", "use_token_type", "True", "token_type_ids", "token_type_ids", "token_type_vocab_size", "config", "type_vocab_size", "token_type_embedding_name", "use_position_embeddings", "True", "position_embedding_name", "initializer_range", "config", "initializer_range", "max_position_embeddings", "config", "max_position_embeddings", "dropout_prob", "config", "hidden_dropout_prob"]], "use_one_hot_embeddings": [["self", "embedding_output", "self", "embedding_table", "embedding_lookup", "input_ids", "input_ids", "vocab_size", "config", "vocab_size", "embedding_size", "config", "hidden_size", "initializer_range", "config", "initializer_range", "word_embedding_name", "use_one_hot_embeddings", "use_one_hot_embeddings"]]}, "params_p": {"args": [], "config": [["bool", 0.2951136824238741], ["dict", 0.2065280238981691]], "input_ids": [["int", 0.1996852318252456], ["List[str]", 0.11052231474507238], ["Union[numpy.ndarray, pandas.DataFrame, None]", 0.09623492904755247], ["float", 0.09615615835355289], ["bool", 0.09193974243643767]], "input_mask": [["Optional[int]", 0.28244310261993566], ["int", 0.27993987209749427], ["bytes", 0.08241262006477139]], "is_training": [["List[Dict[str, Any]]", 0.2], ["bool", 0.2], ["str", 0.2], ["Dict[str, Dict[str, Any]]", 0.1], ["int", 0.1], ["Dict[int, Set[int]]", 0.1]], "kwargs": [], "scope": [["str", 0.20452724938686095], ["int", 0.11087974758440898], ["Optional[Any]", 0.10008451911044136], ["Sequence[Any]", 0.09510590310533946]], "self": [], "token_type_ids": [["str", 0.19860192656115117], ["bool", 0.18569600449187124], ["int", 0.08623869438567698]], "use_one_hot_embeddings": [["str", 0.21434089840341178], ["Dict[str, Optional[str]]", 0.10803143230331295], ["Set[int]", 0.10078842640084067], ["List[Dict[str, Any]]", 0.09968269263397893], ["Optional[Dict]", 0.09461934959381776], ["Sequence[str]", 0.08492505393183043]]}, "q_name": "BertModel.__init__", "ret_exprs": [], "ret_type": "", "variables": {"all_encoder_layers": "", "attention_mask": "", "attention_probs_dropout_prob": "", "batch_size": "", "config": "", "embedding_output": "", "embedding_table": "", "first_token_tensor": "", "hidden_dropout_prob": "", "input_mask": "", "input_shape": "", "pooled_output": "", "seq_length": "", "sequence_output": "", "token_type_ids": ""}, "variables_p": {"all_encoder_layers": [["int", 0.13503010918124053], ["str", 0.10324995417427359]], "attention_mask": [["str", 1.0]], "attention_probs_dropout_prob": [], "batch_size": [["int", 0.5139968554780006], ["str", 0.09814347515883308], ["List[int]", 0.09110078691336647]], "config": [], "embedding_output": [["int", 0.36054782493306725], ["List[str]", 0.19036079979158332], ["Type[DefaultDict[int, Any]]", 0.16584876601709278], ["Tuple[Any, Any]", 0.08911802898712637], ["bytearray", 0.08573999171683781]], "embedding_table": [["Pattern[str]", 0.2863742745442876], ["Dict[str, str]", 0.2028359464974016], ["Dict[str, Union[Any, Any]]", 0.11961463188236734], ["bool", 0.10064087520867353], ["Mapping[str, List[Any]]", 0.09849670666508653], ["Tuple[str, str]", 0.09772976353811086], ["list", 0.09430780166407249]], "first_token_tensor": [["str", 0.8065513856888203]], "hidden_dropout_prob": [], "input_mask": [["int", 0.9084875674946296], ["str", 0.09151243250537046]], "input_shape": [["Tuple[Literal, Literal, Literal, Literal, Literal]", 0.22156542083436873], ["float", 0.19504679255675866], ["list", 0.11278637271306348], ["str", 0.09352714987521613], ["dict", 0.09146243265028275], ["int", 0.08778493844199152]], "pooled_output": [["str", 1.0]], "seq_length": [["int", 0.2564009885147201]], "sequence_output": [["str", 0.8826261074302527], ["bytes", 0.11737389256974738]], "token_type_ids": [["int", 0.9999999999999998]]}}, {"docstring": {"func": null, "long_descr": null, "ret": null}, "fn_lc": [[234, 2], [235, 29]], "fn_var_ln": {}, "fn_var_occur": {}, "name": "get_pooled_output", "params": {"self": ""}, "params_descr": {"self": ""}, "params_occur": {"self": [["self", "pooled_output"]]}, "params_p": {"args": [], "kwargs": [], "self": []}, "q_name": "BertModel.get_pooled_output", "ret_exprs": ["return self.pooled_output"], "ret_type": "", "ret_type_p": [["str", 0.6016061373748209], ["Optional[str]", 0.09937149613976985]], "variables": {}, "variables_p": {}}, {"docstring": {"func": "Gets final hidden layer of encoder.", "long_descr": null, "ret": "float Tensor of shape [batch_size, seq_length, hidden_size] corresponding\nto the final hidden of the transformer encoder."}, "fn_lc": [[237, 2], [244, 31]], "fn_var_ln": {}, "fn_var_occur": {}, "name": "get_sequence_output", "params": {"self": ""}, "params_descr": {"self": ""}, "params_occur": {"self": [["self", "sequence_output"]]}, "params_p": {"args": [], "kwargs": [], "self": []}, "q_name": "BertModel.get_sequence_output", "ret_exprs": ["return self.sequence_output"], "ret_type": "", "ret_type_p": [["str", 0.6016061373748209], ["Optional[str]", 0.09937149613976985]], "variables": {}, "variables_p": {}}, {"docstring": {"func": null, "long_descr": null, "ret": null}, "fn_lc": [[246, 2], [247, 34]], "fn_var_ln": {}, "fn_var_occur": {}, "name": "get_all_encoder_layers", "params": {"self": ""}, "params_descr": {"self": ""}, "params_occur": {"self": [["self", "all_encoder_layers"]]}, "params_p": {"args": [], "kwargs": [], "self": []}, "q_name": "BertModel.get_all_encoder_layers", "ret_exprs": ["return self.all_encoder_layers"], "ret_type": "", "ret_type_p": [["str", 0.28226203713901443], ["set", 0.09088050526806325]], "variables": {}, "variables_p": {}}, {"docstring": {"func": "Gets output of the embedding lookup (i.e., input to the transformer).", "long_descr": null, "ret": "float Tensor of shape [batch_size, seq_length, hidden_size] corresponding\nto the output of the embedding layer, after summing the word\nembeddings with the positional embeddings and the token type embeddings,\nthen performing layer normalization. This is the input to the transformer."}, "fn_lc": [[249, 2], [258, 32]], "fn_var_ln": {}, "fn_var_occur": {}, "name": "get_embedding_output", "params": {"self": ""}, "params_descr": {"self": ""}, "params_occur": {"self": [["self", "embedding_output"]]}, "params_p": {"args": [], "kwargs": [], "self": []}, "q_name": "BertModel.get_embedding_output", "ret_exprs": ["return self.embedding_output"], "ret_type": "", "ret_type_p": [["str", 0.6016061373748209], ["Optional[str]", 0.09937149613976985]], "variables": {}, "variables_p": {}}, {"docstring": {"func": null, "long_descr": null, "ret": null}, "fn_lc": [[260, 2], [261, 31]], "fn_var_ln": {}, "fn_var_occur": {}, "name": "get_embedding_table", "params": {"self": ""}, "params_descr": {"self": ""}, "params_occur": {"self": [["self", "embedding_table"]]}, "params_p": {"args": [], "kwargs": [], "self": []}, "q_name": "BertModel.get_embedding_table", "ret_exprs": ["return self.embedding_table"], "ret_type": "", "ret_type_p": [["str", 0.3979725922579269], ["bool", 0.2175315841541099], ["Optional[str]", 0.0945443171084366], ["Callable", 0.08678205201933936]], "variables": {}, "variables_p": {}}], "name": "BertModel", "q_name": "BertModel", "variables": {}, "variables_p": {}}], "funcs": [{"docstring": {"func": "Gaussian Error Linear Unit.", "long_descr": "This is a smoother version of the RELU.\nOriginal paper: https://arxiv.org/abs/1606.08415", "ret": "`x` with the GELU activation applied."}, "fn_lc": [[264, 0], [277, 16]], "fn_var_ln": {"cdf": [[275, 2], [275, 5]]}, "fn_var_occur": {"cdf": [["cdf", "tf", "tanh", "np", "sqrt", "np", "pi", "x", "tf", "pow", "x"], ["x", "cdf"]]}, "name": "gelu", "params": {"x": ""}, "params_descr": {"x": "float Tensor to perform activation."}, "params_occur": {"x": [["cdf", "tf", "tanh", "np", "sqrt", "np", "pi", "x", "tf", "pow", "x"], ["x", "cdf"]]}, "params_p": {"args": [], "kwargs": [], "x": [["float", 0.1571455921856437], ["Iterable[T]", 0.08334058402553242], ["int", 0.08334058402553242], ["Sequence[int]", 0.0823795209735388], ["Sequence[float]", 0.0823795209735388], ["numpy.ndarray", 0.07608086631816839]]}, "q_name": "gelu", "ret_exprs": ["return x * cdf"], "ret_type": "", "ret_type_p": [["str", 0.20762776128935173]], "variables": {"cdf": ""}, "variables_p": {"cdf": [["List[Tuple[str, Any]]", 0.11429091236194971], ["int", 0.10991683152071782], ["Dict[str, Union[int, str]]", 0.1000954165832847]]}}, {"docstring": {"func": "Maps a string to a Python function, e.g., \"relu\" => `tf.nn.relu`.", "long_descr": null, "ret": "A Python function corresponding to the activation function. If\n`activation_string` is None, empty, or \"linear\", this will return None.\nIf `activation_string` is not a string, it will return `activation_string`."}, "fn_lc": [[280, 0], [314, 56]], "fn_var_ln": {"act": [[304, 2], [304, 5]]}, "fn_var_occur": {"act": [["act", "activation_string", "lower"], ["ValueError", "act"]]}, "name": "get_activation", "params": {"activation_string": ""}, "params_descr": {"activation_string": "String name of the activation function."}, "params_occur": {"activation_string": [["isinstance", "activation_string", "six", "string_types"], ["act", "activation_string", "lower"]]}, "params_p": {"activation_string": [["str", 0.7961105033394064], ["Dict[str, str]", 0.0944958611691604]], "args": [], "kwargs": []}, "q_name": "get_activation", "ret_exprs": ["return activation_string", "return None", "return None", "return tf.nn.relu", "return gelu", "return tf.tanh"], "ret_type": "", "ret_type_p": [["str", 0.5316810919377711], ["bool", 0.0931023714385939]], "variables": {"act": ""}, "variables_p": {"act": [["str", 0.7564187706452322], ["int", 0.15506775020536182]]}}, {"docstring": {"func": "Compute the union of the current variables and checkpoint variables.", "long_descr": null, "ret": null}, "fn_lc": [[317, 0], [341, 53]], "fn_var_ln": {"assignment_map": [[332, 2], [332, 16]], "init_vars": [[330, 2], [330, 11]], "initialized_variable_names": [[320, 2], [320, 28]], "m": [[325, 4], [325, 5]], "name": [[334, 5], [334, 9]], "name_to_variable": [[322, 2], [322, 18]], "var": [[334, 11], [334, 14]]}, "fn_var_occur": {"assignment_map": [["assignment_map", "collections", "OrderedDict"], ["assignment_map", "name", "name"], ["assignment_map", "initialized_variable_names"]], "init_vars": [["init_vars", "tf", "train", "list_variables", "init_checkpoint"]], "initialized_variable_names": [["initialized_variable_names", "name"], ["initialized_variable_names", "name"], ["assignment_map", "initialized_variable_names"]], "m": [["m", "re", "match", "name"], ["m", "None"], ["name", "m", "group"]], "name": [["name", "var", "name"], ["m", "re", "match", "name"], ["name", "m", "group"], ["name_to_variable", "name", "var"], ["name", "var", "x", "x"], ["name", "name_to_variable"], ["assignment_map", "name", "name"], ["initialized_variable_names", "name"], ["initialized_variable_names", "name"]], "name_to_variable": [["name_to_variable", "collections", "OrderedDict"], ["name_to_variable", "name", "var"], ["name", "name_to_variable"]], "var": [["name", "var", "name"], ["name_to_variable", "name", "var"], ["name", "var", "x", "x"]]}, "name": "get_assignment_map_from_checkpoint", "params": {"init_checkpoint": "", "tvars": ""}, "params_descr": {"init_checkpoint": "", "tvars": ""}, "params_occur": {"init_checkpoint": [["init_vars", "tf", "train", "list_variables", "init_checkpoint"]], "tvars": []}, "params_p": {"args": [], "init_checkpoint": [["List[str]", 0.10375503874592772], ["numpy.ndarray", 0.099617239068255], ["str", 0.09537459739891313]], "kwargs": [], "tvars": [["bool", 0.2317337461506142], ["str", 0.228980610762764], ["Mapping[str, Any]", 0.1612563618669118], ["List[dict]", 0.08453399049654238], ["Sequence[int]", 0.07954834558631095]]}, "q_name": "get_assignment_map_from_checkpoint", "ret_exprs": ["return (assignment_map, initialized_variable_names)"], "ret_type": "", "ret_type_p": [["str", 0.21516554252540282]], "variables": {"assignment_map": "", "init_vars": "", "initialized_variable_names": "", "m": "", "name": "", "name_to_variable": "", "var": ""}, "variables_p": {"assignment_map": [["dict", 0.2898952788579564], ["str", 0.19761901817613592], ["list", 0.11072758089961006], ["Dict[str, list]", 0.1086195854249694], ["Type[Tuple[str, ...]]", 0.09096970348511244]], "init_vars": [["bool", 0.1480064910671589], ["str", 0.11764701654903283], ["Type[Dict[str, Any]]", 0.09145270243945113], ["Pattern[str]", 0.08804899669850885], ["Type[bytes]", 0.08679758425149965], ["Type[str]", 0.08679758425149965]], "initialized_variable_names": [["str", 0.5668456122997148], ["set", 0.1003580099184958], ["List[str]", 0.08988862844096884], ["Tuple[str, str, str, str]", 0.0765307368927122], ["Tuple[str, str]", 0.0765307368927122]], "m": [["str", 0.3537127881866534], ["dict", 0.18199116190301062], ["Optional[str]", 0.17062675730386911], ["List[str]", 0.11532270134888294], ["Optional[int]", 0.0904157850739573], ["int", 0.08793080618362663]], "name": [["str", 0.5711173967382678]], "name_to_variable": [["Dict[str, Any]", 0.3161089661053974], ["List[str]", 0.10747938667585483], ["Dict[Type[Any], str]", 0.10083314667557668], ["Dict[str, Literal]", 0.09177974695764668], ["dict", 0.0907591555561001]], "var": [["Dict[str, List[str]]", 0.20274943264557216], ["Pattern[str]", 0.16234569813033425], ["Dict[str, Tuple[float, float]]", 0.1507103193341132], ["Dict[str, Any]", 0.11419356424086001], ["Tuple[Literal, Literal, Literal, Literal, Literal]", 0.08271136995373861]]}}, {"docstring": {"func": "Perform dropout.", "long_descr": null, "ret": "A version of `input_tensor` with dropout applied."}, "fn_lc": [[344, 0], [359, 15]], "fn_var_ln": {"output": [[358, 2], [358, 8]]}, "fn_var_occur": {"output": [["output", "tf", "nn", "dropout", "input_tensor", "dropout_prob"]]}, "name": "dropout", "params": {"dropout_prob": "", "input_tensor": ""}, "params_descr": {"dropout_prob": "Python float. The probability of dropping out a value (NOT of\n*keeping* a dimension as in `tf.nn.dropout`).", "input_tensor": "float Tensor."}, "params_occur": {"dropout_prob": [["dropout_prob", "None", "dropout_prob"], ["output", "tf", "nn", "dropout", "input_tensor", "dropout_prob"]], "input_tensor": [["output", "tf", "nn", "dropout", "input_tensor", "dropout_prob"]]}, "params_p": {"args": [], "dropout_prob": [["str", 0.1630902785528715]], "input_tensor": [["int", 0.30000000000000004], ["Callable", 0.2], ["str", 0.1]], "kwargs": []}, "q_name": "dropout", "ret_exprs": ["return input_tensor", "return output"], "ret_type": "", "ret_type_p": [["List[int]", 0.2548270775300986], ["str", 0.2548270775300986]], "variables": {"output": ""}, "variables_p": {"output": [["str", 0.4000000000366222], ["List[str]", 0.19999999999498208], ["bytes", 0.19999999998419793]]}}, {"docstring": {"func": "Run layer normalization on the last dimension of the tensor.", "long_descr": null, "ret": null}, "fn_lc": [[362, 0], [365, 80]], "fn_var_ln": {}, "fn_var_occur": {}, "name": "layer_norm", "params": {"input_tensor": "", "name": ""}, "params_descr": {"input_tensor": "", "name": ""}, "params_occur": {"input_tensor": [["tf", "contrib", "layers", "layer_norm", "inputs", "input_tensor", "begin_norm_axis", "begin_params_axis", "scope", "name"]], "name": [["tf", "contrib", "layers", "layer_norm", "inputs", "input_tensor", "begin_norm_axis", "begin_params_axis", "scope", "name"]]}, "params_p": {"args": [], "input_tensor": [["str", 0.30493088991305634], ["Optional[str]", 0.11166592925611193]], "kwargs": [], "name": [["str", 0.30493088991305634], ["Optional[str]", 0.11166592925611193]]}, "q_name": "layer_norm", "ret_exprs": ["return tf.contrib.layers.layer_norm( inputs=input_tensor, begin_norm_axis=-1, begin_params_axis=-1, scope=name)"], "ret_type": "", "ret_type_p": [["int", 0.30000000000000004], ["Callable", 0.2], ["str", 0.1]], "variables": {}, "variables_p": {}}, {"docstring": {"func": "Runs layer normalization followed by dropout.", "long_descr": null, "ret": null}, "fn_lc": [[368, 0], [372, 22]], "fn_var_ln": {"output_tensor": [[371, 2], [371, 15]]}, "fn_var_occur": {"output_tensor": [["output_tensor", "layer_norm", "input_tensor", "name"], ["output_tensor", "dropout", "output_tensor", "dropout_prob"]]}, "name": "layer_norm_and_dropout", "params": {"dropout_prob": "", "input_tensor": "", "name": ""}, "params_descr": {"dropout_prob": "", "input_tensor": "", "name": ""}, "params_occur": {"dropout_prob": [["output_tensor", "dropout", "output_tensor", "dropout_prob"]], "input_tensor": [["output_tensor", "layer_norm", "input_tensor", "name"]], "name": [["output_tensor", "layer_norm", "input_tensor", "name"]]}, "params_p": {"args": [], "dropout_prob": [["str", 0.7827990388297654], ["int", 0.11172880927312036]], "input_tensor": [["str", 0.2885404914607003], ["Optional[dict]", 0.1110385250086123], ["dict", 0.10512332050489798], ["bool", 0.09529201403625093], ["Dict[str, Any]", 0.09507216068676701]], "kwargs": [], "name": [["str", 0.2885404914607003], ["Optional[dict]", 0.1110385250086123], ["dict", 0.10512332050489798], ["bool", 0.09529201403625093], ["Dict[str, Any]", 0.09507216068676701]]}, "q_name": "layer_norm_and_dropout", "ret_exprs": ["return output_tensor"], "ret_type": "", "ret_type_p": [["str", 0.5242230958551379], ["int", 0.17440464992359625], ["Optional[str]", 0.1048446191710276]], "variables": {"output_tensor": ""}, "variables_p": {"output_tensor": [["int", 0.20409479020441412], ["str", 0.18106626261712871], ["dict", 0.12062850139899378], ["TextIO", 0.10700113372302937], ["set", 0.1016434674589894], ["bytes", 0.09872192137592396], ["List[int]", 0.09618692809520632], ["bool", 0.09065699512631421]]}}, {"docstring": {"func": "Creates a `truncated_normal_initializer` with the given range.", "long_descr": null, "ret": null}, "fn_lc": [[375, 0], [377, 66]], "fn_var_ln": {}, "fn_var_occur": {}, "name": "create_initializer", "params": {"initializer_range": ""}, "params_descr": {"initializer_range": ""}, "params_occur": {"initializer_range": [["tf", "truncated_normal_initializer", "stddev", "initializer_range"]]}, "params_p": {"args": [], "initializer_range": [["int", 0.6224737636670586], ["str", 0.1328351520820676]], "kwargs": []}, "q_name": "create_initializer", "ret_exprs": ["return tf.truncated_normal_initializer(stddev=initializer_range)"], "ret_type": "", "ret_type_p": [["Dict[str, Any]", 0.09087262995687148], ["int", 0.07305665011158462], ["str", 0.07121374608700556]], "variables": {}, "variables_p": {}}, {"docstring": {"func": "Looks up words embeddings for id tensor.", "long_descr": null, "ret": "float Tensor of shape [batch_size, seq_length, embedding_size]."}, "fn_lc": [[380, 0], [425, 34]], "fn_var_ln": {"embedding_table": [[409, 2], [409, 17]], "flat_input_ids": [[414, 2], [414, 16]], "input_ids": [[407, 4], [407, 13]], "input_shape": [[421, 2], [421, 13]], "one_hot_input_ids": [[416, 4], [416, 21]], "output": [[423, 2], [423, 8]]}, "fn_var_occur": {"embedding_table": [["embedding_table", "tf", "get_variable", "name", "word_embedding_name", "shape", "vocab_size", "embedding_size", "initializer", "create_initializer", "initializer_range"], ["output", "tf", "matmul", "one_hot_input_ids", "embedding_table"], ["output", "tf", "gather", "embedding_table", "flat_input_ids"], ["output", "embedding_table"]], "flat_input_ids": [["flat_input_ids", "tf", "reshape", "input_ids"], ["one_hot_input_ids", "tf", "one_hot", "flat_input_ids", "depth", "vocab_size"], ["output", "tf", "gather", "embedding_table", "flat_input_ids"]], "input_ids": [["input_ids", "shape", "ndims"], ["input_ids", "tf", "expand_dims", "input_ids", "axis"], ["flat_input_ids", "tf", "reshape", "input_ids"], ["input_shape", "get_shape_list", "input_ids"]], "input_shape": [["input_shape", "get_shape_list", "input_ids"], ["output", "tf", "reshape", "output", "input_shape", "input_shape", "embedding_size"]], "one_hot_input_ids": [["one_hot_input_ids", "tf", "one_hot", "flat_input_ids", "depth", "vocab_size"], ["output", "tf", "matmul", "one_hot_input_ids", "embedding_table"]], "output": [["output", "tf", "matmul", "one_hot_input_ids", "embedding_table"], ["output", "tf", "gather", "embedding_table", "flat_input_ids"], ["output", "tf", "reshape", "output", "input_shape", "input_shape", "embedding_size"], ["output", "embedding_table"]]}, "name": "embedding_lookup", "params": {"embedding_size": "", "initializer_range": "", "input_ids": "", "use_one_hot_embeddings": "", "vocab_size": "", "word_embedding_name": ""}, "params_descr": {"embedding_size": "int. Width of the word embeddings.", "initializer_range": "float. Embedding initialization range.", "input_ids": "int32 Tensor of shape [batch_size, seq_length] containing word\nids.", "use_one_hot_embeddings": "bool. If True, use one-hot method for word\nembeddings. If False, use `tf.gather()`.", "vocab_size": "int. Size of the embedding vocabulary.", "word_embedding_name": "string. Name of the embedding table."}, "params_occur": {"embedding_size": [["embedding_table", "tf", "get_variable", "name", "word_embedding_name", "shape", "vocab_size", "embedding_size", "initializer", "create_initializer", "initializer_range"], ["output", "tf", "reshape", "output", "input_shape", "input_shape", "embedding_size"]], "initializer_range": [["embedding_table", "tf", "get_variable", "name", "word_embedding_name", "shape", "vocab_size", "embedding_size", "initializer", "create_initializer", "initializer_range"]], "input_ids": [["input_ids", "shape", "ndims"], ["input_ids", "tf", "expand_dims", "input_ids", "axis"], ["flat_input_ids", "tf", "reshape", "input_ids"], ["input_shape", "get_shape_list", "input_ids"]], "use_one_hot_embeddings": [], "vocab_size": [["embedding_table", "tf", "get_variable", "name", "word_embedding_name", "shape", "vocab_size", "embedding_size", "initializer", "create_initializer", "initializer_range"], ["one_hot_input_ids", "tf", "one_hot", "flat_input_ids", "depth", "vocab_size"]], "word_embedding_name": [["embedding_table", "tf", "get_variable", "name", "word_embedding_name", "shape", "vocab_size", "embedding_size", "initializer", "create_initializer", "initializer_range"]]}, "params_p": {"args": [], "embedding_size": [["int", 0.6981305862685214], ["bool", 0.10947790387003369], ["Optional[int]", 0.10144104698588023]], "initializer_range": [["int", 0.7685035837654056], ["Optional[int]", 0.1292735133138883], ["bool", 0.1022229029207063]], "input_ids": [["int", 0.47394998984141035], ["List[int]", 0.12163041121939244], ["numpy.ndarray", 0.10325455537912608], ["Optional[str]", 0.09478999796828207]], "kwargs": [], "use_one_hot_embeddings": [["int", 0.4144635470839171], ["str", 0.30467979919804394], ["Any", 0.05669343008927416]], "vocab_size": [["int", 0.7685035837654056], ["Optional[int]", 0.1292735133138883], ["bool", 0.1022229029207063]], "word_embedding_name": [["int", 0.7685035837654056], ["Optional[int]", 0.1292735133138883], ["bool", 0.1022229029207063]]}, "q_name": "embedding_lookup", "ret_exprs": ["return (output, embedding_table)"], "ret_type": "", "ret_type_p": [["str", 0.30291272639072375], ["List[str]", 0.10290980733090482], ["int", 0.09677670286019735], ["numpy.ndarray", 0.09540704197741343]], "variables": {"embedding_table": "", "flat_input_ids": "", "input_ids": "", "input_shape": "", "one_hot_input_ids": "", "output": ""}, "variables_p": {"embedding_table": [["int", 0.6911436914852347], ["Tuple[str, str, str, str, str, str, str, str, str, str, str, str]", 0.0971917084718992], ["Dict[str, Union[Any, Any]]", 0.09584805317217579]], "flat_input_ids": [["Dict[str, str]", 0.5426393194352102], ["int", 0.2440881024253077], ["str", 0.0749280776853535], ["Optional[int]", 0.06376045849437091]], "input_ids": [["str", 0.23864450853663738], ["int", 0.14689989797034975], ["dict", 0.07209695331498753]], "input_shape": [["int", 0.43261139709510643], ["Dict[str, Dict[str, str]]", 0.17128454413683303], ["list", 0.11564904725180811]], "one_hot_input_ids": [["int", 0.6622143179892994], ["str", 0.1538606597030633], ["bool", 0.09830331000575003]], "output": [["Dict[str, str]", 0.11298815218223573], ["str", 0.1007476101751813], ["Dict[str, Type[Any]]", 0.08007810295554897], ["float", 0.0762292394657309]]}}, {"docstring": {"func": "Performs various post-processing on a word embedding tensor.", "long_descr": null, "ret": "float tensor with same shape as `input_tensor`."}, "fn_lc": [[428, 0], [521, 15]], "fn_var_ln": {"assert_op": [[490, 4], [490, 13]], "batch_size": [[466, 2], [466, 12]], "flat_token_type_ids": [[482, 4], [482, 23]], "full_position_embeddings": [[492, 6], [492, 30]], "input_shape": [[465, 2], [465, 13]], "num_dims": [[507, 6], [507, 14]], "one_hot_ids": [[483, 4], [483, 15]], "output": [[520, 2], [520, 8]], "position_broadcast_shape": [[512, 6], [512, 30]], "position_embeddings": [[516, 6], [516, 25]], "seq_length": [[467, 2], [467, 12]], "token_type_embeddings": [[485, 4], [485, 25]], "token_type_table": [[476, 4], [476, 20]], "width": [[468, 2], [468, 7]]}, "fn_var_occur": {"assert_op": [["assert_op", "tf", "assert_less_equal", "seq_length", "max_position_embeddings"], ["tf", "control_dependencies", "assert_op"]], "batch_size": [["batch_size", "input_shape"], ["token_type_embeddings", "tf", "reshape", "token_type_embeddings", "batch_size", "seq_length", "width"]], "flat_token_type_ids": [["flat_token_type_ids", "tf", "reshape", "token_type_ids"], ["one_hot_ids", "tf", "one_hot", "flat_token_type_ids", "depth", "token_type_vocab_size"]], "full_position_embeddings": [["full_position_embeddings", "tf", "get_variable", "name", "position_embedding_name", "shape", "max_position_embeddings", "width", "initializer", "create_initializer", "initializer_range"], ["position_embeddings", "tf", "slice", "full_position_embeddings", "seq_length"]], "input_shape": [["input_shape", "get_shape_list", "input_tensor", "expected_rank"], ["batch_size", "input_shape"], ["seq_length", "input_shape"], ["width", "input_shape"]], "num_dims": [["num_dims", "len", "output", "shape", "as_list"], ["range", "num_dims"]], "one_hot_ids": [["one_hot_ids", "tf", "one_hot", "flat_token_type_ids", "depth", "token_type_vocab_size"], ["token_type_embeddings", "tf", "matmul", "one_hot_ids", "token_type_table"]], "output": [["output", "input_tensor"], ["output", "token_type_embeddings"], ["num_dims", "len", "output", "shape", "as_list"], ["output", "position_embeddings"], ["output", "layer_norm_and_dropout", "output", "dropout_prob"]], "position_broadcast_shape": [["position_broadcast_shape", "append"], ["position_broadcast_shape", "extend", "seq_length", "width"], ["position_embeddings", "tf", "reshape", "position_embeddings", "position_broadcast_shape"]], "position_embeddings": [["position_embeddings", "tf", "slice", "full_position_embeddings", "seq_length"], ["position_embeddings", "tf", "reshape", "position_embeddings", "position_broadcast_shape"], ["output", "position_embeddings"]], "seq_length": [["seq_length", "input_shape"], ["token_type_embeddings", "tf", "reshape", "token_type_embeddings", "batch_size", "seq_length", "width"], ["assert_op", "tf", "assert_less_equal", "seq_length", "max_position_embeddings"], ["position_embeddings", "tf", "slice", "full_position_embeddings", "seq_length"], ["position_broadcast_shape", "extend", "seq_length", "width"]], "token_type_embeddings": [["token_type_embeddings", "tf", "matmul", "one_hot_ids", "token_type_table"], ["token_type_embeddings", "tf", "reshape", "token_type_embeddings", "batch_size", "seq_length", "width"], ["output", "token_type_embeddings"]], "token_type_table": [["token_type_table", "tf", "get_variable", "name", "token_type_embedding_name", "shape", "token_type_vocab_size", "width", "initializer", "create_initializer", "initializer_range"], ["token_type_embeddings", "tf", "matmul", "one_hot_ids", "token_type_table"]], "width": [["width", "input_shape"], ["token_type_table", "tf", "get_variable", "name", "token_type_embedding_name", "shape", "token_type_vocab_size", "width", "initializer", "create_initializer", "initializer_range"], ["token_type_embeddings", "tf", "reshape", "token_type_embeddings", "batch_size", "seq_length", "width"], ["full_position_embeddings", "tf", "get_variable", "name", "position_embedding_name", "shape", "max_position_embeddings", "width", "initializer", "create_initializer", "initializer_range"], ["position_broadcast_shape", "extend", "seq_length", "width"]]}, "name": "embedding_postprocessor", "params": {"dropout_prob": "", "initializer_range": "", "input_tensor": "", "max_position_embeddings": "", "position_embedding_name": "", "token_type_embedding_name": "", "token_type_ids": "", "token_type_vocab_size": "", "use_position_embeddings": "", "use_token_type": ""}, "params_descr": {"dropout_prob": "float. Dropout probability applied to the final output tensor.", "initializer_range": "float. Range of the weight initialization.", "input_tensor": "float Tensor of shape [batch_size, seq_length,\nembedding_size].", "max_position_embeddings": "int. Maximum sequence length that might ever be\nused with this model. This can be longer than the sequence length of\ninput_tensor, but cannot be shorter.", "position_embedding_name": "string. The name of the embedding table variable\nfor positional embeddings.", "token_type_embedding_name": "string. The name of the embedding table variable\nfor token type ids.", "token_type_ids": "(optional) int32 Tensor of shape [batch_size, seq_length].\nMust be specified if `use_token_type` is True.", "token_type_vocab_size": "int. The vocabulary size of `token_type_ids`.", "use_position_embeddings": "bool. Whether to add position embeddings for the\nposition of each token in the sequence.", "use_token_type": "bool. Whether to add embeddings for `token_type_ids`."}, "params_occur": {"dropout_prob": [["output", "layer_norm_and_dropout", "output", "dropout_prob"]], "initializer_range": [["token_type_table", "tf", "get_variable", "name", "token_type_embedding_name", "shape", "token_type_vocab_size", "width", "initializer", "create_initializer", "initializer_range"], ["full_position_embeddings", "tf", "get_variable", "name", "position_embedding_name", "shape", "max_position_embeddings", "width", "initializer", "create_initializer", "initializer_range"]], "input_tensor": [["input_shape", "get_shape_list", "input_tensor", "expected_rank"], ["output", "input_tensor"]], "max_position_embeddings": [["assert_op", "tf", "assert_less_equal", "seq_length", "max_position_embeddings"], ["full_position_embeddings", "tf", "get_variable", "name", "position_embedding_name", "shape", "max_position_embeddings", "width", "initializer", "create_initializer", "initializer_range"]], "position_embedding_name": [["full_position_embeddings", "tf", "get_variable", "name", "position_embedding_name", "shape", "max_position_embeddings", "width", "initializer", "create_initializer", "initializer_range"]], "token_type_embedding_name": [["token_type_table", "tf", "get_variable", "name", "token_type_embedding_name", "shape", "token_type_vocab_size", "width", "initializer", "create_initializer", "initializer_range"]], "token_type_ids": [["token_type_ids", "None"], ["flat_token_type_ids", "tf", "reshape", "token_type_ids"]], "token_type_vocab_size": [["token_type_table", "tf", "get_variable", "name", "token_type_embedding_name", "shape", "token_type_vocab_size", "width", "initializer", "create_initializer", "initializer_range"], ["one_hot_ids", "tf", "one_hot", "flat_token_type_ids", "depth", "token_type_vocab_size"]], "use_position_embeddings": [], "use_token_type": []}, "params_p": {"args": [], "dropout_prob": [["str", 0.4181337437762911], ["bool", 0.3890140865577496], ["int", 0.10453343594407277]], "initializer_range": [["bool", 0.3968627243796325], ["int", 0.10679537099883796], ["List[str]", 0.10010904114146858], ["Iterable[str]", 0.09659191630137472]], "input_tensor": [["Optional[int]", 0.32355904598315416], ["bool", 0.32355904598315416], ["Callable", 0.08800629335222432], ["int", 0.0838854156940383]], "kwargs": [], "max_position_embeddings": [["int", 0.2999776263143127], ["str", 0.19397619482868753], ["list", 0.10108975002580331]], "position_embedding_name": [["int", 0.21117430812051613], ["bool", 0.2029162226572972], ["str", 0.09375357252648356], ["Optional[int]", 0.0874957396036591]], "token_type_embedding_name": [["bool", 0.3968627243796325], ["int", 0.10679537099883796], ["List[str]", 0.10010904114146858], ["Iterable[str]", 0.09659191630137472]], "token_type_ids": [["int", 0.20822452385662762], ["Optional[int]", 0.10072643296330033], ["Optional[str]", 0.08915369196119], ["Optional[Dict[str, Any]]", 0.08915369196119]], "token_type_vocab_size": [["bool", 0.3968627243796325], ["int", 0.10679537099883796], ["List[str]", 0.10010904114146858], ["Iterable[str]", 0.09659191630137472]], "use_position_embeddings": [["bool", 0.6954209665684478], ["str", 0.10784023876551671], ["Callable[[], Iterator[Any]]", 0.0986756395035639]], "use_token_type": [["bool", 0.6954209665684478], ["str", 0.10784023876551671], ["Callable[[], Iterator[Any]]", 0.0986756395035639]]}, "q_name": "embedding_postprocessor", "ret_exprs": ["return output"], "ret_type": "", "ret_type_p": [["str", 0.5242230958551379], ["int", 0.17440464992359625], ["Optional[str]", 0.1048446191710276]], "variables": {"assert_op": "", "batch_size": "", "flat_token_type_ids": "", "full_position_embeddings": "", "input_shape": "", "num_dims": "", "one_hot_ids": "", "output": "", "position_broadcast_shape": "", "position_embeddings": "", "seq_length": "", "token_type_embeddings": "", "token_type_table": "", "width": ""}, "variables_p": {"assert_op": [["int", 0.17634487507793573], ["List[List[float]]", 0.1264899738230776], ["Dict[str, str]", 0.10445027294183398], ["List[str]", 0.0961826373264514], ["Dict[str, Optional[Any]]", 0.08968325555337049]], "batch_size": [["int", 0.5247283504635907], ["float", 0.3847174883092145], ["Sequence[Callable]", 0.09055416122719498]], "flat_token_type_ids": [["str", 0.28225771894135177], ["int", 0.27288415277394323]], "full_position_embeddings": [["int", 0.3399958460852706]], "input_shape": [["int", 0.6178463675723799], ["six.moves.range", 0.11351358592930012], ["str", 0.09623469481934291], ["Tuple[Any, Any, Any, Any]", 0.08535968541166164]], "num_dims": [["int", 0.9223905049764372], ["float", 0.0776094950235628]], "one_hot_ids": [["int", 0.27273462165980944], ["str", 0.1311054317131108], ["Pattern[Any]", 0.1253484982027397]], "output": [["set", 0.4094772715642654], ["list", 0.09448479045214675]], "position_broadcast_shape": [["list", 0.6793646808421471], ["List[bytes]", 0.17195419271351872], ["int", 0.08095992250871055], ["List[str]", 0.06772120393562367]], "position_embeddings": [["List[str]", 0.13586444418187094], ["bool", 0.08172524503603275]], "seq_length": [], "token_type_embeddings": [["str", 0.523096022112451], ["Type[Callable[..., None]]", 0.1089440833164919], ["List[Tuple[str, str]]", 0.09877421735715976], ["List[Tuple[str, Any]]", 0.09460407691253991], ["Union[Tuple[Any], tuple]", 0.08959976879940233]], "token_type_table": [["int", 0.5586351245600242], ["Pattern[str]", 0.11105198069589697], ["str", 0.08949974155293385]], "width": [["str", 0.1464541012724487], ["set", 0.092390855912996]]}}, {"docstring": {"func": "Create 3D attention mask from a 2D tensor mask.", "long_descr": null, "ret": "float Tensor of shape [batch_size, from_seq_length, to_seq_length]."}, "fn_lc": [[524, 0], [555, 13]], "fn_var_ln": {"batch_size": [[535, 2], [535, 12]], "broadcast_ones": [[549, 2], [549, 16]], "from_seq_length": [[536, 2], [536, 17]], "from_shape": [[534, 2], [534, 12]], "mask": [[553, 2], [553, 6]], "to_mask": [[541, 2], [541, 9]], "to_seq_length": [[539, 2], [539, 15]], "to_shape": [[538, 2], [538, 10]]}, "fn_var_occur": {"batch_size": [["batch_size", "from_shape"], ["to_mask", "tf", "cast", "tf", "reshape", "to_mask", "batch_size", "to_seq_length", "tf", "float32"], ["broadcast_ones", "tf", "ones", "shape", "batch_size", "from_seq_length", "dtype", "tf", "float32"]], "broadcast_ones": [["broadcast_ones", "tf", "ones", "shape", "batch_size", "from_seq_length", "dtype", "tf", "float32"], ["mask", "broadcast_ones", "to_mask"]], "from_seq_length": [["from_seq_length", "from_shape"], ["broadcast_ones", "tf", "ones", "shape", "batch_size", "from_seq_length", "dtype", "tf", "float32"]], "from_shape": [["from_shape", "get_shape_list", "from_tensor", "expected_rank"], ["batch_size", "from_shape"], ["from_seq_length", "from_shape"]], "mask": [["mask", "broadcast_ones", "to_mask"]], "to_mask": [["to_shape", "get_shape_list", "to_mask", "expected_rank"], ["to_mask", "tf", "cast", "tf", "reshape", "to_mask", "batch_size", "to_seq_length", "tf", "float32"], ["mask", "broadcast_ones", "to_mask"]], "to_seq_length": [["to_seq_length", "to_shape"], ["to_mask", "tf", "cast", "tf", "reshape", "to_mask", "batch_size", "to_seq_length", "tf", "float32"]], "to_shape": [["to_shape", "get_shape_list", "to_mask", "expected_rank"], ["to_seq_length", "to_shape"]]}, "name": "create_attention_mask_from_input_mask", "params": {"from_tensor": "", "to_mask": ""}, "params_descr": {"from_tensor": "2D or 3D Tensor of shape [batch_size, from_seq_length, ...].", "to_mask": "int32 Tensor of shape [batch_size, to_seq_length]."}, "params_occur": {"from_tensor": [["from_shape", "get_shape_list", "from_tensor", "expected_rank"]], "to_mask": [["to_shape", "get_shape_list", "to_mask", "expected_rank"], ["to_mask", "tf", "cast", "tf", "reshape", "to_mask", "batch_size", "to_seq_length", "tf", "float32"], ["mask", "broadcast_ones", "to_mask"]]}, "params_p": {"args": [], "from_tensor": [["int", 0.12269944581162541], ["List[int]", 0.09094850618363569], ["Tuple[int]", 0.09094850618363569]], "kwargs": [], "to_mask": [["numpy.ndarray", 0.11443126953623477], ["int", 0.10639840273886901]]}, "q_name": "create_attention_mask_from_input_mask", "ret_exprs": ["return mask"], "ret_type": "", "ret_type_p": [["bool", 0.520343721516813], ["float", 0.09593125569663735]], "variables": {"batch_size": "", "broadcast_ones": "", "from_seq_length": "", "from_shape": "", "mask": "", "to_mask": "", "to_seq_length": "", "to_shape": ""}, "variables_p": {"batch_size": [["List[int]", 0.15930487799351747], ["str", 0.09173491467614903], ["float", 0.07965243899675874], ["int", 0.07965243899675874], ["Type[Tuple[int, int]]", 0.07965243899675874]], "broadcast_ones": [["List[float]", 0.18555698215950686], ["int", 0.15746237806947588], ["dict", 0.08076782263432596], ["bytes", 0.08059481471741704]], "from_seq_length": [["int", 0.7835436898833583], ["Tuple[str, str, str, str, str]", 0.11973663880441045]], "from_shape": [["int", 0.3939542568944029], ["float", 0.20463911682635375], ["List[List[int]]", 0.19748005564034446], ["str", 0.10349803150055624], ["List[str]", 0.10042853913834268]], "mask": [["int", 0.2], ["dict", 0.1], ["List[int]", 0.1], ["List[List[int]]", 0.1], ["str", 0.1]], "to_mask": [["int", 0.4785341868872949], ["str", 0.3086707098061725], ["Type[DefaultDict[int, Any]]", 0.10648878327105003], ["numpy.array", 0.1063063200354825]], "to_seq_length": [["float", 0.2966712724700055], ["int", 0.29348070639013935], ["list", 0.10459010990169497], ["Union[Literal, Literal]", 0.10205526502716547]], "to_shape": [["str", 0.6272869458299286], ["Type[DefaultDict[int, Any]]", 0.10696259349203165], ["bytes", 0.09637200233237475], ["int", 0.08506645158774472], ["list", 0.08431200675792022]]}}, {"docstring": {"func": null, "long_descr": null, "ret": null}, "fn_lc": [[629, 2], [635, 24]], "fn_var_ln": {"output_tensor": [[634, 4], [634, 17]]}, "fn_var_occur": {"output_tensor": [["output_tensor", "tf", "reshape", "input_tensor", "batch_size", "seq_length", "num_attention_heads", "width"], ["output_tensor", "tf", "transpose", "output_tensor"]]}, "name": "transpose_for_scores", "params": {"batch_size": "", "input_tensor": "", "num_attention_heads": "", "seq_length": "", "width": ""}, "params_descr": {"batch_size": "", "input_tensor": "", "num_attention_heads": "", "seq_length": "", "width": ""}, "params_occur": {"batch_size": [["output_tensor", "tf", "reshape", "input_tensor", "batch_size", "seq_length", "num_attention_heads", "width"]], "input_tensor": [["output_tensor", "tf", "reshape", "input_tensor", "batch_size", "seq_length", "num_attention_heads", "width"]], "num_attention_heads": [["output_tensor", "tf", "reshape", "input_tensor", "batch_size", "seq_length", "num_attention_heads", "width"]], "seq_length": [["output_tensor", "tf", "reshape", "input_tensor", "batch_size", "seq_length", "num_attention_heads", "width"]], "width": [["output_tensor", "tf", "reshape", "input_tensor", "batch_size", "seq_length", "num_attention_heads", "width"]]}, "params_p": {"args": [], "batch_size": [["int", 0.30142020657898055], ["bool", 0.19834058205263938], ["float", 0.19834058205263938], ["Union[Tuple[int, int], int]", 0.09842442022553376]], "input_tensor": [["int", 0.30142020657898055], ["bool", 0.19834058205263938], ["float", 0.19834058205263938], ["Union[Tuple[int, int], int]", 0.09842442022553376]], "kwargs": [], "num_attention_heads": [["int", 0.30142020657898055], ["bool", 0.19834058205263938], ["float", 0.19834058205263938], ["Union[Tuple[int, int], int]", 0.09842442022553376]], "seq_length": [["int", 0.30142020657898055], ["bool", 0.19834058205263938], ["float", 0.19834058205263938], ["Union[Tuple[int, int], int]", 0.09842442022553376]], "width": [["int", 0.30142020657898055], ["bool", 0.19834058205263938], ["float", 0.19834058205263938], ["Union[Tuple[int, int], int]", 0.09842442022553376]]}, "q_name": "attention_layer.<locals>.transpose_for_scores", "ret_exprs": ["return output_tensor"], "ret_type": "", "ret_type_p": [["str", 0.6080941756949936], ["Optional[str]", 0.10316684915602378], ["Set[str]", 0.09761960006016838]], "variables": {"output_tensor": ""}, "variables_p": {"output_tensor": [["int", 0.4146570201410068], ["six.moves.range", 0.2677063614353046], ["range", 0.2312795657655747], ["List[Tuple[str, Any]]", 0.08635705265811411]]}}, {"docstring": {"func": "Performs multi-headed attention from `from_tensor` to `to_tensor`.", "long_descr": "This is an implementation of multi-headed attention based on \"Attention\nis all you Need\". If `from_tensor` and `to_tensor` are the same, then\nthis is self-attention. Each timestep in `from_tensor` attends to the\ncorresponding sequence in `to_tensor`, and returns a fixed-with vector.\n\nThis function first projects `from_tensor` into a \"query\" tensor and\n`to_tensor` into \"key\" and \"value\" tensors. These are (effectively) a list\nof tensors of length `num_attention_heads`, where each tensor is of shape\n[batch_size, seq_length, size_per_head].\n\nThen, the query and key tensors are dot-producted and scaled. These are\nsoftmaxed to obtain attention probabilities. The value tensors are then\ninterpolated by these probabilities, then concatenated back to a single\ntensor and returned.\n\nIn practice, the multi-headed attention are done with transposes and\nreshapes rather than actual separate tensors.", "ret": "float Tensor of shape [batch_size, from_seq_length,\nnum_attention_heads * size_per_head]. (If `do_return_2d_tensor` is\ntrue, this will be of shape [batch_size * from_seq_length,\nnum_attention_heads * size_per_head])."}, "fn_lc": [[558, 0], [751, 22]], "fn_var_ln": {"adder": [[712, 4], [712, 9]], "attention_mask": [[707, 4], [707, 18]], "attention_probs": [[724, 2], [724, 17]], "attention_scores": [[702, 2], [702, 18]], "batch_size": [[645, 4], [645, 14]], "context_layer": [[747, 4], [747, 17]], "from_seq_length": [[646, 4], [646, 19]], "from_shape": [[637, 2], [637, 12]], "from_tensor_2d": [[662, 2], [662, 16]], "key_layer": [[695, 2], [695, 11]], "query_layer": [[690, 2], [690, 13]], "to_seq_length": [[647, 4], [647, 17]], "to_shape": [[638, 2], [638, 10]], "to_tensor_2d": [[663, 2], [663, 14]], "value_layer": [[732, 2], [732, 13]]}, "fn_var_occur": {"adder": [["adder", "tf", "cast", "attention_mask", "tf", "float32"], ["attention_scores", "adder"]], "attention_mask": [["attention_mask", "None"], ["attention_mask", "tf", "expand_dims", "attention_mask", "axis"], ["adder", "tf", "cast", "attention_mask", "tf", "float32"]], "attention_probs": [["attention_probs", "tf", "nn", "softmax", "attention_scores"], ["attention_probs", "dropout", "attention_probs", "attention_probs_dropout_prob"], ["context_layer", "tf", "matmul", "attention_probs", "value_layer"]], "attention_scores": [["attention_scores", "tf", "matmul", "query_layer", "key_layer", "transpose_b", "True"], ["attention_scores", "tf", "multiply", "attention_scores", "math", "sqrt", "float", "size_per_head"], ["attention_scores", "adder"], ["attention_probs", "tf", "nn", "softmax", "attention_scores"]], "batch_size": [["batch_size", "from_shape"], ["batch_size", "None", "from_seq_length", "None", "to_seq_length", "None"], ["query_layer", "transpose_for_scores", "query_layer", "batch_size", "num_attention_heads", "from_seq_length", "size_per_head"], ["key_layer", "transpose_for_scores", "key_layer", "batch_size", "num_attention_heads", "to_seq_length", "size_per_head"], ["value_layer", "tf", "reshape", "value_layer", "batch_size", "to_seq_length", "num_attention_heads", "size_per_head"], ["context_layer", "tf", "reshape", "context_layer", "batch_size", "from_seq_length", "num_attention_heads", "size_per_head"], ["context_layer", "tf", "reshape", "context_layer", "batch_size", "from_seq_length", "num_attention_heads", "size_per_head"]], "context_layer": [["context_layer", "tf", "matmul", "attention_probs", "value_layer"], ["context_layer", "tf", "transpose", "context_layer"], ["context_layer", "tf", "reshape", "context_layer", "batch_size", "from_seq_length", "num_attention_heads", "size_per_head"], ["context_layer", "tf", "reshape", "context_layer", "batch_size", "from_seq_length", "num_attention_heads", "size_per_head"]], "from_seq_length": [["from_seq_length", "from_shape"], ["batch_size", "None", "from_seq_length", "None", "to_seq_length", "None"], ["query_layer", "transpose_for_scores", "query_layer", "batch_size", "num_attention_heads", "from_seq_length", "size_per_head"], ["context_layer", "tf", "reshape", "context_layer", "batch_size", "from_seq_length", "num_attention_heads", "size_per_head"], ["context_layer", "tf", "reshape", "context_layer", "batch_size", "from_seq_length", "num_attention_heads", "size_per_head"]], "from_shape": [["from_shape", "get_shape_list", "from_tensor", "expected_rank"], ["len", "from_shape", "len", "to_shape"], ["len", "from_shape"], ["batch_size", "from_shape"], ["from_seq_length", "from_shape"], ["len", "from_shape"]], "from_tensor_2d": [["from_tensor_2d", "reshape_to_matrix", "from_tensor"], ["query_layer", "tf", "layers", "dense", "from_tensor_2d", "num_attention_heads", "size_per_head", "activation", "query_act", "name", "kernel_initializer", "create_initializer", "initializer_range"]], "key_layer": [["key_layer", "tf", "layers", "dense", "to_tensor_2d", "num_attention_heads", "size_per_head", "activation", "key_act", "name", "kernel_initializer", "create_initializer", "initializer_range"], ["key_layer", "transpose_for_scores", "key_layer", "batch_size", "num_attention_heads", "to_seq_length", "size_per_head"], ["attention_scores", "tf", "matmul", "query_layer", "key_layer", "transpose_b", "True"]], "query_layer": [["query_layer", "tf", "layers", "dense", "from_tensor_2d", "num_attention_heads", "size_per_head", "activation", "query_act", "name", "kernel_initializer", "create_initializer", "initializer_range"], ["query_layer", "transpose_for_scores", "query_layer", "batch_size", "num_attention_heads", "from_seq_length", "size_per_head"], ["attention_scores", "tf", "matmul", "query_layer", "key_layer", "transpose_b", "True"]], "to_seq_length": [["to_seq_length", "to_shape"], ["batch_size", "None", "from_seq_length", "None", "to_seq_length", "None"], ["key_layer", "transpose_for_scores", "key_layer", "batch_size", "num_attention_heads", "to_seq_length", "size_per_head"], ["value_layer", "tf", "reshape", "value_layer", "batch_size", "to_seq_length", "num_attention_heads", "size_per_head"]], "to_shape": [["to_shape", "get_shape_list", "to_tensor", "expected_rank"], ["len", "from_shape", "len", "to_shape"], ["to_seq_length", "to_shape"]], "to_tensor_2d": [["to_tensor_2d", "reshape_to_matrix", "to_tensor"], ["key_layer", "tf", "layers", "dense", "to_tensor_2d", "num_attention_heads", "size_per_head", "activation", "key_act", "name", "kernel_initializer", "create_initializer", "initializer_range"], ["value_layer", "tf", "layers", "dense", "to_tensor_2d", "num_attention_heads", "size_per_head", "activation", "value_act", "name", "kernel_initializer", "create_initializer", "initializer_range"]], "value_layer": [["value_layer", "tf", "layers", "dense", "to_tensor_2d", "num_attention_heads", "size_per_head", "activation", "value_act", "name", "kernel_initializer", "create_initializer", "initializer_range"], ["value_layer", "tf", "reshape", "value_layer", "batch_size", "to_seq_length", "num_attention_heads", "size_per_head"], ["value_layer", "tf", "transpose", "value_layer"], ["context_layer", "tf", "matmul", "attention_probs", "value_layer"]]}, "name": "attention_layer", "params": {"attention_mask": "", "attention_probs_dropout_prob": "", "batch_size": "", "do_return_2d_tensor": "", "from_seq_length": "", "from_tensor": "", "initializer_range": "", "key_act": "", "num_attention_heads": "", "query_act": "", "size_per_head": "", "to_seq_length": "", "to_tensor": "", "value_act": ""}, "params_descr": {"attention_mask": "(optional) int32 Tensor of shape [batch_size,\nfrom_seq_length, to_seq_length]. The values should be 1 or 0. The\nattention scores will effectively be set to -infinity for any positions in\nthe mask that are 0, and will be unchanged for positions that are 1.", "attention_probs_dropout_prob": "(optional) float. Dropout probability of the\nattention probabilities.", "batch_size": "(Optional) int. If the input is 2D, this might be the batch size\nof the 3D version of the `from_tensor` and `to_tensor`.", "do_return_2d_tensor": "bool. If True, the output will be of shape [batch_size\n* from_seq_length, num_attention_heads * size_per_head]. If False, the\noutput will be of shape [batch_size, from_seq_length, num_attention_heads\n* size_per_head].", "from_seq_length": "(Optional) If the input is 2D, this might be the seq length\nof the 3D version of the `from_tensor`.", "from_tensor": "float Tensor of shape [batch_size, from_seq_length,\nfrom_width].", "initializer_range": "float. Range of the weight initializer.", "key_act": "(optional) Activation function for the key transform.", "num_attention_heads": "int. Number of attention heads.", "query_act": "(optional) Activation function for the query transform.", "size_per_head": "int. Size of each attention head.", "to_seq_length": "(Optional) If the input is 2D, this might be the seq length\nof the 3D version of the `to_tensor`.", "to_tensor": "float Tensor of shape [batch_size, to_seq_length, to_width].", "value_act": "(optional) Activation function for the value transform."}, "params_occur": {"attention_mask": [["attention_mask", "None"], ["attention_mask", "tf", "expand_dims", "attention_mask", "axis"], ["adder", "tf", "cast", "attention_mask", "tf", "float32"]], "attention_probs_dropout_prob": [["attention_probs", "dropout", "attention_probs", "attention_probs_dropout_prob"]], "batch_size": [["batch_size", "from_shape"], ["batch_size", "None", "from_seq_length", "None", "to_seq_length", "None"], ["query_layer", "transpose_for_scores", "query_layer", "batch_size", "num_attention_heads", "from_seq_length", "size_per_head"], ["key_layer", "transpose_for_scores", "key_layer", "batch_size", "num_attention_heads", "to_seq_length", "size_per_head"], ["value_layer", "tf", "reshape", "value_layer", "batch_size", "to_seq_length", "num_attention_heads", "size_per_head"], ["context_layer", "tf", "reshape", "context_layer", "batch_size", "from_seq_length", "num_attention_heads", "size_per_head"], ["context_layer", "tf", "reshape", "context_layer", "batch_size", "from_seq_length", "num_attention_heads", "size_per_head"]], "do_return_2d_tensor": [], "from_seq_length": [["from_seq_length", "from_shape"], ["batch_size", "None", "from_seq_length", "None", "to_seq_length", "None"], ["query_layer", "transpose_for_scores", "query_layer", "batch_size", "num_attention_heads", "from_seq_length", "size_per_head"], ["context_layer", "tf", "reshape", "context_layer", "batch_size", "from_seq_length", "num_attention_heads", "size_per_head"], ["context_layer", "tf", "reshape", "context_layer", "batch_size", "from_seq_length", "num_attention_heads", "size_per_head"]], "from_tensor": [["from_shape", "get_shape_list", "from_tensor", "expected_rank"], ["from_tensor_2d", "reshape_to_matrix", "from_tensor"]], "initializer_range": [["query_layer", "tf", "layers", "dense", "from_tensor_2d", "num_attention_heads", "size_per_head", "activation", "query_act", "name", "kernel_initializer", "create_initializer", "initializer_range"], ["key_layer", "tf", "layers", "dense", "to_tensor_2d", "num_attention_heads", "size_per_head", "activation", "key_act", "name", "kernel_initializer", "create_initializer", "initializer_range"], ["value_layer", "tf", "layers", "dense", "to_tensor_2d", "num_attention_heads", "size_per_head", "activation", "value_act", "name", "kernel_initializer", "create_initializer", "initializer_range"]], "key_act": [["key_layer", "tf", "layers", "dense", "to_tensor_2d", "num_attention_heads", "size_per_head", "activation", "key_act", "name", "kernel_initializer", "create_initializer", "initializer_range"]], "num_attention_heads": [["query_layer", "tf", "layers", "dense", "from_tensor_2d", "num_attention_heads", "size_per_head", "activation", "query_act", "name", "kernel_initializer", "create_initializer", "initializer_range"], ["key_layer", "tf", "layers", "dense", "to_tensor_2d", "num_attention_heads", "size_per_head", "activation", "key_act", "name", "kernel_initializer", "create_initializer", "initializer_range"], ["value_layer", "tf", "layers", "dense", "to_tensor_2d", "num_attention_heads", "size_per_head", "activation", "value_act", "name", "kernel_initializer", "create_initializer", "initializer_range"], ["query_layer", "transpose_for_scores", "query_layer", "batch_size", "num_attention_heads", "from_seq_length", "size_per_head"], ["key_layer", "transpose_for_scores", "key_layer", "batch_size", "num_attention_heads", "to_seq_length", "size_per_head"], ["value_layer", "tf", "reshape", "value_layer", "batch_size", "to_seq_length", "num_attention_heads", "size_per_head"], ["context_layer", "tf", "reshape", "context_layer", "batch_size", "from_seq_length", "num_attention_heads", "size_per_head"], ["context_layer", "tf", "reshape", "context_layer", "batch_size", "from_seq_length", "num_attention_heads", "size_per_head"]], "query_act": [["query_layer", "tf", "layers", "dense", "from_tensor_2d", "num_attention_heads", "size_per_head", "activation", "query_act", "name", "kernel_initializer", "create_initializer", "initializer_range"]], "size_per_head": [["query_layer", "tf", "layers", "dense", "from_tensor_2d", "num_attention_heads", "size_per_head", "activation", "query_act", "name", "kernel_initializer", "create_initializer", "initializer_range"], ["key_layer", "tf", "layers", "dense", "to_tensor_2d", "num_attention_heads", "size_per_head", "activation", "key_act", "name", "kernel_initializer", "create_initializer", "initializer_range"], ["value_layer", "tf", "layers", "dense", "to_tensor_2d", "num_attention_heads", "size_per_head", "activation", "value_act", "name", "kernel_initializer", "create_initializer", "initializer_range"], ["query_layer", "transpose_for_scores", "query_layer", "batch_size", "num_attention_heads", "from_seq_length", "size_per_head"], ["key_layer", "transpose_for_scores", "key_layer", "batch_size", "num_attention_heads", "to_seq_length", "size_per_head"], ["attention_scores", "tf", "multiply", "attention_scores", "math", "sqrt", "float", "size_per_head"], ["value_layer", "tf", "reshape", "value_layer", "batch_size", "to_seq_length", "num_attention_heads", "size_per_head"], ["context_layer", "tf", "reshape", "context_layer", "batch_size", "from_seq_length", "num_attention_heads", "size_per_head"], ["context_layer", "tf", "reshape", "context_layer", "batch_size", "from_seq_length", "num_attention_heads", "size_per_head"]], "to_seq_length": [["to_seq_length", "to_shape"], ["batch_size", "None", "from_seq_length", "None", "to_seq_length", "None"], ["key_layer", "transpose_for_scores", "key_layer", "batch_size", "num_attention_heads", "to_seq_length", "size_per_head"], ["value_layer", "tf", "reshape", "value_layer", "batch_size", "to_seq_length", "num_attention_heads", "size_per_head"]], "to_tensor": [["to_shape", "get_shape_list", "to_tensor", "expected_rank"], ["to_tensor_2d", "reshape_to_matrix", "to_tensor"]], "value_act": [["value_layer", "tf", "layers", "dense", "to_tensor_2d", "num_attention_heads", "size_per_head", "activation", "value_act", "name", "kernel_initializer", "create_initializer", "initializer_range"]]}, "params_p": {"args": [], "attention_mask": [["bool", 0.22018966762364267], ["Optional[bool]", 0.11490581184762193], ["Optional[List]", 0.106315796292301], ["list", 0.08776673819055993], ["Optional[str]", 0.08698260212066676]], "attention_probs_dropout_prob": [["numpy.ndarray", 0.18431058570833123], ["int", 0.1735845542732315], ["Callable", 0.0933470055681223], ["Optional[List[Any]]", 0.09215529285416561], ["List[numpy.ndarray]", 0.09215529285416561], ["Tuple[int, int, int]", 0.08643883967158675], ["str", 0.08122850842342294]], "batch_size": [["Optional[int]", 0.41158504418987435], ["int", 0.18207950888561059], ["Union[int, List[\"Outcome\"]]", 0.1020992571657801], ["List[\"Issue\"]", 0.09946719139030191]], "do_return_2d_tensor": [["int", 0.2834167546173109], ["bool", 0.20473807010933975], ["BinaryIO", 0.20473807010933975], ["Optional[int]", 0.20473807010933975], ["Optional[bytes]", 0.10236903505466988]], "from_seq_length": [["Optional[List[int]]", 0.22684616866714236], ["Optional[int]", 0.2133591076632341], ["str", 0.0927375254106391], ["float", 0.09218639520950749]], "from_tensor": [["int", 0.2810260833966386], ["numpy.ndarray", 0.11984725220138422], ["float", 0.09876206335140862], ["str", 0.09653752072562247]], "initializer_range": [["int", 0.22583062081880262], ["float", 0.08288755250724678]], "key_act": [["float", 0.10646667004304686], ["int", 0.09962016024939725]], "kwargs": [], "num_attention_heads": [["int", 0.22583062081880262], ["float", 0.08288755250724678]], "query_act": [["int", 0.22583062081880262], ["float", 0.08288755250724678]], "size_per_head": [["int", 0.22583062081880262], ["float", 0.08288755250724678]], "to_seq_length": [["Optional[int]", 0.19393978596858752], ["Optional[List[int]]", 0.122326333197937], ["Optional[float]", 0.1082776038609956], ["Optional[Union[Any, Any]]", 0.10071275866366061], ["float", 0.09787601100540322], ["Optional[str]", 0.09266842829475898], ["int", 0.09077767711101699]], "to_tensor": [["int", 0.2810260833966386], ["numpy.ndarray", 0.11984725220138422], ["float", 0.09876206335140862], ["str", 0.09653752072562247]], "value_act": [["int", 0.32180400910550044], ["numpy.array", 0.21193867930926444], ["bool", 0.10596933965463222], ["Optional[float]", 0.083162689608048]]}, "q_name": "attention_layer", "ret_exprs": ["return context_layer"], "ret_type": "", "ret_type_p": [["int", 0.30000000000000004], ["Callable", 0.2], ["str", 0.1]], "variables": {"adder": "", "attention_mask": "", "attention_probs": "", "attention_scores": "", "batch_size": "", "context_layer": "", "from_seq_length": "", "from_shape": "", "from_tensor_2d": "", "key_layer": "", "query_layer": "", "to_seq_length": "", "to_shape": "", "to_tensor_2d": "", "value_layer": ""}, "variables_p": {"adder": [["float", 0.1772151726593931], ["int", 0.16788910205765673]], "attention_mask": [["Optional[str]", 0.19999999999999998], ["frozenset[str]", 0.09999999999999999], ["ImportError", 0.09999999999999999], ["bool", 0.09999999999999999], ["str", 0.09999999999999999], ["Dict[str, str]", 0.09999999999999999]], "attention_probs": [["int", 0.2], ["dict", 0.1], ["List[int]", 0.1], ["List[List[int]]", 0.1], ["str", 0.1]], "attention_scores": [["List[int]", 0.19567128497843622], ["str", 0.1940854584045346], ["Pattern[str]", 0.11335488289212278], ["tuple", 0.09456337446934529]], "batch_size": [["int", 0.34204860051970754], ["float", 0.07984582357922507], ["bytes", 0.06913941346170688], ["Optional[int]", 0.06718932611174608]], "context_layer": [["str", 0.26478089907433977], ["tuple", 0.11511020844666173], ["List[str]", 0.11201919431816011]], "from_seq_length": [["list", 0.19571385012829456], ["Optional[int]", 0.17745902630680122], ["int", 0.16677517950968596], ["List[int]", 0.09035702060541846]], "from_shape": [["numpy.array", 0.13333668395418377], ["Tuple[int]", 0.10532523095111024], ["Tuple[int, int, int, int]", 0.10532523095111024], ["int", 0.10018942486614915], ["List[int]", 0.09646114469864217], ["bytes", 0.09627050717220434], ["str", 0.08791106967425907], ["Set[str]", 0.0871570759930018], ["Tuple[int, int]", 0.08702027630297926]], "from_tensor_2d": [["int", 0.7794439815454351], ["float", 0.22055601845456482]], "key_layer": [["str", 0.33392020074965584], ["bytearray", 0.13085476141769986], ["List[Tuple[str, Any]]", 0.08116556919426825], ["list", 0.08114638942140574]], "query_layer": [["float", 0.2155921895390931], ["str", 0.10798148338658801], ["bytearray", 0.10240702470546754], ["tuple", 0.08868766907880336]], "to_seq_length": [["list", 0.20500939945019686], ["List[None]", 0.13663219506562455], ["str", 0.09371723298032246]], "to_shape": [["str", 0.2758257332182197], ["int", 0.2494805128846259], ["List[str]", 0.1931808051798795], ["list", 0.18534435281989398], ["List[float]", 0.09616859589738082]], "to_tensor_2d": [["int", 0.601031796019303], ["Dict[int, bytes]", 0.10403787348012454], ["float", 0.10143914731197134]], "value_layer": [["float", 0.1072337156208798], ["Type[tuple]", 0.09632092676620235], ["int", 0.09005451499449987]]}}, {"docstring": {"func": "Multi-headed, multi-layer Transformer from \"Attention is All You Need\".", "long_descr": "This is almost an exact implementation of the original Transformer encoder.\n\nSee the original paper:\nhttps://arxiv.org/abs/1706.03762\n\nAlso see:\nhttps://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py", "ret": "float Tensor of shape [batch_size, seq_length, hidden_size], the final\nhidden layer of the Transformer."}, "fn_lc": [[754, 0], [892, 23]], "fn_var_ln": {"all_layer_outputs": [[825, 2], [825, 19]], "attention_head": [[833, 10], [833, 24]], "attention_head_size": [[807, 2], [807, 21]], "attention_heads": [[831, 8], [831, 23]], "attention_output": [[863, 10], [863, 26]], "batch_size": [[809, 2], [809, 12]], "final_output": [[891, 4], [891, 16]], "final_outputs": [[885, 4], [885, 17]], "input_shape": [[808, 2], [808, 13]], "input_width": [[811, 2], [811, 13]], "intermediate_output": [[867, 8], [867, 27]], "layer_input": [[828, 6], [828, 17]], "layer_output": [[880, 8], [880, 20]], "prev_output": [[881, 8], [881, 19]], "seq_length": [[810, 2], [810, 12]]}, "fn_var_occur": {"all_layer_outputs": [["all_layer_outputs", "append", "layer_output"]], "attention_head": [["attention_head", "attention_layer", "from_tensor", "layer_input", "to_tensor", "layer_input", "attention_mask", "attention_mask", "num_attention_heads", "num_attention_heads", "size_per_head", "attention_head_size", "attention_probs_dropout_prob", "attention_probs_dropout_prob", "initializer_range", "initializer_range", "do_return_2d_tensor", "True", "batch_size", "batch_size", "from_seq_length", "seq_length", "to_seq_length", "seq_length"], ["attention_heads", "append", "attention_head"]], "attention_head_size": [["attention_head_size", "int", "hidden_size", "num_attention_heads"], ["attention_head", "attention_layer", "from_tensor", "layer_input", "to_tensor", "layer_input", "attention_mask", "attention_mask", "num_attention_heads", "num_attention_heads", "size_per_head", "attention_head_size", "attention_probs_dropout_prob", "attention_probs_dropout_prob", "initializer_range", "initializer_range", "do_return_2d_tensor", "True", "batch_size", "batch_size", "from_seq_length", "seq_length", "to_seq_length", "seq_length"]], "attention_heads": [["attention_heads", "append", "attention_head"], ["len", "attention_heads"], ["attention_output", "attention_heads"], ["attention_output", "tf", "concat", "attention_heads", "axis"]], "attention_output": [["attention_output", "None"], ["attention_output", "attention_heads"], ["attention_output", "tf", "concat", "attention_heads", "axis"], ["attention_output", "tf", "layers", "dense", "attention_output", "hidden_size", "kernel_initializer", "create_initializer", "initializer_range"], ["attention_output", "dropout", "attention_output", "hidden_dropout_prob"], ["attention_output", "layer_norm", "attention_output", "layer_input"], ["intermediate_output", "tf", "layers", "dense", "attention_output", "intermediate_size", "activation", "intermediate_act_fn", "kernel_initializer", "create_initializer", "initializer_range"], ["layer_output", "layer_norm", "layer_output", "attention_output"]], "batch_size": [["batch_size", "input_shape"], ["attention_head", "attention_layer", "from_tensor", "layer_input", "to_tensor", "layer_input", "attention_mask", "attention_mask", "num_attention_heads", "num_attention_heads", "size_per_head", "attention_head_size", "attention_probs_dropout_prob", "attention_probs_dropout_prob", "initializer_range", "initializer_range", "do_return_2d_tensor", "True", "batch_size", "batch_size", "from_seq_length", "seq_length", "to_seq_length", "seq_length"]], "final_output": [["final_output", "reshape_from_matrix", "layer_output", "input_shape"], ["final_outputs", "append", "final_output"], ["final_output", "reshape_from_matrix", "prev_output", "input_shape"]], "final_outputs": [["final_outputs", "append", "final_output"]], "input_shape": [["input_shape", "get_shape_list", "input_tensor", "expected_rank"], ["batch_size", "input_shape"], ["seq_length", "input_shape"], ["input_width", "input_shape"], ["final_output", "reshape_from_matrix", "layer_output", "input_shape"], ["final_output", "reshape_from_matrix", "prev_output", "input_shape"]], "input_width": [["input_width", "input_shape"], ["input_width", "hidden_size"], ["ValueError", "input_width", "hidden_size"]], "intermediate_output": [["intermediate_output", "tf", "layers", "dense", "attention_output", "intermediate_size", "activation", "intermediate_act_fn", "kernel_initializer", "create_initializer", "initializer_range"], ["layer_output", "tf", "layers", "dense", "intermediate_output", "hidden_size", "kernel_initializer", "create_initializer", "initializer_range"]], "layer_input": [["layer_input", "prev_output"], ["attention_head", "attention_layer", "from_tensor", "layer_input", "to_tensor", "layer_input", "attention_mask", "attention_mask", "num_attention_heads", "num_attention_heads", "size_per_head", "attention_head_size", "attention_probs_dropout_prob", "attention_probs_dropout_prob", "initializer_range", "initializer_range", "do_return_2d_tensor", "True", "batch_size", "batch_size", "from_seq_length", "seq_length", "to_seq_length", "seq_length"], ["attention_output", "layer_norm", "attention_output", "layer_input"]], "layer_output": [["layer_output", "tf", "layers", "dense", "intermediate_output", "hidden_size", "kernel_initializer", "create_initializer", "initializer_range"], ["layer_output", "dropout", "layer_output", "hidden_dropout_prob"], ["layer_output", "layer_norm", "layer_output", "attention_output"], ["prev_output", "layer_output"], ["all_layer_outputs", "append", "layer_output"], ["final_output", "reshape_from_matrix", "layer_output", "input_shape"]], "prev_output": [["prev_output", "reshape_to_matrix", "input_tensor"], ["layer_input", "prev_output"], ["prev_output", "layer_output"], ["final_output", "reshape_from_matrix", "prev_output", "input_shape"]], "seq_length": [["seq_length", "input_shape"], ["attention_head", "attention_layer", "from_tensor", "layer_input", "to_tensor", "layer_input", "attention_mask", "attention_mask", "num_attention_heads", "num_attention_heads", "size_per_head", "attention_head_size", "attention_probs_dropout_prob", "attention_probs_dropout_prob", "initializer_range", "initializer_range", "do_return_2d_tensor", "True", "batch_size", "batch_size", "from_seq_length", "seq_length", "to_seq_length", "seq_length"]]}, "name": "transformer_model", "params": {"attention_mask": "", "attention_probs_dropout_prob": "", "do_return_all_layers": "", "hidden_dropout_prob": "", "hidden_size": "", "initializer_range": "", "input_tensor": "", "intermediate_act_fn": "", "intermediate_size": "", "num_attention_heads": "", "num_hidden_layers": ""}, "params_descr": {"attention_mask": "(optional) int32 Tensor of shape [batch_size, seq_length,\nseq_length], with 1 for positions that can be attended to and 0 in\npositions that should not be.", "attention_probs_dropout_prob": "float. Dropout probability of the attention\nprobabilities.", "do_return_all_layers": "Whether to also return all layers or just the final\nlayer.", "hidden_dropout_prob": "float. Dropout probability for the hidden layers.", "hidden_size": "int. Hidden size of the Transformer.", "initializer_range": "float. Range of the initializer (stddev of truncated\nnormal).", "input_tensor": "float Tensor of shape [batch_size, seq_length, hidden_size].", "intermediate_act_fn": "function. The non-linear activation function to apply\nto the output of the intermediate/feed-forward layer.", "intermediate_size": "int. The size of the \"intermediate\" (a.k.a., feed\nforward) layer.", "num_attention_heads": "int. Number of attention heads in the Transformer.", "num_hidden_layers": "int. Number of layers (blocks) in the Transformer."}, "params_occur": {"attention_mask": [["attention_head", "attention_layer", "from_tensor", "layer_input", "to_tensor", "layer_input", "attention_mask", "attention_mask", "num_attention_heads", "num_attention_heads", "size_per_head", "attention_head_size", "attention_probs_dropout_prob", "attention_probs_dropout_prob", "initializer_range", "initializer_range", "do_return_2d_tensor", "True", "batch_size", "batch_size", "from_seq_length", "seq_length", "to_seq_length", "seq_length"]], "attention_probs_dropout_prob": [["attention_head", "attention_layer", "from_tensor", "layer_input", "to_tensor", "layer_input", "attention_mask", "attention_mask", "num_attention_heads", "num_attention_heads", "size_per_head", "attention_head_size", "attention_probs_dropout_prob", "attention_probs_dropout_prob", "initializer_range", "initializer_range", "do_return_2d_tensor", "True", "batch_size", "batch_size", "from_seq_length", "seq_length", "to_seq_length", "seq_length"]], "do_return_all_layers": [], "hidden_dropout_prob": [["attention_output", "dropout", "attention_output", "hidden_dropout_prob"], ["layer_output", "dropout", "layer_output", "hidden_dropout_prob"]], "hidden_size": [["hidden_size", "num_attention_heads"], ["ValueError", "hidden_size", "num_attention_heads"], ["attention_head_size", "int", "hidden_size", "num_attention_heads"], ["input_width", "hidden_size"], ["ValueError", "input_width", "hidden_size"], ["attention_output", "tf", "layers", "dense", "attention_output", "hidden_size", "kernel_initializer", "create_initializer", "initializer_range"], ["layer_output", "tf", "layers", "dense", "intermediate_output", "hidden_size", "kernel_initializer", "create_initializer", "initializer_range"]], "initializer_range": [["attention_head", "attention_layer", "from_tensor", "layer_input", "to_tensor", "layer_input", "attention_mask", "attention_mask", "num_attention_heads", "num_attention_heads", "size_per_head", "attention_head_size", "attention_probs_dropout_prob", "attention_probs_dropout_prob", "initializer_range", "initializer_range", "do_return_2d_tensor", "True", "batch_size", "batch_size", "from_seq_length", "seq_length", "to_seq_length", "seq_length"], ["attention_output", "tf", "layers", "dense", "attention_output", "hidden_size", "kernel_initializer", "create_initializer", "initializer_range"], ["intermediate_output", "tf", "layers", "dense", "attention_output", "intermediate_size", "activation", "intermediate_act_fn", "kernel_initializer", "create_initializer", "initializer_range"], ["layer_output", "tf", "layers", "dense", "intermediate_output", "hidden_size", "kernel_initializer", "create_initializer", "initializer_range"]], "input_tensor": [["input_shape", "get_shape_list", "input_tensor", "expected_rank"], ["prev_output", "reshape_to_matrix", "input_tensor"]], "intermediate_act_fn": [["intermediate_output", "tf", "layers", "dense", "attention_output", "intermediate_size", "activation", "intermediate_act_fn", "kernel_initializer", "create_initializer", "initializer_range"]], "intermediate_size": [["intermediate_output", "tf", "layers", "dense", "attention_output", "intermediate_size", "activation", "intermediate_act_fn", "kernel_initializer", "create_initializer", "initializer_range"]], "num_attention_heads": [["hidden_size", "num_attention_heads"], ["ValueError", "hidden_size", "num_attention_heads"], ["attention_head_size", "int", "hidden_size", "num_attention_heads"], ["attention_head", "attention_layer", "from_tensor", "layer_input", "to_tensor", "layer_input", "attention_mask", "attention_mask", "num_attention_heads", "num_attention_heads", "size_per_head", "attention_head_size", "attention_probs_dropout_prob", "attention_probs_dropout_prob", "initializer_range", "initializer_range", "do_return_2d_tensor", "True", "batch_size", "batch_size", "from_seq_length", "seq_length", "to_seq_length", "seq_length"]], "num_hidden_layers": [["range", "num_hidden_layers"]]}, "params_p": {"args": [], "attention_mask": [["int", 0.6926670561371078], ["bool", 0.1225120337810017], ["List[float]", 0.09241045504094533], ["float", 0.09241045504094533]], "attention_probs_dropout_prob": [["int", 0.6926670561371078], ["bool", 0.1225120337810017], ["List[float]", 0.09241045504094533], ["float", 0.09241045504094533]], "do_return_all_layers": [["bool", 0.7399533115075355], ["Callable[..., None]", 0.12429094095065586]], "hidden_dropout_prob": [["int", 0.29749878994806483], ["bool", 0.21074963718314324], ["bytes", 0.11115836215841968], ["Optional[tensorflow.keras.initializers.Initializer]", 0.1009066897102438], ["Optional[int]", 0.1009066897102438]], "hidden_size": [["int", 0.8225960282092157], ["bool", 0.08654056070089292]], "initializer_range": [["int", 0.6926670561371078], ["bool", 0.1225120337810017], ["List[float]", 0.09241045504094533], ["float", 0.09241045504094533]], "input_tensor": [["int", 0.24886521257408956], ["Optional[int]", 0.18750805978850602], ["float", 0.10156220255522899], ["Optional[numpy.ndarray]", 0.09647620712331823], ["numpy.ndarray", 0.09647620712331823]], "intermediate_act_fn": [["int", 0.37412109391018766], ["str", 0.11049967029597749], ["float", 0.11049967029597749], ["Optional[tensorflow.keras.initializers.Initializer]", 0.10471287588626253], ["Optional[int]", 0.10471287588626253], ["Tuple[int]", 0.08495414342935473]], "intermediate_size": [["int", 0.37412109391018766], ["str", 0.11049967029597749], ["float", 0.11049967029597749], ["Optional[tensorflow.keras.initializers.Initializer]", 0.10471287588626253], ["Optional[int]", 0.10471287588626253], ["Tuple[int]", 0.08495414342935473]], "kwargs": [], "num_attention_heads": [["int", 0.8225960282092157], ["bool", 0.08654056070089292]], "num_hidden_layers": [["int", 0.8000235698637773]]}, "q_name": "transformer_model", "ret_exprs": ["return final_outputs", "return final_output"], "ret_type": "", "ret_type_p": [["dict", 0.20869149400876963], ["str", 0.20431230558698552], ["Awaitable", 0.09769874510505444]], "variables": {"all_layer_outputs": "", "attention_head": "", "attention_head_size": "", "attention_heads": "", "attention_output": "", "batch_size": "", "final_output": "", "final_outputs": "", "input_shape": "", "input_width": "", "intermediate_output": "", "layer_input": "", "layer_output": "", "prev_output": "", "seq_length": ""}, "variables_p": {"all_layer_outputs": [["list", 0.9999999999999998]], "attention_head": [["Tuple[int, int, int]", 0.10964941842782752], ["List[float]", 0.09981514462445605]], "attention_head_size": [["int", 0.49616357845872133], ["str", 0.20402074289653788], ["List[int]", 0.18765464958893674], ["float", 0.11216102905580405]], "attention_heads": [["list", 0.8936693225674519]], "attention_output": [["Optional[type]", 0.11735316836804371], ["list", 0.10102178305346418]], "batch_size": [["int", 0.16956082673105916], ["float", 0.08913188736049674]], "final_output": [["List[bytes]", 1.0]], "final_outputs": [["list", 0.9999999998402876], ["List[str]", 1.3747571971040537e-10], ["str", 2.223676734826291e-11]], "input_shape": [["int", 0.6142032092698173], ["six.moves.range", 0.11267052136648514], ["str", 0.09685025684273016], ["Tuple[Any, Any, Any, Any]", 0.0889485466151087]], "input_width": [["int", 1.0]], "intermediate_output": [["int", 0.3208534729478789], ["str", 0.10486517995328773], ["List[Tuple[str, Any]]", 0.09483293714487429], ["Type[DefaultDict[int, Any]]", 0.0892566029391059], ["Dict[int, str]", 0.08151056887202703], ["float", 0.0801681336627612]], "layer_input": [["List[str]", 0.07560701043425719], ["str", 0.07102468508962537], ["frozenset[str]", 0.07102468508962537], ["int", 0.06390403132171624]], "layer_output": [["int", 0.28137917512400507], ["List[int]", 0.24250668498647826], ["str", 0.11985277071999544], ["memoryview", 0.09614088750528571], ["Dict[int, bytes]", 0.09075222322086841]], "prev_output": [["bytes", 0.41572898604219055], ["str", 0.24348935685518885], ["memoryview", 0.06679927917425887]], "seq_length": [["List[str]", 0.08801236903702236]]}}, {"docstring": {"func": "Returns a list of the shape of tensor, preferring static dimensions.", "long_descr": null, "ret": "A list of dimensions of the shape of tensor. All static dimensions will\nbe returned as python integers, and dynamic dimensions will be returned\nas tf.Tensor scalars."}, "fn_lc": [[895, 0], [929, 14]], "fn_var_ln": {"dyn_shape": [[926, 2], [926, 11]], "name": [[911, 4], [911, 8]], "non_static_indexes": [[918, 2], [918, 20]], "shape": [[916, 2], [916, 7]]}, "fn_var_occur": {"dyn_shape": [["dyn_shape", "tf", "shape", "tensor"], ["shape", "index", "dyn_shape", "index"]], "name": [["name", "None"], ["name", "tensor", "name"], ["assert_rank", "tensor", "expected_rank", "name"]], "non_static_indexes": [["non_static_indexes", "append", "index"]], "shape": [["shape", "tensor", "shape", "as_list"], ["enumerate", "shape"], ["dyn_shape", "tf", "shape", "tensor"], ["shape", "index", "dyn_shape", "index"]]}, "name": "get_shape_list", "params": {"expected_rank": "", "name": "", "tensor": ""}, "params_descr": {"expected_rank": "(optional) int. The expected rank of `tensor`. If this is\nspecified and the `tensor` has a different rank, and exception will be\nthrown.", "name": "Optional name of the tensor for the error message.", "tensor": "A tf.Tensor object to find the shape of."}, "params_occur": {"expected_rank": [["expected_rank", "None"], ["assert_rank", "tensor", "expected_rank", "name"]], "name": [["name", "None"], ["name", "tensor", "name"], ["assert_rank", "tensor", "expected_rank", "name"]], "tensor": [["name", "tensor", "name"], ["assert_rank", "tensor", "expected_rank", "name"], ["shape", "tensor", "shape", "as_list"], ["dyn_shape", "tf", "shape", "tensor"]]}, "params_p": {"args": [], "expected_rank": [["str", 0.26370834314123043], ["Optional[Callable[[], Any]]", 0.13091674011926582], ["Optional[Any]", 0.13091674011926582], ["bool", 0.11666822813194888]], "kwargs": [], "name": [["str", 0.47555581099096716], ["Optional[str]", 0.18735952681352327], ["List[str]", 0.13536889860890564], ["Optional[List]", 0.10699431787023508], ["dict", 0.09472144571636887]], "tensor": [["str", 0.2991913065075193], ["Callable[[List], bool]", 0.09936412107395082], ["bool", 0.09538683454907906], ["Callable[[str], bool]", 0.09262266670505147]]}, "q_name": "get_shape_list", "ret_exprs": ["return shape", "return shape"], "ret_type": "", "ret_type_p": [["str", 0.19645186107451418], ["Callable", 0.15418938099010876], ["List[str]", 0.14797168247513307], ["list", 0.05017609764628941]], "variables": {"dyn_shape": "", "name": "", "non_static_indexes": "", "shape": ""}, "variables_p": {"dyn_shape": [["int", 0.28043269544588684], ["List[str]", 0.08842108664191481], ["tuple", 0.0782382297279128], ["str", 0.07164917700670212]], "name": [["str", 0.3898751608181301], ["List[str]", 0.18967258460993275], ["Dict[str, str]", 0.1443915167676993], ["Pattern[str]", 0.11577923081210352], ["Set[str]", 0.07698968271519313]], "non_static_indexes": [["str", 0.37044434690010486], ["list", 0.3247845968776049], ["Iterator[Tuple[Any, Any]]", 0.11516614650031258], ["List[float]", 0.10050128153171424], ["Sequence[str]", 0.08910362819026342]], "shape": [["int", 0.31379291178661867], ["list", 0.19317431640053706], ["Dict[str, List[str]]", 0.09024686931616584], ["dict", 0.08966683111978467]]}}, {"docstring": {"func": "Reshapes a >= rank 2 tensor to a rank 2 tensor (i.e., a matrix).", "long_descr": null, "ret": null}, "fn_lc": [[932, 0], [943, 22]], "fn_var_ln": {"ndims": [[934, 2], [934, 7]], "output_tensor": [[942, 2], [942, 15]], "width": [[941, 2], [941, 7]]}, "fn_var_occur": {"ndims": [["ndims", "input_tensor", "shape", "ndims"]], "output_tensor": [["output_tensor", "tf", "reshape", "input_tensor", "width"]], "width": [["width", "input_tensor", "shape"], ["output_tensor", "tf", "reshape", "input_tensor", "width"]]}, "name": "reshape_to_matrix", "params": {"input_tensor": ""}, "params_descr": {"input_tensor": ""}, "params_occur": {"input_tensor": [["ndims", "input_tensor", "shape", "ndims"], ["ValueError", "input_tensor", "shape"], ["width", "input_tensor", "shape"], ["output_tensor", "tf", "reshape", "input_tensor", "width"]]}, "params_p": {"args": [], "input_tensor": [["int", 0.6165296331939313], ["numpy.ndarray", 0.1058630342929698]], "kwargs": []}, "q_name": "reshape_to_matrix", "ret_exprs": ["return input_tensor", "return output_tensor"], "ret_type": "", "ret_type_p": [["List[int]", 0.26863143900661224], ["str", 0.26863143900661224]], "variables": {"ndims": "", "output_tensor": "", "width": ""}, "variables_p": {"ndims": [["int", 0.2], ["dict", 0.1], ["List[int]", 0.1], ["List[List[int]]", 0.1], ["str", 0.1]], "output_tensor": [["List[str]", 0.41182584662461447], ["bool", 0.09233998708529365], ["Iterator[str]", 0.08719324220170128], ["Dict[str, int]", 0.08127481723546252], ["Dict[str, str]", 0.06739998928500265]], "width": [["str", 0.41035290468438473], ["int", 0.2666266758009467], ["bytes", 0.11716482116394107], ["float", 0.11255129855480432], ["Optional[int]", 0.09330429979592327]]}}, {"docstring": {"func": "Reshapes a rank 2 tensor back to its original rank >= 2 tensor.", "long_descr": null, "ret": null}, "fn_lc": [[946, 0], [956, 55]], "fn_var_ln": {"orig_dims": [[953, 2], [953, 11]], "output_shape": [[951, 2], [951, 14]], "width": [[954, 2], [954, 7]]}, "fn_var_occur": {"orig_dims": [["orig_dims", "orig_shape_list"], ["tf", "reshape", "output_tensor", "orig_dims", "width"]], "output_shape": [["output_shape", "get_shape_list", "output_tensor"], ["width", "output_shape"]], "width": [["width", "output_shape"], ["tf", "reshape", "output_tensor", "orig_dims", "width"]]}, "name": "reshape_from_matrix", "params": {"orig_shape_list": "", "output_tensor": ""}, "params_descr": {"orig_shape_list": "", "output_tensor": ""}, "params_occur": {"orig_shape_list": [["len", "orig_shape_list"], ["orig_dims", "orig_shape_list"]], "output_tensor": [["output_shape", "get_shape_list", "output_tensor"], ["tf", "reshape", "output_tensor", "orig_dims", "width"]]}, "params_p": {"args": [], "kwargs": [], "orig_shape_list": [["List[List[int]]", 0.3960061079035681], ["Dict[str, str]", 0.1320224732654417], ["list", 0.09359285554173137], ["str", 0.0901990405833607], ["bool", 0.08725645603620587]], "output_tensor": [["Optional[int]", 0.3039512926705335], ["bool", 0.3039512926705335], ["str", 0.08550211536898954]]}, "q_name": "reshape_from_matrix", "ret_exprs": ["return output_tensor", "return tf.reshape(output_tensor, orig_dims + [width])"], "ret_type": "", "ret_type_p": [["bytes", 0.3859789538338725], ["Dict[str, str]", 0.12063353446314559], ["Callable", 0.1007977093682738], ["int", 0.09533187543299872], ["Awaitable", 0.09526405445544972]], "variables": {"orig_dims": "", "output_shape": "", "width": ""}, "variables_p": {"orig_dims": [["list", 0.30617967782411626], ["int", 0.17715150909076258], ["float", 0.1317139033810897], ["List[List[int]]", 0.09842141910624531], ["str", 0.09842141910624531], ["Dict[int, str]", 0.09566023943637461]], "output_shape": [["list", 0.29233398277466693], ["bool", 0.29073557684162954], ["List[int]", 0.17597461301721384], ["Tuple[int, int, Any, Any]", 0.08545370727293115], ["List[float]", 0.08037919309341063], ["numpy.array", 0.0751229270001478]], "width": [["list", 0.17809167621245037], ["set", 0.10659556148558873], ["List[Dict[str, Any]]", 0.10400646348054803], ["Set[str]", 0.09143186112424977], ["Dict[str, str]", 0.08804931159266369], ["Pattern[str]", 0.08033655050491205], ["float", 0.0793767371592359]]}}, {"docstring": {"func": "Raises an exception if the tensor rank is not of the expected rank.", "long_descr": null, "ret": null}, "fn_lc": [[959, 0], [986, 79]], "fn_var_ln": {"actual_rank": [[980, 2], [980, 13]], "expected_rank_dict": [[973, 2], [973, 20]], "name": [[971, 4], [971, 8]], "scope_name": [[982, 4], [982, 14]]}, "fn_var_occur": {"actual_rank": [["actual_rank", "tensor", "shape", "ndims"], ["actual_rank", "expected_rank_dict"], ["ValueError", "name", "scope_name", "actual_rank", "str", "tensor", "shape", "str", "expected_rank"]], "expected_rank_dict": [["expected_rank_dict", "expected_rank", "True"], ["expected_rank_dict", "x", "True"], ["actual_rank", "expected_rank_dict"]], "name": [["name", "None"], ["name", "tensor", "name"], ["scope_name", "tf", "get_variable_scope", "name"], ["ValueError", "name", "scope_name", "actual_rank", "str", "tensor", "shape", "str", "expected_rank"]], "scope_name": [["scope_name", "tf", "get_variable_scope", "name"], ["ValueError", "name", "scope_name", "actual_rank", "str", "tensor", "shape", "str", "expected_rank"]]}, "name": "assert_rank", "params": {"expected_rank": "", "name": "", "tensor": ""}, "params_descr": {"expected_rank": "Python integer or list of integers, expected rank.", "name": "Optional name of the tensor for the error message.", "tensor": "A tf.Tensor to check the rank of."}, "params_occur": {"expected_rank": [["isinstance", "expected_rank", "six", "integer_types"], ["expected_rank_dict", "expected_rank", "True"], ["ValueError", "name", "scope_name", "actual_rank", "str", "tensor", "shape", "str", "expected_rank"]], "name": [["name", "None"], ["name", "tensor", "name"], ["scope_name", "tf", "get_variable_scope", "name"], ["ValueError", "name", "scope_name", "actual_rank", "str", "tensor", "shape", "str", "expected_rank"]], "tensor": [["name", "tensor", "name"], ["actual_rank", "tensor", "shape", "ndims"], ["ValueError", "name", "scope_name", "actual_rank", "str", "tensor", "shape", "str", "expected_rank"]]}, "params_p": {"args": [], "expected_rank": [["str", 0.23878242756855572], ["int", 0.18864565869380545], ["Tuple[int, int]", 0.08873898899295579]], "kwargs": [], "name": [["str", 0.2321156259183364], ["Optional[str]", 0.18977993095455126], ["Optional[bool]", 0.09651781743880959], ["bool", 0.09452610314320593], ["Optional[List[str]]", 0.09401743160410193]], "tensor": [["str", 0.5084181695241998], ["Optional[str]", 0.10776553240678761], ["Union[str, List[str]]", 0.09971965064206191], ["bool", 0.0906690131669666]]}, "q_name": "assert_rank", "ret_exprs": [], "ret_type": "", "variables": {"actual_rank": "", "expected_rank_dict": "", "name": "", "scope_name": ""}, "variables_p": {"actual_rank": [], "expected_rank_dict": [["dict", 0.4082775504949991], ["list", 0.1972152554149254], ["Dict[str, Dict]", 0.10032876275841293], ["set", 0.09687436725620835], ["Dict[str, str]", 0.09447780368251986]], "name": [["Optional[str]", 0.4031703364060266], ["str", 0.1978208298042053], ["List[str]", 0.08338444497707724]], "scope_name": [["str", 0.6968137229552139], ["Optional[str]", 0.18668756276244763], ["Dict[str, Any]", 0.11649871428233835]]}}], "imports": ["__future__", "absolute_import", "__future__", "division", "__future__", "print_function", "collections", "copy", "json", "math", "re", "numpy", "np", "six", "tensorflow", "tf"], "mod_var_ln": {}, "mod_var_occur": {}, "no_types_annot": {"D": 0, "I": 0, "U": 210}, "session_id": "hImhw2G004wkWVJzCGijYmVy6Z8BfAiG64DoKSJRAng", "set": null, "tc": [false, null], "type_annot_cove": 0.0, "typed_seq": "", "untyped_seq": "", "variables": {}, "variables_p": {}}}