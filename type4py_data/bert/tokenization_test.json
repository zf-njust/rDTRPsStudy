{"error": null, "response": {"classes": [{"cls_lc": [[26, 0], [133, 56]], "cls_var_ln": {}, "cls_var_occur": {}, "funcs": [{"docstring": {"func": null, "long_descr": null, "ret": null}, "fn_lc": [[28, 2], [49, 69]], "fn_var_ln": {"tokenizer": [[42, 4], [42, 13]], "tokens": [[45, 4], [45, 10]], "vocab_file": [[40, 6], [40, 16]], "vocab_tokens": [[29, 4], [29, 16]]}, "fn_var_occur": {"tokenizer": [["tokenizer", "tokenization", "FullTokenizer", "vocab_file"], ["tokens", "tokenizer", "tokenize"], ["self", "assertAllEqual", "tokenizer", "convert_tokens_to_ids", "tokens"]], "tokens": [["tokens", "tokenizer", "tokenize"], ["self", "assertAllEqual", "tokens"], ["self", "assertAllEqual", "tokenizer", "convert_tokens_to_ids", "tokens"]], "vocab_file": [["vocab_file", "vocab_writer", "name"], ["tokenizer", "tokenization", "FullTokenizer", "vocab_file"], ["os", "unlink", "vocab_file"]], "vocab_tokens": [["vocab_writer", "write", "join", "x", "x", "vocab_tokens"], ["vocab_writer", "write", "join", "x", "x", "vocab_tokens", "encode"]]}, "name": "test_full_tokenizer", "params": {"self": ""}, "params_descr": {"self": ""}, "params_occur": {"self": [["self", "assertAllEqual", "tokens"], ["self", "assertAllEqual", "tokenizer", "convert_tokens_to_ids", "tokens"]]}, "params_p": {"args": [], "kwargs": [], "self": []}, "q_name": "TokenizationTest.test_full_tokenizer", "ret_exprs": [], "ret_type": "", "variables": {"tokenizer": "", "tokens": "", "vocab_file": "", "vocab_tokens": ""}, "variables_p": {"tokenizer": [["str", 0.2734350074271494], ["dict", 0.18631558804752346], ["Union[int, str]", 0.11852126836433144], ["List[str]", 0.09278604987468693]], "tokens": [["int", 0.22877925494959916], ["str", 0.09102033736053698]], "vocab_file": [["Dict[str, str]", 0.38456389429448024], ["dict", 0.28866821419648736], ["Dict[str, List[str]]", 0.1322187828750747]], "vocab_tokens": [["str", 0.6956220615997436], ["Union[Literal, str]", 0.09429605608994714]]}}, {"docstring": {"func": null, "long_descr": null, "ret": null}, "fn_lc": [[51, 2], [56, 45]], "fn_var_ln": {"tokenizer": [[52, 4], [52, 13]]}, "fn_var_occur": {"tokenizer": [["tokenizer", "tokenization", "BasicTokenizer"], ["self", "assertAllEqual", "tokenizer", "tokenize"]]}, "name": "test_chinese", "params": {"self": ""}, "params_descr": {"self": ""}, "params_occur": {"self": [["self", "assertAllEqual", "tokenizer", "tokenize"]]}, "params_p": {"args": [], "kwargs": [], "self": []}, "q_name": "TokenizationTest.test_chinese", "ret_exprs": [], "ret_type": "", "variables": {"tokenizer": ""}, "variables_p": {"tokenizer": [["int", 0.39999999997499996], ["bytes", 1.3149481847249651e-11], ["Type[str]", 8.087451549011318e-12]]}}, {"docstring": {"func": null, "long_descr": null, "ret": null}, "fn_lc": [[58, 2], [64, 69]], "fn_var_ln": {"tokenizer": [[59, 4], [59, 13]]}, "fn_var_occur": {"tokenizer": [["tokenizer", "tokenization", "BasicTokenizer", "do_lower_case", "True"], ["self", "assertAllEqual", "tokenizer", "tokenize"], ["self", "assertAllEqual", "tokenizer", "tokenize"]]}, "name": "test_basic_tokenizer_lower", "params": {"self": ""}, "params_descr": {"self": ""}, "params_occur": {"self": [["self", "assertAllEqual", "tokenizer", "tokenize"], ["self", "assertAllEqual", "tokenizer", "tokenize"]]}, "params_p": {"args": [], "kwargs": [], "self": []}, "q_name": "TokenizationTest.test_basic_tokenizer_lower", "ret_exprs": [], "ret_type": "", "variables": {"tokenizer": ""}, "variables_p": {"tokenizer": []}}, {"docstring": {"func": null, "long_descr": null, "ret": null}, "fn_lc": [[66, 2], [71, 49]], "fn_var_ln": {"tokenizer": [[67, 4], [67, 13]]}, "fn_var_occur": {"tokenizer": [["tokenizer", "tokenization", "BasicTokenizer", "do_lower_case", "False"], ["self", "assertAllEqual", "tokenizer", "tokenize"]]}, "name": "test_basic_tokenizer_no_lower", "params": {"self": ""}, "params_descr": {"self": ""}, "params_occur": {"self": [["self", "assertAllEqual", "tokenizer", "tokenize"]]}, "params_p": {"args": [], "kwargs": [], "self": []}, "q_name": "TokenizationTest.test_basic_tokenizer_no_lower", "ret_exprs": [], "ret_type": "", "variables": {"tokenizer": ""}, "variables_p": {"tokenizer": []}}, {"docstring": {"func": null, "long_descr": null, "ret": null}, "fn_lc": [[73, 2], [91, 76]], "fn_var_ln": {"tokenizer": [[82, 4], [82, 13]], "vocab": [[79, 4], [79, 9]], "vocab_tokens": [[74, 4], [74, 16]]}, "fn_var_occur": {"tokenizer": [["tokenizer", "tokenization", "WordpieceTokenizer", "vocab", "vocab"], ["self", "assertAllEqual", "tokenizer", "tokenize"], ["self", "assertAllEqual", "tokenizer", "tokenize"], ["self", "assertAllEqual", "tokenizer", "tokenize"]], "vocab": [["vocab", "token", "i"], ["tokenizer", "tokenization", "WordpieceTokenizer", "vocab", "vocab"]], "vocab_tokens": [["enumerate", "vocab_tokens"]]}, "name": "test_wordpiece_tokenizer", "params": {"self": ""}, "params_descr": {"self": ""}, "params_occur": {"self": [["self", "assertAllEqual", "tokenizer", "tokenize"], ["self", "assertAllEqual", "tokenizer", "tokenize"], ["self", "assertAllEqual", "tokenizer", "tokenize"]]}, "params_p": {"args": [], "kwargs": [], "self": []}, "q_name": "TokenizationTest.test_wordpiece_tokenizer", "ret_exprs": [], "ret_type": "", "variables": {"tokenizer": "", "vocab": "", "vocab_tokens": ""}, "variables_p": {"tokenizer": [], "vocab": [["int", 0.29428813670243426], ["str", 0.22705327932214509], ["bytes", 0.1755540961828233], ["dict", 0.11566142796602422], ["deque[str]", 0.09021588744327858]], "vocab_tokens": [["frozenset[Any]", 0.7], ["Set[str]", 0.09999999999999999], ["List[Union[Any, Any]]", 0.09999999999999999], ["List[str]", 0.09999999999999999]]}}, {"docstring": {"func": null, "long_descr": null, "ret": null}, "fn_lc": [[93, 2], [105, 79]], "fn_var_ln": {"vocab": [[99, 4], [99, 9]], "vocab_tokens": [[94, 4], [94, 16]]}, "fn_var_occur": {"vocab": [["vocab", "token", "i"], ["self", "assertAllEqual", "tokenization", "convert_tokens_to_ids", "vocab"]], "vocab_tokens": [["enumerate", "vocab_tokens"]]}, "name": "test_convert_tokens_to_ids", "params": {"self": ""}, "params_descr": {"self": ""}, "params_occur": {"self": [["self", "assertAllEqual", "tokenization", "convert_tokens_to_ids", "vocab"]]}, "params_p": {"args": [], "kwargs": [], "self": []}, "q_name": "TokenizationTest.test_convert_tokens_to_ids", "ret_exprs": [], "ret_type": "", "variables": {"vocab": "", "vocab_tokens": ""}, "variables_p": {"vocab": [["int", 0.46097098441559947], ["list", 0.45894947374908707], ["deque[int]", 0.08007954183531348]], "vocab_tokens": [["frozenset[Any]", 0.7], ["Set[str]", 0.09999999999999999], ["List[Union[Any, Any]]", 0.09999999999999999], ["List[str]", 0.09999999999999999]]}}, {"docstring": {"func": null, "long_descr": null, "ret": null}, "fn_lc": [[107, 2], [115, 55]], "fn_var_ln": {}, "fn_var_occur": {}, "name": "test_is_whitespace", "params": {"self": ""}, "params_descr": {"self": ""}, "params_occur": {"self": [["self", "assertTrue", "tokenization", "_is_whitespace"], ["self", "assertTrue", "tokenization", "_is_whitespace"], ["self", "assertTrue", "tokenization", "_is_whitespace"], ["self", "assertTrue", "tokenization", "_is_whitespace"], ["self", "assertTrue", "tokenization", "_is_whitespace"], ["self", "assertFalse", "tokenization", "_is_whitespace"], ["self", "assertFalse", "tokenization", "_is_whitespace"]]}, "params_p": {"args": [], "kwargs": [], "self": []}, "q_name": "TokenizationTest.test_is_whitespace", "ret_exprs": [], "ret_type": "", "variables": {}, "variables_p": {}}, {"docstring": {"func": null, "long_descr": null, "ret": null}, "fn_lc": [[117, 2], [124, 61]], "fn_var_ln": {}, "fn_var_occur": {}, "name": "test_is_control", "params": {"self": ""}, "params_descr": {"self": ""}, "params_occur": {"self": [["self", "assertTrue", "tokenization", "_is_control"], ["self", "assertFalse", "tokenization", "_is_control"], ["self", "assertFalse", "tokenization", "_is_control"], ["self", "assertFalse", "tokenization", "_is_control"], ["self", "assertFalse", "tokenization", "_is_control"], ["self", "assertFalse", "tokenization", "_is_control"]]}, "params_p": {"args": [], "kwargs": [], "self": []}, "q_name": "TokenizationTest.test_is_control", "ret_exprs": [], "ret_type": "", "variables": {}, "variables_p": {}}, {"docstring": {"func": null, "long_descr": null, "ret": null}, "fn_lc": [[126, 2], [133, 56]], "fn_var_ln": {}, "fn_var_occur": {}, "name": "test_is_punctuation", "params": {"self": ""}, "params_descr": {"self": ""}, "params_occur": {"self": [["self", "assertTrue", "tokenization", "_is_punctuation"], ["self", "assertTrue", "tokenization", "_is_punctuation"], ["self", "assertTrue", "tokenization", "_is_punctuation"], ["self", "assertTrue", "tokenization", "_is_punctuation"], ["self", "assertFalse", "tokenization", "_is_punctuation"], ["self", "assertFalse", "tokenization", "_is_punctuation"]]}, "params_p": {"args": [], "kwargs": [], "self": []}, "q_name": "TokenizationTest.test_is_punctuation", "ret_exprs": [], "ret_type": "", "variables": {}, "variables_p": {}}], "name": "TokenizationTest", "q_name": "TokenizationTest", "variables": {}, "variables_p": {}}], "funcs": [], "imports": ["__future__", "absolute_import", "__future__", "division", "__future__", "print_function", "os", "tempfile", "tokenization", "six", "tensorflow", "tf"], "mod_var_ln": {}, "mod_var_occur": {}, "no_types_annot": {"D": 0, "I": 0, "U": 21}, "session_id": "oxqDPvsJ4ZhUgF7xqCyI2cbScwDEvVfeRZK4viTJNpw", "set": null, "tc": [false, null], "type_annot_cove": 0.0, "typed_seq": "", "untyped_seq": "", "variables": {}, "variables_p": {}}}