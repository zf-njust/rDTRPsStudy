{"error": null, "response": {"classes": [{"cls_lc": [[161, 0], [182, 48]], "cls_var_ln": {}, "cls_var_occur": {}, "funcs": [{"docstring": {"func": null, "long_descr": null, "ret": null}, "fn_lc": [[164, 2], [168, 67]], "fn_var_ln": {"basic_tokenizer": [[167, 4], [167, 24]], "inv_vocab": [[166, 4], [166, 18]], "vocab": [[165, 4], [165, 14]], "wordpiece_tokenizer": [[168, 4], [168, 28]]}, "fn_var_occur": {"basic_tokenizer": [["self", "basic_tokenizer", "BasicTokenizer", "do_lower_case", "do_lower_case"]], "inv_vocab": [["self", "inv_vocab", "v", "k", "k", "v", "self", "vocab", "items"]], "vocab": [["self", "vocab", "load_vocab", "vocab_file"], ["self", "inv_vocab", "v", "k", "k", "v", "self", "vocab", "items"], ["self", "wordpiece_tokenizer", "WordpieceTokenizer", "vocab", "self", "vocab"]], "wordpiece_tokenizer": [["self", "wordpiece_tokenizer", "WordpieceTokenizer", "vocab", "self", "vocab"]]}, "name": "__init__", "params": {"do_lower_case": "", "self": "", "vocab_file": ""}, "params_descr": {"do_lower_case": "", "self": "", "vocab_file": ""}, "params_occur": {"do_lower_case": [["self", "basic_tokenizer", "BasicTokenizer", "do_lower_case", "do_lower_case"]], "self": [["self", "vocab", "load_vocab", "vocab_file"], ["self", "inv_vocab", "v", "k", "k", "v", "self", "vocab", "items"], ["self", "basic_tokenizer", "BasicTokenizer", "do_lower_case", "do_lower_case"], ["self", "wordpiece_tokenizer", "WordpieceTokenizer", "vocab", "self", "vocab"]], "vocab_file": [["self", "vocab", "load_vocab", "vocab_file"]]}, "params_p": {"args": [], "do_lower_case": [["str", 0.22656406902928222], ["List[str]", 0.09447173247022689], ["dict", 0.06966047462170857]], "kwargs": [], "self": [], "vocab_file": [["str", 0.22656406902928222], ["List[str]", 0.09447173247022689], ["dict", 0.06966047462170857]]}, "q_name": "FullTokenizer.__init__", "ret_exprs": [], "ret_type": "", "variables": {"basic_tokenizer": "", "inv_vocab": "", "vocab": "", "wordpiece_tokenizer": ""}, "variables_p": {"basic_tokenizer": [["str", 0.30000000000000004], ["Dict[str, Any]", 0.1], ["dict", 0.1], ["int", 0.1]], "inv_vocab": [["Dict[str, str]", 0.7163484724197023], ["Dict[Tuple[str, int], str]", 0.09682528260530458], ["str", 0.09408709595643319], ["Dict[Tuple[str, str], str]", 0.09273914901855966]], "vocab": [["Pattern[str]", 0.35129921886913057], ["Dict[str, Dict[str, int]]", 0.13725156054753945], ["frozenset[Any]", 0.09827910352255036], ["Counter[str]", 0.0865175788659527]], "wordpiece_tokenizer": [["int", 0.3], ["Dict[str, str]", 0.3], ["str", 0.19999999999999998]]}}, {"docstring": {"func": null, "long_descr": null, "ret": null}, "fn_lc": [[170, 2], [176, 23]], "fn_var_ln": {"split_tokens": [[171, 4], [171, 16]]}, "fn_var_occur": {"split_tokens": [["split_tokens", "append", "sub_token"]]}, "name": "tokenize", "params": {"self": "", "text": ""}, "params_descr": {"self": "", "text": ""}, "params_occur": {"self": [["self", "basic_tokenizer", "tokenize", "text"], ["self", "wordpiece_tokenizer", "tokenize", "token"]], "text": [["self", "basic_tokenizer", "tokenize", "text"]]}, "params_p": {"args": [], "kwargs": [], "self": [], "text": [["str", 0.9999999999999999]]}, "q_name": "FullTokenizer.tokenize", "ret_exprs": ["return split_tokens"], "ret_type": "", "ret_type_p": [["str", 0.18138589583382367], ["OrderedDict", 0.1435753906787657], ["Callable", 0.10027879876049921], ["bool", 0.0988845177903579], ["Dict[str, str]", 0.09565563875819137]], "variables": {"split_tokens": ""}, "variables_p": {"split_tokens": [["list", 0.5492216130489026], ["str", 0.2683881812001787], ["Counter[Any]", 0.10564644891503007], ["dict", 0.07674375683588851]]}}, {"docstring": {"func": null, "long_descr": null, "ret": null}, "fn_lc": [[178, 2], [179, 47]], "fn_var_ln": {}, "fn_var_occur": {}, "name": "convert_tokens_to_ids", "params": {"self": "", "tokens": ""}, "params_descr": {"self": "", "tokens": ""}, "params_occur": {"self": [["convert_by_vocab", "self", "vocab", "tokens"]], "tokens": [["convert_by_vocab", "self", "vocab", "tokens"]]}, "params_p": {"args": [], "kwargs": [], "self": [], "tokens": [["str", 0.31802302831027857], ["Tuple[int, str]", 0.10053584126109617]]}, "q_name": "FullTokenizer.convert_tokens_to_ids", "ret_exprs": ["return convert_by_vocab(self.vocab, tokens)"], "ret_type": "", "ret_type_p": [["str", 0.8351093360203474], ["Dict[str, str]", 0.08488727505391623], ["List[Dict[str, Any]]", 0.08000338892573633]], "variables": {}, "variables_p": {}}, {"docstring": {"func": null, "long_descr": null, "ret": null}, "fn_lc": [[181, 2], [182, 48]], "fn_var_ln": {}, "fn_var_occur": {}, "name": "convert_ids_to_tokens", "params": {"ids": "", "self": ""}, "params_descr": {"ids": "", "self": ""}, "params_occur": {"ids": [["convert_by_vocab", "self", "inv_vocab", "ids"]], "self": [["convert_by_vocab", "self", "inv_vocab", "ids"]]}, "params_p": {"args": [], "ids": [["str", 0.5424667793597822], ["Iterable[int]", 0.18720169784818697]], "kwargs": [], "self": []}, "q_name": "FullTokenizer.convert_ids_to_tokens", "ret_exprs": ["return convert_by_vocab(self.inv_vocab, ids)"], "ret_type": "", "ret_type_p": [["bytes", 0.3607414956259939], ["str", 0.19809447909358902], ["Dict[str, Any]", 0.11069452169772849]], "variables": {}, "variables_p": {}}], "name": "FullTokenizer", "q_name": "FullTokenizer", "variables": {}, "variables_p": {}}, {"cls_lc": [[185, 0], [297, 26]], "cls_var_ln": {}, "cls_var_occur": {}, "funcs": [{"docstring": {"func": "Constructs a BasicTokenizer.", "long_descr": null, "ret": null}, "fn_lc": [[188, 2], [194, 38]], "fn_var_ln": {"do_lower_case": [[194, 4], [194, 22]]}, "fn_var_occur": {"do_lower_case": [["self", "do_lower_case", "do_lower_case"]]}, "name": "__init__", "params": {"do_lower_case": "", "self": ""}, "params_descr": {"do_lower_case": "Whether to lower case the input.", "self": ""}, "params_occur": {"do_lower_case": [["self", "do_lower_case", "do_lower_case"]], "self": [["self", "do_lower_case", "do_lower_case"]]}, "params_p": {"args": [], "do_lower_case": [["int", 0.6878754016889279], ["dict", 0.126686257297041], ["bool", 0.12662932744240388], ["float", 0.05880901357162729]], "kwargs": [], "self": []}, "q_name": "BasicTokenizer.__init__", "ret_exprs": [], "ret_type": "", "variables": {"do_lower_case": ""}, "variables_p": {"do_lower_case": [["int", 0.5337958625816787], ["str", 0.21776740968572422], ["float", 0.15551105434990772], ["bytes", 0.09292567338268928]]}}, {"docstring": {"func": "Tokenizes a piece of text.", "long_descr": null, "ret": null}, "fn_lc": [[196, 2], [218, 24]], "fn_var_ln": {"orig_tokens": [[209, 4], [209, 15]], "output_tokens": [[217, 4], [217, 17]], "split_tokens": [[210, 4], [210, 16]], "text": [[207, 4], [207, 8]], "token": [[214, 8], [214, 13]]}, "fn_var_occur": {"orig_tokens": [["orig_tokens", "whitespace_tokenize", "text"]], "output_tokens": [["output_tokens", "whitespace_tokenize", "join", "split_tokens"]], "split_tokens": [["split_tokens", "extend", "self", "_run_split_on_punc", "token"], ["output_tokens", "whitespace_tokenize", "join", "split_tokens"]], "text": [["text", "convert_to_unicode", "text"], ["text", "self", "_clean_text", "text"], ["text", "self", "_tokenize_chinese_chars", "text"], ["orig_tokens", "whitespace_tokenize", "text"]], "token": [["token", "token", "lower"], ["token", "self", "_run_strip_accents", "token"], ["split_tokens", "extend", "self", "_run_split_on_punc", "token"]]}, "name": "tokenize", "params": {"self": "", "text": ""}, "params_descr": {"self": "", "text": ""}, "params_occur": {"self": [["text", "self", "_clean_text", "text"], ["text", "self", "_tokenize_chinese_chars", "text"], ["self", "do_lower_case"], ["token", "self", "_run_strip_accents", "token"], ["split_tokens", "extend", "self", "_run_split_on_punc", "token"]], "text": [["text", "convert_to_unicode", "text"], ["text", "self", "_clean_text", "text"], ["text", "self", "_tokenize_chinese_chars", "text"], ["orig_tokens", "whitespace_tokenize", "text"]]}, "params_p": {"args": [], "kwargs": [], "self": [], "text": [["str", 0.808558438450721]]}, "q_name": "BasicTokenizer.tokenize", "ret_exprs": ["return output_tokens"], "ret_type": "", "ret_type_p": [["str", 0.10024231489482435], ["Dict[str, Any]", 0.10024231489482435], ["dict", 0.09905474765158162], ["Awaitable", 0.0961839424995826]], "variables": {"orig_tokens": "", "output_tokens": "", "split_tokens": "", "text": "", "token": ""}, "variables_p": {"orig_tokens": [["frozenset[Any]", 0.7], ["Set[str]", 0.09999999999999999], ["List[Union[Any, Any]]", 0.09999999999999999], ["List[str]", 0.09999999999999999]], "output_tokens": [["str", 0.7528282612910366]], "split_tokens": [["list", 0.41312635107430606], ["List[str]", 0.4129764085666985], ["Type[List[int]]", 0.08902236612271139], ["Iterator[Any]", 0.08487487423628422]], "text": [["Tuple[Type[set], Type[frozenset]]", 0.1883193423662781], ["List[str]", 0.11999867130226981], ["str", 0.10713633655902977]], "token": [["list", 0.21134111516656062], ["Tuple[str, str]", 0.10090698398541496], ["List[Union[Any, Any, Any]]", 0.09495183765402529], ["Dict[str, Any]", 0.0926075999356421], ["tuple", 0.08509860318180873], ["Dict[str, int]", 0.0791669064316912]]}}, {"docstring": {"func": "Strips accents from a piece of text.", "long_descr": null, "ret": null}, "fn_lc": [[220, 2], [229, 26]], "fn_var_ln": {"cat": [[225, 6], [225, 9]], "output": [[223, 4], [223, 10]], "text": [[222, 4], [222, 8]]}, "fn_var_occur": {"cat": [["cat", "unicodedata", "category", "char"]], "output": [["output", "append", "char"], ["join", "output"]], "text": [["text", "unicodedata", "normalize", "text"]]}, "name": "_run_strip_accents", "params": {"self": "", "text": ""}, "params_descr": {"self": "", "text": ""}, "params_occur": {"self": [], "text": [["text", "unicodedata", "normalize", "text"]]}, "params_p": {"args": [], "kwargs": [], "self": [], "text": [["str", 0.9999999999999999]]}, "q_name": "BasicTokenizer._run_strip_accents", "ret_exprs": ["return \"\".join(output)"], "ret_type": "", "ret_type_p": [["Set[str]", 0.29018735931857614], ["str", 0.19559992928453718], ["IO[str]", 0.09614419986957823], ["List[Dict[str, Any]]", 0.08855360879959795]], "variables": {"cat": "", "output": "", "text": ""}, "variables_p": {"cat": [["int", 0.2], ["dict", 0.1], ["List[int]", 0.1], ["List[List[int]]", 0.1], ["str", 0.1]], "output": [["List[str]", 0.5459689351376019], ["list", 0.35344640732271454], ["Set[str]", 0.10058465753968361]], "text": [["int", 0.7999999999999999], ["str", 0.19999999999999998]]}}, {"docstring": {"func": "Splits punctuation on a piece of text.", "long_descr": null, "ret": null}, "fn_lc": [[231, 2], [249, 39]], "fn_var_ln": {"char": [[238, 6], [238, 10]], "chars": [[233, 4], [233, 9]], "i": [[234, 4], [234, 5]], "output": [[236, 4], [236, 10]], "start_new_word": [[245, 8], [245, 22]]}, "fn_var_occur": {"char": [["char", "chars", "i"], ["_is_punctuation", "char"], ["output", "append", "char"], ["output", "append", "char"]], "chars": [["chars", "list", "text"], ["i", "len", "chars"], ["char", "chars", "i"]], "i": [["i", "len", "chars"], ["char", "chars", "i"]], "output": [["output", "append", "char"], ["output", "append"], ["output", "append", "char"], ["join", "x", "x", "output"]], "start_new_word": [["start_new_word", "True"], ["start_new_word", "True"], ["start_new_word", "False"]]}, "name": "_run_split_on_punc", "params": {"self": "", "text": ""}, "params_descr": {"self": "", "text": ""}, "params_occur": {"self": [], "text": [["chars", "list", "text"]]}, "params_p": {"args": [], "kwargs": [], "self": [], "text": [["str", 0.8989165661928522], ["list", 0.023373943864536402], ["int", 0.021320451699329605]]}, "q_name": "BasicTokenizer._run_split_on_punc", "ret_exprs": ["return [\"\".join(x) for x in output]"], "ret_type": "", "ret_type_p": [["str", 0.4384409180264316], ["int", 0.10093836672130273], ["List[float]", 0.09286240986644592], ["Tuple[int]", 0.09268763788418503], ["List[str]", 0.09268763788418503]], "variables": {"char": "", "chars": "", "i": "", "output": "", "start_new_word": ""}, "variables_p": {"char": [["str", 0.47073887639637163], ["List[str]", 0.2868234780216704], ["int", 0.142614332506154]], "chars": [["list", 0.4664723075284567], ["List[str]", 0.24562029411386394], ["Set[int]", 0.10351369775725436], ["str", 0.10036947938932629]], "i": [["int", 0.6616051500648659], ["List[int]", 0.2696856295374523]], "output": [["list", 0.8760231248595711], ["List[str]", 0.12397687514042892]], "start_new_word": [["str", 0.4539153154128158], ["List[int]", 0.17787003143081623], ["int", 0.10257151400580723], ["list", 0.0873826226248982], ["dict", 0.08328288183813684]]}}, {"docstring": {"func": "Adds whitespace around any CJK character.", "long_descr": null, "ret": null}, "fn_lc": [[251, 2], [262, 26]], "fn_var_ln": {"cp": [[255, 6], [255, 8]], "output": [[253, 4], [253, 10]]}, "fn_var_occur": {"cp": [["cp", "ord", "char"], ["self", "_is_chinese_char", "cp"]], "output": [["output", "append"], ["output", "append", "char"], ["output", "append"], ["output", "append", "char"], ["join", "output"]]}, "name": "_tokenize_chinese_chars", "params": {"self": "", "text": ""}, "params_descr": {"self": "", "text": ""}, "params_occur": {"self": [["self", "_is_chinese_char", "cp"]], "text": []}, "params_p": {"args": [], "kwargs": [], "self": [], "text": [["str", 0.8246998072714257], ["Iterable[Tuple[int, int]]", 0.09623939307368826], ["Optional[str]", 0.07906079965488633]]}, "q_name": "BasicTokenizer._tokenize_chinese_chars", "ret_exprs": ["return \"\".join(output)"], "ret_type": "", "ret_type_p": [["str", 0.42482485457307356], ["bytes", 0.10200956721499047], ["Dict[str, Any]", 0.09832516482397555], ["dict", 0.09659938500134263], ["bool", 0.09147176637587959]], "variables": {"cp": "", "output": ""}, "variables_p": {"cp": [["int", 0.22378766841786984], ["Tuple[int, int, int]", 0.10621089285193384], ["List[Tuple[Any, Any]]", 0.0904088482429965], ["float", 0.08924785082476613], ["Iterator[Any]", 0.08918806724300657]], "output": [["list", 0.6370187112366031], ["str", 0.22869876562776234], ["List[str]", 0.07056506535549295], ["bytes", 0.06371745778014147]]}}, {"docstring": {"func": "Checks whether CP is the codepoint of a CJK character.", "long_descr": null, "ret": null}, "fn_lc": [[264, 2], [284, 16]], "fn_var_ln": {}, "fn_var_occur": {}, "name": "_is_chinese_char", "params": {"cp": "", "self": ""}, "params_descr": {"cp": "", "self": ""}, "params_occur": {"cp": [["cp", "cp", "cp", "cp", "cp", "cp", "cp", "cp", "cp", "cp", "cp", "cp", "cp", "cp", "cp", "cp"]], "self": []}, "params_p": {"args": [], "cp": [["int", 0.43621564121355966], ["str", 0.2889764572887959]], "kwargs": [], "self": []}, "q_name": "BasicTokenizer._is_chinese_char", "ret_exprs": ["return True", "return False"], "ret_type": "", "ret_type_p": [["bool", 0.9999999999999999]], "variables": {}, "variables_p": {}}, {"docstring": {"func": "Performs invalid character removal and whitespace cleanup on text.", "long_descr": null, "ret": null}, "fn_lc": [[286, 2], [297, 26]], "fn_var_ln": {"cp": [[290, 6], [290, 8]], "output": [[288, 4], [288, 10]]}, "fn_var_occur": {"cp": [["cp", "ord", "char"], ["cp", "cp", "_is_control", "char"]], "output": [["output", "append"], ["output", "append", "char"], ["join", "output"]]}, "name": "_clean_text", "params": {"self": "", "text": ""}, "params_descr": {"self": "", "text": ""}, "params_occur": {"self": [], "text": []}, "params_p": {"args": [], "kwargs": [], "self": [], "text": [["str", 0.9999999991659341], ["Iterable[Tuple[int, int]]", 5.222051798454238e-10], ["Optional[str]", 3.118608671651177e-10]]}, "q_name": "BasicTokenizer._clean_text", "ret_exprs": ["return \"\".join(output)"], "ret_type": "", "ret_type_p": [["str", 0.793550235499042], ["Optional[str]", 0.11565527407266687], ["Set[str]", 0.09079449042829106]], "variables": {"cp": "", "output": ""}, "variables_p": {"cp": [["int", 0.2], ["dict", 0.1], ["List[int]", 0.1], ["List[List[int]]", 0.1], ["str", 0.1]], "output": [["list", 0.779198951865993], ["str", 0.12454497438959752], ["List[str]", 0.0962560737444095]]}}], "name": "BasicTokenizer", "q_name": "BasicTokenizer", "variables": {}, "variables_p": {}}, {"cls_lc": [[300, 0], [359, 24]], "cls_var_ln": {}, "cls_var_occur": {}, "funcs": [{"docstring": {"func": null, "long_descr": null, "ret": null}, "fn_lc": [[303, 2], [306, 60]], "fn_var_ln": {"max_input_chars_per_word": [[306, 4], [306, 33]], "unk_token": [[305, 4], [305, 18]], "vocab": [[304, 4], [304, 14]]}, "fn_var_occur": {"max_input_chars_per_word": [["self", "max_input_chars_per_word", "max_input_chars_per_word"]], "unk_token": [["self", "unk_token", "unk_token"]], "vocab": [["self", "vocab", "vocab"]]}, "name": "__init__", "params": {"max_input_chars_per_word": "", "self": "", "unk_token": "", "vocab": ""}, "params_descr": {"max_input_chars_per_word": "", "self": "", "unk_token": "", "vocab": ""}, "params_occur": {"max_input_chars_per_word": [["self", "max_input_chars_per_word", "max_input_chars_per_word"]], "self": [["self", "vocab", "vocab"], ["self", "unk_token", "unk_token"], ["self", "max_input_chars_per_word", "max_input_chars_per_word"]], "unk_token": [["self", "unk_token", "unk_token"]], "vocab": [["self", "vocab", "vocab"]]}, "params_p": {"args": [], "kwargs": [], "max_input_chars_per_word": [["int", 0.7990673586480022], ["List[Dict[str, float]]", 0.10046632067599895], ["List[str]", 0.10046632067599895]], "self": [], "unk_token": [["str", 0.20237194451550236], ["List[str]", 0.1270332950761054], ["bytes", 0.11213129363738758], ["Dict[str, int]", 0.10613304034324722], ["bool", 0.09375692713696834], ["float", 0.08125152972619855]], "vocab": [["str", 0.20237194451550236], ["List[str]", 0.1270332950761054], ["bytes", 0.11213129363738758], ["Dict[str, int]", 0.10613304034324722], ["bool", 0.09375692713696834], ["float", 0.08125152972619855]]}, "q_name": "WordpieceTokenizer.__init__", "ret_exprs": [], "ret_type": "", "variables": {"max_input_chars_per_word": "", "unk_token": "", "vocab": ""}, "variables_p": {"max_input_chars_per_word": [["str", 0.7622343649530772], ["Pattern[str]", 0.1325072100880293], ["int", 0.10525842495889369]], "unk_token": [["str", 0.4014560785703404], ["Tuple[int, str]", 0.17136760073965046], ["Set[str]", 0.1701249195805336]], "vocab": [["str", 0.30000000000000004], ["Dict[str, Any]", 0.1], ["dict", 0.1], ["int", 0.1]]}}, {"docstring": {"func": "Tokenizes a piece of text into its word pieces.", "long_descr": "This uses a greedy longest-match-first algorithm to perform tokenization\nusing the given vocabulary.\n\nFor example:\n  input = \"unaffable\"\n  output = [\"un\", \"##aff\", \"##able\"]", "ret": "A list of wordpiece tokens."}, "fn_lc": [[308, 2], [359, 24]], "fn_var_ln": {"chars": [[330, 6], [330, 11]], "cur_substr": [[346, 12], [346, 22]], "end": [[339, 8], [339, 11]], "is_bad": [[350, 10], [350, 16]], "output_tokens": [[328, 4], [328, 17]], "start": [[353, 8], [353, 13]], "sub_tokens": [[337, 6], [337, 16]], "substr": [[344, 12], [344, 18]], "text": [[326, 4], [326, 8]]}, "fn_var_occur": {"chars": [["chars", "list", "token"], ["len", "chars", "self", "max_input_chars_per_word"], ["start", "len", "chars"], ["end", "len", "chars"], ["substr", "join", "chars", "start", "end"]], "cur_substr": [["cur_substr", "None"], ["cur_substr", "substr"], ["cur_substr", "None"], ["sub_tokens", "append", "cur_substr"]], "end": [["end", "len", "chars"], ["start", "end"], ["substr", "join", "chars", "start", "end"], ["start", "end"]], "is_bad": [["is_bad", "False"], ["is_bad", "True"]], "output_tokens": [["output_tokens", "append", "self", "unk_token"], ["output_tokens", "append", "self", "unk_token"], ["output_tokens", "extend", "sub_tokens"]], "start": [["start", "len", "chars"], ["start", "end"], ["substr", "join", "chars", "start", "end"], ["start", "end"]], "sub_tokens": [["sub_tokens", "append", "cur_substr"], ["output_tokens", "extend", "sub_tokens"]], "substr": [["substr", "join", "chars", "start", "end"], ["substr", "substr"], ["substr", "self", "vocab"], ["cur_substr", "substr"]], "text": [["text", "convert_to_unicode", "text"], ["whitespace_tokenize", "text"]]}, "name": "tokenize", "params": {"self": "", "text": ""}, "params_descr": {"self": "", "text": "A single token or whitespace separated tokens. This should have\nalready been passed through `BasicTokenizer."}, "params_occur": {"self": [["len", "chars", "self", "max_input_chars_per_word"], ["output_tokens", "append", "self", "unk_token"], ["substr", "self", "vocab"], ["output_tokens", "append", "self", "unk_token"]], "text": [["text", "convert_to_unicode", "text"], ["whitespace_tokenize", "text"]]}, "params_p": {"args": [], "kwargs": [], "self": [], "text": [["str", 0.5000000001175171], ["Optional[str]", 0.24999999994124134]]}, "q_name": "WordpieceTokenizer.tokenize", "ret_exprs": ["return output_tokens"], "ret_type": "", "ret_type_p": [["str", 0.10024231489482435], ["Dict[str, Any]", 0.10024231489482435], ["dict", 0.09905474765158162], ["Awaitable", 0.0961839424995826]], "variables": {"chars": "", "cur_substr": "", "end": "", "is_bad": "", "output_tokens": "", "start": "", "sub_tokens": "", "substr": "", "text": ""}, "variables_p": {"chars": [["str", 0.5532505462939369], ["list", 0.2841905668161109], ["Tuple[Type[str]]", 0.16255888688995224]], "cur_substr": [["Tuple[int, int, int, int, int, int, int, int, int, int]", 0.11106790301020245], ["List[None]", 0.09859745985253189], ["str", 0.09527759759161034]], "end": [["int", 0.34142013028233387], ["Type[range]", 0.18143105524053046], ["str", 0.18046949272341664], ["Counter[int]", 0.09337828056094974]], "is_bad": [["bool", 1.0]], "output_tokens": [["list", 0.2795894137109785], ["str", 0.21166580961940965], ["Dict[str, int]", 0.0955779650292482], ["Dict[Any, List[str]]", 0.09301348302240363]], "start": [["int", 0.8073110407657402], ["str", 0.19268895923425974]], "sub_tokens": [["list", 0.7262199052378033], ["str", 0.155078369805559]], "substr": [["str", 0.7380971488691283], ["int", 0.12726738673727941]], "text": [["str", 0.3670716528104828], ["Type[str]", 0.12309583544264206], ["Pattern[str]", 0.09935039315409339], ["List[Tuple[str, Any]]", 0.09017372022263132], ["Type[Dict[str, Any]]", 0.08882612638510848]]}}], "name": "WordpieceTokenizer", "q_name": "WordpieceTokenizer", "variables": {}, "variables_p": {}}], "funcs": [{"docstring": {"func": "Checks whether the casing config is consistent with the checkpoint name.", "long_descr": null, "ret": null}, "fn_lc": [[28, 0], [75, 80]], "fn_var_ln": {"actual_flag": [[64, 4], [64, 15]], "case_name": [[65, 4], [65, 13]], "cased_models": [[50, 2], [50, 14]], "is_bad_config": [[63, 4], [63, 17]], "lower_models": [[45, 2], [45, 14]], "m": [[39, 2], [39, 3]], "model_name": [[43, 2], [43, 12]], "opposite_flag": [[66, 4], [66, 17]]}, "fn_var_occur": {"actual_flag": [["ValueError", "actual_flag", "init_checkpoint", "model_name", "case_name", "opposite_flag"]], "case_name": [["ValueError", "actual_flag", "init_checkpoint", "model_name", "case_name", "opposite_flag"]], "cased_models": [["model_name", "cased_models", "do_lower_case"]], "is_bad_config": [["is_bad_config", "False"], ["is_bad_config", "True"], ["is_bad_config", "True"]], "lower_models": [["model_name", "lower_models", "do_lower_case"]], "m": [["m", "re", "match", "init_checkpoint"], ["m", "None"], ["model_name", "m", "group"]], "model_name": [["model_name", "m", "group"], ["model_name", "lower_models", "do_lower_case"], ["model_name", "cased_models", "do_lower_case"], ["ValueError", "actual_flag", "init_checkpoint", "model_name", "case_name", "opposite_flag"]], "opposite_flag": [["ValueError", "actual_flag", "init_checkpoint", "model_name", "case_name", "opposite_flag"]]}, "name": "validate_case_matches_checkpoint", "params": {"do_lower_case": "", "init_checkpoint": ""}, "params_descr": {"do_lower_case": "", "init_checkpoint": ""}, "params_occur": {"do_lower_case": [["model_name", "lower_models", "do_lower_case"], ["model_name", "cased_models", "do_lower_case"]], "init_checkpoint": [["m", "re", "match", "init_checkpoint"], ["ValueError", "actual_flag", "init_checkpoint", "model_name", "case_name", "opposite_flag"]]}, "params_p": {"args": [], "do_lower_case": [["float", 0.3866491915497047], ["str", 0.20546226257729902], ["Optional[str]", 0.10727130787707377]], "init_checkpoint": [["str", 0.4934949796998632], ["Match", 0.1999510354560465], ["Optional[str]", 0.09916909615547416], ["bool", 0.08829145328809943]], "kwargs": []}, "q_name": "validate_case_matches_checkpoint", "ret_exprs": ["return", "return"], "ret_type": "", "ret_type_p": [["Iterator[Dict[str, str]]", 0.09554728827877883]], "variables": {"actual_flag": "", "case_name": "", "cased_models": "", "is_bad_config": "", "lower_models": "", "m": "", "model_name": "", "opposite_flag": ""}, "variables_p": {"actual_flag": [["str", 1.0]], "case_name": [["str", 1.0]], "cased_models": [["List[str]", 0.29692137314063693], ["list", 0.10203134886837806], ["Tuple[Literal, Literal]", 0.09897379104687898], ["dict", 0.09863842568449138]], "is_bad_config": [["bool", 0.7889761261881428], ["Dict[str, Optional[bool]]", 0.11844806933446599], ["dict", 0.09257580447739117]], "lower_models": [["List[str]", 0.29692137314063693], ["list", 0.10203134886837806], ["Tuple[Literal, Literal]", 0.09897379104687898], ["dict", 0.09863842568449138]], "m": [["str", 0.5612870763315768], ["set", 0.14259218380379432], ["Type[Tuple[Any, str]]", 0.09603977136842025]], "model_name": [["str", 0.9111285700447348], ["Optional[str]", 0.0888714299552651]], "opposite_flag": [["str", 1.0]]}}, {"docstring": {"func": "Converts `text` to Unicode (if it's not already), assuming utf-8 input.", "long_descr": null, "ret": null}, "fn_lc": [[78, 0], [95, 59]], "fn_var_ln": {}, "fn_var_occur": {}, "name": "convert_to_unicode", "params": {"text": ""}, "params_descr": {"text": ""}, "params_occur": {"text": [["isinstance", "text", "str"], ["isinstance", "text", "bytes"], ["text", "decode"], ["ValueError", "type", "text"], ["isinstance", "text", "str"], ["text", "decode"], ["isinstance", "text", "unicode"], ["ValueError", "type", "text"]]}, "params_p": {"args": [], "kwargs": [], "text": [["str", 0.8925321467707531], ["bool", 0.10746785322924694]]}, "q_name": "convert_to_unicode", "ret_exprs": ["return text", "return text.decode(\"utf-8\", \"ignore\")", "return text.decode(\"utf-8\", \"ignore\")", "return text"], "ret_type": "", "ret_type_p": [["bytes", 0.12413401529678125], ["Iterable[int]", 0.10993665090190555], ["bool", 0.09776969276836894]], "variables": {}, "variables_p": {}}, {"docstring": {"func": "Returns text encoded in a way suitable for print or `tf.logging`.", "long_descr": null, "ret": null}, "fn_lc": [[98, 0], [118, 59]], "fn_var_ln": {}, "fn_var_occur": {}, "name": "printable_text", "params": {"text": ""}, "params_descr": {"text": ""}, "params_occur": {"text": [["isinstance", "text", "str"], ["isinstance", "text", "bytes"], ["text", "decode"], ["ValueError", "type", "text"], ["isinstance", "text", "str"], ["isinstance", "text", "unicode"], ["text", "encode"], ["ValueError", "type", "text"]]}, "params_p": {"args": [], "kwargs": [], "text": [["str", 0.7960562712238816], ["bool", 0.11513076932948592], ["Union[str, bytes]", 0.08881295944663255]]}, "q_name": "printable_text", "ret_exprs": ["return text", "return text.decode(\"utf-8\", \"ignore\")", "return text", "return text.encode(\"utf-8\")"], "ret_type": "", "ret_type_p": [["Type", 0.12219742794984244], ["str", 0.08726826771424058]], "variables": {}, "variables_p": {}}, {"docstring": {"func": "Loads a vocabulary file into a dictionary.", "long_descr": null, "ret": null}, "fn_lc": [[121, 0], [133, 14]], "fn_var_ln": {"index": [[124, 2], [124, 7]], "token": [[130, 6], [130, 11]], "vocab": [[123, 2], [123, 7]]}, "fn_var_occur": {"index": [["vocab", "token", "index"]], "token": [["token", "convert_to_unicode", "reader", "readline"], ["token", "token", "strip"], ["vocab", "token", "index"]], "vocab": [["vocab", "collections", "OrderedDict"], ["vocab", "token", "index"]]}, "name": "load_vocab", "params": {"vocab_file": ""}, "params_descr": {"vocab_file": ""}, "params_occur": {"vocab_file": [["tf", "gfile", "GFile", "vocab_file", "reader"]]}, "params_p": {"args": [], "kwargs": [], "vocab_file": [["str", 0.6268256325643944], ["bytes", 0.1766064259310187], ["bool", 0.10184990617190609]]}, "q_name": "load_vocab", "ret_exprs": ["return vocab"], "ret_type": "", "ret_type_p": [["str", 0.6], ["bool", 0.19999999999999998]], "variables": {"index": "", "token": "", "vocab": ""}, "variables_p": {"index": [["str", 0.9999999998698345], ["List[str]", 6.328840686460834e-11], ["int", 2.3286723644833832e-11], ["list", 1.541006070907159e-11], ["Type[Tuple[int, int]]", 1.4403360637701629e-11]], "token": [["str", 0.6679767123339335]], "vocab": [["List[str]", 0.6930303713243493], ["int", 0.08627045405692109], ["Counter[Any]", 0.07324547316953815]]}}, {"docstring": {"func": "Converts a sequence of [tokens|ids] using the vocab.", "long_descr": null, "ret": null}, "fn_lc": [[136, 0], [141, 15]], "fn_var_ln": {"output": [[138, 2], [138, 8]]}, "fn_var_occur": {"output": [["output", "append", "vocab", "item"]]}, "name": "convert_by_vocab", "params": {"items": "", "vocab": ""}, "params_descr": {"items": "", "vocab": ""}, "params_occur": {"items": [], "vocab": [["output", "append", "vocab", "item"]]}, "params_p": {"args": [], "items": [["int", 0.20330839274046147], ["T", 0.10165419637023074], ["float", 0.10059295496114427], ["bool", 0.10059295496114427], ["Callable", 0.10045838506005732]], "kwargs": [], "vocab": [["bytes", 0.10916777222542468], ["bool", 0.09234280108548588], ["Dict[str, Any]", 0.087443637198961], ["int", 0.08461856150699348], ["str", 0.08148362997398684], ["Sequence[str]", 0.07874875810318922]]}, "q_name": "convert_by_vocab", "ret_exprs": ["return output"], "ret_type": "", "ret_type_p": [["str", 0.10127279938208307], ["int", 0.09620158380187394], ["dict", 0.09618707545533217], ["tensorflow.Tensor", 0.09382515222143588]], "variables": {"output": ""}, "variables_p": {"output": [["list", 0.34160587764647105], ["List[Dict[str, Any]]", 0.19513031512750645], ["Dict[str, Any]", 0.0983804343447181], ["List[str]", 0.09417980512017493], ["Dict[int, Any]", 0.08775202820194856], ["List[None]", 0.08459792707460863]]}}, {"docstring": {"func": null, "long_descr": null, "ret": null}, "fn_lc": [[144, 0], [145, 40]], "fn_var_ln": {}, "fn_var_occur": {}, "name": "convert_tokens_to_ids", "params": {"tokens": "", "vocab": ""}, "params_descr": {"tokens": "", "vocab": ""}, "params_occur": {"tokens": [["convert_by_vocab", "vocab", "tokens"]], "vocab": [["convert_by_vocab", "vocab", "tokens"]]}, "params_p": {"args": [], "kwargs": [], "tokens": [["str", 0.7809066743674253]], "vocab": [["str", 0.7809066743674253]]}, "q_name": "convert_tokens_to_ids", "ret_exprs": ["return convert_by_vocab(vocab, tokens)"], "ret_type": "", "ret_type_p": [["str", 0.8351093360203474], ["Dict[str, str]", 0.08488727505391623], ["List[Dict[str, Any]]", 0.08000338892573633]], "variables": {}, "variables_p": {}}, {"docstring": {"func": null, "long_descr": null, "ret": null}, "fn_lc": [[148, 0], [149, 41]], "fn_var_ln": {}, "fn_var_occur": {}, "name": "convert_ids_to_tokens", "params": {"ids": "", "inv_vocab": ""}, "params_descr": {"ids": "", "inv_vocab": ""}, "params_occur": {"ids": [["convert_by_vocab", "inv_vocab", "ids"]], "inv_vocab": [["convert_by_vocab", "inv_vocab", "ids"]]}, "params_p": {"args": [], "ids": [["str", 0.29412002280581123], ["Iterable[int]", 0.11332371453319205]], "inv_vocab": [["str", 0.29412002280581123], ["Iterable[int]", 0.11332371453319205]], "kwargs": []}, "q_name": "convert_ids_to_tokens", "ret_exprs": ["return convert_by_vocab(inv_vocab, ids)"], "ret_type": "", "ret_type_p": [["bytes", 0.3607414956259939], ["str", 0.19809447909358902], ["Dict[str, Any]", 0.11069452169772849]], "variables": {}, "variables_p": {}}, {"docstring": {"func": "Runs basic whitespace cleaning and splitting on a piece of text.", "long_descr": null, "ret": null}, "fn_lc": [[152, 0], [158, 15]], "fn_var_ln": {"text": [[154, 2], [154, 6]], "tokens": [[157, 2], [157, 8]]}, "fn_var_occur": {"text": [["text", "text", "strip"], ["tokens", "text", "split"]], "tokens": [["tokens", "text", "split"]]}, "name": "whitespace_tokenize", "params": {"text": ""}, "params_descr": {"text": ""}, "params_occur": {"text": [["text", "text", "strip"], ["tokens", "text", "split"]]}, "params_p": {"args": [], "kwargs": [], "text": [["str", 0.9999999999999999]]}, "q_name": "whitespace_tokenize", "ret_exprs": ["return []", "return tokens"], "ret_type": "", "ret_type_p": [["bool", 0.2141315794840783], ["dict", 0.21367779916013616], ["Iterable[str]", 0.12018174912075806], ["List[List[str]]", 0.10531437715088209], ["str", 0.08092707459073291]], "variables": {"text": "", "tokens": ""}, "variables_p": {"text": [["Pattern[str]", 0.5450921538068483], ["str", 0.4549078461931516]], "tokens": [["str", 0.6807328870272219], ["Optional[str]", 0.17800372279639576], ["int", 0.14126339017638237]]}}, {"docstring": {"func": "Checks whether `chars` is a whitespace character.", "long_descr": null, "ret": null}, "fn_lc": [[362, 0], [371, 14]], "fn_var_ln": {"cat": [[368, 2], [368, 5]]}, "fn_var_occur": {"cat": [["cat", "unicodedata", "category", "char"]]}, "name": "_is_whitespace", "params": {"char": ""}, "params_descr": {"char": ""}, "params_occur": {"char": [["char", "char", "char", "char"], ["cat", "unicodedata", "category", "char"]]}, "params_p": {"args": [], "char": [["str", 0.9999999998212032]], "kwargs": []}, "q_name": "_is_whitespace", "ret_exprs": ["return True", "return True", "return False"], "ret_type": "", "ret_type_p": [["bool", 0.9999999999999999]], "variables": {"cat": ""}, "variables_p": {"cat": [["int", 0.2], ["dict", 0.1], ["List[int]", 0.1], ["List[List[int]]", 0.1], ["str", 0.1]]}}, {"docstring": {"func": "Checks whether `chars` is a control character.", "long_descr": null, "ret": null}, "fn_lc": [[374, 0], [383, 14]], "fn_var_ln": {"cat": [[380, 2], [380, 5]]}, "fn_var_occur": {"cat": [["cat", "unicodedata", "category", "char"]]}, "name": "_is_control", "params": {"char": ""}, "params_descr": {"char": ""}, "params_occur": {"char": [["char", "char", "char"], ["cat", "unicodedata", "category", "char"]]}, "params_p": {"args": [], "char": [["str", 0.9999999998212032]], "kwargs": []}, "q_name": "_is_control", "ret_exprs": ["return False", "return True", "return False"], "ret_type": "", "ret_type_p": [["bool", 0.9999999999999999]], "variables": {"cat": ""}, "variables_p": {"cat": [["int", 0.2], ["dict", 0.1], ["List[int]", 0.1], ["List[List[int]]", 0.1], ["str", 0.1]]}}, {"docstring": {"func": "Checks whether `chars` is a punctuation character.", "long_descr": null, "ret": null}, "fn_lc": [[386, 0], [399, 14]], "fn_var_ln": {"cat": [[396, 2], [396, 5]], "cp": [[388, 2], [388, 4]]}, "fn_var_occur": {"cat": [["cat", "unicodedata", "category", "char"], ["cat", "startswith"]], "cp": [["cp", "ord", "char"], ["cp", "cp", "cp", "cp", "cp", "cp", "cp", "cp"]]}, "name": "_is_punctuation", "params": {"char": ""}, "params_descr": {"char": ""}, "params_occur": {"char": [["cp", "ord", "char"], ["cat", "unicodedata", "category", "char"]]}, "params_p": {"args": [], "char": [["str", 0.9999999998212032]], "kwargs": []}, "q_name": "_is_punctuation", "ret_exprs": ["return True", "return True", "return False"], "ret_type": "", "ret_type_p": [["bool", 0.9999999999999999]], "variables": {"cat": "", "cp": ""}, "variables_p": {"cat": [["int", 0.2], ["dict", 0.1], ["List[int]", 0.1], ["List[List[int]]", 0.1], ["str", 0.1]], "cp": [["int", 0.2], ["dict", 0.1], ["List[int]", 0.1], ["List[List[int]]", 0.1], ["str", 0.1]]}}], "imports": ["__future__", "absolute_import", "__future__", "division", "__future__", "print_function", "collections", "re", "unicodedata", "six", "tensorflow", "tf"], "mod_var_ln": {}, "mod_var_occur": {}, "no_types_annot": {"D": 0, "I": 0, "U": 100}, "session_id": "4n-VNt7nQ_jhr5TbidLcEKcoPd9e_W-rot-P64mr674", "set": null, "tc": [false, null], "type_annot_cove": 0.0, "typed_seq": "", "untyped_seq": "", "variables": {}, "variables_p": {}}}